                              NBER WORKING PAPER SERIES


     EDUCATION FOR ALL? A NATIONWIDE AUDIT STUDY OF SCHOOL CHOICE

                                         Peter Bergman
                                       Isaac McFarlin Jr.

                                      Working Paper 25396
                              http://www.nber.org/papers/w25396

                     NATIONAL BUREAU OF ECONOMIC RESEARCH
                              1050 Massachusetts Avenue
                                Cambridge, MA 02138
                          December 2018, Revised January 2020


The authors are co-leaders in the production of research presented herein, and their names are
listed alphabetically. This research is supported by the University of Michigan Office of
Research, Center for the Education of Women, Center for Public Policy in Diverse Societies;
the Russell Sage Foundation and the Walton Family Foundation. We thank Joe Altonji,
David Card, Sarah Cohodes, Julie Cullen, John DiNardo, Will Dobbie, Maria Ferreya,
Caroline Hoxby, Brian Jacob, Larry Kotlikoff, Mike Lovenheim, Paco Martorell, Dick
Murnane, Jonah Rockoff, Rich Romano, David Sappington, Elizabeth Setren, Christopher
Walters, Seth Zimmerman for their detailed feedback. We also thank seminar participants at
the Columbia University, Harvard University, Princeton University, University of California-
Davis, University of Chicago, University of Florida, University of Michigan and the
Tinbergen Institute as well as conference participants at APPAM, ASSA, CESifo, IZA,
NBER Labor Studies, Stanford GSE, and University of Wisconsin IRP. We thank the Bureau
of Economic and Business Research at the University of Florida for assisting with
implementation and Katie Brown for assisting with the content analysis. We also thank Maron
Alemu, Melis Balta, Magdalena Bennett, Emily Case, Frank Cousin, Maria Keller, Kelle Parsons,
Rachel Rickles, Sarah Rinehart, Susha Roy, Michael Spaeth, Katherine Wen, and Bing Zhao for
outstanding research assistance. The study was approved by University of Michigan Institutional
Review Board (HUM00080890), Teachers College, Columbia University Institutional Review
Board (15-118), and the University of Florida Institutional Review Board (IRB201702513). The
experiment is registered under ID AEARCTR-0005288. The views expressed herein are those of
the authors and do not necessarily reflect the views of the National Bureau of Economic
Research.

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

© 2018 by Peter Bergman and Isaac McFarlin Jr.. All rights reserved. Short sections of text, not
to exceed two paragraphs, may be quoted without explicit permission provided that full credit,
including © notice, is given to the source.
Education for All? A Nationwide Audit Study of School Choice
Peter Bergman and Isaac McFarlin Jr.
NBER Working Paper No. 25396
December 2018, Revised January 2020
JEL No. I20,I21,I24,I28

                                          ABSTRACT

School choice may allow schools to impede access to students perceived as costlier to educate.
To test this, we sent emails from fictitious parents to 6,452 charter schools and traditional public
schools subject to school choice in 29 states and Washington, D.C. The fictitious parent asked
whether any student is eligible to apply to the school and how to apply. Each email signaled a
randomly assigned attribute of the child. We find that schools are less likely to respond to
inquiries from students with poor behavior, low achievement, or a significant special need. Lower
response rates to students with this special need are driven by charter schools. Otherwise, these
results hold for traditional public schools, high value-added schools, including high-value added,
urban charter schools.


Peter Bergman
Columbia University
525 W. 120th Street
Box 174
New York, NY 10027
bergman@tc.columbia.edu

Isaac McFarlin Jr.
College of Education
University of Florida
2-230D Norman Hall
P.O. Box 117049
Gainesville, FL 32611
and NBER
imcfar@ufl.edu
1. Introduction

      In the last 15 years, researchers have leveraged the randomization inherent in many school-

choice admissions policies to demonstrate that certain schools have large impacts on test scores,

college enrollment, risky health behaviors, and criminality (Hoxby and Rockoff, 2004; Deming,

2011; Angrist et al., 2013; Wong et al., 2014; Dobbie and Fryer, 2015). These studies, which have

strong internal validity, examine the impacts on those who apply to a given set of schools. These

estimates may change as schools expand, especially if they have been strategically altering the

pool of applicants along hard-to-observe characteristics. We conduct a nationwide audit study to

test whether schools attempt to manipulate their applicant pool by providing less application

information to the parents of children perceived as more difficult or costlier to educate.

      The concern that school choice enables schools to impede access for certain students is salient

to policymakers (Cohodes and Dynarski, 2016).1 To minimize this practice, regulators use lotteries

and common applications to control admissions. While the educational costs of students with

severe disabilities can exceed 10 times that of other students, the government provides financial

offsets and requires that all districts serve these students—and procure their required services

(Griffith, 2008). But frictions in the choice process may still allow schools to influence who applies.

Many families lack information about schools’ eligibility requirements, quality, and admissions

process (DeArmond et al., 2014; Hastings and Weinstein, 2008; Kapor et al., 2019; Bergman et

al., 2019). Qualitative research has found instances of schools taking advantage of these frictions

to hinder access for certain groups of students (Drame, 2011; Orfield et al., 2013; Welner, 2013).

      This behavior is difficult to detect. Differences in how families choose schools may reflect

heterogeneous preferences rather than steering away applicants. Observational studies have

focused on specific contexts using administrative data, which has limited information on student




1
  There is also controversy in popular press on access to choice schools. See “Are Charter Schools Cherry-Picking Students” in the New
York Times (Dec. 10, 2014), which features a debate by policymakers on charter school access. In an article in the Washington Post,
“The Masquerade of School Choice: A Parent’s Story” (April 1, 2017), a parent describes her experience with racial discrimination and
school choice.
                                                                 1
behaviors and needs (Lacireno-Paquet et al., 2002; Bifulco et al., 2009; Zimmer et al., 2009; Hoxby

and Murarka, 2009; Zimmer and Guarino, 2013; Nichols-Barrer et al., 2015; Walters, 2018).2 The

findings are mixed.

        We conduct a nationwide audit study to detect whether schools selectively provide

application information to families.3 Across two experiments, we sent emails from fictitious

parents to 6,452 charter and traditional public schools subject to school choice. The parent asked

whether any student is eligible to apply to the school and how to apply.4 Each email signaled one

of the following randomly-assigned attributes about the student: their disability status, poor

behavior, high or low prior academic achievement, or no indication of these characteristics. We

also randomly varied students’ implied race, household structure, and gender. In the first

experiment, we sent messages only to charter schools. Based on constructive feedback from

researchers, we ran a second experiment, where we sent messages to both charter schools and

traditional public schools to replicate our previous findings and to compare response rates between

these two types of schools.

        Our results were the same across both experiments: We find that schools respond less often

to messages regarding students whom schools may perceive as more challenging to educate. The

baseline response rate was 53 percent. Messages signaling that a student has a potentially severe

special need were 5 percentage points less likely to receive a response than the baseline message.

Messages signaling a behavior problem and messages indicating low prior achievement were 7 and



2
 Several studies also look at voucher systems (Epple et al., 2017). Altonji et al. (2015) find little evidence that vouchers negatively impact
students who remain in traditional public schools. Outside the United States, Hsieh and Urquiola (2006) show that the voucher system in
Chile caused high-achieving and high-income students to move to private schools. Muralidharan and Sundararaman (2015) use a two-
stage experiment in India to estimate whether the introduction of private-school vouchers negatively impacts non-voucher recipients;
they find no evidence of spillovers on student outcomes.

3
  Prior studies using this design include Bertrand and Mullainathan (2004) and Oreopoulos (2011) to study racial and ethnic discrimination
in labor markets. Ayres and Siegelman (1995) investigate racial and gender discrimination in bargaining for a new car. In education
settings, Darolia et al. (2015) and Deming et al. (2016) examine the value of a credential from a for-profit postsecondary institution while
Baker et al. (2018) study bias in online learning environments. Giulietti et al. (2019) examine racial discrimination in the provision of
public services in the United States. Investigating the sharing economy, Edelman et al. (2017) find evidence of racial bias in the online
market for housing rentals.
4
    Understanding eligibility requirements is the most commonly-cited barrier to selecting a school of choice (DeArmond et al, 2014).

                                                                     2
2 percentage points less likely to receive a response, respectively. A message indicating good grades

and attendance, however, was neither more nor less likely to receive a response than the baseline

message.

      A key question is whether these results differ between traditional publics subject to school

choice and charter schools. Charter schools represent the fastest growing form of school choice in

the country.5 To answer this question, we matched charter schools to nearby traditional public

schools subject to school choice. We find that, overall, traditional public schools’ response rates

are similar to the response rates from charter schools across treatment messages. However, there

is a different response rate to messages that signal a child has a significant special need.

Traditional public schools exhibit no differential response rate to these messages, but charter

schools are 7 percentage points less likely to respond to them than to the baseline message. This

result is important because the differential cost of serving students with severe special needs cited

above (Moore et al., 1988; Chambers, 1998; Collins and Zirkel, 1992, Griffith, 2008).6

      These results hold for high-value-added schools, including urban, high-value added charter

schools. Prior research has also shown that strict (“no-excuses”) charter schools also tend to have

high value added (Abdulkadiroglu et al., 2011; Angrist et al., 2013, 2016; Chabrier et al., 2016;

Clark et al., 2015; Dobbie and Fryer, 2013; Dobbie and Fryer, 2015; Hoxby and Murarka, 2009).

We identified 272 such schools in our data; these schools have a value-added one-half standard

deviation above other charter schools. No-excuses charter schools are significantly less likely (10

percentage points) to respond to inquiries that signal a child’s potentially significant disability

than to the baseline message.

      We also present evidence that the monetary cost of serving students matters. States fund

students with special needs in several different ways, including block grants designated for special

education, cost-reimbursements for services rendered, and formulae that provide additional


5
  See the National Alliance for Public Charter Schools’ report.
6
  Quality of response may matter as well. In a later section, we provide corroborating evidence that the messages traditional public
schools send to parents of special needs students are viewed as more encouraging.
                                                                3
general funds to schools (Griffith, 2008; Millard and Aragon, 2015).7 We find that charter schools

in states, such as Wisconsin and Michigan, that reimburse districts for a large share of the realized

cost of serving special-needs students exhibit no differential response rate to messages signaling a

potentially high-cost special need.

        Allowing charter schools to integrate with another Local Education Agency (LEA) could

spread the risk of serving higher-cost students across multiple schools.8 The legal obligation to

provide services to students with special needs falls on the LEA. Integration between a charter

school and a traditional-public school LEA could enable the schools to pool resources. We coded

each charter school’s LEA status based on states’ LEA policies.9 We find that LEA status does

not moderate the differential response rate to messages signaling a disability.

        We find exploratory evidence of differences in response rates by the implied race of the family,

but not by household structure. Schools may interpret these attributes as signals of families’ socio-

economic status. Overall, schools are 2 percentage points less likely to respond to emails signed

by Hispanic-sounding names than to other messages. There is weaker evidence of a differential

response rate for messages signed by Black-sounding names.

        This paper makes several primary contributions to the literature on school choice. We provide

the first experimental evidence testing whether schools of choice provide less application

information to students whom schools may perceive as harder to educate. Second, we incorporate

signals of several student attributes—beyond race and gender—across a wide variety and a large

number of schools. These features increase the external validity of our study and allow us to

investigate an important dimension of heterogeneity comparing traditional public schools to

charter schools.




7
 Per-student education costs have steadily risen over time. Average expenditure per pupil for the 1990-1991 school year was $9,936. In
2016 dollars, this increased to $13,119 in the 2014-2015 school year (NCES Table 236.69). About 20 percent of the growth in new
education spending is directed to special education services (Hanushek and Rivkin, 1997).
8
    An LEA is equivalent to a school district in most instances.
9
    LEA status varies within states because it can depend on what entity authorized the charter school (e.g. the state or a district).
                                                                      4
      Our results also have implications for interpreting studies that use evidence from schools’

admission lotteries to examine school effectiveness. Our findings do not undermine the internal

validity of lottery-based studies, but they underscore that lottery-based studies are conditional

on the set of students who apply.10 Hastings et al. (2006), Walters (2014) and Kline and Walters

(2016) find that students who may benefit the most in terms of academic achievement are also

the least likely to apply to high-performing schools or education programs. However, certain

students—even those who may benefit most—may also impose high costs or negative behavioral

spillovers (cf. Carrell and Hoekstra, 2010; Carrell et al., 2018) that reduce schools’ demand to

serve them. If the hardest-to-educate students were evenly distributed across schools, the impacts

of highly-effective schools could decrease due to negative behavioral or fiscal spillovers.

      Lastly, our research highlights the importance of providing transparent information to

families to ensure all students have equal access in the choice process.11 DeArmond et al. (2014)

survey 4,000 parents across eight cities and find that the most common barriers in choosing a

school is understanding eligibility requirements, followed by transportation issues and obtaining

accurate information about school characteristics. Hastings and Weinstein (2008) show that

providing schools’ test score information to families increases their chances of selecting a higher-

scoring school, which increases achievement. Corcoran et al. (2018) also show that providing

information about nearby high schools’ selectiveness and graduation rates improved the quality

of middle-school students’ high-school choices, particularly for non-English speaking households.

Kapor et al. (2019) find that low-income families can misunderstand the school selection process

and are less likely to be placed into their preferred school. Impeding access to information about

how to apply could reduce opportunities for disadvantaged students even when there is, ostensibly,

equal access (“open enrollment”). More broadly, we document the use of one mechanism to reduce




10
  Fryer (2014) finds that the practices of high-performing charter schools, when installed in traditional public schools, produce comparable
gains in achievement. Abdulkadiroğlu et al. (2016) find those with lower application costs to charter schools experience similar
achievement gains to other students.
11
  For examples in the U.S. context, see Hoxby and Turner (2013) and Bergman (2015) in post-secondary and secondary education
contexts.
                                                                    5
access, but schools may use a number of ordeal mechanisms to screen applicants and the scope

for choice, despite the intentions of policymakers.12

         There is a normative question about what optimal policy should require of choice schools

with respect to whom they enroll. Our paper does not aim to identify an optimal policy, which

depends on a particular social welfare function. Instead, our results inform how a socially optimal

policy might be achieved. For instance, if a particular social welfare function implies that all

schools should have the capacity to serve any student, bolstering existing efforts to reimburse

schools for realized costs may ensure this opportunity for a variety of students. Other policies

could unify and simplify application processes to help families make informed decisions. Several

education agencies have undertaken audits like ours to monitor whether schools are providing

information equitably across families (Prothero, 2014).

         The rest of our paper proceeds as follows. Section 2 provides background on school choice for

our sample. Sections 3 and 4 describe our intervention, experimental design and empirical analysis.

Section 5 presents our results and Section 6 concludes.


2. Background Information on School Choice

         Our sample, which we describe in detail in the following section, covers traditional public

schools in districts with various forms of school choice and charter schools. Charter schools are

public, open-enrollment schools that have greater autonomy over their finances, staffing decisions,

and curricula than traditional public schools, but they must admit students by lottery if more

students apply to the school than can be accommodated.13 Charter schools are the fastest growing

form of school choice in the United States. Since 2010, more than 2,000 new charter schools have

opened (NCES, 2015). They enroll nearly 3 million students at nearly 7,000 schools in 43 states

and the District of Columbia. While their performance overall tends to be no better or worse than



12
  Welner (2013) documents that schools use a variety of mechanisms, such as releasing applications only for short periods of time early
in the school year, demanding the presentation of social security cards and birth certificates, and documentation of disabilities.
13
     Specifically, lottery admission is required if they receive federal funding.
                                                                         6
traditional public schools, charter schools in urban areas, which are often, no-excuses charter

schools, have been shown to have large, positive impacts on student achievement (Abdulkadiroglu

et al., 2011; Angrist et al., 2013, 2016; Cohodes, 2018; Chabrier et al., 2016; Clark et al., 2015;

Dobbie and Fryer, 2013; Dobbie and Fryer, 2015; Hoxby and Murarka, 2009).

         Among the traditional public schools in our sample, school choice operates in several ways.

The rules are governed by both state and local laws.14 For example, some states like Arizona

make school choice mandatory for all school districts in the state. In states where it is voluntary,

it is often practiced mostly by large urban school districts. At the local level, school districts

commonly establish attendance areas, and students’ default to neighborhood-based assigned

schools but allow applications to other schools within the district, subject to capacity constraints.

Other school districts have no neighborhood assignment and require that families apply to schools

as a requirement for enrollment. Schools may also offer priority enrollment to applicants based

on residence, having a sibling within a school, or a safety concern. Among schools with more

applicants than slots, schools may assign students based on a first-come, first-serve basis or a

lottery.

         Both traditional public schools and charter schools must comply with a number of anti-

discrimination laws, several of which have overlapping protections. The Civil Rights Act of 1964

prohibits discrimination on the basis of race, religion, and country of origin.15 The Rehabilitation

Act of 1973, Individuals with Disabilities Education Act (IDEA) and Americans with Disabilities

Act of 1990 require LEAs to provide all necessary services to students with physical or mental

disabilities. IDEA mandates the creation of an Individualized Education Plan (IEP) for students

with disabilities. The IEP, formulated with parents, school and other professional representatives,

dictates what services the LEA is legally obligated to provide a student to address their special


14
  National Center for Education Statistics Table 4.2 outlines states with mandatory open-enrollment policies for their traditional public
schools and briefly describes the nature of each policy. For voluntary open-enrollment policies by state, we discovered inaccuracies within
Table 4.2 and do not rely upon this particular information. For example, it is reported that the state of New York does not have any
voluntary admittance of students from other schools when in fact New York City public schools have one of the largest open-enrollment
programs in the country.
15
     The Civil Rights Act of 1964 further prohibits discrimination based on gender and sex, except for same-sex schools.
                                                                     7
needs.16

      This last requirement may have greater consequences for charter schools that are authorized

as their own LEA than for those authorized as part of an existing LEA or district (Heubert, 1997).

The latter arrangement implies that charter schools may be able to draw on resources from the

broader school district to help serve special-education students, while the former implies charter

schools may have to address this requirement entirely on their own.17 For this reason, charter

schools that serve as their own LEA may respond to different incentives during the application

process than those that are not their own LEA.

      Traditional public and charter school funding comes from federal, state, and local

governments. The degree of funding parity between charter schools and traditional public schools

within the same state varies across states.18 Supplementary funds for students with disabilities

can also vary based on state and local policies. Special education funds overwhelmingly (90%)

come from state and local sources (Cullen and Rivkin, 2003; Rhim et al., 2015). As a point of

reference, the average cost of educating a child with special needs is roughly 2.3 to 2.5 times that

of a child without special needs (Moore et al., 1988; Chambers, 1998; Collins and Zirkel, 1992).

      A point of controversy is whether charter schools serve the most disadvantaged or costliest-

to-educate students at similar rates to traditional public schools. Nationally, the Government

Accountability Office found that charter schools enroll a smaller proportion of students classified with severe

disabilities than traditional public schools (US Government Accountability Office, 2012). But evidence


16
  We summarize the above requirements as background information, but whether or not our findings constitute legal or illegal behavior on the
part of a school is not germane to our first-order research question, which is to determine experimentally whether schools practice any form of
differential treatment during the application process with respect to specific student characteristics.
17
   Akin to other social insurance programs such as Medicare and Food Stamps, the economic justification for special education services
is multi-tiered. First, it provides a form of insurance to protect families who have children that are expensive to educate due to a
disability; second, federal and state funding works as a form of insurance to protect local schools from the high cost of absorbing a
disproportionate number of disabled students (Cullen and Rivkin, 2003). IDEA also permits the allocation of funds for a statewide “risk
pool” to help LEAs serve students with high-cost disabilities (Rhim et al., 2015). States may also designate charter schools to be part of
a larger LEA specifically for IDEA purposes. There is some evidence that individual schools existing as their own LEA may form consortia
to pool resources, making it easier to establish economies of scale and provide appropriate services for all students (NCESCS, 2017).
18
   For example, the typical Oregon charter school receives only about 60 percent of the level of funding that a typical traditional public
school receives while both charter and traditional public schools in Tennessee receive similar levels of funding (Batdorff et al., 2014; Epple
et al., 2016).
                                                                     8
from specific locations is more nuanced. Setren (2015) finds that Boston-area charter schools

classify fewer students as special needs (irrespective of whether they have a disability).19 Hoxby

and Murarka (2009) find that New York City charter schools enroll more low-income and minority

students than traditional public schools as a percentage of total enrollment. Using data from

California and Texas, Booker et al. (2005) show that students who enroll in charter schools tend

to have lower achievement than the students in the traditional public schools they left.

3. Experimental Design and Data

Messages

      The field experiment consisted of email messages sent to charter schools and traditional

public schools subject to school choice. We framed each message as coming from a parent looking

for a school. The parent contacts the school to ask about their child’s eligibility and how to apply.

We developed our messages in consultation with charter school and traditional-public school

administrators who have received application inquiries via email. Our conversations with

administrators at charter schools and traditional public schools found that parents do make

eligibility inquiries and provided examples.20 The baseline message indicated that the parent is

looking for a school for their son or daughter and they would like to know whether anyone can

apply to that school and how to apply. Each treatment message added a sentence to this baseline

message to signal a child’s potential cost to educate, disadvantage, or prior academic performance.

This sentence indicated the child has one of the following: an IEP requiring they be taught in a

classroom separate from mainstream students; poor behavior; bad grades; or good attendance and

good grades. We show examples of the exact wording of these treatments in Figure 1 and Figure

A1.



19
  In part, this could be because the traditional public-school students were enrolled in prior to enrolling the charter school did not readily
transfer their IEPs.
20
  For instance, parents frequently asked whether the school admits students with special needs or low grades. Administrators reported
other questions that parents ask as well, including whether parents have to volunteer, pay any fees, purchase school books or provide
particular documentation about their child, whether there is a lottery or preferences for neighborhood families, and lastly, whether a
charter school diploma is valid for college or university admission.
                                                                     9
     We chose these messages based on existing concerns about how schools may screen potential

applicants. For instance, there is a national discrepancy in enrollment rates of students with

severe disabilities between charter schools and traditional public schools. Students with severe

disabilities may require significant support services, which may require teaching in a separate classroom

from mainstream students. Note that this does not imply one-on-one instruction; instead, this is

often instruction by a certified Special Education teacher in a separate or “self-contained” classroom

with fewer students (e.g. a common set up is 12 students to one teacher with special-education

certification, plus any other “para-professionals” that may be specified in a student’s IEP to

provide further services). The poor-behavior message ties to a contention that some schools push

out or screen children with behavior issues (Zimmer and Guarino, 2013). The poor-grades message

and the good grades and good attendance message reflects concerns that schools may seek out

students or screen students based on their academic performance (Winters, Clayton, and

Carpenter, 2017).

     Lastly, demographic characteristics of the parent and student were varied at random across all

messages. We randomized a signal of household structure by indicating that the parent and their

spouse were making the inquiry (e.g. “My husband and I…”) or that just one parent was making

the inquiry. Following Bertrand and Mullainathan (2004), the name of the parent signaled the gender

of the parent and whether they are Hispanic, Black, or white. 21 The choice to randomly vary demographic

signals also reflects concerns about how schools may screen potential applicants. Minority background

may signal socio-economic disadvantage and a child’s gender may signal disruptive behavior, as male

students tend to be more disruptive in class than female students (Bertrand and Pan, 2013).

Experimental Design and Sample Frames

     We conducted the first experiment in late 2014, in which we sent messages only to charter

schools. Based on feedback we received in this first experiment, we replicated and extended the




21
  We chose names based on New York state data that associated names with race and gender. We then chose names that were
overwhelmingly associated with one particular race and gender combination.
                                                          10
experiment in early 2018 with the goal of comparing the response rates of traditional public

schools in areas with school choice to the response rates of nearby charter schools. The three-year

gap between the two experiments provides a check on the consistency of the results across time.

         We constructed the sample for each experiment using the Common Core Data from the

National Center for Education Statistics, which has information on the universe of both charter

and traditional public schools. In the first experiment, we chose the 17 states with the largest

number of charter schools. These 3,131 schools were roughly half of the charter schools in the

country at the time.

         The initial sample for the second experiment was the 29 states (and the District of Columbia)

with the largest number of charter schools and some form of intra-district choice. We list these

states in Appendix Table A9. For every charter school in these states, we matched it to the

nearest traditional public school with the same entry-grade level and within the same district

boundaries. We then further restricted this sample to school districts in the top 40 in terms of

enrollment because of the fixed costs of data collection.22 The final sample consisted of 4,338

schools (1,016 of which were charter schools that were also in the first experiment) or 2,169

matched-pairs of traditional public and charter schools. Figure A2 shows states with charter

schools colored in white, states in our sample colored in dark blue, and states with no charter

schools colored in gray. The sample has broad geographic coverage across the United States.

Appendix B discusses our sample construction in further detail.

         In each experiment, we sent two emails to each school three-to-four weeks apart. The

treatment messages were randomly assigned at the school level in the first experiment. For the

second experiment, treatment messages were clustered at the pair level; identical messages from

the same fictitious parent were sent to each charter school and its match-paired traditional public

school. Within each experiment, no school received the same message treatment twice and a

school was assigned a treatment without replacement. We also randomized the order in which


22
     Specifically, we selected the largest districts because of the fixed costs of investigating district-specific school choice policies.
                                                                          11
schools were contacted.

       Table 1 shows the results of regressing school characteristics on an indicator for each

treatment. Each column restricts the sample to the baseline messages and the treatment message

indicated in the column header. The results and joint test of covariates within each column show

that randomization generated assignments uncorrelated with school characteristics.

       Figure 1 shows the number of emails sent per treatment. More baseline and IEP messages

were sent than behavior- and grades-related messages so that we would have additional precision

with respect to those two treatments.23 Across the two experiments, we sent the same baseline,

IEP, poor behavior and low-grades messages.24 The second experiment added the good-grades and

attendance message to this list of treatments as well as the two-parent household signal.

     Data

       Data come from information on school websites, national databases of school demographic

information, the census, a school-rating non-profit organization, and the responses to our emails.

We hired research assistants to find and visit the website of every school in our sample frame.

We then coded several variables including whether the school has a website, and, if so, whether

the website includes a link to the school’s contact information on its landing page, a webform to

contact the school, or a requirement to add a phone number via the webform. We also used

information on the school websites (and schools’ handbooks on these websites) to identify the

“no-excuses” charter schools in the sample. We based this determination on a template of

characteristics common to such schools.



23
   In our funding proposal, we pre-specified our primary treatments as the IEP, poor behavior, and poor grades in the first experiment,
and these plus the good grades and attendance treatment in the second experiment. A key difference specified in the second experiment
was to test for differences in response rates between the traditional public schools and charter schools. Sending one baseline message to
every school would have drastically reduced our power to detect treatment effects for each of these message types relative to the baseline
message. In our first experiment, we wanted added precision for the IEP message, so we sent out relatively more IEP messages. Our
primary specification, compares response rates to each treatment message to the response rate of the baseline message. A power analysis
showed that power in the second experiment was maximized by sending roughly twice as many baseline messages as treatment messages.
See Figure 1 for the exact message counts per treatment in each experiment.
24
   Each message had randomly assigned wording variations as well. For instance, we randomly varied the subject line, the sign off
(“thanks” or “thank you” or just signing the name) and the greeting.
                                                                   12
    We supplemented these data with information on schools from two national databases: the

Common Core of Data (CCD) and the Civil Rights Data Collection (CRDC). We used the CCD

data to compute enrollment size, the share of students who are Black, Hispanic or white, and the

share of students who receive free or reduced-price lunch at each school. These data also recorded

the latitude and longitude of a school and whether it was located in an urban, suburban or rural

area. We used the location data to link each school to census tract information on the share of

individuals by race, education, income and disability in a tract. From the CRDC data, we used

the number of students with a disability for each school, which the CRDC breaks down by the

portion of the day students are not in a restricted environment (less than 40%, between 40% and

79%, and 80% or more). We translated these numbers into shares of enrollment. We also used

the CRDC to calculate the share of students who are chronically absent (missing 10% of days or

more), suspended, and have limited English proficiency.

    Third, we used data provided by a nonprofit organization, GreatSchools, which collects

proficiency rates based on test scores for traditional public schools and charter schools across the

country. We average the proficiency rates across subjects (e.g. math and reading) and grade levels

for each school. We use this measure to estimate each school’s value added by measuring the

difference between its observed average proficiency rate and the rate predicted based on the

covariates specified above, state fixed effects, and an indicator for charter school or not. We then

standardize this measure of value added according to the mean and standard deviation within the

sample.

    Fourth, we coded the responses of schools to our emails. We created an indicator for whether

or not a school responds to a message, which is our outcome variable. Some schools provide

automated responses (3% of emails receive an automated response) to our messages. Since each

treatment is as likely to receive an automated response as another, this practice only raises overall

response rates and our results are robust to excluding these messages (available upon request).

    Finally, a response does not necessarily indicate encouragement to apply. Schools may

                                                  13
attempt to persuade a parent to either apply or not to apply, or it may contain other information

that differs by message content. Thus, a reply is not necessarily encouragement to apply. We

worked with a qualitative researcher to develop a codebook and to train a team of research

assistants to review each message and manually code message responses as either encouraging or

not encouraging. Appendix C discusses in further detail how we created this measure.

         In Appendix Table 1, we show the characteristics of our sample schools. On average, sample

schools serve primarily students from low-income families; more than half of the students served

in these schools receive free or reduced-priced lunch. Demographically, there are relatively similar

shares of white, Black and Hispanic students across all sample schools. Over half the sample is

located in an urban area. We also show that school-level correlates of disadvantage predict lower

response rates, and that high-performing schools are less likely to respond overall (Table A2).

4. Empirical Strategy

         Our main outcome is a binary variable for whether a school responds to our inquiry or not.

We estimate a linear-probability model as follows:

                         Ri = α + Treatmenti β + Demographicsi γ + Xi δ + Wavei θ + εi

         In this regression, Ri is our binary outcome. Treatmenti is a vector of treatment indicators

for the mutually-exclusive treatments: the IEP treatment, the behavior treatment, the low-grades

treatment, and the good-grades and good-attendance treatment. Demographicsi is a vector of

indicators for the randomized implied race, gender of the child and parent, and whether the

message indicates a two-parent household. Xi is a vector of school covariates, including school

demographics and whether or not the school is a traditional public school. Any missing covariates

are imputed and an indicator for missing data is included in the regression. All regressions include

indicators for the wave of emails.25 Standard errors are clustered at the school or school-by-pair




25
     There are four waves of emails: two in the first experiment and two in the second experiment.
                                                                    14
level.26

       Outcomes include binary indicators for whether or not a school of choice responded to the

email as well as indicators for the encouragement of the response. To ameliorate concerns about

selection into response, all outcomes are unconditional on whether or not a school responds. For

instance, if the outcome is an indicator for whether or not a response is “encouraging,” then a one

indicates that the school responded and wrote a message using an encouraging tone while a zero

indicates either that the school’s response was discouraging or that the school did not respond at

all.

       To test whether traditional public schools respond at different rates to each type of message

than charter schools, we interact each treatment with an indicator for whether or not the school

is a traditional public school. The significance of this interaction effect tests whether the

traditional public school responded more or less frequently than the nearby charter school. To

conduct other heterogeneity analyses, we either restrict the sample according to a certain variable

or fully interact it with our treatment messages as specified in the text.

       Our results replicate similarly across experiments, which may be unsurprising given that the

treatment messages were the same (except for the addition of new treatments) and the policy

environment was similar. For ease of presentation, we combine the two experiments in our analysis

of the results. However , the appendix Table A3 shows results from each experiment separately.

       Lastly, we show our results are robust to different specifications and multiple-testing

corrections. We show whether controls for school characteristics from the CCD and CRDC, census

tract characteristics, and pair fixed effects affect our results. Given random assignment, none of

these additional covariates or fixed effects is required for identification. In the appendix, we adjust

our main results—the effects of each treatment message and the differential effect between charter



26
   Specifically, clustering is at the school level for schools that are only in the first experiment, clustering is at the pair level for schools
only in the second experiment, and clustering is at the school and pair level for schools that are in both the first and second experiment.
In practice, the level of clustering does not alter the standard errors much and therefore the significance of the results; other versions of
clustering are available upon request.
                                                                      15
schools and traditional public schools—for multiple-hypothesis testing (Table A5). We use Holm’s

step-down procedure for groups of treatment effects specified in the text (Holm, 1979). This

procedure controls for the family-wise error rate, which is the probability of at least one false

rejection of the null hypothesis.


5. Results

     Table 2 presents the effects of our primary treatments—messages indicating poor behavior,

the IEP, poor grades, and good grades and attendance—on response rates. The control mean at

the bottom of Column (1) shows that the baseline message received a response 53% of the time.

     We find that schools are less likely to reply to messages signaling that the potential applicant

has had bad grades, poor behavior, and an IEP (Column (1)) than the baseline message. The bad-

grades treatment reduces response rates by 2.4 percentage points. The IEP message and poor

behavior message reduce response rates by 5.2 percentage points and       7.0   percentage    points,

respectively. The signal of good grades and attendance has no discernable impact on response

rates (the coefficient is 0.0 percentage points). One caveat of the latter result is that parents’

describing their child’s good grades may be considered “cheap talk” by the schools, and not taken

seriously.

     In interpreting these results, note that the IEP message signals a student who requires a

restrictive environment. If the signal were for less restrictive or less costly services, the results

may have differed. And while schools do not actively provide more information to higher-

performing students, this signal could be viewed by schools as “cheap talk.” Messages signaling

these positive traits may be perceived as less truthful than the messages signaling other student

traits.

     Column (1) of Table 2 also shows the results of our other treatments signaling race, gender

and household structure. Only the message that indicates a Hispanic-sounding name results in




                                                  16
significantly lower—by 2.0 percentage points—response rates.27 The coefficient on Black-sounding

names is not significant. An F-test for whether we can reject the null hypothesis that these

treatments of demographic characteristics are jointly equal to zero cannot be rejected (p=0.14 for

the specification in column 1). One point of consideration for the Hispanic-sounding names is that

the emails were sent in English; the results could be a lower bound if schools are concerned about

a student’s English-language skills.

        In the appendix (Table A4), we show that the results above are extremely similar across

different specifications. The coefficients do not vary significantly with the addition of school-level

covariates, census tract-level covariates, or pair fixed effects.

Heterogeneity across Traditional Public and Charter Schools


        Given the growth rate of charter schools and concerns over whether they encourage all

students to apply, we test whether our findings differ between charter schools and traditional

public schools. Columns (2) and (3) of Table 2 show results separately for traditional public

schools and charter schools, respectively. Column (4) shows the difference in response rates

between traditional public schools and charter schools and its statistical significance.

        Traditional public schools in areas of school choice generally respond at similar rates as

charter schools for each treatment, with one exception. Traditional public schools are

significantly—5.8 percentage points—more likely to respond to the IEP message than charter

schools. This result remains statistically significant even after adjusting for multiple testing of

these interaction terms (p=0.05, Table A5). A test of whether the interaction effects for the

primary treatments are jointly different from zero is also statistically significant (p=0.03).

Treatment effects for signals of demographic characteristics do not differ significantly between

traditional public schools and charter schools.




27
     These results are larger in magnitude for predominantly-white schools (results available upon request), though we do not know how
common it is for these schools to receive inquiries from Black or Hispanic families.

                                                                    17
      Although we observe that traditional public schools respond at higher rates than charters for

children with IEPs, it could be that traditional public schools are systematically less encouraging

in their responses, which might discourage parents from applying. Based on qualitative data

collected by trained coders, Table 3 reports findings on whether school responses are encouraging,

with no response coded as a zero and an encouraging response coded as a one. (See Appendix C

for a description of our qualitative data and reports on inter-rater reliability.) Responses to the

IEP message are 4.2 percentage points more likely to be encouraging from traditional public

schools than from charter schools. However, the evidence suggests otherwise for the bad grades

and poor behavior treatments, where traditional publics provide significantly fewer encouraging

responses.28

Exploratory Analyses


Heterogeneity Across School Performance Levels and “No-Excuses” Charter Schools


      Access to high-performing schools is important if school choice is to reduce gaps in

achievement across groups of students with different histories and attributes. We explore

heterogeneity in Table 4. Columns (1) and (2) show treatment effects for all high and all low-

value-added schools, which we define as above- or below-average value added in the sample. The

IEP, behavior, and grades treatment effects are similar for high- and low-value-added schools.

Differences arise with the race implied by the messages, however: high value-added schools are

less likely to respond to messages from families with Black or Hispanic-sounding names than low

value-added schools. Results for messages from parents with Hispanic-sounding names remain

significant at the 1% level after adjusting for multiple testing (p =0.01 and p =0.15 for Hispanic-

sounding name and Black-sounding name, respectively). This result is driven by the high-value


28
   Table A8 also presents results on the likelihood that a typical person may apply to the school. Column (4) shows differences in impacts
between traditional public schools and charter schools on the likelihood of applying. The findings are broadly supportive of earlier results:
the typical person—or coder—is similarly likely to apply to traditional public and charter schools across the primary treatments, except
for the IEP treatment. Specifically, we find that the typical person is 4.5 percentage points more likely to apply to a traditional public
versus a charter school when sent the IEP message. Given that coders are in fact non-applicant parents, we generally view this evidence
as suggestive.

                                                                    18
added charter schools in the sample and by schools serving low shares of minority students (results

available upon request). All of these findings are exploratory, however.

      Prior research has shown that urban, high-value added charter schools can significantly close

racial gaps in student achievement (Abdulkadiroglu et al., 2011). Column (3) shows the treatment

effects from the primary treatment messages do not qualitatively differ for this subgroup. Many

“No-excuses” charter schools have demonstrated particular success in closing racial-achievement

gaps in school districts. We identified a list of 272 no-excuses schools in our sample. These schools

tend to have much higher test scores, value added, and serve larger shares of minority and low-

income students relative to other charter schools. These schools also, however, serve fewer

students with IEPs and have higher rates of suspensions than other charter schools (see Table

A1). While the sample of “no-excuses” charter schools is small, the coefficient on the IEP

treatment remains large (10 percentage points), negative, and significant. Again, this analysis is

just exploratory.

Moderating Factors: Funding Strategies

      States typically provide funding for special education students in three ways. Most common

is formula-based funding. The formula may be a funding multiplier based on student or staff

counts. Schools receive additional funds, but these are not necessarily ear-marked for special-

education services. Categorical funds or block grants similarly provide funds for schools, but these

are ear-marked for special-education services. Finally, states may provide partial or full

reimbursement to districts for their realized special education expenditures. We generate a

variable categorizing states by how they provide funding and we interact this variable with our

IEP treatment indicator. We restrict the sample to charter schools, since traditional public schools

are no less likely to respond to students with an IEP than to the baseline message.29

      Table 5 shows heterogeneous effects by funding strategy. Charter schools in states that



 Traditional public schools exhibit a similar pattern of heterogeneity with respect to funding as charter schools, but the estimates are
29

much less precise.

                                                                  19
reimburse schools for (at least a portion of) their realized expenditures are 7 percentage points

more likely to respond to the IEP treatment message than charter schools in other states. This

result is statistically significant at conventional levels (p=0.05). Charter schools in states with

categorical funding have slightly higher response rates as well, though the difference is not

statistically significant. In states with formula-based funding, there are several weighting schemes;

we do not find any significant heterogeneity with respect to these different schemes (results

available upon request).

     This suggestive pattern of results is consistent with an explanation that the costs of educating

certain students are imperfectly compensated in most contexts. The latter could create an

incentive for schools to provide less application encouragement to students with special needs.

One caveat to a cost-reimbursement policy is that it can create incentives to classify students as

requiring an IEP and associated services, irrespective of the student’s actual needs.

Diversifying Risk: Local Education Agency (LEA) Status

     Table 5 also shows heterogeneity by LEA status. LEAs, rather than schools, bear the legal

obligation to provide the services specified in students’ IEPs. Whether or not a charter school is

its own LEA varies across and within states. Charter schools that are their own LEA may have

difficulty pooling resources and risk across multiple schools. We assess whether the opportunity

to pool resources in this way is associated with differential response rates to the IEP message. We

restrict the sample to charter schools and interact an indicator for LEA status with our IEP

treatment. While own-LEA charter schools are slightly less likely to respond to the IEP treatment,

this result is not statistically significant.

Household Structure and Demographics


     Lastly, we explore whether signals of household structure exacerbate or attenuate differential

response rates across treatments. We randomized whether a message indicated that a husband

and wife are or just one parent is looking for a school and. We also randomized the race and

gender of the child. Results restricting the sample to each of these groups are shown in Table A7.
                                                  20
Messages signaling a two-parent household, which could signal socio-economic advantage, tend to

increase the likelihood of response to children with poor behaviors and decrease the likelihood of

response regarding children with good grades. Messages signaling a two-parent household also

significantly increase the likelihood of response for Black families. Messages with a Hispanic-

sounding name tend to have higher response rates to the good-grades treatment. Finally,

treatment effects for the primary treatment messages do not differ significantly for messages

signaling a male student compared to female students.


6.   Conclusion

     While school choice can, in theory, improve access and quality, competitive pressures may

also induce schools to keep costs low and discourage students perceived as costly to educate from

enrolling. These incentives may be present outside of school choice as well, but the application

process presents a distinct opportunity for schools to treat costlier or students perceived as harder-

to-educate students differently than with a neighborhood-based, school-assignment mechanism.

Using an audit study approach, we provide the first experimental assessment of whether schools

fail to provide application information to families with children who have a severe special need,

behavior problem, or level of academic achievement, as well as a perceived race and gender. Our

study is also much broader in scope than previous research; we sample schools across 29 states

and the District of Columbia and approximately half of the charter schools in the United States.

     We find that, on average, schools are significantly less likely to provide information to families

with students who have low grades, behavior problems, or an IEP requiring they be taught in a

separate classroom, than to families of students without these attributes. Charter schools are

significantly less likely to reply to students to the IEP message than to the baseline message,

while traditional public schools are not. There is also some evidence that schools are less likely to

respond to families with Hispanic-sounding names.

     One limitation of our analysis is that we chose one particular signal of disability, and one

that may be perceived as particularly costly for schools to provide services. Our findings may not

                                                   21
generalize to other disabilities or students requiring other services, such as students with limited

English proficiency. We did not incorporate a wider variety of treatments because of statistical

power concerns. A second limitation is that we cannot estimate the impacts of these behaviors on

the enrollment rates of different types of students into schools. We capture just one way schools

can dissuade certain students from applying, but schools can use other ordeal mechanisms to

screen students as well, such as moving up application deadlines, requiring parents fill out a paper

applications, and attending school open houses.

      The implications of our results for optimal policy requires specifying a social welfare function.

Some might favor maximizing a weighted average of student achievement, but many families see

inclusion as essential for improving pro-social outcomes, especially as they pertain to students

with special needs or diversity.30 Our results suggest that funding is a key constraint on schools’

willingness to serve students with disabilities. We show suggestive evidence that cost-

reimbursement funding mitigates charter schools’ differential response rates for students with

IEPs. All schools—including traditional public and charter—are less likely to respond to students

with poor prior behavior and low grades. Conducting systematic audits could help deter this

behavior. Some education agencies have already undertaken audits via phone calls asking

eligibility questions to charter schools.




30
  For instance, see work by Gautam Rao (2019) on how the integration of castes in India impacted social preferences around inequality,
inclusion and altruism.

                                                                 22
References

         Abdulkadiroğlu, Atila, Joshua D Angrist, Susan M Dynarski, Thomas J Kane, and Parag A

 Pathak, “Accountability and flexibility in public schools: Evidence from Boston’s charters and pilots,” The

 Quarterly Journal of Economics, 2011, 126 (2), 699–748.


         Abdulkadiroğlu, Atila, Joshua D Angrist, Peter Hull, and Parag A Pathak, “Charters without

 Lotteries: Testing Takeovers in New Orleans and Boston,” American Economic Review, 2016, 106 (7), 1878–

 1920.


         Abdulkadiroğlu, Atila, Parag Pathag, Jonathan Schellenberg and Christopher Walters, “Do

 Parents Value School Effectiveness?” NBER Working Paper 23912, 2017


         Altonji, Joseph G, Ching-I Huang, and Christopher R Taber, “Estimating the cream skimming

 effect of school choice,” Journal of Political Economy, 2015, 123 (2), 266–324.


         Angrist, Joshua D, Parag A Pathak, and Christopher R Walters, “Explaining Charter School

 Effectiveness,” American Economic Journal: Applied Economics, 2013, 5 (4), 1–27.


         Angrist, Joshua D., Sarah R. Cohodes, Susan M. Dynarski, Parag A. Pathak, and

 Christopher R. Walters, “Stand and Deliver: Effects of Boston’s Charter High Schools on College

 Preparation, Entry, and Choice,” Journal of Labor Economics, April 2016, 34 (2).


         Ayres, Ian and Peter Siegelman, “Race and Gender Discrimination in Bargaining for a New Car,”

 The American Economic Review, 1995, 85(3): 304-321.


         Baker, Rachel, Thomas Dee, Brent Evans, and Juhn John, “Bias in Online Classes: Evidence

 from a Field Experiment,” Center for Education Policy Analysis working paper, 2018.


         Batdorff, Meagan, Larry Maloney, Jay F May, Sheree T Speakman, Patrick J Wolf,

 Albert Cheng, “Charter School Funding: Inequity Expands” School Demonstration Project, University of

 Arkansas.      April   2014.    http://www.uaedreform.org/       wp-content/uploads/charter-funding-inequity-

 expands.pdf

                                                        23
       Bertrand, Marianne and Jessica Pan, “The trouble with boys: Social influences and the gender

gap in disruptive behavior,” American Economic Journal: Applied Economics, 2013, 5 (1), 32–64.


           and Sendhil Mullainathan, “Are Emily and Greg More Employable Than Lakisha and Jamal?

A Field Experiment on Labor Market Discrimination,” American Economic Review, 2004, 94 (4), 991–1013.


       Bergman, Peter, “Parent-Child Information Frictions and Human Capital Investment: Evidence from a

Field Experiment,” CESifo Working Paper 5391, 2015.


       Bergman, Peter, “The Risks and Benefits of School Integration for Participating Students: Evidence from

a Randomized Desegregation Program,” No. 11602. Institute for the Study of Labor (IZA), 2018.


       Bergman, Peter, Eric Chan, and Adam Kapor, “Housing Search Frictions: Evidence from

Detailed Search Data and a Field Experiment,” 2019.


       Bifulco, Robert, Helen F Ladd, and Stephen L Ross, “The effects of public school choice on

those left behind: Evidence from Durham, North Carolina,” Peabody Journal of Education, 2009, 84 (2), 130–

149.


       Booker, Kevin, Ron Zimmer, and Richard Buddin, “The effects of charter schools on school peer

composition,” 2005.


       Chabrier, Julia, Sarah Cohodes, and Philip Oreopoulos, “What Can We Learn from Charter School

Lotteries?,” The Journal of Economic Perspectives, 2016, 30 (3), 57–84.


       Chambers, JG, “Geographic Variations in Public Schools Costs (NCES 98-04),” US Department of

Education. Washington, DC: National Center for Education Statistics Work- ing Paper, 1998.


       Chetty, Raj, John N. Friedman, Nathaniel Hilger, Emmanuel Saez, Diane Whitmore

Schanzenbach, and Danny Yagan, “How does your kindergarten classroom affect your earnings?

Evidence from Project STAR.” The Quarterly Journal of Economics, 2011, 126(4), 1593-1660.


       Chetty, Raj, John N. Friedman, and Jonah E. Rockoff, “Measuring the impacts of teachers II:

Teacher value-added and student outcomes in adulthood,” American Economic Review, 2014, 104(9), 2633-
                                                     24
79.


       Clark, Melissa A, Philip M Gleason, Christina C Tuttle, and Marsha K Silverberg, “Do

Charter Schools Improve Student Achievement,” Educational Evaluation and Policy Analysis, 2015, 37 (4),

419–436.


       Cohodes, Sarah, “Charter schools and the achievement gap,” The Future of Children, winter, 2018.


       Cohodes, Sarah and Susan Dynarski, “Massachusetts charter cap holds back disadvantaged,”

Evidence Speaks Reports, Economics Studies at Brookings, 2016, Vol 2 (1) pp. 1-6.


       Collins, LA and PA Zirkel, “To What Extent, If Any, May Cost Be a Factor in Special Education

Cases?,” Education Law Reporter, 1992, pp. 11–25.


       Corcoran, Sean P and Jennifer L Jennings, “The Gender Gap in Charter School Enrollment,”

Education Policy, forthcoming.


       Cullen, Julie Berry and Steven G Rivkin, “The role of special education in school choice,” in

“The economics of school choice,” University of Chicago Press, 2003, pp. 67– 106.


       Darolia, Rajeev, Cory Koedel, Paco Martorell, Katie Wilson, and Francisco Perez-Arce,

“Do Employers Prefer Workers Who Attend For-Profit Colleges? Evidence from a Field Experiment,”

Journal of Policy Analysis and Management, 2015, 34(4): 881-903.


       DeArmond, Michael, Ashley Jochim, and Robin Lake, “Making School Choice Work,” Center

on          Reinventing       Public       Education,        July        2014.      http://www.crpe.org/

sites/default/files/CRPE_MakingSchoolChoiceWork_Report.pdf


       Deming, David J., Justine S. Hastings, Thomas J. Kane, and Douglas O. Staiger, “School

Choice, School Quality, and Postsecondary Attainment,” American Economic Re- view, 2014, 104 (3), 991–

1013.


     Deming, David J. “Better schools, less crime?,” Quarterly Journal of Economics, 2011, 126(4), 2063-


                                                   25
2115.


        Deming, David J., Noam Yuchtman, Amira Abulafi, Claudia Goldin, and Lawrence F.

Katz, “The Value of Postsecondary Credentials in the Labor Market: An Experimental Study,” American

Economic Review, 106(3): 778-806.


         Dobbie, Will and Jr. Fryer Roland G., “Getting beneath the Veil of Effective Schools: Evidence

  from New York City,” American Economic Journal: Applied Economics, 2013, 5 (4), 28–60.


          and Roland G. Fryer, “The Medium-Term Impacts of High-Achieving Charter Schools,” Journal of

Political Economy, 2015, 123 (5), 985–1037.


         Drame, Elizabeth R. “An analysis of the capacity of charter schools to address the needs of students

  with disabilities in Wisconsin.” Remedial and Special Education, 2011, 32(1): 55-63.


         Edelman, Benjamin, Michael Luca, and Dan Svirsky, “Racial Discrimination in the Sharing

  Economy: Evidence from a Field Experiment,” American Economic Journal: Applied Economics, 2017,

  9(2), 1-22.


         Epple, Dennis, Richard Romano, and Miguel Urquiola, “School Vouchers: A Survey of the

  Economics Literature,” Journal of Economic Literature, 2017, 55(2), 441-492.


         Epple, Dennis, Richard Romano, and Ronald Zimmer, “Charter Schools: A Survey of Research

  on their Characteristics and Effectiveness,” Handbook of Economics of Education, Editors, Eric Hanushek,

  Stephen Machin, and Ludger Woessmann, 2016, Vol. 5, 139-208.


         Fryer, Roland, “Injecting Charter Schools Best Practices into Traditional Public Schools: Evidence

  from Field Experiments,” The Quarterly Journal of Economics, 2014, 29 (3), 1355-1407.


         Giuletti, Corrado, Tonin, Mirco, and Michael Vlassopoulos, “Racial Discrimination in Local

  Public Services: A Field Experiment in the United States,” Journal of the European Economic Association,

  2019, 17(1): 165-204.

                                                      26
     Griffith, Michael. “State Funding Programs for High-Cost Special Education Students,” Education

Commission of the States: State Notes, May 2008.


     Government Accountability Office (GAO), “Charter Schools: Additional Federal Attention

Needed to Help Protect Access for Students with Disabilities. Report to Congressional Requesters. GAO-12-

543.,” US Government Accountability Office, 2012.


     Hanushek, Eric and Steven Rivkin. “Understanding the Twentieth-Century Growth in U.S. School

Spending,” Journal of Human Resources, 1997, 32(1), 35-68.


     Hastings, Justine, Thomas Kane, and Douglas O. Staiger, “Parental Preferences and School

Competition: Evidence from a Public School Choice Program,” NBER Working Paper No. 11805, 2006.


     Hastings, Justine and Jeffrey Weinstein, “Information, School Choice, and Academic Achievement:

Evidence from Two Experiments,” The Quarterly Journal of Economics, 2008, 123 (4), 1373–1414.


     Heubert, Jay P, “Schools without Rules-Charter Schools? Federal Disability Law, and the Paradoxes

of Deregulation,” Harv. CR-CLL Rev., 1997, 32, 301.


     Holm, Sture, “A Simple Sequentially Rejective Multiple Test Procedure”, Scandinavian Journal of

Statistics, 1979, 6(2), pp. 65-70.


     Hoxby, Caroline Minter, and Jonah E. Rockoff, “The impact of charter schools on student

achievement,” Cambridge, MA: Department of Economics, Harvard University, 2004.


     Hoxby, Caroline and Sonali Murarka, “Charter schools in New York City: Who enrolls and how they

affect their students’ achievement,” Technical Report, National Bureau of Economic Research 2009.


     Hoxby, Caroline M and Sarah Turner, “Expanding College Opportunities for High-Achieving,

Low Income Students,” 2013. SIEPR Discussion Paper 12-014.


     Hsieh, Chang-Tai and Miguel Urquiola, “The effects of generalized school choice on achievement

and stratification: Evidence from Chile’s voucher program,” Journal of public Economics, 2006, 90 (8), 1477–


                                                    27
1503.


       Kapor, Adam, Christopher Neilson, and Seth D Zimmerman, “Heterogeneous Beliefs and School

Choice,” forthcoming, American Economic Review.


       Kline, Patrick and Christopher R. Walters, “Evaluating Public Programs with Close Substitutes:

The Case of Head Start,” Quarterly Journal of Economics, 2016, pp. 1795-1848.


       Lacireno-Paquet, Natalie, Thomas T Holyoke, Michele Moser, and Jeffrey R Henig,

“Creaming versus cropping: Charter school enrollment practices in response to market incentives,” Educational

Evaluation and Policy Analysis, 2002, 24 (2), 145–158.


        Lazear, Edward P, “Educational Production,” The Quarterly Journal of Economics, 2001, 116 (3), 777–
803.

        Loeb, Susanna, Jon Valant, and Matt Kasman, “Increasing Choice in the Market for Schools:

Recent Reforms and their Effects on Student Achievement,” National Tax Journal, 2011, 64 (1), 141-164.


       Lubienski, Christopher, “Marketing Schools: Consumer Goods and Competitive Incentives for Consumer

Information,” Education and Urban Society, 2007, 40 (1), 118-141.


       Lubienski, Christopher, Charisse Gulosino, and Peter Weitzel, “School Choice and Competitive

Incentives: Mapping the Distribution of Educational Opportunities across Local Educational Markets,” American

Journal of Education 2009, 115.


       Millard, Maria and Stephanie Aragon. “State funding for students with disabilities,” Education

Commission of the States: 50-State Review, June 2015.


       Moore, Mary, William Strang, Myron Schwartz, and Mark Braddock, “Patterns in Special

Education Service Delivery and Cost.” 1988.


       Muralidharan, Karthik and Venkatesh Sundararaman, “The Aggregate Effect of School Choice:

Evidence from a Two-Stage Experiment in India,” The Quarterly Journal of Economics, 2015, 130 (3), 1011–

1066.

                                                         28
       National Center for Education Statistics (NCES), “Table 216.90: Public elementary and

secondary charter schools and enrollment, by state: Selected years, 1999-2000 through 2013-14,” in “Digest of

Education Statistics,” National Center for Education Statistics, 2015.


       National Center for Special Education in Charter Schools (NCSECS), “What is an

Infrastructure? Special Education Support Systems in Charter Schools,” 2017, Issue Brief—Infrastructures,


       Nichols-Barrer, Ira, Philip Gleason, Brian Gill, and Christina Clark Tuttle, “Student selection,

attrition, and replacement in KIPP middle schools,” Educational Evaluation and Policy Analysis, 2015, p. 162-

73.


       Oreopoulos, Philip, “Why Do Skilled Immigrants Struggle in the Labor Market? A Field Experiment

with Thirteen Thousand Resumes,” American Economic Journal: Economic Policy, 3(4): 148-171.


       Orfield, Myron W., Baris Gumus-Dawes, and Thomas Luce, “The state of public schools in

post-Katrina New Orleans: The challenge of creating equal opportunity,” Educational Delusions?: Why

Choice Can Deepen Inequality and How to Make Schools Fair. University of California Press, 2013. 159-

184.


       Prothero, Arianna, “Mystery Parents Test Charters’ Enrollment of Special Ed., ELL Students,”

Education Week, 2014.


       Rao, Gautam, “Familiarity Does Not Breed Contempt: Diversity, Discrimination and Generosity in

Delhi Schools,” 2019, American Economic Review.


       Rhim, Lauren M., Paul O’Neill, Amy Ruck, Kathryn Huber, and Sivan Tuchman, “Getting lost

while trying to find the money: Special education finance in charter schools,” 2015 Report.


       Scott, George A, “Charter Schools: Additional Federal Attention Needed to Help Protect Access for

Students with Disabilities. Report to Congressional Requesters. GAO-12-543.,” US Government Accountability

Office, 2012.


       Setren, Elizabeth, “Special Education and English Language Learner Students in Boston Charter

                                                     29
Schools: Impact and Classification,” 2015.


     Walters, Christopher, “The Demand for Effective Charter Schools,” Journal of Political Economy,

2018, Forthcoming.


     Welner, Kevin G, “The Dirty Dozen: How Charter Schools Influence Student Enrollment” Teachers

College Record 17104.


     Winters, Marcus A., Grant Clayton, and Dick M. Carpenter II, “Are low-performing students

more likely to exit charter schools? Evidence from New York City and Denver, Colorado,” Economics of

Education Review 2017, 56 pp. 110-117.


     Wong, Mitchell D., Karen M. Coller, Rebecca N. Dudovitz, David P. Kennedy, Richard

Buddin, Martin F. Shapiro, Sheryl H. Kataoka Arleen F. Brown, Chi-Hong Tseng, Peter

Bergman, and Paul J. Chung, “Successful schools and risky behaviors among low-income adolescents,”

Pediatrics, 2014, 134(2).


     Zimmer, Ron, Brian Gill, Kevin Booker, Stephane Lavertu, Tim R Sass, and John Witte,

Charter schools in eight states: Effects on achievement, attainment, integration, and competition, Vol. 869, Rand

Corporation, 2009.


     Zimmer, Ron and Cassandra Guarino, “Is there empirical evidence that charter schools push out low-

performing students?,” Educational Evaluation and Policy Analysis, 2013, 35 (4), 461–80.




                                                      30
Figures




                                              (a) First Experiment




                                             (b)Second Experiment

   Figure 1 - Experimental Design for (a) First experiment sample and (b) Second experiment sample
  (Note: This figure shows the study design and examples of the treatment phrasings). Race, gender and
  other email characteristics were assigned with equal probability across these treatment messages.


                                                31
Tables
                 Table 1 – Testing for baseline balance across treatment messages

                                                        Bad                           Bad            Good
                                        Baseline                        IEP
                                                     Grades                       Behavior        Grades
      TPS                                0.293            0.008         0.010         -0.004         -0.007
                                        (0.455)          (0.008)       (0.007)        (0.007)        (0.007)
      % LEP students                     0.123            0.026         0.019         -0.039         -0.013
                                         (0.18)          (0.042)       (0.037)        (0.04)         (0.038)
 % Chronically absent students           0.159           -0.031         0.041         -0.012          0.019
                                        (0.187)          (0.038)       (0.033)        (0.039)        (0.034)
      % Students with IEP                0.075           -0.034        -0.049          0.032          0.029
                                        (0.084)          (0.069)       (0.066)        (0.068)        (0.06)
      % Proficiency                      0.463            0.011        0.073*          0.045          0.060
                                        (0.225)          (0.047)       (0.042)        (0.047)        (0.045)
      % White students                   0.288           0.094*         0.024         -0.004         -0.026
                                        (0.308)          (0.057)       (0.051)        (0.054)        (0.051)
      % Black students                   0.254           -0.082         0.022          0.022          0.010
                                        (0.321)          (0.054)       (0.05)         (0.051)        (0.05)
      % Hispanic students                0.321           -0.036         0.046          0.060         -0.019
                                         (0.31)          (0.054)       (0.049)        (0.051)        (0.049)
      % FRPL students                     0.59           -0.036         0.001         -0.013          0.019
                                        (0.329)          (0.026)       (0.024)        (0.026)        (0.023)
      Value added measure                0.017           -0.002        -0.012         -0.008         -0.010
                                        (0.992)          (0.008)       (0.007)        (0.008)        (0.007)
      % Urban                            0.606           -0.043        -0.026          0.014          0.000
                                        (0.489)          (0.032)       (0.027)        (0.033)        (0.027)
      % Suburban                         0.269           -0.046        -0.015          0.005          0.003
                                        (0.443)          (0.033)       (0.028)        (0.034)        (0.028)
      % Rural                            0.058           -0.024         0.010          0.042          0.022
                                        (0.235)          (0.037)       (0.03)         (0.038)        (0.029)
      P-value, joint test                                0.391          0.194          0.691         0.862
       Observations                          5,031       2,441         3,522            2,462         1,434
       Notes: Column (1) shows means for the baseline message sample and the standard deviations in
 parenthesis. Each other column is as follows: For each treatment, the sample is restricted to baseline and
 treatment observation indicated in the column header, and the treatment indicator is regressed on the
 covariates shown in each row. School-level demographic data including absenteeism, Free-Reduced Priced
 Lunch (FRPL), Limited English Proficiency (LEP), and disability data are from the Civil Rights Data
 Collection 2013-14 public dataset. % Proficiency measures the rate of students at or above proficiency, as
 reported by Great Schools from 2016. See text for the construction of the Value-added measure. Two-way
 cluster-robust standard errors (by pair and school) in parentheses. The joint test is a test of whether the
 covariates are jointly different from zero.
       *** p<0.01, ** p<0.05, * p<0.1


                                                    32
           Table 2 – Effects of Message Treatments on School Response Rates

     Sample                    Full Sample                  TPS              Charter    TPS - Charter diff.

     Bad Grades                       -0.024**            -0.044**           -0.015               -0.030
                                       (0.011)            (0.021)            (0.013)             (0.025)
     IEP                             -0.052***             -0.002          -0.065***             0.058**
                                       (0.010)            (0.021)            (0.011)             (0.023)
     Bad Behavior                    -0.070***           -0.053***         -0.076***              0.021
                                       (0.011)            (0.021)            (0.013)             (0.024)
     Good Grades                        0.001              0.013              -0.005              0.015
                                       (0.014)            (0.021)            (0.020)             (0.029)
     Black                             -0.012              -0.004             -0.015              0.010
                                       (0.010)            (0.018)            (0.011)             (0.022)
     Hispanic                         -0.020**             -0.015            -0.023**             0.009
                                       (0.010)            (0.018)            (0.011)             (0.021)
     Father                             0.003              -0.009             0.008               -0.017
                                       (0.008)            (0.015)            (0.009)             (0.017)
     Son                               -0.011              -0.007             -0.011              0.004
                                       (0.007)            (0.011)            (0.008)             (0.014)
     Two Parents                        0.010              0.007              0.014               -0.009
                                       (0.011)            (0.015)            (0.015)             (0.021)
     TPS                                0.008
                                       (0.013)


     Observations                      14,806              4,296              10,510              14,806
      Control Group Mean                 0.533               0.496               0.548              0.533
      Notes: Table shows the results of a multivariate regression of an indicator for whether or not a
school responded to the message on message-treatment indicators. Columns (1)-(3) show the results for
different samples: (1) full sample, (2) only traditional public schools and (3) only charter schools. Column
(4) interaction between primary and secondary treatments and TPS. TPS is an indicator variable (not
a treatment) for whether a school is a traditional public school. All other variables included in the table
are randomly assigned characteristics of the emails. Regressions include fixed effects by wave and state.
Two-way cluster-robust standard errors (by pair and school) in parentheses.
      *** p<0.01, ** p<0.05, * p<0.1




                                                   33
            Table 3 - Effects of Message Treatments on School Response Perceived as Encouraging
              Sample                           Full Sample               TPS               Charter             TPS - Charter diff


Bad Grades                                       -0.027***             -0.065***            -0.012                   -0.052**
                                                  (0.010)               (0.016)            (0.012)                    (0.020)
IEP                                              -0.035***               0.001            -0.044***                   0.042**
                                                  (0.009)               (0.017)            (0.010)                    (0.020)
Bad Behavior                                     -0.061***             -0.083***          -0.051***                   -0.033*
                                                  (0.010)               (0.015)            (0.012)                    (0.019)
Good Grades                                        -0.003               -0.009              0.001                      -0.013
                                                  (0.012)               (0.017)            (0.017)                    (0.024)
Black                                             -0.018**              -0.013             -0.019*                     0.006
                                                  (0.008)               (0.014)            (0.010)                    (0.017)
Hispanic                                           -0.010                0.004              -0.016                     0.019
                                                  (0.008)               (0.014)            (0.010)                    (0.017)
Father                                             -0.005               -0.008              -0.004                     -0.006
                                                  (0.007)               (0.011)            (0.008)                    (0.014)
Son                                                -0.005               -0.013              -0.002                     -0.012
                                                  (0.006)               (0.010)            (0.008)                    (0.013)
Two Parents                                        0.012                0.022*              0.003                      0.017
                                                  (0.009)               (0.012)            (0.013)                    (0.017)
TPS                                              -0.056***
                                                  (0.009)


Observations                                       14,806                4,296             10,510                     14,806
Control Group Mean                                 0.256                 0.195              0.281                      0.256
Notes: Table shows the results of a multivariate regression of an indicator for whether or not a school responds in an "encouraging"
manner to the message. Non-response is coded as not encouraging (or zero). See Appendix C for a description of our qualitative
data and reports on inter-rater reliability. Columns (1)-(3) show the results for different samples: (1) full sample, (2) only traditional
public schools and (3) only charter schools. Column (4) interaction between primary and secondary treatments and TPS. TPS is
an indicator variable (not a treatment) for whether a school is a traditional public shcool. All other variables included in the table
are randomly assigned characteristics of the emails. Regressions include fixed effects by wave and state. Two-way cluster-robust
standard errors (by pair and school) in parentheses. *** p<0.01, ** p<0.05, * p<0.1




                                                                  34
          Table 4 - Differential Effect of Messages by VAM Performance and Type of Charter

       Sample                       High VAM                Low VAM               High-VAM,                 No Excuses
                                                                               Urban Charter               Charter
       Bad Grades                      -0.022                  -0.025                   0.010                  0.016
                                      (0.018)                 (0.018)                  (0.028)                (0.052)
       IEP                          -0.049***                -0.056***               -0.093***               -0.104**
                                      (0.016)                 (0.016)                  (0.024)                (0.044)
       Bad Behavior                 -0.067***                -0.066***                -0.070**                 0.009
                                      (0.018)                 (0.018)                  (0.028)                (0.054)
       Good Grades                     0.035                   -0.034                   0.054                  0.033
                                      (0.022)                 (0.022)                  (0.036)                (0.063)
       Black                         -0.032**                  0.007                  -0.049**                 0.036
                                      (0.015)                 (0.016)                  (0.024)                (0.050)
       Hispanic                     -0.049***                  0.019                  -0.062**                 0.029
                                      (0.015)                 (0.016)                  (0.024)                (0.044)
       Father                          0.012                   -0.006                   0.017                 0.085**
                                      (0.013)                 (0.013)                  (0.020)                (0.037)
       Son                             -0.006                  -0.012                   -0.007               -0.069**
                                      (0.011)                 (0.010)                  (0.017)                (0.031)
       Two Parents                     0.012                   0.007                    0.002                  0.046
                                      (0.017)                 (0.018)                  (0.029)                (0.047)
       TPS                             0.016                   0.003
                                      (0.020)                 (0.020)

       Observations                    8,835                   2,516                   3,457                    689
       Control Group
Mean                                   0.549                   0.538                    0.538                  0.559
      Notes: Table shows the effect of different treatments on response rate for different samples. TPS is a covariate for
traditional public school. All other variables included in the table are randomly assigned characteristics of the emails.
Sample row shows the population for each regression: (1) Schools with high value-added measure (>0), (2) Schools with
low high value-added measures (<0), (3) Urban charter schools with high VAM, and (4) “No Excuses” charter schools.
Regressions include fixed effects by wave and state. Two-way cluster-robust standard errors (by pair and school) in
parentheses.
      *** p<0.01, ** p<0.05, * p<0.1




                                                          35
Table 5 - Effect of IEP Message by State Funding Category and LEA status for Charter
                                     Schools

                                                                 (1)             (2)
                 IEP                                         -0.080***       -0.064***
                                                               (0.015)         (0.016)
                 IEP x Funding = Categorical                    0.022
                                                               (0.025)
                 IEP x Funding = Reimbursement                 0.068**
                                                               (0.035)
                 IEP x Own LEA                                                  -0.008
                                                                               (0.023)


                 Observations                                  10,510           10,087
                 Control Group Mean                             0.548           0.548
                  Notes: Table shows the effect of different treatments on response
            rate for charter schools in the sample depending on state funding and
            whether they are their own LEA (Local Education Agency). The
            categories for forms of funding are “Formula” (base), “Categorical,” and
            “Reimbursement.” All other treatment variables are randomly assigned
            characteristics of the emails. Regressions include all treatment indicators
            (only IEP is shown), wave and state fixed effects. Nested cluster-robust
            standard errors (by pair and school) in parentheses.
                  *** p<0.01, ** p<0.05, * p<0.1




                                               36
Appendix A.
Figures




                                             (a)




                                                 (b)


       Figure A1 - Message sent to parents based for (a) baseline and (b) IEP intervention
    (Note: This figure shows an example of one phrasing for the Baseline and IEP message)




                                            37
         Figure A2 - Map of Sample Frame
(Note: The map shows the sample frame for the experiment)




                      38
    Appendix Tables

    Table A1 – Total Population of Traditional Public Schools and Charter Schools vs. Sample

                                                       CRDC Population                              Sample
                                                                                           Charter      Charter     Charter
                                                        TPS        Charter      TPS
                                                                                            (All)       (Exp 1)     (Exp 2)
% White students                                        0.57        0.351       0.237       0.342        0.405       0.251
                                                       (.327)       (.325)      (.262)      (.323)       (.332)      (.285)
% Black students                                        0.133       0.291       0.279       0.262        0.231       0.306
                                                       (.219)       (.338)      (.311)      (.324)       (.304)      (.347)
% Hispanic students                                     0.205       0.275       0.399       0.314        0.278       0.367
                                                       (.258)       (.285)      (.307)      (.304)       (.285)      (.322)
% FRPL                                                  0.684        0.53       0.684       0.549        0.533       0.572
                                                       (.293)       (.329)      (.293)      (.33)        (.321)      (.34)
% Limited English Proficiency                           0.091       0.094       0.186       0.096        0.079       0.124
                                                       (0.148)     (0.162)      (0.2)      (0.167)      (0.154)     (0.182)
Absenteeism Rate                                        0.112       0.162       0.188        0.17            0.18    0.152
                                                       (0.292)     (0.384)     (0.226)     (0.333)      (0.313)     (0.363)
% IEP Students                                          0.108       0.086       0.117       0.088        0.093        0.08
                                                       (0.134)     (0.166)     (0.099)     (0.146)      (0.167)     (0.099)
% of IEP students in regular class <40% of time         0.008       0.004       0.017       0.002        0.003       0.002
                                                       (0.023)      (0.03)     (0.034)     (0.024)       (0.03)     (0.008)
% Students Receiving In-School Suspension               0.022       0.016       0.028       0.018        0.016       0.021
                                                       (0.296)     (0.304)     (0.109)     (0.227)      (0.207)     (0.257)
% Students Receiving 1 Out-of-school Suspension         0.01        0.024       0.031       0.027        0.027       0.026
                                                       (0.263)     (0.299)     (0.077)     (0.223)      (0.203)     (0.253)
% Proficiency                                          40.329       44.475     42.177       48.374       48.406     48.327
                                                      (15.725)     (20.445)    (22.273)    (22.515)     (21.998)    (23.282)
% Urban schools                                         0.239       0.518       0.682       0.575        0.504       0.682
                                                       (0.427)     (0.500)     (0.466)     (0.494)      (0.500)     (0.466)
% Suburban schools                                      0.318       0.291       0.285       0.269        0.263       0.278
                                                       (0.466)     (0.454)     (0.452)     (0.443)      (0.440)     (0.448)
% Rural schools                                         0.299       0.117       0.017       0.098        0.148       0.023
                                                       (0.458)     (0.321)     (0.130)     (0.298)      (0.355)     (0.151)

Observations                                               89,898       5,813      2,170        4,283     2,635       1,648
Notes: Means are reported for all schools that are available in CRDC 2013-14 dataset. The CRDC does not have data on all
schools in the study sample. TPS schools are traditional public schools. IEP students refers to students that have an
Individualized Education Program. School proficiency scores show the percentage of students scoring at or above proficiency
on state assessments across grades and subject as reported by Great Schools. Charter (All) column represents all the charter
schools in the experimental sample for both experiments. Charter (Exp 1) and Charter (Exp 2) columns show the
characteristics of the charter schools included in the first and second experiment, respectively.



                                                           39
Table A2 – The Relationship between Response and School Locations and Demographics

                                                                       Responded
                   City                                                   0.008
                                                                         (0.027)
                   Suburb                                                 -0.021
                                                                         (0.027)
                   Rural                                                  0.015
                                                                         (0.031)
                   % FRL                                                -0.070***
                                                                         (0.019)
                   % Female                                              0.180***
                                                                         (0.067)
                   % Black                                              -0.321***
                                                                         (0.047)
                   % White                                                0.006
                                                                         (0.051)
                   % Hispanic                                           -0.252***
                                                                         (0.047)
                   Enrollment                                            0.000**
                                                                         (0.000)
                   % Proficiency                                         -0.001**
                                                                         (0.000)
                   Proficiency (missing)                                -0.055***
                                                                         (0.018)
                   Constant                                              0.662***
                                                                         (0.059)


                   Observations                                           14,064
                    R-squared                                              0.049
                    Notes: Table shows regression coefficients and standard errors
              for a regression of school covariates on an indicator for response.
              School-level demographic data include enrollment, FRPL population,
              race and ethnicity breakdowns is retrieved from National Center for
              Education Statistics (NCES) Common Core of Data for the 2015-16
              school year. School proficiency shows the % of proficient (or higher)
              students as reported by GreatSchools in their most recent dataset
              from 2016. Nested clustered standard errors in parentheses.
                    *** p<0.01, ** p<0.05, * p<0.1




                                              40
       Table A3 - Effect of Message Treatments on Response Rates from Schools by Experiment

       Sample                  1st Experiment             2nd               TPS, 2nd            Charter, 2nd         Diff. TPS -
                                                    Experiment           Experiment           Experiment           Charter, 2nd
                                                                                                                   Experiment
       Bad Grades                   0.004               -0.044***             -0.044**             -0.043**              -0.005
                                   (0.017)                (0.014)              (0.021)              (0.020)              (0.029)
       IEP                        -0.055***             -0.042***              -0.002             -0.082***             0.073**
                                   (0.013)                (0.015)              (0.021)              (0.020)              (0.029)
       Bad Behavior               -0.064***             -0.072***            -0.053***            -0.091***               0.037
                                   (0.017)                (0.015)              (0.021)              (0.020)              (0.029)
       Good Grades                                        -0.003               0.013                -0.019                0.027
                                                          (0.014)              (0.021)              (0.020)              (0.029)
       Black                       -0.024                 -0.003               -0.004               -0.002               -0.003
                                   (0.015)                (0.013)              (0.018)              (0.018)              (0.026)
       Hispanic                   -0.031**                -0.013               -0.015               -0.011               -0.003
                                   (0.015)                (0.013)              (0.018)              (0.018)              (0.025)
       Father                       0.012                 -0.001               -0.009                0.007               -0.016
                                   (0.012)                (0.011)              (0.015)              (0.015)              (0.021)
       Son                         -0.020*                -0.004               -0.007               -0.000               -0.007
                                   (0.011)                (0.008)              (0.011)              (0.010)              (0.015)
       Two Parents                                        0.011                0.007                 0.014               -0.008
                                                          (0.011)              (0.015)              (0.015)              (0.021)
       TPS                                                0.008
                                                          (0.013)


       Observations                 6,210                 8,596                4,296                 4,300                8,596
       R-squared                    0.039                 0.043                0.064                 0.038                0.044
       Control Group
Mean                                0.575                  0.503               0.496                 0.510                0.503
      Notes: Table shows the effect of different treatments on response rate for the first (old) and second (new) experimental
sample. TPS is a covariate for traditional public school. All other variables included in the table are randomly assigned
characteristics of the emails. Regressions include fixed effects by wave and states. Sample row shows the population for each
regression: (1) Old sample (full), which only included charter schools), (2) New sample (full), (3) only traditional public schools
for new sample, (4) only charter schools for new sample, and (5) interaction between primary and secondary treatments and
TPS for new sample. Nested cluster-robust standard errors (by pair and school for new sample (full and diff TPS-Charter) and
only school for others) in parentheses.
      *** p<0.01, ** p<0.05, * p<0.1




                                                            41
    Table A4 - Effect of Message Treatments on Response Rates from Schools for Different
Specifications

                                                   (1)               (2)            (3)              (4)              (5)
          Bad Grades                           -0.024**         -0.027**         -0.027**          -0.022*          -0.022*
                                                 (0.011)           (0.011)        (0.011)          (0.012)          (0.012)
          IEP                                 -0.052***       -0.051***        -0.051***        -0.056***        -0.056***
                                                 (0.010)           (0.010)        (0.010)          (0.010)          (0.010)
          Bad Behavior                        -0.070***       -0.069***        -0.069***        -0.070***        -0.069***
                                                 (0.011)           (0.011)        (0.011)          (0.012)          (0.012)
          Good Grades                             0.001             0.002          0.002            -0.007          -0.007
                                                 (0.014)           (0.014)        (0.014)          (0.015)          (0.015)
          Black                                  -0.012            -0.014          -0.014          -0.019*          -0.019*
                                                 (0.010)           (0.010)        (0.010)          (0.011)          (0.011)
          Hispanic                             -0.020**         -0.021**         -0.021**          -0.018*          -0.017*
                                                 (0.010)           (0.009)        (0.009)          (0.010)          (0.010)
          Father                                  0.003             0.000          -0.000           -0.001          -0.000
                                                 (0.008)           (0.008)        (0.008)          (0.009)          (0.009)
          Son                                    -0.011            -0.010          -0.010           -0.009          -0.009
                                                 (0.007)           (0.007)        (0.007)          (0.007)          (0.007)
          Two Parents                             0.010             0.009          0.010            0.013            0.013
                                                 (0.011)           (0.011)        (0.011)          (0.012)          (0.012)
          TPS                                     0.008            0.027**        0.030**          0.033**          0.031*
                                                 (0.013)           (0.013)        (0.014)          (0.016)          (0.016)


          Observations                           14,806            14,806          14,806           14,806          14,806
          R-squared                               0.039             0.068          0.070            0.267            0.269
          Control Group Mean                      0.533
          School-level Controls 1                                    X               X                X                X
          School-level Controls 2                                                    X                X                X
          Pair Fixed Effects                                                                          X                X
           Tract-level Controls                                                                                        X
           Notes: Table shows the effect of different treatments on response rate. TPS is a covariate for traditional
     public school. All other variables included in the table are randomly assigned characteristics of the emails.
     School-level controls 1 include fraction of school population that is black, Hispanic, free-and-reduced price lunch
     eligible, female, the school's proficiency rating, and city, suburb, and rural status of the school's location. School-
     level controls 2 include fraction of the school population that has a disability, proportion of students with
     disabilities that are taken out of regular class <39%, 40-79%, and 80%+ of the time, fraction of school population
     that is chronically absent, and fraction of the population that has received an out-of-school suspension. Tract-
     level controls include disability shares, race/ethnicity shares, share of residents with a Bachelor’s Degree, median
     earnings, poverty rates, and food stamp recipients for the school’s associated census tract. Nested cluster-robust
     standard errors (by pair and school) in parentheses.
           *** p<0.01, ** p<0.05, * p<0.1




                                                              42
Table A5 – Effects of Message Treatments on School Response Rates Adjusting by Multiple
                                 Hypothesis Testing

        Sample                   Full Sample                 TPS              Charter    TPS - Charter diff.

        Bad Grades                       -0.024*            -0.044*            -0.015             -0.030
                                         [0.064]            [0.097]           [0.512]             [0.648]
        IEP                            -0.052***            -0.002          -0.065***             0.058*
                                        [<0.001]            [0.917]          [<0.001]             [0.053]
        Bad Behavior                   -0.070***           -0.053**         -0.076***             0.021
                                        [<0.001]            [0.038]          [<0.001]             [0.772]
        Good Grades                      0.001               0.013             -0.005             0.015
                                         [0.941]            [1.000]           [0.817]             [0.595]
        Black                            -0.012             -0.004             -0.015             0.010
                                         [0.631]            [0.818]           [0.578]             [1.000]
        Hispanic                          -0.02             -0.015             -0.023             0.009
                                         [0.166]            [1.000]           [0.201]             [1.000]
        Father                           0.003              -0.009             0.008              -0.017
                                         [0.724]            [1.000]           [0.396]             [1.000]
        Son                              -0.011             -0.007             -0.011             0.004
                                         [0.441]            [1.000]           [0.599]             [0.76]
        Two Parents                      0.010               0.007             0.014              -0.009
                                         [0.719]            [1.000]           [0.708]             [1.000]


        Observations                     14,806              4,296            10,510              14,806
         Control Group Mean               0.533              0.496              0.548              0.533
         Notes: Table shows the results of regressing message-treatment indicators on an indicator variable
   for whether or not a school responded to the message. Columns (1)-(3) show the results for different
   samples: (1) full sample, (2) only traditional public schools and (3) only charter schools. Column (4)
   interaction between primary and secondary treatments and TPS. TPS is an indicator variable (not a
   treatment) for whether a school is a traditional public school. All other variables included in the table
   are randomly assigned characteristics of the emails. Regressions include fixed effects by wave and state.
   Adjusted p-values by multiple hypothesis testing in squared parentheses. Multiple hypothesis
   adjustment was done using Holm’s method separately for primary and secondary treatments. ***
   p<0.01, ** p<0.05, * p<0.1




                                                     43
Table A6 - Total Population of Charter Schools vs Sample Charter Schools and No Excuses
                                    Charter Schools

                                                           CRDC                          Sample
                                                                                                  No Excuses
                                                           Charter            Charter              Charter
  % Limited English Proficiency                             0.094              0.096                 0.157
                                                           (0.162)            (0.167)                (0.19)
  Absenteeism Rate                                          0.162               0.17                 0.112
                                                           (0.384)            (0.333)               (0.157)
  % IDEA Students                                           0.086              0.088                 0.069
                                                           (0.166)            (0.146)               (0.054)
  % of IEP students in regular class <40% of time           0.004              0.002                 0.001
                                                            (0.03)            (0.024)               (0.006)
  % of IEP students in regular class 40-79% of time         0.005              0.005                 0.004
                                                            (0.02)            (0.021)               (0.012)
  % of IEP students in regular class >80% of time           0.067              0.069                 0.047
                                                           (0.088)            (0.096)               (0.051)
  % Students Receiving In-School Suspension                 0.016              0.018                 0.041
                                                           (0.304)            (0.227)               (0.064)
  % Students Receiving 1 Out-of-school Suspension           0.024              0.027                 0.045
                                                           (0.299)            (0.223)               (0.042)
  % Proficiency                                            44.475              48.374                62.733
                                                           (20.445)           (22.515)              (21.945)
  Value Added Measure                                         -                0.000                 0.775
                                                                              (1.092)               (0.976)
  Observations                                                 5813             4283                   272
  Notes: Means are reported for all schools that are available in CRDC 2013-14 dataset. The CRDC does not
  have data on all schools in the study sample, which was from 2016-17. Charter schools are all schools that are
  classified by CRDC as charter schools. IDEA students refer to students in the Individuals with Disabilities
  Education Act. IEP students refers to students that have an Individualized Education Program covered by
  IDEA. Charter schools may be alternative, magnet, or special education schools. School proficiency scores show
  the percentage of students scoring at or above proficiency on state assessments across grades and subject as
  reported by Great Schools in their most recent dataset from 2016. Value-added measure (VAM) is constructed
  by calculating the normalized difference between the observed proficiency and the predicted proficiency from a
  regression including school-level and tract-level controls. VAM measures can only be estimated for the sample.




                                                      44
   Table A7 - Effect of Message Treatments on Response Rates by Different Randomized
Characteristics

        Sample                Two parent     One parent              Son          Daughter           Black          Hispanic
        Bad Grades                 -0.031         -0.023*          -0.023           -0.023           -0.011         -0.035*
                                   (0.028)        (0.012)          (0.017)         (0.017)          (0.020)          (0.021)
        IEP                       -0.049*       -0.052***        -0.051***        -0.053***      -0.053***         -0.038**
                                   (0.026)        (0.011)          (0.016)         (0.015)          (0.018)          (0.018)
        Bad Behavior               -0.025       -0.080***        -0.058***        -0.084***      -0.091***         -0.061***
                                   (0.027)        (0.013)          (0.017)         (0.017)          (0.020)          (0.021)
        Good Grades               -0.047*          0.029           -0.012           0.015            -0.027         0.058**
                                   (0.026)        (0.018)          (0.022)         (0.021)          (0.027)          (0.025)
        Black                       0.022         -0.021*          -0.023*          -0.001                           0.000
                                   (0.022)        (0.011)          (0.014)         (0.014)                           (0.000)
        Hispanic                   -0.002        -0.025**          -0.024*          -0.018           0.000
                                   (0.022)        (0.011)          (0.014)         (0.014)          (0.000)
        Father                      0.011          0.001            0.007           -0.001           0.017           0.010
                                   (0.018)        (0.009)          (0.011)         (0.011)          (0.013)          (0.014)
        Son                         0.014        -0.017**                                           -0.022*          -0.010
                                   (0.017)        (0.008)                                           (0.013)          (0.014)
        Two Parents                                                 0.025           -0.004          0.033*           0.005
                                                                   (0.016)         (0.016)          (0.020)          (0.019)
        TPS                         0.003          0.010            0.003           0.012            0.006           0.007
                                   (0.019)        (0.015)          (0.015)         (0.015)          (0.020)          (0.019)


        Observations                2,916          11,890           7,375           7,431            4,999           4,916
        R-squared                   0.052          0.041            0.039           0.042            0.044           0.044
        Control Group
 Mean                                0.522           0.535           0.532             0.551            0.546          0.523
       Notes: Table shows the effect of different treatments on response rate for different samples. TPS is a covariate for
 traditional public school. All other variables included in the table are randomly assigned characteristics of the emails.
 Regressions include fixed effects by wave and states. Sample row shows the population for each regression identified in
 the e-mails: (1) two parents, (2) one parent, (3) student is a son, (4) students is a daughter, (5) black-sounding name, and
 (6) Hispanic-sounding name. Two-way cluster-robust standard errors (by pair and school) in parentheses.
       *** p<0.01, ** p<0.05, * p<0.1




                                                            45
           Table A8 - Effects of Message Treatments on the Likelihood of Applying to School

             Sample                     Full Sample          TPS           Charter          TPS - Charter diff



Bad Grades                               -0.045***         -0.065***      -0.038***                -0.027
                                          (0.009)           (0.015)        (0.011)                (0.019)
IEP                                      -0.054***          -0.016        -0.063***               0.045**
                                          (0.008)           (0.017)        (0.010)                (0.019)
Bad Behavior                             -0.071***         -0.081***      -0.067***                -0.016
                                          (0.009)           (0.015)        (0.011)                (0.018)
Good Grades                                -0.015            0.006         -0.033**                0.037
                                          (0.011)           (0.017)        (0.016)                (0.023)
Black                                     -0.013*          -0.027**         -0.007                 -0.021
                                          (0.008)           (0.014)        (0.009)                (0.016)
Hispanic                                   -0.003           -0.002          -0.003                 0.001
                                          (0.008)           (0.013)        (0.009)                (0.016)
Father                                     0.002             0.011          -0.001                 0.009
                                          (0.006)           (0.011)        (0.008)                (0.013)
Son                                        -0.003           -0.012          0.002                  -0.014
                                          (0.006)           (0.010)        (0.007)                (0.012)
Two Parents                                0.010             0.016          0.004                  0.011
                                          (0.008)           (0.011)        (0.012)                (0.016)
TPS                                      -0.020**
                                          (0.009)


Observations                              14,806             4,296          10,510                 14,806
Control Group Mean                           0.211            0.180           0.225                0.211
Notes: Table shows the results of a multivariate regression of a binary indicator for whether a school's response is
likely to elicit an application based on trained coders review of school responses. See Appendix C for a description
of our qualitative data and reports on inter-rater reliability. Non-response by the school is coded as not likely to
apply to the school (or zero). Columns (1)-(3) show the results for different samples: (1) full sample, (2) only
traditional public schools and (3) only charter schools. Column (4) interaction between primary and secondary
treatments and TPS. TPS is an indicator variable (not a treatment) for whether a schoool is a traditional public
shcool. All other variables included in the table are randomly assigned characteristics of the emails. Regressions
include fixed effects by wave and state. Two-way cluster-robust standard errors (by pair and school) in parentheses.
*** p<0.01, ** p<0.05, * p<0.1




                                                           46
                                Table A9 - States Included in Field Experiments



   States                                          1st Experiment                           2nd Experiment


   Alaska                                                                                          ✔

   Arizona                                                ✔                                        ✔

   Arkansas                                                                                        ✔
   California                                             ✔                                        ✔

   Colorado                                               ✔                                        ✔

   Connecticut                                                                                     ✔
   Delaware                                                                                        ✔

   DC                                                                                              ✔
   Florida                                                ✔                                        ✔
   Georgia                                                ✔                                        ✔
   Illinois                                                                                        ✔
   Indiana                                                                                         ✔
   Louisiana                                              ✔
   Maryland                                                                                        ✔
   Massachusetts                                                                                   ✔
   Michigan                                               ✔                                        ✔
   Minnesota                                              ✔                                        ✔
   Nevada                                                                                          ✔
   New Jersey                                             ✔                                        ✔
   New Mexico                                                                                      ✔
   New York                                               ✔                                        ✔
   North Carolina                                         ✔                                        ✔

   Ohio                                                   ✔                                        ✔
   Oregon                                                 ✔                                        ✔
   Pennsylvania                                           ✔                                        ✔

   South Carolina                                                                                  ✔
   Tennessee                                                                                       ✔
   Texas                                                  ✔                                        ✔

   Utah                                                   ✔                                        ✔

   Wisconsin                                              ✔                                        ✔
Notes: Table reports which states were included in the first and second field experiments, respectively.




                                                            47
    Appendix B.
    Data Collection and Implementation

    First field experiment

    This subsection describes data collection for the first field experiment. Data collection of school
contact information took place between August and October 2014. Email and web-form contact
information was collected for charter schools from the 17 states with the most charter schools. Once
these states were identified we used the U.S. Department of Education’s Common Core of Data to
determine which public schools identify as charter schools and limited our sample to these schools. The
next step involved collecting contact information. To the extent possible, we visited charter school
websites to obtain contact information. Sometimes websites were not easily located, so we used
databases maintained by state Departments of Education. We emphasized the collection of contact
information from school websites, because this is the likely contact information a parent would use if
emailing a school. For both first and second field experiments, we prioritized the type of contact
information collected. If there is an email address or webform that is used to field general inquiries,
then we used this. Otherwise, we would identify a front office receptionist or office manager. If this was
not available, then we would look for the contact information for one of the school principals.

    To conduct the first experiment, we created several internet domains. For each internet domain,
we created email addresses that signal ethnic- and gender-sounding names. In the first and second field
experiments, we used Tor browsers to obscure the IP address of the sending location to make it appear
messages were sent from local parents.

    Second field experiment

    This describes how we collected data on schools for the second field experiment for the school
choice audit study. Data collection for the field experiment took place between Nov. 2017 and Jan 2018.

    We worked with research assistants to identify school districts across states that practice some
type of intra-district school choice and that also have charter schools within their respective catchment
areas. In identifying these school districts, we first focused on whether intra-district choice is practiced.
We used geographic information systems (GIS) data from U.S. Census Bureau to identify local school
district catchment areas. We also used latitude and longitude data on each school from the U.S.
Department of Education’s Common Core of Data to identify whether charter schools are geographically
situated in school districts that practice intra-district choice.

    One challenge is that there is a variation in how intra-district choice is practiced across school
districts within states. These practices helped guide the selection of catchment areas to include in our
sample. Open enrollment represents one end of the spectrum. This type of intra-district school choice
is well-studied. See, for example, the research conducted by David Deming on Charlotte-Mecklenburg
                                                     48
schools. Bergman (2016) investigates another type of inter-district choice involving transfers in
California schools. Typically, open enrollment and transfer policies are well known and formal
application processes have been established. Less known are intra-district choice policies that require
some form of administrative approval. For example, a superintendent may need to grant approval before
parents can send a child to another school in a district, or, within a specific district, principals from
sending and receiving schools may have to agree upon a child moving from one school to another. These
types of intra-district choice might be viewed as more restrictive than open-enrollment policies. For
school districts that place strong restrictions on intra-district choice, it may not be viewed as
unreasonable that teachers, receptionists, front desk and office managers may be neither aware of the
policy nor its eligibility requirements.

     We restricted our attention to states with the most charter schools such as California, Texas,
Florida, Arizona, Ohio, Michigan, New York, Nevada, and the District of Columbia. For each of these
states, we then identified the top 40 school districts in terms of student enrollments. Some states, such
as Arizona, have mandatory uniform intra-district school choice policies. Uniformity in implementation
of intra-district school choice laws simplified the process of selecting school district geographic areas to
include in our sample because we did not have to check choice policies district by district. Other states,
such as Tennessee, allow intra-district choice but it is only practiced among its largest school district
located in Nashville. National Center for Education Statistics Table 4.2 identifies more than 25 different
types of policies across states with open enrollment policies.31

        For each state, our research assistants visited school districts’ websites to learn about their intra-
district enrollment policy. Once we determined that a school district within a state practices intra-
district choice, we made the decision to include that school district’s geographic area in our sampling
frame (provided there are charter schools within the catchment area).

        The NCES data used are for the 2014-2015 school year. Although these are not the latest files from
the Common Core of Data, at the time we defined the sampling frame, they were the most recent files
that are complete.

        For each charter school, research assistants found the nearest traditional public school. We matched
the charter school to the nearest traditional public based on the type of school (i.e., regular, special
education, etc.) and the entry grade level.

        We kept charter schools in our sample (and their respective matched pair) if the charter school
enrollment was greater than 200 students. We also limited the number of schools we attempted to
contact in a single school district to 80.

        In order to implement the experiment, we created 24 domains and developed distinct websites for


31
     See Table 4.2: “Numbers and types of open enrollment policies, by state: 2017.”

                                                                      49
each domain. For each website, we created email addresses that signal ethnic-sounding names as well
as whether a fictitious parent is male or female. When sending emails to schools, we used Tor browsers
to obscure our IP address and make it appear that the email was being sent from a local parent. For
our study, we did not observe any increase in website visits based on the implementation of the
experiment.




                                                 50
Appendix C.
Content Analysis Methodology

     Content analyses turn unstructured textual content into quantitative data that can be analyzed
as such. Specifically, features of text are converted into categorical, ordinal, or binary numbers. The
current content analysis quantified the emailed responses from charter schools and traditional public
schools subject to school choice to the inquiring “parents,” as described above.

     A data scientist who specializes in text analytics and the intersection of qualitative and
quantitative research methods oversaw the content analysis utilized in this research. Specifically, the
researcher created a codebook based on the research hypotheses and a grounded textual analysis of
sample responses from the schools to the parents that creates quantitative variables from the text
regarding the content of the email responses. The codebook continued to be adapted during preliminary
coding.

     The content analysis coded for two variables of interest. For whether the response was considered
encouraging or discouraging, coders rated this on a four-point scale: very discouraging, somewhat
discouraging, somewhat encouraging, and very encouraging. And for whether the response was
considered to evoke the likelihood a coder would apply to the charter school or traditional public school,
which also was coded on a four-point scale: very unlikely, somewhat unlikely, somewhat likely, and very
likely.

     The researcher also oversaw the training of four independent coders. Training included coding
several emails as a group, working through any questions or differences of opinion on how to code.
Then, the coders and researcher each independently coded the same 25 emails. The results were
compared, with any discrepancies worked through as a group and resolved. All four coders then recoded
the same 25 responses to ensure consistent coding. The codebook creates quantitative variables from
the text regarding the content of the email. With consistent quality established, the group then
proceeded to code a new set of 100 responses. Inter-coder reliability was calculated in several ways
across each coded variable in order to ensure coders were consistent in their approach to coding.

     Inter-coder reliability was calculated in four ways. First, average percent agreement is simply the
percent of cases agreed upon across all coders. Scores above 80% are considered reliable for most studies,
while scores above 70% are considered valid in exploratory studies. In this study, the mean average
percent agreement across all coded variables is 96.8%. Second, Pairwise Cohen’s Kappa is similar to
average percent agreement but also factors in values coded by chance, with scores ranging from -1 to
1. Generally, .41 to .60 is considered moderate reliability, .61 to .80 substantial reliability, and .81 to
1.0 almost perfect reliability. The average across all variables in this study is .92. Fleiss Kapp, which
also ranges from -1 to 1, is if the observed agreement is less than expected, while also penalizing 100%
agreement. Above .75 is considered excellent. In this study, the average Fleiss’ Kappa across coded
                                                   51
variables is .92. Finally, Krippendorf’s Alpha measured observed and expected agreement, and is
considered the gold standard of inter-coder reliability. Scores above .80 are considered adequate. The
average across all variables coded in this study is .92.




                                                    52
