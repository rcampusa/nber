                              NBER WORKING PAPER SERIES




                   THE GENERALIZED INFORMATIVENESS PRINCIPLE

                                        Pierre Chaigneau
                                          Alex Edmans
                                         Daniel Gottlieb

                                       Working Paper 20729
                               http://www.nber.org/papers/w20729


                     NATIONAL BUREAU OF ECONOMIC RESEARCH
                              1050 Massachusetts Avenue
                                Cambridge, MA 02138
                                   December 2014




The views expressed herein are those of the authors and do not necessarily reflect the views of the
National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2014 by Pierre Chaigneau, Alex Edmans, and Daniel Gottlieb. All rights reserved. Short sections
of text, not to exceed two paragraphs, may be quoted without explicit permission provided that full
credit, including © notice, is given to the source.
The Generalized Informativeness Principle
Pierre Chaigneau, Alex Edmans, and Daniel Gottlieb
NBER Working Paper No. 20729
December 2014
JEL No. D86,J33

                                              ABSTRACT

This paper shows that the informativeness principle, as originally formulated by Holmstrom (1979),
does not hold if the first-order approach is invalid. We introduce a "generalized informativeness principle"
that takes into account non-local incentive constraints and holds generically, even without the first-order
approach. Our result holds for both separable and non-separable utility functions.


Pierre Chaigneau                                     Daniel Gottlieb
HEC Montreal                                         The Wharton School
3000 chemin de la côte-Sainte-Catherine              University of Pennsylvania
Montreal H3T 2A7                                     3303 Steinberg Hall-Dietrich Hall
Canada                                               3620 Locust Walk
pierre.chaigneau@hec.ca                              Philadelphia, PA 19104
                                                     dgott@wharton.upenn.edu
Alex Edmans
The Wharton School
University of Pennsylvania
2460 Steinberg Hall - Dietrich Hall
3620 Locust Walk
Philadelphia, PA 19104
and NBER
aedmans@wharton.upenn.edu
1       Introduction
The informativeness principle, or su¢ cient statistic theorem, states that a signal has
positive value if and only if it a¤ects the likelihood ratio. This principle is believed to be
the most robust result from the moral hazard literature. For example, the textbook of
Bolton and Dewatripont (2005) states that this literature has produced very few general
results, but the informativeness principle is one of the few results that is general.1
    Due to its perceived robustness, the informativeness principle has been applied
to many settings. It is the key concept behind the theories of relative performance
evaluation (Baiman and Demski (1980), Holmstrom (1982)), tournaments (Lazear and
Rosen (1981)), and yardstick competition (Shleifer (1985)). This wide applicability in
turn has led to substantial impact in many …elds, such as compensation, insurance,
and regulation. For example, in Bebchuk and Fried’s (2004) in‡uential book on the
ine¢ ciency of executive compensation practices, one of their leading arguments is that
exogenous “luck”is not …ltered out from the contract.
    The original formulation of the informativeness principle, in Holmstrom (1979)
and Shavell (1979), assumes the validity of the …rst-order approach (“FOA”): that
the agent’s incentive constraint can be replaced by its …rst-order condition. All of
its generalizations assume either the FOA (e.g. Gjesdal (1982), Amershi and Hughes
(1989), Kim (1995)) or that the agent chooses between two actions only (e.g. Hart and
Holmstrom (1987), Bolton and Dewatripont (2005)). As is well-known, the FOA is
generally not valid.2 Assuming only two actions has a similar e¤ect to using the FOA,
as it means that only one incentive constraint binds, but is unrealistic.
    Due to the signi…cance of the informativeness principle and the restrictive setting in
which it was derived, it is important to understand whether it is a robust property that
holds more generally. In this paper, we show that the informativeness principle may
not hold when the FOA is invalid. Our main contribution is to propose a “generalized
    1
     They write: “The basic moral hazard problem has a fairly simple structure, yet general conclusions
have been di¢ cult to obtain ... Among the main general predictions of the model is the informativeness
principle” (p129 and p169).
   2
     Rogerson (1985) derives the most well-known su¢ cient conditions for the validity of the FOA in
the single-signal case. As Jewitt (1988) points out, these assumptions are so strong that they are not
satis…ed by any standard distribution. Moreover, they are no longer su¢ cient if the principal observes
multiple signals, which is needed to analyze the informativeness principle (as the principal observes
output and an additional signal). Jewitt (1988), Sinclair-Desgagné (1994), Conlon (2008), and Ke
(2012) obtain su¢ cient conditions for the validity of the FOA in the mutliple-signal case.


                                                  2
informativeness principle”that provides su¢ cient conditions for a signal to have value
and holds generically (i.e., for all parameters except for sets of measure zero).
    Since the original informativeness principle assumed the FOA, only the likelihood
ratio between adjacent e¤orts mattered. When the FOA is invalid, the binding incentive
constraint(s) are not local. Thus, even if the signal is informative about the local
likelihood ratio, it may have zero value for the contract. Since we do not know from the
outset which incentive constraints will bind, our generalized informativeness principle
requires the signal to a¤ect the likelihood ratio between the principal’s preferred e¤ort
and all other e¤orts, rather than only adjacent e¤orts.
    When only one incentive constraint binds, the generalized informativeness principle
always holds. If the signal a¤ects all incentive constraints, it will a¤ect the binding
incentive constraint, and thus have strictly positive value by the same intuition as in
Holmstrom (1979). The principal can use the signal to relax the binding incentive
constraint by transferring payments from states with low likelihood ratios to states
with high likelihood ratios, in turn reducing the expected payment.
    With more than two e¤orts, however, multiple incentive constraints bind for an
open set of parameters. In this case, signals that a¤ect all likelihood ratios may still
have zero value. While the principal can use such a signal to transfer payments to relax
one binding constraint, the same transfer may tighten another binding constraint by
the same magnitude, and so the overall payment reduction is exactly zero.
    Counter-examples such as this are knife-edge in that they require the bene…t from
relaxing one binding constraint to exactly equal the cost of tightening other binding
constraints. Intuitively, they require the shadow prices of the binding constraints to be
equal, and so they are non-robust to small perturbations in the probability distribution
or the utility function. Accordingly, we show that, except for a set of parameters with
measure zero, any signal that a¤ects all likelihood ratios has positive value.
    This generalized informativeness principle allows not only for additively separable
utility (as in all previous versions of the principle), but also for multiplicatively separa-
ble and non-separable utility. This generalization, however, requires us to distinguish
between e¤orts that can be implemented with no agency costs (i.e. a constant wage,
so that the …rst best can be achieved) and those that cannot. Previous versions of
the informativeness principle assume additively separable utility (in which case only
the cheapest e¤ort can be implemented with a constant wage) and an interior e¤ort
(so that it cannot be implemented with a constant wage). With non-separable utility,


                                             3
many e¤orts may be implementable with a constant wage, in which case the signal
automatically has no value. We show that, for both separable and non-separable util-
ity, the informativeness principle (generically) holds only for e¤orts that cannot be
implemented with a constant wage.
    Finally, Holmstrom’s (1979) original theorem was an “if and only if” result, pro-
viding necessary and su¢ cient conditions for a signal to have strictly positive value in
contracting. While our main result concerns the more surprising (“su¢ cient”) part,
we also generalize the “necessary” part of his theorem. That is, we show that an
uninformative signal has no value for the contract even when utility is non-separable.
    Our paper proceeds as follows. Section 2 revisits the original informativeness prin-
ciple and shows that it may not hold if the FOA is invalid. Section 3 shows that a
generalized informativeness principle generically holds. Section 4 concludes.


2    The Informativeness Principle and the FOA
This section shows that the informativeness principle needs to be modi…ed when non-
local incentive constraints bind. There is a single risk-neutral principal (“she”) and a
single risk-averse agent (“he”). The agent chooses an action e 2 E; which we refer to as
“e¤ort” and is not observable by the principal. The principal observes output x 2 X
and a signal s 2 S, which may be informative about e. Both output and the signal are
contractible. We refer to a pair (x; s) as a “state.” In Section 3, we will assume that
the action, output, and signal spaces are …nite; for now, to achieve comparability with
Holmstrom (1979), we allow them to be intervals of the real line as well.
    While Holmstrom (1979) considers an additively separable utility function, we fol-
low Grossman and Hart (1983) and generalize to the following utility function:

Assumption 1. The agent’s Bernoulli utility function over income w and e¤ort e is

                            U (w; e) = G (e) + K (e) V (w) :                         (1)

(i) K (e) > 0 for all e; (ii) V : W ! R is continuously di¤erentiable, strictly in-
creasing, and strictly concave, and W = (w; +1) is an open interval of the real line
(possibly with w = 1); and (iii) U (w1 ; e1 ) U (w1 ; e2 ) =) U (w2 ; e1 ) U (w2 ; e2 )
for all e1 ; e2 2 E and w1 ; w2 2 W.



                                           4
    The agent has utility function (1) if and only if his preferences over income lot-
teries are independent of his e¤ort. Conditions (i) and (ii) state that the agent likes
money and dislikes risk. Condition (iii) requires preferences over known e¤orts to be
independent of income.3 When K (e) = K for all e, the utility function is additively
separable between e¤ort and income as in Holmstrom (1979). When G (e) = 0 for all
e, it is multiplicatively separable.4 The agent’s reservation utility is U .
    As Grossman and Hart (1983) show, the principal’s problem can be split in two
stages. First, she …nds the cheapest contract that induces each e¤ort e 2 E. Second,
she determines which e¤ort e to induce. This paper focuses on the …rst stage: whether
the principal can use the signal s to reduce the cost of implementing a given e¤ort.5
    First, we state Holmstrom’s (1979) original theorem6 :

Theorem. (Informativeness Principle): Assume that the utility function is additively
separable and that the FOA is valid. Suppose states are distributed according to a
continuously di¤erentiable probability density function f (x; sje). The signal has zero
value for implementing e if and only if

                                       fe (x; sje )
                                                    = (x; e )                                        (2)
                                       f (x; sje )

for almost all x; s.

   The expression on the right of (2) corresponds to the change in the likelihood ratio
f (x;sje + e)
           for in…nitessimal changes in e¤ort e 0. Since only the local IC matters
   f (x;sje )
when the FOA is valid, the value of the signal only depends on this local e¤ect.
   We now present an example in which the signal is informative (i.e., (2) fails to hold)
and yet has zero value because the FOA is not valid and so the relevant IC is not local.
This violation motivates the generalized informativeness principle of Section 3.7
   3
     Assumption 1(iii) still allows the agent’s preferences for lotteries over e¤ort to depend on income.
   4
     Multiplicative separability is commonly used in macroeconomics (e.g. Cooley and Prescott
(1995)). In addition, Edmans, Gabaix, and Landier (2009) show that they are necessary and suf-
…cient to obtain empirically consistent scalings of CEO incentives with …rm size.
   5
     Holmstrom (1979) avoids this issue by assuming that either the signal is informative for all e¤ort
levels or for no e¤ort level.
   6
     In Supplementary Appendix B.2 we formally de…ne what it means for a signal to have value.
   7
     We are not the …rst to point out that signals that only a¤ect non-binding ICs have zero value
(see, e.g., footnote 7 in Holmstrom and Milgrom (1987)). We are, however, the …rst to show that even
signals that a¤ect all ICs (including the binding ones) may have zero value: see Example 2.


                                                   5
Example 1. Assume that the utility function is additively separable between income
                                                                                  b
and e¤ort: U (w; e) = V (w) + G (e) : The utility of income V is bounded above by U
and the cost of e¤ort is
                                  8
                                  >
                                  < 0 if e = 0
                          G (e) =      K if e 2  = f0; 1g ;
                                  >
                                  :
                                       C if e = 1
where K U   b and C > 0.
    Since the cost of any e¤ort e 2 = f0; 1g exceeds the maximum utility of income U  b,
the agent will never choose e 2= f0; 1g. If the principal wishes to implement e = 1, the
relevant IC is the one preventing the agent from selecting e = 0. The signal has zero
value for implementing e = 1 if and only if the likelihood ratio between e¤orts 0 and 1
is independent of the signal, i.e.:
                                 f (x; sj1)   f (x; s0 j1)
                                            =
                                 f (x; sj0)   f (x; s0 j0)
for almost all (x; s) and (x; s0 ). This condition does not imply and is not implied by
the local condition ffe(x;sj1)
                       (x;sj1)
                               = 0:
   When the FOA is not valid, non-local ICs may bind. Then, as Example 1 shows,
the relevant likelihood ratio is the one comparing the implemented e¤ort (e = 1) to
the e¤ort exerted in the binding IC (e = 0). Thus, a¤ecting the likelihood ratio for
adjacent e¤orts is no longer su¢ cient for a signal to have positive value.
   Note that Holmstrom’s (1979) informativeness principle is an “if and only if”result.
The less surprising part shows that uninformative signals have zero value (“necessity”).
The more interesting part shows that every informative signal has strictly positive value
(“su¢ ciency”). The main contribution of this paper is to generalize the su¢ ciency part.
However, before doing so, we …rst generalize the necessity part to settings in which the
FOA is not valid and the utility function is not additively separable. The proof is in
Supplementary Appendix B.4.
Proposition 1. Let (x; s) be either continuously or discretely distributed, and let
f (x; sje) denote either the probability density function or the probability mass func-
tion. Suppose ff(x;sje
                  (x;sje)
                         )
                           = e (x; e) for all e and almost all (x; s) under e . Then, the
signal has zero value in implementing e .
Thus, uninformative signals have zero value even when utility is not additively separable
or the FOA is not valid. The remainder of our paper focuses on the su¢ ciency part.

                                            6
3         The Generalized Informativeness Principle
Following Grossman and Hart (1983), we assume that states and e¤orts are …nite:
E    f1; : : : ; Eg, X    fx1 ; :::; xX g, and S      f1; :::; Sg.8 The probability of observing
state (x; s) conditional on e¤ort e is denoted by pex;s Pr(~         x = x; s~ = sj~e = e) > 0.
                   1
    Let h V          denote the inverse utility function. Since u is increasing and strictly
concave, h is increasing and strictly convex. Letting ux;s               u (wx;s ), the principal’s
program can be written in terms of “utils”:
                                             X
                                         min     pex;s h (ux;s )                                (3)
                                             fux;s g
                                                       x;s

subject to
                                                             X
                                    G (e ) + K (e )                    pex;s ux;s       U;               (4)
                                                                 x;s
                      X
                             K (e ) pex;s     K (e) pex;s ux;s                G (e)          G (e ) 8e   (5)
                       x;s

where (4) and (5) are the agent’s participation and incentive constraints (IR and IC).
   We …rst note that a signal can only have positive value when there are agency costs.
Let we denote the wage that gives the agent his reservation utility if he exerts e¤ort e:
                                                             U     G (e)
                                            we = h                                  :
                                                                 K (e)
The principal can implement e¤ort e with no agency costs if all ICs are satis…ed when
she o¤ers the constant wage we that satis…es the IR with equality:

                                       U (we ; e )           U (we ; e) 8e:                              (6)

We say that the …rst best is feasible for e if condition (6) holds. The principal then
obtains the …rst-best payo¤ by using a constant wage and so signals automatically have
zero value. When utility is either additively or multiplicatively separable, the …rst
best is only feasible for the least costly e¤ort. With non-separable utility, however,
it may be feasible for several di¤erent e¤orts. (The …rst-best is never achieved in
Holmstrom (1979) because he assumes additively separable utility and an interior e.)
The informativeness principle does not hold if the …rst-best is feasible, i.e. no IC binds.
Remark 1 notes that it holds whenever exactly one IC binds:
    8
        Finite e¤orts allow us to use Kuhn-Tucker methods to obtain necessary optimality conditions.

                                                             7
Remark 1. Suppose that exactly one IC binds in Program (3)-(5) and let e be an
e¤ort for which the …rst best is not feasible. The necessary Kuhn-Tucker conditions
from the principal’s program yield, for all (x; s) in the support,
                                                         !
                                                     e0
                                                   p x;s
                   h0 (ux;s ) +  K (e ) K(e0 ) e           + K (e ) = 0;        (7)
                                                   px;s

where       0 is the multiplier associated with the binding IC. Subtracting these condi-
tions in states (x; s) and (x; s0 ) gives
                                                                     !
                                                         e0      e0
                                                       p x;s 0 p x;s
                       h0 (ux;s ) h0 (ux;s0 ) = K(e0 )                 :             (8)
                                                       pex;s0  pex;s

If = 0, then (8) implies a constant wage, which contradicts our assumption that the
…rst best is not feasible.9 Therefore, > 0 and, because K (e) > 0 for all e, it follows
                                                                      0             0
                                                                    pex;s0        pex;s
from (8) and the convexity of h that ux;s 6= ux;s0 whenever         pex;s0
                                                                             6=   pex;s
                                                                                        :

    The …nal case to consider is when multiple ICs bind. When there are at least three
states, it is not unusual for multiple ICs to bind. Formally, we show in Supplementary
Appendix B.3 that multiple ICs bind for a non-empty and open set of parameter values.
Since any non-trivial model with informative signals requires at least three states (at
least two outputs and at least two signals conditional on at least one output), it is
important to study the case of multiple binding ICs.
    We start with an example showing that, if multiple ICs bind, the generalized infor-
mativeness principle may not hold. Notice that our example follows Holmstrom (1979)
and the subsequent literature in assuming additive separability:

Example 2. There are three e¤orts, two outputs, and two signals: E = f1; 2; 3g ;
X = f0; 1g; and S = f0; 1g. Let K (1) = K (2) = K (3) = 1, G (1) = G (2) = 0; and
G (3) = 1. Thus, e = 1 and e = 2 both cost zero and e = 3 costs one. The reservation
utility is U = 0.
    Conditional on e = 3, states are uniformly distributed: p3x;s = 41 8 x; s: For e 2
f1; 2g, the conditional probabilities are:
                            1                 1                                 5
             p11;0 = p21;1 = ; p11;1 = p21;0 = ; p10;0 = p10;1 = p20;0 = p20;1 = :
                            4                 8                                 16
   9
    Since the agent’s preferences over e¤orts are independent of income (Assumption (1iii)), e¤ort e
can be implemented with the minimum constant wage we if and only if it can be implemented with
any other wage w we .

                                                 8
Note that the likelihood ratios between any two e¤orts are not constant:

           p31;1           p31;0        p31;1           p31;0         p21;1       1   p21;0
                 = 1 6
                     = 2 =       ;            = 2 6
                                                  = 1 =       ;             = 2 6
                                                                                =   =       :
           p21;1           p21;0        p11;1           p11;0         p11;1       2   p11;0

Let e = 3 be the e¤ort to be implemented. The principal’s program is

                          min h(u1;0 ) + h(u1;1 ) + h(u0;0 ) + h(u0;1 )
                         fux;s g


subject to the IR and the two ICs:
             u1;0 + u1;1 + u0;0 + u0;1
                                                 1         0
                         4
             u1;0 + u1;1 + u0;0 + u0;1                     u1;0 u1;1    5
                                                 1             +     +    (u0;0 + u0;1 )
                         4                                  4    8     16
             u1;0 + u1;1 + u0;0 + u0;1                     u1;1 u1;0    5
                                                 1             +     +    (u0;0 + u0;1 ) :
                         4                                  4    8     16
Rewrite the constraints as

                               u1;0 + u1;1 + u0;0 + u0;1              4                          (9)
                                      2u1;1      (u0;0 + u0;1 )       16                        (10)
                                      2u1;0      (u0;0 + u0;1 )       16:                       (11)

The solution must entail u0;0 = u0;1 since, if they were di¤erent, replacing them
both by u0;0 +u 2
                    0;1
                         would keep all constraints unchanged and reduce the objective func-
tion (by convexity of h). The solution must also entail u1;0 = u1;1 . To see this, let
(u0;0 ; u0;1 ; u1;0 ; u1;1 ) be a solution and consider the vector that replaces u1;0 and u1;1
by their average u1;0 +u     2
                               1;1
                                   . Since the original vector was a solution, it satis…ed the IR
(equation (9)) and both ICs (equations (10) and (11)). Since the new vector gives the
same expected utility, it also satis…es the IR (9). Moreover, taking the average between
(10) and (11) establishes that the new vector is also incentive compatible:

                                   u1;0 + u1;1       (u0;0 + u0;1 )   16:

Thus, even though the likelihood ratio is not constant for all e¤orts, the signal has zero
value: ux;0 = ux;1 for x 2 f0; 1g.

   The intuition for the failure of the informativeness principle is as follows. For e = 2,
the likelihood ratio at state (1; 0) is twice as large as at (1; 1). To relax the second IC

                                                       9
(11), we should increase u1;0 and decrease u1;1 . For e = 1, the likelihood ratio at state
(1; 1) is twice as large as at (1; 0). To relax the …rst IC (10), we should increase u1;1
                                                      p3       p3
and decrease u1;0 . Since both the likelihood ratios p21;0 and p1;1
                                                                 1   and the costs of e¤orts
                                                        1;0      1;1
1 and 3 coincide, the shadow prices of both ICs are the same. Thus, the bene…t from
relaxing one IC is exactly the same as the cost from tightening the other one. As a
result, it is optimal not to make the agent’s utility depend on the signal.
    This result requires that the shadow prices of the binding ICs exactly coincide.
If we perturb either the probabilities or the utility function slightly, the bene…t from
relaxing each constraint will di¤er. We can then improve the contract by increasing
utility in the state with the highest likelihood ratio under the e¤ort associated with the
IC with the highest shadow cost. This intuition suggests that counterexamples such as
the one in Example 2 are non-generic. We now establish that this is indeed the case.
    To establish results that can be applied to settings with additive and multiplicative
separability, we hold either K or G …xed in our economy parametrization. Therefore,
we refer to an economy as either a vector of parameters (K(e); pes;x )s;x;e (which holds
G(e) …xed), or a vector of parameters (G(e); pes;x )s;x;e ) (which holds K(e) …xed). Our
results still hold if we parametrize an economy by K, G, and p. However, in this case,
economies with additive or multiplicative separability are non-generic.
    Theorem 1 is the main result of our paper. It states that, generically, signals that
are informative about deviations to all e¤orts have positive value:

Theorem 1. (Generalized Informativeness Principle) Let e be an e¤ort for which the
…rst best is not feasible. For all economies except for a set of Lebesgue measure zero,
   pe       pe 0
if pex;s 6= px;s
             e   for all e, then the signal has positive value.
    x;s     x;s0




4         Conclusion
This paper shows that the informativeness principle may not hold when the …rst-order
approach is violated. We establish a generalization that gives su¢ cient conditions for
a signal to have positive value and is generically true. Our generalized informativeness
principle requires the signal to a¤ect the likelihood ratio between the implemented
e¤ort and all other e¤orts, and that the e¤ort cannot be implemented with a constant
payment. Our results hold for both separable and non-separable utility functions.



                                            10
References
 [1] Amershi, Amin H. and John S. Hughes (1989): “Multiple signals, statistical suf-
     …ciency, and Pareto orderings of best agency contracts.” RAND Journal of Eco-
     nomics 20, 102–112.

 [2] Baiman, Stanley, and Joel S. Demski (1980): “Economically optimal performance
     evaluation and control systems.” Journal of Accounting Research 18, 184–220.

 [3] Bebchuk, Lucian Arye, and Jesse M. Fried (2004): Pay Without Performance:
     The Unful…lled Promise of Executive Compensation (Harvard University Press,
     Cambridge).

 [4] Bolton, Patrick, and Mathias Dewatripont (2005): Contract Theory (MIT Press,
     Cambridge).

 [5] Conlon, John R. (2009): “Two new conditions supporting the …rst-order approach
     to multisignal princpal-agent problems.”Econometrica 77, 249–278.

 [6] Cooley, Thomas and Edward C. Prescott (1995): “Economic growth and business
     cycles” in Thomas Cooley (ed.) Frontiers in Business Cycle Research (Princeton
     University Press, Princeton).

 [7] Edmans, Alex, Xavier Gabaix, and Augustin Landier (2009): “A multiplicative
     model of optimal CEO incentives in market equilibrium.” Review of Financial
     Studies 22, 4881–4917.

 [8] Gjesdal, Frøystein (1982): “Information and incentives: the agency information
     problem.”Review of Economic Studies 49, 373–390.

 [9] Grossman, Sanford J., and Oliver D. Hart (1983): “An analysis of the principal-
     agent problem.”Econometrica 51, 7–45.

[10] Hart, Oliver and Bengt Holmstrom (1987): “The theory of contracts” in Tru-
     man F. Bewley (ed.) Advances in Economic Theory (Cambridge University Press,
     Cambridge).

[11] Holmstrom, Bengt (1979): “Moral hazard and observability.”Bell Journal of Eco-
     nomics 10, 74–91.

                                        11
[12] Holmstrom, Bengt (1982): “Moral hazard in teams.” Bell Journal of Economics
     13, 326–340.

[13] Holmstrom, Bengt and Paul R. Milgrom (1987): “Aggregation and linearity in the
     provision of intertemporal incentives.”Econometrica 55, 303–328.

[14] Jewitt, Ian (1988): “Justifying the …rst-order approach to principal-agent prob-
     lems.”Econometrica 56, 1177–1190.

[15] Ke, Rongzhu (2012): “A …xed-point method for validating the …rst-order ap-
     proach.”Working paper, Chinese University of Hong Kong.

[16] Kim, Son Ku (1995): “E¢ ciency of an information system in an agency model.”
     Econometrica 63, 89–102.

[17] Lazear, Edward P. and Sherwin Rosen (1981): “Rank-order tournaments as opti-
     mum labor contracts.”Journal of Political Economy 89, 841–864.

[18] Rogerson, William P. (1988): “The …rst-order approach to principal-agent prob-
     lems.”Econometrica 53, 1357–1368.

[19] Shavell, Steven (1979): “Risk sharing and incentives in the principal and agent
     relationship.”Bell Journal of Economics 10, 55–73.

[20] Shleifer, Andrei (1985): “A theory of yardstick competition.” RAND Journal of
     Economics 16, 319–327.

[21] Sinclair-Desgagné, Bernard (1994): “The …rst-order approach to multi-signal
     principal-agent problems.”Econometrica 62, 459–465.




                                         12
A     Proof of Theorem 1
Throughout the proof, we will use bold letters to denote vectors. We will use the
following corollary of Sard’s Theorem:

Corollary 1. (Sard) Let X Rn and           Rp be open, F : X       ! Rm be continu-
ously di¤erentiable, and let n < m: Suppose that for all (x; ) such that F (x; ) = 0;
DF (x; ) has rank m. Then, for all except for a set of Lebesgue measure zero,
F (x; ) = 0 has no solution.

   For simplicity, suppose that only two ICs bind; it is straightforward but notationally
cumbersome to generalize the analysis for more than two binding ICs. Without loss of
generality (renumbering e¤orts if necessary), let e = 3 denote the implemented e¤ort,
and let e = 1 and e = 2 denote the two e¤orts with binding ICs. By assumption, the
…rst best is not feasible for e = 3: The principal’s program is
                                      xX X
                                      X  S
                               min               pex;s h (ux;s )                                         (12)
                               ux;s
                                      x=x1 s=1
                                          xX X
                                          X  S
  subject to       G (e ) + K (e )                    pex;s ux;s   U;                                    (13)
                                         x=x1 s=1
                                          xX X
                                          X  S                                     xX X
                                                                                   X  S
                   G (e ) + K (e )                    pex;s ux;s   G (e) + K (e)              pex;s ux;s 8e
                                         x=x1 s=1                                  x=x1 s=1
                                                                                                         (14)

   There are two possible cases depending on whether the IR (13) binds. Here, we
consider the case where it binds. The case where it does not bind is analogous and is
presented in Supplementary Appendix B.4.
   The (necessary) …rst-order condition with respect to ux;s is

            pex;s h0 (ux;s )            1
                                 1 K(1)px;s
                                                             2
                                                      2 K(2)px;s   + K (e ) pex;s = 0 8x; s:             (15)

Following the parametrization of an economy, we keep either G (G(3); G(2); G(1))
or K (K(3); K(2); K(1)) constant. Accordingly, let either = K (if G is being held
constant) or = G (if K is being held constant).
    For the agent’s payments to be independent of the signal, the system of equations
(13), (14), and (15) must have ux;s = ux as a solution for all x; s. Combining these
equations, they can be written as F (u; 1 ; 2 ; 3 ; ; p) = 0; where

                                                       13
                                  2                                                                3
                                      p31;1 h0 (u1 ) +           1
                                                          1 K(1)p1;1 +
                                                                                 2
                                                                          2 K(2)p1;1   K(3)p31;1
                                  6                                                                7
                                  6   p31;2 h0 (u1 ) +           1
                                                          1 K(1)p1;2 +
                                                                                 2
                                                                          2 K(2)p1;2   K(3)p31;2   7
                                  6                                  ..                            7
                                  6                                   .                            7
                                  6                                                                7
                                  6                                                                7
                                  6 p31;S h0 (u1 ) + 1 K(1)p11;S + 2 K(2)p21;S      K(3)p31;S      7
                                  6                                                                7
                                  6                              ..                                7
  0                           1   6                               .                                7
                                  6                                                                7
                                  6 p3 h0 (uX ) + K(1)p1 + K(2)p2                   K(3)p3X;1      7
    u ; 1 ; 2 ; ; |{z}; p A
F @|{z}                           6 X;1               1      X;1        2     X;1                  7;
        | {z }         |{z}       6 3 0                                                            7
      X             3             6 pX;2 h (uX ) + 1 K(1)p1X;2 + 2 K(2)p2X;2        K(3)p3X;2      7
             3          3XS       6                               ..                               7
                                  6                                .                               7
                                  6                                                                7
                                  6 3                                                              7
                                  6 pX;S h0 (uX ) + 1 K(1)p1X;S + 2 K(2)p2X;S       K(3)p3X;S      7
                                  6             P              P                                   7
                                  6                X                  3                            7
                                  6                x=1 ux K(3)     s px;s + G(3)  U                7
                                  6             P  X           P      2                            7
                                  4                x=1 ux K(2)       p + G(2) U                    5
                                                PX             Ps 1x;s
                                                   x=1 ux K(1)     s px;s + G(1)  U

and the terms under brackets indicate the number of elements. The remainder of the
proof veri…es that DF has full row rank so we can apply Corollary 1.
   Write the derivative of F as:
                            "                               #
                               A C      D H3 H2 H1
                     DF =                                     ;
                               B 03 3 E       J3 J2 J1

where the terms inside the matrix will be de…ned below.
   The XS X matrix A is the derivative of the …rst XS entries with respect to u
                        2                                         3
                          h00 (u1 )P31       0      :::     0
                        6                                         7
                        6       0      h00 (u2 )P32 :::     0     7
                   A=6  6       ..           ..             ..    7;
                                                                  7
                        4        .            .              .    5
                                                         00     3
                                0            0      ::: h (uX )PX
                         0
where Pex = pex;1 ; :::; pex;S . The 3 X matrix B        includes the derivatives of the last 3
equations (IR and ICs) with respect to u:
                      2                                                       3
                          K(3)P31 1S K(3)P32 1S          ::: K(3)P3X 1S
                      6                                                       7
              B = 4 K(2)P21 1S K(2)P22 1S                ::: K(2)P2S 1S       5;          (16)
                          K(1)P11 1S K(1)P12 1S          ::: K(1)P1S 1S


                                           14
where 1S     (1; 1; :::; 1) is the vector of ones with length S. The XS 3 matrix C is
the derivative of the the …rst XS equations with respect to the multipliers:
                               2                                    3
                                 K(1)p11;1 K(2)p21;1      K(3)p31;1
                               6                                    7
                               6 K(1)p11;2 K(2)p21;2      K(3)p31;2 7
                               6     ..                             7
                               6                                    7
                               6      .                             7
                               6                                    7
                               6 K(1)p11;S K(2)p21;S      K(3)p1;S 7
                                                               3
                               6                                    7
                               6      ..                            7
                         C=6           .                            7:           (17)
                               6                                    7
                               6 K(1)p1            2           3    7
                               6          X;1 K(2)pX;1   K(3)pX;1 7
                               6          1        2                7
                               6 K(1)pX;2 K(2)pX;2       K(3)p3X;2 7
                               6       ..                           7
                               6                                    7
                               4        .                           5
                                          1        2           3
                                 K(1)pX;S K(2)pX;S       K(3)pX;S

and 03 3 is a 3 3 null matrix corresponding to the derivative of the last 3 equations
(IR and ICs) with respect to the multipliers.
    The derivative of the …rst XS equations with respect to fG(3); G(2); G(1)g is the
XS 3 null matrix 0XS 3 . The derivative of the last 3 equations (IR and ICs) with
respect to fG(3); G(2); G(1)g is the 3 3 identity matrix I3 . Thus, if K is constant,
   = G, and we have D = DG = 0XS 3 , and E = EG = I3 .
    The derivative of the …rst XS equations with respect to fK(3); K(2); K(1)g is
                                2                               3
                                        p31;1    p 2
                                               2 1;1      p 1
                                                        1 1;1
                                6                               7
                                6       p31;2    p 2
                                                          p11;2 7
                                6    ..
                                               2   1;2  1       7
                                6                               7
                                6     .                         7
                                6                               7
                                6       p 3
                                                p 2
                                                         p 1    7
                                6         1;S 2 1;S    1 1;S 7
                                6     ..                        7
                         DK = 6        .                        7:
                                6                               7
                                6    p    3
                                                p 2
                                                         p 1    7
                                6         X;1 2 X;1    1 X;1 7
                                6                               7
                                6    p3X;2        2
                                              2 pX;2
                                                           1
                                                       1 pX;2 7
                                6      ..                       7
                                6                               7
                                4       .                       5
                                          3       2        1
                                     pX;S     2 pX;S   1 pX;S

The derivative of the last 3 equations with respect to fK(3); K(2); K(1)g is
                 2 PX          P 3                                        3
                       x=1 ux    p
                                s x;s         0                0
                 6                    PX        P 2                       7
          EK = 4             0          x=1 ux   s px;s        0          5:
                                                        PX       P 1
                             0                0           x=1 ux   s px;s


                                         15
Thus, if G is constant, = K, and we have D = DK , and E = EK .
    Next, we calculate the derivative with respect to the probabilities. The derivative
of the …rst XS equations with respect to (p3x;s ) is the XS XS matrix:
          2                                                                      3
            [h0 (u1 ) K(3) ] IS           0S S           :::         0S S
          6                                                                      7
          6          0S S        [h0 (u2 ) K(3) ] IS :::             0S S        7
    H3 = 66            .                    .            .             .         7;
                                                                                 7
          4            .
                       .                    .
                                            .              . .         .
                                                                       .         5
                                                               0
                     0S S                 0S S           ::: [h (uX ) K(3) ] IS
where IS is the S S identity matrix. The derivative of the last three equations with
respect to (p3x;s ) is:
                             2                           3
                               u1 K(3)1S ::: uX K(3)1S
                             6                           7
                        J3 = 4     0S    :::     0S      5;
                                   0S    :::     0S
which is a 3 XS matrix and, as before, 1S is the row vectors of ones with length S.
   Proceeding in a similar way with respect to (p2x;s ) and (p1x;s ), gives
                   2                                           3
                       2 K(2)IS     0S S     :::      0S S
                   6                                           7
                   6 0S S         2 K(2)IS :::        0S S 7
             H2 = 66       ..         ..     ..          ..    7 = 2 ISX ;
                                                               7
                   4        .          .        .         .    5
                           0S   S        0S   S        :::   2 K(2)IS
                                2                         3
                                      0S    :::     0S
                                6                         7
                           J2 = 4 u1 K(2)1S ::: uX K(2)1S 5 ;
                                      0S    :::     0S
and
                                       H1 =       1 K(1)ISX ;
                                2                         3
                                   0S      :::    0S
                             6                            7
                        J1 = 4     0S      :::    0S      5:
                               u1 K(1)1S ::: uX K(1)1S
                  "             #
                    H3 H2 H1
Note that DFP =                    has XS+3 rows and 3XS columns. Since XS+3 <
                     J3 J2 J1
3XS; it su¢ ces to show that DFP has full row rank: for any y 2 RXS+3 ;

                     y              DFP       = |{z}
                                                 0 =) y =                0
                                                                        |{z} :
                    |{z}            | {z }
                  1 (XS+3)      (XS+3) 3XS         1 3XS            1 (XS+3)


                                                  16
                  "        #
                      Hi
   Let DFPi =                  . First, expanding y              DFP2 = 0 gives:
                      Ji

                                       x1      2 K(2)   + xSX+2 u1 K(2) = 0;
                                                                          ..
                                                                           .
                                       xS      2 K(2)   + xSX+2 u1 K(2) = 0;
                                   xS+1        2 K(2)   + xSX+2 u2 K(2) = 0;
                                                                          ..
                                                                           .
                                    x2S        2 K(2)   + xSX+2 u2 K(2) = 0;
                                                                          ..
                                                                           .
                           xS(X    1)+1 2 K(2)          + xSX+2 uX K(2) = 0;
                                                                          ..
                                                                           .
                                   xSX        2 K(2)    + xSX+2 uX K(2) = 0;

which implies

              2 K(2)y1   + u1 K(2)yXS+2 =               2 K(2)y2       + u1 K(2)yXS+2
              = ::: =    2 K(2)yS    + u1 K(2)yXS+2 = 0
              2 K(2)yS+1   + u2 K(2)yXS+2 =               2 K(2)yS+2       + u2 K(2)yXS+2
              = ::: =    2 K(2)y2S     + u2 K(2)yXS+2 = 0
         ..
          .
              2 K(2)yS(X 1)+1     + uX K(2)yXS+2 =                 2 K(2)yS(X 1)+2      + uX K(2)yXS+2
              = ::: =    2 K(2)ySX      + uX K(2)yXS+2 = 0:

Dividing through by K(2) > 0 and rearranging gives:

                           2 y1    =          2 y2   = ::: =    2 yS   =   u1 yXS+2                      (18)
                         2 yS+1    =          2 yS+2   = ::: =     2 y2S   =   u2 yXS+2
                                         ..
                                          .
                  2 yS(X 1)+1      =          2 yS(X 1)+2      = ::: =     2 ySX   =   uX yXS+2 :




                                                          17
Similarly, expanding y            DFP1 = 0; yields

                  1 K(1)y1    =           1 K(1)y2        = ::: =             1 K(1)yS    =     u1 K(1)yXS+3              (19)
           1 K(1)yS+1         =           1 K(1)yS+2             = ::: =            1 K(1)y2S   =   u2 K(1)yXS+3
                                     ..
                                      .
      1 K(1)yS(X 1)+1         =           1 K(1)yS(X 1)+2                 = ::: =       1 K(1)ySX    =   uX K(1)yXS+3 :

with K(1) > 0. Recall that                 1       0 and             2        0 and at least one of them is strict. Thus,

                                           y1 = y2 = ::: = yS = y 1
                                     yS+1 = yS+2 = ::: = y2S = y 2
                                            ..
                                             .
                             yS(X         1)+1     = yS(X                1)+2   = ::: = yXS = y X :

From equation (18), we have:
                                                             1
                                                        2y       =       u1 yXS+2
                                                   ..
                                                    .                                                                     (20)
                                                             X
                                                        2y        =       uX yXS+2 :
                                                                 "          #
                                                                   C
    Second, recall that DF(                1; 2;   )    =                       ; where C is described in (17). Thus,
                                                                  03 3
y   DF(   1; 2;   )   = 0 gives
          X                                      X                                       X
                y x K(1)p1x;s = 0;                      y x K(2)p2x;s = 0;                     y x K(3)p3x;s = 0 8x:      (21)
          x;s                                    x;s                                     x;s

Multiplying both sides of the …rst equation in (21) by 2 0:
                        X                       X
                             x       1
                      2    y   K(1)p x;s = K(1)   ( 2 y x ) p1x;s = 0:                                                    (22)
                               x;s                                              x;s

However, from equation (20), we have
             X                                X
        K(1)     ( 2 y x ) p1x;s = yXS+2 K(1)   ux p1x;s =                                       yXS+2 (U    G(1));       (23)
                      x;s                                                     x;s


where the last equality follows from the IC associated with e = 1. Let G(1) 6= U (the
set of parameters for which U = G(1) have zero Lebesgue measure). Then, (22) and

                                                                         18
(23) imply yXS+2 = 0. Applying the same logic to the second equation in (21) gives
yXS+3 = 0:
   Third, recall from equations (18) and (19) that, for all x,

                             x                            x
                        2y       =    ux yXS+2 and   1y       =   ux yXS+3 :

Moreover, 1        0 and 2 0 with one of them strict. Given yXS+2 = yXS+3 = 0, it
then follows that 1 y = 2 y x = 0; which, because either 1 6= 0 or 2 6= 0, implies
                       x

y x = 0 for all x.
    Fourth, expanding y DFP3 = 0 gives:

                             y1 [h0 (u1 )    K(3) ] + ySX+1 u1 K(3) = 0;
                                                                      ..
                                                                       .
                             yS [h0 (u1 )    K(3) ] + ySX+1 u1 K(3) = 0;
                        yS+1 [h0 (u2 )       K(3) ] + ySX+1 u2 K(3) = 0;
                                                                      ..
                                                                       .
                          y2S [h0 (u2 )      K(3) ] + ySX+1 u2 K(3) = 0;
                                                                      ..
                                                                       .
                 yS(X   1)+1     [h0 (uX )   K(3) ] + ySX+1 uX K(3) = 0;
                                                                      ..
                                                                       .
                        ySX [h0 (uX )        K(3) ] + ySX+1 uX K(3) = 0:

Given that y1 = y2 =        = ySX = 0 and K(3) > 0, this implies that either u1 =
u2 =      = uX or ySX+1 = 0. However, the former is impossible: either such a
contract violates at least one IC, or it satis…es all ICs. In the latter case, a constant
wage would induce e , which was ruled out. It follows that ySX+1 = 0. Therefore,
y DFP = 0 =) y = 0; showing that DFP has full row rank.




                                                19
