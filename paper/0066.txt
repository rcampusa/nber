                     NBER WORKING PAPER SERIES




               DETECTING AND ASSESSING THE PROBLEMS
                   CAUS BY I'iJLTICOLLINEARfl?:
             A USE OF THE SINGULAR-VALUE DECOMPOSITION


                          D3vid A. Belsley*
                         Virginia C. 1<lema**


                      Working Paper No. 66




 COMPUTER RESEARCH CENTER FOR ECONOMICS AND MANAGEMENT SCIENCE
             National Bureau of Economic Research, Inc.
                      575 Technology Square
                   Cambridge, Massachusetts 02139

                           December   l97L

                  Preliminary: not for quotation



 NBER working papers are distributed infonnally and in limited
 niiibers for comments only. They should not be quoted without
 written perTIission.

 This report has not undergone the review accorded official NBER
 publications; in particular, it has not yet been submitted for
 approval by the Board of Directors.

*BER Catputer Research Center and Boston College. Research
 supported in part by National Science Foundation Grant   GJ-115'4X3
 to the National Bureau of Economic Research, Inc.
**NBER   Canputer Research Center. Research supported in part by
 National Science Foundation Grant GJ-115'4X3 to the National Bureau
 of Economic Research, Inc.
                                        Abstract


This paper presents a means for detecting the presence of nulticollinearity
and for      assessing the damage that such collinearity may cause estimated
coefficients in the standard linear regression model. The means of analysis
 is   the singular value decomposition, a nuTkerical analytic device that
directly Exposes th the conditioning of the data matrix X             and the   linear
dependencies that       may   exist among its coluTins. The same infonTation is
employed in the second part        of   the paper to detenriine the extent to .thich
each regression coefficient is being adversely affected by each linear
re3Ation among the colimins of X that lead to its ill conditioning.


                                   Acknowledgments

The authors wish to express their gratitude to Professor Gene Golub of
Stanford University, Professor John Dennis of Cornell, and          Edwin Kuh

of    the   NBER for many helpful discussions. Moreover, the first author

wishes      to express his gratitude to the Center for Advanced Studies in
the Behavorial Sciences at Stanford for the opportunity to initiate his
research     in   this area during his fellowship there.




                                                                                         .
                                    Contents

INTRODUCTION                                                               ..1
PART 1. •rI SIULAR-VAllJE DECOMPOSITION AND THE DETECTION OF
          LINEAR DEPENDENCIES

1.1 The Singular-Value Decomposition                                  ....2
1.2 The Determination of the Linear Dependencies of X .
1.3   Determination of p (X)    r                                     ....5
1.4 Determining the Structure in the       Linear Dependencies of X    •   .   12

     1.4.1 Defin:isig the Structure                                    •   .   12

      1.4.2   Determining the Zeros of G                               •   .   13

Appendix to Section 1. Scaling                                         •   .   19



PART 2. AN ASSESSMENT    OF THE DAMAGE CAUSED BY LINEAR
          DEPENDENCIES                                                         22

2.1 The Basic Decomposition of the Variance of lDb                             23

2.2 An Interpretive Consideration: Orthogonality and the Zero
     Structure of V                                                            26

     2.2.1 The Zero Structure of V When X has Orthogonal Parts                 27

     2.2.2 Near Collinearity Nullified by Near Orthogonality                   32

      2.2.3 An Example                                                         33


2.3 Assessing the Damage Caused by Collinear       Data                        36

      2.3.1 At Least Tc.x Variates Must Be Involved                            36

      2.3.2 Variance Proportions: Necessary but not Sufficient                 39

      2.3.3 A Suggested Test for Harmful Coilinearity                          41

      2.3.4    Multicollinearity as a Practical   Problem                      42


PART 3. SOME GENERAL CONSIDERATIONS ON MULTICOLLINEARI'I? AND ITS
        CORRECTIONS                                                            44

3.1 Other Tests for Multicollinearity                                          1414
      3.1.1 Sir1e Correlations
      3.1.2 The Determinant
      3.1.3 Method of Far'rar

3.2 Corrective Measures
                              of
                                   .
                                   X'X .
                                and Glauber


     3.2.1 The Introduction of Identifying Infoiiition
     3.2.2 The Failure of Ridge


REFERENCEs



                   Addenda to bibliography, p. 49
                                                                         44
                                                                         [1.5

                                                                         45

                                                                         46
                                                                         46
                                                                         47




Becker, R., Kaden, N., arid KLema, V., [1974], "The Singular   Value Analysis
in Matrix Computation", NBER Working Paper 46.

Golub, G. H., [1969], "Matrix Decomposition and Statistical Calculation",
in R. C. Milton and J. A. Nelder (eds.), Statistical Corxutation,
Academic Press 365-397.

Golub, G. H., and Kahan, W., [1965], "Calculating the Singular Values and
Pseudo-Inverse of a Matrix", J. SIAM Numer. Anal., Ser. B. Vol. 2, No. 2,
205—224.

Golub, G. H., and Reinsch, C., [1971], "Singular Value Decomposition and
Least Squares Solutions," in J. H. Wilkinson and C. Reinsch (eds.),
Handbook for Automatic Computation, Volume II: Linear Algebra, Springer
Verlag, 134-151.

Hanson, R. and Lawson, C. L., [1969], "Extensions and Applications of the
Householder Algorithm for Solving Linear Least Squares Problems,"
Mathematics of Computation, vol. 23, no. 1080, 782-812.

                                Errata


page 6. line 13     triangulation +   triangularization

         line 16    replace line 16 with from Golub and KaJan (1965)
                    and Wilkinson (1965) p. 195 illustrate this point.

page 7. line 3      posses + possesses
                                 INODUCTION




      There are three major questions related to the problem of mnJ.±icollinearity:
when does it exist? how much damage has it caused? and what, if anything, can
be done about it? Making use of a technique of numerical analysis, the singular-
va.lue decomposition, this paper suggests a means for answering the first two of
these questions that is devoid of the ad hoc quality of previous attempts.
Part 1 introduces the concept of the singular-value decomposition and applies it
to the determination of -the existence of linear dependencies among the columns
of any given data matrix X. An Appendix to Part 1 deals with the problems caused
by scalIng of the data matrix. Part 2 addresses the question of assessing the
damege caused by the presence of multicollinearity and applies the mderstanding
gained fran Part 1 toward an answer. Part 3 presents an assessment of several of
the techniques previously advanced in the literati.me fcr diagnosing collinearity
and, additionally, presents a fundamental critique against the use of non-
Baysian "ridge regression" as a means of corTecting the problems caused by
collinear data. While some contrived examples are provided for illustration,
a true study of the application of these techniques to economic data will be
the subject of a future paper.
                                            —2—



                    Part   1. The Singular-Value Decomposition and
                           The Detection of Linear Dependencies




1.1 The Singular-Value Decoirposition
         We learn from the numerical analysts1 that any TcK matrix X, considered
here to be a matrix of T observations of K economic variates, nay be decomposed
as

              X UEV'                                                                  (1.1)

where U 'U V 'V        'K and E is   diagonal   with non-negative diagonal elenEnts

crk,k_l.K.
                                                                                                   .
1 See, for exanpie, Golub (1969), Golub and Reinsch (1970), Hanson and Lawson
    (1969), and Becker     et al (1974).
2
    This decomposition     is efficiently and stably effected by a piogrern    called
    MINFIT   [Golub and   Reinsch (1970)].
    In (1) U is DcK, E is KxK and V is KxK. Mteative fonTulations are also
    possible and nay prove more suitable to other applications. Hence one may            have
             'IcK   TxT Th.K ThK
              x=u             Vt
                                                                                        (1   la)



    or       TxK    Txr' ra' ri.K                                                     (1 ib)
                         V
    there r p CX). In this latter fonnulation E is          always   of full rank, even if
    X is not.




                                                                                                   .
                                                    —3—




       The singular-value deccznposition is closely related to the familar concepts
of eigenvalues and eigenvectors, but its difference frau those concepts is inpor-
tant. The non-negative diagonal elements of E are called the singular values of
X, and these are also the non-negative square roots of the eigenvalues of X'X.
This   is readily   seen by noting


              X'X       vEu'UV'     VE2V'.                                          (1.2)

       Recalling the orthononility of V, we note that V diagonalizes X' X, and
hence the diagonal elements of E2 must be the eigenvalues of the real synmetric
imatr'ix   X'X.


     Equally clear, the orthononnal colimins of V must be the eigenvectors of
X'X, and, as is similarly denonstra.ted, the columns of U must be the eigen-
vectors of XX'.

       The singular -value decouosition does not, however, merely duplicate know-
ledge of the eigensystem of X'X, for the singular value decouosition applies
directly to the data matrix X, and not to the manent matrix X'X. The singular -
value deccnçosition thus leads to a means of detennining the linear dependencies,
if   any, among the colnris of the data matrix X.


1.2 The Detennination of the Linear Dependencies of X.
       Ass.une that X is    rank deficient, i.e., p(X) r < K.         Since U and   V are

orthononral,      and   hence   necessarily of full rank, we must have p (X) = p (E).

There wi.ll, therefore,         be as meny   zero   elements along the diagonal of as the

nullity     of X, and hence      we may   partition the   singular-value decouosition in
                                                 —4—




 (1.1) as

             x     uEv'    u   11.o1
                                                                                (1.3)
where       is r'w and nonsingular.

      After postrrultip1ying (1.3)        by   V and further partitioning we obtain

             x [V1 V2] =     Eu1
                                   u2]h1                                        (1.4)


where   V1 is Kxr            U1 is Thr'
        V2 is Kx(K-r)        U2 is Th(K-r).

(1.4) results in the to matrix equations

            X V1     U1E11                                                      (1.5)
and

            x V2 = 0.                                                           (1.6)

      Interest centers on (1.6), for it displays all of the linear dependencies
of X: the Kx(K-r) matrix V2 provides an orthonorinal basis for the nufl space
that is spanned by the columns of X.

      Two problems arise in applying the exact algebra leading to (1.6) to real
data. First, how does one determine the rank of X, r, i.e •,   are the zeros
                                                                       how

of E discovered? And second, how are the zeros of V2 discovered? Both of these
problems arise because computers use finite arithmstic, and only in very special
cases will "true" zeros be calculated as such. There are problems of both round-
ing er:ror and error in the representation of the data

1. Also sorrtimes called truncation error. However, this tern also applies to
    the error introduced by truncating an infinite series after a finite number
    of steps, and hence will not be enployed here.
                                            —6--




       The importance of the first problem is obvious: only through            a correct

determination     of the zeros of E can     we   correctly assess how many   linear depen-

dericies exist anng the columns of X.            The   importance of the second problem is
less   obvious. But, in general, all elements of V2 will be calculated as non-

zeros, however small some may      be relative to others. Since scaling         of   X will

alter these non-zero elements arbitrarily (a problem that is dealt with in

length   in the   appendix to this section), we may        arrive   at the conclusion that

many   columi-is of X enter   each linear   dependency,    whether or not this is    true.
The economnetrician will rarely be satisfied with such an answer; he would like
to identify the zeros of V2 (or some manipulation of it) so that he can say
which variates do and which variates do not enter into a specific linear
relation. The next two sect ipns deal with these two problems in turn.
1.3 Determination of (X) r
       The singular value decomposition presents a means for determining the
rank of the data matrix X. Referring to (1.1) and recalling that U and
V are orthogonal we see that has both the same norm and the same rank
as X. Since        is diagonal, were there no problems of calculation introduced
by the impr'ecis ion of the computer, one need only determine the number of
nonzero elements of E to discover the rank of X. Unfortunately the task
is not quite so sinple, for the nonexact, finite arithmetic necessarily
employed by computers and the problems of rounding error will result in
nonzero elements of E when, under ideal conditions, they should be zero.
it is necessary, therefore, to find a means for determining when an element
of E is "small enough" to be considered zero, and hence evidence of X' s
being rank deficient.
                                   -6--



      Proposed Alternatives. The singular value decomposition is useful
in this context of deteniiining rank because it preserves the norm of X
(i.e. column lengths). The singular values are in the same units as the
colins of X, and hence are measurably interpretable. Other suggested
means for detenTlining rank fail on this and other counts
     The determinant of the matrix (if square - or X'X if not) clearly
faild, for a small determinant has little to do with the invertability
of a matrix. The matrix CIn has determinant c' which can be made arbitrarily
small, yet it is clear that aI has orthogonal colurrns and is always
invertable for
     It is equally infeasible to obtain information on the invertability
(conditioning) of a matrix from the smallness of some of the diagonal elements
of a triangulation of the given matrix. This process is closely related
to the use of the determinant, since the determinant will be the product
of the diagonal elennts of the triangular factorization. Two exanples
from Golub and Reinsch (1970) and Wilkinson (1965) illustmte this point.
Consider

               .501   —l
                      .502   —l




                 0                .599    —l

                                          .60j
                                              -7..-




and




      Each of these matrices will be shown by the singular value decoirosition
to be quite ill-conditioned even though neither posses a snail diagonal
elennt.
      The Condition Number. A nans of determining the conditioning of a
matrix that avoids the pitfalls nntioned above is afforded by the singular
value decomposition. The notivation behind this technique derives from a
nore correct nEthod of determining whether an inverse of a given matrix
"blows up". As                shall see it is reasonable to consider a matrix to be
ill-conditioned if its inverse is large in spectral norm' in corrarison
with the spectral norm of the given matrix itself. TO examples aid this
point. Consider first the matrix


          A=I            I

              [clj.
Clearly as cx -'-   1,       this   matrix tends toward perfectly singularity. Also
the singular values of A are easily shown to be li-ct, and those of A1 are
(l+cx).    Now   as a + 1,          the product I IAI   I   I IAH   I
                                                                        =   urn (li-ct) (l_ci)Tl explodes,
                                                                            (2+]
arid hence we conclude the norm of A is large relative                             to that of A. A is
ill-conditioned for small cx.


  The spectral norm of A (a..), denoted IIAH, is siirply
  maximum singular value.   1]
                                                                                      .   , the
                                        —8—




          By   way of contrast, consider the matrix, introduced above,


               BI[aol   I




          There   is some feeling that B becomes ill—conditioned as cx + 0. However,
 IB   I a and I BI I = a1, and the product I IBI I 1B11 I a = 1 is
                                                          I




constant as a + 0. In this case, then, the norm of B' does not blow up
relative to that of B, arid B is well conditioned for all a.
          The conditioning of any square matrix can be smmarized, then, by a condi-
tion number K (A) defined as the product of the maximal singular value of A
                                    -l . This concept is readily extended
times the maximal singular value of A
to   a rectangular matrix and can be calculated without recourse to the inverse
matrix. From the singular value decomposition of X UEV', it is easily
                                                        +
shown that the generalized inverse  of X is UEV', where   is the generalized
 inverse of E and is simply E with its nonzero diagonal elements inverted.1
Hence the singular values of X are merely the inverses of those of X, and
the maximal sIngular value of X is the reciprocal of the minimum (nonzero)
singular value of X. We may therefore define the condition number. of X
as K(X) = 1Tax

          The Use of The Condition Number in Determining Rank.        We   will now discuss
the sense in which the condition number has meaning as a measure of the ill-
conditioning of a matrix. This will further result            in a   meaningful criterion
for determining when a singular value is small enough (relative to                   to
provide evidence of a renk deficiency.




__
1.   See       Golub and Reinsch (1970) or   Becker et al. (l97).
                                                                                              .
                                          —9—




        Consider the linear system Xb a, and suppose the data are known
exactly, but       stored in finite    precision. It is shown in Stewart (1973) or
Hanson and Lawson (1969) that a change in the last digit of the elements of
X can result in a change in K (X) times as great in the solution b. That is,
if the machine zero is io-10, and K (X) is   then a change in X in the
tenth decimal place can affect b in the 10—10 x 10L or 10 —6           place. Clearly,

then,    a condition number sufficiently large can wipe out all significance
to a solution to a linear system. Such u1d be the case if K were larger
-than the rd length of the machine.
     In a least—squares problem, the solution to X' X bX'y, a similar result
holds, except that now a perturhation in X affects X'X as the square, and we
JTU.lst have the square of the condition number to be like the word length, or,
equivalently, the condition number like the square root of the word length.
     Rather generelly, then, in the least-squares context, we would suppose
that any singular value, ak which, relative to -the         was less than the
square root of the machine zero (the reciprocal of the word 1ength-about
2_26 for IBM 360/370 long precision) to be evidence of rank deficiency.

       When there is Fuzziness in the Data. The determination of the rank of
the data matrix X is less straightforward when the data are known imprecisely-
with fuzziness. The analysis of the previous section is based on data known
exactly, and from it we learn that a perturbation in the last digit of the
data's word length can affect digits on the order of K(X) from the            last in

the    solution for b of a linear system.        Thus if   the word length is io8
K(X)   is      a    change in the eighth digit of X can affect      b in the 5th digit,

and    a K(X) of   io8   can   remove all significance from b.
                                             -10-


       When the data are fuzzy, further problems are encountered, because
relevant   perturbations     in the data now affect, not necessarily the last digit it
the   word length,   but   possibly much     higher order digits. Suppose again a
word length of io8 and      a   K Cx)   = 1O3 but the data   are known only   up to 1O3.

Now   relevant   perturbations of the data as stored in the computer are

108x105      1O3 times greater      than perturbations    of the last digit of the

word length. Hence the solution to the linear            system will   be known with

even   less precision, and      could1    be affected in the digits on the order of

KCX)X103. In this case that would be io6, leaving              only the first two digits
to be known with any accuracy.
       In the least—squares so1utions--as contrasted to the solution-to a
linear system used in the explanation above—the treatment of data fuzziness
is   quite analogous. If the data in X          are exact to, say, l0, then the data
of XIX are exact to 106. A word length of 108 now inplies that perturbations
of the order of 108x106 =         102    are now relavant, and these can in turn be
n.gnified in the least-squares solution by a factor K             CX), the   condition
number of X'X. Here, this would be C103)2xl02                108, and hence the solution
b rry have no definition at all with an 8 digit word length.




1. The word could is used because the figure is an upper bound telling the
    worst possible story. It could be better in any given case.
                                          —11-




     The preceding leads to the following suggestion for determining when
a singular value is small enough to be considered evidence of rank deficiency
when there is fuzziness       in the data. Let    w be the word       length1, and f

be the fuzziness2 in the data matrix X --        f2. -t1iat of X'X.     Then the foregoing
argues that we must have wf2K2(X) < w if the least squares solution is to
have any meaning (any stable digits) at all. That is we must have K(X) < f.
If the data are known up to 1O3, we can allow X to have K(X) = a max <
                                                                          a

                          a
Hence any ak such that        max   <   lO (f) would indicate the possibility
                          ak
of rank deficiency.




1. w can be measured as lOs", where 2. is the number of digits carried by the
    machine.
2. f can be measured as 10h, where h is the number of places known with
     exactness.
3. Provided X'X is accumulated in double precesion relative to that of X.
                                           —12—




1.4 Determining the Structure of the Linear Dependencies Of X.
        1.4.1 Defining the Structure
                                                                                             .
                 In this subsection we assume we have already detenmined the rank of
X as described in the previous subsection. Our interest here centers on deter-
mining which variates do and which do not enter any specific linear dependency.
It is this information that is meant by the term structure of the linear
dependency. It is not sufficient to examine the zero structure of V2 in (1.6)
to determine the structure of the linear dependencies, for clearly, for any
(k-r)2 nonsingular matrix A, (1.6) becomes

            X VA 0,                                                            (1.7)

and we can alter the zero structure of these linear dependencies (given by the
zeros of   the matrix   V2A) arbitrarily. Father we must rework        (1.6) into   a form
that is   invariant to linear   transformations.    This   is   accomplished by partition-

ing (1.6) to produce a "reduced form" as follows:


            x V2                   0   ,                                       (1.8)
                     [xix2][2]=
where   X1 is T x (k-r)         V21 is (k-r) x (k—r)
        X2 is T xr              V22 is r x (k-r)


and   V21 is chosen to be   nonsingular. Since V2, having orkhononnal        columns,   is

of   full rank, such   a nonsingular   subrratrix must exist. Fran (1.8)    we obtain


            X1   -XV     VXG
                   2 22 21                                                     (1.9)



where C -
                                        —13—




The structure of (1.9) is clearly invariant to linear transformations since
X V2A      0 :Lrrplies cx1x2[] A = 0   or X1   -X2V22A A1V = - X2G.        The determina-


tion of the structure of the    linear dependencies of X therefore   is   precisely

the   determination of the zero structn?e of the matrix G. flom it we learTl which
colunuis of X2 are involved in linear relationships with the variates composing
the co1iiins of X1.

        Unfoxkunately we cannot sinply calculate G and look for its zeros, for,
as already mentioned, the finite arit]-tic used in determining V2__ now further
compounded by the calculations determing G as -V22V —will not guarentee that
the zeros of G will indeed be calculated as zero.

        l.Li.2 Determining the Zeros of G.
              Io methods are suggested here for giving nmrica1 specification to
the zeros of C.' The first is a 1inear-proanming approach, the second a least-
squares approach. Both methods axe based upon the following rationale. Linear
dependencies are exact only in perfect algebra. The econoiztrician has always
sought to extend this concept to one of "near dependency", a notion that has been
more intuitive than rigorous. In the previous section, however, we saw how
"nearness" could be given rianing in a realistic contect both by the natural
fuzziness given by a "n iiine zero", and by the more usually encountered fuzziness
that results from data inaccuracies. This latter concept requires some discus-
sion.

1
    Theauthors are greatly indebted to Gene Golub of Stanford University and
    John Dennis of Cornell University for their contributiàns to these techniques.
                 ObservatIona1 Equivalence
              A published GNP figure of 1.054 triflion dollars is clearly not exact.
    Indeed all additional information regarding digits beyond 10 have been sur-
    pressed. The datim 1.054 is therefore observationaiLy indistinguishable from
    1.0542 or 1.0539. That is ,there is       some region of fuzziness such that,   given
    noimai rounding procedures, any data point lying in that region is equally valid
    for an entry into X. This concept of truncated data repoxing is quite distinct
    from errors in observation. The latter would argue that one might not know for
 sure the corl2ectness of the data actually reported. Hence observations error
 introduces yet another element of fuzziness into the degree of accuracy with
which one knows one         s data.

                In any event   there   is reason to suppose that there exists a matrix E,
determined by the investigator, that puts limits on the accuracy to which he
believes he knows his data. These limits may, for exarrple, take the form that
"coluim 6 of X         is known only up to 10". Hence, any data matrix X such that
    I   X - X    E is   observationally equivalent to     This notion of observational

equivalence       (which could no      doubt also be cast into a statistical framework)
is a data-analytic analogue to the identification problem. Given the fuzziness
in X, any results based on any X observationally equivalent to X iraist also be
indistinguishable within the degree of precision to which the data are known.
Hence the investigator must consider as observationally indistinguishable any
V resulting from the singular value decouosition of any appropriate X = UEV'..
It is this notion of observational equivalence that is exploited to determine the
zeros of G.

1
        The notation IXI here is used to mean absolute value of a matrix, not the
        determinant.
                                     —15--




          Zero Enrichment
          Given the data matrix X, we have from (1.9) that

             —
                  XG   0,                                            (1.10)

and we propose to determine the zero structure of G by determining whether any
of its elements (or specific of its elements) are observationally indistinguish-
able from (equivalent to) zero. To do thIs we employ a numeric-analytical
analogue to hypothesis testing.' It is proposed that the investigator examine
the G determined by the singular value decomposition of X and specify which of
its elements he has reason to believe to be zero. This may be based upon
a priori considerations of which variates uld not belong in certain linear
dependencies (hence inplying the corresponding elements of C to be zero) or it
may be based on experience he has regarding which values of G that are calculated
to be small numericafly are in fact zero. In any event the matrix C has, as a
rna-ter of hypothesis, certain of its elements made to be zero. The resulting
zero enriched matrix is denoted G. In lx)th of the following procedures a method
is presented to test the hypothesized zero enrichment by determining whether G
is observationally equivalent to G in the sense that G could indeed by calculated
as the G matrix for a data matrix X that is observationally equivalent to X.


     Method 1: A Linear -Programming Approach
     Let A (S) be a IK matrix to be detennined. X is the given 1d( data
matrix and E is the "limits" matrix defined above. C is the matrix defined in

1Again a statistical fornuilation of this procedure may well be possible, but is not
exaiBined here.
                                                —16—




 (1.9)   by the   singular value       decomposition of X and   for which   (1. 10) holds.

 Partion A [Lt] to correspond to X1 and X2. G is an hypothesised zero-
 enriched   rr.trIx   subject to test. We will say that G is observationally equi-

 valent to G (arid hence accept the hypothesIsed zero enrichnnt) if there exists
 a A [AA] such that G satisfies

                                                                                    (1.11)

and


             jAIE,                                                                  (1.12)

i.e., if can result from the singular-value decomposition of a data matrix
that is observationally equivalent to X.
      The existence of such a A can be established from the feasibility of a
linear progiem. From (1.11) we have

            A1 -         = - (X1   -
                                       X2G)                                         (1.13)
or
            AFT    —XH                                                             (1.14)

              JE1
where
            H1j.
Using   the change of    variable


              = A ÷ E,
                                                                                   (1.15)

the problem of finding a A that satisfies (1.14) subject to the inequalities
(1.12) is equivalent to finding the that satisfies

                   (E-X)H          subject to                                      (1.16)
                                            -17—




              j2E a                                                            (1.17)



     The existence of such a ' (q) is clearly established if there exists a
feasible solution to the contrived linear pxoam

            mm t                K
                                    EE
                                    tic
                                          , a vector of n ones]                (1.18)



subject   to (1.16 and 1.17).
       It is   rth errhasizing that it      Is not necessary to solve the 12 (1.18)

to accept the hypothesis of the zero        erxriched G, rather it is   only required to

dnonstrate      the feasibility of the    program.

      Method 2: A Least-Squares (minimum norm) Approach
       The 12 given above will, even for moderate sized econcznic problems, be
large. Even the demonstration of a feasible solution could prove costly, and,
hence, a second method appears worthy of consideration.
       Our problem is to find a satisfying (1. 14) also obeys the inequalities
(1.12).   Since H in (1.14) necessarily has full rank, we can find all , satisfy-
ing this relation without regard to (1.12) (in general there will be an infinity
of them) by considering all

                    —   XEIFI                                                  (1.19)

where H is any pseudoinverse of H. Among all these solutions, however, is one
with minimum norm (i.e., a E with minimum               ), which is found by using the
genere1ized inverse H , i.e.

                =                                                             (1.20)

      There  is, of coia'se, no guarantee that £ will satisfy (1.12) in all cases,
but   there is reason to hope that its property of minimum norm will indeed also
                                         —18-




result in (1.12) as a px.ctica1 matter. This second method of determining t,,
then, is sufioient brt not necessary to accept the zero ern'ichnnt hypothesis.
That is, a solution to (1.20) that also satisfies (1.12) accepts the observational
equivalence     of G (the hypothesized zero   enrichment), but   a solution to (1.20)

that   does   not also satisfj (1.12) does not mean that   a   solution to the 12 (1.18)

does not   exi1       The advantage of this   technique over the 12   is that   it is
quick   and cheap to employ. If it works, no further effort        is required. If it

doesn't, further    investigation iray be warranted. It will be a metter for
experience to determine just how well this short cut works in practice.




1. We are indebted to our colleague, Paul Holland, for highlighting these points.
                                                   —19—




                               APPENDIX '10 SECUON 1. SCALING



          The seemingly elaborate test procedures given in the previous section axe
nDtivated by the fact that the elements of G are scale sensitive and can be
made arbitrarily small sInly by a choice of scale. Detmination of the zero
structure of G, therefore, requires some meaningful (not arbitrary) measure of
snJl, and this measure is afforded by the procedures outlined.
          The purpose of this appendix is to deuonslte this probletnful scale
sensitivity.
          Let   X be the    data matrix in     "original units",        arid let D =
                                                                                       diag(d1
                                                                                                 ...
                                                                                                       d
be    a   scaling matrix (all d1            0). Call the scaled data matrix               Xi). Now

(using      the notation of the text)          the SVD of   X is

                XUZV',inplyingXV2=O                                                          (1.21)


and   that of X      is

                 A   A A                     A A
                XUEV', implyingXV2                  0.



The reduced fonns corresponding to the original and                      scaled   data are therefore

                                  _i   —                           —i
                      —X2    V22 V21 = X2 G,         G —V22 V21                             (1.23a)

and                              A_i       A A       A      A A_                            (1 23b)
                X1    —X2 V22 V2i          X2 G,     G = —V22   Vi

arid the     econometrician must insist that the zero structure of G be the same                       as
G, since arbitrary scaling             cannot affect the real      linear dependencies.
                                                             —20—




        We    will now show that with exact arithmetic, these zero structures are
 indeed the same, but that they can be made to appear different due to finite
                                                                                                                           .
 arithmetic, hence necessitating the test procedures of Section 1.4.2.
         From X V2               0 we may write

                   X D D1 V
                          2
                            EXD' V 2                    0.                                                        (1.24)
               A                                              A
 Since       p (X)         p CX), the null space of X must              have    the same   dimension     as X, arid

 hence D1 V2 provides                   a basis    (not   orhonormal) for the null             space   of 2.

 Hence   any orrthonounal basis for thIs null space (such as V2) must be a non-
 singular transformtian                   of D'V2. Let         this be
                                                     rD1 o] rv211


                                                                                                                           .
                                               1V21
                   A
                   V2ED'V2H                  orI,I
                                               LVzJ
                                                    I    _II
                                                                  [E       D1LV2J
                                                                                                                  (1.25)


                                  1
 for   H nonsa.ngular.


         Putting (1.25) into (l.23b) gives

                   A         AA          A         A    —              —    —
                            —X2       V22 vJ    = —X2   D V22 H H V2 D
                       =    X2 D1 V22 V D1 = X2 D1 G D1                                                           (1.26)


 Comparing (1.26)             with (1.2 3b) shows


                   G D' G D1.                                                                                   (1.27)




                           seen
                                                      A   1 lvii    A
                                                                    V2 can be any ortho-
1 .It is reacti,ly
  normal basis             for
                                      froni X     [U1 U2]
                                                         f
                                  the null space of X. One
                                                           LJ  that    can therefore   derive      at least one
  such V2 from V2 by taking the QR decoosition of DV2 =
         Q      D'V2R'.
                                                                                           Q   R to produce


                                                                                                                           .
                                            —21—




     Since D2 and   D1 are both diagonal, we       have     0 if and   only if g1           0,


where   G (g) and G (g).            Hence,    in exact arithmetic scaling does not
change the zero structure of G. However in finite arithmetic it              is   clear that
any nonzero element of G rray be irade as small as desired in G by appropriate
scaling. A nonzero in G nay therefore be a zero in G        arid   vice versa within the
limiits of the machine' s   calculations.

     The solution to this problem (that the determination of linear dependencies
nay be scale—affected) is one of numrical analysis. Since there would be no
problem from scaling if we had exact calculations, we should analyze the data
matrix x in units chosen to allow for the rn.mierically most stable calculations
in light of the finite arithmetic. Column equilibration (scaling to produce
roughly equal column lengths) enjoys some usefulness in this          context     .   Conclusions
regarding the zero structure of V2 should be based on a data matrix so scaled.
Then, should the user desire infoniation on a differently scaled matrix, the
aixve detenTdned V2 with the zero structe imposed should provide the basis of
the transfo±'med structure. That is, let X be the data scaled for numerical
accuracy, and let X =   X D be the   data scaled in tenris of the user' s preferences.
Then the zero structure of the G applicable to the data in X is determined by
analysis of (1.23a). Let G be the calculated matrix, and denote G with its
"zero" elements replaced by exact zeros by G*. Infonnation on    can then be
had by the analog to (1.27), namely

                D1G*D.                                                                 (1.28)
           G*


Clearly *   will have   the same   zero structure, invariant to     scale.




   1. See Van   der Sluis    (1969) and   (1970).
                                           —22—




          Part   2. An Assessment o the Dàe Caused by Linear tpendencies



         In this part we address the second njor question set out in the opening
paragraph, namely, how much damage is caused to the regression estiiiiates due to
the presence of linear dependencies (near dependencies) in the data matrix. It
is well known that any such dage manifests itself in mstable regression
coefficients and in inflated sampling variances. But it has not been possible
quickly to deteunine whether the size of any specific sanpling variance was
large because of collinear data or because of inherent noise (arising, for
example, because the given variate does not belong in the hypothesized relation-
ship). The former problem is potentially corctable through additional informa-
tion that mIght take the form of new noncollinear data, a prior distribution
'or the regssion parameters, or outside estintes for specific coefficients.
The analysis presented here helps to determine whether collinear data is in fact
a cause of inflated sampling variance, and further it helps to highlight which
regression estima,tes are being nDst adversely affected - thereby keying where
corTective measures are nvist profitably employed.
         In Section 1, the decomposition of the sampling variance that forms the
basis of the analysis is presented. Section 2 presents a theoretical result that
helps to interpret possible outcomes of the decomposition. Section 3 examines
the procedures suggested in Section 1 for assessing the danage caused to regres-
sion     estimates from   the use of collinear1 data.

1
    It should be highlighted that the term collinear here means rank deficient
    in the sense of Part 1 and does not mean the existence of an exact linear
    dependency; nor, obviously is it the common but loose usage in econanetrics.
                                                 —23—




2.1 The Basic Decomposition of the Variance of bb.

        The singular value deconposition of a data matrix X, as we saw in Part 1
of    this paper produces a set of singular values that can be associated with
potential linear dependencies in the data. The rd "potential" is used because
(as per SectIon 1.3) it must first be deternined, through machine and data con-
siderations, which sIngular values are small, and for each of these there is a
linear dependency to be identified. As any one singular value, then, gets
snail relative to     there is a near dependency to be associated with that
singular value.
        The basis for the analysis presented here is the deconposition of the
variances of the regression coefficients into cononents that are associated
with the singular values of X and hence are directly related to the specific
linear dependencies possesed by X. A derivation of this variance decorrosition
using   eigensysterns of X'X due to Silvey (1969)          is given in Johnston (1972), but
we rederive the result      here   using   the   singular-value decompostion to highlight

the correspondence of the components to                     values,
                                                   the singular             and   hence   the

linear   dependencies, of X    (not   of the niount matrix X'X).
       The   variance-covariance matrix of the least squares estimator b (X'XY'X'y
is,   of course,


             Var (b)   a(X'X)1                                                              (2.1)

where   a2 is the corrupn variance of the conponents of           the T   disturbances c in
y x $fc. Making use of       the   singular value       decomposition of X


             KX
                    TXK d( d(
                       UE     VT    with           diag (a ..                               (2.2)
                                                                a1?, and V(v1)
                                                             —24—




 we    may rewrite (2.1)          as (recalling            U' U I)
                                                                                                                     .
                   Var(b)                                                                                (2.3)


 or,    for the )c-th component            of b,


                var(bk)
                               a2 E                .                                                     (2.4)


 (2. LI.),   it will be noticed, decomposes var(bk) into a sum of components each
containing         the   square   of   one of the singular            values, a.    We recall   from Section
1.3 how, for each linear               dependency of           X, some a becomes    small. Since these
are    In the      denominator      in (2.4),          other   things equal, those components     of var(b)
associated      with a linear          dependency          (with small   a) will be   large   relative   to

the other      components. This suggests,                    then,   that an unusually high proportion of
the variance        of   one or irore coefficients concentrated in components                   associated
with    a specific       singular      value    gives evidence that        the   corresponding linear

dependency may be           causing problems.              This suggestion     is pursued   in Section   2.3 after

some interpretive considerations are developed in Section 2.2.

        It is a relatively easy matter                     to display   these proportions for all
                                                                                                     var(b)
so   that    the   investigator can tell at a glance                  where   problems may be arising

        Define

                          v.                           K
                                                                     k 1 .. K
               1kj a                   '
                                           1k                kj                                          (2.5)


and

                                     k,j1...K.
Then   all    infonnation      is   sunlii3rized by th tables
                                       —25—




                             Variance-Components Table
                                (all entries x 2)
                                   Components of


                   var(b1)           var(b2)
                                                         .
                                                             var(b)
                    1111                                      k1
                    11                 n
                         12                22                  2

       .
       r                 .

       49                .                                            (2.6a)
       C)
       0C))


              aK         1K                2K                 KK


arid
                                            —26—



                               Variance—proportions table

                                     Coriponents of
                             var(b1) var(b2)    ...   var(b<)

                              11       12       •..
                              21       22

                                                                                       (2.6b)

                               .

                I

An example of these tables is given in Sections 2.2.4 and       2.3.3      below.




2.2 An Interpretive Consideration:          Ozkhogonality and the   Zero   Structure of   V.

        It will be necessary -to gain much practical experience with the decouçosition

(2.4)    before reasonable   guidelines can be established for its use       as   a diagnostic

tool. There     is,   however, one uiimediate consideration that    can   be given a

rigorous    foundation, namely, that If in (2.4) some  are zero, then it makes
                                                   v,
no difference to var (1) If the corTespondlng    are very small, i.e •, the
coefficient will be inirmne from collinearity associated with those particular
singular values. This section examines the conditions under which certain of
the v1 will be zero (or small relative to the corresponding a) and hence
develops conditions under which certain regression coefficients need not be
adversely affected by the presence of multicollinear data. We can anticipate
this result by recalling the well known fact that the addition to a regression
equation of a variate that is orthogonal to all previous variates will not affect
the regression calculations based only on the original variates. Clearly then,
it should also not affect any regressIon calculations to add a set of variates
                                               —27.-




    that  are orthogonal to all previous vai,tes whether or not this additjonal
    set itself contains with it a perfectly - colliriear relationship.
           Indeed, through a series of telescoping theorems of increasing generality,
    we arrive at sufficient condition on X (and its singular values) under which
    orthogonal partitions of X ixrly specific V..' s to be zero in the singular
    value decomposition of X. These are approxinate conditions, then, under which
    regression estin.tes may possibly be salvaged even in the presence of strongly
    collinear data. Special computational algorithms are required to exploit this
    possibility, however, for mDst reession proanis are incapable of dealing with
    collinear data no matter how it occurs, and hence can make no attempt to identify
    and salvage any coefficients that need not be adversely affected.1
           In the rest of this section four theorems axe proved that show the condi-
    tions under which orthogonal blocks In the data matrix X imply specific v.. 's
    to be zero • 2 The reader not interested In the proofs to these theorems is
    advised to read Theorems 2 and L for gist and continue to the next section.

           2.2.1 The Zero Structure of V when X hs Orthogonal Parts
                 Let us begin with a Th}( data matrix X partitioned into two ortho-
    gonaJ. blocks X1 ('IK1) and X2 (K2) with X1' X2          0. In this    case we can   determine

    the   singular   values   of X by determining them   separately   for X1 and X2   Indeedx




    1A set of calculations      that proceed
                                       correctly in the presence of perfectly
    collmear data axe given in Belsley (l97'). These algorithms form the basis
    of the NBER Computer Research Center's GREMLIN system - a
                                                              comprehensive package
     for esta.nating sinLiltaneous systems available through the Center' s time sharing
    network.
    21t should be emphasized that these are sufficient, but not necessary conditions.
     Indeed there may well be other conditions leading to v. . 's being zero - and
)    these too would lead to coefficients isolated fran col±?tear relationships.
                                                —28—




the SVD of X is
                                                                                                      .
         XUEV'                                                                               (2.7)

while those of X1 and X2 are

          X1        U1 E VI          where UU1 VV 'K1 E1 = diag.                    matrix
                                                                                             (2.8)
                -   U2   EV2                  U2U2 - V2V2      I< E2                matrix


It is clear that the matrix V derived from (2.8)               as


                    rv1       0-1
                    I                                                                        (2.9)
                    10        V2



is orthogonal and has the property of diagonalizing X' X

          -                   /v'   o\ /'x1       0    '\ Iv     0   '\       /E 0
                                                                          =                  (2.10)
          V'(X'X)V (\Q1             v2J (01       xx2) [o v2)                 (o   z2 j




Hence the matrix

           -.             o
                =                                                                            (2.11)
                    0     E2)

must be the matrix of singular values of X.
Since these values are unique they mist be the same e1nents as E in

 (2.7) - although the order is not unique. We have shown
                                                       —29—




Theorem    1.
        Let X         (X1 X2) with X X2           0. Then   the   singular    values   of X niy   be
determined          directly from    the     separate SVD     of X.1    U.E.V!, il,2.
                                                                           3-3-i




This result can be used to show that orthogonality anong sets of columns of X
implies a certain zero structure on the elennts of V in (2.7), and hence on
certain relevant v.. in the numerator of the variance decomposition (2 .14)• We
begin with

Theoren 2.

        Let X         [X1 X2]   with X' X2        0. Then, if the singular values of
X    are distinct, the natrix V              in   the SVD of X =    UEV'   has the form [viol
where V.
       1
         is K.xlK..  ii                                                                    L   V2J



Proof:         The SVD of X. is as in (2.8), and because of Theorem 1, we
Cdfl   write    E    as




Now
                                         0
                (X'X)           (            T<
                                                  I = vv
                                           22)
and    one V that clearly works is                V
                                                       -1
                                                              ol
                                                                    .   But   since   the columns
                                                       0
                                                              V2j
of the V. are the eigenvectors of       the distinctness of the singular
values guarentees the uniqueness of the V1 (up to permutations arid a
                                          —30—




multiplier of nDdulus 1). Hence V is unique up to permutations within its
first K1 coluns and its last K2 colunu-is - which clearly will not alter the
zero structure


                                                                                 QED



        The condition in Theorem 2 that the singular values be distinct is over.-
strong for the purpose at hand. Problems in guarenteeing the desired zero struc—
ture occ.xr only when there are multiple roots in camion between E 1and Z2' overlap
of roots. The following exairple demons-b:'ates this. Let

                                                                r
               x   [X1X2]   I


                            Lo     o:o
                                     :I           that x'x = -— --:— . -
                                                                [jo ooJ
The matrix

                   1   0    0 ol
                   o
          V=
                   o   --L of
                   0
                        ooj
is   easily shown to be orthogonal arid   diagonalize   X'X, but it clearly does not

possess the desired zero structure. Even here, however, there        is   a V matrix

-that   does   possess the desired structure, namely V=I, but such a structure   is
not guarenteed.
                                                     —31—




        If, however, there are multiple roots tha.t do not overlap X1 and X2 (are
not in conun to           and E2) the desired zero structure is assured. This is
seen by assinnIng otherwise, i.e., assume

                   [vi'      v*1
              v—              121

                   [v vj
in any other orthogonal       V such that           x' x v* 2 Since the          and E2 have no

overlap, the non-uniqueness of V                  (beyond permutations   of columns) can occur

only up to linear combinations with its first K1 columns and within its last
K2 columns. Linear combinations aoss these two sets of columns are not
possible. But we already know that["] is a basis for the renge space of the
                             rol                   LOJ
first   '<1   columns, and
                             [j     a.   basis for the last K2 columns. Hence
sible linear combinations must preserve the zero structure. We have proved
                                                                                    any   permis-



Theorem 3.
     If in Theorem 2          and        2   have   no values in connon (however eat the
multiplicities within each), then V in the SVD of X retains the zero structure
shown there.
     The assi..uitions behind Theorem 3 are too s-txong, but they nay be weakened
to produce a useful result, nanely.

Theorem 4.
     Let X        [x1x2] with XX2 =           0   and let    be the ]<±h singular value of
(kth element of E2). Then,                   2k is distinct fixm all other        (in both
arid E2), regardless of any other             multiplicities or overlaps, V =
                                                                                   (v) in the
SVD of X has the property that
                                                       —32—




             V,K+k_
                1
                    P            for jl, ...,


i.e •,   the ffrst K1 elements of           the K1 +k column          of V   are zero.


Proof       Beyond permutations ,the K1+]<±h column of V is uniquely determined up
to a linear combination of the eigenvectors associated with the value
Since this value is assumed distinct, there is only a one dimensional space
associated with It, and we know that this space is spanned by the K1+kth
column of V =       J
                        V1 0 ,   which   clearly has the required zero.
                    L0YzJ

         2.2.2 Nearcollinearity Nullified By Near Orthogonality
                  Theorem 4 has the generality required to analyze the variance
decomposition (2. Li). Let us assume, in the eth'eme, that X has two oikhogonal
parts X1    and X2 and      that X1 is well conditioned but X2 is ill conditioned.
This means that the elements of                      are roighly of the same magnitude but that
there are some elements of E2 that are relatively small. Break up the sum
(2.4) into its first K1          terms   and it last K2 terms as
                             K v2.              K1     v2.          1<2 vic,Ki+j
             var(bk) =       E   _!a
                            j=l cr2         j=l 2 jl
                                                E      isa.

                                                        1
                                                            J
                                                                +
                                                                          ...
                                                                           2
                                                                                            (2.12)




The ill conditioning of X means that some a2 will be small - indeed zero
                                           J
if X2 Is perfectly collinear. Let thas    be       Now Theorem 4 guarantees
                                                                      2p
that for k 1 ...           K,., v2
                                 k,K1-I-p
                                            =       0, and hence the term


              2
                  K +p
                   21
                                               —33—




for k 1 ...        K1..   That is, var(bk) is unaffected by near collinearity for
k 1 ...      K .    These   estin.tes are salvaged in the presence of collinearity due
to orthogonàlity of Xi from X2. Of eater generality, however, one clearly
need not assume X1 strictly orthogonal to X2. Since the V.. 's are continuous
functions of the óoluiins of X, as the blocks of X become more nearly orthogonal
(their :inner' products get closer to zero) the relevant elements of V also go to
zero in the Limit. Hence some v can be snafl if the data axe pleasantly well
behaved. That Is, the adverse effects of near collinearity in one block of
data, X2 (as measured by sane small Ozj's) can be mitigated in the estimates of
the coefficients corresponding to another block of data, Xi, as these two blocks
are the more nearly orthogonal (as measured by small V]'S, k K1+l ... K).


     2.2.3    An Examle
               An exanple of the preceding result is useful here. We wifl consider
the matrix
                                 —71.1.   80   18' —56         —112
                                   14 —69      21:     52       1014
             X =                   66 —72      —5'    7614     1528
                                 —12      66 —30 '4096   8192                                   (2.13)
                                     3     8   —7—13276 —26552
                                     4 —12      4 8421 16842


 This matrix, essentially due to Bauer          (1971),      has   the   property   that its   fifth

 column   is exactly twice its fourth, and both of these                 are orthogonal to the

 first   three columns. That is, X2 is singular and XIX2Z 0.
                The preceding theorems tell us the following about the                   and V
matrices that result from the singular value deconosition of X: unless there
are mnmltiplicities of roots (which, as a practical matter will occur with
                                                    _3L1._




probability zero), 1) one of the singular values associated with X2 will be zero
                                                                                           12
                                                                                                                                .
(i.e., within the machine toler8nce of zero), and 2) in V                                             v          0
                                                                                                          12
and V21      0.                                                                     IYzi   v2j
     Application        of the progrmn NINZLT1 to obtain the singular value decomposition
of X results in:

                                                                                                               (2 iLl.)


          0.54786Ll.D      00   —.625347D        00     0.5556850          00' 0.148362D        —18   —.543l83D           —14
          —.835930D        00   0.383313D        00     0.392800D          001 0.2l56l8D        —19   —.470435D
          0.3263Ll.2D     —01   0.6797l5D        00     0.732750D          001 0.l58ll3D        —18   —.729449D           —14

          —.642653D      —15    —.216297D       —15     0.913326D                —.'t47214D      00   0.894427D
                                                                                                                           00
          0.321L1.23D    —15    0.108174D       —15    -.456672D          —14' —.894427D         00   —.447214D            00



                                                                                                                                .
                                                                             I




arid the following diagonal elennts of E

                           0.170701D       03

                   a2 = 0.605332D          02
                   a3 = 0.760190D          01                                                              (2.15)
                         = 0.36368L1.D     05

                   a5 =    0.l3ll59D      —11


     A glance at V       verifies     that the off-diagonal block pardtions are indeed
small —   all of the magnitude of 10 or smaller - arid well within the effective
zero of                                         2
          the computational prei                      Only somewhat less obvious is that one                         of

the a associated with x2         is   zero. Actually a5            is of    the order      of   iO_11,     and




1Golub and   Reinsch (1970), and         Becker,      et a]-.   (1974).
210 n the     IBM 67 in     double    precision.
                                                 —35—




would   seem to be non-zero, but the relevant conparison1 is the order of magnitude
                                                         6
of the scale-free value k , which, in this case, is 10_i                         The practical
                            umax
results are thus in full accord with theory, and we can now exantine the effects
of the perfectly collinear data matrix on the estimated variances of the regres-
sion paranters    b = (X'X)1X'y.
        It is clear that any problem in the calculation of Var(bk) in (2.4) for
this    particular case will arise because of the very small (1g. However, (15,
small as it is, is several orders of magnitude larger than its corvesponding
                                                   v2
v.. for i1, 2, 3. Hence the contributions of the i.5 corronents to calcula-
tions    of Var(b1),   Var(b2) and Var(b3) in (2.4)          will    be small. That is,     the

presence of ptu'e rrnilticollinearity        will not      significantly upset the precision

with    which we can   estinate the coefficients of other variates provided                 these
other variates are reasonably isolated from the offending collinear variables
through near orthogonality.
        To denonstrate this point, we calculate the relative cononents of var( b)
by maans of (2.4).                                 2
                                         25 V
                        Var(b*1)
                                            jl
                                        c E 1j
                                                   2
                                                                 2                     _2
         ci (.0010 +   .0107 + .5343    +   0.0 + .0017) 10          = a2    (.5488 x 10 ). (2.16)

        It is clear from (2.4)     that     the cononent of var(b) affected adversely
                                                                            2
by the collinearity,      nanly     -       , is   small   (.0017 x 10 ) relative      to the total
                                    5



Professor Golub shows any k having the property that                             I jT,
where c is the effective machine zero, is considered evidence of rank deficiency.
 [Golub arid Reinsch (1970)].
                                               —36—




(.51488 x 102). Indeed, it is only through            the    finite arithmetic of the machine

that this term has any definition, for it, in theory, is an i.n-ideterrnined ratio
of zeros. In practice, there is reason to cast out this component in actual
calculations of var(b).
      The preceding is in stark contrast to the calculation of var(b) or var(b),
for these are the variances of coefficients that correspond to variables involved
in the singularity of X. Indeed
                              5    v2
            var(b*)      c2         5j   cy2    (0.0 + 0.0   + 0.0 + .0000 + 1.1626 x 1023)1.
                              jl   2
                                                                                       (2.17)
      This variance is obviously huge and completely dominated by the last tern
and its role in causing the singularity of X.


2.3 Assessing the Damage Caused by Collinear Data.
       2.3.1 At Least Two Variates I&.st Be Involved
                The theorems and example of the preceding section help to put
meaning to the variance components and proportions suumarized in tables like
(2.6 a and b). At first it might seem that the concentration of the variance
of any one regression coefficient (var(bK)) in any one of its compoents
      (j    1      k) signals the fact that multicollinearity may be causing
 kj
problems.       But it is clear from Theorem 14 that if collinearity (ill conditioning)



The difference between 0.0 and .0000 in these expressions is designed to
differentiate between a number within the machine's zero (0.0), and a nonzero
number with highly negative exponent (.0000). The 0.0 's in (2.17), for example,
are of the order of io° ,          while the .0000 is of the order 10'°.
                                                 —37—




is causing problems, nre than one variance must be adversely affected by
variance components associated with a single singular value. This is seen
from the following example.
     Suppose the data matrix X consists of K mutually orthogonal coli.mnis, and
the singular values satisfy the conditions of Theorem '-I. (as they will with
probability 1). Theorem 4 immediately implies that the V matrix of the singular
valua decomposition of X is of the form'


                   F"                        I
                           22          0
                   I
           v=

                   L
Hence   only the           terms in (2.5) will be non-zero, and (2. 6b) will take the form

                         Proportions in
                       var         var
                           (b1)        (I)
                                  '
                       1

           .j
            •U)    k        0          1


1
 While V has been made diagonal here, Theorem 4 insists only that it have one
 non-zero eleiient in each row and column. V is unique only up to column pennuta-
 tions and a multiplier of nodules 1. This, of course, does not affect the cal-
 culations of (2 . it) or (2.5) since the cr permute in a compensating nnner and
 since the
             vj5 are squared arid unique despite the multiplier of modulus 1.
                                                —38—




It is clear that a high      proportion    of each variance associated with a single

singular   value is hardly indicative of multicollinearity, for        the variance

proportions here are for an ideally conditioned, orthogonal data matrix.
Indeed, problems can arise only when a single singular value              is associated
with a large proportion of the variance Df two or more coefficients. This
sisnply reflects the fact that there must be two or more columns of X involved
in any linear dependency.
     We know by Theorem       that each of the columns, k, of V      involved in such
a linear dependency must necessarily have a nonzero Vkj associated with the
small singular value                                                      must, there-
                        a.    The ratio of these vkj to the small

fore, loom large in the calculation of the variances var (bk) by (2J) for
those coefficients corresponding to the collinear (nearly collinear) variates.
If, for example, in a case of K =         5, columns L and   5 are collinear and all
other columns are mutually orthogonal we would expect a variance-proportions
table like (2. 6b) that has the form, say

                                     Proporations in
                             var var var var var
                              (b1)   (b2) (b3) (bk) (b5)

                    F


                        2l0           1
                                      0
                                            0

                                            1
                                                   0

                        c3Jo                       0

                  •3F   cijo          0     0      1
                                                       •9J
                                      0     0      0




Here cL plays a large role in both var(b1) and
                                                        var(b5)
                                        —39—




     2.3.2 Variance Proportions : Necessary but not Sufficient
            We have learned from the foregoing that near collinearity (ill
conditioning) will manifest itself as high proportions for two or more variances
in components associated with a single singular value . Unfortunately, for the
purposes of testing, the converse does not hold; such a pattern of high pro-
portions need not imply the existence of collinearity. Whereas several variances
may have most of their weight in a component associated with the same singular
value, the overall magnitude of the variance may be pleasantly low--near collirt-
earity, if it exists at all, causes no problem. The variance proportions table,
then, is merely a quick means of telling whether collinearity may be problemful,
but once the pattern of high proportions is detected, one must turn to the actual
variance components in Table (2.6a) to tell whether the overall levels are high.
An example will serve to make this clear.
     Let us return to the ndified Bauer matrix of Section 2.2.3. This five
column matrix, we recall, has the property that column 4 is exactly twice
column 5, and these two coluna-is are orthogonal to columns 1, 2 and 3. We would
fully expect that the siiall singular value      (   .1312 x 1&)associated with
the linear dependency X1   . 5X5   would daninate several variances--at least
var(b) arid var (b5). The variance proportions table (2 .6b) for the modified
Bauer matrix is given below in Table 1, and a glance at the bottom row verifies
that (35 does indeed account for the entirety of these two variances (the first
three variances are isolated fran this relationship by the orthogonality of the
first three columns of X from the last two).

1. It should be noted in passing that the existence of collinearity in X may
    not produce practically hannful problems in estates of a linear model
    relating y to X, as in y X+c. Such problems also depend upon the size
    of     (which also enters in Var (b)). This point is dealt with below in
   greater detail in section 2.3..
                                              —Lo—




                                              TABLE    1
                        Variance Proportions - Modified Bauer Matrix

              Var(b1)           Var(b2)          Var(b3)           Var(b)               Var(b5)
               .002              .009                .000              .000              .000
               .019              .015                .013              .000              .000

(13
               .976              .972                .983              .000              .000
               .000              .000                .000              .000              .000

(15
               .003              .005                .003          1.000             1.000


      A somewhat unexpected pattern, however, is also apparent: The single
singular value   (13 accounts for 97%      or more of var(b1), var(b2) and var(b3).
It may well be      the case that   a   second linear relationship among the cohnns

of X, one associated with (13 is accounting for these high proportLons. But

two facts would tend to discount this possibility. First, the three columns

X1, X2 and
                that   could   be involved in such a relationship1 (X and
                                                                                   X5
                                                                                        are

orthogonal)   are   reasonably well     conditioned; and     second,   in spite of the con-

centrated   variance proportions, the overall magnitudes of var(b1), var(b2) and
var (b3) are small. This latter fact is seen fran the actual variance components
for the modified Bauer matrix given in Table            2.



1.
 Prom Theorem 1 we know that the singular values for the matrix X1 which is
 comprised of the first three columns of the rnodifed Bauer matrix X are pre-
 cisely the same as o, 2 and a for the modified Baier 9triX itself.                    Hence,
 the condition number of X1 is K(X1)                         .171 x 10        22.5, a number
                                                             .76 xlO
                                                mm
 quite low relative to most matrices of economic data.
                                           TABLE 2

                                   Variance   -   Components

                                   Modified Bauer Matrix
                                            xa2
         Var (b1)         Var(b2)             Var(b3)           Var(b4)              Var (b5)

a      .103 x   l0      .240 x    10       .366 x              .142 x              .354 x


a2     .107 x            .401 x   l0       .126 x l0           .128 x              .319 x

a3     .534 X   ia2      .267     io2      .929                .144     io_29      .361 x   i030
a,     .166 x   i046     .351 x   io_48     .189 x             .151 x   l0         .604 x   10
a5     .172 x   l0       .129 x             .309 x l0          .465 x               .116    i024


Sum   =.548 x io_2       .275 x io_2        .945 io2           .465     io24        .116    io24




     In order to get the actual variances and variance components, each of the
                                           2
figures of Table 2 must be rruitiplied by a , the variance of the error term in
the linear model y X +            e.   But, at least on a relative basis, it is clear
that the high proportions associated with a5 are reflecting massive sizes for
                                          2    24
          and                the order of a x 10 ,        while   tl-se associated with a3
var(b4)         var(b5)-on
                                                      2
reflect    smaller variances on the order of a x 10—2 .               Whether   this latter
                                                                     2
figure    is small   in fact depends,     of course, on the size of a



       2.3.3 A Suggested Test for Harmful Collinearity

                High variance proportions, then, in themselves are not sufficient

to   reveal the existence of harmful ôbllinearity--for, as the preceding example
shows, the high proportions may not be associated with a singular value that
has been determined to be small enough (in the sense of Section 1.3) to indicate
rank deficiency. Such is the case with             the high proportions associated with            a.
 5
a ,    however,     has been      determined   to be   associated with   a linear dependency,
arid   its high variance proportions indicate collinearity to be harmful.
        It is suggested here, then, that an appropriate means for detecting
harmful      collinearity is the double condition of
        1) high variance proportions for two              or   more variances associated with
        2) a single singular          value    determined by the methods       of   Section 1.3 to
              be small and hence evidence of rank deficiency.


       2.3.        Multicollineari-ty as a Practical Problem
                   Whether   multicollinearity turns       out   to be a problem of practical
consequence        is a different question from that addressed above. It will be noted

that   the    test for harmful collinearity suggested above wholly ignores the error

variance
               2
              a that also enters the relation Var(b) =a (X XY .
                                                                    2, 1
                                                                                    Indeed,   the

terms cancel from the variance proportions                     of (2 . 6b),   but they are a factor

in each of the entries of (2.6a). It is possible, then, that collinearity

resulting in high variance proportions 4, and indeed high components                                can
                              2                                                     2
be rratigated by low a ,           for,   from (2.14) and (2.5), var (bk)       a k where
       K
k j1jk In such a case, the actual variances may be small enough to allow

acceptance of all desired tests of hypothesis, in spite of the fact that the

precision of the least squares estimates would be better in the absence of ill-

conditioned data. In other words, the presence of multicollinearity as deter-

mined here, need not be problemful as a practical matter.1 The test suggested

1 Another view of this point is useful. It will be noted that the entire
  analysis of collinearity presented here is based on the data matrix X in
  the linear regression model y X + c and no where requires knowledge of y.
 This is because ill conditioning, and the instability of calculations arid
 estimates that result from it, has only to do with X, and one would be
 better off with a nicely conditioned X matrix whether or not the ill con-
 ditioning is bad enough to cause practical problems. It is the latter
 point that depends upon y, for only through the introduction of y can a2
 be estimated in order to determine if the overall levels of the estimated
 variances are too high for conducting desired hypothesis tests. If they
 are, and ill conditioning can be determined as a problem, then corrective
 action      is rthwhile.
here, however, highlights when estimated variances are being adversely affected
(whether to a point of being problemful or not), and hence indicates when and
where such variances could be improved should the need arise through the intro-
duàtion of additional inforution that "breaks up" the ill conditioning. This
point will be discussed further in Part 3.
                                            _LL_




              Part 3.   Some General Considerations on Multicollinearity
                                  and Its   Corrections




       It is not the purpose of this paper to suggest an answer to the third          ques-
tion raised in its introduction: that dealing with corrective measures. However,
some general remarks on multicollinearity and its correction seem called for.

Section 1 of this third part examines other tests for multicollinearity that

have been proposed. Section 2 discusses corrective procedures and presents a funda-

mental criticism of the use of non—Bayesian ridge regression as a means of correction.


3.1 Other Tests for Multicollinearity

       3.1.1 Simple Correlations

              The use of simple, pairwise correlations as a means of showing the

presence of multicollinearity has been so basically discredited that it          seems
hardly necessary to mention it. However, the technique appears to flair up anew

with   some regularity, and   seems to require constant care to keep it extinguished.

In   favor of the procedure,it must be said that    the   existence of two   variates

with correlation +1 is a clear indication of multicollinearity and         therefore    it
would seem that "high" correlation would be problemful. But a correlation of .9
need not result in any real problem of estimation. The test is, therefore,
without proper interpretation, for there is no well defined notion of "high".
Conversely,   low correlations are no indication of the absence of multicollinearity,
for three or more variates may be perfectly collinear but have low pairwise
correlations.    Examination of the correlation matrix,     therefore,   offers, at
worst, erroneous and, at best, misleading information.
                                                  _Lt 5—




       3.1.2       The Determinant   of   X' X


                  Another discredited test for multicollinearity is the value of

det XtX.         Since X singular inplies det     X'X      0,   the   motivation is clearly that

low det X 'X indicates near singularity. The problem with this notion comes from
the fact that nonsingularity-singularity is not a contini.mi. This is readily
seen by considering the obviously nonsingular nm matrix A aI                       for   U>0. Clearly
the determinant of A (         c11) may    be made as small as        desired by choosing c

sufficiently        snail, but equally clearly A is        always perfectly invertable.



       3 • 1.3    Method of Farrar and Glauber
                  Farrar and Glauber (1967) suggest determining the presence of multi-
collinearity based upon a statistical test of the hypothesis that the columns of X
are in fact orthogonal. A rejection of the hypothesis leads to the alternative
hypothesis that the columns of X are nonorthogonal, arid hence collinear. There
are several weaic-iesses with this approach, both theoretical and applied.
                  1)   Th€. FarTar and Glauber approach is based on the assumption that
the X data resulted fran sane stochastic process whose orthogonality is subject
to test. If the X data are properly assumed as nonstochastic, however, (as they
are in the classicial linear model) the Farrar-Glauber analysis is irrelevant.
                  2)   If the X data are     assumed stochastic, the previous consideration
does   not apply, but it     is   still doubtful    that the Farrar-Glauber technique is        proper.
To   see this one must realize ti-at multicollinearity is a condition when sdme
linear canbina-tion of the data are observationally indistinguishable from zero,
and as such multicollinearity is seen to be a special case of the identification
problem. As is well )<nown, identification is a problem logically preceding, arid
not a part of, the statistical problem of estimation. Multicollinearity, then,
is not an estimation problem and is not properly treated as such.


                3) As a practical matter the test against the null hypothesis of
orthogonality seems to lack power; that is, it indicates nonorthogonality
very often when     there is no real    problem (all coefficients are alive,      well arid

with   strong   t' s).   This   practica1   problem is not surprising in light of the

general   inappropriateness of       the technique. Haitovsky    (1968)   attempts   to over-
cane   this practical problem of Far'rar and Glauber by making the test against the
null hypothesis of singularity. Haitovsky' s procedure, however, falls prey to
the same criticisms advanced above.


3.2 Crrective Measures
       3.2.1 The Introduction of Identifying Information
                The recognition above that multicollinearii:y is an identification
problem has implications not only for the proper way to test for it, but also
for the proper way to correct it. A multicollinear data set results in an
unidentified equation. As is well known1,            it requires the addition of new,
independent information to identify an unidentified equation. As we shall see

below, the use of ridge         regression   as has been suggested by some fails to add

identifying information and, indeed, fails to remove the estimation problem that

results from colliriear data. Two methods          have been suggested, however, that can


1
    See Fisher (1966).
                                                _Ll7_




properly introduce additional information, and hence stand as appropriate correc-
tive measures. These are the time-honored methods of using outside estimates
(such as cathining estimates of coefficients in a time-series equation previously
estimated from cross-sectional data), and the method of using a Bayesian prior
for the coefficients. The former method has the practical weaa'iess that it is
very difficult to find "outside" conditions that are appropriate to obtain
estimates for the given situation. A marginal propensity to consume, for
example, determined from cross-sectional budget studies has dubious relevance
to a time-series estimated consumption function. The second method, proposed
in Zeilner      (1971) and   Learner (1973),   has   much promise.


        3.2.2   The Failure of Ridge

                Attempts   have been    irade recently to utilize ridge regression to miti-
gate the effects of multicollinearity) Short of a meanìs of combining this
procedure with some method of        bringing    in legitimate   identifying   information ,2
however, this method is doard           to failure--merely substituting    collinearity in

the data for a degenerate distribution of the estimated coefficients.

        We begin with the usual normal equations for least squares

(3.1)            X'X   b X'y
and we assume X to be rank        deficient. The suggested ridge solution         is to create

an invertable matrix by constructing and solving the             ridge equation

(3.2)            (X'X +   k)b*    X'y
where Q    is   some positive definite matrix--often taken as I, and        b*   is the ridge



1
    See, for example, Bushnell and Huettner (1973), Hoerl and Kennard (1970).
2
    Such, for example, as is done by Holland (1973) in which he caribines ridge
    with a Bayesian prior.
estimator. k arid Q are taken so that (X'X + kQYa                         does exist---arid the        pre-

sumption    is that b* is          now solvable and uniquely so as

(3.3)             b (X'X + kQ)X'y
Unfortunately,      this trick does not solve the problem                  for    it is readily shown

that    Var(b*)    is singular, i.e., b* has          a   degenerate distribution arid            is   no more

amenable    to   proper     hypothesis    testing than      is   the nonuniquely defined OLS esti-

niator b from      (3.1).
        To see this, note that,          since X     is rank deficient,         there exists a non-

trivial y         0 such that Xy         0.    Hence (3.2) becomes

(3.')                (X'X + kQ)b*         Xy     0

or
(3.5)               C'b*       0


where               C'       (X'X + kQ)

        Clearly    C depends only on X (k fixed), arid             hence       remains   fixed   in repeated

samplings.        (3.5)   therefore implies a          fixed     linear   restriction on        the ridge

estimates b*, and         renders     them degenerately distributed.1

        This   exercise serves to highlight the             point   made above regarding the             need

for   identifying information. In multicollinearity,                      as   strongly as anywhere else,
you cannot get something for nothing. There is something about rnulticollinearity
that brings out the alchemist in econometric:Lans, but there is no way one can
squeeze, stamp or club more out of the data than was there in                            the   first place.
If several variates are all giving the same information, yoi cannot make them
speak differently simply by looking at them from a different angle. Only through
the addition of new, independent identifying information can the confounded effects
of collinear data be undone.

1. Again, combining ridge with                a Bayesian prior as in            Holland (1973)     solves
    this problem.
                                         —49—




                                    REFERENCES



Beisley, D.A. [1974], "Estimation of Systems of Sinui1taneus Equations, arid
  Cont:ationa1 Specifications of GREMLThP', Annals of !oonomic and Social
   Measurement, October.
Bushnefl, R.C. and D.A. Huettner [1973], "Multicollinearity, Orthogonality
   and Ridge Regression Analysis", Unpublished mimso, presented December 1973
   Meetings of Econometric Society, N.Y.
Farr, D.E. and R.R. Glauber [1967], "Multicollinearity in Regression Analysis:
  The Problem Revisited", Review of Economonics and Statistics, February,
   pp. 92—107.
Fisher,   F.M. [1969], The Identification Problem.

Haitovsky,   Yoek [1969], "Multicollinearity in Regression Analysis: Comnnt",
   Review of     Economics and Statistics,   November, pp. 486—489.
Hoerl, A.E. and R.W. Kennard [1970a],, "Ridge         Regression: Biased Estimation
   for Nonorthogonal Problems", Technome        trios, No. 1,   pp. 55-68.

Hoerl, A. E. andR. W. Kennaxd [1970b], "Ridge Regression: Applications to
   Nonorthogonal Problems", Technometric8, No. , pp. 69-82.

Holland, P.W. 1973], "Weighted Ridge Regression:          Combining Ridge    and Ibust
  Regression Methods", NBER CRC Working Paper No.         11.

Leaner,   E.E. [1973], "Multicollinearity: A Bayesian       Interpretation", Review
   of Economics and Statistics, August,      pp. 371-380.
Silvey,   S.D. [1969], "Multicollinearity and Inrecise Estimation", Journal
   of   the Royal Statistical Society, Series B, Vol. 31, pp. 539—552.

Stewart:, G.W.    [1973], Introduction   to Matrix Conrputations.

Van der Sluis, A. [1969], "Condition, Equilibration and Pivoting in Linear
   Algebraic Systems", Numeriache Mathematik, 15., pp. 74-88.

Wilkinson, J.H. [1965], The Algebraic      Eigenvalue Problem.

Zellner, A. [1971], An Introduction to Bayesian Inference in Econometrics.
