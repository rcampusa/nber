                               NBER WORKING PAPER SERIES




                 MEASUREMENT ERRORS IN INVESTMENT EQUATIONS

                                         Heitor Almeida
                                        Murillo Campello
                                       Antonio F. Galvao Jr.

                                       Working Paper 15951
                               http://www.nber.org/papers/w15951


                     NATIONAL BUREAU OF ECONOMIC RESEARCH
                              1050 Massachusetts Avenue
                                Cambridge, MA 02138
                                     April 2010




Corresponding author: Murillo Campello, 4039 BIF, MC-520, 515 East Gregory Drive, Champaign,
IL 61820. E-mail: campello@uiuc.edu. We thank two anonymous referees, Malcolm Baker, Maurice
Bun, John Graham, Charles Hadlock, Josh Rauh, Tom Wansbeek, Michael Weisbach, Ivo Welch and
Toni Whited for their comments and suggestions, as well as participants at seminars at the University
of Illinois and the University of Wisconsin. Marco Aurelio Rocha, Fabricio D'Almeida and Quoc Nguyen
provided excellent research assistance. The views expressed herein are those of the authors and do
not necessarily reflect the views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2010 by Heitor Almeida, Murillo Campello, and Antonio F. Galvao Jr.. All rights reserved. Short
sections of text, not to exceed two paragraphs, may be quoted without explicit permission provided
that full credit, including © notice, is given to the source.
Measurement Errors in Investment Equations
Heitor Almeida, Murillo Campello, and Antonio F. Galvao Jr.
NBER Working Paper No. 15951
April 2010
JEL No. G3

                                              ABSTRACT

We use Monte Carlo simulations and real data to assess the performance of alternative methods that
deal with measurement error in investment equations. Our experiments show that individual-fixed
effects, error heteroscedasticity, and data skewness severely affect the performance and reliability
of methods found in the literature. In particular, estimators that use higher-order moments are shown
to return biased coefficients for (both) mismeasured and perfectly-measured regressors. These estimators
are also very inefficient. Instrumental variables-type estimators are more robust and efficient, although
they require fairly restrictive assumptions. We estimate empirical investment models using alternative
methods. Real-world investment data contain firm-fixed effects and heteroscedasticity, causing high-order
moments estimators to deliver coefficients that are unstable across different specifications and not
economically meaningful. Instrumental variables methods yield estimates that are robust and seem
to conform to theoretical priors. Our analysis provides guidance for dealing with the problem of measurement
error under circumstances empirical researchers are likely to find in practice.


Heitor Almeida                                       Antonio F. Galvao Jr.
University of Illinois at Urbana-Champaign           8QLYHUVLW\RI:LVFRQVLQ
515 East Gregory Drive, 4037 BIF                     Department of Economics PO Box 413
Champaign, IL, 61820                                 Bolton Hall, Room 868
and NBER                                             Milwaukee , WI 53201
halmeida@illinois.edu                                agalvao@uwm.edu

Murillo Campello
University of Illinois at Urbana Champaign
4039 BIF
515 East Gregory Drive, MC- 520
Champaign, IL 61820
and NBER
campello@illinois.edu
1        Introduction
OLS estimators are the work-horse of empirical research in corporate …nance. Researchers see a
number of advantages in these estimators. Most notably, they are easy to implement and the results
they generate are easy to replicate. Another appealing feature of OLS estimators is that they easily
accommodate the inclusion of individual (e.g., …rm and time) idiosyncratic e¤ects. Researchers have
long recognized the importance of these e¤ects in describing corporate behavior. Despite their pop-
ularity, however, OLS estimators are weak in dealing with the problem of errors-in-variables. When
the independent (right-hand side) variables of an empirical model are mismeasured, coe¢ cients esti-
mated via standard OLS are inconsistent (attenuation bias). This poses a problem since, in practice,
it is hard to think of any empirical proxies in corporate research whose measurement is not a concern.
        Concerns about measurement errors have been emphasized in the context of the model introduced
by Fazzari et al. (1988), where a …rm’s investment is regressed on a proxy for investment demand
(Tobin’s q) and cash ‡ows.1 Following Fazzari et al., investment–cash ‡ow sensitivities became a
standard metric in the literature that examines the impact of …nancing imperfections on corporate
investment (see Stein (2003)). These empirical sensitivities are also used for drawing inferences about
e¢ ciency in internal capital markets (Lamont (1997) and Shin and Stulz (1998)), the e¤ect of agency
on corporate spending (Hadlock (1998) and Bertrand and Mullainathan (2005)), the role of business
groups in capital allocation (Hoshi et al. (1991)), and the e¤ect of managerial characteristics on
corporate policies (Bertrand and Schoar (2003) and Malmendier and Tate (2005)).
        In recent years, an estimator proposed by Erickson and Whited (2000, 2002) [henceforth, EW]
has been touted as a way to tackle the issue of mismeasurement in Fazzari et al.-type models (see
Whited (2001, 2006), Colak and Whited (2007), Bakke and Whited (2008), and Riddick and Whited
(2008)). Notably, the EW estimator has almost invariably led to “reversals” of prior results in the
literature.2 At the same time, however, there has been no careful evaluation of the reliability and
usefulness of this estimator.3 The literature has little understanding of the assumptions required
by the EW estimator, the biases it engenders, and the practical limitations it faces when applied to
corporate data. At a more basic level, the literature lacks a comparison between this estimator and
better-known instrumental variables-type estimators, such as OLS-IV and GMM-IV.
        In this paper, we use Monte Carlo simulations and real data to assess the performance of alter-
native approaches that deal with the problem of mismeasurement; namely, the higher-order moment
estimator proposed by Erickson and Whited (2000, 2002) [EW], the standard instrumental variables
    1
      Theory suggests that the correct proxy for a …rm’s investment demand is captured by marginal q, but this quantity
is unobservable and researchers use instead its measurable proxy, average q. Since the two variables are not the same,
a measurement problem naturally arises (see Hayashi (1982) and Poterba (1988)).
    2
      For example, Whited (2001) uses the EW estimator to cast doubt on previous …ndings in Shin and Stulz (1998)
and Scharfstein (1999) about the ine¢ ciency of internal capital markets in conglomerates.
    3
      Numerous researchers have reported di¢ culties with the implementation of the EW estimator (e.g., Almeida and
Campello (2007), Gan (2007), Lyandres (2007), and Agca and Mozumdar (2007)). These papers do not systematically
evaluate the performance and reliability of the EW estimator as we do here.


                                                          1
approach extended by Biorn (2000) [OLS-IV], and the Arellano and Bond (1991) instrumental vari-
able estimator [AB-GMM]. Our simulations show that the EW estimator su¤ers from a number of
limitations when used in typical corporate …nance applications. In the presence of …xed e¤ects, under
heteroscedasticity, or in the absence of a very high degree of skewness in the data, the EW estima-
tor is ine¢ cient and returns biased estimates for mismeasured and perfectly-measured regressors.
Consistent with the conclusions from our simulations, subsequent empirical tests show that the EW
estimator produces inconsistent results when applied to real-world investment data. In contrast to
the EW estimator, IV-type estimators (OLS-IV and AB-GMM) easily handle individual e¤ects, het-
eroscedastic errors, and di¤erent degrees of data skewness. The IV approach, however, requires some
assumptions about the autocorrelation structure of the mismeasured regressor and the measurement
error. Accordingly, we characterize the conditions under which the IV approach yields consistent
estimates. Our simulations and empirical tests show that, if correctly used, the IV approach yields
robust, sensible results. The analysis suggests that the presence of measurement error does not
justify the use of the EW estimator in lieu of the better-understood IV methodology.
       While we provide a formal presentation in the next section, it is useful to discuss the intuition
behind the estimation approaches we analyze. Both approaches share the attractive property that
they do not require the researcher to look for instruments outside the model being considered.4
They di¤er, however, on how identi…cation is achieved. The EW estimator is based on high-order
moments of residuals obtained by “partialling out” perfectly measured regressors from the depen-
dent, observed mismeasured, and latent variables, as well as high-order moments of the innovations
of the model.5 The key idea is to create a set of auxiliary equations as a function of these moments
and cross-moments. Implementation then requires a very high degree of skewness in the distribution
of the partialled out latent variable.6 In this way, model identi…cation hinges on technical condi-
tions for high moments of unobservable quantities. While the researcher can statistically test those
conditions, they lack obvious economic intuition.
       The instrumental variable approach relies on assumptions about the serial correlation of the latent
variable and the innovations of the model (the model’s disturbances and the measurement error).
There are two conditions that must hold to ensure identi…cation. First, the true value of the mismea-
sured regressor must have some degree of autocorrelation. In the Fazzari et al. investment model, this
is akin to conjecturing that investment demand (true q) varies over time and is autocorrelated. In this
case, lags of observed q are natural candidates for the instrumental set since they contain information
   4
      Naturally, if extraneous instruments are available they can help solve the identi…cation problem. See Rauh (2006)
for the use of discontinuities in pension contributions as a source of variation in cash ‡ows in an investment model. Bond
and Cummins (2000) use information contained in …nancial analysts’forecasts to instrument for investment demand.
    5
      Iin the standard investment application, investment is the dependent variable, true q is the latent variable,
observed q is the observed mismeasured variable, and cash ‡ow is the perfectly measured regressor.
    6
      In other words, the distribution of (unobserved) true q, after being partialled out of the in‡uence of cash ‡ow,
must be highly skewed.




                                                            2
about the current value of true q.7 This condition is akin to the standard requirement that the in-
strument be correlated with the variable of interest. The other necessary condition is associated with
the exclusion restriction that is standard in IV methods and relates to the degree of serial correlation
of the innovations. A standard assumption guaranteeing identi…cation is that the measurement error
process is independently and identically distributed. This condition ensures that past values of the ob-
served variables are uncorrelated with the current value of the measurement error, validating the use
of lags of observed variables as instruments. As emphasized by Erickson and Whited (2000), however,
this assumption might be too strong since it is natural to believe that the measurement error might be
autocorrelated. On the other hand, recent research shows that one can allow for autocorrelation in the
measurement error a¤ecting q and the investment model disturbances.8 Examples in which identi…-
cation works are when autocorrelation is constant over time, or when it evolves according to a moving
average process. The …rst assumption ensures identi…cation because it means that while lagged values
of the measurement error are correlated with its current value, any past shocks to the measurement
error process do not persist over time. The moving average assumption allows for shocks to persist
over time, but it imposes restrictions on the instrumental set. In particular, as we show below, it pre-
cludes the use of shorter lags of observed variables in the instrument set, as the information contained
in short lags about true q may be correlated with the current value of the measurement error.
    We perform a series of Monte Carlo simulations to compare the performance of the EW and IV
estimators in …nite samples. Emulating the types of environments commonly found by empirical re-
searchers, we set up a panel data model with individual-…xed e¤ects and potential heteroscedasticity
in the errors. Monte Carlo experiments enable us to study those estimators in a “controlled envi-
ronment,”where we can investigate the role played by each element (or assumption) of an estimator
in evaluating its performance. Our simulations compare the EW and IV (OLS-IV and AB-GMM)
estimators in terms of bias and root mean squared error (RMSE), a standard measure of e¢ ciency.
    We consider several distributional assumptions to generate observations and errors. Experiment-
ing with multiple distributions is important because researchers often …nd a variety of distribu-
tions in real-world applications, and because one ultimately does not observe the distribution of
the mismeasurement term. Since the EW estimator is built around the notion of skewness of the
relevant distributions, we experiment with three skewed distributions (Lognormal, Chi-square, and
F -distribution), using the standard Normal (non-skewed) as a benchmark. The simulations also
allow for signi…cant correlation between mismeasured and well-measured regressors (as in Erickson
and Whited (2000, 2002)), so that the attenuation bias of the mismeasured regressor a¤ects the
coe¢ cient of the well-measured regressor.
    Our simulation results can be summarized as follows. First, we examine the identi…cation test
   7
     Lags of cash ‡ow (the well-measured variable) may also be included in the instrument set since they also contain
information about q.
   8
     See, among others, Biorn (2000), Wansbeek (2001), and Xiao et al. (2008).




                                                         3
proposed by Erickson and Whited (2002). This is a test that the data contain a su¢ ciently high
degree of skewness to allow for the identi…cation of their model. We study the power of the EW
identi…cation test by generating data that do not satisfy its null hypothesis of non-skewness. In this
case, even for the most skewed distribution (Lognormal), the test rejects the null hypothesis only
47% of the time — this is far less than desirable, given that the null is false. The power of the test
becomes even weaker after we treat the data for the presence of …xed e¤ects in the true model (“within
transformation”). In this case, the rejection rate under the Lognormal distribution drops to 43%. The
test’s power declines even further when we consider alternative skewed distributions (Chi-square and
F distributions). The upshot of this …rst set of experiments is that the EW model too often rejects
data that are generated to …t its identifying assumptions. These …ndings may help explain some of
the di¢ culties previous researchers have reported when attempting to implement the EW estimator.
      We then study the bias and e¢ ciency of the EW and IV estimators. Given that the true model
contains …xed e¤ects, it is appropriate to apply the within transformation to the data. However, be-
cause most empirical implementations of the EW estimator have used data in “level form”(that is, not
treated for the presence of …xed e¤ects),9 we also experiment with cases in which we do not apply the
within transformation. EW propose three estimators that di¤er according to the number of moments
used, which they dub GMM3, GMM4, and GMM5. We consider all of them in our experiments.
      In a …rst round of simulations, we impose error homoscedasticity. When we implement the EW
estimator with the data in level form, we …nd that the coe¢ cients returned are very biased even
when the data have a high degree of skewness (i.e., under the Lognormal case, which is EW’s pre-
ferred case). Indeed, for the mismeasured regressors the EW biases are in excess of 100% of their
true value. As should be expected, the performance of the EW estimator improves once the within
transformation is used. In the case of the Lognormal distribution, the EW estimator bias is relatively
small. Critically, as we show, any deviations from the Lognormal assumption generate large biases
under the EW framework. For every set of EW tests, we perform analogous simulations using the
OLS-IV and AB-GMM estimators. We …nd that these IV estimators dominate the EW estimator in
terms of bias and e¢ ciency under virtually all circumstances considered.
      In a second round of simulations, we allow for heteroscedasticity in the data. We focus our
attention on simulations that use data that are generated using a Lognormal distribution after ap-
plying the within transformation (the best case scenario for the EW estimator). Heteroscedasticity
introduces heterogeneity in the model and consequently in the distribution of the partialled out
dependent variable, compromising identi…cation in the EW framework. The simulations show that
the EW estimator is biased and ine¢ cient for both the mismeasured and well-measured regressors.
In fact, biases emerge even for very small amounts of heteroscedasticity, where we …nd biases of
approximately 40% for the mismeasured regressor. Paradoxically, biases “switch signs” depending
on the degree of heteroscedasticity that is allowed for in the model. For instance, for small amounts
  9
      Examples are Whited (2001, 2006), Hennessy (2004), and Colak and Whited (2007).


                                                        4
of heteroscedasticity the bias of the mismeasured regressor is negative (i.e., the coe¢ cient is biased
downwards). However, the bias turns positive for a higher degree of heteroscedasticity. Since het-
eroscedasticity is a naturally occurring phenomenon in corporate data, our simulations imply that
empirical researchers might face serious drawbacks when using the EW estimator.
       Our simulations also show that, in contrast to the EW estimator, the bias in the IV estimates is
small and insensitive to the degree of skewness and heteroscedasticity in the data. Focusing on the
OLS-IV estimator, we consider the case of time-invariant correlation in the error structure and use
the second lag of the observed mismeasured variable as an instrument for its current (di¤erenced)
value.10 We also allow the true value of the mismeasured regressor to have a moderate degree of
autocorrelation. Our results suggest that the OLS-IV estimator renders fairly unbiased estimates.
In general, that estimator is also distinctly more e¢ cient than the EW estimator.
       We also examine the OLS-IV estimator’s sensitivity to the autocorrelation structures of the
mismeasured regressor and the measurement error. First, we consider variations in the degree of
autocorrelation in the process for the true value of the mismeasured regressor. Our simulations show
that the IV bias is largely insensitive to variations in the autoregressive (AR) coe¢ cient (except for
extreme values of the AR coe¢ cient). Second, we replace the assumption of time-invariant autocor-
relation in the measurement error with a moving average (MA) structure. Our simulations show that
the OLS-IV bias remains small if one uses long enough lags of the observable variables as instruments.
In addition, provided that the instrument set contains suitably long lags, the results are robust to
variations in the degree of correlation in the MA process. As we discuss below, understanding these
features (and limitations) of the IV approach is important given that the researcher will be unable
to pin down the process followed by the measurement error process.
       To illustrate the performance of these alternative estimators on real data, in the …nal part of our
analysis we estimate empirical investment models under the EW and IV frameworks. Using data from
COMPUSTAT from 1970 to 2005, we estimate investment equations in which investment is regressed
on proxies for q and cash ‡ow. We perform standard tests to check for the presence of individual-
…xed e¤ects and heteroscedasticity in the data. In addition, we perform the EW identi…cation test
to check whether the data contain a su¢ ciently high degree of skewness.
       Theory does not pin down exact values for the expected coe¢ cients on q and cash ‡ow in an
investment model. However, two conditions would seem reasonable in practice. First, given that the
estimator is addressing measurement error in q that may be “picked up” by cash ‡ow (joint e¤ects
of attenuation bias and regressor covariance), we should expect the q coe¢ cient to go up and the
cash ‡ow coe¢ cient to go down, when compared to standard (likely biased) OLS estimates. Second,
we would expect the q and cash ‡ow coe¢ cients to be non-negative after addressing the problem
of mismeasurement. If the original q-theory of investment holds and the estimator does a good job
  10
    The results for the Arellano-Bond GMM estimator are similar to those of the OLS-IV estimator. To save space
and because the OLS-IV estimator is easier to implement, we focus on this estimator.



                                                      5
of addressing mismeasurement, then the cash ‡ow coe¢ cient would be zero. Alternatively, the cash
‡ow coe¢ cient could be positive because of …nancing frictions.11
    Our results are as follows. First, our tests reject the hypotheses that the data do not contain
…rm-…xed e¤ects and that errors are homoscedastic. Second, the EW identi…cation tests indicate that
the data fail to display su¢ ciently high skewness. These initial tests suggest that the EW estimator
is not suitable for standard investment equation applications. In fact, we …nd that, when applied to
the data, the EW estimator returns coe¢ cients for q and cash ‡ow that are highly unstable across
di¤erent years. Moreover, following the EW procedure for panel models (which comprises combining
yearly cross-sectional coe¢ cients into single estimates), we obtain estimates for q and cash ‡ow that
do not satisfy the conditions discussed above. In particular, EW estimators do not reduce the cash
‡ow coe¢ cient relative to that obtained by standard OLS, while the q coe¢ cient is never statistically
signi…cant. In addition, those estimates are not robust with respect to the number of moments used:
EW’s GMM3, GMM4, and GMM5 models procedure results that are inconsistent with one another.
    In contrast to EW, the OLS-IV procedure yields estimates that are fairly sensible. The q coe¢ -
cient goes up by a factor of 3 to 5, depending on the set of instruments used. At the same time, the
cash ‡ow coe¢ cient goes down by about two-thirds of the standard OLS value. Similar conclusions
apply to the AB-GMM estimator. We also examine the robustness of the OLS-IV to variations in the
set of instruments used in the estimation, including sets that feature only longer lags of the variables
in the model. The OLS-IV coe¢ cients remain fairly stable after such changes. In all, our empirical
tests support the idea that standard IV-type estimators are likely to dominate the EW estimator in
corporate …nance applications in which measurement error is a relevant concern.
    Prior literature has suggested alternative ways to deal with mismeasurement in investment mod-
els. Blanchard et al. (1994) and Lamont (1997), for example, use “natural experiment”-type ap-
proaches that capture variation in internal funds that is uncorrelated with investment opportunities.
These approaches allow for consistent estimates of the correlation between cash ‡ow and investment,
even in the presence of measurement error in q. However, they do not address the bias in the q
coe¢ cient itself. Issues of this kind are fully characterized in our analysis. More broadly, our paper
…ts into a new stream of studies dealing with the challenges researchers …nd in empirical corporate
work (e.g., Bertrand et al. (2004) and Petersen (2008)).
    The remainder of the paper is structured as follows. We start the next section discussing in detail
the EW estimator, clarifying the assumptions that are needed for its implementation. Subsequently,
we show how alternative IV models deal with the errors-in-variables problem. In Section 3, we use
Monte Carlo simulations to examine how the EW estimator fares when we relax the assumptions
underlying it. We also compare the performance of the EW with that of the IV estimators. In
  11
     See Hubbard (1998) and Stein (2003) for comprehensive reviews. We note that the presence of …nancing frictions
does not necessarily imply that the cash ‡ow coe¢ cient should be positive. See Gomes (2001) and Chrinko (1993) for
arguments suggesting that …nancing frictions are not su¢ cient to generate positive cash ‡ow coe¢ cients.




                                                        6
Section 4, we take our investigation to actual data, estimating investment regressions under the EW
and IV frameworks. Section 5 concludes the paper.


2      Dealing with Mismeasurement: Alternative Estimators
2.1      The Erickson-Whited Estimator

In this section we discuss the estimator proposed in companion papers by Erickson and Whited
(2000, 2002). Those authors present a two-step generalized method of moments (GMM) estimator
that exploits information contained in the high-order moments of residuals obtained from perfectly-
measured regressors (similar to Cragg (1997)). We follow EW and present the estimator using
notation of cross-section estimation. Let (yi ; zi ; xi ); i = 1; :::; n; be a sequence of observable vectors,
where xi           (xi1 ; :::; xiJ ) and zi          (1; zi1 ; :::; ziL ). Let (ui ;                       i ; "i )   be a sequence of unobservable vectors,
where     i        (   i1 ; :::;   iJ )   and "i      ("i1 ; :::; "iJ ). Consider the following model:

                                                                 y i = zi +                i           + ui ;                                                    (1)

where yi is the dependent variable, zi is a perfectly-measured regressor,                                                          i   is a mismeasured regressor,
ui is the innovation of the model, and                                (                                   0                                    0
                                                                            0;       1 ; :::;           L)      and          (    1 ; :::;   J) .   The measurement
error is assumed to be additive such that

                                                                      xi =            i   + "i ;                                                                 (2)

where xi is the observed variable and "i is the measurement error. The observed variables are yi , zi ;
and xi ; and by substituting (2) in (1) we have

                                                                 yi = zi + xi +                              i;


where     i   = ui "i . In the new regression, the observable variable xi is correlated with the innovation
term     i,   causing the coe¢ cient of interest, , to be biased.
    To compute the EW estimator it is necessary to …rst partial out the e¤ect of the well-measured
variable, zi , in (1) and (2) and rewrite the resulting expressions in terms of residual populations

                                                                 yi   zi         y   =         i       + ui                                                      (3)

                                                                 xi       zi     x   =             i   + "i ;                                                    (4)

where (       y;    x;      ) = [E(zi0 zi )]       1 E[z 0 (y ; x ;
                                                        i i i             i )]   and               i          i       zi    . One can then consider a two-
step estimation approach, where the …rst step is to substitute least squares estimates (^ y ; ^ x )
 P               P
( ni=1 zi0 zi ) 1 ni=1 zi0 (yi ; xi ) into (3) and (4) to obtain a lower dimensional errors-in-variables model.
The second step consists of estimating                            using GMM using high-order sample moments of yi                                              zi ^ y
and xi        zi ^ x . Estimates of           are then recovered via                           y       =      +       x   . Thus, the estimators are based on
equations giving the moments of yi zi                        y   and xi zi                x    as functions of                   and the moments of (ui ; "i ;   i ).


                                                                                     7
     To give a concrete example of how the EW estimator works, we explore the case of J = 1 (see
Appendix A for a more general case). By substituting
                                                                                            1 n
                                                                     P
                                                                     n                        P
                                         (^ y ; ^ x )                     zi0 zi                    zi0 (yi ; xi )
                                                                  i=1                        i=1

into (1) and (2) one can estimate , E(u2i ), E("2i ), and E( 2i ) via GMM. Estimates of the l th element
of    are obtained by substituting the estimate of                                 and the l th elements of ^ y and ^ x into

                                                        l   =        yl          xl     ; l 6= 0:

     There are three second-order moment equations:

                                                                   2               2
                                         E[(yi          zi       y) ]     =            E( 2i ) + E(u2i );                        (5)

                                         E[(yi          zi       y )(xi          zi      x )]   = E( 2i );                       (6)
                                                                    2
                                          E[(xi             zi    x) ]       = E( 2i ) + E("2i ):                                (7)

The left-hand side quantities are consistently estimable, but there are only three equations with
which to estimate four unknown parameters on the right-hand side. The third-order product mo-
ment equations, however, consist of two equations in two unknowns

                                                                2                                      2
                                    E[(yi          zi        y ) (xi             zi      x )]   =          E( 3i );              (8)

                                                                                          2
                                     E[(yi            zi        y )(xi         zi       x) ]    = E( 3i ):                       (9)

It is possible to solve these two equations for . Crucially, a solution exists if the identifying assump-
tions    6= 0 and E( 3i ) 6= 0 are true, and one can test the contrary hypothesis (that is,                               = 0 and/or
E( 3i ) = 0) by testing whether their sample counterparts are signi…cative di¤erent from zero.
     Given    , equations (5)–(7) and (9) can be solved for the remaining right-hand side quantities.
One obtains an overidenti…ed equation system by combining (5)–(9) with the fourth-order product
moment equations, which introduce only one new quantity, E( 4i ):

                                               3                                        3
                           E[(yi     zi     y ) (xi             zi    x )]    =             E( 4i ) + 3E( 2i )E(u2i );          (10)

                             2               2              2
             E[(yi   zi   y ) (xi   zi     x) ]   =             [E( 4i ) + E( 2i )E(u2i )] + E(u2i )[E( 2i ) + E("2i )];        (11)
                                                                       3
                           E[(yi     zi     y )(xi           zi      x) ]     = [E( 4i ) + 3E( 2i )E("2i )]:                    (12)

The resulting eight-equation system (5)–(12) contains the six unknowns ( , E(u2i ), E("2i ), E( 2i );
E( 3i ), E( 4i )). It is possible to estimate this vector by numerically minimizing a quadratic form that
minimizes asymptotic variance.
     The conditions imposed by EW imply restrictions on the residual moments of the observable vari-
ables. Such restrictions can be tested using the corresponding sample moments. EW also propose a

                                                                             8
test for residual moments that is based on several assumptions.12 These assumptions imply testable
restrictions on the residuals from the population regression of the dependent and proxy variables on
the perfectly-measured regressors. Accordingly, one can develop Wald-type partially-adjusted statis-
tics and asymptotic null distributions for the test. Empirically, one can use the Wald test statistic and
critical values from a Chi-square distribution to test whether the last moments are equal to zero. This
test is an identi…cation test, and if in a particular application one cannot reject the null hypothesis,
then the model is unidenti…ed and the EW estimator may not be used. We study the …nite sample
performance of this test and its sensitivity to di¤erent data generating processes in the next section.
       One should notice that the estimator proposed by Erickson and Whited (2002) was originally de-
signed for cross-sectional data. To accommodate a panel-like structure, Erickson and Whited (2000)
propose transforming the data before the estimation using the within transformation or di¤erencing.
To mimic a panel structure, the authors propose the idea of combining di¤erent cross-sectional GMM
estimates using a Minimum Distance estimator (MDE). We discuss the MDE in Appendix A.

2.2      An OLS-IV Framework

In this section, we revisit the work of Griliches and Hausman (1986) and Biorn (2000) to discuss a
class of OLS-IV estimators that can help address the errors-in-variables problem.
       Consider the following single-equation model

                                      yit =      i   +   it    + uit,     i = 1; :::; N; t = 1; :::; T;                                           (13)

where uit is independently and identically distributed, with mean zero and variance                                               2,   and Cov(
                                                                                                                                  u               it ; uis )
= Cov( i ; uit ) = 0 for any t and s, but Cov( i ;                      it )   6= 0, y is an observable scalar,                   is a 1   K vector,
and       is K    1 vector. Suppose we do not observe                            it   itself, but rather the error-ridden measure

                                                                xit =      it   + "it ;                                                           (14)

where Cov(                                                                                          2,                                    2
                 it ; "it )   = Cov( i ; "it ) = Cov(uis ; "it ) = 0, Var("it ) =                   "    Cov("it ; "it       1)   =     " ";   and " is
a1       K vector. If we have a panel data with T > 3, by substituting (14) in (13), we can take …rst
di¤erences of the data to eliminate the individual e¤ects                                 i   and obtain

                              yit   yit   1   = (xit     xit    1)   + [(uit           "it )     (uit    1   "it   1   )]:                        (15)

Because of the correlation between the mismeasured variable and the innovations, the coe¢ cient of
interest is known the be biased.
       Griliches and Hausman (1986) propose an instrumental variables approach to reduce the bias.
If the measurement error "it is i.i.d. across i and t, and x is serially correlated, then, for example,
  12
    First, the measurement errors, the equation error, and all regressors have …nite moments of su¢ ciently high order.
Second, the regression error and the measurement error must be independent of each other and of all regressors. Third,
the residuals from the population regression of the unobservable regressors on the perfectly-measured regressors must
have a nonnormal distribution.


                                                                          9
xit   2,   xit   3;   or (xit    2   xit   3)   are valid as instruments for (xit   xit   1 ).   The resulting instrumental
variables estimator is consistent even though T is …nite and N might tend to in…nity.
      As emphasized by Erickson and Whited (2000), for some applications the assumption of i.i.d.
measurement error can be seen as too strong. Nonetheless, it is possible to relax this assumption to
allow for autocorrelation in the measurement errors. While other alternatives are available, here we
follow the approach suggested by Biorn (2000).13
      Biorn (2000) relaxes the i.i.d. condition for innovations in the mismeasured equation and proposes
alternative assumptions under which consistent IV estimators of the coe¢ cient of the mismeasured
regressor exists. Under those assumptions, as we will show, one can use the lags of the variables
already included in the model as instruments. A notable point is that the consistency of these esti-
mators is robust to potential correlation between individual heterogeneity and the latent regressor.
      Formally, consider the model described in equations (14)–(13) and assume that (                        it ,   uit , "it ,     i)
are independent across individuals. For the necessary orthogonality assumptions we refer the reader
to Biorn (2000), since these are quite standard. More interesting are the assumptions about the
measurement errors and disturbances. The standard Griliches-Hausman’s assumptions are:

      (A1) E("0it "i ) = 0KK ; t 6= ,
      (A2) E(uit ui ) = 0; t 6= ,

which impose nonautocorrelation on innovations. It is possible to relax these assumptions in di¤erent
ways. For example, we can replace (A1) and (A2) with:

      (B1) E("0it "i ) = 0KK ; jt                j> ,
      (B2) E(uit ui ) = 0; jt               j> .

This set of assumptions is weaker since (B1) and (B2) allow for a vector moving average (MA) struc-
ture up to order                ( 1) for the innovations. Alternatively, one can use the following assumptions:

      (C1) E("0it "i ) is invariant to t, ; t 6= ,
      (C2) E(uit ui ) is invariant to t, ; t 6= .

Assumptions (C1) and (C2) allow for a di¤erent type of autocorrelation, more speci…cally they allow
for any amount of autocorrelation that is time invariant. Assumptions (C1) and (C2) will be satis-
…ed if the measurement errors and the disturbances have individual components, say "it = "1i + "2it ,
uit = u1i + u2it , where "1i , "2it , u1i , u2it are i.i.d. Homoscedasticity of "it and/or uit across i and t
need not be assumed; the model accommodates various forms of heteroscedasticity.
      Biorn also considers assumptions related to the distribution of the latent regressor vector                            it :

      (D1) E(         it )   is invariant to t,
      (D2) E(         i it )   is invariant to t.
  13
     A more recent paper by Xiao et al. (2008) also show how to relax the classical Griliches-Hausman assumptions
for measurement error models.


                                                                  10
Assumptions (D1) and (D2) hold when            it   is stationary. Note that    it   and i need not be uncorrelated.
    To ensure identi…cation of the slope coe¢ cient vector when panel data are available, it is necessary
to impose restrictions on the second-order moments of the variables (                 it ,   uit , "it ,   i ).   For simplicity,
Biorn assumes that this distribution is the same across individuals and that the moments are …nite.
                                P                 P                  P
More speci…cally, C( it ; it ) = t ; E( it i ) = t ; E("0it "it ) = ""                   uu     2
                                                                       t ; E(uit uit ) = t ; E( i ) =
    , where C denotes the covariance matrix operator. Then, it is possible to derive the second-order
moments of the observable variables and show that they only depend on these matrices and the co-
e¢ cient .14 In this framework, there is no need to use assumptions based on higher-order moments.
    Biorn proposes several strategies to estimate the slope parameter of interest. Under the OLS-IV
framework, he proposes estimation procedures of two kinds:

          OLS-IV A: The equation is transformed to di¤erences to remove individual heterogeneity and is
          estimated by OLS-IV. Admissible instruments for this case are the level values of the regressors
          and/or regressands for other periods.

          OLS-IV B : The equation is kept in level form and is estimated by OLS-IV. Admissible instru-
          ments for this case are di¤erenced values of the regressors and/or regressands for other periods.

    Using moment conditions from the OLS-IV framework, one can de…ne the estimators just de-
scribed. In particular, using the mean counterpart and the moment conditions, one can formally
de…ne the OLS-IV A and OLS-IV B estimators (the details are in Appendix B). For some applica-
tions, it might be useful to impose weaker conditions on the autocorrelation of measurement errors
and disturbances. In this case, it is necessary to restrict slightly further the conditions on the in-
strumental variables. More formally, if one replaces assumptions (A1) and (A2), or (C1) and (C2),
by the weaker assumptions (B1) and (B2), then it is necessary to ensure that the IV set has a lag of
at least        2, and/or lead of at least     + 1 periods of the regressor in order to “clear” the                       period
memory of the MA process. Consistency of these estimators is discussed in Biorn                            (2000).15
    To sum up, there are two simple ways to relax the standard assumption of i.i.d. measurement
errors. Under the assumption of time-invariant autocorrelation, the set of instruments can contain
the same variables used under Griliches-Hausman. For example, if one uses the OLS-IV A estima-
tor (equation in di¤erences and instruments in levels), then twice-lagged levels of the observable
variables can be used as instruments. Under a moving average structure for the innovations in the
measurement error (assumptions (B1) and (B2)), identi…cation requires the researcher to use longer
                                                 P      P                      P      P                        P
  14
P Formally,    Pone can show that: C(xit ; xi ) = t + ""    t ; E(xit ; yi ) =  t   + t ; and E(yit ; yi ) = 0 t     +
            0       0    uu
   t     +    (   )   +  t  +   :
  15
     In particular, if jt pj, j pj > , then (B1) and rank E 0ip ( it ) = K for some p 6= t 6= ensure consistency
of OLS-IV B, xp(t ) , and (B2) and the same rank condition ensure consistency of ^ yp(t ) : In the same way, if jp tj,
                ^
                                             h             i
                                                     0
jq tj > , (B1), (D1), (D2) and rank E            ipq   it )    = K for some p 6= q 6= t ensure consistency of OLS-IV B,
^       , and (B2), (D1), (D2) and the same rank condition ensure consistency of ^        :
 x(pq)t                                                                              y(pq)t




                                                          11
lags of the observable variables as instruments. For example, if the innovations follow a MA(1) struc-
ture, then consistency of the OLS-IV A estimator requires the use of instruments that are lagged
three periods and longer. Finally, identi…cation requires the latent regressor to have some degree
of autocorrelation (since lagged values are used as instruments). Our Monte Carlo simulations will
illustrate the importance of these assumptions, and will evaluate the performance of the OLS-IV
estimator under di¤erent sets of assumptions about the structure of the errors.

2.3      GMM Estimator

Within the broader instrumental variable approach, we also consider an entire class of GMM esti-
mators that deal with mismeasurement. These GMM estimators are close to the OLS-IV estimator
discussed above, but may attain appreciable gains in e¢ ciency by combining numerous orthogonality
conditions (see Biorn (2000) for a detailed discussion). GMM estimators that use all the available
lags at each period as instruments for equations in …rst-di¤erences were proposed by Holtz-Eakin,
Newey, and Rosen (1988) and Arellano and Bond (1991). We provide a brief discussion in turn.
       In the context of a standard investment model, Blundell et al. (1992) use GMM allowing for
correlated …rm-speci…c e¤ects as well as endogeneity (mismeasurement) of q. The authors use an
instrumental variables approach on a …rst-di¤erenced model in which the instruments are weighted
optimally so as to form the GMM estimator. In particular, they use qit                 2   and twice-lagged invest-
ments as instruments for the …rst-di¤erenced equation for …rm i in period t. The Blundell et al.
estimator can be seen as an application of the GMM instrumental approach proposed by Arellano
and Bond (1991), which was originally applied to a dynamic panel.
       A GMM estimator for the errors-in-variables model of equation (15) based on IV moment condi-
tions takes the form

                             ^=                                1
                                       x0 Z VN 1 Z 0 x             x0 Z VN 1 Z 0 y ;

where      x is the stacked vector of observations on the …rst di¤erence of the mismeasured variable
and      y is the stacked vector of observations on the …rst di¤erence of the dependent variable. As in
Blundell et al. (1992), the instrument matrix Z has the following form16
                                  0                                  1
                                    x1 0 0            0          0
                                  B 0 x1 x2           0          0 C
                                  B                                  C
                            Zi = B .    .    .   .    .     .    .. C :
                                  @ ..  .
                                        .    .
                                             .   .
                                                 .    .
                                                      .     .
                                                            .     . A
                                    0 0 0            x1        xT 2

According to standard GMM theory, an optimal choice of the inverse weight matrix VN is a consis-
tent estimate of the covariance matrix of the orthogonality conditions E(Zi0 vi vi0 Zi ), where                     vi
are the …rst-di¤erenced residuals of each individual. Accordingly, a one-step GMM estimator uses
  16
     In models with exogenous explanatory variables, Zi may consist of sub-matrices with the block diagonal (exploiting
all or part of the moment restrictions), concatenated to straightforward one-column instruments.


                                                          12
     P
V^ = N       0   0
        i=1 Zi DD Zi , where D is the …rst-di¤erence matrix operator. A two-step GMM estimator
                          P
uses a robust choice V~ = N i=1 Z v^i v^i0 Zi , where v^i are one-step GMM residuals.
         Biorn (2000) proposes estimation of linear, static regression equations from panel data models
with measurement errors in the regressors, showing that if the latent regressor is autocorrelated or
non-stationary, several consistent OLS-IV and GMM estimators exist, provided some structure is
imposed on the disturbances and measurement errors. He considers alternative GMM estimations
that combine all essential orthogonality conditions. The procedures are very similar to the one
described just above under non-autocorrelation in the disturbances. In particular, the required
assumptions when allowing autocorrelation in the errors are very similar to those discussed in the
previous section. For instance, when one allows for an MA( ) structure in the measurement error, for
instance, one must ensure that the variables in the IV matrix have a lead or lag of at least +1 periods
to the regressor. We brie‡y discuss the GMM estimators proposed by Biorn (2000) in Appendix C.
         Let us brie‡y contrast the OLS-IV and AB-GMM estimators. The advantages of GMM over IV
are clear: if heteroskedasticity is present, the GMM estimator is more e¢ cient than the IV estimator;
while if heteroskedasticity is not present, the GMM estimator is no worse asymptotically than the
IV. Implementing the GMM estimator, however, usually comes with a high price. The main problem,
as Hayashi (2000) points out (p. 215), concerns the estimation of the optimal weighting matrix that
is at the core of the GMM approach. This matrix is a function of fourth moments, and obtaining
reasonable estimates of fourth moments requires very large sample sizes. Problems also arise when
the number of moment conditions is high; that is, when there are “too many instruments.” This
latter problem a¤ects squarely the implementation of the AB-GMM, since it relies on large numbers
of lags (especially in long panels). The upshot is that the e¢ cient GMM estimator can have poor
small sample properties (see Baum, Scha¤er and Stillman (2003) for a discussion). These problems
are well document and remedies have been proposed by, among others, Altonji and Segal (1996) and
Doran and Schmidt (2006).


3         Monte Carlo Analysis
We use Monte Carlo simulations to assess the …nite sample performance of the EW and IV estima-
tors discussed in Section 2. Monte Carlo simulations are an ideal experimental tool because they
enable us to study those two estimators in a controlled setting, where we can assess and compare the
importance of elements that are key to estimation performance. Our simulations use several distri-
butions to generate observations. This is important because researchers will often …nd a variety of
distributions in real-world applications, and because one ultimately does not see the distribution of
the mismeasurement term. Our Monte Carlos compare the EW, OLS-IV, and AB-GMM estimators
presented in Section 2 in terms of bias and RMSE.17 We also investigate the properties of the EW
    The mean squared error (MSE) of an estimator ^ incorporates a component measuring the variability of the
    17

estimator (precision) and another measuring its bias (accuracy). An estimator with good MSE properties has small


                                                      13
identi…cation test, focusing on the empirical size and power of this test.

3.1     Monte Carlo Design

A critical feature of panel data models is the observation of multiple data points from the same
individuals over time. It is natural to consider that repeat samples are particularly useful in that
individual idiosyncrasies are likely to contain information that might in‡uence the error structure of
the data generating process.
      We consider a simple data generating process to study the …nite sample performance of the EW
and OLS-IV estimators. The response variable yit is generated according to the following model,

                                                                             0
                                           yit =       i    +        it   + zit + (1 + wit )uit ;                                       (16)

where       i    captures the individual-speci…c intercepts,                               is a scalar coe¢ cient associated with the mis-
measured variable          it ,   =(   1;    2;    3   )0   is 3      1 vector of coe¢ cients associated with the 3                 1 vector
of perfectly-measured variables zit = (zit1 ; zit2 ; zit3 ), uit is the error in the model, and                               modulates the
amount of heteroscedasticity in the model. When                                           = 0 the innovations are homoscedastic. When
  > 0 there is heteroscedasticity associated with the variable wit , and this correlation is stronger as
the coe¢ cient gets larger. The model in (16) is ‡exible enough to allow us to consider two di¤erent
variables as wit : (1) the individual speci…c intercept                                    i,   and (2) the well-measured regressor zit .
      We consider a standard additive measurement error

                                                                 xit =          it   + vit ;                                            (17)

where       it   follows an AR(1) process
                                                                (1         L)        it   =     it :                                    (18)

In all simulations we set          i; 50    = 0 and generate                         it   for t =      49; 48; :::; T , such that we drop the
…rst 50 observations. This ensures that the results are not unduly in‡uenced by the initial values of
the    it   process.
      Following Biorn (2000), we relax the assumption of i.i.d. measurement error. Our benchmark
simulations will use the assumption of time-invariant autocorrelation ((C1) and (C2)). In particular,
we assume that uit = u1i + u2it and vit = v1i + v2it . We draw all the innovations (u1i ; u2it ; v1i ; v2it )
from a Lognormal distribution; that is, we exponentiate two Normal distributions and standardize
the resulting variables to have unit variances and zero means (this follows the approach used by
EW). In Section 3.5, we analyze the alternative case in which the innovations follow a MA structure.
      The perfectly-measured regressor is generated according to

                                                                   zit =        i    + "it :                                            (19)
combined variance and bias. The MSE of ^ can be de…ned as: Var(^) + [Bias(^)]2 . The root mean squared error (RMSE)
is simply the square root of the MSE. This is an easily interpretable statistic, since it has the same unit as the estimator ^.
For an approximately unbiased estimator, the RMSE is just the square root of the variance, that is, the standard error.


                                                                            14
And the …xed e¤ects,         i   and   i,   are generated as

                                                     i    = e1i                                                                (20)
                                                                                    T
                                                                                    X
                                                                   1
                                                     i    = e2i + p                       Wit ;
                                                                    T               t=1

where Wit is the sum of the explanatory variables. Our method of generating                               i   and   i   ensures that
the usual random e¤ects estimators are inconsistent because of the correlation that exists between
the individual e¤ects and the error term or the explanatory variables. The variables (e1i ; e2i ) are
…xed as standard Normal distributions.18
       We employ four di¤erent schemes to generate the disturbances ( it ; "it ). Under Scheme 1, we
generate them under a Normal distribution, N (0;                             2 ).     Under Scheme 2, we generate them from a
                                                                             u
Lognormal distribution, LN (0;              2 ).   Under Scheme 3, we use a Chi-square with 5 degrees of freedom,
                                            u
 2.    Under Scheme 4, we generate the innovations from a Fm;n -distribution with m = 10 and n = 40.
 5
The latter three distributions are right-skewed so as to capture the key distributional assumptions
behind the EW estimator. We use the Normal (non-skewed) distribution as a benchmark.
       Naturally, in practice, one cannot determine how skewed — if at all — is the distribution of the
partially out latent variable. One of our goals is to check how this assumption a¤ects the properties
of the estimators we consider. Figure 1 provides a visual illustration of the distributions we employ.
By inspection, at least, the three skewed distributions we study appear to be plausible candidates
for the distribution governing mismeasurement, assuming EW’s prior that measurement error must
be markedly rightly-skewed.
                                                         Figure 1 about here

       As in Erickson and Whited (2002), our simulations allow for cross-sectional correlation among
the variables in the model. We do so because this correlation may aggravate the consequences of
mismeasurement of one regressor on the estimated slope coe¢ cients of the well-measured regressors.
Notably, this source of correlation is emphasized by EW in their argument that the inferences of Faz-
zari et al. (1988) are ‡awed in part due to the correlation between q and cash ‡ows. To introduce
this correlation in our application, for each period in the panel we generate ( i ; zi1 ; zi2 ; zi3 ) using
the correspondent error distribution and then multiply the resulting vector by [var( i ; zi1 ; zi2 ; zi3 )]1=2
with diagonal elements equal to 1 and o¤-diagonal elements equal to 0.5.
       In the simulations, we experiment with T = 10 and N = 1,000. We set the number of replications
to 5,000 and consider the following values for the remaining parameters:

                                               ( ;       1;   2;   3)   = (1; 1; 1; 1)

                                                                   2         2         2
                                                    = 0:6;         u   =     e1   =    e2   = 1;
  18
       Robustness checks show that the choice of a standard Normal does not in‡uence our results.



                                                                        15
where the set of slope coe¢ cients ;        i   is set similarly to EW.
       Notice that the parameter      controls the amount of autocorrelation of the latent regressor. As
explained above, this autocorrelation is an important requirement for the identi…cation of the IV
estimator. While we set        = 0:6 in the following experiments, we also conduct simulations in which
we check the robustness of the results with respect to variations in          between 0 and 1 (see Section 3.5).

3.2      The EW Identi…cation Test

We study the EW identi…cation test in a simple panel data set up. In the panel context, it is im-
portant to consider individual-…xed e¤ects. If the data contain …xed e¤ects, according to Erickson
and Whited (2000), a possible strategy is to transform the data …rst and then apply their high-order
GMM estimator. Accordingly, throughout this section our estimations consider data presented in two
forms: “level” and “within.” The …rst refers to data in their original format, without the use of any
transformation; estimations in level-form ignore the presence of …xed e¤ects.19 The second applies
the within transformation to the data — eliminating …xed e¤ects — before the model estimation.
       We …rst compute the empirical size and power of the test. Note that the null hypothesis is that
the model is incorrectly speci…ed, such that           = 0 and/or E[ 3i ] = 0. The empirical size is de…ned
as the number of rejections of the null hypothesis when the null is true — ideally, this should hover
around 5%. In our case, the empirical size is given when we draw the innovations ( it ; "it ) from a
non-skewed distribution, which is the Normal distribution since it generates E[ 3i ] = 0. The empirical
power is the number of rejections when the null hypothesis is false — ideally, this should happen
with very high probability. In the present case, the empirical power is given when we use skewed
distributions: Lognormal, Chi-square, and F -distribution.
       Our purpose is to investigate the validity of the skewness assumption once we are setting                6= 0.
Erickson and Whited (2002) also restrict every element of                 to be nonzero. We conduct a Monte
Carlo experiment to quantify the second part of this assumption. It is important to note that we can
compute the E[( i )3 ] since, in our controlled experiment, we generate             i,   and therefore observe it.
       Since the EW test is originally designed for cross-sectional data, the …rst di¢ culty the researcher
faces when implementing a panel test is aggregation. Following EW, our test is computed for each
year separately. We report the average of empirical rejections over the years.20 To illustrate the
size and power of the test for the panel data case, we set the time series dimension of the panel to
T = 10. Our tests are performed over 5; 000 samples of cross-sectional size equal to 1; 000. We use
a simple homoscedastic model with           = 0, with the other model parameters given as above.
       Table 1 reports the empirical size and power of the statistic proposed by EW for testing the null
hypothesis H0 : E(y_ i2 x_ i ) = E(y_ i x_ 2i ) = 0. This hypothesis is equivalent to testing H0 :     = 0 and/or
  19
     To our knowledge, all but one of the empirical applications of the EW model use the data in level form. In other
words, …rm-…xed e¤ects are ignored outright in panel setting estimations of parameters in‡uencing …rm behavior.
  20
     The results using the median are similar.



                                                         16
E( 3i ) = 0. Table 1 reports the frequencies at which the statistic of test is rejected at the 5% level
of signi…cance, for respectively, the Normal, Lognormal, Chi-square, and F distributions of the data
generating process. Recall, when the null hypothesis is true we have the size of the test, and when
the null is false we have the power of the test.
   The results reported in Table 1 imply an average size of approximately 5% for the test. In par-
ticular, the …rst two rows in the table show the results in the case of a Normal distribution for the
residuals (implying that we are operating under the null hypothesis). For both the level and within
cases, the empirical sizes match the target signi…cance level of 5%.
   When we move to the case of skewed distributions (Lognormal, Chi-square, and F ), the null
hypothesis is not satis…ed by design, and the number of rejections delivers the empirical power of the
test. In the case when the data is presented in levels and innovations are drawn from a Lognormal
distribution (see row 2), the test rejects about 47% of the time the null hypothesis of no skewness.
Using within data, the test rejects the null hypothesis 43% of the time. Not only are these frequen-
cies low, but comparing these results one can see that the within transformation slightly reduces the
power of the test.
   The results associated with the identi…cation test are more disappointing when we consider other
skewed distributions. For example, for the F -distribution, we obtain only 17% of rejections of the null
hypothesis in the level case, and only 28% for the within case. Similarly poor statistical properties
for the model identi…cation test are observed in the Chi-square case.

                                          Table 1 about here

Discussion

It is important that we discuss the intuition behind the results of our simulations thus far. We exam-
ine situations where we estimate a model in which one variable is plagued by mismeasurement and
the latent and observable variables have a very skewed distribution. Table 1 shows what happens to
the identi…cation test proposed by EW as we consider various degrees of skewness. For the skewed
distributions we examine, the EW identi…cation test should reject the null of non-skewness of     i,   the
partially out latent variable, with very high probability, since it is false by construction. However,
this is not what we observe. The simulations make it transparent that the EW estimator depends
on particular distributional assumptions, and even then it may fail to give right answers — it rejects
data that is generated to …t the model requirements.
   The analysis puts into question the usefulness of the EW estimator. Naturally, it is important
to understand how the EW estimator performs when it is operationalized. This is done in the next
section.




                                                   17
3.3         Bias and E¢ ciency of the EW, OLS-IV, and AB-GMM Estimators

In this section we present simulation results that assess the …nite sample performance of the estima-
tors discussed in Section 2. The simulations compare the estimators in terms of bias and e¢ ciency
under several distributional assumptions. In the next subsection we consider the cross-sectional set-
ting, focusing on the properties of the EW estimator. Subsequently, we examine the panel case in
detail, comparing the performance of the EW, OLS-IV, and AB-GMM estimators in terms of bias
and e¢ ciency.

3.3.1        The Cross-Sectional Case

We generate data using a simple model as in (16)–(17) with T = 1, such that there are no …xed
e¤ects, no autocorrelation (          = 0), and no heteroscedasticity ( = 0). The other parameters are
( ;    1;    2;   3)   = (1; 1; 1; 1). Table 2 shows the results for bias and RMSE for four di¤erent dis-
tributions: Lognormal, Chi-Square, F -distribution, and standard Normal. For each distribution we
estimate the model using three di¤erent EW estimators: EW-GMM3, EW-GMM4, and EW-GMM5.
These estimators are based on the respective third, fourth, and …fth moment conditions. By com-
bining the estimation of 4 parameters, under 4 di¤erent distributions, for all 3 EW estimators — a
total of 48 estimates — we aim at establishing robust conclusions about the bias and e¢ ciency of
the EW approach.
      Panel A of Table 2 presents the results for bias and RMSE when we use the Lognormal distri-
bution to generate innovations ( i ; "i ) that produce       i   and zi . Under this particular scenario, point
estimates are approximately unbiased, and the small RMSEs indicate that coe¢ cients are relatively
e¢ ciently estimated.
      Panels B and C of the table present the results for the Chi-square and F -distribution, respec-
tively. The experiments show that coe¢ cient estimates produced by the EW approach are generally
very biased. For example, Panel B shows that the            coe¢ cient returned for the EW-GMM4 and EW-
GMM5 estimators are biased downwards by approximately 35%. Panel C shows that for EW-GMM3
the     coe¢ cient is biased upwards about 35%. Paradoxically, for EW-GMM4 and EW-GMM5, the
coe¢ cients are biased downwards by approximately 25%. The coe¢ cients returned for the perfectly-
measured regressors are also noticeably biased. And they, too, switch bias signs in several cases.
Panels B and C show that the EW RMSEs are very high. Notably, the RMSE for EW-GMM4
under the Chi-square distribution is 12.23, and under F-distribution it is 90.91. These RMSE results
highlight the lack of e¢ ciency of the EW estimator. Finally, Panel D presents the results for the
Normal distribution case, which has zero skewness. In this case, the EW estimates are severely
biased and the RMSEs are extremely high. The estimated coe¢ cient for the mismeasured variable
using EW-GMM3 has a bias of 1.91 (about 3 times larger than its true value) and a RMSE of 2305.
      These results reveal that the EW estimators only have acceptable performance in the case of



                                                       18
very strong skewness (Lognormal distribution). They relate to the last section in highlighting the
poor identi…cation of the EW framework, even in the most basic cross-sectional set up. Crucially,
for the other skewed distributions we study, the EW estimator is signi…cantly biased for both the
mismeasured and the well-measured variables. In addition, the RMSEs are quite high, indicating
low e¢ ciency.
                                          Table 2 about here

3.3.2   The Panel Case

We argue that a major drawback of the EW estimator is its limited ability to handle individual
heterogeneity — …xed e¤ects and error heteroscedasticity — in panel data. This section compares
the impact of individual heterogeneity on the EW, OLS-IV, and AB-GMM estimators in a panel
setting. In the …rst round of experiments we assume error homoscedasticity by setting the parameter
  in (16) equal to zero. We shall later allow for changes in this parameter.
   Although the EW estimations are performed on a period-by-period basis, one generally wants
a single coe¢ cient for each of the variables in an empirical model. To combine the various (time-
speci…c) estimates, EW suggest the minimum distance estimator (MDE) described in Appendix A.
Accordingly, the results presented in this section are for the MDE that combines the estimates ob-
tained from each of the 10 time periods considered. For example, EW-GMM3 is the MDE that
combines the 10 di¤erent cross-sectional EW-GMM3 estimates in our panel.
   The OLS-IV is computed after di¤erencing the model and using the second lag of the observed
mismeasured variable x, xt   2,   as an instrument for   xt . The AB-GMM estimates (Arellano and
Bond (1991)) use all the orthogonality conditions, with all available lags of x’s as instrumental
variables. We also concatenate the well-measured variables z’s in the instruments’ matrix. The
AB-GMM estimator is also computed after di¤erencing equation (16). To highlight the gains of
these various estimators vis-à-vis the standard (biased) OLS estimator, we also report the results of
simulations for OLS models using equation in …rst-di¤erence without instruments.
   We …rst estimate the model using data in level form. While the true model contains …xed e¤ects
(and thus it is appropriate to use the within transformation), it is interesting to see what happens in
this case since most applications of the EW estimator use data in level form, and as shown previously,
the EW identi…cation test performs slightly better using data in this form.
   The results are presented in Table 3. The table makes it clear that the EW method delivers
remarkably biased results when ignoring the presence of …xed e¤ects. Panel A of Table 3 reports
the results for the model estimated with the data under strong skewness (Lognormal). In this case,
the coe¢ cients for the mismeasured regressor are very biased, with biases well in excess of 100%
of the true coe¢ cient for the EW-GMM3, EW-GMM4, and EW-GMM5 estimators. The biases for
the well-measured regressors are also very strong, all exceeding 200% of the estimates’ true value.
Panels B and C report results for models under Chi-square and F distributions, respectively. The


                                                  19
EW method continues to deliver very biased results for all of the estimates considered. For example,
the EW-GMM3 estimates that are returned for the mismeasured regressors are biased downwardly
by about 100% of their true values — those regressors are deemed irrelevant when they are not.
Estimates for the well-measured regressors are positively biased by approximately 200% — they are
in‡ated by a factor of 3. The RMSEs reported in Panels A, B, and C show that the EW method-
ology produces very ine¢ cient estimates even when one assumes pronounced skewness in the data.
Finally, Panel D reports the results for the Normal distribution. For the non-skewed data case, the
EW framework can produce estimates for the mismeasured regressor that are downwardly biased by
about 90% of their true parameter values for all models. At the same time, that estimator induces
an upward bias of larger than 200% for the well-measured regressors.

                                          Table 3 about here

   Table 4 reports results for the case in which we apply the within transformation to the data. Here,
we introduce the OLS, OLS-IV, and AB-GMM estimators. We …rst present the results associated
with the set up that is most favorable for the EW estimations, which is the Lognormal case in Panel
A. The EW estimates for the Lognormal case are relatively unbiased for the well-measured regressors
(between 4% and 7% deviation from true parameter values). The same applies for the mismeasured
regressors. Regarding the OLS-IV, Panel A shows that coe¢ cient estimates are unbiased in all mod-
els considered. AB-GMM estimates are also approximately unbiased, while standard OLS estimates
are very biased. In terms of e¢ ciency, the RMSEs of the EW-GMM3 are somewhat smaller than
those of the OLS-IV and AB-GMM for the well-measured and mismeasured regressors. However, for
the mismeasured regressor, both OLS-IV and AB-GMM have smaller RMSEs than EW-GMM4 and
EW-GMM5.
   Panel B presents the results for the Chi-square distribution. One can see that the EW yields
markedly biased estimates in this case. The bias in the mismeasured regressor is approximately 38%
(downwards), and the coe¢ cients for the well-measured variable are also biased (upwards). In con-
trast, the OLS-IV and AB-GMM estimates for both well-measured and mismeasured regressors are
approximately unbiased. In terms of e¢ ciency, as expected, the AB-GMM presents slightly smaller
RMSEs than the OLS-IV estimator. These IV estimators’ RMSEs are much smaller than those
associated with the EW estimators.
   Panels C and D show the results for the F and standard Normal distributions, respectively. The
results for the F -distribution in Panel C are essentially similar to those in Panel B: the instrumental
variables estimators are approximately unbiased and EW estimators are very biased. Finally, Panel
D shows that deviations from a strongly skewed distribution are very costly in terms of bias for
the EW estimator, since the bias for the mismeasured regressor is larger than 70%, while for the
well-measured it is around 20%. A comparison of RMSEs shows that the IV estimators are more
e¢ cient in both the F and Normal cases. In all, our simulations show that standard IV methods

                                                  20
almost universally dominate the EW estimator in terms of bias and e¢ ciency.
       We reiterate that the bias and RMSE of the IV estimators in Table 4 are all relatively invariant to
the distributional assumptions, while the EW estimators are all very sensitive to those assumptions.
In short, this happens because the EW relies on the high-order moment conditions as opposed to
the OLS and IV estimators.
                                                    Table 4 about here

3.4      Heteroscedasticity

One way in which individual heterogeneity may manifest itself in the data is via error heteroscedas-
ticity. Up to this point, we have disregarded the case in which the data has a heteroscedastic error
structure. However, most empirical applications in corporate …nance entail the use of data for which
heteroscedasticity might be relevant. It is important that we examine how the EW and the IV
estimators are a¤ected by heteroscedasticity.21
       The presence of heteroscedasticity introduces heterogeneity in the model and consequently in the
distribution of the partialled out dependent variable. This compromises identi…cation in the EW
framework. Since the EW estimator is based on equations giving the moments of (yi                           zi   y)   and
(xi      zi   ) as functions of     and moments of (ui ; "i ;       i ),   the heteroscedasticity associated with the
…xed e¤ects ( i ) or with the perfectly measured regressor (zit ) distorts the required moment con-
ditions associated with (yi         zi   y ),   yielding biased estimates. These inaccurate estimates enter the
minimum distance estimator equation and consequently produce incorrect weights for each estimate
along the time dimension. As our simulations of this section demonstrate, this leads to biased MDE
estimates, where the bias is a function of the amount of heteroscedasticity.
       We examine the biases imputed by heteroscedasticity by way of graphical analysis. The graphs
we present below are useful in that they synthesize the outputs of numerous tables and provide a
fuller visualization of the contrasts we draw between the EW and OLS-IV estimators. The graphs
depict the sensitivity of those two estimators with respect to heteroscedasticity as we perturb the
coe¢ cient      in equation (16).
       In our simulations, we alternatively set wit =         i   or wit = zit . In the …rst case, heteroscedasticity
is associated with the individual e¤ects. In the second, heteroscedasticity is associated with the the
well-measured regressor. Each of our …gures describes the biases associated with the mismeasured
and the well-measured regressors for each of the OLS-IV, EW-GMM3, EW-GMM4, and EW-GMM5
estimators.22 In order to narrow our discussion, we only present results for the highly skewed distri-
bution case (Lognormal distribution) and for data that is treated for …xed e¤ects using the within
transformation. As Section 3.3.2 shows, this is the only case in which the EW estimator returns
  21
    We focus on the OLS-IV estimator hereinafter for the purpose of comparison with the EW estimator.
  22
    Since estimation biases have the same features across all well-measured regressors of a model, we restrict attention
to the …rst well-measured regressor of each of the estimated models.




                                                            21
relatively unbiased estimators for the parameters of interest. In all the other cases (data in levels, and
for data generated by Chi-square, F, and Normal distributions), the estimates are strongly biased
even under the assumption of homoscedasticity.23
    Figure 2 presents the simulation results under the assumption that wit =                      i,   as we vary the
amount of heteroscedasticity by changing the parameter               .24   The results for the mismeasured coe¢ -
cients show that biases in the EW estimators are generally small for               equal to zero (this is the result
reported in Section 3.3.2). However, as this coe¢ cient increases, the bias quickly becomes large. For
example, for     = 0:45, the biases in the coe¢ cient of the mismeasured variable are, respectively, –11%,
–20%, and –43%, for the EW-GMM3, EW-GMM4, and EW-GMM5 estimators. Notable, those bias,
which are initially negative, turn positive for moderate values of . As heteroscedasticity increases,
some of the biases diverge to positive in…nite. The variance of the biases of the EW estimators is also
large. The results regarding the well-measured variables using EW estimators are analogous to those
for the mismeasured one. Biases are substantial even for small amounts of heteroscedasticity, they
switch signs for some level of heteroscedasticity, and their variances are large. In sharp contrast, the
same simulation exercises show that the OLS-IV estimates are approximately unbiased even under
heteroscedasticity. While the EW estimator may potentially allow for some forms of heteroskedas-
ticity, it is clear that it is not well equipped to deal with this problem in more general settings.

                                                Figure 2 about here

Discussion

Our Monte Carlo experiments show that the EW estimator has a poor handle of individual-…xed
e¤ects and that biases arise for any deviations from the assumption of strict Lognormality. Biases in
the EW framework are further magni…ed if one allows for heteroscedasticity in the data (even under
Lognormality). The biases arising from the EW framework are hard to measure and sign, ultimately
implying that it can be very di¢ cult to replicate the results one obtains under that framework.
    Overall, the results from this section con…rm our previous inferences that the EW estimators are
likely to be dominated by easy-to-implement instrumental variables estimators in most of the envi-
ronments an applied researcher might work. Importantly, while the precision of the OLS-IV results
we present depend on the correlation structure we use — a structure we believe is plausible — our
conclusions about the poor performance of the EW estimator are independent of those assumptions.
For most practical purposes, the EW estimator is biased and ine¢ cient, leaving researchers with
incorrect point estimates for the coe¢ cients of interest.
  23
     Our simulation results (available upon request) suggest that introducing heteroscedasticy makes the performance
of the EW estimator even worse in these cases.
  24
     The results for wit = zit are quite similar to those we get from setting wit = i . We report only one set of graphs
to save space.




                                                          22
3.5         Revisiting the OLS-IV Assumptions

Our Monte Carlo simulations show that the OLS-IV estimator is consistent even when one allows
for autocorrelation in the measurement error structure. In other words, the OLS-IV framework of-
fers viable estimation strategies for the standard investment equation. We have assumed, however,
some structure on the processes governing innovations. In this section, we examine the sensitivity
of the OLS-IV results with respect to our assumptions about measurement error correlation and the
amount of autocorrelation in the latent regressor. These assumptions can a¤ect the quality of the
instruments and therefore should be examined in some detail.
         We …rst examine conditions regarding the correlation of the measurement errors and distur-
bances. The assumption of time-invariant autocorrelation for measurement errors and disturbances
implies that past shocks to measurement errors do not a¤ect the current level of the measurement
error. One way to relax this assumption is to allow for the measurement error process to have a
moving average structure. This structure satis…es Biorn’s assumptions (B1) and (B2). In this case,
Proposition 1 in Biorn (2000) shows that for a MA( ) the instruments should be of order of at most
t             2. Intuitively, the set of instruments must be “older” than the memory of the measure-
ment error process. For example, if the measurement error is MA(1), then one must use third- and
longer-lagged instruments to identify the model.
         To analyze this case, we conduct Monte Carlo simulations in which we replace the time invariant
assumption for innovation uit and vit with a MA(1) structure for the measurement error process.
The degree of correlation in the MA process is set to                  = 0:4. Thus, the innovation in equations (16)
and (17) have the following structure

                                      uit = u1it      u1it   1   and vit = v1it   v1it   1;

with j j          1, and u1it and v1it are i.i.d. Lognormal distributions. The other parameters in the
simulation remain the same.
         The results are presented in Table 5. Using MA(1) in the innovations and the third lag of the
latent regressor as an instrument (either on its own or in combination with the fourth lag), the bias
of the OLS estimator is very small (approximately 2% to 3%). The bias increases somewhat when
we use only the fourth lag. While the fourth is an admissible instrument in this case, using longer
lags decreases the implied autocorrelation in the latent regressor (which follows an AR(1) process by
equation (18)). This e¤ect decreases somewhat the quality of the instruments. Notice also that when
we do not eliminate short lags from the instrument set the identi…cation fails. For example, the bias
is 60% when we use the second lag as an instrument. These results thus underscore the importance
of using long enough lags in this MA case. Table 5 also reports results based on a MA(2) structure.
The results are qualitatively identical to those shown in the MA(1) case. Once again, the important
condition for identi…cation is to use long enough lags (no less than four lags in this case).25
    25
         We note that if the instrument set uses suitably long lags, then the OLS-IV results are robust to variations in the


                                                                 23
                                              Table 5 about here

    The second condition underlying the use of the OLS-IV is that the latent regressor is not time
invariant. Accordingly, the degree of autocorrelation in the process for the latent regressor is an
important element of the identi…cation strategy. We assess the sensitivity of the OLS-IV results to
this condition by varying the degree of autocorrelation through the autoregressive coe¢ cient in the
AR(1) process for the latent regressor. In these simulations, we use a time-invariant autocorrelation
condition for the measurement error, but the results are very similar for the MA case.
    Figure 3 shows the results for the bias in the coe¢ cients of interest for the well-measured and mis-
measured variables, using the second lag of the mismeasured variable as an instrument. The results
show that the OLS-IV estimator performs well for a large range of the autoregressive coe¢ cient. How-
ever, as expected, when the      coe¢ cient is very close to zero or one, we have evidence of a weak instru-
ments problem. For example, when           = 1, then         it   is uncorrelated with any variable dated at time
t   2 or earlier. These simulations show that, provided that one uses adequately lagged instruments,
the exact amount of autocorrelation in the latent variable is not a critical aspect of the estimation.

                                              Figure 3 about here

    The simulations of this section show how the performance of the OLS-IV estimator is a¤ected by
changes in assumptions concerning measurement errors and latent regressors. In practical applica-
tions, it is important to verify whether the results obtained with OLS-IV estimators are robust to the
elimination of short lags from the instrumental set. This robustness check is particularly important
given that the researcher will be unable to pin down the process followed by the measurement error.
Our empirical application below incorporates this suggestion. In addition, identi…cation relies on
some degree of autocorrelation in the process for the latent regressor. While this condition cannot
be directly veri…ed, we can perform standard tests of instrument adequacy that rely on “…rst-stage”
test statistics calculated from the processes for the observable variables in the model.

3.6    Distributional Properties of the EW and OLS-IV Estimators

Our simulations cast doubt on the accuracy of the EW estimator. When applied to simulated data,
that estimator performs poorly in terms of bias and e¢ ciency, even under “ideal conditions.”A nat-
ural question is whether our simulation results are rooted in the lack of accuracy of the asymptotic
approximation of the EW method. It is important that we understand this issue, else one may have
little hope that the estimates produced by the EW model will be any better than estimates obtained
from a random number generator process.
degree of correlation in the MA process. In unreported simulations under MA(1), we show that the OLS bias is nearly
invariant to the parameter .




                                                        24
    Inference in models with mismeasured regressors are based on asymptotic approximations, hence
inference based on estimators with poor approximations might lead to wrong inference procedures.
For instance, we might select wrong critical values for a test under poor asymptotic approximations,
and make inaccurate statements under such circumstances. In this section, we use the panel data
simulation procedure of Section 3.3.2 to study and compare the accuracy of the asymptotic approx-
imation of the EW and IV methods. To save space, we restrict our attention to the mismeasured
regressor coe¢ cient for the EW-GMM5 and OLS-IV cases. We present results where we draw the
data from the Lognormal, Chi-square, and F distributions. The EW-GMM5 estimator is computed
after the within transformation and the OLS-IV uses second lags as instruments.
    One should expect both the IV and EW estimators to have asymptotically normal representa-
tions, such that when we normalize the estimator by subtracting the true parameter and divide by
the standard deviation this quantity behaves asymptotically as a normal distribution. Accordingly,
we compute the empirical density and the distribution functions of the normalized sample estimators
and their normal approximations. These functions are plotted in Figure 4. The true normal density
and distribution functions (drawn in red) serve as benchmarks. The graphs in Figure 4 depict the
accuracy of the approximation. We calculate the density of the estimators using simple Gaussian
Kernel estimator, and also estimate the empirical cumulative distribution function.26
    Consider the Lognormal distribution (…rst panel). In that case, the OLS-IV (black line) displays a
very precise approximation to the normal curve in terms of both density and distribution. The result
for the OLS-IV is robust across all of the distributions considered (Lognormal, Chi-square, and F ).
These results are in sharp contrast to those associated with the EW-GMM5 estimator. This estimator
presents a poor asymptotic approximation for all distributions examined. For the Lognormal case, the
density is not quite centered at zero, and its shape does not …t the normal distribution. For the Chi-
square and F distributions, Figure 4 shows that the shapes of the density and distribution functions
are very unlike the normal case, with the center of the distribution located far away from zero. These
results imply that inference procedures using the EW estimator might be asymptotically invalid in
simple panel data with …xed e¤ects, even when the relevant distributions present high skewness.

                                               Figure 4 about here


4     Empirical Application
We apply the EW and OLS-IV estimators to the Fazzari et al.’s (1988) investment equation. In
this model, a …rm’s investment spending is regressed on a proxy for investment demand (Tobin’s q)
and the …rm’s cash ‡ow. Theory suggests that the correct proxy for the …rm’s investment demand
is marginal q, but this quantity is unobservable and researchers use instead its measurable proxy,
   26
      The empirical cumulative distribution function Fn is a step function with jumps i=n at observation values, where
i is the number of tied observations at that value.



                                                         25
average q. Because average q measures marginal q imperfectly, a measurement problem naturally
arises. Erickson and Whited (2002) uses the Fazzari et al. (1988) model to motivate the adoption
of their estimator in applied work in panel data.
   A review of the corporate investment literature shows that virtually all empirical work in the area
considers panel data models with …rm-…xed e¤ects (see, among others, Kaplan and Zingales (1997),
Rauh (2006), and Almeida and Campello (2007)). From an estimation point of view, there are distinct
advantages in exploiting repeated observations from individuals to identify the model (see Blundell
et al. (1992)). In an investment model setting, exploiting …rm e¤ects contributes to estimation
precision and allows for model consistency in the presence of unobserved idiosyncrasies that may be
simultaneously correlated with investment and q. The baseline model in this literature has the form:

                                            Iit =Kit =   i   + qit + CFit =Kit + uit ;                        (21)

where I denotes investment, K capital stock, q is marginal q, CF cash ‡ow,                      is the …rm-speci…c
e¤ect, and u is the innovation term.
   As mentioned earlier, if q is measured with error, OLS estimates of                   will be biased downwards.
In addition, given that q and cash ‡ow are likely to be positively correlated, the coe¢ cient             is likely
to be biased upwards in OLS estimations. In expectation, these biases should be reduced by the use
of estimators like the ones discussed in the previous section.

4.1    Theoretical Expectations

In order to better evaluate the performance of the two alternative estimators, we develop some hy-
potheses about the e¤ects of measurement-error correction on the estimated coe¢ cients                      and
from equation (21). Theory does not pin down the exact values that these coe¢ cients should take.
Nevertheless, one could argue that the two following conditions should be reasonable.
   First, an estimator that is addressing measurement error in q in a standard investment equation
should return a higher estimate for                and a lower estimate for      when compared with standard OLS
estimates. Recall, measurement error causes an attenuation bias on the estimate for the coe¢ cient
 . In addition, since q and cash ‡ow are likely to be positively correlated, measurement error should
cause an upward bias on the empirical estimate returned for                     under the standard OLS estimation.
Accordingly, if one denotes the OLS and the measurement-error consistent estimates, respectively,
       OLS       OLS )           M EC        M EC ),
by (         ,           and (          ,              one should expect:

                     OLS         M EC             OLS         M EC .
Condition 1                <                and          >

   Second, one would expect the coe¢ cients for q and the cash ‡ow to be non-negative after treating
the data for measurement error. The q-theory of investment predicts a positive correlation between
investment and q (see, e.g., Hayashi (1982)). If the theory holds and the estimator does a good
job of adjusting for measurement error, then the cash ‡ow coe¢ cient should be zero (“neoclassical

                                                                  26
view”). However, the cash ‡ow coe¢ cient could be positive either because of the presence of …nanc-
ing frictions (as posited by Fazzari et al. (1988)),27 or due to fact that cash ‡ow picks up variation
in investment opportunities even after we apply a correction for mismeasurement in q. Accordingly,
one should observe:

                   M EC              M EC
Condition 2                 0 and             0.

       Notice that these conditions are fairly weak. If a particular measurement-error consistent esti-
mator does not deliver these basic results, one should have reasons to question the usefulness of that
estimator in applied work.

4.2      Data Description

Our data collection process follows that of Almeida and Campello (2007). We consider a sample of
manufacturing …rms over the 1970 to 2005 period with data available from COMPUSTAT. Following
those authors, we eliminate …rm-years displaying asset or sales growth exceeding 100%, or for which
the stock of …xed capital (the denominator of the investment and cash ‡ow variables) is less than
$5 million (in 1976 dollars). Our raw sample consists of 31,278 observations from 3,084 individual
…rms. Summary statistics for investment, q, and cash ‡ow are presented in Table 6. These statistics
are similar to those reported by Almeida and Campello, among other papers. To save space we omit
the discussion of these descriptive statistics.

                                               Table 6 about here

4.3      Testing for the Presence of Fixed E¤ects and Heteroscedasticity

Before estimating our investment models, we conduct a series of tests for the presence of …rm-…xed
e¤ects and heteroscedasticity in our data. As a general rule, these phenomena might arise naturally
in panel data applications and should not be ignored. Importantly, whether they appear in the data
can have concrete implications for the results generated by di¤erent estimators.
       We …rst perform a couple of tests for the presence of …rm-…xed e¤ects. We allow for individual
…rm intercepts in equation (21) and test the null hypothesis that the coe¢ cients associated with
those …rm e¤ects are jointly equal to zero (see Baltagi (2005)). Table 7 shows that the F -statistic
for this test is 4.4 (the associated p-value is 0.000). Next, we contrast the random e¤ects OLS and
the …xed e¤ects OLS estimators to test again for the presence of …xed e¤ects. The Hausman test
statistic reported in Table 7 rejects the null hypothesis that the random e¤ects model is appropriate
  27
    However, …nancial constraints are not su¢ cient to generate a strictly positive cash ‡ow coe¢ cient because the
e¤ect of …nancial constraints is capitalized in stock prices and may thus be captured by variations in q (see Chirinko
(1993) and Gomes (2001)).




                                                         27
with test statistic of 8.2 (p-value of 0.017). In sum, standard tests strongly reject the hypothesis
that …xed e¤ects can be ignored.
   We test for homoscedasticity using two di¤erent panel data-based methods. First, we compute
the residuals from the least squares dummy variables estimator and regress the squared residuals
on a function of the independent variables (see Frees (2004) for additional details). We use two
                                                                            2 ; CF ; CF 2 ) — and both
di¤erent combinations of independent regressors — (qit ; CFit ) and (qit ; qit    it   it
of them robustly reject the null hypothesis of homoscedasticity. We report the results for the …rst
combination in Table 7, which yields a test statistic of 55.2 (p-value of 0.000). Our second approach
for testing the null of homoscedasticity is the standard random e¤ects Breusch-Pagan test. Table
7 shows that the Breusch-Pagan test yields a statistic of 7396.2 (p-value of 0.000). Our tests hence
show that the data strongly reject the hypothesis of error homoscedasticity.

                                          Table 7 about here

4.4   Implementing the EW Identi…cation Test

Our preliminary tests show that one should control for …xed e¤ects when estimating investment
models using real data. In the context of the EW estimator, it is thus appropriate to apply the
within transformation before the estimation. However, in this section we also present results for the
data in level form to illustrate the point made in Section 3.2 that applying the within transformation
compromises identi…cation in the EW context. Prior papers adopting the EW estimator have ignored
(or simply dismissed) the importance of …xed e¤ects (e.g., Whited (2001, 2006)).
   We present the results for EW’s identi…cation test in Table 8. Using the data in level form, we
reject the hypothesis of no identi…cation in 12 out of 30 years (or 36% rejection). For data that is
transformed to accommodate …xed e¤ects (within transformation), we …nd that in only 7 out of 33
(or 21%) of the years between 1973 and 2005 one can reject the null hypothesis that the model is
not identi…ed at the usual 5% level of signi…cance. These results suggest that the power of the test
is low and decreases further after applying the within transformation to the data. These results are
consistent with Almeida and Campello’s (2007) use of the EW estimator. Working with a 15-year
COMPUSTAT panel, those authors report that they could only …nd a maximum of three years of
data passing the EW identi…cation test.

                                          Table 8 about here

   The results in Table 8 reinforce the notion that it is quite di¢ cult to operationalize the EW
estimator in real-world applications; particularly in situations in which the within transformation is
appropriate due to the presence of …xed e¤ects. We recognize that the EW identi…cation test rejects
the model for most of the data at hand. However, recall from Section 3.2 that the test itself is likely
to be misleading (“over-rejecting” the data). In the next section, we take the EW estimator to the


                                                  28
data (a standard COMPUSTAT sample extract) to illustrate the issues applied researches face when
using that estimator, contrasting it to an easy-to-implement alternative.

4.5      Estimation Results

We estimate equation (21) using the EW, OLS-IV, and AB-GMM estimators. For comparison pur-
poses, we also estimate that investment equation using standard OLS and OLS with …xed e¤ects
(OLS-FE). The estimates for the standard OLS are likely to be biased, providing a benchmark to eval-
uate the performance of the other estimators. As discussed in Section 4.1, we expect estimators that
improve upon the problem of mismeasurement to deliver results that satisfy Conditions 1 and 2 above.
       As is standard in the empirical literature, we use an unbalanced panel in our estimations. Erickson
and Whited (2000) propose a minimum distance estimator (MDE) to aggregate the cross-sectional
estimates obtained for each sample year, but their proposed MDE is designed for balanced panel
data. Following Riddick and Whited (2008), we use a Fama-MacBeth procedure to aggregate the
yearly EW estimations.28
       To implement our OLS-IV estimators, we …rst take di¤erences of the model in equation (21). We
then employ the estimator denoted by OLS-IV A from Section 2.2, using lagged levels of q and cash
‡ow as instruments for (di¤erenced) qit . Our Monte Carlos suggest that identi…cation in this context
may require the use of longer lags of the model variables. Accordingly, we experiment with speci…-
cations that use progressively longer lags of q and cash ‡ow to verify the robustness of our results.
       Table 9 reports our …ndings. The OLS and OLS-FE estimates, reported in columns (1) and (2)
respectively, disregard the presence of measurement error in q. The EW-GMM3, EW-GMM4, and
EW-GMM5 estimates are reported in columns (3), (4), and (5). For the OLS-IV estimates reported
in column (6), we use qt      2   as an instrument.29 The AB-GMM estimator, reported in column (7),
uses lags of q as instruments. Given our data structure, this implies using a total of 465 instruments.
We account for …rm-…xed e¤ects by transforming the data.

                                              Table 9 about here

       When using OLS and OLS-FE, we obtain the standard result in the literature that both q and
cash ‡ow attract positive coe¢ cients (see columns (1) and (2)). In the OLS-FE speci…cation, for
example, we obtain a q coe¢ cient of 0:025, and a cash ‡ow coe¢ cient of 0:121. Columns (3), (4), and
(5) show that the EW estimator does not deliver robust inferences about the correlations between
investment, cash ‡ow, and q. The q-coe¢ cient estimate varies signi…cantly with the set of moment
conditions used, even ‡ipping signs. In addition, none of the q coe¢ cients is statistically signi…cant.
The cash ‡ow coe¢ cient is highly in‡ated under EW, and in the case of the EW-GMM4 estimator
  28
     Fama-McBeth estimates are computed as a simple standard errors for yearly estimates. An alternative approach
could use the Hall-Horowitz bootstrap. For completeness, we present in the appendix the actual yearly EW estimates.
  29
     In the next section, we examine the robustness of the results with respect to variation in the instrument set.



                                                        29
it is more than three times larger than the (supposedly biased) OLS coe¢ cient. These results are
inconsistent with Conditions 1 and 2 above. These …ndings agree with the Monte Carlo simulations
of Section 3.3.2, which also point to a very poor performance of the EW estimator in cases in which
…xed e¤ects and heteroscedasticity are present.
    By comparison, the OLS-IV delivers results that are consistent with Conditions 1 and 2. In
particular, the q coe¢ cient increases from 0:025 to 0:063, while the cash ‡ow coe¢ cient drops from
0:131 to 0:043. These results suggest that the proposed OLS-IV estimator does a fairly reasonable
job at addressing the measurement error problem. This conclusion is consistent with the Monte
Carlo simulations reported above, which show that the OLS-IV procedure is robust to the presence
of …xed e¤ects and heteroscedasticity in simulated data. The AB-GMM results also generally satisfy
Conditions 1 and 2. Notice, however, that the observed changes in the q and cash ‡ow coe¢ cients
(“corrections” relative to the simple, biased OLS estimator) are less signi…cant than those obtained
under the OLS-IV estimation.
    We emphasize that the data comparisons of this section (including the benchmarking against
Conditions 1 and 2) are a purely heuristic way of gauging the relative strengths and weaknesses of
the various estimators we consider. We believe, however, that they are useful in helping researchers
understand some of the main points uncovered by our Monte Carlo simulations.

4.6     Robustness of the Empirical OLS-IV Estimator

It is worth demonstrating that the OLS-IV we consider is robust to variations in the set of instru-
ments that is used for identi…cation. While the OLS-IV delivered results that are consistent with
our priors, note that we examined a just-identi…ed model, for which tests of instrument quality are
not available. As we have discussed previously, OLS-IV estimators should be used with care in this
setting, since the underlying structure of the error in the latent variable is unknown. In particular,
the Monte Carlo simulations suggest that it is important to show that the results remain when we
use longer lags to identify the model.
    We present the results from our robustness checks in Table 10. We start by adding one more lag of
q (i.e., qt   3)   to the instrumental set. The associated estimates are in the …rst column of Table 10. One
can observe that the slope coe¢ cient associated with q increases even more with the new instrument
(up to 0:090), while that of the cash ‡ow variable declines further (down to 0:038). One problem with
this estimation, however, is the associated J -statistic. If we consider a 5% hurdle rule, the J -statistic
of 4.92 implies that, with this particular instrumental set, we reject the null hypothesis that the
identi…cation restrictions are met (p-value of 3%). As we have discussed, this could be expected if,
for example, the measurement error process has a MA structure. This suggests that the researcher
should look for longer lagging schemes, lags that “erase” the MA memory of the error structure.

                                               Table 10 about here


                                                        30
         Our next set of estimations use longer lagging structures for our proposed instruments and even
a instrumental set with only lags of cash ‡ow, the exogenous regressors in the model. We use com-
binations of longer lags of q (such as the fourth and …fth lags) and longer lags of cash ‡ow (fourth
and …fth lags). This set of tests yield estimates that more clearly meet standard tests for instrument
validity.30 Speci…cally, the J -statistics now indicate we do not reject the hypothesis that the exclu-
sion restrictions are met. The results reported in columns (2) through (7) of Table 10 also remain
consistent with Conditions 1 and 2. In particular, the q coe¢ cient varies from approximately 0:040
to 0:091, while the cash ‡ow coe¢ cient varies roughly from 0:044 to 0:046. These results are con-
sistent with our simulations, which suggest that these longer lag structures should deliver relatively
consistent, stable estimates of the coe¢ cients for q and cash ‡ow in standard investment regressions.
These results further support our more general conclusion that the OLS-IV estimator is likely to
dominate the EW estimator in real-world applications.


5         Concluding Remarks
OLS estimators have been used as a reference in empirical work in …nancial economics. Despite their
popularity, those estimators perform poorly when dealing with the problem of errors-in-variables.
This is a serious problem since in most empirical applications one might raise concerns about issues
such as data quality and measurement errors.
         In recent work, Erickson and Whited (2000, 2002) propose an appealing solution to the errors-
in-variables problem. In doing so, the authors revisit the empirical investment equation introduced
by Fazzari et al. (1988), where a …rm’s investment spending is regressed on a proxy for investment
demand and the …rm’s cash ‡ow. Theory suggests that the correct proxy for a …rm’s investment de-
mand is marginal q, but this quantity is unobservable and researchers use instead average q. Since the
two variables are not the same, a measurement problem arises. In lieu of using economic intuition to
address measurement in the Fazzari et al.’s equation, Erickson and Whited propose the use of higher
moments of the variables in the equation as identifying restrictions in a GMM framework. The ad-
vantage of their approach is that one need not go outside the model governing the investment process
— investment spending, capital productivity, cash ‡ows — to identify the model in the presence of
errors-in-variables. However, little was known about the costs entailed by this identi…cation strategy.
         This paper uses Monte Carlo simulations and real data to assess the performance of EW’s higher-
order moment estimator, contrasting it with alternative instrumental variable-type approaches. We
show that in the presence of individual-…xed e¤ects, under heteroscedasticity, or in the absence of
high degree of skewness in the data, the EW estimator returns biased coe¢ cients for both mismea-
sured and perfectly-measured regressors. The EW estimator is also very ine¢ cient. In contrast, we
    30
    All of the F -statistics associated with the …rst-stage regressions have p-values that are close to zero. These
statistics (reported in Table 10) suggest that we do not incur a weak instrument problem when we use longer lags in
our instrumental set.



                                                        31
…nd that IV estimators remain fairly unbiased under those same conditions and that they are more
e¢ cient than the EW estimator. Our simulations suggest that the costs associated with the EW
estimator make it less useful and reliable than the framework it is supposed to defeat.
   We also estimate empirical investment models using the two methods. Because real-world invest-
ment data contain …rm-…xed e¤ects and heteroscedasticity, the EW estimator delivers coe¢ cients
that are unstable across di¤erent speci…cations and not economically meaningful. In contrast, a sim-
ple OLS-IV estimator yields results that conform to theoretical expectations. We conclude that the
EW estimator underperforms the easy-to-implement OLS-IV estimator we present in most situations
empirical researchers might …nd in practice.




                                                 32
References
Agca, S., and A. Mozumdar, 2007, “Investment-Cash Flow Sensitivity: Myth or Reality?” mimeo,
    George Washington University.

Almeida, H., and M. Campello, 2007, “Financial Constraints, Asset Tangibility and Corporate
   Investment”, Review of Financial Studies 20, 1429-1460.

Altonji, J., and L. Segal, 1996, “Small-Sample Bias in GMM Estimation of Covariance Structures,”
    Journal of Business & Economic Statistics 14, 353-366.

Arellano, M., and S. Bond, 1991, “Some Tests of Speci…cation for Panel Data: Monte Carlo Evidence
    and an Application to Employment Equations,” Review of Economic Studies 58, 277-297.

Bakke, T., and T. Whited, 2009, “What Gives? A Study of Firms’Reactions to Cash Shortfalls,”
    mimeo, University of Rochester.

Baltagi, B., 2005, Econometric Analysis of Panel Data, John Wiley and Sons Press, 3rd Edition.

Baum, C., M. Scha¤er, and S. Stillman, 2003, “Instrumental Variables and GMM: Estimation and
   Testing,” Stata Journal 3, 1-31.

Bertrand, M., E. Du‡o, and S. Mullainathan, 2004, “How Much Should We Trust Di¤erences-in-
    Di¤erences Estimates?” Quarterly Journal of Economics 119, 249-275.

Bertrand, M., and S. Mullainathan, 2005, “Bidding for Oil and Gas Leases in the Gulf of Mexico:
    A Test of the Free Cash Flow Model?” mimeo, University of Chicago and MIT.

Bertrand, M., and A. Schoar, 2003, “Managing with Style: The E¤ect of Managers on Firm Poli-
    cies,” Quarterly Journal of Economics 118, 1169-1208.

Biorn, E., 2000, “Panel Data with Measurement Errors: Instrumental Variables and GMM Proce-
    dures Combining Levels and Di¤erences,” Econometric Reviews 19, 391-424.

Blanchard, O., F. Lopez-de-Silanes, and A. Shleifer, 1994, “What Do Firms Do with Cash Wind-
    falls?” Journal of Financial Economics 36, 337-360.

Blundell, R., S. Bond, M. Devereux, and F. Schiantarelli, 1992, “Investment and Tobin’s Q: Evidence
    from Company Panel Data,” Journal of Econometrics 51, 233-257.

Bond, S., and J. Cummins, 2000, “The Stock Market and Investment in the New Economy”Brook-
   ings Papers on Economic Activity 13, 61-124.

Chirinko, R., 1993, “Business Fixed Investment Spending: Modeling Strategies, Empirical Results,
    and Policy Implications”, Journal of Economic Literature 31, 1875-1911.

Colak, G., and T. Whited, 2007, “Spin-o¤s, Divestitures, and Conglomerate Investment,” Review
    of Financial Studies 20, 557-595.

Cragg, J., 1997, “Using Higher Moments to Estimate the Simple Errors-In-Variables Model,”RAND
    Journal of Economics 28, 71-91.

Doran, H., and P. Schmidt, 2006, “GMM Estimators with Improved Finite Sample Properties
    Using Principal Components of the Weighting Matrix, with an Application to the Dynamic
    Panel Data Model,” Journal of Econometrics133, 387-409.



                                               33
Erickson, T., and T. Whited, 2000, “Measurement Error and the Relationship between Investment
    and Q,” Journal of Political Economy 108, 1027-1057.
Erickson, T., and T. Whited, 2002, “Two-Step GMM Estimation of the Errors-In-Variables Model
    Using High-Order Moments,” Econometric Theory 18, 776-799.
Fazzari S., R. G. Hubbard, and B. Petersen, 1988, “Financing Constraints and Corporate Invest-
    ment,” Brooking Papers on Economic Activity 1, 141-195.
Frees, E., 2004, Longitudinal and Panel Data Analysis and Applications in the Social Sciences,
    Cambridge University Press, New York, NY.
Gan, J., 2007, “Financial Constraints and Corporate Investment: Evidence from an Exogenous
    Shock to Collateral,” Journal of Financial Economics 85, 709-734.
Gomes, J., 2001, “Financing Investment”, American Economic Review 91, 1263-85.
Griliches, Z., and J. A. Hausman, 1986, “Errors in Variables in Panel Data,”Journal of Economet-
     rics 31, 93-118.
Hadlock, C., 1998, “Ownership, Liquidity, and Investment,”RAND Journal of Economics 29, 487-
    508.
Hayashi, F., 1982, “Tobin’s Marginal q and Average q: A Neoclassical Interpretation,”Econometrica
    50, 213-24.
Hayashi, F., 2000, Econometrics. Princeton, NJ: Princeton University Press.
Hennessy, C., 2004, “Tobin’s Q, Debt Overhang, and Investment,” Journal of Finance 59, 1717-
   1742.
Holtz-Eakin, D., W. Newey, and H. S. Rosen, 1988, “Estimating Vector Autoregressions with Panel
    Data,” Econometrica 56, 1371-1395.
Hubbard, R. G., 1998, “Capital Market Imperfections and Investment,” Journal of Economic Lit-
   erature 36, 193-227.
Kaplan, S., and L. Zingales, 1997, “Do Financing Constraints Explain Why Investment Is Correlated
    with Cash Flow?” Quarterly Journal of Economics 112, 169-215.
Lamont, O., 1997, “Cash Flow and Investment: Evidence from Internal Capital Markets,” Journal
   of Finance 52, 83-110.
Lyandres, E., 2007, “External Financing Costs, Investment Timing, and Investment-Cash Flow
    Sensitivity,” Journal of Corporate Finance 13, 959-980.
Malmendier, U., and G. Tate, 2005, “CEO Overcon…dence and Corporate Investment,” Journal of
   Finance 60, 2661-2700
Petersen, M., 2009, “Estimating Standard Errors in Finance Panel Data Sets: Comparing Ap-
    proaches,” Review of Financial Studies 22, 435-480.
Poterba, J., 1988, “Financing Constraints and Corporate Investment: Comment”, Brookings Papers
    on Economic Activity 1, 200-204.
Rauh, J., 2006, “Investment and Financing Constraints: Evidence from the Funding of Corporate
   Pension Plans,” Journal of Finance 61, 33-71.

                                              34
Riddick, L., and T. Whited, 2008, “The Corporate Propensity to Save,” forthcoming, Journal of
    Finance.

Shin, H., and R. Stulz, 1998, “Are Internal Capital Markets E¢ cient?” Quarterly Journal of Eco-
    nomics 113, 531-552.

Stein, J., 2003, “Agency Information and Corporate Investment,”in G. Constantinides, M. Harris,
     and R. Stulz (eds.), Handbook of the Economics of Finance, Elsevier/North-Holland, Amster-
     dam.

Xiao, Z., J. Shao, and M. Palta, 2008, “A Uni…ed Theory for GMM Estimation in Panel Data
    Models with Measurement Error,” mimeo, University of Wisconsin Madison.

Wansbeek, T. J., 2001, “GMM Estimation in Panel Data Models with Measurement Error,”Journal
   of Econometrics 104, 259-268.

Whited, T., 2001, “Is It Ine¢ cient Investment that Causes the Diversi…cation Discount?” Journal
   of Finance 56, 1667-1691.

Whited, T., 2006, “External Finance Constraints and the Intertemporal Pattern of Intermittent
   Investment,” Journal of Financial Economics 81, 467-502.




                                              35
Appendix A Derivation of the EW Estimator
The EW estimators are based on the equations for the moments of yi zi y and xi zi x as functions
                                                                                              P
of and the moments ui , "i , and i . To derive these equations, write (3) as yi zi y = Jj=1 ij j +ui ,
where J is the number of well-measured regressors, and the j th equation in (4) as xij zi xj = ij +"ij ,
where xj is the j th column of x and ( ij ; "ij ) is the j th row of ( 0ij ; "0ij ). Next write
                   "                                             #        "                      !r0                         #
                                   r0   Q
                                        J
                                                            rj                P
                                                                              J                         Q
                                                                                                        J
                 E (yi      zi   y)           (xi    zi   x)         =E               i   + ui                (   i   + "i )rj ;   (22)
                                        j=1                                   j=1                       j=1


where (r0 , r1 ,..., rJ ) are nonnegative integers. After expanding the right hand side of (22) using the
multinomial theorem, it is possible to write the above moment condition as

                                                          E [gi ( )] = c( );
                                                                                      Q
where = vec( y ; x ), gi ( ) is a vector of distinct elements of the form (yi zi y )r0 Jj=1 (xi zi x )rj ,
c( ) contains the corresponding expanded version of the right-hand side of (22), and is a vector
containing the elements of and the moments of (ui , "i ; i ). The GMM estimator is de…ned as

                                     ^ = arg min(gi (^ )                    ^ (gi (^ )
                                                                     c(t))0 W                  c(t));
                                                    t2
                Pn                           ^
where gi (s)       t=1 gi (s) for all s, and W is a positive de…nite matrix. Assuming a number of
regularity conditions,31 the estimator is consistent and asymptotically Normal.

   Regarding the Minimum Distance Estimator, the estimator is derived by minimizing the distance
between the auxiliary parameter vectors under the following restrictions:

                                                     f ( ; ^) = H             ^ = 0;

where the R K K matrix H imposes (R 1) K restrictions on . The R K 1 vector ^ contains
the R stacked auxiliary parameter vectors, and is the parameter of interest. Moreover, H is de…ned
by a R K K–dimensional stacked identity matrix.
    The MDE is given by the minimization of:

                                                D( ) = f ( ; ^)0 V^ [^]        1
                                                                                   f ( ; ^);                                       (23)

where V^ [^] is the common estimated variance–covariance matrix of the auxiliary parameter vectors.
    In order to implement the MDE, it is necessary to determine the covariances between the cross-
sections being pooled. EW propose to estimate the covariance by using the covariance between the
estimators’respective in‡uence functions.32 The procedure requires that each cross-section have the
same sample size, that is, the panel needs to be balanced.
  31
       More speci…cally, these conditions are as follows: (zi , i , ui , "i ), is an independent and identically distributed
sequence; ui and the elements of zi , i , and "i have …nite moments of every order; (ui , "i ) is independent of (zi ,
  i ), and the individual elements in (ui , "i ) are independent of each other; E(ui )=0 and E("i )=0; E[(zi ,   i )’(zi , i )]
is positive de…nite; every element of is nonzero; and the distribution of satis…es E[( i c)3 ]6=0 for every vector of
constants c = (c1 ; :::; cJ ) having at least one nonzero element.
    32
       See Erickson and Whited (2002) Lemma 1 for the de…nition of their proposed in‡uence function.



                                                                     36
      Thus, minimization of D in equation (23) leads to

                                                 ^ = (H 0 V^ [^]      1
                                                                          H)    1
                                                                                    H 0 V^ [^]        1^


with variance–covariance matrix
                                                      V^ [ ^ ] = (H 0 V^ [^]        1
                                                                                        H)    1
                                                                                                  :
H is a vector in which R is the number of GMM estimates available (for each time period) and K = 1,
^ is a vector containing all the EW estimates for each period, and is the MDE of interest. In ad-
dition, V^ [^] is a matrix carrying the estimated variance–covariance matrices of the GMM parameter
vectors. In order to implement the MDE it is necessary to determine the covariances between the
cross-sections being pooled. EW propose to estimate the covariance by using the covariance between
the estimators’respective in‡uence functions.33 The procedure requires that each cross-section have
the same sample size, that is, the panel needs to be balanced.


Appendix B OLS-IV A and OLS-IV B Estimators
The estimator for OLS-IV A can be de…ned as
                                                  "   N
                                                                            #   1" N                           #
                                                      X                           X
                                ^            =              x0ip (   xit )                    x0ip (       yit ) ;
                                    xp(t )
                                                      i=1                               i=1

where (t; ; p) are indices. Let the dimension of                          be de…ned by K. If K = 1, it is possible to de…ne
the following estimator for a given (t, ; p)
                                                  "   N
                                                                            #   1" N                           #
                                                      X                           X
                                ^            =              yip ( xit )                       yip ( yit ) :
                                    yp(t )
                                                      i=1                               i=1

If K > 1, the latter estimator is infeasible, but it is possible to modify the former estimator by
replacing one element in x0ip by yip .
    The estimator for OLS-IV B (equation in level and instruments in di¤erence) can be de…ned as
                                                 "   N
                                                                            #   1" N                            #
                                                     X                            X
                               ^
                                   x(pq)t    =              ( xipq )0 xit                     ( xipq )0 yit :
                                                     i=1                                i=1

As in the previous case, if the dimension of ; K is equal to 1, it is possible to de…ne the following
estimator for (t, p, q)
                                       "N              # 1" N             #
                                        X                  X
                            ^
                              y(pq)t =    ( yipq ) xit        ( yipq ) yit :
                                                      i=1                               i=1

If K > 1, the latter estimator is infeasible, but it is possible to modify the former estimator by
replacing one element in xip by yip .
 33
      See Erickson and Whited (2002) Lemma 1 for the de…nition of their proposed in‡uence function.




                                                                      37
Appendix C GMM Estimators
First, consider estimation using the equation in di¤erences and instrumental variables in levels. After
taking di¤erences of the model, there are (T 1)+(T +1) equations that can be stacked for individual
i as                     2            3 2               3     2                3
                               yi21              xi21                    i21
                         6     yi32 7      6     xi32 7       6                7
                         6            7 6               7     6          i32   7
                         6      ..    7 6         ..    7     6        ..      7
                         6       .    7 6          .    7     6         .      7
                         6            7 6               7     6                7
                         6 yi;T;T 1 7 6 xi;T;T 1 7            6                7
                         6            7=6               7 +6        i;T;T    1 7;
                         6            7
                               yi31 7 6    6     xi31 7 7     6                7
                         6                                    6          i31   7
                         6     yi42 7      6     xi42 7       6                7
                         6            7 6               7     6          i42   7
                         6       ..   7 6          ..   7     6       ..       7
                         4        .   5 4           .   5     4        .       5
                             yi;T;T 2          xi;T;T 2             i;T;T 2

or compactly
                                                          yi =   Xi +             i:

The IV matrix is the ((2T          3)       KT (T         2)) diagonal matrix with the instruments in the diagonal
de…ned by Z. Let

                            y = [( y1 )0 ; : : : ; ( yN )0 ]0 ;                  = [(               0
                                                                                                1) ; : : : ; (
                                                                                                                    0 0
                                                                                                                  N) ]

                           X = [( X1 )0 ; : : : ; ( XN )0 ]0 ;                Z=          [Z10 ; : : : ; ZN
                                                                                                          0 0
                                                                                                             ]:

The GMM estimator that minimizes [N 1 ( )0 Z](N 2 V ) 1 [N 1 Z 0 ( )] for V = Z 0 Z can be written
as
                          2"               #"          # 1"             #3 1
                             X                X             X
                ^
                  Dx =
                          4     ( Xi )0 Zi      Zi0 Zi        Zi0 ( Xi ) 5
                                            i                        i                         i
                                        2"                       #"                   #                           #3
                                                                                            1"
                                                X                        X                         X
                                        4           ( Xi )0 Zi               Zi0 Zi                    Zi0 ( yi ) 5 :
                                                i                        i                         i


If     has a non-scalar covariance matrix, a more e¢ cient GMM estimator, ~ Dx , can be obtained
setting V = VZ( ) = E [Z 0 ( )( )0 Z], and estimating V^Z( ) by

                                            V^Z(      )       1 X 0 c c 0
                                                          =      Z ( )( ) Z;
                                               N              N
                                                                 i


where c i = yi ( Xi ) ^ Dx . This procedure assumes that (A1) and (A2) are satis…ed. However,
as Biorn (2000) argues, one can replace them by (B1) or (B2), and then ensure that the variables in
the IV matrix have a lead or lag of at least + 1 periods to the regressor, to “get clear of” the
period memory of the MA( ) process. The procedure described below is also based on the same set
of assumptions and can be extend similarly.34
 34
      See Propositions 1 and 2 in Biorn (2000) for a formal treatment of the conditions.




                                                                 38
   The procedure for estimation using equation in                             levels and IV’s in di¤erence is similar. Consider
the T stacked level equations for individual i
                            2       3 2 3 2                                       3              2          3
                                yi1        c                                  xi1                     i1
                            6 .. 7 6 .. 7 6                                    .. 7         6     7
                            4 . 5=4 . 5+4                                       . 5       + 4 ... 5 ;
                                        yiT               c                   xiT                     iT

or more compactly,
                                                        yi = eT c + Xi + ;
where eT denotes a (T 1) vector of ones. Let the (T T (T 2)K) diagonal matrix of instrument
be denoted by Zi . This matrix has the instruments in di¤erence in the main diagonal. In addition
de…ne

                           y = [y10 ; : : : ; yN
                                               0 0
                                                 ];                  = [ 01 ; : : : ;    0 0
                                                                                         N]
                           X = [X10 ; : : : ; XN
                                               0 0
                                                 ];                         Z = [(      Z1 )0 ; : : : ; (       ZN )0 ]0 :

The GMM estimator that minimizes [N                      1 0(       Z)0 ](N      2V      )   1 [N 1 (           Z)0 ] for V = ( Z)0 ( Z) is
                              2"                         #"                                  #                               #3
                                                                                                 1"                               1
                                   X                          X                                       X
                ^
                    Lx   = 4             Xi0 ( Zi )                 ( Zi )0 ( Zi )                          ( Zi )0 Xi 5
                                    i                           i                                      i
                                2"                         #"                                    #                           #3
                                                                                                     1"
                                        X                           X                                      X
                                4             Xi ( Zi )                     ( Zi )0 ( Zi )                       ( Zi )0 yi 5 :
                                         i                          i                                       i


    If has a non-scalar covariance matrix, a more e¢ cient GMM estimator, ~ Lx , can be obtained
setting V = V( Z) = E [( Z)0 0 ( Z)], and estimating V^( Z) by

                                              V^(   Z)        1 X
                                                         =        ( Z)0^^0 ( Z);
                                                    N         N
                                                                        i

where ^ = yi   Xi ^ Lx .




                                                                        39
               Table 1. The Performance of the EW Identification Test
This table shows the performance of the EW identification test for different distributional assumptions,
displayed in column 1. The tests are computed for the data in levels, and after applying a within
transformation. Column 4 shows the the frequencies at which the null hypothesis that the model is not
identified is rejected, at the 5% level of significance.


Distribution                Null is               Data Form                  Frequency of Rejection

  N ormal                    True                   Level                              0.05
                                                    Within                             0.05
Lognormal                    False                  Level                              0.47
                                                    Within                             0.43
     χ23                     False                  Level                              0.14
                                                    Within                             0.28
   F10,40                    False                  Level                              0.17
                                                    Within                             0.28
                 Table 2. The EW Estimator: Cross-Sectional Data
This table shows the bias and the RMSE associated with the estimation of the model in equations (17) to
(21) using the EW estimator in simulated cross-sectional data. β is the coefficient on the mismeasured
regressor, and α1 to α3 are the coefficients on the perfectly measured regressors. The Table shows
the results associated with GMM3, GMM4, and GMM5, for all the alternative distributions. These
estimators are based on the respective third, fourth, and fifth moment conditions.

                                        β                  α1                α2                 α3

 Panel A. Lognormal Distribution
 EW-GMM3           Bias               0.0203            -0.0054            -0.0050           -0.0056
                 RMSE                 0.1746             0.0705            0.0704            0.0706
 EW-GMM4           Bias               0.0130            -0.0034            -0.0033           -0.0040
                 RMSE                 0.2975             0.1056            0.1047            0.1083
 EW-GMM5           Bias               0.0048            -0.0013            -0.0013           -0.0019
                 RMSE                 0.0968             0.0572            0.0571            0.0571
 Panel B. Chi-Square Distribution
 EW-GMM3            Bias             -0.0101            0.0092             0.0101            -0.0060
                  RMSE               61.9083            16.9275            16.1725           14.7948
 EW-GMM4            Bias             -0.3498            0.0938             0.0884             0.0831
                  RMSE               12.2386             3.2536             2.9732            3.1077
 EW-GMM5            Bias             -0.3469            0.0854             0.0929             0.0767
                  RMSE               7.2121             1.8329             1.8720             1.6577
 Panel C. F-Distribution
 EW-GMM3            Bias               0.3663           -0.1058            -0.0938           -0.0868
                   RMSE              190.9102           53.5677            52.4094           43.3217
 EW-GMM4            Bias              -0.2426            0.0580            0.0649             0.0616
                   RMSE              90.9125            24.9612            24.6827           21.1106
 EW-GMM5            Bias              -0.2476            0.0709            0.0643             0.0632
                   RMSE              210.4784           53.5152            55.8090           52.4596
 Panel D. Normal Distribution
 EW-GMM3           Bias                1.9179            -0.6397           -0.5073            -0.3512
                  RMSE              2305.0309           596.1859          608.2098           542.2125
 EW-GMM4           Bias               -1.0743             0.3012           0.2543              0.2640
                  RMSE               425.5931           111.8306          116.2705           101.4492
 EW-GMM5           Bias                3.1066            -1.0649           -0.9050            -0.5483
                  RMSE               239.0734            60.3093           65.5883            58.3686
                 Table 3. The EW Estimator: Panel Data in Levels
This table shows the bias and the RMSE associated with the estimation of the model in equations (17)
to (21) using the EW estimator in simulated panel data. The table reports results from data in levels
(that is, without applying the within transformation). β is the coefficient on the mismeasured regressor,
and (α1 , α2 , α3 ) are the coefficients on the perfectly measured regressors. The table shows the results
for the EW estimator associated with EW-GMM3, EW-GMM4, and EW-GMM5, for all the alternative
distributions. These estimators are based on the respective third, fourth, and fifth moment conditions.

                                            β                 α1                 α2                 α3

 Panel A. Lognormal Distribution
 EW-GMM3             Bias                -1.6450            2.5148             2.5247             2.5172
                    RMSE                 1.9144             2.5606             2.5711             2.5640
 EW-GMM4             Bias                -1.5329            2.5845             2.5920             2.5826
                    RMSE                 1.9726             2.6353             2.6443             2.6354
 EW-GMM5             Bias                -1.3274            2.5468             2.5568             2.5490
                    RMSE                 1.6139             2.5944             2.6062             2.5994
 Panel B. Chi-Square Distribution
 EW-GMM3             Bias                -1.0051            2.2796             2.2753             2.2778
                    RMSE                 1.1609             2.2887             2.2841             2.2866
 EW-GMM4             Bias                -0.9836            2.2754             2.2714             2.2736
                    RMSE                 1.0540             2.2817             2.2776             2.2797
 EW-GMM5             Bias                -0.9560            2.2661             2.2613             2.2653
                    RMSE                 1.0536             2.2728             2.2679             2.2719
 Panel C. F-Distribution
 EW-GMM3              Bias               -0.9926            2.2794             2.2808             2.2777
                     RMSE                1.1610             2.2890             2.2904             2.2870
 EW-GMM4              Bias               -0.9633            2.2735             2.2768             2.2720
                     RMSE                1.0365             2.2801             2.2836             2.2785
 EW-GMM5              Bias               -0.9184            2.2670             2.2687             2.2654
                     RMSE                2.0598             2.2742             2.2761             2.2725
 Panel D. Normal Distribution
 EW-GMM3             Bias                -0.8144            2.2292              2.228             2.2262
                   RMSE                  0.9779             2.2363             2.2354             2.2332
 EW-GMM4             Bias                -0.9078            2.2392             2.2363             2.2351
                   RMSE                  0.9863             2.2442             2.2413             2.2400
 EW-GMM5             Bias                -0.8773            2.2262             2.2225             2.2217
                   RMSE                  0.9846             2.2316             2.2279             2.2269
Table 4. OLS, OLS-IV, AB-GMM, and EW Estimators: Panel Data After
         Within Transformation
This table shows the bias and the RMSE associated with the estimation of the model in equations (17) to
(21) using the OLS, OLS-IV, AB-GMM, and EW estimators in simulated panel data. The table reports
results from the estimators on the data after applying the within transformation. β is the coefficient
on the mismeasured regressor, and (α1 , α2 , α3 ) are the coefficients on the perfectly measured regressors.
The table shows the results for the EW estimator associated with EW-GMM3, EW-GMM4, and EW-
GMM5, for all the alternative distributions. These estimators are based on the respective third, fourth,
and fifth moment conditions.
                                         β                α1                α2                  α3

Panel A. Lognormal Distribution
     OLS              Bias            -0.7126           0.1553            0.1558              0.1556
                     RMSE             0.7131            0.1565            0.1570              0.1568
   OLS-IV             Bias             0.0065           -0.0019          -0.0014              -0.0015
                     RMSE              0.1179            0.0358           0.0357               0.0355
  AB-GMM              Bias            -0.0248           0.0080            0.0085              0.0081
                     RMSE             0.0983            0.0344            0.0344              0.0340
 EW-GMM3              Bias            -0.0459           0.0185            0.0184              0.0183
                     RMSE             0.0901            0.0336            0.0335              0.0335
 EW-GMM4              Bias            -0.0553           0.0182            0.0182              0.0183
                     RMSE             0.1405            0.0320            0.0321              0.0319
 EW-GMM5              Bias            -0.0749           0.0161            0.0161              0.0161
                     RMSE             0.1823            0.0303            0.0297              0.0297

Panel B. Chi-Square Distribution
     OLS              Bias            -0.7126           0.1555            0.1553              0.1556
                     RMSE             0.7132            0.1565            0.1563              0.1567
   OLS-IV             Bias             0.0064           -0.0011          -0.0017              -0.001
                     RMSE              0.1149            0.0348           0.0348              0.0348
  AB-GMM              Bias            -0.0231           0.0083            0.0077              0.0081
                     RMSE             0.0976            0.0339            0.0338              0.0342
 EW-GMM3              Bias            -0.3811           0.0982            0.0987              0.0982
                     RMSE             0.4421            0.1133            0.1136              0.1133
 EW-GMM4              Bias            -0.3887           0.0788            0.0786              0.0783
                     RMSE             0.4834            0.0927            0.0923              0.0919
 EW-GMM5              Bias            -0.4126           0.0799            0.0795              0.0798
                     RMSE             0.5093            0.0926            0.0921              0.0923
                               Table 4. (continued)

                                 β          α1          α2        α3

Panel C. F-Distribution
    OLS             Bias       -0.7123     0.1554     0.1549    0.1555
                   RMSE        0.7127      0.1565     0.1559    0.1566
   OLS-IV           Bias       0.0066     -0.0013     -0.0023   -0.001
                   RMSE        0.1212      0.0359      0.0362   0.0361
 AB-GMM             Bias       -0.0232     0.0079     0.0072    0.0085
                   RMSE        0.0984      0.0343     0.0342    0.0344
 EW-GMM3            Bias       -0.3537     0.0928     0.0916    0.0917
                   RMSE        0.4239      0.1094     0.1086    0.1095
 EW-GMM3            Bias       -0.3906     0.0802     0.0790    0.0791
                   RMSE        0.4891      0.0939     0.0930    0.0932
 EW-GMM3            Bias       -0.4188     0.0818     0.0808    0.0813
                   RMSE        0.5098      0.0939     0.0932    0.0935

Panel D. Normal Distribution
    OLS             Bias       -0.7119     0.1553     0.1554    0.1551
                   RMSE        0.7122      0.1563     0.1564    0.1562
   OLS-IV           Bias       0.0060     -0.0011     -0.0012   -0.0014
                   RMSE        0.1181      0.0353      0.0355    0.0358
 AB-GMM             Bias       -0.0252     0.0086     0.0085    0.0084
                   RMSE        0.0983      0.0344     0.0339    0.0343
 EW-GMM3            Bias       -0.7370     0.1903     0.1904    0.1895
                   RMSE        0.7798      0.2020     0.2024    0.2017
 EW-GMM4            Bias       -0.8638     0.2141     0.2137    0.2137
                   RMSE        0.8847      0.2184      0.218    0.2182
 EW-GMM5            Bias       -0.8161     0.1959     0.1955    0.1955
                   RMSE        0.8506      0.2021     0.2018    0.2017
  Table 5. Moving Average Structures for the Measurement Error Process
This table shows the bias in the well measured coefficient for OLS-IV using moving average structure
for the measurement error process. Numbers inside parenthesis are the RMSE.


     Instrument                                    MA(1)                                   MA(2)

        Xit−2                                      -0.593                                  -0.368
                                                   (0.60)                                   (0.38)
        Xit−3                                       0.028                                  -0.707
                                                   (0.30)                                   (0.71)
    Xit−3 , Xit−4                                   0.025                                   0.077
                                                   (0.63)                                   (1.01)
 Xit−3 , Xit−4 , Xit−5                             -0.011                                  -0.759
                                                   (0.30)                                   (0.76)
        Xit−4                                      -0.107                                  -0.144
                                                   (1.62)                                   (2.01)
    Xit−4 , Xit−5                                  -0.113                                  -0.140
                                                   (0.58)                                   (0.59)
        Xit−5                                      -0.076                                  -0.758
                                                   (0.31)                                   (0.76)
                               Table 6. Descriptive Statistics
This table shows the basic descriptive statistics for q, cash flow, and investment. The data are taken
from the annual COMPUSTAT industrial files over the 1970 to 2005 period. See text for details.


  Variable          Obs.           Mean           Std. Dev.             Median         Skewness

 Investment        22556          0.2004            0.1311              0.17423         2.6871
      q            22556          1.4081            0.9331              1.1453          4.5378
  Cash flow        22556          0.3179            0.3252              0.27845         -2.2411




                                   Table 7. Diagnosis Tests
This table reports results for specification tests. Hausman test for fixed effects models considers fixed
effects models against the simple pooled OLS and the random effects model. A homocedasticity test for
the innovations is also reported. The data are taken from the annual COMPUSTAT industrial files over
the 1970 to 2005 period. See text for details


                 Test                                     Test statistic                       p-value

         Pooling test                                          4.397                             0.0000
 Random Effects vs. Fixed Effects                              8.17                              0.0169
       Homocedasticity 1                                       55.19                             0.0000
       Homocedasticity 2                                      7396.21                            0.0000
                  Table 8. The EW Identification Test using Real Data
This table shows the test-statistic and its p-value for the EW identification test, which tests the null hypothesis
that the model is not identified. The tests are performed on a yearly basis. In the last columns, we collect the
number of years in which the null hypothesis is rejected (sum), and compute the percentage of years in which
the null is rejected. The data are taken from the annual COMPUSTAT industrial files over the 1970 to 2005
period. See text for details
                       Level                                                 Within Transformation
                                      #Rejections                                                    #Rejections
                                        Null                                                           Null
 1973   t-statistic      1.961              0                1973      t-statistic      1.349              0
         p-value         0.375                                          p-value         0.509
 1974   t-statistic      5.052              0                1974      t-statistic      7.334              1
         p-value          0.08                                          p-value         0.026
 1975   t-statistic      1.335              0                1975      t-statistic      1.316              0
         p-value         0.513                                          p-value         0.518
 1976   t-statistic      7.161              1                1976      t-statistic      5.146              0
         p-value         0.028                                          p-value         0.076
 1977   t-statistic      1.968              0                1977      t-statistic      1.566              0
         p-value         0.374                                          p-value         0.457
 1978   t-statistic      9.884              1                1978      t-statistic      2.946              0
         p-value         0.007                                          p-value         0.229
 1979   t-statistic      9.065              1                1979      t-statistic      1.042              0
         p-value         0.011                                          p-value         0.594
 1980   t-statistic      9.769              1                1980      t-statistic      7.031              1
         p-value         0.008                                          p-value          0.03
 1981   t-statistic     10.174              1                1981      t-statistic      7.164              1
         p-value         0.006                                          p-value         0.028
 1982   t-statistic      3.304              0                1982      t-statistic      2.991              0
         p-value         0.192                                          p-value         0.224
 1983   t-statistic      5.724              0                1983      t-statistic      9.924              1
         p-value         0.057                                          p-value         0.007
 1984   t-statistic     15.645              1                1984      t-statistic      6.907              1
         p-value           0                                            p-value         0.032
 1985   t-statistic     16.084              1                1985      t-statistic      1.089              0
         p-value           0                                            p-value          0.58
 1986   t-statistic      4.827              0                1986      t-statistic      5.256              0
         p-value         0.089                                          p-value         0.072
 1987   t-statistic     19.432              1                1987      t-statistic     13.604              1
         p-value           0                                            p-value         0.001
 1988   t-statistic      5.152              0                1988      t-statistic      1.846              0
         p-value         0.076                                          p-value         0.397
 1989   t-statistic      0.295              0                1989      t-statistic      0.687              0
         p-value         0.863                                          p-value         0.709
 1990   t-statistic      0.923              0                1990      t-statistic        1.3              0
         p-value          0.63                                          p-value         0.522
 1991   t-statistic      3.281              0                1991      t-statistic       3.17              0
         p-value         0.194                                          p-value         0.205
 1992   t-statistic       2.31              0                1992      t-statistic      2.573              0
         p-value         0.315                                          p-value         0.276
 1993   t-statistic      1.517              0                1993      t-statistic      1.514              0
         p-value         0.468                                          p-value         0.469
 1994   t-statistic      2.873              0                1994      t-statistic      4.197              0
         p-value         0.238                                          p-value         0.123
 1995   t-statistic      0.969              0                1995      t-statistic      1.682              0
         p-value         0.616                                          p-value         0.431
 1996   t-statistic     17.845              1                1996      t-statistic      4.711              0
         p-value           0                                            p-value         0.095
 1997   t-statistic       0.14              0                1997      t-statistic      1.535              0
         p-value         0.933                                          p-value         0.464
 1998   t-statistic      0.623              0                1998      t-statistic      5.426              0
         p-value         0.732                                          p-value         0.066
 1999   t-statistic      0.354              0                1999      t-statistic      2.148              0
         p-value         0.838                                          p-value         0.342
 2000   t-statistic      13.44              1                2000      t-statistic     13.502              1
         p-value         0.001                                          p-value         0.001
 2001   t-statistic      3.159              0                2001      t-statistic      3.309              0
         p-value         0.206                                          p-value         0.191
 2002   t-statistic     13.616              1                2002      t-statistic      0.693              0
         p-value         0.001                                          p-value         0.707
 2003   t-statistic     12.904              1                2003      t-statistic      4.006              0
         p-value         0.002                                          p-value         0.135
 2004   t-statistic      5.212              0                2004      t-statistic      2.801              0
         p-value         0.074                                          p-value         0.246
 2005   t-statistic      2.365              0                2005      t-statistic      4.127              0
         p-value         0.306                                          p-value         0.127
                         Sum               12                                           Sum                7
                       % of Years        0.3636                                       % of Years        0.2121
         Table 9. EW, GMM and OLS-IV Coefficients, Real World Data
This table shows the coefficients and standard deviations that we obtain when we use the OLS, EW
and the GMM estimators in equation (22). The table also displays the standard OLS-FE coefficients
(after applying the differencing transformation to treat the fixed effects) in column (2) and OLS-IV
in last column. Robust standard errors in parentheses for OLS and GMM, and clustered in firms for
OLS-FE and OLS-IV. Each EW coefficient is an average of the yearly coefficients reported in Table A1
and the standard error for these coefficients is a Fama-McBeth standard error. The table shows the EW
coefficients for the data after applying the within transformation. The data are taken from the annual
COMPUSTAT industrial files over the 1970 to 2005 period. See text for details. *, **, and *** represent
statistical significance at the 10%, 5%, and 1% level, respectively.

         Variables            OLS         OLS-FE      EW-GMM3         EW-GMM4      EW-GMM5     OLS-IV      AB-GMM

             q             0.0174***      0.0253***      0.0679        -0.3031      0.0230     0.0627***   0.0453***
                            (0.002)        (0.003)       (0.045)       (0.302)      (0.079)     (0.007)     (0.006)
         Cash flow           0.1310***    0.1210***     0.1299***      0.3841*     0.1554***   0.0434***   0.0460***
                              (0.011)      (0.017)       (0.031)       (0.201)      (0.052)     (0.007)     (0.016)
       Observations            22556        22556         22556         22556        22556       17348       19748
 F-stat p-value (first-step)     –            –             –             –            –         0.000         –




                      Table 10. OLS-IV Coefficients, Robustness Tests
This table shows the results of varying the set of instruments that are used when applying the OLS-IV
estimator to equation (22). In the first column we use the second and third lags of q as instruments for
current (differenced) q, as in Table 6. In column (2) we use third, fourth and fifth lags of q as instruments.
In column (3) we use the fourth and fifth lags of q and the first lag of cash flow as instruments. In column
(4) we use the third lag of q and fourth lag of cash flow as instruments. In column (5) we use the fourth
and fifth lags of cash flow as instruments. In column (6) we use {qt−4 , qt−5 , qt−6 , CFt−3 , CFt−4 , CFt−5 }
as instruments. Finally, in column (7) {qt−5 , qt−6 , CFt−3 , CFt−4 , CFt−5 } as instruments. The estima-
tions correct the errors for heteroskedasticity and firm-clustering. The data are taken from the annual
COMPUSTAT industrial files over the 1970 to 2005 period. See text for details. *, **, and *** represent
statistical significance at the 10%, 5%, and 1% level, respectively.

          Variables              (1)            (2)           (3)          (4)         (5)        (6)          (7)

                 q            0.0901***     0.0652***     0.0394***    0.0559***   0.0906***   0.0660***    0.0718***
                               (0.014)       (0.014)       (0.015)      (0.012)     (0.033)     (0.024)      (0.026)
         Cash flow            0.0383***     0.0455***     0.0434***    0.0449***   0.0421***   0.0450***    0.0444***
                               (0.007)       (0.011)       (0.011)      (0.010)     (0.008)     (0.012)      (0.012)
       Observations          15264            11890         12000        13448       12000       10524        10524
 F-stat p-value (first-step) 0.000            0.000         0.000        0.000       0.001       0.000        0.000
           J-stat             4.918           3.119         0.122       0.00698      0.497       8.955        5.271
       J-stat p-value        0.0266           0.210         0.727        0.933       0.481       0.111        0.261
      Table A.1. EW Coefficients for Real Data (Within Transformation)
This Table shows the coefficients and standard deviations that we obtain when we use the EW estimator in
Equation (22), estimated year-by-year. The Table shows the results for the EW estimator associated with
GMM3, GMM4 and GMM5. The Table shows the EW coefficients for the data that is treated for fixed effects
via the within transformation. The data are taken from the annual COMPUSTAT industrial files over the 1970
to 2005 period. See text for details.
                               q coefficient                                Cash-Flow coefficient
      Year        GMM3          GMM4           GMM5               GMM3            GMM4            GMM5
      1973         -0.029          0.000         0.000              0.347           0.265         0.264
                  (0.075)        (0.073)       (4.254)            (0.207)         (0.207)      (11.968)
      1974          0.050          0.029         0.019              0.168           0.199         0.214
                  (0.037)        (0.012)       (0.016)            (0.073)         (0.043)       (0.043)
      1975          0.225          0.001         0.000              0.161           0.292         0.292
                  (0.475)        (0.149)       (0.125)            (0.281)         (0.095)       (0.094)
      1976          0.137          0.001         0.000              0.156           0.276         0.276
                  (0.094)        (0.273)       (0.042)            (0.090)         (0.251)       (0.048)
      1977          0.082          0.243         0.000              0.203           0.091         0.261
                  (0.263)        (0.109)       (0.108)            (0.179)         (0.090)       (0.083)
      1978          0.263          0.514         0.281              0.122         (0.067)         0.108
                  (0.282)        (0.927)       (0.146)            (0.224)         (0.689)       (0.125)
      1979          0.020          0.001         0.001              0.249           0.266         0.266
                  (0.161)        (0.048)       (0.031)            (0.155)         (0.056)       (0.044)
      1980          0.349          0.116         0.183              0.021           0.219         0.163
                  (0.294)        (0.071)       (0.055)            (0.273)         (0.074)       (0.067)
      1981          0.334          0.185         0.324             -0.145           0.061        -0.131
                  (0.165)        (0.045)       (0.128)            (0.248)         (0.093)       (0.191)
      1982          0.109          0.383         0.238              0.125          -0.206        -0.031
                  (0.155)        (0.316)       (0.126)            (0.195)         (0.398)       (0.174)
      1983          0.081          0.001         0.001              0.132           0.184         0.184
                  (0.037)        (0.041)       (0.059)            (0.033)         (0.034)       (0.040)
      1984          0.230          0.210         0.185              0.125           0.138         0.154
                  (0.083)        (0.050)       (0.043)            (0.067)         (0.052)       (0.048)
      1985          0.198          0.349         0.230              0.050         (0.018)         0.035
                  (0.483)        (0.137)       (0.024)            (0.212)         (0.086)       (0.032)
      1986          0.672          0.244         0.593             -0.179           0.070        -0.133
                  (0.447)        (0.089)       (0.162)            (0.303)         (0.079)       (0.128)
      1987          0.102          0.104         0.115              0.078           0.078         0.077
                  (0.039)        (0.020)       (0.003)            (0.021)         (0.021)       (0.020)
      1988          0.129          0.179         0.148              0.030           0.027         0.029
                  (0.051)        (0.029)       (0.014)            (0.011)         (0.007)       (0.007)
      1989         -0.365         -0.015        -0.111              0.285           0.162         0.196
                  (1.797)        (0.082)       (0.196)            (0.642)         (0.063)       (0.078)
      1990         -0.437         -0.419        -0.529              0.395           0.386         0.440
                  (0.404)        (0.137)       (0.024)            (0.214)         (0.094)       (0.093)
      1991          0.384          0.260         0.240             -0.098           0.007         0.023
                  (0.225)        (0.105)       (0.038)            (0.199)         (0.099)       (0.055)
      1992          0.105          0.102         0.040              0.086           0.088         0.148
                  (0.016)        (0.008)       (0.016)            (0.034)         (0.033)       (0.037)
      1993          0.274          0.322         0.452             -0.076          -0.118        -0.232
                  (0.394)        (0.352)       (0.273)            (0.360)         (0.297)       (0.276)
      1994         -0.110         -4.436        -0.047              0.255           3.550         0.207
                  (0.136)       (86.246)       (0.011)            (0.108)        (65.488)       (0.045)
      1995         -0.574         -8.847        -2.266              0.537           5.898         1.633
                  (1.862)      (145.827)       (5.565)            (1.275)        (94.154)       (3.749)
      1996          0.220          0.167         0.196              0.101           0.106         0.103
                  (0.068)        (0.022)       (0.013)            (0.036)         (0.033)       (0.030)
      1997          0.089          0.177         0.158              0.059           0.020         0.028
                  (0.082)        (0.042)       (0.021)            (0.042)         (0.041)       (0.034)
      1998         -0.620         -0.245        -0.119              0.688           0.355         0.242
                  (1.634)        (0.187)       (0.027)            (1.446)         (0.169)       (0.037)
      1999         -0.031         -0.003         0.000              0.160           0.126         0.123
                  (0.059)        (0.028)       (0.055)            (0.074)         (0.038)       (0.068)
      2000          0.071          0.126         0.118              0.032          -0.029        -0.021
                  (0.024)        (0.030)       (0.020)            (0.043)         (0.057)       (0.051)
      2001          0.050          0.077         0.055              0.034           0.020         0.031
                  (0.021)        (0.020)       (0.013)            (0.016)         (0.016)       (0.012)
      2002          0.047          0.048         0.048              0.030           0.030         0.030
                  (0.128)        (0.016)       (0.014)            (0.033)         (0.013)       (0.012)
      2003          0.131          0.066         0.157             -0.013           0.025        -0.027
                  (0.043)        (0.025)       (0.010)            (0.031)         (0.026)       (0.014)
      2004          0.005          0.030         0.030              0.092           0.079         0.079
                  (0.066)        (0.018)       (0.009)            (0.045)         (0.034)       (0.034)
      2005          0.049          0.029         0.026              0.078           0.095         0.098
                  (0.025)        (0.009)       (0.011)            (0.040)         (0.032)       (0.034)
 Fama-MacBeth     0.0679         -0.3031       0.0232             0.1299          0.3841        0.1554
 Standard Error   0.0455         0.3018        0.0787             0.0310          0.2027       0.05222
    Figure 1. Theoretical Distributions Examined



                         Normal Density                                                       Chi-Square Density
     0.4




                                                            0.15
     0.3




                                                            0.10
     0.2
y




                                                        y

                                                            0.05
     0.1




                                                            0.00
     0.0




               -3   -2    -1   0    1      2   3                                          0   5       10      15   20

                               x                                                                      x


                           F Density                                                          Lognormal Density
                                                            0.0 0.1 0.2 0.3 0.4 0.5 0.6
     0.8
     0.6
y




                                                        y
     0.4
     0.2
     0.0




           0         5         10         15       20                                     0   5       10      15   20

                               x                                                                      x
Figure 2. The Effect of Heteroscedasticity on the Bias in EW’s GMM
and OLS-IV estimators.
 Figure 3. Autocorrelation in the Latent Variable and the OLS-IV Bias




                              Mismeasured Variable
       2
       1
Bias

       0
       -1
       -2




              0.0   0.2         0.4          0.6         0.8       1.0

                                        φ



                           Perfectly-Measured Variable
       0.4
Bias

       0.0
       -0.4




              0.0   0.2         0.4          0.6         0.8       1.0

                                        φ
      Figure 4. Asymptotic Approximations




                       Density functions (Lognormal)                                          Distribution functions (Lognormal)

                     Normal                                                                    Normal




                                                                                  0.8
          0.8




                     OLS-IV                                                                    OLS-IV




                                                                   distribution
                     EW-GMM5                                                                   EW-GMM5
density

          0.4




                                                                                  0.4
          0.0




                                                                                  0.0
                -4        -2          0          2             4                        -4          -2          0          2           4

                                      x                                                                         x



                 Density functions (Chi-square Distribution)                            Distribution functions (Chi-square Distribution)
          0.6




                                                     Normal                                    Normal
                                                                                  0.8



                                                     OLS-IV                                    OLS-IV
                                                                   distribution




                                                     EW-GMM5                                   EW-GMM5
          0.4
density




                                                                                  0.4
          0.2
          0.0




                                                                                  0.0




                -4        -2          0          2             4                        -4          -2          0          2           4

                                      x                                                                         x



                     Density functions (F-Distribution)                                      Distribution functions (F-Distribution)
          0.6




                                                     Normal                                    Normal
                                                                                  0.8




                                                     OLS-IV                                    OLS-IV
                                                                   distribution




                                                     EW-GMM5                                   EW-GMM5
          0.4
density




                                                                                  0.4
          0.2
          0.0




                                                                                  0.0




                -4        -2          0          2             4                        -4          -2          0          2           4

                                      x                                                                         x
