                                NBER WORKING PAPER SERIES




     DO REPORT CARDS TELL CONSUMERS ANYTHING THEY DON'T ALREADY
                  KNOW? THE CASE OF MEDICARE HMOS

                                           Leemore Dafny
                                           David Dranove

                                        Working Paper 11420
                                http://www.nber.org/papers/w11420


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                      June 2005




We thank two anonymous referees and the editor for invaluable suggestions. We are grateful for comments
by David Cutler, Tom Hubbard, Ilyana Kuziemko, Mara Lederman, Gautam Gowrisankaran, Phillip
Leslie, Mike Mazzeo, Aviv Nevo, Dennis Scanlon, Scott Stern, Andrew Sweeting, and participants
at various seminars. Laurence Baker, Su Liu, and Robert Town generously shared data with us, and
Yongbae Lee, Shiko Maruyama, and Subramaniam Ramanarayanan provided outstanding research
assistance. This research was made possible by a grant from The Searle Fund for Policy Research.
The views expressed herein are those of the author(s) and do not necessarily reflect the views of the
National Bureau of Economic Research.

© 2005 by Leemore Dafny and David Dranove. All rights reserved. Short sections of text, not to exceed
two paragraphs, may be quoted without explicit permission provided that full credit, including © notice,
is given to the source.
Do Report Cards Tell Consumers Anything They Don't Already Know? The Case of Medicare
HMOs
Leemore Dafny and David Dranove
NBER Working Paper No. 11420
June 2005, Revised October 2007
JEL No. D8,H4,I1

                                              ABSTRACT

The use of government-mandated report cards to diminish uncertainty about the quality of products
and services is widespread. However, report cards will have little effect if they simply confirm consumers'
prior beliefs. Moreover, documented "responses" to report cards may reflect learning about quality
that would have occurred in their absence ("market-based learning"). Using panel data on Medicare
HMO market shares between 1994 and 2002, we examine the relationship between enrollment and
quality before and after report cards were mailed to 40 million Medicare beneficiaries in 1999 and
2000. We find evidence that consumers learn from both public report cards and market-based sources,
with the latter having a larger impact during our study period. Consumers are especially sensitive to
both sources of information when the variance in HMO quality is greater. The effect of report cards
is driven by beneficiaries' responses to consumer satisfaction scores; other reported quality measures
such as the mammography rate did not affect enrollment decisions.

Leemore Dafny
Department of Management and Strategy
Kellogg School of Management
Northwestern University
2001 Sheridan Road
Evanston, IL 60208-2001
and NBER
l-dafny@kellogg.northwestern.edu

David Dranove
Management & Strategy Department
Kellogg School of Management
Northwestern University
2001 Sheridan Road
Evanston, IL 60208-2001
d-dranove@kellogg.northwestern.edu
Governments devote substantial resources to developing and disseminating quality report cards

in a variety of settings, ranging from public schools to restaurants to airlines. The value of these

interventions depends on the strength of market-based mechanisms for learning about quality.

For example, the value of reports by the Department of Transportation on airline delays and lost

luggage will be minimal if consumers can easily learn about performance along these dimensions

through word of mouth, prior experience, or a scorecard created by a private company.

       In this study we quantify the effect of the largest public report-card experiment to date,

the release of HMO report cards in 1999 and 2000 to 40 million Medicare enrollees, on the

subsequent healthplan choices of enrollees. We compare the magnitude of the learning induced

by the report cards to that of ongoing, market-based learning, as measured by the trend toward

higher-quality plans manifested in the years prior to the intervention. A variety of factors may

be responsible for such learning, including word of mouth, referrals by healthcare providers,

personal experience, privately-organized report cards, and advertising.

       We conclude that both the public report card and market-based learning produced

substantial swings in Medicare HMO market shares during the study period, 1994-2002.

Market-based learning was largest in markets with private-sector report cards, which provides

secondary evidence that report cards are an effective means of disseminating quality information,

whether publicly or privately sponsored. The effect of report cards is entirely due to customer

satisfaction ratings; other reported measures did not affect subsequent enrollment.

       Our parameter estimates, obtained from a model of healthplan choice, enable us to

simulate the effects of market-based learning and report cards in a variety of scenarios. The

exact magnitudes of both effects depend on the number of plans and their relative quality levels.

Some general patterns emerge from the simulations, including: (1) Market-based learning is




                                                 2
associated with dramatic (i.e. 30 percent or greater) changes in market shares of high or low-

quality plans between 1994 and 2002, where high (low) quality is defined as scoring one

standard deviation above (below) the national mean on a composite of six audited measures of

healthplan quality; (2) Report-card-induced learning is also associated with large swings in

market shares, although the effect is smaller than cumulative market learning between 1994 and

2002. This is partly due to the low rate of enrollment in low-quality plans by the time the report

cards are released; (3) Learning of both types has the greatest impact on market shares when

plans are more differentiated; (4) The report cards increased net enrollment in Medicare HMOs,

but most of the changes in HMO market shares were due to shifts in enrollment within the HMO

market.

       In sum, we find that public report cards do tell consumers something they didn’t know

and wouldn’t otherwise have learned on their own. However, we also find an important role for

market-based learning about healthcare quality, an intriguing result given the difficulties in

measuring quality in this market. Our estimates also suggest that quality reporting is unlikely to

generate large increases in the HMO penetration rate among Medicare beneficiaries, one of the

stated goals of the report-card intervention.

       Our study complements recent economic research on the effects of quality report cards in

various settings, ranging from restaurants (Jin and Leslie 2003) to public schools (e.g. Hastings

and Weinstein 2007) to HMOs (e.g. Chernew et al. 2006, Jin and Sorensen 2006, Scanlon et al.

2004, and Beaulieu 2002). The Medicare experiment is noteworthy not only for its size and the

importance of its target audience, but also for features that enable us to carefully control for and

study behavior that may be correlated with, but not caused by, the report-card intervention.

Chief among these is the availability of a lengthy study period, which we use to estimate the




                                                  3
pattern of market-based learning that predated the report-card intervention; data on reported as

well as unreported quality scores, which we use to conduct a falsification exercise; and some

data on contemporaneous quality scores, which we include together with the (lagged) reported

scores as a robustness check to confirm that measured report-card responses are not in fact

reactions to changes in contemporaneous quality.

       The paper proceeds as follows. Section 1 provides background on Medicare HMOs and

the report card mandate imposed as part of the Balanced Budget Act of 1997. Section 2

summarizes prior related research, and section 3 presents the data. Section 4 describes the main

analysis and results, and Section 5 discusses extensions and robustness tests. Section 6

concludes.




1      Medicare HMOs and the Report Card Mandate

Although the vast majority of Medicare beneficiaries are enrolled in fee-for-service “traditional

Medicare,” the option of receiving coverage through participating, privately-managed HMOs has

been available since the introduction of Medicare in 1966 (Newhouse 2002). Medicare HMOs

offer lower out-of-pocket costs and greater benefits in exchange for reduced patient choice and

utilization controls. Enrollment in Medicare HMOs grew slowly at first, reaching just 1.8

million, or 5 percent of beneficiaries, by 1993. Between 1993 and 1998, enrollment in Medicare

HMOs increased threefold, mirroring enrollment patterns among the privately-insured at large.

Figure 1 graphs the HMO penetration rate for Medicare-eligibles and the privately insured

between 1993 and 2001. HMO penetration in both populations peaked in 1999-2000, declined

through 2003, and has increased slightly since. Surveys of Medicare beneficiaries find that




                                                4
Medicare HMOs draw disproportionately from low-income, minority populations without

supplemental private coverage.

         Although there have been many changes in the statutes governing Medicare HMOs,

throughout our study period (1994-2002) several key features remained intact. First, Medicare

reimbursed participating HMOs a fixed amount per enrollee which varied by geographic area,

gender, age, institutional and work status, and source of eligibility. Enrollees were required to

pay the Medicare “Part B” premium for physician and outpatient care ($54/month in 2002)

directly to the federal government.1 Second, HMOs were permitted to charge enrollees additional

monthly premiums as well as service copayments, subject to regulations designed to prevent

profit margins from the HMO’s Medicare business from exceeding profit margins on the HMO’s

non-Medicare business. Negative premiums were not permitted.2 The result was substantial

premium compression from above and below, which constrained the ability of plans to “price

out” quality differentials. In every year in our study period, the median enrollee paid no

premium at all, and the 75th percentile for monthly premiums ranged between $15 and $35.3

Third, during the November “open enrollment” period, plans were required to accept new

enrollees for the following January. Most plans also accepted enrollees throughout the year, at




1
  In 2001 and 2002, very small adjustments were also made for enrollees’ health status. Between 1982 and 1997,
the payment amount was 95 percent of the average cost for a traditional Medicare enrollee of the same age, gender,
institutional status, and eligibility source, living in the same county. Following the BBA, payment rates were a blend
of area costs and national costs (beginning with 90:10 and ending at 50:50 by 2003), subject to a minimum annual
increase of 2 percent as well as an absolute floor (Newhouse 2002). CMS began implementing a risk adjustment
formula in 2000, with transition to full risk-adjustment delayed to 2007 by the Benefits Improvement and Protection
Act of 2000. Between 2001 and 2003, only 10 percent of the payment from the blend/floor formula was adjusted
for health status, as determined by the enrollee’s “worst principal inpatient diagnosis” to date, if any. As of 2004,
CMS began implementing a risk-adjustment formula based on multiple sites of care (Pope et al. 2004, CMS 2004).
2
  These regulations are summarized in Newhouse (2002) If the combination of Medicare and enrollee contributions
exceeded the rate charged to non-Medicare enrollees (adjusted for “arbitrary” utilization factors), plans were
required to add benefits, reduce premiums, or refund the difference to the government.
3
  Authors’ tabulations using data described in Section 3.


                                                          5
the start of each month. Enrollees were permitted to switch plans or return to traditional

Medicare at the end of every month.

        The Balanced Budget Act of 1997 (BBA 1997) required all managed care plans

participating in the Medicare program to gather and disclose quality data to the Health Care

Financing Agency, now known as The Centers for Medicare and Medicaid Services (CMS).

Plans must report a set of standardized performance measures developed by the National

Consortium for Quality Assurance (NCQA). 4 These measures are collectively called The

Health Plan Employer Data and Information Set (HEDIS®).5 Beginning in 1998, CMS began

supplementing this data by conducting an independent annual survey of Medicare beneficiaries

called the Consumer Assessment of Health Plans Study (CAHPS). Respondents are asked a

series of questions designed to assess their satisfaction with various aspects of their healthcare,

including the communication skills of their physicians and the ease of obtaining care.

        BBA 1997 also required CMS to provide Medicare beneficiaries with information about

health plans and the enrollment process in November of each year. CMS responded with a multi-

pronged educational campaign that included print materials, websites, telephone hotlines,

informational sessions, and more (Goldstein et al. 2001). As part of this effort, CMS created a

handbook called Medicare & You, which is updated and mailed annually to all Medicare

eligibles. Both Medicare & You 2000 (mailed November 1999) and Medicare & You 2001

(mailed November 2000) contained selected HEDIS and CAHPS scores for most plans operating

in the beneficiary’s market area; plans with very low enrollments were exempted from reporting

HEDIS data. Figure 2 presents an excerpt of the report card printed on pages 28-35 of the 73-

4
  NCQA is a private not-for-profit organization whose mission is “to improve healthcare quality everywhere.” In
addition to collecting, standardizing, and releasing HEDIS data, NCQA uses this information to accredit health
plans. Many employers refuse to contract with unaccredited plans.
5
  HEDIS consists of a broad range of measures covering areas such as patient access to care, quality of care as
measured by “best practices,” provider qualifications, and financial stability.


                                                        6
page Medicare & You 2001 booklet mailed to Illinois eligibles. The editions since 2001 refer

readers interested in quality scores to the Medicare website and a toll-free number.6

        For the report cards to have a discernible effect on enrollee behavior, the following chain

of events must transpire: (1) beneficiaries must read and comprehend the publications or

communicate with someone who has done so; (2) beneficiaries must change their beliefs about

plan quality in response to the reported scores; (3) these changes must be of sufficient magnitude

to imply a change in the optimal plan for some enrollees; (4) some of these enrollees must take

actions to switch to their optimal plan. The enrollment changes we examine will only reveal the

extent to which these requirements were collectively satisfied by Medicare & You.

        There are several other formal and informal mechanisms for enrollees to learn about the

quality of Medicare HMOs, including word of mouth, prior experience in a private-sector HMO

offered by the same carrier, current experience in the Medicare HMO, information provided

directly by the HMO, and publications of quality measures for a private-sector HMO offered by

the same carrier. Some carriers made their HEDIS scores for private-sector enrollees available

on NCQA’s website. The popular magazine U.S. News & World Report published selected

scores for all of these plans in their annual “America’s Top HMOs” series from 1996-1998.

Of the 16 percent of beneficiaries who reported seeking managed care information in a

nationwide survey conducted in 2001, the majority used non-CMS information sources. The

most frequent sources cited were the managed care plans themselves, followed by physicians and




6
 The quality data was available some months earlier on the web (January 1999) and through the telephone helpline
(March 1999). However, surveys suggest this information was rarely accessed through these sources in 1999. A
survey performed at 6 case study locations in 1999 showed that only 2 percent of beneficiaries used the internet to
obtain any Medicare information (Goldstein 1999). By 2001, only 6 percent of beneficiaries reported using the
Medicare helpline for any reason (Goldstein et al 2001). We therefore consider the report card mailing to be the
primary source of exposure to the quality data, and use 2000 as the first post-intervention year.


                                                         7
their staff, and friends and family (Goldstein et al. 2001). These statistics suggest a substantial

role for market-based learning, a hypothesis that is supported by the empirical results.



2      Prior Research

The few empirical papers on market-based learning focus on the ability of consumers to learn

about the quality of so-called “experience goods” through personal experience. These studies

find rapid learning in markets with low switching costs (e.g. yogurt, Ackerberg 2002), but slower

learning when switching costs are high (e.g. auto insurance, Israel 2005). Hubbard (2002) finds

evidence that consumers also learn through the aggregate experiences of others: vehicle

emissions inspectors with low aggregate failure rates enjoy more business, controlling for

consumers’ prior experience at these firms.

       Recent studies suggest that report cards also facilitate consumer learning. Jin and Leslie

(2003) find that restaurants posting an “A” grade enjoyed a 5-percent revenue boost relative to

restaurants posting a “B.” They find no evidence that revenues responded to changes in hygiene

scores during the two years before grade cards were introduced. There are at least two reasons to

expect more market-based learning about Medicare HMOs as compared to restaurants. First, in a

broad class of learning models, learning will occur most rapidly in new markets, and the

restaurant market is much more mature. Second, market-based mechanisms that facilitate

learning are more likely to evolve in healthcare due to the magnitude of spending involved as

well as the private incentives for large, private-sector buyers to assess quality.

       Several recent studies estimate the impact of healthplan report cards on enrollment

decisions. Most pursue a before-and-after research design in which the intervention is a report-

card provided by an employer. These studies find small increases in the market share of highly-



                                                  8
rated plans offered to employees of the federal government (Wedig and Tai-Seale 2002),

Harvard University (Beaulieu 2002), and General Motors (Chernew et al. 2006 and Scanlon et al.

2002).7 If market-based learning is occurring independently of the report cards, even these

modest effects may overstate their influence.

         Our study design is most similar to Jin and Sorensen (2006), who compare responses of

federal retirees (and their survivors) to quality ratings for plans that did and did not make these

ratings publicly available (via the periodical U.S. News and World Report and a website

maintained by the National Committee for Quality Assurance). The report-card effect is

measured by the difference in responses. Thus, the “response” to ratings for nondisclosing plans

is akin to market-based learning.8 Jin and Sorensen find evidence of both effects. Chernew et al

(2006) also document a potentially important role for market-based learning. Using data from

General Motors, they estimate a formal Bayesian learning model, in which consumers update

their priors on plan quality using the signal provided by report cards. They find reported

information had a small but statistically-significant impact on consumer beliefs. The high weight

on the priors may be due in part to confidence in market-based learning that had already taken

place prior to the report-card intervention.

         A key advantage of these studies relative to ours is significant variation in plan prices,

conditional on all covariates included in the estimating models. This variation identifies enrollee

responsiveness to price, which in turn can be used to scale responsiveness to quality into dollar

terms. We lack such variation and therefore present our findings in terms of market-share


7
  The report card released to federal employees included six highly-correlated measures of enrollee satisfaction
gathered through mailed survey responses. Wedig and Tai-Seale include two of these measure in their models:
overall quality of care and plan coverage. The Harvard and GM report cards included HEDIS measures as well as
patient satisfaction scores. Beaulieu (2002), Chernew et al. (2001), and Scanlon et al. (2002) use aggregations of all
reported scores in logit models of plan choice.
8
  Technically, we consider privately-organized report cards to be “market-based” sources, so the term “non-report-
card learning” would be more precise here.


                                                          9
responses. These studies also have individual-level data, which facilitates interesting

comparisons of behavior across different groups (e.g. new enrollees vs. existing enrollees,

families vs. individuals).

        Our study contributes to the literature on consumer responses to report cards by

controlling for and examining market-based learning that likely confounds estimated report-card

responses in many studies, by providing estimates of the relative importance of both types of

learning, and by doing so in the context of the largest healthcare report-card experiment to date.




3       Data

We use several datasets available online or through direct requests to CMS. We obtain

enrollment data from the Medicare Managed Care Quarterly State/County/Plan Data Files for

December of each year from 1994 to 2002. 9 Enrollment is available at the plan-county-year

level, where “plan” refers to a unique contract number assigned by CMS.10 Note that carriers

may offer several different products within the same plan, such as a benefits package that

includes prescription drug coverage and one that does not. Enrollment and benefits data is not

available at this level of detail throughout the study period. Fortunately, the quality scores in

Medicare and You were reported at the plan level, so combining enrollment across products

within the same plan should not bias the coefficient estimates. Plan-county-year cells with

fewer than 10 enrollees are not included in the data. The enrollment files also contain the base




9
  http://cms.hhs.gov/healthplans/statistics/mpscpt.
10
   CMS assigns unique contract numbers to carriers (e.g. Aetna) for each geographic area they serve. Because these
geographic areas are defined by the carriers and areas served by different carriers need not coincide, we follow CMS
in considering the county as our market definition.


                                                        10
CMS payment rate for HMO enrollees in each county, as well as the total number of Medicare

eligibles in each county.11

         The plan-level quality measures included in Medicare & You 2000 and 2001 were

extracted from the Medicare HEDIS files and the Medicare Compare Database.12 Three

measures were reported in each booklet: one from the HEDIS dataset, one from the CAHPS

survey (included in the Medicare Compare Database), and the voluntary disenrollment rate.13

The reported HEDIS measure in both years is mammography, the percent of women aged 50-69

who had a mammogram within the past 2 years. The CAHPS measure reported in Medicare &

You 2000 is communicate, the percent of enrollees who reported that the doctors in their plan

always communicate well. Medicare & You 2001 replaced communicate with best care, the

percent of enrollees who rated their own care as the “best possible,” a rating of 10 out of 10. The

reported HEDIS scores were based on data gathered by plans 3 years prior, while the CAHPS

scores and disenrollment rates were lagged 2 years. Appendix Table 1 provides detail on the

sources and data years for reported scores.

         Although Medicare & You reports the disenrollment rate for each plan, we do not include

this measure in our analyses because it is a lagged component of the dependent variable

(enrollment). The three reported scores we match to the enrollment data are therefore

mammography from 2000 (which is highly correlated with reported 2001 scores),14 communicate




11
   The base payment rate is county and year-specific, and is adjusted to reflect enrollee characteristics. See footnote
1 for details.
12
   HEDIS data is available at http://cms/hhs.gov/healthplans/HEDIS/HEDISdwn.asp. CAHPS data is available from
the Medicare Compare Database at http://www.medicare.gov/Download/DownloadDB.asp.
13
   Involuntary disenrollment is produced by plan exits. Participating plans must accept all Medicare beneficiaries
desiring to enroll.
14
   The correlation coefficient for mammography reported in 2000 and mammography reported in 2001 is .86.


                                                          11
from 2000, and best care from 2001. To facilitate comparisons across measures, we create z-

scores for each year and measure.15

           The HEDIS files also include the three measures that were audited by CMS but not

included in the publications: beta blocker (the percent of enrollees aged 35+ receiving a beta

blocker prescription upon discharge from the hospital after a heart attack), ambulatory visit (the

percent of enrollees who had an ambulatory or preventive-care visit in the past year), and

diabetic eye exams (the percent of diabetic enrollees aged 31+ who had a retinal examination in

the past year. We use these measures to compute unreported composite, which is the average of

a plan’s z-scores on all three unreported measures.16

             Most plans report a single set of quality measures pertaining to all of their enrollees. A

small number of plans report data separately by submarket, e.g. San Francisco and Sacramento.

These submarkets do not correspond to county boundaries, so we create enrollee-weighted

average scores by plan in these cases, using enrollment data reported in the HEDIS files. For

plans reporting CAHPS data separately by submarket, we create simple averages by plan because

the CAHPS files do not include enrollments, and the CAHPS submarkets do not always

correspond to the HEDIS submarkets.

           Our sample includes plans with quality data for all six measures. Note the quality data is

measured at a single point in time, and it is matched to the panel data on plan enrollments. In

Section 5, we describe and utilize the panel data that is available for some of the quality

measures.

                                                                                                         x−x
15
     This normalization produces variables with a mean of zero and a standard deviation of 1, i.e. z =       , where
                                                                                                          s
 x is the sample mean and s the sample standard deviation. When calculating z-scores, we count the scores for
each plan only once, and include all plans with nonmissing values for that score. There is no substantive difference
between results obtained using normalized and raw scores.
16
   The unreported measures were obtained from the same source as mammography in 2000, and therefore pertain to
data from 1996-97.


                                                           12
        We obtain the minimum monthly enrollee premium for each plan and year from the

December Medicare Coordinated Care Plans Monthly Report for 1994-1998, and directly from

CMS for 2000-2002.17 We estimate 1999 premiums using the average of each plan’s 1998 and

2000 premiums, where available. We also construct an indicator variable that takes a value of 1

if a plan had an affiliate that was rated at least once by U.S. News. A plan is considered to have

such an affiliate if both the Medicare plan and the plan appearing in U.S. News had a common

carrier (e.g. CIGNA, Humana) and state; Medicare plans were not directly included in the U.S.

News publications.18

        Last, we use the annual March Current Population Survey (CPS) for 1994-2002 to obtain

the income distribution for individuals aged 65+. For each year, we calculate the quintiles of the

national income distribution. Next, we calculate the fraction of the elderly falling into each

quintile for every county. For counties that are too small to be identified in the CPS, we assign

figures obtained for the relevant state and year.

        Table 1 presents descriptive statistics for the complete plan-county-year dataset. During

the study period, HMO enrollment averaged 3,557 per plan-county, or just under 5 percent

percent of eligible enrollees in the county. Nearly two-thirds of the observations come from

plans whose affiliates were rated by U.S. News. Table 2 provides additional detail regarding the

number of competitors in each market and the variation in quality scores within markets. For

markets with more than one HMO, we calculate the difference between the maximum and

minimum reported quality scores in each market, and report the means in Table 2. The table


17
   The Medicare Coordinated Care Plans Monthly Reports are available at
http://www.cms.hhs.gov/healthplans/statistics/monthly/. Many plans offer multiple products with varying benefits
and premiums. We follow the literature and select the minimum premium.
18
   When the carrier name did not appear as part of the plan name, carrier identity was obtained by examining names
in prior and subsequent years, performing literature searches, and searching the Interstudy database of publicly-
reported data on HMOs. We do not incorporate the ratings measures reported by U.S. News due to the high number
of missing values.


                                                        13
reveals substantial variation in quality within markets. For example, in markets with 2

competitors, the mean difference in mammography scores is 8.85 points, or .86 after the scores

are normalized. In markets with 3 or more competitors, the mean spread between the highest and

lowest-scoring plan is 15.23 points. Table 3 presents a correlation matrix for the quality scores.

Mammography is highly correlated with unreported composite, but uncorrelated with

communicate and best care, the correlated subjective measures from the CAHPS survey.19




4          Analysis

Each participating market contains plans with different quality levels, so the report-card mandate

effectively created hundreds of “mini-experiments” that we use to identify the response to the

reported information. Some of the observed changes in enrollments following the mandate may

be due to market-based learning, so we control for the underlying trend toward highly-rated

plans. Assuming this trend would have continued in the absence of the report cards, we can

estimate the report-card effect by comparing the deviation from trend following the intervention.

We use the data on unreported scores to conduct a falsification exercise, i.e. to confirm that

consumers do not “react” the same way to unreported scores. In section 5 (Extensions and

Robustness) we use the limited panel data on quality to confirm that measured report-card

responses are not in fact responses to changes in contemporaneous quality.

4.1        Methods

We estimate a discrete choice demand model in which each Medicare enrollee selects the option

in her county that offers her the highest utility, including the “outside good” represented by




19
     These correlations have been noted by other researchers, e.g. Schneider et al. (2001).


                                                            14
traditional Medicare.20 As is well-known, the standard assumption of i.i.d. errors in consumer

utility produces stringent restrictions on the substitution patterns across options. Our utility

specification has a separate “nest” for HMOs to permit the substitution among HMOs to differ

from substitution between HMOs and traditional Medicare. We also allow individual income to

affect the propensity to select the HMO nest by interacting dummies for the individual’s income

quintile with the dummy for the HMO nest. 21 Quintiles allow more flexibility than a linear

interaction with income, and can be reasonably well-estimated given the CPS data available in

each market and year. The utility consumer i obtains from selecting plan j in market c(s)t and

nest g can be written

(1)      uijc(s)t = xjc(s)tβ - αpjc(s)t + ξjc(s)t +φiζg + (1-σ)εijc(s)t.

c(s)t denotes a county (within state s) during year t; the notation c(s) reflects the fact that some

variables are at the state rather than the county level. (We follow CMS and consider the county

as the market definition.) The xjc(s)t are observed plan-market-year characteristics (described

below), pjc(s)t is the monthly premium charged by the plan, ξjc(s)t represents the mean utility to

consumers of unobserved plan-market-year characteristics, ζg is a dummy for choices in nest g,

and εijc(s)t is an i.i.d. extreme value random error term.22 We define φi =diγ+υi, where di is a set

of dummies for the income quintiles, and υi is independent random term that reflects individual-

specific preferences for HMOs that are uncorrelated with income. As in Cardell (1997) and


20
   All Medicare enrollees have the same choice set within the county.
21
   Ideally, we would interact additional consumer characteristics, such as health status and wealth, with the choice of
the HMO nest; unfortunately, we lack county-year data on these characteristics, and the fixed effects in the
specifications preclude the use of more aggregated data. Empirically, however, these three measures are strongly
correlated. In addition, researchers studying healthplan choice in the Medicare population have found income to be
the strongest predictor of decisions (Dowd et al. 1994).
22
    Town and Liu (2003) point out that this premium should be expressed relative to the traditional FFS “premium,”
which can be viewed as the expected out-of-pocket costs associated with achieving the same benefits offered by an
HMO while enrolling in traditional FFS Medicare. They use Medigap premiums as an estimate of these costs.
These premiums are only available at the state-year level, however, so they would not affect the premium coefficient
in our models, which include state-year fixed effects.


                                                            15
Berry (1994), we assume the distribution of υi is such that υi + (1-σ)εijc(s)t is an extreme value

random variable. Under this assumption, the parameter σ will range between 0 and 1, with

values closer to 1 indicating the within-nest correlation of utility levels is high and values closer

to 0 indicating that substitution patterns do not differ across nests. The mean utility of traditional

Medicare, denoted by j=0, is normalized to zero.

       As compared to a reduced-form demand equation, our method not only emerges from a

structural model of choice but also corrects for changes in the choice set, e.g. those caused by

entry and exit. This model is particularly appropriate for our analysis because of the frequency

of healthplan exit in the post-BBA era. It generates consistent utility parameters that do not

depend on the specific competitors in a market. We can then use these parameters to measure

the effects of report cards, abstracting away from entry and exit that independently affect

enrollment. The model captures both movement across HMOs and movement between

traditional Medicare and HMOs.

       For our primary specification, we define the vector of observed plan-market-year

characteristics to be

(2)
                   3
                         [                                           ]
        x jc ( s ) t = ∑ score lj * f ( yeart ) + score lj * post tl + ω j + κ c ( s ) + ψ st .
                  l =1


       This specification includes a separate time trend for each reported score l to capture

learning about that score over time, as well as interactions between each score and a post dummy

to capture deviations from trend following the publication of individual scores. For

mammography and communicate, posttl takes a value of 1 beginning in 2000; for best care,

posttl equals 1 beginning in 2001. To determine the functional form for f(yeart), we estimate

three separate models in which the score terms in equation (2),




                                                               16
 3

l =1
       [
       l                       l         l
                                           ]
∑ score j * f ( yeart ) + score j * post t , are replaced with interactions between a single score and


dummies for every year, i.e. scorelj * τ t . This specification allows for a flexible learning pattern

and easy detection of post-reporting deviations from trend. We cannot include all scores jointly

in this manner as the data cannot identify 24 parameters at once (3 scores*8 year dummies), but

we use the results from these separate regressions to select f(yeart), and to inform our discussion

of the results.

           Before discussing the control variables, we have two additional conceptual remarks.

First, our model focuses on enrollee responses to report cards; for the most part, plan responses

are not incorporated into the analysis. There is, of course, potential for a supply-side response:

plans could choose to exit, invest in raising reported scores, reduce quality in unreported

dimensions, and/or advertise their quality (although this latter practice is unlikely as it attracts

high-risk enrollees).23 Only the last of these activities will be captured by our study.24 In the

final section, we discuss why we are unable to study supply-side responses, and why these

responses are likely to be small during the study period. The second point is that our main

specification explores the relationship over time between enrollment and reported quality, which

is measured at a single point in time. This model isolates the effect of the report cards by

controlling for the enrollment trend toward highly-rated plans that was evident prior to their


23
   Mehrotra et al. 2006, “The Relationship Between Health Plan Advertising and Market Incentives: Evidence of
Risk-Selective Behavior,” Health Affairs 25(1): 759-65. In fact, Medicare HMOs successfully recruited healthier-
than-average beneficiaries, a practice denounced as “cream-skimming” (Batata 2004, “The Effect of HMOs on Fee-
For-Service Health Care Expenditures: Evidence from Medicare Revisited, Journal of Health Economics 23(5):
951-63.) Melhotra et al find “the use of ads that are attractive to healthy patients increased nationally from the
1970s through the 1990s.” Features of such ads include small print, pictures of active seniors, and mentions of
“wellness programs.”
24
   If plans publicize quality prior to the release of report cards, this will be incorporated in the estimate of the
market-based learning trends and cause a downward bias in our estimated report-card effect. As noted, however,
quality advertising is unlikely, especially in a market where selection is critical for plan profits. Moreover, the
report-card response we find is sudden and occurs immediately following the release of the satisfaction score (one
year after the other two scores were released).


                                                        17
release. This trend is consistent with market-based learning, but there are other possible

explanations. In section 5, we describe a series of extensions and robustness checks we perform

to evaluate alternative hypotheses. Note these hypotheses affect the interpretation of the trend

coefficients, but not of the estimated report-card effects.

        In all specifications, we include plan fixed effects (ω j ) to capture time-invariant

differences in the unobservable quality of plans (as perceived by consumers), and county fixed

effects ( κ c ( s ) ) to capture time-invariant differences in consumer utility across markets. Such

differences can be driven by local demographics, economic conditions, and market structure.

For example, HMO penetration in the private sector is larger in urban counties and on the west

coast. To the extent that Medicare HMO penetration tracks private sector penetration, county

fixed effects will eliminate the time-invariant component of these differences across counties.

The county fixed effects also imply that we are examining the relationship between relative

quality scores within a county and plan market shares in that county. Because changes in

national or state economic conditions and regulations may be correlated with quality levels and

enrollment decisions, we also include state-year fixed effects, denoted by ψ st .

4.2 Estimation

To estimate the parameters of the utility function described by (1), we use the approaches

delineated in Berry (1994) and Berry, Levinsohn, and Pakes (1995), hereafter BLP. These

papers outline estimation methods that can be implemented using data on the distribution of

consumer characteristics in each market and the market shares of all products. Market shares are

a nonlinear function of the mean utility for products; the utility parameters are recovered by first

inverting the equations for market share, and then regressing mean utility on covariates. The

inversion formula has a closed-form solution in the case of a logit or a nested logit; in these



                                                   18
models, individual heterogeneity is assumed to be averaged out across the sample, so only the

aggregate unobserved characteristic (ξjc(s)t) remains.

            In our model, the individual’s income quintile is permitted to affect the mean utility of

plans within the HMO nest. Applying Berry (1994), we can write the following equation for

mean plan utility δ:

(3)      δ qjc ( s ) t ≡ ln( s qjc ( s ) t ) − ln( s0qc ( s ) t ) = αp js ( c ) t + x js ( c ) t β + ξ js ( c ) t + γ q + σ ln( s (q j / g ) c ( s ) t ) ,

where q denotes income quintiles, s qjc ( s ) t denotes absolute market share for plan j in county c

(within state s) and year t, s0qc ( s ) t denotes absolute market share for the outside good, and

s (q j / g )c ( s ) t denotes plan j’s market share among HMO enrollees in c(s)t. ( s (q j / g )c ( s ) t is also called the

“within group share.”) Note we have replaced φiζg in equation (1) with gamma γ q (q=1,2…5),

which economizes on notation given our specification. (Because we have one nest g we omit the

g subscript on γ q ).

            We do not observe s qjc ( s ) t and s0qc ( s ) t , so we cannot use (3) as our estimating equation.

The Appendix provides greater detail on how we recover parameter estimates. 25 Because the

choice of HMO conditional on selecting the HMO nest is a simple logit, the closed-form

market share inversion formula can be utilized as part of the procedure to derive mean utilities.

However, the choice between the outside good and the HMO nest is a function of individual

income, so no closed-form solution for this layer of the choice problem exists. Holding γ q fixed,

we apply BLP’s contraction mapping method to obtain the dependent variable for a linear

estimating equation:

(4)      δ jc ( s )t ≡ αp js ( c )t + x js ( c )t β + ξ js ( c )t + σ ln(s         ( j / g )c( s)t
                                                                                                     ).

25
     We thank Yongbae Lee for developing and implementing our estimation.


                                                                                         19
The derivation of this equation is presented in the Appendix. γ q is then estimated via 3-stage

GMM where the moment conditions are adjusted to reflect the possibility of serial correlation in

errors for the same plan-county.26

        Due to the large number of moment conditions (over 800) relative to the number of

observations (N=8,212), in practice our sample covariance matrix is singular. To obtain an

invertible matrix to use in the multi-stage GMM procedure, we use the sample covariance

estimator proposed by Ledoit and Wolf (2004). This estimator is designed to address precisely

our situation: many moment conditions relative to the sample size. The estimator is a weighted

average of the sample covariance matrix and an arbitrary diagonal matrix (e.g. the identity

matrix). The latter matrix ensures invertibility. Consistency is obtained by allowing the weight

on the sample covariance matrix to approach 1 as the sample size increases. We use Ledoit and

Wolf’s estimator to obtain an invertible matrix for our first n-1 iterations, and the sample

covariance matrix for all reported standard errors.

4.3 Identification

Our model assumes no time-varying omitted variable is simultaneously correlated with any

regressor as well as with enrollment. In this section, we discuss our efforts to address omitted

variables bias in all the regressors of interest.

        Both price and the within-group share in equations (3) and (4) above may be correlated

with ξjc(s)t, which represents the mean utility of unobserved shocks to plan quality. Because the

model includes plan, county, and state-year fixed effects, correlation between price and the error

term will only occur if changes in price for a given plan-county are correlated with changes in

quality for that plan-county, controlling for any changes that are common to all plan-counties in

26
  To use the terminology of BLP (1995), we modify the moment conditions so as to treat the sum of the moment
conditions for each plan-county as a single observation.


                                                      20
that state. Such a correlation would arise, for example, if United Healthcare of Illinois, which

operates in several Illinois counties, increased its unobserved quality and price only in Cook

County. Correlation between the within-group share and the error term will occur if changes in

plan j’s unobservable quality in a given county (controlling for changes common to the state) are

correlated with changes in how attractive plan j is relative to other plans available in that county.

         Given this potential for omitted variables bias, we include instruments for both measures

in all specifications. We follow Town & Liu (2003) in using the following 6 variables in our set

of instruments: the minimum, maximum, and mean payment amount by CMS for the other

counties in which plan j operates during year t; the mean number of competing plans in those

counties in year t, the one-year lagged number of hospitals in county c, and the one-year lagged

number of hospital beds in county c.27           The first four of these reflect the competitive conditions

the HMO faces in other markets. If there are economics of scale or scope in operating an HMO,

these conditions will be correlated with the costs of operating in county c. The county-level

hospital figures should also be correlated with HMO costs, as HMOs in counties with increasing

supply should be better-able to negotiate lower input prices.28 These cost-shifters are assumed to

be uncorrelated with unobserved quality.

         We also use functions of competitors’ characteristics as instruments for within-group

share. These will be valid instruments if competitors do not alter their product characteristics in

response to changes in plan j’s unobserved quality, and if competitors’ entry/exit decisions are




27
   Town & Liu include as instruments the minimum, maximum, and mean premium charged by plan j in the other
counties in which it operates in year t. In our data, premium does not vary across the counties in which plan j
operates. The difference appears to be due to a different definition of “plan.”
28
   However, the inclusion of county and state-year fixed effects in the regression leaves little variation in these
measures.


                                                         21
uncorrelated with changes in plan j’s unobserved quality.29 We use indicator variables for not-

for-profit ownership, chain membership, and whether the HMO is organized as an “Independent

Practice Association (IPA).”30 These variables are reported annually to CMS and are good

individual predictors of sjc(s)t|gc(s)t in separate first-stage regressions.

         We take all of the quality*time interactions to be exogenous. In the case of the learning

trends, this means we are ruling out any time-varying factors that affect enrollment decisions and

are also correlated with plan quality, except those that are common to a given state and year. For

example, if high-quality plans within each state are likelier to increase the benefits they provide,

increases in the popularity of such plans will reflect these changes and not learning about a given

quality level. Section 5 provides several tests of this demanding assumption. The identifying

assumption for the report-card effect is much less strenuous: it rules out the possibility of

changes in omitted factors timed to the report-card release and correlated with plan quality, again

excluding any changes occurring statewide. For example, if the BBA included other provisions

that differentially affected low- and high-quality plans within a given state, and these provisions

coincided with the release of the report cards, these would be included in the report-card effect.

We discuss this possibility in Section 5.4.

4.4      Results

We begin by examining the results from the specifications using a single reported score

interacted with individual year dummies. Figure 3 plots the estimated coefficients on these

interactions; the data is presented in Appendix Table 2. The vertical line in each graph signifies



29
    The inclusion of plan fixed effects relaxes the usual assumptions substantially; rather than positing that observable
competitor characteristics are uncorrelated with unobservable plan characteristics, we only require changes in
observable competitor characteristics to be uncorrelated with changes in unobservable plan characteristics.
30
   IPA-model HMOs contract with independent physicians and groups of physicians, and they tend to offer a broader
network of providers than “staff-model” or “group-model” HMOs, in which physicians are fully or mostly employed
by the HMO.


                                                          22
the start of the post period for each measure. We draw three conclusions from these graphs.

First, mean utility for plans with higher scores is increasing throughout the study period.

Second, the only measure that clearly deviates upward from trend during the post-period is best

care. Prior to the report-card intervention, plans with high best care scores were generating

more utility over time, but at a decreasing rate. In the first year after best care was reported, the

effect of best care on utility increased more than it had over the three prior years combined.

Third, it appears that a log time trend is more appropriate than a linear time trend for modeling

the underlying increase in utility for plans with high scores.31

        The first column in Table 4 presents results from a model with log trends for each

reported score as well as interactions between each reported score and post; i.e. the specification

in which x jc ( s ) t is represented by equation (2) above. The reported coefficients are estimates of

the mean marginal utilities associated with the corresponding scores in different years. Thus the

positive coefficients on the trend variables imply that consumers value higher-quality plans more

over time (at a decreasing rate). The trend for mammography is statistically significant at p<.01,

the trend for communicate is marginally significant, and the trend for best care is smaller and

imprecisely-estimated. As we will see below, the point estimates and statistical significance of

the individual trends fluctuate from one specification to the next, but combining the measures

into a composite and estimating a single market-learning coefficient produces sizeable,

significant, and stable estimates.

        The post interactions reveal that plans with high best care scores generate significantly

more utility following the publication of their scores. Publicizing the scores for mammography

and communicate does not have a significant impact on utility; we easily reject the null

31
 The concave trend is consistent with a learning model in which a decreasing percentage of the population learns
each year.


                                                        23
hypothesis of equality between either coefficient and best care*post.32 The estimate of the

nesting parameter, .0.838 (.046), strongly indicates a separate nest for HMOs is appropriate.

         In this and all subsequent specifications, the premium coefficient is small and imprecisely

estimated. We attribute this result both to premium compression (discussed above), and to the

inclusion of fixed effects that absorb nearly all of the variation in premiums that helps to identify

this parameter in other studies.33 As described in Section 4, these fixed effects are important for

mitigating omitted variables bias in the score coefficients. The casualty is the price coefficient.

Unfortunately, without it we are unable to monetize the value of changes in ratings, as in

Chernew et al (2006). As a result, we report all findings in terms of market shares.

         The estimates of the income quintile dummies appear at the bottom of each column in

Table 4.34 In all specifications, the γ̂ q increase in q, and γ̂ 3 and γ̂ 4 are statistically

distinguishable from the top (omitted) quintile. These estimates indicate that the poorest and

wealthiest beneficiaries avoid Medicare HMOs. The low penetration among the affluent is easily

explained: the carrots of better benefits and lower copays are least valuable for this population.

They are better able to finance out-of-pocket expenses, and most likely to have supplemental

“Medigap” coverage, often subsidized by former employers. At first blush, the finding that the

least affluent are not significantly more likely to enroll in Medicare HMOs than the most affluent

is surprising. However, there are at least two plausible explanations for this result.




32
   The negative coefficient estimate on communicate*post, though statistically insignificant, reflects the concave
learning trend for communicate revealed in Figure 3.
33
   A regression of premiums on all of the fixed effects in our model produces an R-squared of 0.79. If we estimate
our model without state-year fixed effects, we obtain a negative and statistically significant price coefficient.
34
  Some county-years have few corresponding observations in the CPS, leading to noisy estimates of the income distribution. We
experimented by replacing the county-year income distribution with the state-year income distribution for those county-years
with fewer than 5, 10 or 20 income observations in the CPS. This had little impact on the estimated parameters, so we retained
the original data.




                                                             24
         First, recent data shows that 15 percent of Medicare beneficiaries are also eligible for

Medicaid, the public insurance program for the indigent.35 Medicaid is responsible for all

copays for so-called “dual eligibles,” and these individuals are also entitled to the more

comprehensive benefits provided under Medicaid (including prescription drug coverage, which

was not offered in traditional Medicare during our study period). Additional groups of low-

income beneficiaries (known as “Qualified Medicare Beneficiaries” (QMBs) or “Specified Low

Income Medicare Beneficiaries” (SPLIMBs)) who do not qualify for Medicaid can receive

subsidies to cover charges associated with traditional Medicare. Hence the poorest beneficiaries

might not benefit from Medicare HMOs.36 Second, lower-income beneficiaries may find it

particularly challenging to enroll in a Medicare HMO. Whereas enrollment/reenrollment in

traditional Medicare is virtually automatic, selecting and enrolling in a Medicare HMO requires

significant cognitive skills and effort. Our results are consistent with a 1988 survey of Medicare

beneficiaries in the Twin Cities metropolitan area, which had several Medicare HMOs at the

time. This survey found the lowest and highest-income beneficiaries were least likely to enroll

in Medicare HMOs (Dowd et al 1994).

         Column 2 of Table 4 presents the results obtained by adding unreported

composite*ln(year) and unreported composite*post to the specification in column (1). Again,

only best care deviates significantly from trend in the post-reporting period. Given the high

correlation between mammography and unreported composite, it is unsurprising that the

coefficients on the interactions with these scores are noisily estimated when jointly included.

However, we again find significantly different responses to best care and communicate, despite

35
  http://www.medpac.gov/publications/congressional_reports/Jun04DatabookSec2.pdf
36
  This argument assumes the utility from FFS care with no copays exceeds that of managed care. Although some
enrollees may prefer services provided by managed care companies (e.g. a primary physician who is apprised of all
health concerns in her role as gatekeeper), surveys suggest the lower out-of-pocket expenses/greater benefits are the
main attraction (Gold et al 2004).


                                                         25
the high raw correlation of these satisfaction scores. To explore why, we extracted additional

measures from the CAHPS survey of consumer satisfaction, and find that one of these measures,

“ease of obtaining specialist referrals,” is highly correlated with the component of best care that

is uncorrelated with communicate. Further analysis confirms this factor is driving the observed

responsiveness to best care.

       Given that the market-based learning coefficients on all scores in column 2 are of similar

magnitude, and that none is precisely estimated, column 3 replaces the individual trends with a

trend for composite, which is the average z-score of all 6 measures (i.e. the 3 reported scores as

well as the 3 unreported scores). This trend is statistically significant at p<.001, and the pattern

of post interactions is unchanged. We use this parsimonious specification for the simulations,

extensions, and robustness checks that follow.

4.5 Simulations

The magnitudes of the coefficients reported in Table 4 are not readily interpretable. However,

we can use them to simulate how enrollee choices in hypothetical markets change over time and

in response to the publication of report cards. Any simulation will be sensitive to the

assumptions chosen. Our assumptions enable us to focus on the absolute and relative importance

of market-based and report-card-induced learning.

       In all scenarios we consider, we predict market shares in the presence and absence of the

Medicare report cards. We also consider the effects of varying the quality and number of plans.

We abstract away from time trends unrelated to consumer learning (the state*year fixed effects)

and do not allow for entry and exit. As in the estimation models, quality for each plan is fixed

over time. To the extent the report-card mandate stimulated quality improvements and/or exit by

low-quality plans, our simulations underestimate the total effect of report cards. The proportion




                                                 26
of individuals in each income quintile is set to 20 percent. Below, we discuss the impact of

varying this distribution. We set the premiums for all plans equal to zero and treat these as

exogenously-determined; due to the small estimated premium coefficient, these assumptions do

not impact our results. We select a plan fixed effect (i.e. a measure of unobserved, time-

invariant plan-level quality) that generates an average market share of 2.74 percentage points, the

nationwide average in 1994. Normalizing the market shares of all plans to this initial value

enables us to easily compare growth patterns across plans and markets. Our assumptions ensure

that plans with the same quality scores will have the same average market shares in any given

year, though actual shares will differ due to the idiosyncratic error term.

           All simulations use parameter estimates and residuals from the model presented in Table

4, Column 3. Each simulation incorporates a randomly-drawn vector of residuals representing

the unobserved, time-varying quality of the plans.37 We perform 10,000 simulations for each

scenario and year and report the mean predicted market shares and standard errors of these

means.

           The first set of scenarios is designed to illustrate learning in typical markets. We look to

the data to select the characteristics of such markets. Referring to Table 2, the percent of county-

year markets with 1, 2, and 3+ HMO plans on offer is 55, 20, and 14, respectively. However, the

percent of HMO enrollees in such markets is 11, 13, and 75, respectively.38 For completeness,

we present results for hypothetical markets with 1, 2, and 3 HMO plans.

           In the hypothetical monopoly market, we consider 3 alternative quality levels for the sole

plan. The “low quality” plan scores 1 standard deviation below the mean on all quality

measures, the “medium quality” plan scores at the mean, and the “high quality” plan scores 1


37
     The number of elements in this vector is determined by the number of plans in the scenario.
38
     There are 4107 county-year markets in our dataset. Due to rounding error, the percentages do not sum to 100.


                                                          27
standard deviation above the mean. The first page of Table 5 presents results for these markets

under the assumption of no report cards, i.e. the coefficients on the post interaction terms are set

to zero and consumer responses to quality derive solely from market-based learning. The second

page incorporates the impact of the report cards; that is, we consider how publicizing the quality

scores affects consumer demand beyond what would be predicted by market-based learning

alone. Note that in all scenarios, the standard errors of the predicted market shares are large

relative to the mean changes in predicted market shares. Thus, the unobserved random shock to

quality in any given period overwhelms the movement in market share that is predicted to come

about as a result of learning. Nevertheless, the estimated effects of market-based and report-

card-induced learning are sizeable.

     By construction, in monopoly markets a medium-quality plan sees no systematic change in

its market share due to learning; this plan earns quality z-scores of zero and beneficiaries are

neither tempted to enroll nor disenroll as they learn this information. If the plan is of high (low)

quality, market-based learning results in a substantial increase (decrease) in market share over

the 9-year study period. The exact amount is on the order of 30-45 percent of enrollment. 39

The second part of Table 5 illustrates that report cards provide an extra boost (penalty) to the

high- (low-) quality plans. The size of the boost is 25 to 45 percent of the market-learning

effect.40

     Adding a second plan magnifies the impact of learning, as beneficiaries not only switch

to/from traditional Medicare but also among available plans (assuming these plans have different


39
   Although the larger estimate is obtained in the high-quality case, suggesting that the reward for high quality
exceeds the penalty for low quality, this result is largely driven by the functional form we assume and hence we do
not make this inference. (When inverting the market share equation, we take a convex transformation of the quality
scores. This has the effect of magnifying increases (or equivalently, minimizing decreases) in quality.)
40
   The relative magnitude is calculated as (Mean market share in 2002 with report card – Mean market share in 2002
without report card)/(Mean market share in 2002 without report card – Mean market share in 1994 without report
card).


                                                        28
quality scores). Table 5 contains estimates from a duopoly setting with one low and one high-

quality plan, as defined above. Figure 4 depicts the results graphically. The solid lines

correspond to mean predicted shares without report cards, and the dotted lines include the report-

card effects.41 The mean market share of the high-quality firm is predicted to double due to

market-based learning between 1994 and 2002. Most of this increase comes at the expense of

the low-quality HMO, with roughly one-third of additional enrollees coming from traditional

Medicare. The report-card effect on market shares is also larger in the duopoly market, but

only in absolute terms. Relative to market-based learning, the report-card effect is 17 to 30

percent as large. The smaller relative importance of report cards in duopoly markets --

particularly for the low-quality plan -- reflects the smaller pool of enrollees in the low-quality

plan by the time best care is released in 2002. There are simply very few enrollees left to leave.

For the same reason, most of the report-card induced increase in enrollment in the high-quality

HMO comes from traditional Medicare, with only one-third due to further declines in the market

share of the low-quality plan.

        Last, we introduce a third plan of medium quality. As compared to the medium-quality

monopolist, this plan loses market share over time even absent the report cards because some of

its enrollees switch into the high-quality plan. The high-quality plan therefore sees bigger

enrollment gains over time and as a result of the report card. The magnitude of the report card

effect relative to the market-based learning effect ranges from 16 percent (for the low-quality

plan) to 70 percent (for the medium-quality plan). This is consistent with the discussion above:



41
  Note that our specification does not permit the impact of report cards to vary over time, although the graph
suggests otherwise. The reason for this illusion is that different scores are released at different points in time.
Mammography and communicate are released in Medicare and You 2001, and best care appears in Medicare and
You 2002 (together with updated mammography scores). The estimated coefficient on communicate*post is slightly
negative, which produces the downward bump in post-report-card enrollment in the high-quality plan.


                                                        29
the relative importance of report cards depends on how big the gains (losses) are by 2001 (when

the best care scores were released).

        While report-card learning is associated with sizeable swings in HMO market share, the

absolute share of enrollees switching plans due to report cards is likely quite low. The exact

figures cannot be obtained without individual-level data, as we can only simulate final market

shares. In the final scenario in Table 5, net switching associated with the report-cards as of 2002

is only 1.24 percent of beneficiaries.42

        Table 6 contains results from a second set of scenarios designed to illustrate the effect of

varying the absolute and relative quality levels of plans within a market. For simplicity, we

focus on hypothetical duopoly markets. We allow each plan to take on quality scores at the 25th,

50th, and 75th percentiles. This generates 9 unique combinations of scores. All other

assumptions are unchanged. To consolidate the presentation of our results, we present only the

2002 mean market shares (and standard errors) for both plans under each score combination,

assuming report cards are distributed. The 2002 shares should be contrasted with the initial 1994

market share of 2.74 points for each plan; to facilitate these comparisons, the implied percent

change in market share is reported in the bottom panel of Table 6.

        The results in Table 6 follow from the scenarios already discussed. The entries along the

diagonal (quality of both plans is the same) illustrate that the combined market share of HMOs

increases in their average quality. The gain to any individual plan from higher quality is largest

when it is maximally differentiated from its competitor. When these results are compared to the

same scenarios without report cards (tables available upon request), we see that report cards are

42
  This estimate is obtained by comparing the final market shares with and without report cards in 2002. We add the
absolute value of market-share changes due to the report-card, and divide the sum by two. (Note this estimate is
only an approximation because we apply the formula to the mean final market shares rather than to the market
shares from each individual simulation. We do this in the interest of clarity. The estimate we report for the
nationwide response is calculated by averaging the results from each simulation.)


                                                       30
associated with reallocations that are 19 to 85 percent as large as the reallocations associated

with market-based learning. (Consistent with the discussion above, the lowest estimate is

obtained from a market with one low-quality plan and one high-quality plan).

        In a third set of (unreported) scenarios, we examine the relationship between individual

heterogeneity (captured via income) and learning patterns. We find that varying the proportion

of individuals in each income quintile has little impact on predicted market shares. This result is

due in part to the relative magnitude of the income coefficients, and in part to our utility

specification, in which income affects the choice of the HMO nest but not the choice within the

HMO nest. Thus, changing the hypothetical income distribution can affect the aggregate market

shares of HMOs, but not the relative shares. Given the low propensity for traditional enrollees of

all income groups to switch into HMOs, the impact on market shares of a change in the income

distribution is negligible.

        In summary, the simulations suggest that both market-based learning and report cards

generated substantial swings in the market shares of individual HMOs. Although the impact of

report cards on these market shares was substantial, it was smaller than that of cumulative

market-based learning during the study period, and the proportion of those exposed to the report

cards who actually responded was likely fairly low. The exact figures cannot be obtained

without individual-level data, as we can only simulate final market shares. We can estimate net

switching however, and this amounts to 3.1 percent nationwide.43 Given the large estimate of the

nesting parameter, switching by Medicare HMO enrollees was likely much greater than

switching by traditional Medicare enrollees.




43
  This national estimate is obtained by comparing market shares in 1999 (the year before report cards were released)
and simulated market shares under the assumption report cards were available, ceteris paribus.


                                                        31
         To compare the net switching figure with estimates from other settings, it is helpful to

normalize it by the total national market share of Medicare HMOs in 2002, 12 percent. The

resulting ratio is fairly large relative to estimates from other settings, such as GM employees (3.9

percent predicted to have switched due to HMO report cards/40 percent in HMOs) and federal

retirees/survivors of retirees, (0.7 percent/12.5 percent). The latter estimate is especially

interesting because of the similarity of the study populations. Part of the difference is likely due

to the fact that Medicare & You was directly delivered to all beneficiaries’ mailboxes, whereas

report cards for federal government HMOs had to be actively located (e.g. by purchasing U.S.

News & World Report).




5        Extensions and Robustness

Our baseline model is designed to provide an unbiased estimate of the effect of report cards. The

trend variables control for the possibility that enrollees are acting on reported scores ahead of

their release; we attribute such behavior to market-based learning. In this section, we describe a

series of extensions and robustness checks we perform to evaluate alternative hypotheses for this

trend.

5.1      Heterogeneity in Learning

We begin by considering the mechanisms through which market-based learning may be

occurring. Our intepretation of the trend coefficients as evidence of market-based learning will

be bolstered if the data are consistent with hypotheses about how this learning takes place. We

examine three potential channels for market-based learning: friends and family (proxied by

stable population, the share of the 1995 county population still living in the county in 2000);

prior HMO experience (proxied by HMO penetration, the county Medicare HMO penetration



                                                 32
rate in 1994, the start of the study period); and other published report cards (proxied by

appearance of affiliated plans in the U.S. News “Best HMO” reports).44 Descriptive statistics for

these proxies are included in Table 1. If word of mouth is a source of learning, we would expect

a positive coefficient on the triple interaction term, composite*ln(year)*stable population,

assuming population stability is correlated with the exchange of information among

beneficiaries. If enrollees learn from prior HMO experience, we would expect diminished

market-based learning during the study period and therefore a negative coefficient on

composite*ln(year)*HMO penetration. Finally, if learning is facilitated by other sources of

report-card data, we expect a positive coefficient on the interaction between composite*ln(year)

and U.S. News, an indicator for whether all plans in a county have an affiliated plan that

appeared at least once in the U.S. News publications.

         Table 7 reports the results of adding each of these terms, first separately and then jointly,

to the baseline specification. The original results from Table 4, column 3 are repeated in column

1, followed by estimates obtained when adding interactions with the county z-score for stable

population (column 2), the county z-score for HMO penetration (column 3), the U.S. News

indicator (column 4), and all three together (column 5). Note that main effects for the learning

proxies are not needed due to the inclusion of county fixed effects in all specifications.

         The data support all three mechanisms, with the strongest evidence for learning

facilitated by other report cards. The magnitude of the learning coefficient in markets with

complete U.S. News coverage is nearly double that in markets with incomplete or no coverage.45

A one-standard-deviation increase in stable population is associated with an increase of roughly


44
  County demographic characteristics are from the U.S. Census Bureau and the 2002 Area Resource File.
45
  We do not incorporate the ratings measures reported by U.S. News due to the high number of missing values. The
correlation between composite and the overall U.S. News rating (which ranges from 1 to 4 stars) is .64 for plans with
data from both sources.


                                                         33
35 percent in the learning coefficient, while a one-standard-deviation decrease in prior HMO

experience is associated with an increase of 19 percent. The coefficients on the score*post

variables are not materially affected by the inclusion of the new interactions.

        To further examine heterogeneity in market-based learning and report card effects, we

also considered interactions with county-level demographic measures such as the fraction of

college graduates and the share of women aged 65-74 (who may be particularly interested in

mammography scores of Medicare HMOs). Studies have found that education is correlated with

the use of healthcare information (e.g. Feldman et al 2000). We find no significant relationships

between these measures and the pace of learning.

5.2     Specification Checks Using Contemporaneous Quality

Our estimation strategy uses quality measured at a single point in time (i.e., the quality scores

actually reported in the Medicare & You handbooks). Ideally, we would add controls for

contemporaneous quality. The coefficients on these controls would reflect “real time” market-

based learning, while the coefficients on the reported measures would capture learning due to the

report cards. Unfortunately, panel quality data is limited to a subset of measures, plans, and

years.46 However, we are able to estimate a model in which

         x jc ( s ) t = mammography j ,t −1 + bestcare j ,t −1 + reported mammography j * post tm
(5)                                                                                                 .
                + reported best care j * post tb + ω j + κ c ( s ) + ψ st

As before, posttm takes on a value of 1 in 2000-2002, and posttb takes on a value of 1 in 2001-

2002.47 Contemporaneous quality is lagged by one year due to the discrete nature of the data

available; the earliest that beneficiaries can respond to quality measured during calendar year


46
   Data on mammography is available for 1996-2001, on communicate from 1998-1999, on best care for 1998-2002,
and on unreported composite for 1996-1998.
47
   Note that reported mammography and reported best care are the measures labeled as mammography and best care
in the main specifications.


                                                        34
1997 is 1998. Given the data limitations, the model can only be estimated using observations

from 1999-2002. All instruments are the same as in the main model. Descriptive statistics for

the panel data appear in Appendix Table 3.

         The results, presented in Table 8, corroborate our main findings. The coefficient

estimates on both contemporaneous quality measures are positive and statistically significant,

and the best care estimate is similar to that obtained in all prior specifications.48 Unlike prior

results, the mammography*post coefficient in this model is positive and statistically-significant.

However, the estimated nesting parameter is significantly different from 1, suggesting the

modeling assumptions are not satisfied in this sample. We are therefore hesitant to emphasize

the findings from this specification.

5.3      Plan Benefits

The identifying assumption of the main specification is that no omitted, plan-specific, time-

varying factor is correlated with both reported quality and enrollment decisions. Apart from

contemporaneous quality (addressed above), plan benefits are the most likely candidate for such

a factor. If high-quality plans are more or less likely to increase benefits over time, the trend

variable will reflect these characteristics as well as learning about quality. Similarly, if plans

react to high reported scores by lowering plan benefits (because they can “afford” to do so in

light of their scores), the post interaction terms will be downward-biased.

         To examine the possibility that changes in benefits are biasing the coefficient estimates,

we assemble panel data on prescription drug benefits offered by plans. Prescription drugs


48
  Because specification 3 is limited to 1999-2002, we use the mammography data from 1996-2001 to further
confirm that changes in contemporaneous plan quality are not producing the enrollment trend toward highly-rated
plans. If plans with high initial quality are more likely to improve their benefits over time, consumers’ valuation of
these improvements will be captured in the market-based learning term. We therefore regress the change in
mammography between 1996 and 2001 on reported mammography (which is measured in 1997). We obtain a
coefficient estimate of 0.08 (0.12), providing little support for this alternative explanation.



                                                          35
accounted for one-third of direct out-of-pocket spending by Medicare beneficiaries in 1999, and

likely more for beneficiaries without supplemental insurance policies, the primary target market

for Medicare HMOs.49 Town and Liu (2003) estimate that 45 percent of the consumer surplus

generated by the Medicare HMO program in 2000 was due to prescription drug coverage

provided by (some of) the plans. For 1994-2000, we have an indicator of drug coverage for the

“base” option within each plan, provided by Town and Liu.50 For 2000-2004, CMS provided us

with indicators of drug coverage for all options within a plan, but we lack the “base” identifier

included in the earlier data.51 However, the median indicator for each plan in 2000 matches the

base plan indicator in 2000 fairly well (sample mean of .85 vs. .83, respectively), so we use the

median indicator for 2001-2004.52

           Column 4 in Table 4 presents the results from the main specification with the addition of

this drug coverage indicator. The coefficient estimate on drug coverage is positive but

imprecisely estimated, and the magnitudes of the coefficients of interest are not materially

affected. Although prescription drug coverage is but one of the unobserved plan characteristics in

our models, this analysis suggests that unobserved changes in plan benefits are not driving the

results.




49
   “Direct” out-of-pocket spending excludes premium payments for Medicare and supplemental insurance policies.
(Source: “Program Information on Medicare, Medicaid, SCHIP, and other programs of the Centers for Medicare &
Medicaid Services,” Office of Research, Development, and Information, June 2002.)
50
   In 1999 and 2000, this indicator varies slightly across counties, so we use the maximum indicator for each plan-
year. Town and Liu obtained the 1994-1998 data from the Medicare Coordinated Care Plans Monthly Reports,
ibid., and the 1999-2000 data from the Medicare Compare Database.
51
   We obtained detailed benefits data for all options offered by participating plans in 2000-2004 by direct request to
CMS. The base plan is not identified, nor is enrollment data (which might be useful in identifying this plan)
included.
52
   Note that any systematic change in the drug coverage indicator between 2000 and 2001 will be captured by the
year dummies.


                                                          36
5.4     Other Robustness Checks

The BBA included several reforms to the Medicare program, some of which may have induced

the observed associations between reported scores and enrollments. We identified a key, testable

candidate: a series of provisions designed to increase Medicare HMO penetration in rural

counties.53 If rural counties have particularly low or high-quality plans, our learning coefficients

will reflect these other reforms. We find that 23 percent of the plan-county-year observations in

our data are from such counties. The results are unchanged when these observations are

dropped. In addition, running the model using only the rural observations yields very similar

coefficient estimates.

        We also considered three additional specification checks. First, we replaced standardized

quality scores with actual scores, and found the results were robust to this change. Second, we

replaced the premium with a binary price indicator coded as 1 for a positive premium. (Using

data on plan switching by University of California employees, Buchmueller and Feldstein (1997)

found that most switched out of plans requiring premiums to free plans.) The premium

coefficient was again small and imprecisely estimated. Third, we confirmed that the results are

not affected by changing post to a common definition for all scores (either taking a value of 1

beginning in 2000, or in 2001).



6       Conclusions

Governments often evaluate the quality of various products and services and publish such

information for consumers. The value of these initiatives depends, in part, on the pace at which

consumers learn about quality in their absence. The health insurance market, through which

53
  These reforms are summarized in Schoenman (1999). We use the “urban influence” measure developed by the
U.S. Department of Agriculture to identify urban areas (defined as counties located in metropolitan areas).


                                                     37
nearly 13 percent of GDP flows, is perhaps the most important laboratory for these government

initiatives.54

         Using panel data on Medicare HMOs and a discrete choice demand model, we examine

whether and how Medicare enrollees learn about the quality of Medicare HMOs. We arrive at

four main conclusions. First, between 1994 and 2002 Medicare enrollees were switching into

higher-quality plans independently of the government report cards issued in 1999 and 2000,

where quality is measured as a composite of the 6 available audited quality scores. This trend,

which we call “market-based learning,” was strongest in markets in which U.S. News provided

report cards, and in which migration and prior HMO experience was relatively low. These

findings suggest that market-based learning is facilitated through the private release of report

cards, word-of-mouth, and prior experience. The evidence for market-based learning implies

that the effect of report cards is typically overestimated.

         Second, after controlling for market-based learning, we still find a response to the

Medicare report cards. The report-card effect is entirely due to beneficiaries’ responses to

consumer satisfaction scores; other reported quality measures such as the mammography rate did

not affect enrollment. Given that public report cards are often justified on the grounds that

individuals’ subjective opinions are not good measures of the true quality of health care, it is

surprising that satisfaction scores were included in the report cards, and potentially disconcerting

that consumers ignored an alternative, objective measure of quality that was also provided. In

our data, enrollee satisfaction is uncorrelated with the mammography rate as well as other

measures that are believed to reflect best practices in disease screening and prevention. This fact

has been documented in several studies of quality measures (e.g. Schneider et al 2001 and Rao et

54
  This figure is calculated as the ratio of total national health expenditures, less out-of-pocket expenditures, divided
by GDP. Because out-of-pocket expenses are often determined by health insurers, this is certainly an underestimate.
Source: U.S. Statistical Abstract (1996), Table 118.


                                                          38
al 2006). Further analysis of unreported satisfaction measures suggests “ease of obtaining

specialist referrals” is the component of satisfaction driving our results. Whether this is

positively correlated with better health is an open question. Regardless, the emphasis on average

enrollee satisfaction creates an incentive for plans to maximize ratings by directing resources

toward average enrollees and away from outliers with catastrophic or expensive chronic

conditions, precisely the individuals for whom insurance is most valuable.55

         Third, we find report-card interventions as well as market-based learning have the

greatest impact in markets with providers of varying quality levels. Though obvious, this point

implies interventions will be most valuable when multiple firms are present, and when these

firms are differentiated. It is of course possible that the act of reporting quality data will

ultimately lead to differentiation on reported dimensions even if little differentiation is present ex

ante.

         Fourth, our estimates suggest that the Medicare report cards produced substantial swings

in market shares among Medicare HMOs (i.e. the “within HMO nest shares”), but only drew a

small fraction of enrollees in traditional Medicare into Medicare HMOs. This result is consistent

with prior research in the private sector (using PPOs as the outside option), and suggests that

quality report cards alone will be insufficient to persuade Medicare enrollees to abandon

traditional Medicare for the Medicare HMO program (currently known as Medicare Advantage).

         Although our findings pertain to a large and important population, we caution against

extrapolating the results to other settings. Studies of healthplan choice reveal that plan switching

occurs less frequently among the aged (e.g. Jin and Sorensen 2006). The healthcare needs,

55
  Prior research on the effect of enrollee satisfaction on health plan choices is mixed. Abraham et al. (2006) find an
employee’s self-rated satisfaction with her healthplan is not associated with her propensity to switch, while
Buchmueller and Feldstein (1997) find benefit managers’ rating of enrollee satisfaction is associated with enrollees’
propensity to switch.



                                                         39
preferences, and learning patterns of the elderly might differ significantly from those of the

nonelderly.

       Evaluating the aggregate welfare effects of government report cards requires complete

estimates of supply-side responses, as well as estimates of the effects of healthplan quality on

health, and of health on welfare. The latter two subjects are clearly beyond the scope of this

paper. As for the former, examining plan responses to Medicare & You is difficult due to the

absence of pre-mandate quality data, as well as simultaneous changes in Medicare payment rates.

Setting aside data and identification concerns, a priori there are several reasons to expect a small

response by plans during our study period. First, plans were required to report hundreds of

measures, and CMS did not announce which would be publicized to enrollees. As compared to

restaurant hygiene inspections, where a final summary grade is posted and the weights on the

component scores are known, this setting made it difficult for suppliers to figure out which

dimensions to improve. Second, plans may not have anticipated a significant enrollee response

to the quality data, both because of assumptions about behavior of the elderly and/or because

they underestimated CMS’ commitment to disseminating the data. As more recent data

becomes available, it will be possible to see whether plans focus disproportionately on improving

their scores on measures included in Medicare & You. Investigating the extent to which firms

“teach to the test” and skimp on unreported quality is an important area for future research, and

one of many inputs that will be needed to estimate the welfare effects of measuring and publicly

disclosing quality information.




                                                 40
References

Abraham, J.M, Feldman, R., Carlin, C. and J. Christianson (2006), “The Effect of Quality
      Information on Consumer Health Plan Switching: Evidence from the Buyers Health Care
      Action Group,” Journal of Health Economics, 25(4): 599-802.

Ackerberg, D. A. (2002), “Advertising, Learning, and Consumer Choice in Experience Good
      Markets: A Structural Empirical Examination,” International Economic Review, 44:1007-
      1040.

Beaulieu, N. D. (2002), “Quality Information and Consumer Health Plan Choices,” Journal of
       Health Economics, 21:43-63.

Buchmueller, T.C. and P.J. Feldstein (1997), “The Effect of Price on Switching Among Health
     Plans,” Journal of Health Economics, 21(1): 43-63.

Berry, S. T. (1994), “Estimating Discrete-Choice Models of Product Differentiation,” Rand
       Journal of Economics, 25(2): 242-262.

Berry, S. T., Levinsohn, J., and A. Pakes (2005), "Automobile Prices in Market Equilibrium",
       Econometrica, July 1995, 841-890.

Cardell, N.S. (1997), “Variance Components Structures for the Extreme Value and Logistic
       Distributions,” Econometric Theory, 13(2): 185-213.

Centers for Medicare and Medicaid Services (2002), “Program Information on Medicare,
       Medicaid, SCHIP, and other programs of the Centers for Medicare & Medicaid Services,”
       http://www.cms.hhs.gov/charts/series/sec3-b3.pdf.

Centers for Medicare and Medicaid Services (2004), “45 Day Notice for 2004 M+C Rates,”
       www.cms.hhs.gov/healthplans/rates/2004/45day-section-a.asp.

Chernew, M., Gowrisankaran, G., McLaughlin, C. and T. Gibson (2004), “Quality and Employers’
      Choice of Health Plans,” Journal of Health Economics, 23: 471-492.

Chernew, M., Gowrisankaran, G., and D. Scanlon (2006), “Learning and the Value of Information:
      Evidence from Health Plan Report Cards,” NBER Working Paper 8589.

Dowd, B., Moscovice, I., Feldman, R., Finch, M., Wisner, C., and S. Hillson (1994), “Health Plan
      Choices in the Twin Cities Medicare Market,” Medical Care, 32(10) 1019-1039.

Feldman, R., Christianson, J., Schultz, J. (2000), “Do Consumers Use Information to Choose A
      Health Care Provider System?” Millbank Memorial Quarterly, 78(1): 47-77.




                                              41
Gold, M., Achman, L., Mittler, J., and B. Stevens (2004), Monitoring Medicare+Choice: What
       Have We Learned? Findings and Operational Lessons for Medicare Advantage, August,
       www.mathematica-mpr.com/publications/PDFs/monitor.pdf.

Goldstein, E. (1999), “Assessment of the National Medicare Education Program: Supply and
       Demand for Information,” Health Care Financing Review, 21 (1): 129-131

Goldstein, E., Teichman, L., Crawley, B., Gaumer, G., Joseph, C., and L. Reardon (2001),
       “Lessons Learned from the National Medicare & You Education Program,” Health Care
       Financing Review, 23(1): 5-20.

Hastings, Justine and Jeffrey M. Weinstein (2007), “No Child Left Behind Rules Raise Student
       Performance,” NBER Working Paper 13009.

Hubbard, Thomas N. (2002), “How Do Consumers Motivate Experts? Reputational Incentives in
      An Auto Repair Market,” Journal of Law and Economics, 45: 437-468.

Israel, Mark (2005), “Services as Experience Goods: An Empirical Examination of Consumer
        Learning in Automobile Insurance,” American Economic Review (forthcoming).

Jin, Ginger Z. (2002), “Did Quality Information Matter? Evidence from Medicare 1993-
        1998”, mimeo, University of Maryland.

Jin, Ginger Z. and Phillip Leslie (2003), “The Effect of Information on Product Quality: Evidence
        from Restaurant Hygiene Grade Cards,” Quarterly Journal of Economics, 118(2): 409–51.

Jin, Ginger Z. and Alan Sorensen (2006), “Information and Consumer Choice: The Value of
        Publicized Health Plan Ratings,” Journal of Health Economics, 25(2): 248-275.

Lederman, Mara (2007), “Do Enhancements to Loyalty Programs Affect Demand? The Impact of
      International Frequent Flyer Partnerships on Domestic Airline Demand,” RAND Journal of
      Economics, forthcoming

Ledoit, O. and M. Wolf, “A Well-Conditioned Estimator for Large-Dimensional Covariance
        Matrices,” Journal of Multivariate Analysis, 88(2): 365-411.

Newhouse, Joseph P. (2002), "Medicare," in American Economic Policy in the 1990s, eds.,
     Jeffrey A. Frankel and Peter R. Orszag; Cambridge: MIT Press, 899-955.

Pope, Gregory C.; Kautter, John; Ellis, Randall P.; Ash, Arlene S.; Ayanian, John Z; Iezzoni, Lisa
       I.; Ingber, Melvin J.; Levy, Jesse M., and John Robst (2004), “Risk Adjustment of the
       Medicare Capitation Payments Using the CMS-HCC Model,” Health Care Financing
       Review, 25(4): 119-141.

Rao M, Clarke A, Sanderson C, Hammersley R. Patients' own assessments of quality of primary
      care compared with objective records based measures of technical quality of care: cross
      sectional study. BMJ. 2006 Jul 1;333(7557):19.


                                               42
Scanlon, D., Chernew, M., McLaughlin, C., and G. Solon (2002), “The Impact of Health Plan
       Report Cards on Managed Care Enrollment,” Journal of Health Economics, 21:19-41.

Schneider, E.C., Zaslavsky, A.M., Landon, B.E., Lied, T.R., Sheingold, S., and P.D. Cleary,
       (2001),“National Quality Monitoring of Medicare Health Plans: The Relationship Between
       Enrollees’ Reports and the Quality of Clinical Care,” Medical Care, 29(12): 1313-1325.

Schoenman, J.A. (1999), “Impact of The BBA on Medicare HMO Payments for Rural Areas,”
      Health Affairs, 18 (1):244-25.

Stern, Scott (1996), “Product Demand in Pharmaceutical Markets,” mimeo, Northwestern
        University.

Town, Robert and Su Liu (2003), “The Welfare Impact of Medicare HMOs,” RAND Journal of
      Economics 34(4): 719-736.

United States General Accounting Office (1996), “HCFA Should Release Data to Aid Consumers,
       Prompt Better HMO Performance”, Report to Congressional Requesters, October 1996.

Wedig, Gerard J. and M. Tai-Seale (2002), “The Effect of Report Cards on Consumer
      Choice in the Health Insurance Market,” Journal of Health Economics 21: 1031-
      1048.




                                             43
        Figure 1. HMO Penetration Rates, 1993-2001




35.0%

30.0%

25.0%

20.0%

15.0%

10.0%

5.0%

0.0%
         1993 1994 1995 1996 1997 1998 1999 2000 2001

        Medicare Enrollees               Privately-Insured Population


 Sources: Author’s tabulations from the Medicare Managed Care
 Quarterly State/County/Plan Data files for December 1993-2001; CMS
 Statistics, National Enrollment Trends; U.S. Statistical Abstract, various
 years; Health, United States, 2003 (CDC). Comparable data for 1996 is
 not available from these sources.




                                    44
Figure 2. Example of Medicare Report Card Appearing in Medicare & You 2001




                                   45
                   Figure 3. Estimated Coefficients on Score*Year Interactions

                                              Mammography

                       0.16

                       0.12

                       0.08

                       0.04

                       0.00
                              1995   1996   1997     1998    1999     2000   2001   2002
                      -0.04




                                               Communicate

                      0.32


                      0.24


                      0.16


                      0.08


                      0.00
                              1995   1996   1997    1998     1999     2000   2001   2002




                                                   Best Care

                       0.32

                       0.24

                       0.16

                       0.08

                       0.00
                              1995   1996   1997     1998      1999   2000   2001   2002
                      -0.08




Notes: Coefficient estimates from 3 separate models in which each score is interacted with individual year dummies.
Data is reported in Appendix Table 2.



                                                            46
                         Figure 4. Effect of Quality on Predicted Plan Market Shares Over Time

                                                                                                                         % change
                         4.00
                                                                                                                         140

                         3.00                                                                                            103

                         2.00
Change in Market Share




                                   High Quality HMO
                         1.00

                                                   Traditional Medicare
                         0.00
                                  1994      1995       1996      1997     1998      1999      2000      2001      2002
                                                                                                                          -1
                         -1.00
                                                                                                                          -2
                                         Low Quality HMO
                         -2.00                                                                                           -69
                                                                                                                         -81

                         -3.00

                         -4.00



                            Notes: Figure is based on duopoly simulation results in Table 5. Solid lines depict
                            share changes associated with market-based learning only; dashed lines add report-
                            card learning.




                                                                     47
                   Table 1. Descriptive Statistics
                                                    Mean        Std. Dev.
 Plan Characteristics

 Enrollment (#)                                     3,559         9,134
 Share of county eligibles (%)                       4.58          6.60
 Share of county HMO enrollment (%)                 49.98         41.66

 Reported Quality Measures
     Mammography (%)                                75.71          7.39
     Communicate (%)                                69.95          4.91
     Best Care (%)                                  49.52          6.22
 Unreported Quality Measures
     Betablocker (%)                                81.20         12.66
     Diabetic Eye Exams (%)                         59.11         12.71
     Ambulatory Visit (%)                           88.72          8.03

 Monthly premium ($)                                16.56         24.73
 Affiliate in U.S. News (%)                         65.17         47.65
 CMS monthly payment rate ($)                       495.53        96.62
 Prescription drug coverage (%)                     70.19         45.39

 Market Characteristics

 Number of rivals (#)                               2.24          2.35
 Number of rivals belonging to a chain (#)           1.70         1.97
 Number of not-for-profit rivals (#)                1.08          1.27
 Number of IPA rivals (#)                           1.17          1.53
 Stable population share, 1995 to 2000 (%)          79.70         9.40
 Medicare HMO penetration rate, 1994 (%)             8.67         11.87
 Percent of population aged 65-74, 2000 (%)          7.10         2.21
 Percent with college degree, 2000 (%)              15.78          6.51

Notes: N=8212. The unit of observation is the plan-county-year. Sample
includes observations with 10 or more Medicare enrollees and nonmissing
data for all variables. Quality measures correspond to data reported in
Medicare & You 2000 (2001 for best care). Stable population share is the
share of a county's 1995 population still living in the county in 2000. All
variables are described in detail in the text.




                                    48
                         Table 2. Market Characteristics
                                                Mean of (Max-Min) quality scores
                           Number of      [Mean of (Max-Min) standardized quality scores]
Number of Plans             Markets       Mammography Communicate            Best care

1                             240                 -                -                -
                                                8.85             2.88             5.70
2                             114
                                                [.86]            [.59]            [.87]
                                               15.23             7.32            10.67
3+                             117
                                               [1.48]           [1.49]           [1.64]
                                               12.08             5.13             8.21
Total                         471
                                               [1.17]           [1.04]           [1.26]

Notes: Sample includes all markets (=counties) in 2000. Quality scores correspond to data
reported in Medicare & You 2000 (2001 for best care). Standardized values have mean zero
and a standard error of 1.




                  Table 3. Correlation Matrix for Quality Scores

                                                                               Unreported
                         Mammography        Communicate        Best Care       Composite

Mammography                    1.00
Communicate                    0.10              1.00
Best Care                      0.02              0.82             1.00
Unreported composite           0.73              0.17             0.05            1.00

Notes: Sample includes all markets (=counties) in 2000. Quality measures correspond to data
reported in Medicare & You 2000 (2001 for best care).




                                            49
                      Table 4. Effect of Quality on Mean Plan Utility

                                                   (1)             (2)             (3)             (4)
Market Learning
 Mammography*ln(year)                            0.100**         0.064
                                                (0.037)         (0.050)
 Communicate*ln(year)                            0.125†          0.093
                                                (0.068)         (0.076)
 Best care*ln(year)                              0.026           0.062
                                                (0.075)         (0.085)
 Unrep. Composite*ln(year)                                       0.063
                                                                (0.074)
 Composite*ln(year)                                                               0.279***       0.278 ***
                                                                                 (0.067)         (0.068)
Report Card Effect
 Mammography*post                                -0.009          -0.012            0.002          0.005
                                                (0.039)         (0.047)          (0.047)         (0.047)
 Communicate*post                                -0.052          -0.059           -0.024          -0.015
                                                (0.038)         (0.041)          (0.045)         (0.046)
 Best care*post                                 0.145*          0.158*           0.178*          0.165*
                                                (0.057)         (0.062)          (0.071)         (0.073)
 Unreported composite*post                                        0.033           -0.025          -0.036
                                                                (0.108)          (0.093)         (0.094)
Price
 Monthly premium                                 0.003           0.001            0.000           0.001
                                                (0.005)         (0.006)          (0.005)         (0.005)
Benefits
 Prescription drug coverage                                                                       0.024
                                                                                                 (0.043)
Income Quintile Fixed Effects (relative to the highest quintile)
  Quintile 1                                   0.170          0.157               0.154           0.132
                                             (0.177)         (0.177)             (0.178)         (0.179)
  Quintile 2                                   0.196          0.186               0.195           0.189
                                             (0.187)         (0.187)             (0.186)         (0.186)
  Quintile 3                                  0.357*         0.356*              0.367*          0.371*
                                             (0.147)         (0.147)             (0.147)         (0.147)
  Quintile 4                                   0.456***       0.461***            0.463***        0.459***
                                             (0.112)         (0.112)             (0.112)         (0.112)
Nesting Parameter                              0.838***       0.838***            0.840***        0.839***
                                             (0.046)         (0.045)             (0.045)         (0.045)
N                                              8212           8212                8212            8212
Notes: All specifications include plan, county, and state-year fixed effects. Post is an indicator variable
that takes a value of one beginning in 2000 (2001 for best care interactions). Z-scores are used for all
quality measures (composite, mammography, communicate, best care, and unreported composite).
Estimation is by 3SGMM; price and the within-nest share are treated as endogenous. Standard errors
are adjusted for correlation in residuals for the same plan-county over time.
† denotes p<.10, * denotes p<.05, ** denotes p<.01, *** denotes p<.001




                                                    50
                     Table 5. Evolution of Predicted Market Shares in Hypothetical Markets

No Report Card                  1994     1995     1996     1997     1998     1999     2000     2001     2002      Δ       %Δ
Monopoly Market
 Sole plan (Low quality)          2.74     2.43     2.24     2.15     2.05     2.02     1.94     1.89     1.86   -0.88   -32.0%
                                (2.13)   (1.95)   (1.67)   (1.66)   (1.59)   (1.73)   (1.64)   (1.51)   (1.62)
 Sole plan (Medium quality)       2.74     2.74     2.72     2.74     2.71     2.77     2.73     2.72     2.73   -0.01    -0.3%
                                (2.13)   (2.15)   (1.97)   (2.05)   (2.02)   (2.25)   (2.18)   (2.06)   (2.22)
 Sole plan (High quality)         2.74     3.09     3.29     3.48     3.58     3.76     3.81     3.89     3.98   1.24    45.3%
                                (2.13)   (2.37)   (2.32)   (2.52)   (2.54)   (2.89)   (2.86)   (2.77)   (3.00)
Duopoly Market
 Low quality plan                 2.76     1.95     1.57     1.33     1.19     1.05     1.00     0.93     0.85   -1.91   -69.4%
                                (3.54)   (3.13)   (2.84)   (2.61)   (2.57)   (2.55)   (2.31)   (2.58)   (2.36)
 High quality plan                2.72     3.59     4.06     4.54     4.77     5.00     5.14     5.31     5.53   2.81    103.4%
                                (3.46)   (3.71)   (3.74)   (4.30)   (4.07)   (4.08)   (4.30)   (4.12)   (4.39)
Triopoly Market
 Low quality plan                 2.79     1.97     1.54     1.30     1.17     1.06     0.98     0.89     0.82   -1.98   -70.8%
                                (4.48)   (3.83)   (3.53)   (3.13)   (3.00)   (3.06)   (3.10)   (2.80)   (2.77)
 Medium quality plan              2.69     2.69     2.54     2.60     2.38     2.42     2.39     2.31     2.30   -0.40   -14.8%
                                (4.34)   (4.23)   (4.17)   (4.42)   (3.98)   (4.19)   (4.22)   (4.24)   (4.28)
 High quality plan                2.74     3.57     4.24     4.61     5.00     5.24     5.51     5.77     6.00   3.26    119.0%
                                (4.30)   (4.76)   (5.02)   (5.21)   (5.37)   (5.33)   (5.56)   (5.71)   (5.65)




                                                              51
               Table 5. Evolution of Predicted Market Shares in Hypothetical Markets (continued)

  With Report Card                        1994      1995     1996      1997      1998      1999      2000      2001     2002        Δ          %Δ
  Monopoly Market
   Sole plan (Low quality)                 2.74      2.43      2.24      2.15      2.05     2.02      2.02      1.65      1.63     -1.11     -40.7%
                                         (2.13)    (1.95)    (1.67)    (1.66)    (1.59)   (1.73)    (1.70)    (1.34)    (1.44)
    Sole plan (Medium quality)             2.74      2.74      2.72      2.74      2.71     2.77      2.73      2.72      2.73     -0.01       -0.3%
                                         (2.13)    (2.15)    (1.97)    (2.05)    (2.02)   (2.25)    (2.18)    (2.06)    (2.22)
    Sole plan (High quality)               2.74      3.09      3.29      3.48      3.58     3.76      3.68      4.44      4.54      1.80      65.6%
                                         (2.13)    (2.37)    (2.32)    (2.52)    (2.54)   (2.89)    (2.78)    (3.08)    (3.32)
  Duopoly Market
   Low quality plan                        2.76      1.95      1.57      1.33      1.19     1.05      1.13      0.58      0.53     -2.23     -81.0%
                                         (3.54)    (3.13)    (2.84)    (2.61)    (2.57)   (2.55)    (2.43)    (2.20)    (1.98)
    High quality plan                      2.72      3.59      4.06      4.54      4.77     5.00      4.88      6.30      6.53      3.81     140.0%
                                         (3.46)    (3.71)    (3.74)    (4.30)    (4.07)   (4.08)    (4.23)    (4.42)    (4.71)
  Triopoly Market
   Low quality plan                        2.79      1.97      1.54      1.30      1.17     1.06      1.10      0.54      0.51     -2.28     -81.8%
                                         (4.48)    (3.83)    (3.53)    (3.13)    (3.00)   (3.06)    (3.24)    (2.31)    (2.29)
    Medium quality plan                    2.69      2.69      2.54      2.60      2.38     2.42      2.45      2.04      2.02     -0.67     -25.0%
                                         (4.34)    (4.23)    (4.17)    (4.42)    (3.98)   (4.19)    (4.24)    (4.17)    (4.20)
    High quality plan                      2.74      3.57      4.24      4.61      5.00     5.24      5.17      7.08      7.31      4.57     167.0%
                                         (4.30)    (4.76)    (5.02)    (5.21)    (5.37)   (5.33)    (5.44)    (6.13)    (6.08)

Notes: Tables report mean market shares (standard errors) from simulations using the model in Table 4, column 3. Unobserved, time-invariant
plan quality is the same for all plans; the magnitude is selected so that mean market share in 1994 is 2.74 percentage points (the national average in
that year). Reported mean market shares may differ due to the randomly-drawn time-varying unobserved quality term. Number of simulations is
10,000 for each cell.




                                                                          52
                        Table 6. Simulated Outcomes in Duopoly Markets in 2002, with Report Card

                                                                       Market Shares

                                                                                   Plan 2 Quality
                                           Low                                        Medium                                           High
  Plan 1 Quality           Plan 1         Plan 2          Total        Plan 1        Plan 2             Total           Plan 1         Plan 2         Total
    Low       Mean              1.67           1.64          3.31
               S.D.           (2.28)         (2.25)        (2.45)
   Medium Mean                  3.60           1.01          4.61           2.77           2.71               5.48
               S.D.           (3.35)         (2.20)        (3.23)         (3.56)         (3.49)             (3.65)
    High      Mean              6.53           0.53          7.06           5.89           1.65               7.54           4.49           4.41         8.90
               S.D.           (4.71)         (1.98)        (4.54)         (5.04)         (3.40)             (4.69)         (5.40)         (5.29)       (5.19)

                                                                       Growth Rates

                                                                                   Plan 2 Quality
                                           Low                                        Medium                                           High
  Plan 1 Quality           Plan 1         Plan 2          Total        Plan 1        Plan 2             Total           Plan 1         Plan 2         Total
    Low       Mean         -38.92%        -40.14%        -39.53%
   Medium Mean              31.49%        -63.31%        -15.91%         1.03%         -0.94%               0.04%
    High      Mean         138.36%        -80.84%         28.76%       114.92%        -39.89%              37.52%        64.01%         60.97%       62.49%

Notes: Table reports mean market shares (standard errors) from simulations using the model in Table 4, column 3. Unobserved, time-invariant plan quality is
the same for all plans; the magnitude is selected so that mean market share in 1994 is 2.74 percentage points (the national average in that year). Reported mean
market shares may differ due to the randomly-drawn time-varying unobserved quality term. Number of simulations is 10,000 for each cell. Growth rates are
calculated using 2.74 as the initial market share.




                                                                              53
                                  Table 7. Sources of Market Learning

                                                   (1)            (2)           (3)            (4)            (5)
Market Learning
 Composite*ln(year)                              0.279***       0.262***       0.322***       0.222***       0.250***
                                                (0.067)        (0.066)        (0.072)        (0.064)        (0.070)
 Composite*ln(year)*stable population                           0.092**                                      0.099**
                                                               (0.033)                                      (0.034)
 Composite*ln(year)*HMO penetration                                           -0.058**                      -0.051*
                                                                              (0.021)                       (0.020)
 Composite*ln(year)*U.S. News                                                                 0.172**        0.141*
                                                                                             (0.066)        (0.067)
Report Card Effect
 Mammography*post                                0.002          0.009          0.010          0.001          0.015
                                                (0.047)        (0.047)        (0.047)        (0.047)        (0.047)
 Communicate*post                                -0.024         -0.025         -0.025         -0.034         -0.036
                                                (0.045)        (0.044)        (0.044)        (0.043)        (0.042)
 Best care*post                                  0.178*         0.171*         0.180*         0.184**        0.180*
                                                (0.071)        (0.071)        (0.070)        (0.070)        (0.070)
 Unreported composite*post                       -0.025         -0.033         -0.030         -0.024        -0.036
                                                (0.093)        (0.093)        (0.093)        (0.093)        (0.093)
Price
 Monthly Premium                              0.000         0.001              0.000          0.000          0.000
                                             (0.005)       (0.005)            (0.005)        (0.004)        (0.004)
Income Quintile Fixed Effects (relative to the highest quintile)
  Quintile 1                                  0.154         0.168              0.153          0.162          0.172
                                             (0.178)       (0.176)            (0.177)        (0.176)        (0.174)
  Quintile 2                                  0.195         0.201              0.190          0.179          0.184
                                             (0.186)       (0.186)            (0.186)        (0.186)        (0.185)
  Quintile 3                                  0.367*        0.361*             0.366*         0.366*         0.359*
                                             (0.147)       (0.147)            (0.146)        (0.146)        (0.145)
  Quintile 4                                  0.463***      0.459***           0.458***       0.470***       0.459***
                                             (0.112)       (0.111)            (0.111)        (0.111)        (0.110)
                                                    ***
Nesting Parameter                             0.840         0.844***           0.840***       0.838***       0.844***
                                             (0.045)       (0.044)            (0.045)        (0.045)        (0.044)
N                                              8212         8212               8212           8212           8212

Notes: All specifications include plan, county, and state-year fixed effects. Post is an indicator variable that
takes a value of one beginning in 2000 (2001 for best care interactions). Z-scores are used for all quality
measures (composite, mammography, communicate, best care, and unreported composite), stable population
and HMO penetration. U.S. News takes on a value of 1 if all plans in a county-year have affiliates that
appeared in the U.S. News “Best HMOs” articles at least once, and 0 otherwise. Estimation is by 3SGMM;
price and the within-nest share are treated as endogenous. Standard errors are adjusted for correlation in
residuals for the same plan-county over time.
† denotes p<.10, * denotes p<.05, ** denotes p<.01, *** denotes p<.001




                                                          54
      Table 8. Effect of Contemporaneous and Reported Quality on Mean Plan Utility

Market Learning
 (Lagged) contemporaneous mammography                                                        0.084*
                                                                                            (0.042)
 (Lagged) contemporaneous best care                                                          0.141**
                                                                                            (0.045)
Report Card Effect
 (Reported) mammography*post                                                                 0.075*
                                                                                            (0.034)
 (Reported) best care*post                                                                   0.180**
                                                                                            (0.059)
Price
 Monthly premium                                                                            -0.008†
                                                                                            (0.004)
Income Quintile Fixed Effects (relative to the highest quintile)
  Quintile 1                                                                                  0.180
                                                                                            (0.203)
    Quintile 2                                                                               -0.162
                                                                                            (0.249)
    Quintile 3                                                                                0.183
                                                                                            (0.141)
    Quintile 4                                                                                0.058
                                                                                            (0.166)
Nesting Parameter                                                                             1.130***
                                                                                            (0.073)
N                                                                                             3408


Notes: All specifications include plan, county, and state-year fixed effects. Post is an indicator variable that takes
a value of one beginning in 2000 for mammography, and 2001 for best care. Annual z-scores are used for
mammography and best care. Estimation is by 3SGMM; price and the within-nest share are treated as
endogenous. Standard errors are adjusted for correlation in residuals for the same plan-county over time.
†
  denotes p<.10, * denotes p<.05, ** denotes p<.01, *** denotes p<.001




                                                         55
                                                                      Appendix 1. Estimation



The model differs from a standard nested logit due to the inclusion of a separate intercept for

each income quintile. This produces different mean utility equations for each quintile q. These

are associated with the quintile-specific choice probabilities, {s qjc ( s )t }q =1,...,Q , in the following way,

derived by Berry (1994):

(A1)     δ jcq ( s )t ≡ ln(s qjc ( s )t ) − ln( s0qc ( s )t ) = αp js ( c )t + x js ( c )t β + ξ js ( c )t + γ q + σ ln(s q
                                                                                                                         ( j / g )c ( s )t
                                                                                                                                             )

However, since the observed market shares are weighted averages of these choice probabilities,

with the weights representing the share of the population in each income quintile, the closed

form inversion formula no longer applies. We therefore use BLP’s contraction mapping theorem

to derive the mean utility numerically.

         An important characteristic of our specification is that the quintile-specific intercepts

have an impact only on the choice of the nest but not on the choice of HMO plan within the nest.

As a result, the choice probabilities over individual HMO plans are identical across income

quintiles, conditional on the choice of the HMO nest. We can take advantage of this feature to

replace a part of the numerical inversion procedure with a simple formula. For this, we designate

a reference HMO plan, plan 1, in each market and define the differential mean utility of each

plan as Δδ jcq ( s ) t ≡ δ jcq ( s ) t − δ 1qc ( s ) t . It can be shown that the differential mean utility is common

across income quintiles and coincides with the log difference of the common conditional choice

probabilities:

(A2)      Δδ jc ( s ) t = ln(s ( j / g ) s ( c ) t ) − ln(s (1 / g ) c ( s )t )

                                                                                                      1
Next, note Δδ 1c ( s ) t = 0 by construction and s (1 / g ) c ( s )t =                                                   .
                                                                                       1 + ∑k = 2 exp(Δδ kc ( s ) t )
                                                                                                J




                                                                                  56
          It remains to obtain {δ 1qc ( s ) t }q =1,...,Q to finalize the estimation equation. This segment of

estimation exploits the observed choice of the nest and requires BLP’s numerical inversion

procedure. First, rewrite the quintile-specific market share of the outside good as a function of

quintile-specific mean utility and the observed within share:

                                               1
(A3)       s 0qc ( s )t =                                                         .
                            1 + exp(δ       q
                                           1c ( s ) t   ) s (−11/ g ) c ( s ) t

The aggregate market share of the outside good is a weighted average of the quintile-specific

shares:

           s0 c ( s ) t = ∑q =1 λcq( s ) t s0qc ( s ) t ,
                               Q
(A4)

where λcq( s ) t are the weights. Holding the quintile-specific intercepts {γ q }q =1,...,Q fixed, the

function f (δ 0c ( s ) t ) = δ 0 c ( s )t + ln(s 0c ( s ) t ) − ln( s 0 c ( s )t (δ 0c ( s ) t )) satisfies all the conditions of BLP’s

contraction mapping theorem, and we can obtain δ 0 c ( s )t as the unique fixed point of f (.) via

iteration.




*Prepared by Yongbae Lee, Northwestern University.




                                                                                      57
         Appendix Table 1. Scores Reported in Medicare & You

 Measure               Description            Data Years          Data Years
                                              (Sources)           (Sources)
                                              for Scores          for Scores
                                              Reported in 2000    Reported in 2001
 mammography           % of women 50-69       1996-1997           1997-1998
                       receiving a            (1998 HEDIS file)   (1999 HEDIS file)
                       mammogram
                       within past 2 years
 communicate           % enrollees            1998                Not reported
                       reporting the          (2000 Medicare
                       doctors in their       Compare Database)
                       plan “always
                       communicate well”
 best care             % enrollees rating     Not reported        1999
                       their own care a 10                        (2001 Medicare
                       out of 10                                  Compare Database)
 disenrollment         % enrollees who        1998                1999
                       voluntarily            (2001 Medicare      (2001 Medicare
                       disenrolled            Compare Database    Compare Database)

Note: Measures matched to enrollment data are shaded in gray.




                                             58
                 Appendix Table 2. Effect of Quality on Mean Plan Utility,
                           Separate Models For Each Score

                                        (1)                   (2)                    (3)
Score                                Mammography           Communicate            Best Care

    Score*1995                             -.023                .004                 -.032
                                          (.051)               (.041)               (.042)
    Score*1996                             .087                 .066                 -.017
                                          (.059)               (.060)               (.062)
    Score*1997                             .071                 .142*                .062
                                          (.061)               (.062)               (.064)
    Score*1998                             .119†                .173**               .089
                                          (.063)               (.063)               (.064)
    Score*1999                             .122†                .240***              .119†
                                          (.065)               (.067)               (.066)
    Score*2000                             .140*                .276***              .127†
                                          (.067)               (.070)               (.068)
    Score*2001                             .079                 .266***              .226**
                                          (.065)               (.074)               (.075)
    Score*2002                             .114                 .236**               .272**
                                          (.074)               (.083)               (.093)
Price
   Monthly premium                         .004                 .009*                .004
                                          (.004)               (.004)               (.004)
Nesting parameter                          .838***              .825***              .837***
                                          (.047)               (.047)               (.047)
N                                          8212                 8212                 8212

Notes: All specifications include plan, county, state-year, and income quintile fixed effects. Z-
scores are used for all quality measures. Estimation is by 3SGMM; price and the within-nest share
are treated as endogenous. Standard errors are adjusted for correlation in residuals for the same
plan-county over time.
† denotes p<.10, * denotes p<.05, ** denotes p<.01, *** denotes p<.001




                                               59
                          Appendix Table 3. Panel Quality Data
                                                 1998          1999          2000           2001

Mammography                                      76.47        75.94          76.34         76.67
                                                (6.15)        (6.58)         (6.58)        (5.68)
Best Care                                       50.21         49.60          48.30         43.31
                                                (7.33)        (6.22)         (6.01)        (6.37)
N (number of plan-counties)                      1124          1024           870           748

Notes: The unit of observation is the plan-county-year. Sample includes observations with 10 or more
Medicare enrollees and nonmissing data for all reported quality measures. All data is reported in
percentages unless otherwise indicated. Standard deviations are in parentheses.




                                                 60
