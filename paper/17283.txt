                                NBER WORKING PAPER SERIES




   A SIMPLE NONPARAMETRIC APPROACH TO ESTIMATING THE DISTRIBUTION
             OF RANDOM COEFFICIENTS IN STRUCTURAL MODELS

                                            Jeremy T. Fox
                                             Kyoo il Kim

                                        Working Paper 17283
                                http://www.nber.org/papers/w17283


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                     August 2011




The views expressed herein are those of the authors and do not necessarily reflect the views of the
National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2011 by Jeremy T. Fox and Kyoo il Kim. All rights reserved. Short sections of text, not to exceed
two paragraphs, may be quoted without explicit permission provided that full credit, including © notice,
is given to the source.
A Simple Nonparametric Approach to Estimating the Distribution of Random Coefficients
in Structural Models
Jeremy T. Fox and Kyoo il Kim
NBER Working Paper No. 17283
August 2011
JEL No. C14,L0

                                             ABSTRACT

We explore a nonparametric mixtures estimator for recovering the joint distribution of random coefficients
in economic models. The estimator is based on linear regression subject to linear inequality constraints
and is computationally attractive compared to alternative, nonparametric estimators. We provide conditions
under which the estimated distribution function converges to the true distribution in the weak topology
on the space of distributions. We verify the consistency conditions for discrete choice, continuous
outcome and selection models.


Jeremy T. Fox
Economics Department
University of Michigan
238 Lorch Hall
611 Tappan St.
Ann Arbor, MI 48104
and NBER
jeremyfox@gmail.com

Kyoo il Kim
Department of Economics
University of Minnesota
4-129 Hanson Hall
1925 4th Street South
Minneapolis, MN 55455
kyookim@umn.edu
1    Introduction
Economic researchers often work with models where the parameters are heterogeneous across the
population. A classic example is that consumers may have heterogeneous preferences over a set of
product characteristics in an industry with diﬀerentiated products. These heterogeneous parameters
are often known as random coeﬃcients. When working with cross sectional data, the goal is often to
estimate the distribution of random coeﬃcients. This distribution captures the essential heterogeneity
that is key to explaining the economic phenomena under study.
    This paper studies estimating the distribution F (β) in the model
                                                  ∫
                                      PA (x) =        gA (x, β) dF (β) ,                              (1)

where A is some subset of the observable outcomes y, x is a vector of covariates, β is the vector of
random coeﬃcients, and gA (x, β) is the probability that an outcome in A occurs for an observation with
random coeﬃcients β and covariates x. Given this structure, PA (x) is the cross sectional probability
of observing an outcome in the set A when the covariates are observed to be x. The researcher picks
gA (x, β) as the underlying model, has an i.i.d. sample of observations (yi , xi ), and wishes to estimate
F (β).
    Previous so-called nonparametric or ﬂexible estimators for F (β) include the EM algorithm, Markov
Chain Monte Carlo (MCMC), simulated maximum likelihood, simulated method of moments, and min-
imum distance. As typically implemented, these estimators suﬀer from a key ﬂaw: they are computa-
tionally challenging. The researcher must code some iterative search or simulation procedure. Often,
convergence may not be to the preferred, global solution. Convergence may be hard to ensure and to
detect.
    Our insight is to notice that the unknown distribution F (β) enters (1) linearly. Thus, we can
exploit linearity and achieve a computationally simpler estimator than the alternatives. Fox, Kim,
Ryan and Bajari (2011), henceforth FKRB, propose dividing the support of β into a ﬁnite and known
grid of points β 1 , . . . , β R . Let yA equal 1 when an outcome is in A, and 0 otherwise. The researcher
then estimates the weights θ1 , . . . , θR on the R grid points as the linear probability model regression
of yA on the R predicted probabilities g (x, β r ). We also impose the constraints that each θr ≥ 0 and
     ∑
that R      r
       r=1 θ = 1. Thus, the estimator of the distribution F (β) with N observations and R grid points
becomes
                                                  ∑R
                                      F̂N (β) =              θ̂r 1 [β r ≤ β]
                                                       r=1

where θ̂r ’s denote estimated weights. The computational advantages of the procedure are immediate.
Computationally, the estimator is linear regression (least squares) subject to linear inequality and
equality constraints. This optimization problem is globally convex and specialized routines (such as
LSSOL, built into MATLAB as the command lsqlin) are guaranteed to converge to the global optimum.
    FKRB highlight the practical usefulness of the estimator by showing how it can be used in a series


                                                       2
of examples of random coeﬃcient models employed in consumer choice and other settings, including
applications with endogenous covariates handled with instruments and applications with aggregate
data. The estimator also has computational savings for complex structural models where economic
models must be solved as part of the estimation procedure. In a dynamic programming application such
as adding random coeﬃcients to Rust (1987), the dynamic programming model must be solved only
R times, once for each random coeﬃcient β r . These solutions occur before optimization commences,
and are not nested inside an iterative search or simulation procedure. This contrasts with competing
approaches, where multiple dynamic programs must be solved for every change in the estimation
algorithm’s guess of F (β). In this respect, our estimator shares some computational advantages with
the parametric approach in Ackerberg (2009).
     A serious limitation is that the analysis in FKRB assumes that the R grid points used in a ﬁnite
sample are indeed the true grid points that take on nonnegative support in the true F0 (β). Thus,
the true distribution F0 (β) is assumed to be known up to a ﬁnite number of weights θ1 , . . . , θR . This
assumption is convenient as the estimator is consistent under standard conditions for the consistency
of least squares under inequality and equality constraints (Andrews 2002). As economists often lack
convincing economic rationales to pick one set of grid points over another, assuming that the researcher
knows the true distribution up to ﬁnite weights is unrealistic.
     This paper seeks to place this appealing, computationally simple estimator on ﬁrmer theoretical
ground. Instead of assuming that the distribution is known up to weights θ1 , . . . , θR , we require the
true distribution F0 (β) to satisfy much weaker restrictions. In particular, the true F0 (β) can have any
of continuous, discrete and mixed continuous and discrete supports. The prior approach in FKRB is
parametric as the true weights θ1 , . . . , θR lie in a ﬁnite-dimensional subset of a real space. Here, the
approach is nonparametric as the true F0 (β) is known to lie only in the inﬁnite-dimensional space of
multivariate distributions on the space of random coeﬃcients β.
     In a ﬁnite sample of N observations, our estimator is still implemented by choosing a grid of points
θ1 , . . . , θR   , ideally to trade oﬀ bias and variance in the estimate F̂N (β). We, however, recognize that
as the sample increases, R and thus the ﬁneness of the grid of points should also increase in order to
reduce the bias in the approximation of F (β). We write R (N ) to emphasize that the number of grid
points (and implicitly the grid of points itself) is now a function of the sample size. The main theorem
in our paper is that, under restrictions on the economic model and an appropriate choice of R (N ), the
estimator F̂N (β) converges to the true F0 (β) as N → ∞, in a function space. The topology on our
function space is induced by the Lévy-Prokhorov metric, a common metrization of the weak topology
on the space of multivariate distributions.
     We recognize that the nonparametric version of our estimator is a special case of a sieve estimator
(Chen 2007). Sieve estimators estimate functions by increasing the ﬂexibility of the approximating
class used for estimation as the sample size increases. A sieve estimator for a smooth function might
use an approximating class deﬁned by a Fourier series, for example. As we are motivated by practical
considerations in empirical work, our estimator’s choice of basis, a discrete grid points, is justiﬁed by



                                                          3
the estimator’s computational simplicity. Further and unlike a typical sieve estimator, we need to
constrain our estimated functions to be valid distribution functions. Our constrained linear regression
approach is both computationally simple and ensures that the estimated CDF satisﬁes the theoretical
properties of a valid CDF.1
       Because our estimator is a sieve estimator, we prove its consistency by satisfying high-level con-
ditions for the consistency of a sieve extremum estimator, as given in an appendix lemma in Chen
and Pouzo (2009). We repeat this lemma and its proof in our paper so our consistency proof is self-
contained. Our estimator is not a special case of the two-step sieve estimators explored using lower-level
conditions in the main text of Chen and Pouzo. Several issues arise in proving consistency. Most inter-
estingly, under the Lévy-Prokhorov metric on the space of multivariate distributions, the problem of
optimizing the population objective function over the space of distributions turns out to be well posed
under the deﬁnition of Chen (2007). Thus, our method does not rely on a sieve space to regularize the
estimation problem to address the ill-posed inverse problem, as much of the sieve literature focuses on.
       Our approach is a general, nonparametric mixtures estimator. The most common frequentist, non-
parametric estimator is nonparametric maximum likelihood or NPMLE (Laird 1978, Böhning 1982,
Lindsay 1983, Heckman and Singer 1984). Often the EM algorithm is used for computation (Dempster,
Laird and Rubin 1977), but this approach is not guaranteed to ﬁnd the global maximum. The literature
worries about the strong dependence of the output of the EM algorithm on initial starting values and
well as the diﬃculty in diagnosing convergence (Seidel, Mosler and Alker 2000, Verbeek, Vlassis and
Kröse 2002, Biernacki, Celeux and Govaert 2003, Karlis and Xekalaki 2003).2 Further, the EM algo-
rithm has a slow rate of convergence even when it does converge to a global solution (Pilla and Lindsay
2001). Li and Barron (2000) introduce another alternative, but again our approach is computationally
simpler. Our estimator is also computationally simpler than the minimum distance estimator of Beran
and Millar (1994), which in our experience often has an objective function with an intractably large
number of local minima. The discrete-grid idea (called the “histogram” approach) is found outside of
economics in Kamakura (1991), who uses a discrete grid to estimate an ideal-point model. He does
not discuss the nonparametric statistical properties of his approach. Of course, mixtures themselves
have a long history in economics, such as Quandt and Ramsey (1978).
       We prove the consistency of our estimator for the distribution of random parameters, in function
space under the weak topology. To our knowledge, many of the alternative estimators discussed above
do not have general, nonparametric consistency theorems for the estimator for the distribution of
random parameters.3 Our consistency theorem is not speciﬁc to the economic model being estimated.4
   1
      FKRB also discuss the cases where β has continuous support and the researcher approximates the density with a
mixture of normals.
    2
      Another drawback of NPMLE that is speciﬁc to mixtures of normal distributions, a common approximating choice,
is that the likelihood is unbounded and hence maximizing the likelihood does not produce a consistent estimator. There
is a consistent root but it is not the global maximum of the likelihood function (McLachlan and Peel 2000).
    3
      Beran (1995, Proposition 3) provides a proof for the consistency of the minimum distance estimator of the distribution
of random coeﬃcients in the linear regression model. However, the proof itself does not rely on properties of the linear
regression model, other than its identiﬁcation.
    4
      In an earlier version of this paper, we also provide the rate of convergence of our estimator. Many of the competing


                                                             4
    Despite the presence of nonparametric estimators in the literature, they are not commonly used by
applied practitioners estimating distributions of random coeﬃcients. Also, the theoretical results on
consistency for these other estimators are not always of the generality needed for many economic models
used in structural empirical work. By proving the nonparametric consistency of a computationally
simple estimator for general economic choice models, we hope that nonparametric methods will be
increasingly adopted by practitioners in industrial organization, marketing and other applied ﬁelds.
    The outline of our paper is as follows. Section 2 reviews the general notation for the economic model
and presents ﬁve examples of mixture models. Section 3 introduces the estimation procedure. Section
4 demonstrates consistency of our estimator in the space of multivariate distributions. Section 5 argues
that our estimation problem is well-posed using the deﬁnition of Chen (2007). Section 6 extends our
consistency results to models with both random coeﬃcients and homogeneous parameters. Section
7 veriﬁes the primitive conditions for consistency established in Section 4 using the ﬁve examples of
mixture models in Section 2.


2    True Model and Examples
The econometrician observes a real valued vector of covariates x. The dependent variable in our model
is denoted y, which indicates an underlying random variable y ∗ that takes values in the range of y ∗ ,
Y ∗ . Note that y ∗ is not a latent variable. Some of our examples will focus primarily on the case where
the range of y ∗ is a ﬁnite number of integer values, as is customary in discrete choice models. However,
much of our analysis extends to the case where y ∗ is real valued.
    Let A denote a (measurable) set in the range of y ∗ , Y ∗ . We let PA (x) denote the probability
that y ∗ ∈ A when the decision problem has characteristics x. Let β denote a random coeﬃcient
that we assume is distributed independently of x. In our framework, this is a ﬁnite-dimensional, real-
valued vector. We let gA (x, β) be the probability of A conditional on the random coeﬃcients β and
characteristics x. The CDF of the random coeﬃcients is denoted by F (β). The function gA is speciﬁed
as a modeling primitive. Given these deﬁnitions it follows that
                                                     ∫
                                          PA (x) =       gA (x, β) dF (β) .                                       (2)

On the right hand side of the above equation, gA (x, β) gives the probability of A conditional on x
and β. We average over the distribution of β using the CDF F (β) to arrive at PA (x), the population
probability of the event A conditional on x.
    In our framework, the object the econometrician wishes to estimate is F (β), the distribution of
random coeﬃcients. One deﬁnition of identiﬁcation means that a unique F (β) solves (2) for all x and
all A. This is the deﬁnition used in certain relevant papers on identiﬁcation in the statistics literature,
for example Teicher (1963).
nonparametric estimators lack results on the rate of convergence. For example, Horowitz (1999, footnote 5) writes that
“The rates of convergence of the Heckman-Singer estimators ... are unknown ...”


                                                          5
2.1    Examples of Mixture Models
We will return to these ﬁve examples later in the paper. Each example considers economic models with
random coeﬃcients that play a large role in empirical work. Some of the example models are nested
in others, but veriﬁcation of the conditions for consistency in Section 7 will use additional restrictions
on the supports of x and β that are non-nested across models.

Example 1. (logit) Let there be a multinomial choice model such that y is one of J + 1 unordered
choices, such as types of cars for sale. The utility of choice j to consumer i is ui,j = x′i,j βi + ϵi,j , where
xi,j is a vector of observable product characteristics of choice j and the demographics of consumer i, βi
is a vector of random coeﬃcients giving the marginal utility of each car’s characteristics to consumer
i, and ϵi,j is an additive, consumer- and choice-speciﬁc error. There is an outside good 0 with utility
ui,0 = εi,0 . The consumer picks choice j when ui,j > ui,h ∀ h ̸= j. The random coeﬃcients logit model
occurs when ϵi,j is known to have the type I extreme value distribution. In this example, (1) becomes
for A = {j},                                                          (     )
                                  ∫                        ∫      exp x′j β
                       Pj (x) =       gj (x, β) dF (β) =          ∑         (     ) dF (β) ,
                                                               1 + Jh=1 exp x′h β
where x = (x1 , . . . , xJ ). A similar expression occurs for other choices h ̸= j. Compared to prior
empirical work using the random coeﬃcients logit, our goal is to estimate F (β) nonparametrically.



Example 2. (binary choice) Let J = 1 in the previous example, so that there is one inside good
and one outside good. Thus, the utility of the inside good 1 is ui,1 = ϵi + x′i β1,i , where βi = (ϵi , β1,i ) is
seen as one long vector and ϵi supplants the logit errors in Example 1 and plays the role of a random
intercept. The outside good has utility ui,0 = 0. In this example, (1) becomes for A = {1},
                                      ∫                        ∫
                                                                    [             ]
                         P1 (x) =         g1 (x, β) dF (β) =       1 ϵ + x′ β1 ≥ 0 dF (β) ,

where 1 [·] is the indicator function equal to 1 if the inequality in the brackets is true. Without logit
errors, the distribution of all unknowns in the model is estimated nonparametrically. In this example
and others below, we allow the case where gA (x, β) in (1) is discontinuous in β.



Example 3. (multinomial choice without logit errors) Consider a multinomial choice model
where the distribution of the previously logit errors is also estimated nonparametrically. In this case,
the utility to choice j is ui,j = x′i,j β̃i + ϵi,j and the utility of the outside good 0 is ui,0 = 0. The notation
                                                                              (                 )
β̃i is used because the full random coeﬃcient vector is now βi = β̃i , ϵi,1 , . . . , ϵi,J , which is seen as
one long vector. We will not assume that the additive errors ϵi,j are distributed independently of βi .



                                                           6
In this example, (1) becomes for A = {j},
                       ∫                            ∫    [                 {              }        ]
           Pj (x) =            gj (x, β) dF (β) =       1 x′j β̃ + ϵj ≥ max 0, x′h β̃ + ϵh ∀ h ̸= j dF (β) .




Example 4. (Cobb-Douglas production function / linear regression) Consider now a contin-
uous outcome yi from a Cobb-Douglas production function in logs, yi = βi0 + βi1 xi,l + βi2 xi,k , where i
is a manufacturing plant, yi is the log of value added output, βi0 is the log of total factor productivity,
βi1 is the input elasticity on labor (logged) xi,l , and βi2 is the input elasticity on capital (logged) xi,k .
                                 (              )
Let xi = (xi,l , xi,k ) and βi = βi0 , βi1 , βi2 . Let A = [a1 , a2 ) be an interval in the range of yi . In this
example, (1) becomes
                                ∫                                ∫
                                                                      [                                   ]
           P[a1 ,a2 ) (x) =         g[a1 ,a2 ) (x, β) dF (β) =       1 a1 ≤ βi0 + βi1 xi,l + βi2 xi,k < a2 dF (β) .

The goal is to estimate the joint distribution of total factor productivity and the input elasticities of
labor and capital.



Example 5. (joint continuous and discrete demand) Consider now a model where a consumer
picks a discrete choice j and, conditional on that choice j, the researcher observes a measure of usage,
yj . For example, j could be a brand of air conditioner, some of which use less energy than others.
In this case, yj might be the observed energy consumption from the chosen air conditioner. This is a
variant of the generalized Roy selection model of Heckman (1990), as energy consumption yj is observed
only for the chosen air conditioner. Let the utility of choice j be ui,j = x̃′i,j β̃i and the utility of choice 0
be ui,0 = 0. Let the outcome for choice j be yi,j = x̄′i,j β̄i + ϵi,j , where x̄i,j is a vector of characteristics
                                                                                            (                         )
that enters energy usage. Let xi = (x̃i,j , x̄i,j ), eliminating overlap, and let βi = β̃i , β̄i , ϵi,1 , . . . , ϵi,J .
Let A = ([a1 , a2 ) , {j}). In this example, (1) becomes
                           ∫
  P([a1 ,a2 ),{j}) (x) =        g([a1 ,a2 ),{j}) (x, β) dF (β) =
                                                 ∫
                                                      [                     ] [             {          }        ]
                                                    1 a1 ≤ x̄′j β̄ + ϵj < a2 1 x̃′j β̃ ≥ max 0, x̃′h β̃ ∀ h ̸= j dF (β) .

In other words, we compute the probability of the discrete choice being j and the continuous outcome
lying in the interval [a1 , a2 ). Compared to Heckman (1990), this model rules out an additive error in
discrete choice utility (which leads to identiﬁcation at inﬁnity if included) but allows random coeﬃcients
in both the discrete choice and continuous outcomes portions of the model, which are not allowed in
Heckman.




                                                                     7
3    Estimator
The researcher picks gA (x, β) as the model of interest and seeks to estimate F (β), the distribution of
heterogeneity. We assume the researcher has access to N observations on (yi , xi ). Given this, we divide
the range Y of y into mutually exclusive sets A1 , . . . , AJ . Let yi,j = 1 if yi ∈ Aj and 0 otherwise.
    Let A = Aj and use j for Aj . Start with the model (2) and add yi,j to both sides while moving
Pj (x) to the right side. For the statistical observation i, this gives
                                        ∫
                               yi,j =        gj (xi , β) dF (β) + (yi,j − Pj (xi )) .                      (3)

By the deﬁnition of Pj (x), the expectation of the composite error term yi,j − Pj (x), conditional on
x, is 0. This is a linear probability model with an inﬁnite-dimensional parameter, the distribution
F (β). We could work directly with this equation if it was computationally simple to estimate this
inﬁnite-dimensional parameter while constraining it to be a valid CDF.
   Instead, we work with a ﬁnite-dimensional sieve space approximation to F . In particular, we let
                                                        (                     )
R (N ) be the number of grid points in the grid BR(N ) = β 1 , . . . , β R(N ) . A grid point is a vector if
β is a vector, so R (N ) is the total number of points in all dimensions. The researcher chooses BR(N ) .
                                                             (               )
Given the choice of BR(N ) , the researcher estimates θ = θ1 , . . . , θR(N ) , the weights on each of the
grid points. With this approximation, (3) becomes

                                            ∑
                                            R(N )
                                 yi,j ≈             θr gj (xi , β r ) + (yi,j − Pj (x)) .
                                            r=1

We use the ≈ symbol to emphasize that this uses a sieve approximation to the distribution function
                                                        (                   )
F (β). Because each θr enters yi,j linearly, we estimate θ1 , . . . , θR(N ) using the linear probability
                                                r = g (x , β r ).
model regression of yi,j on the R “regressors” zi,j  j  i
                                       ∑  R(N ) r
  To be a valid CDF, θ ≥ 0 ∀ r and r=1 θ = 1. Therefore, the estimator is
                          r

                                                                      2
                                   1 ∑ N   ∑ J           ∑
                                                         R(N )
                      θb = arg min               yi,j −           r 
                                                               θr zi,j                                     (4)
                                θ NJ   i=1   j=1
                                                                              r=1

                                                                                       ∑
                                                                                      R(N )
                              subject to θr ≥ 0 ∀ r = 1, . . . , R (N ) and                   θr = 1.
                                                                                       r=1

There are J “regression observations” for each statistical observation (yi , xi ). This minimization prob-
lem is a quadratic programming problem subject to linear inequality constraints. The minimization
problem is convex and routines like MATLAB’s lsqlin guarantee ﬁnding a global optimum. One can
construct the estimated cumulative distribution function for the random coeﬃcients as
                                                        ∑R(N )
                                        F̂N (β) =                 θ̂r 1 [β r ≤ β] ,
                                                            r=1



                                                              8
where 1 [β r ≤ β] is equal to 1 when β r ≤ β. Thus, we have a structural estimator for a distribution of
random parameters in addition to a ﬂexible method for approximating choice probabilities.
    The approach just presented has two main advantages over other approaches to estimating distri-
butions of random coeﬃcients. First, the approach is computationally simple: we can always ﬁnd a
                                    r = g (x , β r ) before optimization commences, we avoid many
global optimum and, by solving for zi,j  j  i
evaluations of complex structural models such as dynamic programming problems. Second, the ap-
proach is nonparametric. In the next section, we show that if the grid of points is made ﬁner as the
sample size N increases, the estimator F̂N (β) converges to the true distribution F0 . We do not need
to impose that F0 lies in known parametric family.
    On the other hand, a disadvantage is that the estimates may be sensitive to the choice of tuning
parameters. While most nonparametric approaches require choices of tuning parameters, here the
choice of a grid of points is a particularly high-dimensional tuning parameter. FKRB propose cross-
validation methods to pick these tuning parameters, including the number of grid points, the support
of the points, and the grid points within the support.
                                                   exp(x′i,j β r )
Example. 1 (logit) For the logit example,         ∑J                     = gj (xi , β r ). Therefore, for each
                                                1+ h=1 exp(x′i,h β r )
statistical observation i, the researcher computes R · J linear probability model regressors zi,j   r =
          ′   r
    exp(xi,j β )
   ∑           ′      . This computation is done before optimization commences. The outcome for choos-
1+ J h=1 exp(xi,h β )
                   r
                                                                                ∑
ing the outside good 0 does not need to included in the objective function, as Jj=0 gj (xi , β r ) = 1.


4    Consistency in Function Space
Assume that the true distribution function F0 lies in the space F of distribution functions on the
support B of the parameters β. We wish to show that the estimated distribution function F̂N (β) =
∑R(N ) r
  r=1 θ̂ 1 [β ≤ β] converges to the true F0 ∈ F as the sample size N grows large. Most of the
             r

competing nonparametric estimators discussed in the introduction are not only computationally more
challenging, but lack nonparametric consistency theorems for as general a class of economic models.
    To prove consistency, we use the recent results for sieve estimators developed by Chen and Pouzo
(2009), hereafter referred to as CP. We deﬁne a sieve space to approximate F as
         {            ∑R                       {                           ∑R      }}
                                                ( 1            )
                                                              R ′
     FR = F | F (β) =    θ 1 [β ≤ β] , θ ∈ ∆R ≡ θ , . . . , θ
                          r    r
                                                                  | θ ≥ 0,
                                                                     r         r
                                                                              θ =1    ,
                             r=1                                                         r=1

                         {                 }
for a choice of grid BR = β 1 , . . . , β R that becomes ﬁner as R increases. We require FR ⊆ FS ⊂ F
for S > R, or that large sieve spaces encompass smaller sieve spaces. The choice of the grid and R (N )
are up to the researcher; however consistency will require conditions on these choices.
    Based on CP, we prove that the estimator F̂N converges to the true F0 . In their main text, CP study
sieve minimum distance estimators that involve a two-stage procedure. Our estimator is a one-stage
sieve least squares estimator (Chen, 2007) and so we cannot proceed by verifying the conditions in the


                                                   9
theorems in the main text of CP. Instead, we show its consistency based on CP’s general consistency
theorem in their appendix, their Lemma B.1, which we quote in the proof of our consistency theorem
for completeness. As a consequence, our consistency proof veriﬁes CP’s high-level conditions for the
consistency of a sieve extremum estimator.
   Let yi denote the J × 1 ﬁnite vector of binary outcomes (yi,1 , . . . , yi,J ) and let g(xi , β) denote the
corresponding J × 1 vector of choice probabilities (g1 (xi , β) , . . . , gJ (xi , β)) given xi and the random
coeﬃcient β. Then we can deﬁne our sample criterion function as
                                ∫
                     1 ∑N                                          2
                                                                            1 ∑N        ∑R                        2
       Q̂N (F ) ≡           yi − g(xi , β)dF (β)                       =           yi −      θr g(xi , β r )            (5)
                    NJ  i=1
                                                                   E       NJ  i=1       r=1
                                                                                                                  E

for F ∈ FR(N ) , where || · ||E denotes the Euclidean norm. We can rewrite our estimator as

                                      F̂N = argminF ∈FR(N ) Q̂N (F ) + C · νN                                           (6)

where we can allow for some tolerance (slackness) of minimization, C · νN , that is a positive sequence
tending to zero as N gets larger.
   Also let                                      [        ∫                                ]
                                                                                  2
                                     Q(F ) ≡ E       y−       g (x, β) dF (β)         /J
                                                                                  E

be the population objective function.
                                                                                           ∫
   We state assumptions on the model ﬁrst. We write P (x, F ) =                                g(x, β)dF (β). As a distance
measure for distributions, we use the Lévy-Prokhorov metric, denoted by dLP (·), which is a metrization
of the weak topology for the space of multivariate distributions F. The Lévy-Prokhorov metric in the
space of F is deﬁned on a metric space (B, d). We use notation dLP (F1 , F2 ) where the measures
are implicit. This denotes the Lévy-Prokhorov metric dLP (µ1 , µ2 ), where µ1 and µ2 are probability
measures corresponding to F1 and F2 . The Lévy-Prokhorov metric is deﬁned as

dLP (µ1 , µ2 ) = inf {ϵ > 0 | µ1 (C) ≤ µ2 (C ϵ ) + ϵ and µ2 (C) ≤ µ1 (C ϵ ) + ϵ for all Borel measurable C ∈ B} ,

where C is some set of random coeﬃcients and C ϵ = {b ∈ B | ∃ a ∈ C, d (a, b) < ϵ}.                             The Lévy-
Prokhorov metric is a metric, so that dLP (µ1 , µ2 ) = 0 only when µ1 = µ2 . See Huber (1981, 2004).
   The following assumptions are on the economic model and data generating process.

Assumption 1.

  1. Let F be a space of distribution functions on a ﬁnite-dimensional real space B, where B is compact.
      F contains F0 .

  2. Let {(yi , xi )}N
                     i=1 be i.i.d.

  3. Let β be independently distributed from x.


                                                              10
   4. Assume the model g (x, β) is identiﬁed, meaning that for any F1 ̸= F0 , F1 ∈ F , we have
       P (x, F0 ) ̸= P (x, F1 ) for almost all x ∈ X̃ , where X̃ is a subset of X , the support of x, with
       positive probability.5

   5. Q (F ) is continuous on F in the weak topology.

    Assumptions 1.1, 1.2, and 1.3 are standard for nonparametric mixtures models with cross-sectional
data. Assumption 1.4 requires that the model be identiﬁed at a set of values of xi that occurs with
positive probability. The assumption rules out so-called fragile identiﬁcation that could occur at values
of x with measure zero (e.g. identiﬁcation at inﬁnity). Assumptions 1.4 and 1.5 need to be veriﬁed for
each economic model gA (x, β). We will do so for our ﬁve examples below.6
Remark 1. Assumption 1.5 is satisﬁed when g(x, β) is continuous in β for all x because in this case
P (x, F ) is also continuous on F for all x in the Lévy-Prokhorov metric. Then by the dominated conver-
gence theorem, the continuity of Q (F ) in the weak topology follows from the continuity of P (x, F ) on
F for all x and P (x, F ) ≤ 1 (uniformly bounded). Here the continuity of P (x, F ) on F means for any
                                                             ∫                   ∫
F1 , F2 ∈ F such that dLP (F1 , F2 ) → 0 it must follow that   gj (x, β)dF1 (β) − gj (x, β)dF2 (β) → 0
for all j. This holds by the deﬁnition of weak convergence when g(x, β) is continuous and bounded
and because the Lévy-Prokhorov metric is a metrization of the weak topology.


Remark 2. If the support B is a ﬁnite set (i.e. types are discrete with known support), the continuity
in the weak topology holds even when gj (x, β) is discontinuous as long as it is bounded because in
this case the Lévy-Prokhorov metric becomes equivalent to the total variation metric (see Huber 1981,
                    ∫                  ∫
p.34). This implies gj (x, β)dF1 (β) − gj (x, β)dF2 (β) → 0 as long as gj (x, β) is bounded.
    In addition to Assumption 1, we also require that the grid of points be chosen so that the grid BR
becomes dense in B in the usual topology on the reals.

Condition 1. Let the choice of grids satisfy the following properties:

   1. Let BR become dense in B as R → ∞.

   2. FR ⊆ FR+1 ⊆ F for all R ≥ 1.
                                                      R(N ) log R(N )
   3. R(N ) → ∞ as N → ∞ and it satisﬁes                     N          → 0 as N → ∞.

    The ﬁrst two parts of this condition have previously been mentioned, and ensure that the sieve
spaces give increasingly better approximations to the space of multivariate distributions. Condition
1.3 speciﬁes a rate condition so that the convergence of the sample criterion function Q̂N (F ) to the
population criterion function Q(F ) is uniform over FR . Uniform convergence of the criterion function
and identiﬁcation are both key conditions for consistency.
   5
     This is with respect to the probability measure of the underlying probability space. This probability is well deﬁned
whether x is continuous, discrete or some elements of xi are functions of other elements (e.g. polynomials or interactions).
   6
     The continuity condition in Assumption 1.5 can be relaxed to lower semicontinuity. The examples we consider in
this paper satisfy the continuity condition.


                                                            11
                                                               (        ) p
Theorem 1. Suppose Assumption 1 and Condition 1 hold. Then, dLP F̂N , F0 → 0.

    See Appendix A for the proof.
Remark 3. The literature on sieve estimation has not established general results on the asymptotic
distribution of sieve estimators, in function space. However, for rich classes of approximating basis
functions that do not include our approximation problem, the literature has shown conditions under
which ﬁnite dimensional functionals of sieve estimators have asymptotically normal distributions. In
the case of nonparametric random coeﬃcients, we might be interested in inference in the mean or
median of β. For the demand estimation, say Example 3, we might be interested in average responses
(or elasticities) of choice probabilities with respect to changes in particular product characteristics. Let
ΠN F0 be a sieve approximation to F0 in our sieve space FR(N ) . If we could obtain an error bound for
dLP (ΠN F0 , F0 ), we could also derive the convergence rate in the Lévy-Prokhorov metric (Chen 2007).
If the error bound shrinks fast enough as R(N ) increases, we conjecture that we could also prove that
plug-in estimators for functionals of F0 are asymptotically normal (Chen, Linton, and van Keilegom
2003).7 Error bounds for discrete approximations are available in the literature for a class of parametric
distributions F , but we are not aware of results for the unrestricted class of multivariate distributions.
In an earlier version of the paper, we derived the convergence rate in the L1 metric instead.


5     Well-Posedness
Chen (2007) and Carrasco, Florens and Renault (2007) distinguish between functional (here the distri-
bution) optimization and identiﬁcation problems that are well-posed and problems that are ill-posed.
Using Chen’s deﬁnition, the optimization problem of maximizing the population criterion function
Q (F ) with respect to the distribution function F will be well-posed if dLP (Fn , F0 ) → 0 for all se-
quences {Fn } in F such that Q(Fn ) − Q(F0 ) → 0. The problem will be ill-posed if there exists a
sequence {Fn } in F such that Q(Fn ) − Q(F0 ) → 0 but dLP (Fn , F0 ) 9 0.8 We now argue that our
problem is well-posed.
    The space F of distributions on B is compact in the weak topology if B itself is compact (Assump-
tion 1.1) in Euclidean space (Parthasarathy 1967, Theorem 6.4). Also, Q (F ) is continuous on F by
Assumption 1.5. It follows that with our choice of the criterion function and metric, our optimization
problem is well posed in the sense of Chen (2007) because for every ϵ > 0 we have

                          inf              (Q(F ) − Q(F0 )) ≥          inf            (Q(F ) − Q(F0 )) > 0,            (7)
                F ∈FR(N ) :dLP (F,F0 )≥ϵ                        F ∈F :dLP (F,F0 )≥ϵ

   7
     We conjecture that we could prove an analog to Theorem 2 in Chen et al (2003) if we could verify analogs to
conditions (2.4)–(2.6) in that paper for our sieve space.
   8
     Whether the problem is well-posed or ill-posed also depends on the choice of the metric. For example, if one uses
the total variation distance metric instead of the the Lévy-Prokhorov metric, the problem will be ill-posed because the
distance between a continuous distribution and any discrete distribution will always be equal to one in the total variation
metric.




                                                            12
where the ﬁrst inequality holds because FR(N ) ⊂ F by construction and the second, strict inequality
holds as the minimum is attained by continuity and compactness and because the model is identiﬁed
(Assumption 1.4), as we argue in the proof of Theorem 1. Therefore, our optimization problem satisﬁes
Chen’s deﬁnition of well-posedness.


6     Consistency for Models with Homogenous Parameters
In many empirical applications, it is common to have both random coeﬃcients β and ﬁnite-dimensional
parameters α ∈ A ⊆ Rdim(α) . We write the model choice probabilities as g(x, β, α) and the aggregate
choice probabilities as P (x, F, α). Here we consider the consistency of estimators for models with both
homogenous parameters and random coeﬃcients.
Remark 4. Estimating a model allowing a parameter to be a random coeﬃcient when in truth the
parameter is homogeneous will not aﬀect consistency if the model with random coeﬃcients is identiﬁed.


Remark 5. Searching over α as a homogeneous parameter requires nonlinear least squares. The opti-
mization problem may also not be globally convex. The objective function may not be diﬀerentiable
for our examples where g(x, β, α) involves an indicator function.
    Our estimator for models with homogeneous parameters is deﬁned as (similarly to (6))

                          (α̂N , F̂N ) = argmin(α,F )∈A×FR(N ) Q̂N (α, F ) + C · νN ,

where Q̂N (α, F ) denotes the corresponding sample criterion function. Q(α, F ) is the population crite-
rion function based on the model g(x, β, α). An alternative computational strategy is that the estimator
can be proﬁled as

                       F̂N (α) = argminF ∈FR(N ) Q̂N (α, F ) + C · νN for all α ∈ A.

Proﬁling gives us                                    (          )
                                  α̂N = argmina∈A Q̂N α, F̂N (α) + C · νN ,                          (8)

and therefore F̂N = F̂N (α̂N ).
    Using Theorem 1 of Chen, Linton, and van Keilegom (2003), below we show α̂N is consistent (so
F̂N is as well when combined with Theorem 1). We replace Assumptions 1.4 and 1.5 with Assumptions
2.2 and 2.3 below and add one restriction on g(x, β, α).

Assumption 2.

    1. Let A be the parameter space of α, which is a compact subset of Rdim(α) .




                                                      13
     2. Assume the model g (x, β, α) is identiﬁed, meaning that for any (α1 , F1 ) ̸= (α0 , F0 ), (α1 , F1 ) ∈
              A × F, we have P (x, F0 , α0 ) ̸= P (x, F1 , α1 ) for almost all x ∈ X̃ , where X̃ is a subset of X , the
              support of x, with positive probability.

     3. Q (α, F ) is continuous on A and is continuous on F in the weak topology.

     4. Either (i) g(x, β, α) is Lipschitz continuous in α, (ii) α enters g (x, β, α) only through indicator
                             (            )
        functions or (iii) g x, β, α1 , α2 with α = (α1 , α2 ) is Lipschitz continuous in α1 and α2 enters
          (            )
        g x, β, α1 , α2 only through indicator functions.

         If homogeneous parameters were added, Assumption 2.4.i would hold for Example 1, the logit model
with random coeﬃcients. Assumption 2.4.ii would hold for Examples 2–5. Finally, Assumption 2.4.iii
would hold for a joint discrete and continuous demand model with logit errors in the discrete choice
utility and for a discrete choice demand model with logit errors and price endogeneity, both of which
are described in FKRB, Section 5.
         We present the consistency theorem for the estimator of the homogenous parameters.

Corollary 1. Suppose Assumptions 1.1 through 1.3, Assumption 2 and Condition 1 hold. Then,
          p
α̂N → α0 .


7         Identiﬁcation and Continuity for Examples
We return to the examples we introduced in Section 2.1. We verify the two key conditions for each model
gA (x, β): Assumption 1.4, identiﬁcation of F (β), and Assumption 1.5, continuity of the population
objective function under the Lévy-Prokhorov metric. Throughout this section, we assume Assumptions
1.1–1.3 hold. Note that Matzkin (2007) is an excellent survey of older results on the identiﬁcation of
models with heterogeneity.

Example. 1 (logit) The identiﬁcation of F (β) in the random coeﬃcients logit model is the main con-
tent of Bajari, Fox, Kim and Ryan (2010, Theorem 13).9 Assumption 12 in Bajari et al states that “The
support of x, X contains x = 0, but not necessarily an open set surrounding               (              it. Further, the support        )
contains a nonempty open set of points (open in R                   dim(xj ) ) of the form x1 , . . . , x′j−1 , x′j , x′j+1 , . . . , x′J =
                                                                                            ′
(                                       )
 0′ , . . . , 0′ , x′j , 0′ , . . . , 0′ .” 10 Bajari et al also require the support of X to be a product space, which
rules out including polynomial terms in an element of xj or including interactions of two elements of
xj . Given this assumption, Assumption 1.4 holds. Assumption 1.5 holds by Remark 1 in the current
paper.


    9
         Theorem 13 of Bajari et al also allows homogeneous, product-speciﬁc intercepts.
    10
         Bajari et al discusses what x = 0 means when the means of product characteristics can be shifted.




                                                                   14
Example. 2 (binary choice) Ichimura and Thompson (1998, Theorem 1) establish the identiﬁcation
of F (β) under the conditions that i) the coeﬃcient on one of the the non-intercept regressors in x
is known to always be positive or negative and ii) there are large and product supports on each of
the regressors other than the intercept. This rules out polynomial terms and interactions. For i), we
formally make this “special regressor” assumption as follows. Let xi,k∗ be an element in xi and xi,−k∗
be the subvector of xi that removes xi,k∗ .

Assumption 3. (binary choice) (i) The conditional CDF of xi,k∗ given xi,−k∗ , labeled as Gxi,k∗ |xi,−k∗ ,
is continuous in xi,k∗ for almost every value of xi,−k∗ . (ii) Let the support of the coeﬃcient βk∗ be
known to be strictly positive or negative.

   If we impose the scale normalization βk∗ = ±1, Assumption 1.4 holds if we add large support
conditions on each regressor in x. It turns out that Assumption 3, without needing large support, also
ensures the continuity of Q(F ) on F, in the weak topology. The proof is in the appendix.

Lemma 1. (binary choice) Suppose Assumptions 1.1-1.3 and 3 hold. Then Q(F ) is continuous on
F in the weak topology and Assumption 1.5 holds.

    An advantage of our estimator is the ease of imposing sign restrictions if necessary. Because the
                         (                     )
researcher picks BR(N ) = β 1 , . . . , β R(N ) , the researcher can choose the grid so that the ﬁrst element of
each vector β r is always positive, for example. Note that binary choice is a special case of multinomial
choice, so the non-nested identiﬁcation conditions in example 3, below, can replace these used here.


Example. 3 (multinomial choice without logit errors) Fox and Gandhi (2010, Theorem 7.6)
study the identiﬁcation of the multinomial choice model without logit errors. Our linear speciﬁcation
of the utility function for each choice is a special case of what they allow. Fox and Gandhi require a
choice-j-speciﬁc special regressor along the lines of Assumption 3. On other hand, Fox and Gandhi
allow polynomial terms and interactions for x’s other than the choice-j-speciﬁc special regressors, unlike
examples 1 and 2. They do not require large support for the x’s that are not special regressors. The
most important additional assumption for identiﬁcation is that Fox and Gandhi require that F (β)
takes on at most a ﬁnite number T of support points, although the number T and support point
identities β 1 , . . . , β T are learned in identiﬁcation. The number T in the true F 0 is not related in any
way to the ﬁnite-sample R (N ) used for estimation in this paper. So Assumption 1.4 holds under this
restriction on F. For the continuity Assumption 1.5, the equivalent of Assumption 3 also implies the
equivalent of Lemma 1. We omit the formal statement and proof for conciseness.



Example. 4 (Cobb-Douglas production function / linear regression) Beran and Millar (1994,
Proposition 2.2) establish identiﬁcation of F (β), in a model with one regressor and one slope coeﬃcient.
However, they use individual realizations of the continuous outcome y and not intervals such as [a1 , a2 ).

                                                      15
To implement our estimator, we discretize the outcome space Y into J intervals [aj−1 , aj ), j = 1, . . . , J.
The following lemma uses techniques related to Fox and Gandhi (2010) to prove that the distribution
of random coeﬃcients is identiﬁed in an interval-censored linear regression model with any number of
regressors.

Lemma 2. (linear regression with interval censoring) Let the underlying regression model be
y = x′ β where the ﬁrst covariate, x1 , can take on any value on R+ , conditional on the other components
of x, x−k . Assume the researcher uses only data on the dependent variable yi,j = 1 [yi ∈ [aj−1 , aj )] for
J intervals. Then F (β) is identiﬁed if the true distribution takes on at most an unknown, ﬁnite number
of support points. Therefore Assumption 1.4 holds.

    The proof is in an appendix. As with example 4, polynomial terms and interactions are not ruled
out, except for those on x1 . Note that there is no common sign restriction on the coeﬃcient on
the regressor with large support. For the continuity Assumption 1.5, the equivalent of Assumption
3 also implies the equivalent of Lemma 1. This assumption is almost without loss of generality in
the production function example, as labor and capital have continuous supports and positive input
elasticities.


Example. 5 (joint continuous and discrete demand) Fox and Gandhi (2011) explore the identi-
ﬁcation of this selection model with random coeﬃcients. Like in Example 3 above, a choice-j-speciﬁc
special regressor must enter the utility for each discrete choice. Also like in Example 3, F (β) takes
on at most a ﬁnite number T of support points, although to emphasize, T is learned in identiﬁcation,
as are the support points. So Assumption 1.4 might hold under this restriction on F. The main new
issue is the same as Example 4: the discretization of the continuous dependent variable in our linear
probability model estimation procedure. The Fox and Gandhi result uses the non-discretized continu-
ous outcomes, and like in Example 4,
                                  [ to implement
                                          )      our estimator, we discretize the continuous portion
of the outcome space into intervals ajι−1 , ajι for each choice j. This requires a large-support regressor
in the outcome equations as well. Combining the arguments in Fox and Gandhi (2011) with similar
arguments to those in the proof of Lemma 2 shows identiﬁcation of the selection model in the case of
interval censoring. For the continuity Assumption 1.5, the equivalent of Assumption 3 also implies the
equivalent of Lemma 1.


8    Conclusion
We analyze a nonparametric method for estimating general mixtures models. Our method allows the
researcher to drop standard parametric assumptions, such as independent normal random coeﬃcients,
that are commonly used in applied work. Convergence of an optimization routine to the global optimum
is guaranteed under linear regression with linear constraints, something that cannot be said for other
statistical objective functions. Also, our estimator is easier to program and to use than alternatives


                                                     16
such as the EM algorithm. The estimator is also useful for reducing the number of times complex
structural models, such as dynamic programs, need to be evaluated in estimation.
   We explore the asymptotic properties of the nonparametric distribution estimator. We show con-
sistency in the function space of all distributions under the weak topology by viewing our estimator
as a sieve estimator and verifying high-level conditions in Chen and Pouzo (2009). Many alternative
mixtures estimators lack consistency results in such generality. We verify the conditions for consistency
for ﬁve example models, each of which is widely used in empirical work.




                                                   17
A     Proof of Consistency: Theorem 1
We verify the conditions of CP’s Lemma B.1 (also see Theorem 3.1 in Chen (2007)) in our consistency
proof. To provide completeness, we ﬁrst present our simpliﬁed version of CP’s Lemma B.1, which does
                                                            ∑           ∫             2
not incorporate a penalty function. Deﬁne Q̂N (F ) = N1J N         yi − g (xi , β) dF E and Q(F ) ≡
  [     ∫                 ]                                   i=1
                     2
E y − g (x, β) dF E /J .

Lemma 3. Lemma B.1 of CP: Let F̂N be such that Q̂N (F̂N ) ≤ inf F ∈FR(N ) Q̂N (F ) + Op (νN ) with
νN → 0. Suppose the following conditions (A.3.1)-(A.3.4) hold:

    • (A.3.1) (i) Q(F0 ) < ∞; (ii) there is a positive function δ(N, R(N ), ε) such that for each N ≥ 1,
      R ≥ 1, and ε > 0, inf F ∈FR(N ) :dLP (F,F0 )≥ε Q(F )−Q(F0 ) ≥ δ (N, R(N ), ε) and lim inf N →∞ δ (N, R(N ), ε) ≥
      0 for all ε > 0.

    • (A.3.2) (i) (F, dLP (·)) is a metric space; (ii) FR ⊆ FR+1 ⊆ F for all R ≥ 1, and there exists a
      sequence ΠN F0 ∈ FR(N ) such that dLP (ΠN F0 , F0 ) = O(ςN ) and ςN → 0 as N → ∞.

    • (A.3.3) (i) Q̂N (F ) is a measurable function of the data {(yi , xi )}N
                                                                            i=1 for all F ∈ FR(N ) ; (ii) F̂N is
      well-deﬁned and measurable with respect to the Borel σ-ﬁeld generated by the weak topology.
                                                                                   p
    • (A.3.4) (i) Let ĉQ (R(N )) = supF ∈FR(N ) Q̂N (F ) − Q(F ) → 0;
               {                                       }                 p
      (ii) max ĉQ (R(N )), νN , |Q (ΠN F0 ) − Q (F0 )| /δ (N, R(N ), ε) → 0 for all ε > 0.
                       p
Then dLP (F̂N , F0 ) → 0.

Proof. Under condition (A.3.3) (ii), F̂N is well-deﬁned and measurable. Then for any ε > 0,



      Pr(dLP (F̂N , F0 ) ≥ ε) ≤ Pr(                   inf              Q̂N (F ) ≤ Q̂N (ΠN F0 ) + O(νN ))
                                            F ∈FR(N ) :dLP (F,F0 )≥ε

  ≤ Pr(             inf           {Q(F ) + (Q̂N (F ) − Q(F ))} ≤ Q(ΠN F0 ) + (Q̂N (ΠN F0 ) − Q(ΠN F0 )) + O(νN ))
          F ∈FR(N ) :dLP (F,F0 )≥ε

  ≤ Pr(             inf              Q(F ) ≤ 2ĉQ (R(N )) + Q(ΠN F0 ) + O(νN ))
          F ∈FR(N ) :dLP (F,F0 )≥ε

  ≤ Pr(             inf              Q(F ) − Q(F0 ) ≤ 2ĉQ (R(N )) + Q(ΠN F0 ) − Q(F0 ) + O(νN ))
          F ∈FR(N ) :dLP (F,F0 )≥ε

  ≤ Pr(δ(N, R(N ), ε) ≤ 2ĉQ (R(N )) + |Q(ΠN F0 ) − Q(F0 )| + O(νN ))

which goes to zero by condition (A.3.4).

    Now we provide our consistency proof for the baseline estimator. Because our estimator is an
extremum estimator, we can take νN to be arbitrary small. We start with the condition (A.3.1). The
condition Q(F0 ) < ∞ holds because Q(F ) ≤ 1 for all F ∈ F. Next we will verify the condition

                                      inf             Q(F ) − Q(F0 ) ≥ δ(N, R(N ), ε) > 0                   (9)
                           F ∈FR(N ) :dLP (F,F0 )≥ε


                                                                 18
for each N ≥ 1, R(N ) ≥ 1, ε > 0, and some positive function δ(N, R(N ), ε) to be deﬁned below. We
will use our assumption of identiﬁcation (Assumption 1.4). Let m (x, F ) = P (x, F0 ) − P (x, F ). Note
that we have
                  [                                  ]   [                  ]   [                  ]
         Q(F ) = E ||y − P (x, F0 ) + m(x, F )||2E /J = E ||y − P (x)||2E /J + E ||m(x, F )||2E /J

because E[(y−P (x))′ m(x, F )] = 0 by the law of iterated expectation and E[y−P (x)|x] = 0. Therefore,
for each F ∈ F, we have
                              [                 ]   [                  ]   [                  ]
            Q(F ) − Q(F0 ) = E ||m(x, F )||2E /J − E ||m(x, F0 )||2E /J = E ||m(x, F )||2E /J                       (10)

because m(x, F0 ) = 0 and the condition (9) holds due to our assumption of identiﬁcation as the
following argument shows.
       Consider E[||m (x, F ) ||2E ], with m (x, F ) deﬁned above, as a map from F to R+ ∪ {0}. For any
F ̸= F0 , E[||m (x, F ) ||2E ] takes on positive values for each F ∈ F , because the model is identiﬁed on a
set X̃ with positive probability. Then note that E[||m (x, F ) ||2E ] is continuous in F and also note that
FR(N ) is compact. Therefore E[||m (x, F ) ||2E ] attains some strictly positive minimum on {F ∈ FR(N ) :
                                                                                   [                 ]
dLP (F, F0 ) ≥ ε}. Then we can take δ(N, R(N ), ε) = inf F ∈FR(N ) :dLP (F,F0 )≥ε E ||m(x, F )||2E /J > 0 for
all R(N ) ≥ 1 with ε > 0.
       We further claim lim inf N →∞ δ(N, R(N ), ε) > 0 because

          δ(N, R(N ), ε) =               inf            (Q(F ) − Q(F0 )) ≥           inf        (Q(F ) − Q(F0 ))
                               F ∈FR(N ) :dLP (F,F0 )≥ε                       F ∈F :dLP (F,F0 )≥ε

                          =           inf            E[||m (x, F ) ||2E /J] > 0,
                               F ∈F :dLP (F,F0 )≥ε


where the ﬁrst inequality holds because FR(N ) ⊆ F by construction and the second, strict inequality
holds because the model is identiﬁed (Assumption 1.4).11
       Next we consider (A.3.2). First note that (F, dLP ) is a metric space and we have FR ⊆ FR+1 ⊆ F
for all R ≥ 1 by construction of our sieve space. Then we claim that there exists a sequence of
functions ΠN F0 ∈ FR(N ) such that dLP (ΠN F0 , F0 ) → 0 as N → ∞. First, BR(N ) becomes dense in B
by assumption. Second, FR(N ) becomes dense in F because the set of distributions on a dense subset
BR(N ) ⊂ B is itself dense. To see this, remember that the class of all distributions with ﬁnite support
is dense in the class of all distributions (Aliprantis and Border 2006, Theorem 15.10). Any distribution
with ﬁnite support can be approximated using a ﬁnite support in a dense subset BR(N ) (Huber 2004).
       Next, to show (A.3.3) holds, we use Remark B.1.(1)(a) of CP. First note that FR is a compact
subset of F for each R because BR is a compact subset of B.12 Second we need to show that for any
  11
     As we discussed in the main text the space F of distributions on B is compact in the weak topology because we
assume B itself is compact.
  12
     Alternatively we can also see that FR is compact because the simplex, ∆R(N ) , itself is compact as we argue below.
For any given R and BR , consider two metric spaces, (FR , dLP ) and (∆R , || · ||E ). Then we can deﬁne a continuous map
ψ : ∆R → FR because any element in ∆R determines an element in FR . The map is continuous in the sense that for



                                                              19
data {(yi , xi )}N
                 i=1 , Q̂N (F ) is lower semicontinuous on FR for each R ≥ 1. Since FR is compact, this
lower semicontinuity or continuity means our estimator is well deﬁned as the minimum in (6). Note
∫               ∑
  g (x, β) dFl = Rr=1 θl g (x, β ) for Fl ∈ FR , l = 1, 2. Then, for any F1 , F2 ∈ FR(N ) , applying the
                       r        r

triangle inequality, we obtain

                                         ∑N ∑dim(yi )      ∫
  |Q̂N (F1 ) − Q̂N (F2 )| ≤ 2                        yi,j     gj (xi , β)(dF1 − dF2 ) /N J
                                          i=1   j=1
                                     ∑N ∑dim(yi ) ∫                               ∫
                                   +                { gj (xi , β)(dF1 + dF2 )}       gj (xi , β)(dF1 − dF2 ) /N J
                                        i=1   j=1
                                       ∑N ∑dim(yi ) ∫
                                   ≤ 4                    gj (xi , β)(dF1 − dF2 ) /N J,
                                              i=1       j=1

                                                                                     ∫
where the second inequality holds because yi,j , gj (xi , β), and                        gj (xi , β)dF (β) are uniformly bounded
by 1 for all j and xi . Then because gj (xi , β) is uniformly bounded by 1 and F1 and F2 are discrete
distributions with the ﬁnite support BR , in this case weak convergence implies that almost surely
Q̂N (F ) is continuous on FR , i.e. for any F1 , F2 ∈ FR such that dLP (F1 , F2 ) → 0, it follows that
|Q̂N (F1 ) − Q̂N (F2 )| → 0 almost surely.13 Continuity is stronger than lower semicontinuity. Therefore
(A.3.3) holds by Remark B.1.(1) (a) of CP.
       Next there are two conditions to verify in (A.3.4). We ﬁrst focus on the uniform convergence of
                                                                                         p
Q̂N (F ) to Q(F ) for F ∈ FR(N ) , supF ∈FR(N ) |Q̂N (F ) − Q(F )| → 0. It is convenient to view Q̂N (F )
and Q(F ) as functions of θ ∈ ∆R(N ) and then write them as Q̂N (θ) and Q(θ), respectively. Then the
uniform convergence condition to verify becomes

                                                                                    p
                                                        sup    |Q̂N (θ) − Q(θ)| → 0.                                       (11)
                                                    θ∈∆R(N )


Using measures of complexity of spaces, let N(ε, T , || · ||) denote the covering number of the set T with
balls of radius ε with an arbitrary norm || · || and let N[] (ε, T , ∥·∥) denote the bracketing number of
the set T with ε-brackets. Deﬁne for any R, the class of measurable functions
                                                                   ∑
                                  GR = {l(y, x, θ) = ||y −               θr g(x, β r )||2E /J : θ ∈ ∆R }.                  (12)
                                                                     r

any sequence θn → θ in ∆R we have ψ(θn ) → ψ(θ) in FR . Then it is a simple proof to show that if ∆R is compact, then
FR = {ψ(θ) : θ ∈ ∆R } is also compact.

Proof. Consider an arbitrary sequence {Fn }n∈N ⊆ FR . Since Fn ∈ {ψ(θ) : θ ∈ ∆R } for all n ∈ N, we know that there
exists θn ∈ ∆R with ψ(θn ) = Fn for all n ∈ N. Then {θn }n∈N ⊆ ∆R . Next note that since ∆R is compact, there exists
some subsequence {θln }n∈N with θln → θ̄ ∈ ∆R . Since the map ψ is continuous, it follows that ψ(θln ) → ψ(θ̄). And
because ψ(θln ) = Fln , then Fln → ψ(θ̄) ∈ FR because θ̄ ∈ ∆R . Therefore, we conclude FR is also compact when ∆R is
compact.
                   ∫                               ∑R
  13
       Note that       gj (xi , β)(dF1 − dF2 ) ≤    r=1   |θ1r − θ2r | → 0 as dLP (F1 , F2 ) → 0 for any ﬁnite R.




                                                                      20
                           ∑N
Note (i) Q̂N (θ) = N −1       i=1 l(yi , xi , θ),   (ii) {(yi , xi )}N
                                                                     i=1 are i.i.d., and (iii) E[supθ∈∆R(N ) |l(y, x, θ)|] ≤
1 < ∞. Then by Pollard (1984, Theorem II.24) (also see Chen (2007, Section 3.1, page 5592) for
                                                                                                                    p
related discussion), the uniform convergence (11) holds if and only if log N(ε, GR , || · ||L1 ,N )/N → 0 for
all ε > 0, where || · ||L1 ,N denotes the L1 (PN )-norm and PN denotes the empirical measure of the data
{(yi , xi )}N
            i=1 .
    The term l(y, x, θ) is Lipschitz in θ, as

                              1 ∑          ∑                               ∑                          ∑
                                dim(y)
|l(y, x, θ1 ) − l(y, x, θ2 )| ≤       (2yj      gj (x, β r )|θ1r − θ2r | +   gj (x, β r )(θ1r + θ2r )   gj (x, β r )|θ1r − θ2r |)
                              J             r                              r                          r
                                 j=1
                                    ∑R                           √
                            ≤ M (·)        |θ1r − θ2r | ≤ M (·) R||θ1 − θ2 ||E
                                               r=1

with some function E[M (·)2 ] < ∞. The ﬁrst inequality is obtained by the triangle inequality and
the third inequality holds due to the Cauchy-Schwarz inequality. We also know ∆R is a compact
                                                               ∑
subset of RR . Now take M (·) = 4, noting that yj , gj (·), and R            r r
                                                                 r=1 gj (x, β )θ are uniformly bounded
by 1. Then from Theorem 2.7.11 of van der Vaart and Wellner (1996), we have N[] (2ε, GR , ∥·∥) ≤
  (                ) ( √ )R
N 4√ε R , ∆R , ∥·∥E = 4 ε R   for any norm ∥·∥. Therefore as long as R(N ) log R(N )/N → 0, the
                                              (              )      (               )  ( √ )R
uniform convergence condition holds because N ε, GR , ∥·∥L1,N ≤ N[] 2ε, GR , ∥·∥L1,N ≤ 4 ε R
(van der Vaart and Wellner 1996, page 84).
    To satisfy the second condition in (A.3.4), we need to bound all three terms in the max{·} function.
We have shown the uniform convergence of the sample criterion function (this also satisﬁes the ﬁrst
condition in (A.3.4.)) and we can take νN to be small enough. We also have |Q(ΠN F0 ) − Q(F0 )| → 0,
which is trivially satisﬁed by the continuity of Q(F ) in F and dLP (ΠN F0 , F0 ) → 0. Therefore because
lim inf N →∞ δ (N, R(N ), ε) > 0, the condition (A.3.4) is satisﬁed.
    We have veriﬁed all the conditions in Lemma 3 (Lemma B.1 of CP) and this completes the consis-
tency proof.

A.1      Proof of Corollary 1
We verify corresponding conditions in Theorem 1 of Chen, Linton, and van Keilegom (2003). Although
their Theorem 1 is written in terms of moment based estimations, it can be easily modiﬁed to “M-
estimators” that include our estimator (as noted in their footnote 3).
    Condition (1.1) (extremum estimator) is satisﬁed by deﬁnition of our estimator as an extremum
estimator in (8). Condition (1.2) (identiﬁcation) is satisﬁed by Assumption 2.2 because

                                         inf           (Q(α, F0 (α)) − Q(α0 , F0 )) > 0
                                  α∈A:||α−α0 ||E ≥ϵ


where F0 (α) = argminF ∈F Q(α, F ) (which is well deﬁned and exists because F is compact and Q(α, F )
is continuous on F), F0 = F0 (α0 ), and again the minimum is attained because Q(F, α) is continuous



                                                               21
in α and F and A × F is compact.
    Condition (1.3) (continuity) holds by Assumption 2.3. Condition (1.4) (consistency for the non-
parametric component) holds by our main Theorem 1, where we treat F0 (α) as the true parameter for
each α ∈ A.
    The last condition we need to verify is their Condition (1.5) (uniform convergence of the sample
criterion function). We have veriﬁed the uniform convergence of the sample criterion function over
FR(N ) in the proof of Theorem 1. Here we need to verify the uniform convergence over A × FR(N ) such
that
                                                                             p
                                      sup          Q̂N (α, F ) − Q(α, F ) → 0.                            (13)
                                (α,F )∈A×FR(N )

For this purpose deﬁne for any R, the class of measurable functions (similar to (12))
                                                 ∑
                  GR
                   α
                     = {l(y, x, θ, α) = ||y −         θr g(x, β r , α)||2E /J : (α, θ) ∈ A × ∆R }
                                                  r

                                   ∑N
and note that Q̂N (α, F ) = N −1     i=1 l(yi , xi , θ, α).   Then again by Pollard (1984, Theorem II.24), the
uniform convergence (13) holds if and only if the entropy satisﬁes log N(ε, GR
                                                                             α , || · ||
                                                                                        L1 ,N ) = op (N ) for
all ε > 0. Note that the entropy measure of GR
                                             α is bounded by the sum of two entropies, one associated

with FR(N ) and the other one associated with A. We have shown that the former is op (N ) in the proof
of Theorem 1. We also note that the latter satisﬁes the entropy condition (and so is op (N )) under
Assumption 2.4 by Theorem 2.7.11 of van der Vaart and Wellner (1996) (for the Lipschitz case) and
because the class of indicator functions belong to the Vapnik-Červonenkis class and has a uniformly
bounded entropy (Theorem 2.6.7 of van der Vaart and Wellner 1996).


B      Proof of Lemma 1
To establish continuity we need to show that for any F1 , F2 ∈ F such that dLP (F1 , F2 ) → 0 we have
|Q(F1 ) − Q(F2 )| → 0. Obtain for any F1 , F2 ∈ F using a similar derivation to (10)

                   |Q(F1 ) − Q(F2 )| = |Q(F1 ) − Q(F0 ) − {Q(F2 ) − Q(F0 )}|                              (14)
                        [{∫                           }2 {∫                          }2 ]
                   = E        g1 (xi , β) (dF1 − dF0 ) −     g1 (xi , β) (dF2 − dF0 )
                         [{∫                                  }∫                         ]
                   = E         g1 (xi , β) (dF1 + dF2 − 2dF0 )   g1 (xi , β) (dF1 − dF2 ) .




                                                         22
By Assumption 1.3 (independence of random coeﬃcients from xi ), applying the law of iterated expec-
tation, we can rewrite
                          [{∫                                }∫                            ]
                     E      g1 (xi , β) (dF1 + dF2 − 2dF0 )        g1 (xi , β) (dF1 − dF2 )
                       [∫ {∫      (       )                      }                          ]
                   = E         g1 xi , β̃ (dF̃1 + dF̃2 − 2dF̃0 ) g1 (xi , β) (dF1 − dF2 )
                     ∫   [{ ∫     (       )                                  }]
                   =   E       g1 xi , β̃ g1 (xi , β) (dF̃1 + dF̃2 − 2dF̃0 ) (dF1 − dF2 )                      (15)
                     ∫              ∫
                   ≡   h(β)dF1 − h(β)dF2 ,                                                                     (16)


where we use β̃ instead of β to emphasize that β̃ is not subject to the outer integral in (15) and
we use (F̃1 , F̃2 , F̃0 ) = (F1 , F2 , F0 ) to emphasize that β is not subject to the inner integral inside the
expectation in (15).
    Now we further analyze the expectation function denoted by h(β) as a function of β in (15). Below
we will show that this function h(β) is continuous in β and is also bounded. Therefore by weak
convergence, for any F1 , F2 ∈ F such that dLP (F1 , F2 ) → 0, we also have |Q(F1 ) − Q(F2 )| → 0 by (14)
and (16). This will complete the proof of the continuity of Q(F ) on F in the weak topology.
    By Assumption 1.3 (independence of random coeﬃcients from xi ) and Assumption 3, applying the
law of iterated expectation several times and assuming (w.l.o.g.) that the support of βk∗ is known to
take strictly positive values (or normalize the coeﬃcient to be 1), we can write
                                  [             [∫      (       )                                   ]]
              h(β) = Exi,−k∗ Exi,k∗ |xi,−k∗           g1 xi , β̃ g1 (xi , β) (dF̃1 + dF̃2 − 2dF̃0 )
                             [∫                      [ (        )           ]                        ]
                   = Exi,−k∗    Exi,k∗ |xi,−k∗        g1 xi , β̃ g1 (xi , β) (dF̃1 + dF̃2 − 2dF̃0 )
                     ∫              ∫
                   =                    G̃xi,k∗ |xi,−k∗ (dF̃1 + dF̃2 − 2dF̃0 )dGxi,−k∗                         (17)
                           support(xi,−k∗ )


where we denote14
                                             (                               )
                        G̃xi,k∗ |xi,−k∗ = Pr x′i β ≥ 0 and x′i β̃ ≥ 0 xi,−k∗
                           (               {                                     })
                      = Pr xi,k∗ ≥ max −x′i,−k∗ β−k∗ /βk∗ , −x′i,−k∗ β̃−k∗ /β̃k∗
                                           (      {                                      })
                      = 1 − Gxi,k∗ |xi,−k∗ max −x′i,−k∗ β−k∗ /βk∗ , −x′i,−k∗ β̃−k∗ /β̃k∗

and Gxi,−k∗ denotes the CDF of xi,−k∗ . Note that G̃xi,k∗ |xi,−k∗ is continuous in β for given others (xi,−k∗
and β̃). Then note that because the function (integrand) inside the inner integral in (17) is measurable
in xi,−k∗ , continuous in β, and bounded, the inner integral itself has these properties itself. Therefore,
 14
    Let the support of xi,k∗ be support(xi,k∗ ) = [xk∗ , x̄k∗ ] (which can also depend on xi,−k∗ ).      Then we let
Gxi,k∗ |xi,−k∗ (c) = 0 for c < xk∗ and Gxi,k∗ |xi,−k∗ (c) = 1 for c > x̄k∗ .


                                                         23
applying the dominated convergence theorem we conclude that the function h(β) is continuous in β
and bounded. This completes the proof.


C      Proof of Lemma 2
Fox and Gandhi (2010, Theorem 3.3 and Lemma 5.1) prove that the distribution F0 (β) is identiﬁed in
the sense of Assumption 1.4 whenever the following two conditions hold (using the notation of interval
censored regression):
                                                          {                   }
    1. For any ﬁnite set of distinct random coeﬃcients S = β 1 , . . . , β |S| , |S| < ∞, there exists a pair
       (j, x) such that exactly one β ∈ S satisﬁes x′ β ∈ [aj−1 , aj ).

    2. There is a neighborhood X̃ of x such that the ﬁrst condition holds.

By the continuity of the term x′ β in x, it is clear that the second condition holds whenever the ﬁrst
condition holds. Therefore, we focus on verifying the ﬁrst condition. In the ﬁrst condition, S is not
the true set of types in the data generating process, but any arbitrary, ﬁnite set of types.
    Let S be given and focus on the lowest interval, [a0 , a1 ). We start with a point x⋆ where at least
one β ∈ S satisﬁes (x⋆ )′ β ∈ [a0 , a1 ). Linearity implies such a point exists. If there is only one β ∈ S
that satisﬁes (x⋆ )′ β ∈ [a0 , a1 ), we are done. So consider the case where two or more β ∈ S satisfy
(x⋆ )′ β ∈ [a0 , a1 ).
    Divide x = (x1 , x−1 ) into the ﬁrst x1 and all other covariates x−1 . Let

                                                            1 (               )
                                     x̃1 (a1 , x−1 , β) ≡       a1 − x′−1 β−1
                                                            β1

be the value of x1 , for β ∈ S and x−1 , that makes x′ β = a1 and hence makes β take on a value
outside the interval [a0 , a1 ). As any two β ∈ S diﬀer by deﬁnition, x̃1 (a1 , x−1 , β) is a distinct, linear
function of x−1 for every β. By the properties of linear or, more generally, multivariate real analytic
                                                                        (            )     (              )
                                 −1 in a neighborhood of x−1 where x̃1 a1 , x−1 , βs ̸= x̃1 a1 , x−1 , βt
functions, there exists a point x⋆⋆                        ⋆                    ⋆⋆                ⋆⋆
                                                                     (            )
for all βs ̸= βt ∈ S (Krantz and Parks 2002). Set β̄ = arg minβ∈S x̃1 a1 , x⋆⋆
                                                                            −1 , β and

                                                    (            )
                                         1 = min x̃1 a1 , x−1 , β − ϵ
                                        x⋆⋆                ⋆⋆
                                                β∈S


for suﬃciently small ϵ > 0 when β̄1 > 0 and suﬃciently small (in absolute value) ϵ < 0 when β̄1 < 0.
                                                                          (         )
Then only β̄ satisﬁes (x⋆⋆ )′ β ∈ [a0 , a1 ) for β ∈ S at the point x⋆⋆ = x⋆⋆    ⋆⋆
                                                                            1 , x−1 . Thus, the ﬁrst
condition above is satisﬁed.




                                                        24
References
      [1] Ackerberg, D. (2009), “A New Use of Importance Sampling to Reduce Computational
          Burden in Simulation Estimation”, Quantitative Marketing and Economics, 7(4), 343–
          376.

      [2] Aliprantis, C.D. and K.C. Border (2006), Inﬁnite Dimensional Analysis: A Hitchhiker’s
          Guide, Springer.

      [3] Andrews D.K.W. (2002), “Generalized Method of Moments Estimation When a Parameter
          is on a Boundary,” Journal of Business & Economics Statistics 20-4, 530–544.

      [4] Bajari, P., J.T.. Fox, K. Kim and S. Ryan (2010), “The Random Coeﬃcients Logit Model
          Is Identiﬁed”, University of Minnesota working paper.

      [5] Beran, R. and Millar, P.W. (1994), “Minimum Distance Estimation in Random Coeﬃcient
          Regression Models”, The Annals of Statistics, 22(4), 1976–1992.

      [6] Biernacki, C., G, Celeux and G. Govaert (2003), “Choosing starting values for the EM
          algorithm for getting the highest likelihood in multivariate Gaussian mixture models,”
          Computational Statistics & Data Analysis, 41, 561–575.

      [7] Böhning, D. “Convergence of Simar’s Algorithm for Finding the Maximum Likelihood
          Estimate of a Compound Poisson Process”, The Annals of Statistics, 10(3), 1006–1008.
          1982.

      [8] Carrasco, M, J.P. Florens, and E. Renault (2007), “Linear Inverse Problems in Structural
          Econometrics Estimation Based on Spectral Decomposition and Regularization”, Hand-
          book of Econometrics, V6B, Elsevier.

      [9] Chen, X. (2007), “Large Sample Sieve Estimation of Semi-Nonparametric Models,” Hand-
          book of Econometrics V7, Elsevier.

     [10] Chen, X, O. Linton, and I. van Keilegom (2003), “Estimation of Semiparametric Models
          When the Criterion Function is Not Smooth”, Econometrica, 71-5, 1591-1608.

     [11] Chen, X. and D. Pouzo (2009), "Estimation of Nonparametric Conditional Moment Mod-
          els With Possibly Nonsmooth Moments", Yale working paper.

     [12] Dempster, A.P., N.M. Laird and D.B. Rubin (1977), "Maximum likelihood from incom-
          plete data via the EM algorithm", Journal of the Royal Statistical Society, 39, 1, 1-38.

     [13] Fox, J.T. and A. Gandhi (2010), “Nonparametric Identiﬁcation and Estimation of Random
          Coeﬃcients in Nonlinear Economic Models”, University of Michigan working paper.

     [14] Fox, J.T. and A. Gandhi (2011), “Using Selection Decisions to Identify the Joint Distri-
          bution of Outcomes”, University of Michigan working paper.

     [15] Fox J.T., K. Kim, S. Ryan, and P. Bajari (2011), “A Simple Estimator for the Distribution
          of Random Coeﬃcients”, Quantitative Economics, forthcoming.

     [16] Heckman, J. (1990), “Varieties of Selection Bias”, The American Economic Review, 1990,
          80 (2), 313–318.

                                              25
[17] Heckman, J. and Singer, B. “Method for Minimizing the Impact of Distributional As-
     sumptions in Econometric Models for Duration Data”, Econometrica 52(2), 271-320.

[18] Horowitz, J. L. (1999), “Semiparametric Estimation of a Proportional Hazard Model with
     Unobserved Heterogeneity”, Econometrica, 67(5), 1001–1028.

[19] Huber, J. (1981, 2004) Robust Statistics, Wiley.

[20] Ichimura, H. and T.S. Thompson (1998), “Maximum likelihood estimation of a binary
     choice model with random coeﬃcients of unknown distribution,” Journal of Econometrics,
     86(2), 269–295.

[21] Kamakura, W.A. (1991), “Estimating ﬂexible distributions of ideal-points with external
     analysis of preferences”, Psychometrika, 56, 3, 419-431.

[22] Karlis, D. and E. Xekalaki (2003), “Choosing initial values for the EM algorithm for ﬁnite
     mixtures”, Computational Statistics & Data Analysis, 41, 577–590.

[23] Krantz, S.G. and H.R. Parks (2002), A Primer on Real Analytic Functions, second edition,
     Birkhauser.

[24] Laird, N. (1978), “Nonparametric Maximum Likelihood Estimation of a Mixing Distribu-
     tion”, Journal of the American Statistical Association, Vol. 73, No. 364, pp. 805–811.

[25] Li, J.Q. and A.R. Barron (2000), “Mixture density estimation”, Advances in Neural Infor-
     mation Processing Systems, Vol. 12, pp. 279–285.

[26] Lindsay, B.G. (1983) “The Geometry of Mixture Likelihoods: A General Theory”, The
     Annals of Statistics, 11(1), 86–94.

[27] Matzkin, R.L. (2007) “Heterogeneous Choice”, Advances in Economics and Econometrics,
     Theory and Applications, Ninth World Congress of the Econometric Society. Cambridge.

[28] McLachlan, G.J. and D. Peel (2000), Finite Mixture Models. Wiley.

[29] Parthasarathy, K.R. (1967), Probability Measures on Metric Spaces, Academic Press.

[30] Pilla, R.S. and B.G. Lindsay (2001), “Alternative EM methods for nonparametric ﬁnite
     mixture models”, Biometrika, 88, 2, 535–550.

[31] Pollard, D. (1984), Convergence of Statistical Processes, Springer-Verlag, New York.

[32] Quandt, R.E. and Ramsey, J.B. (1978), "Estimating Mixtures of Normal Distributions
     and Switching Regressions," Journal of the American Statistical Association, 73, 364,
     730-738.

[33] Rust, J. (1987), “Optimal Replacement of GMC Bus Engines: An Empirical Model of
     Harold Zurcher”, Econometrica, 55(5): 999–1033.

[34] Seidel, W., K. Mosler and M. Alker (2000), “A Cautionary Note on Likelihood Ratio Tests
     in Mixture Models”, Annals of the Institute of Statistical Mathematics, 52, 3, 418-487,



                                          26
[35] Teicher, H. (1963), “Identiﬁability of Finite Mixtures”, Annals of Mathematical Statistics,
     34, 1265-1269.

[36] Van der Vaart, W. and J.A. Wellner (1996), Weak Convergence and Empirical Processes,
     Springer Series in Statistics, New York.

[37] Verbeek, J.J., N. Vlassis and B. Kröse (2003), “Eﬃcient Greedy Learning of Gaussian
     Mixture Models”, Neural Computation, Vol. 15, pp 469–485.




                                          27
