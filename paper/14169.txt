                                 NBER WORKING PAPER SERIES




                        EFFICIENT PREDICTION OF EXCESS RETURNS

                                               Jon Faust
                                          Jonathan H. Wright

                                         Working Paper 14169
                                 http://www.nber.org/papers/w14169


                       NATIONAL BUREAU OF ECONOMIC RESEARCH
                                1050 Massachusetts Avenue
                                  Cambridge, MA 02138
                                       July 2008




We thank Frank Diebold, Graham Elliott, Art Goldberger, Atsushi Inoue, Shinichi Sakata and Jim
Stock for helpful comments. All remaining errors are our own. The views expressed in this paper are
solely the responsibility of the authors and should not be interpreted as reflecting the views of the Board
of Governors of the Federal Reserve System, any other employee of the Federal Reserve System, or
the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2008 by Jon Faust and Jonathan H. Wright. All rights reserved. Short sections of text, not to exceed
two paragraphs, may be quoted without explicit permission provided that full credit, including © notice,
is given to the source.
Efficient Prediction of Excess Returns
Jon Faust and Jonathan H. Wright
NBER Working Paper No. 14169
July 2008
JEL No. C22,C53,G14

                                               ABSTRACT

It is well known that augmenting a standard linear regression model with variables that are correlated
with the error term but uncorrelated with the original regressors will increase asymptotic efficiency
of the original coefficients. We argue that in the context of predicting excess returns, valid augmenting
variables exist and are likely to yield substantial gains in estimation efficiency and, hence, predictive
accuracy. The proposed augmenting variables are ex post measures of an unforecastable component
of excess returns: ex post errors from macroeconomic survey forecasts and the surprise components
of asset price movements around macroeconomic news announcements. These "surprises" cannot
be used directly in forecasting--they are not observed at the time that the forecast is made--but can
nonetheless improve forecasting accuracy by reducing parameter estimation uncertainty. We derive
formal results about the benefits and limits of this approach and apply it to standard examples of forecasting
excess bond and equity returns. We find substantial improvements in out-of-sample forecast accuracy
for standard excess bond return regressions; gains for forecasting excess stock returns are much smaller.


Jon Faust
Johns Hopkins University
Department of Economics
Mergenthaler Hall 456
3400 N. Charles Street
Baltimore, MD 25218
and NBER
faustj@jhu.edu

Jonathan H. Wright
Division of International Finance
The Federal Reserve Board
20th Street & Constitution Avenue, NW
Washington, DC 20551
jonathan.h.wright@frb.gov
1. Introduction

      Many empirical papers in …nance explore the predictability of excess returns

using a simple regression-based approach: estimate a regression for future excess

returns based on current predictors and then measure the degree of predictive power of

the estimated model. We derive a method to increase e¢ ciency in the estimation step

and show that this method can lead to substantial gains in measured forecastability.

      The key idea is familiar from …rst-year econometrics. If we take any regression

and augment it with regressors that are correlated with the error term, but are known

to be uncorrelated with the original regressors in population, we increase asymptotic

e¢ ciency of the estimates of the original coe¢ cients without compromising consis-

tency. The augmenting variables are not of direct interest, but soak up some residual

variance, increasing precision of the estimates of the coe¢ cients that are of interest.

This idea is an example of the familiar principle that system estimation imposing

correct cross-equation restrictions is more e¢ cient than single equation estimation,1

but we are not aware systematic treatment of the sort we are proposing in forecasting

context.

      We argue that forecasting excess returns provides an excellent opportunity for

gains from this approach. The standard predictive regression is of the form,

                                             0
                                      yt =       xt   h   + "t

where yt is the excess return from t h to t and the xt           h   is observed at t h. In classic

  1
    As other examples, Hansen (1995) shows how to construct more powerful unit root tests by
augmenting the autoregression with additional covariates that are known to be stationary, and
Eichenbaum, Hansen and Singleton (1988) show how the power of tests of moment restrictions in a
GMM context can be enhanced by imposing other moment restrictions.

                                                 1
applications, the excess return is for government bonds or equity portfolios, and the

predictors are term spreads or dividend yields at t    h (e.g., Fama and Bliss (1987),

Fama and French (1988), Campbell and Shiller (1988, 1991), Cochrane and Piazzesi

(2005)). The variables in such regressions in practice have several distinct properties:

excess returns are highly variable and have a large unpredictable component. The

predictors are smooth relative to the excess returns and much less variable. In short,

we expect large residual variance relative to the variance of the regressors. In …nite

samples, this is equivalent to saying that the coe¢ cient standard errors will be large.

     Our goal is to soak up some of the residual variance to improve the estimates of

 . Formally, a valid augmenting variable, wt must satisfy what we call an identifying

assumption of being uncorrelated with the regressors, xt     h.   If wt is also correlated

with "t , then augmenting the estimated regression with wt will increase the asymptotic

e¢ ciency of the estimate of .

     We consider two sets of augmenting variables: The ex-post errors in published

macroeconomic forecasts, and the surprise components of macroeconomic news an-

nouncements that are released between t        h and t. Whether or not these measures

satisfy our identifying assumptions is an important question. We derive results about

the implications of small violations of the assumption of unpredictability and present

tests of this assumption. In the end, however, we argue that out of sample testing

presents a stringent test of the empirical bene…ts of the approach.

     Applying our approach to the prediction of excess returns, we show that the

e¢ ciency gain in pseudo out-of-sample prediction exercises can be quite substantial

for excess bond returns— a 5 to 30 percent reduction in root mean square prediction


                                           2
error. Gains for forecasting excess stock returns are only found in some cases, and

even then are very modest. In no case do we …nd that the method, if sensibly applied,

substantially degrades predictive performance.

     In section 2, we describe the econometric methodology in a stylized model and

section 3 lays out the speci…c inference procedures that we apply. Sections 4 and 5,

then, report the application to excess bond and equity returns, respectively. Section

6 concludes.


2. Methodology

2.1 The baseline case

This section illustrates the formal logic of our idea in a simpli…ed framework. Begin

with a system of equations for the scalar variable to be forecasted, y, the (p           1)

vector of predictors, x, and a (k     1) vector of augmenting variables, w:

                                            0
                                     yt =       xt   1   + "t ;                         (1)


                                     xt = Axt        1   + vt                           (2)

                                     wt = xt         1   + ut                           (3)

t = 1; : : : ; T . Assume that the process for x is stationary and that x0 is known and

…xed. The shock vector,     t   = ("t ; vt0 ; u0t )0 , is iid with 2+ …nite moments for some

  > 0, and E( t jxt 1 ) = 0. While the s are never directly observed, the other

variables with t subscripts are in the forecaster’s information set at time t and after,

but not before.

     In our baseline case, wt is not correlated with xt 1 , which along with what


                                                3
has come before, requires              = 0. Note that "t and vt can be correlated, but "t is

independent of xs , s < t. Thus, x is predetermined but not strictly exogenous in (1).

     We consider two estimators of the key forecasting parameter, , in (1): the usual

OLS estimator, call it ^ , and the seemingly unrelated regressions (SUR) estimator

based on (1)-(3) imposing              = 0: Call this estimator ~ . Of course, without the

restriction that     = 0, the regressors in each equation of this system are identical,

and the SUR and OLS estimates of                        coincide.

     When we impose            = 0, the SUR estimator of                                  coincides with a di¤erent OLS

estimator: the estimator of            in a version of (1) augmented with wt :

                                                    0                      0
                                         yt =           xt     1   +           wt +   t                                             (4)

We codify this result in,

Theorem 1. ~ , the pseudo-Gaussian maximum likelihood estimator of                                                    in equations

(1)-(3) imposing the restriction that                    = 0 is the same as the OLS estimator of                                      in

equation (4).

The proofs of the Theorems are collected in Appendix 1. Theorem 2 compares the

asymptotic distributions of ^ and ~ . De…ne                                    to be the variance
                                                                                              0 covariance matrix
                                                                                                              1
                                                                                                       0   0
                                                                                                            ""        v"       w"
of   and partition this matrix conformably with ("t ; vt0 ; u0t )0 as @                                     v"        vv
                                                                                                                               0
                                                                                                                               uv
                                                                                                                                      A.
                                                                                                            u"        uv       uu
                              1                                        0
Note that in (4),        =   uu   u"   and      t   = "t                   wt .

Theorem 2. T 1=2 ( ^         ) !d N (0;         2        1
                                                        xx )   and T 1=2 ( ~                   ) !d N (0;    2
                                                                                                                 (1        )    1
                                                                                                                               xx )   as

T ! 1, where        xx   = E(xt x0t ),    2
                                              =         ""   and               =      2   0
                                                                                          u"
                                                                                                 1
                                                                                                uu   u" :


Theorem 2 implies that both estimators are consistent, regardless of the correlation

between the elements of wt and "t — that is, even if wt is worthless, augmenting the

                                                               4
regression does not a¤ect consistency.                  If wt and "t are correlated, however, then

  6= 0, and ~ is asymptotically more e¢ cient than ^ . The stronger is the association,

the greater is the e¢ ciency gain. Note that                  is the population R2 in a regression of

"t on wt .

          Now consider forecast accuracy. The researcher observes xT and wishes to

forecast yT +1 .         A conventional forecast is

                                                             0
                                                 y^T +1jT = ^ xT ;

                                                                                                    0
with mean square prediction error E((yT +1                    y^T +1jT )2 jxT ) =   2
                                                                                        +T   1 2
                                                                                                   xT    1
                                                                                                        xx xT      +
          1
o(T           ) as T ! 1. The alternative predictor proposed in this paper is

                                                             0
                                                 y~T +1jT = ~ xT ;

which has mean square prediction error E((yT +1                       y~T +1jT )2 jxT ) =    2
                                                                                                 +T     1 2
                                                                                                              (1
      0        1              1
 )xT          xx xT   + o(T       ). Once again, the alternative MSE is smaller if           6= 0.

          Note that the augmenting regressors are not used directly in forming the alter-

native forecast; they are used only at the estimation stage to improve the precision

of the estimate of the projection coe¢ cient of yt on xt 1 . The augmented regression

soaks up a component of the error term that is uncorrelated with xt 1 , thereby re-

ducing the error variance and giving more precise parameter estimates and, hence,

better forecasts.

          This idea is related to the recent work of Campbell and Yogo (2006), who

consider a system consisting of equations (1) and (2) alone, with a scalar xt . They

note that if A is known, then vt (the innovations to xt ) are observed and that one

can then obtain a more powerful test of the hypothesis                         =    0   by subtracting the

                                                        5
component of "t that is correlated with vt . More precisely, if              0   = 0 and the errors

are Gaussian with known variance-covariance matrix, they show that the optimal

test reduces to the conventional t-statistic in a regression of yt                   v"
                                                                                     vv
                                                                                          vt on xt 1 .2

When the error variance-covariance matrix is not known, this suggests augmenting

the regression of yt on xt    1   with the additional regressor vt . Of course the parameter

A is generally not known, but Campbell and Yogo model it as being local-to-unity

allowing superconsistent estimation. They show how to use Bonferroni methods in

conjunction with a superconsistent estimate of A to improve inference on .

       Our setup is di¤erent. The A parameter need only be T 1=2 -consistently es-

timable. In this case, the system estimation of (1) and (2) as in Campbell and Yogo,

would reduce to OLS. We posit an additional measured variable in (3), which allows

us to form a more e¢ cient system estimator.

       There are two clear ways one could go awry in applying this approach: using a

large number of augmenting variables that are of limited value (small ), and using

augmenting variables that are, in fact, predictable. In the next two subsections, we

provide some results about these two problems.


2.2 Many augmenting variables: the Large k Case

Asymptotically, adding augmenting variables cannot cause an e¢ ciency loss, even

if they are uncorrelated with "t . However, in a …nite sample, choosing a number of

augmenting variables, k, that is large relative to T will reduce e¢ ciency. To illustrate

   2
     This test is optimal in the sense that it is the uniformly most powerful (UMP) test in the system
given by equations (1) and (2), conditional on the ancillary statistic Tt=1 x2t 1 . Campbell and Yogo
(2006) derive the conditional UMP test for the general hypothesis = 0 . It only reduces to this
t-statistic when 0 = 0.


                                                  6
this intuition formally, we consider an alternative asymptotic nesting of our baseline

model in which the number of extra regressors goes to in…nity at the same rate as

the sample size so that k=T approaches a …xed, positive limit.

      As usual with such asymptotic nestings, it is not that we are taking a position

about what we would do if someone gave us arbitrarily large samples. Rather, we

hope this alternative asymptotic theory will give a better guide as to the …nite-sample

distributions of ^ and ~ in moderate sample sizes when k is fairly large relative to T .

      We …nd that there still may be e¢ ciency gains, but there may be losses, and

the trade-o¤ turns on how strongly the extra regressors are correlated with "t .

Theorem 3. Take the assumptions of the baseline model with the alterations that

k = T and     t   is Gaussian such that         v"   = 0 and       uv   = 0. Then


                              T 1=2 ( ^        ) !d N (0;      2     1
                                                                    xx )



and


                            T 1=2 ( ~        ) !d N (0;    21
                                                            1
                                                                         1
                                                                        xx )



as T ! 1:

Under this asymptotic formulation, the e¢ ciency of ~ relative to ^ falls as k=T rises.

As before, e¢ ciency rises with .


2.2 Small Violations of the Identifying Assumption

The assumption that wt and xt           1   are uncorrelated ( = 0) is central: the gains in

e¢ ciency come exclusively from imposing what we call our identifying assumption. If


                                                     7
this assumption is not satis…ed, then ~ will not be consistent. Of course, it will often

be di¢ cult in practice to rule out small violations of the assumption. To illustrate

the implications of such violations we consider a variation on the baseline model in
                                                                                                   1=2
which       is not exactly zero, but instead is local-to-zero:                                =T         G. The limiting

distributions of ^ and ~ are provided in,

Theorem 4. Take the assumptions of the baseline model with the alteration that
            1=2
  = GT            .

T 1=2 ( ^    ) !d N (0;      2    1
                                 xx )

T 1=2 ( ~    ) !d N ( G 0 ;         2
                                        (1   )     1
                                                  xx )
                                                       0
E((yT +1      y^T +1jT )2 jxT ) =   2
                                        +T   1 2
                                                      xT    1
                                                           xx xT    + o(T         1
                                                                                      )

and
                                                  0            0
E((yT +1      y~T +1jT )2 jxT ) =   2
                                        +T   1
                                                 xT fG0            G+    2
                                                                             (1           )    1
                                                                                              xx gxT   + o(T   1
                                                                                                                   )

as T ! 1.

When G 6= 0, ~ is biased and inconsistent, which will tend to degrade forecasting

performance. It still may be true, however, that the variance of ~ is smaller than

that of ^ . Thus, we have the sort of bias-variance trade-o¤ that often arises in

forecasting. Theorem 4 suggests that the balance can go either way.

       Both theorems 3 and 4 set out trade-o¤s, and one might seek to optimize these

trade-o¤s to choose a number of augmenting variables, k, taking account of modest

violations of the identifying assumption, G 6= 0. Following up this idea would require

a much richer framework, and we do not pursue these lines. In the end, we believe

that the case for using the augmented regression would be hard to make if we do not

have strong reason to believe that wt and xt                       1   are very nearly uncorrelated and so

                                                           8
we henceforth revert to our baseline model in which it is assumed that       = 0:


2.3 A Monte Carlo Simulation

We illustrate the potential gains or losses from using augmenting variables using a

Monte Carlo simulation. The design of the experiment is given by the baseline model

with p = 1 where vt and "t are iid standard normal random variables with correlation

  =     v"   and the sample size is T . The value of A is set at 0.9 to capture the

high degree of persistence that is common in the explanatory variables in the returns

prediction exercises we take up below. Without loss of generality, the true value of

  is normalized to zero. The k augmenting variables in wt are iid N(0; Ik ). Table 1

shows the simulated mean square error of ~ relative to that of ^ for various values of

k, ,    and T . Entries less than one mean that the augmented estimator, ~ , is doing

better than the OLS estimator in a mean-square error sense.

       As expected, the higher is   (the population R2 in a regression of "t on wt ), the

better the augmented estimator fares, consistent with Theorem 2. Meanwhile, other

things being equal, the augmented estimator works less well as k increases relative

to T , consistent with Theorem 3. The augmented estimator can even give a larger

mean square prediction error than the ordinary estimator if k is very large relative to

T , or if    very small.

       Perhaps the most notable …nding here, however, is that with          as small as

10 percent, the augmented estimator gives a notable improvement in mean square

prediction error in all the cases considered except for those in which k=T > 0:1.


2.5 The General Model


                                           9
The baseline model is quite stylized. The general regression that we consider in this

paper is the h-period-ahead forecasting regression

                                                     0
                                        yt =             xt      h   + "t                                (5)

where the p regressors are stationary and predetermined (perhaps including an inter-

cept) and we allow for heteroskedasticity and serial correlation in the errors, with the

latter being the norm in the case of overlapping forecasts, h > 1. Let ^ denote the

OLS estimator of        in (5). Let ~ denote the OLS estimator of                       in the augmented

regression

                                            0                        0
                                     yt =       xt   h       +           wt + "t                         (6)

where wt is a k      1 vector of additional regressors such that


                                        wt = xt                  h   + ut                                (7)

where      = 0 and E( t jxt    h)   = 0 where            t    = ("t ; vt0 ; u0t )0 which of course means that

E(wt jxt   h)   = 0. Given that      = 0, both ^ and ~ will be consistent. Motivated by

the results in the baseline model, we expect that ~ should be more e¢ cient so long as

the extra regressors are relevant ( 6= 0), though ~ is no longer the system maximum

likelihood estimator. We deliberately do not pursue these gains: any further gains

above the ones we focus on require stronger assumptions, and we seek to focus on

the gains ‡owing only from existence of an unpredictable, but ex post measurable

component.3

   3
    For example, in our baseline model, we assumed that xt was a VAR(1). If instead it were a
vector autoregression of order greater than 1, then the regressors in (1) and (2) would no longer be


                                                     10
3. Our speci…c inference procedures

There are three natural hypotheses of interest: 1) Is the identifying assumption that

the ws are unpredictable satis…ed (H1 :              = 0)? 2) Are the augmenting variables

irrelevant (H2 :     = 0)? 3) Is the relative root mean square prediction error of the

augmented approach smaller or greater than one (H3 : RRM SP E = 1 against the

one-sided alternative RRM SP E < 1)? As we de…ne it, RRMSPEs less than one

indicate that the augmented approach has smaller errors. In the best case for the ap-

proach, we fail to reject the …rst hypothesis, and reject the latter two. Unfortunately,

all of these inferences are likely to be complicated by well-known problems arising

from persistent variables.

      It is straightforward to test the identifying assumption that            = 0 and that the

augmenting variables are worthless ( = 0): simply run the relevant regression-based

Wald test that the parameters of interest are zero. In the present case, however,

conventional asymptotics for evaluating marginal signi…cance are likely to provide

a poor approximation. In our applications— and in most predictive regressions in

…nance— the predictors, the xs, are highly persistent. Meanwhile, the overlapping

nature of the long-horizon returns we will be predicting implies that the appropriate

augmenting variables will also be overlapping. Hence, they will be persistent as well.

Regressing persistent variables on each other poses severe challenges to small-sample

inference: the relationship between fwt g and fxt         hg   is akin to a spurious regression

identical (because of di¤erent lag lengths), the SUR estimator of (1) and (2) would not be the same
as equation-by-equation OLS, and Theorem 1 would not go through. In this case, taking account
of the di¤erent lag lengths would admit some further e¢ ciency gain. In general, while the system
maximum likelihood estimator should be more e¢ cient than ^ or ~ , it requires stronger assumptions
to be made.


                                                11
(see, for example, Hodrick (1992), Goetzmann and Jorion (1993) Elliott and Stock

(1994) and Stambaugh (1999)). Thus, we expect that the conventional asymptotic p-

values would be misleading and that an appropriate bootstrap would be more reliable

for testing the hypothesis that          = 0 and, similarly, for testing the hypothesis that

  = 0.

         We base our bootstrap on a resampling scheme in which                     = 0 and        = 0

by construction, but the persistence properties of the variables are preserved. In

particular, each re-sample holds the excess returns fyt g and predictors fxt                hg   …xed,

but we re-sample from the ws, making them uncorrelated with both excess returns

and the predictors. The details of how we re-sample the ws depend on the character

of the ws and are described below.

         We use this same resampling scheme to assess the statistical signi…cance of all

three inferences. For the …rst two hypotheses we compare the relevant Wald statistic

to the empirical distribution of the test statistic under the resampling scheme. In

particular, we compute the Wald statistics using Newey-West heteroskedasticity and

autocorrelation consistent variance-covariance matrices.4

         For assessing the statistical signi…cance of any deviation of RRMPSE from 1,

we compare the distribution of the relevant Diebold-Mariano statistic to the empirical

distribution, in a one-sided test.5

   4
       The truncation lag is h.
   5
     The hypothesis that we are e¤ectively testing in this inference procedure is that the augmenting
variables are strictly exogenous with respect to fyt g and fxt g. That is su¢ cient, but not necessary,
for the augmenting variables to be irrelevant for forecasting. If there is feedback from wt to future
values of xt , then wt is not strictly exogenous, but is not necessarily of any help in prediction.




                                                  12
     We assess whether our concerns with conventional asymptotics are warranted

and whether our bootstrap approach overcomes the problems using a Monte Carlo

experiment summarized here and reported in detail in Appendix 2. Using the 5
                                               2
percent critical value from the asymptotic         distribution, we …nd that the actual

sizes of the Wald tests of    = 0 and      = 0 are about 25 percent and 60 percent,

respectively. The well-known problems with inference in persistent data manifest

themselves quite dramatically in this case. The actual size of the nominal 5 percent

bootstrap tests are between 2 and 5 percent. For the test of statistical signi…cance of

the RRMSPE, the empirical size of a nominal 5 percent bootstrap test is close to 5

percent. Thus, we conclude our inference approach is fairly well calibrated.

4. Predicting Excess Bond Returns

As noted in the introduction, regression-based excess return prediction is a natural

application for our method. We expect these regressions to have very modest pre-

dictive power (if any) for excess returns, which are very volatile. These facts suggest

that the relevant   may be di¢ cult to estimate precisely. Further, the facts suggest

that there will be a great deal of variance in the forecast error that could potentially

be soaked up if we can …nd augmenting variables. These intuitions are examined in

two widely studied areas: excess bond and equity return prediction.

     There are many regressions predicting excess bond returns, but we take as

our baseline the recent and in‡uential work of Cochrane and Piazzesi (2005). Their

predictions are based on a regression of excess bond returns on the term structure of

forward rates.

     To describe the regressions, de…ne Pn;t to be the price of an n-month zero-

                                          13
                                                                                                             1
coupon bond in month t; the yield on this bond is zn;t =                                                     n
                                                                                                                 log(Pn;t ), and the

12-month forward rate ending n months hence is fn;t = log(Pn                                              12;t )    log(Pn;t ). The

return from buying an n-month bond in month t 12 and selling it as an n 12-month

bond in month t is log(Pn                          12;t )     log(Pn;t      12 )   and the excess return from holding an

n-month bond for 12 months over holding a 12-month bond for that same holding

period is

                                    rxnt    12;t   = log(Pn        12;t )      log(Pn;t     12 )   z1;t   12


Cochrane and Piazzesi consider the regression of excess returns on forward rates at

the …rst …ve annual horizons:

                   rxnt      12;t       =          0;n   +   1;n z12;t 12   +      2;n f24;t 12


                                             +      3;n f36;t 12   +    4;n f48;t 12    +     5;n f60;t 12   + "t   12;t            (8)

for n = 24; 36; 48; 60. They also estimate a restricted version of this model in which

the coe¢ cients on the forward rates are the same, up to a scaling factor, for each

maturity n, i.e.                  j;n   =     n j.          To estimate this restricted model, they …rst run the

regression


 rxt   12;t   =      0+           1 z12;t 12 +           2 f24;t 12 +   3 f36;t 12 +      4 f48;t 12 +       5 f60;t 12 + "t 12;t   (9)

                              1     5     12j
where rxt         12;t   =    4     j=2 rxt 12;t ;        and then regress the excess returns on the …tted values

from (9).

       We focus on improving the precision of the estimates in equations (8) and (9).

Our baseline regressions consist of (8) and (9) estimated using the CRSP Fama-Bliss

dataset of monthly zero-coupon bond prices.

                                                                        14
       We consider several di¤erent sets of augmenting variables, each constructed as

a sort of ex post forecast error from some forecast related to the macroeconomy.

       A1. Ex post errors from the Survey of Professional Forecasters. Each quarter,

in the middle of February, May, August and November, the Survey of Professional

Forecasters (SPF) reports analysts’predictions for several variables over the next four

quarters. For each SPF back to the beginning of the survey in 1968Q4 we take the

median forecast of nominal GDP growth, GDP de‡ator in‡ation and the unemploy-

ment rate four quarters hence and then take the di¤erences between these forecasts

and the actual realized values to form the augmenting variables fwt g.6 Because the

survey data are available only at the quarterly frequency, the regressions (8) and (9)

are run only using data from each January, April, July and October (i.e. just before

the survey) for a total of four observations per year. For example, we use the ex-post

forecast error from the forecast made in February as the augmenting variable for pre-

dicting 12-month excess bond returns starting in January. The timing ensures that

only what forecasters learned after time t            12 goes into the augmenting variables7 .

       A2. Expanded SPF errors. Starting with the 1981Q3 survey, the SPF ex-

panded the set of variables being predicted to include the CPI and some interest

rates. Accordingly, for each SPF back to 1981Q3, we take the median predictions

of the variables considered in A1 plus CPI in‡ation, short-term Treasury bill yields

   6
    For GDP growth and GDP de‡ator in‡ation, this is the annualized growth rate from the quarter
before the survey to four quarters later. As the actual realized values of the series, we use the …rst
released values from the Federal Reserve Bank of Philadelphia’s realtime dataset.
   7
    The survey deadline date is a few days before the SPF publication date, but is always in the
second month of each quarter.




                                                 15
and long-term Treasury bond yields and again construct realized forecast errors as

described above. We then take the …rst 3 principal components of these 6 realized

forecast errors to form the augmenting variables fwt g. As with A1, only four obser-

vations are used per year.

       A3. A News Index of Macro Announcement Surprises. We take the following

monthly macroeconomic news announcements: CPI, durable goods orders, housing

starts, industrial production, index of leading indicators, nonfarm payrolls, PPI, retail

sales and unemployment. For each month and each of these announcement types,

we construct the di¤erence between the actual released value and the expected value

as found in the MMS survey taken the previous Friday. We form news index as a

weighted average of these surprises, giving each type of release a weight equal to

the slope coe¢ cient in a regression of the intraday changes in the fourth Eurodollar

futures contract8 from 5 minutes before the announcement until 15 minutes afterwards

on the surprise component of the news announcement.9 This is designed to weight

each type of announcement by its market impact. We then cumulate the resulting

index over all months from t          h + 1 to t, inclusive, to form the augmenting variable

wt where h denotes the horizon of the regression. Our data for these announcements

   8
   This is the fourth contract in the quarterly cycle and settles to the three-month interest rate
about one year hence.
   9
    Some announcements come out concurrently. In A3, A4 and A5, the slope coe¢ cients were
obtained from a single regression of the intraday change in the fourth Eurodollar futures rate on
the surprise components of all of the following announcements: capacity utilization, consumer con-
…dence, CPI (total and core), durable goods orders, the employment cost index, factory orders, the
advance release of GDP, hourly earnings, housing starts, initial jobless claims, industrial production,
the index of leading indicators, the Michigan survey, NAPM, nonfarm payrolls, new homes sales,
personal consumption expenditures, PMI, PPI (total and core), retail sales (total and ex autos) and
unemployment. Each surprise was set to zero whenever that particular announcement type did not
come out or was missing from our dataset. The regression was run over the period 1982 to 2006.


                                                  16
and, hence, the news index spans 1985:02 to 2006:12.

          A4. Expanded News Index. As in A3, except adding the following announce-

ments as well: capacity utilization, core CPI, factory orders, the advance release of

GDP10 , new home sales, personal consumption expenditures, core PPI, retail sales

excluding autos. Our data for this larger set of announcements go back to 1989:09.

The extra variables are not available earlier.

          A5. Further Expanded News Index. This is as in A3, except adding the following

announcements as well: consumer con…dence, initial jobless claims11 , the NAPM

index. Our data for this largest set of announcements go back only to 1991:07.

          A6. Alternative Aggregation of News Index. We take the …rst three principal

components of the monthly surprises that go into the construction of the index in

A5. We then cumulate these principal components over all months from t                      h + 1 to

t, inclusive, to form the augmenting variables wt .

The cumulative economic news indexes in A3, A4 and A5 are plotted in Figure 1. If

the SPF and MMS forecasts are e¢ cient forecasts of upcoming releases and quarterly

macroeconomic data respectively, then all the variables A1–A6 must be orthogonal to

xt   h.   Evidence on the e¢ ciency of SPF survey forecasts is mixed. Froot (1989) and

Romer and Romer (2000) report evidence against the e¢ ciency of survey forecasts,

but Thomas (1999), Mehra (2002) and Ang, Bekaert and Wei (2007) report more

favorable evidence. Much of the discrepancy appears to relate to the sample period

  10
     This is the one quarterly release that we consider. The monthly advance GDP surprise series
is set to zero in all months for which there was no advance GDP announcement.
  11
     This is the one weekly release that we consider. All surprises within a given month are cumulated
to form the monthly claims surprise series.


                                                 17
considered. In the 1970s and early 1980s the surveys appear to have had poor success

in forecasting some variables, notably in‡ation, but have been more successful subse-

quently. Evidence on the e¢ ciency of MMS survey forecasts (forecasts for a speci…c

news release taken the previous Friday) is more uniformly favorable (see, for example,

Balduzzi, Elton and Green (2001)).

     In any case, forecast e¢ ciency is a su¢ cient but not necessary condition for

our identifying assumption. If the expectations are e¢ cient but for an expectational

error that is orthogonal to xt    h,   then wt and xt   h   will still be uncorrelated and the

augmented regression will still give a consistent estimate of the population projection

coe¢ cient of yt onto xt   h.   Our identifying assumption will only fail if the expecta-

tional error is correlated with xt      h,   which seems unlikely to us, but which we test

below.

     With the sets of augmenting variables de…ned, we can now complete our de-

scription of the method for re-sampling from the augmenting variables for the boot-

strap. Remember that we hold the excess returns and predictor variables …xed in the

bootstrap samples, but re-sample from the augmenting variables so as to generate

augmenting variables that are of no value in increasing e¢ ciency ( = 0) and are not

correlated with the predictors ( = 0). Owing to di¤erences in the SPF versus news

index variables, the re-sampling methods are slightly di¤erent. The SPF data are

for overlapping forecast periods and to preserve this structure insofar as possible, we

draw blocks of four-quarter-ahead SPF forecast errors with a block length of 3 years.

For the news index, we have underlying monthly surprises, which are not overlapping

and are arguably uncorrelated. Thus, we re-sample randomly from the monthly sur-


                                                18
prise indexes, and then, as with the actual data, we aggregate these monthly surprise

data between t    h and t to form augmenting variables in each bootstrap sample.

4.1 Results

The results from estimating equation (8) using the augmenting regressors A1-A6, as

well as the results of the baseline regression over the same sample periods for n =

24; 36; 48 and 60 are shown in Table 2. The corresponding results for the estimation

of equation (9) are also shown in Table 2.     The Table reports estimates of      and

asymptotic standard errors from the regressions, both baseline and augmented.

     Consistent with the theory, the standard errors on the elements of         in the

augmented regressions are typically— though not always— lower than in the baseline

regressions. Often they are often substantially lower. This is a preliminary indication

that the inclusion of additional regressors may be improving e¢ ciency.

     Cochrane and Piazzesi emphasized a “tent” shape in the coe¢ cients on the

forward rates whereby the shortest and longest term forward rates have negative

coe¢ cients while the intermediate term forward rates have positive coe¢ cients. In

some cases, the inclusion of the additional regressors indicates a more pronounced

tent shape than is found in the OLS regressions.

     Table 2 also gives the Wald test statistics testing the hypothesis that the coef-

…cients on the augmenting regressors are jointly equal to zero ( = 0 in (6)) which

would imply that the augmenting variables are not correlated with the forecast error.

For augmenting variables A1–A5, this hypothesis is rejected at conventional signi…-

cance levels, typically at levels between 0:001 and 0:05. For variables A6 (panel 2-6),

the hypothesis is not rejected at the 10 percent level.

                                          19
     Finally Table 2 gives p-values for testing our identifying restriction   = 0. This

assumption is never rejected at conventional signi…cance levels using the bootstrap

p-values. It is true, however, that for variables A1, the result is borderline with a

p-value of about 0.12.

4.2 Pseudo-Out-of-Sample Forecasting

Perhaps the most stringent test of the practical usefulness of the proposed approach

to prediction can be obtained in a standard pseudo-out-of-sample recursive predic-

tion exercise. Starting half-way through each of the respective estimation periods, we

estimate equation (8) using the data that would have been available in that month,

and then construct predictions of excess bond returns over the subsequent year us-

ing benchmark and augmented regressions; we repeat this estimation and prediction

exercise in each subsequent month through the end of the sample. We compute

the out-of-sample root mean square prediction error for each of the augmented re-

gressions, relative to the out-of-sample root mean square prediction error from the

corresponding baseline regression model. The results are reported in Table 3 for both

the unrestricted, (8), and restricted (9), models. A relative root mean square pre-

diction error (RRMSPE) below one means that the augmented regression is giving

better out-of-sample predictions of excess bond returns than the baseline.

     As can be seen from the Table, the relative root mean square prediction errors

are mostly between 0.7 and 0.95, implying about a 5-30 percent reduction in root mean

square prediction error. The best results obtain with the SPF regressors (additional

regressors A1 and A2) and the news index that combines the largest number of

announcements (additional regressors A5). The weakest results are found when using

                                          20
the three principal components of the surprises as additional regressors (A6).

     The improvement in root mean square prediction error is signi…cant at the 5

percent level for additional regressors A1, A2 and A5 and the p-values are between

5 and 20 percent for A3 and A4. Using the principal components of the surprises as

additional variables (A6), the improvement in root mean square prediction error is

not close to being signi…cant at any conventional level.


5. Predicting Excess Stock Returns

The second predictive regression that we consider is the prediction of excess stock

returns, following authors such as Fama and French (1988), Campbell and Shiller

(1988) and Ang and Bekaert (2007). We use the following notation: the return on

the CRSP value-weighted portfolio from month t 1 to month t is Rt                         1;t   = log( PPt +D
                                                                                                           t 1
                                                                                                               t
                                                                                                                 )

where Pt denotes the price and Dt denotes the dividend in month t. We de…ne three-

month excess stock returns as rxSt         3;t   =     2
                                                       j=0 Rt 3+j;t 3+j+1       z1;t 3 , where z1;t     3   is the

three-month Fama-Bliss risk-free rate. De…ne the log dividend-price ratio in month
                  11
t as dpt = log(   j=0 Dt j =Pt ).   Summing the dividends over the past year deals with

the seasonality in dividends. Finally de…ne the stochastically detrended short-term
                                     1 RF
interest rate as r~tRF = rtRF         (r
                                     4 t
                                                 + rtRF3 + rtRF6 + rtRF9 ) where rtRF is the three-

month Fama-Bliss riskfree rate. We consider predicting excess stock returns with the

following predictors: dpt alone, r~tRF alone, and dpt and r~tRF together in the following

three regressions:


                              rxSt   3;t   =     1;0   +    1;1 dpt 3   + "1t                                (10)




                                                       21
                               rxSt    3;t   =    2;0   +        ~tRF3
                                                             2;1 r       + "2t           (11)

and


                      rxSt   3;t   =    3;0   +   3;1 dpt 3     +        ~tRF3
                                                                     3;2 r       + "3t   (12)

As in section 3, we take these regressions as the baseline prediction equations for

excess stock returns, and our focus is on improving the precision of the estimates in

equations (10), (11) and (12). The horizon here is h = 3. We estimate these baseline

regressions and augmented regressions including the augmenting regressors A1-A6,

where these news surprise measures are now cumulated over 3-month rather than

12-month horizons.

      Table 4 shows the coe¢ cient estimates and Wald tests from the estimation of

(10), (11) and (12) both with and without the additional regressors. Exactly the same

information is shown as for the corresponding excess bond return prediction equations.

With additional regressors A1 and A2 (SPF forecast errors), the standard errors in

the augmented regressions are a bit lower than those in the baseline regressions.

The Wald test testing the hypothesis that the additional variables are relevant is

signi…cant in (10), (11) or (12), using the bootstrap p-values. On the other hand, using

additional regressors A3-A6 (announcement surprise measures), the standard errors in

the augmented regressions are little changed from those in the baseline regressions and

the Wald test for the joint relevance of these extra regressors is not signi…cant. This

is quite di¤erent from what we found in the excess bond return prediction equations,

but is perhaps not surprising since many authors have found that a considerably


                                                        22
greater fraction of bond price movements can be explained by macroeconomic news

announcements than is the case for stock prices (see e.g. Andersen, Bollerslev, Diebold

and Vega (2007)).

     For each of the augmented regressions in Table 4, we recursively compute the

out-of-sample mean square prediction error relative to that from the baseline regres-

sion starting half-way through the sample period. The results are reported in Table 5.

Again, the results are di¤erent depending on whether one uses the SPF forecast errors

or announcement surprise measures as additional variables. With the SPF forecast

errors, the RRMSPE is a bit below 1, showing some improvement in root mean square

prediction error. Using our bootstrap test, the improvement is statistically signi…-

cant, at least at the 10 percent level, in all cases. The improvement is more modest

than was obtained in forecasting excess bond returns with additional regressors A1

and A2. Meanwhile, with additional regressors A3-A6, the RRMSPE is around 1 in

all cases and there are no cases in which the augmented regression gives a signi…cant

improvement in out-of-sample forecasting performance, as one might expect given

that the announcement surprise measures do not have a signi…cant association with

quarterly excess stock returns.

6. Conclusions

Researchers using a regressor xt      h   to forecast excess returns, yt , conventionally regress

the excess returns on xt    h   and use the resulting coe¢ cients for forecasting. However,

if there exists a variable wt that is correlated with the regression error, but not

with xt   h,   then a more e¢ cient approach to estimating the coe¢ cient on xt         h   in the

forecasting regression is to augment the regression with wt . This may in turn enable

                                                 23
better forecasts to be constructed, because the coe¢ cient on xt   h   is more precisely

estimated, even though wt is not observed at the time the forecast is made, and so

cannot be directly used in prediction.

     In this paper, we demonstrate the merits of augmenting the estimation model

for predictive regressions with ex post measures of any unpredictable component of

variable being forecasted. This method is most likely to yield advantages in cases

such as forecasting excess returns where there is a large unforecastable component

and precision of the coe¢ cient estimates is likely to be a major issue. In the excess

returns context, we argue that ex-post SPF survey forecast errors and the surprise

components of macroeconomic news announcements may satisfy the required condi-

tions for augmenting variables.

     We demonstrate the merits of the approach using canonical predictive regres-

sions for excess bond and equity returns. The gains are quite pronounced in our

extension of the Cochrane and Piazzesi (2005) study of excess bond returns. We …nd

little, if any, gains in conventional equity returns regressions. Our goal in the em-

pirical work was to show the bene…ts in well-known cases. We suspect further gains

may be found in other cases and using other augmenting variables. For example,

other possible augmenting variables include data revisions (Koenig et al. (2003)) and

oil supply shocks (as identi…ed, for example, by Kilian (2006)), though, at least for

the oil supply shocks, we think that the case that these are uncorrelated with the

predictors is relatively weak.

     The approach could of course easily be misused, say, by searching over a large

set of potential augmenting variables for those that give the greatest reduction in


                                         24
(in sample) standard errors. While out-of-sample tests can provide some protection

against the sort of false inference this could promote, we believe that, in practice,

our approach should only be entertained if the researcher has a strong belief that the

identifying assumption is satis…ed. In any case, we think our results suggest that the

approach deserves serious consideration.


                                      Appendix 1: Proof of Theorems
                                                   Proof of Theorem 1.
                                                                                                                 0               0       0
                                                                                                                                              1
                                                                                                                      11         21      31
Let     =        1
                     be partitioned conformably with ("t ; vt0 ; u0t )0 as @                                          21         22
                                                                                                                                         0
                                                                                                                                         32
                                                                                                                                              A.
                                                                                                                      31         32      33
The submatrices           11 ,        21 ,       22 ,     31 ,       32   and             33   are 1     1; p      1; p      p; k      1; k    p
and k     k, respectively. The log-likelihood of                                     fyt ; xt ; wt gTt=1   is given by
                                                         0                       0
                                                                                              10               0                 0
                                                                                                                                              1
                                                                     yt              xt 1                             yt             xt 1
  T (p+k 1) ln(2 )      T ln(j j)            1    T
                                                  t=1
                                                         @ xt             (Ip         xt 1 )a A
                                                                                       0                   1   @ xt        (Ip        xt 1 )a A
                                                                                                                                       0
         2                  2                2
                                                                             wt                                               wt

where a = vec(A0 ). The …rst order conditions for the maximization of the log-
likelihood with respect to                       and a are

             T                          0                    0                                 0               0
             t=1 f     11 (yt               xt 1 ) +         21 (xt            (Ip         xt 1 )a) +          31 wt gxt 1      =0

and

         T                       0                                                    0                 0      0            0
         t=1 f   21 (yt              xt 1 ) +           22 (xt       (Ip         xt 1 )a) +             32 wt g (Ip       xt 1 ) = 0

      De…ning          and ~ as the solutions to the equations

                                                                 0         0 ~0                    0
                                                          11               21         =            31


and

                                                          21
                                                                 0
                                                                           22
                                                                                ~0 =               0
                                                                                                   32


                                                                          25
the …rst order conditions can be rewritten as

              T                           0                                 0                 ~ 0 wt                       0
              t=1 f    11 (yt                 wt            xt 1 ) +        21 (xt                              (Ip       xt 1 )a)gxt         1   =0

and

          T                      0                                                       ~ 0 wt                       0
                                                                                                                     xt 1 )a)g0 (Ip
                                                                                                                                              0
          t=1 f   21 (yt             wt            xt 1 ) +            22 (xt                             (Ip                                xt 1 ) = 0

which are satis…ed by setting                                 and a to the OLS estimated coe¢ cients on xt                                                         1   in
regressions of yt and xt on wt and xt 1 , respectively.
                                                                                                                                                           0
                                                                                                                                                  11       21
          We can obtain an expression for                                           as follows. De…ne                          A   =                                    ;
                                                                                                                                                  21       22
                  0                                          0                                             0
                  31                               ""        v"                                            u"                       1                  A           B
  B   =           0     ,    A   =                                     and           B    =                0         so that             =             0
                  32                               v"        vv                                            uv                                          B           33
                                                                                                                                    0
and          =          A
                        0
                                     B
                                              . From the de…nition of                                           and ~ ,            ~0        =             A
                                                                                                                                                               1
                                                                                                                                                                       B.
                        B        uu
                                                                                                                                                            1
From the formula for the inverse of a partitioned matrix                                                                       B   =          A        B   uu ,        so
      0
               1              1
  ~ 0 = B uu and so = uu u" , as required for equation (4). Although we are
not aware of any paper where precisely this result has been proven, the idea is not
new. For example, similar results were proven by Goldberger (1970) and Goldberger
and Olkin (1971).


                                                            Proof of Theorem 2.

This follows from the usual formula for the asymptotic distribution of OLS given that
                   2
V ar("t ) =            and

                                                   0                            0                          0                            0     1
          V ar( t ) = V ar("t                          wt ) =      "e   +            uu               2         u"   =    "e            u"   uu    u"      =
                                                        2         0      1                        2
                                                                  u"    uu          u"   =            (1         )




                                                                             26
                                        Proof of Theorem 3.

The result for ^ follows from the usual formula for the asymptotic distribution of
OLS with a …xed number of regressors. To prove the result for ~ , write the model in
matrix form as y = X +W + where W is a T                              k matrix, the tth row of which is wt0 .
Under the stated assumptions, the regressors in this equation are strictly exogenous
and so we may condition on them. We can then write

                                    ~=            + (X 0 PW X) 1 X 0 PW                                        (A1)

where PW = I      W (W 0 W ) 1 W 0 .
     Let U be any n          m matrix with rows fui gni=1 that are iid Gaussian with mean
zero and variance-covariance matrix , c be any …xed m                               1 vector, and let P be any
nonstochastic n       n projection matrix of rank r: Lemma 2 of Bekker (1994) states
that if r=n !     as n ! 1, then E(n 1 U 0 P U c) !                             c and
                  1
                       (U 0 P U c       E(U 0 P U c)) !d N (0; c0 c +                          cc0 )           (A2)
                n1=2
                                    1=2
as n ! 1. Since V ar(n                    U 0 P U a) = O(1), V ar(n 1 U 0 P U a) ! 0, and so, by
Chebyshev’s Inequality,

                                              n 1 U 0 P U c !p         c                                       (A3)

Now PW is a projection matrix of rank T                        k. Letting U = [X                ], n = T and

                                                       xx         0
                                              =
                                                      0        V ar( t )

From (A3), conditional on W ,

                                          1
                                    T         X 0 PW X !p (1               )   xx


and, from (A2), conditional on W ,

                             1=2
                         T         X 0 PW !d N (0; V ar( t )(1                      )   xx )


and so, from (A1), again conditional on W;

                                                          27
                                               T 1=2 ( ~                ) !d N (0; 1 1 V ar( t )                                    1
                                                                                                                                   xx )


As this limiting distribution does not depend on W , it holds unconditionally as
                                       2
well. As V ar( t ) =                       (1           ), it follows that
                                                                                                             2 (1
                                                     T 1=2 ( ~                  ) !d N (0;                       1
                                                                                                                       )    1
                                                                                                                           xx )


as required.


                                                             Proof of Theorem 4.

The proof for ^ is as in Theorem 2. Turning to ~ , let                                                                         =         1
                                                                                                                                        uu       u"   and             t   = "t         0
                                                                                                                                                                                           ut .
From (1),

        0                          0                   0                             0                   0                                                   0                   0
yt =        xt   1 + "t    =           xt      1+          ut +         t   =            xt       1+         (wt           xt 1 ) +          t   =               xt       1          wt +    t


where            =             0
                                        and E( t jxt 1 ; wt ) = 0. The estimator ~ is the OLS estimate
                                                                                                                                                 1=2
of the coe¢ cient on xt                          1    in this regression. Since                                                = GT                       , T 1=2 (                    ) =
G0 . From the usual formula for the asymptotic distribution of OLS, given that
                                                           0         1                        2
V ar( t jxt 1 ; wt 1 ) =                    "e             u"       uu          u"   =            (1             ), and

            1              0               1                    0           0            0               3=2                   0                      1                   0
    T            xt 1 wt = T                     xt 1 (xt           1           + ut ) = T                            xt 1 xt       1   +T                 xt 1 ut !p 0,

it follows that

                                                T 1=2 ( ~                       ) !d N (0;                   2
                                                                                                                 (1        )        1
                                                                                                                                   xx )


Finally, since T 1=2 ( ~                         ) = T 1=2 ( ~                                )        T 1=2 (                     ) we have T 1=2 ( ~                                ) !d
N ( G0 ;          2
                      (1       )        1
                                       xx ),     as required.




                                                                                             28
          Appendix 2: Monte Carlo Evaluation of Inference Procedures

We rely mainly on bootstrap p-values to assess statistical signi…cance of our results.
To investigate the properties of our bootstrap procedure, and of the alternative based
on conventional asymptotics, we run a small Monte-Carlo experiment. In each simu-
lation, fyt g and fxt   hg   were …xed at their values in the sample periods beginning in
1985:02, 1989:09 and 1991:07, respectively. These correspond to the regressions with
the three alternative economic news indexes and give sample sizes of T=263, T=208
and T=186, respectively. We draw augmenting variables to mimic key features of the
economic news index. In particular, we draw news index values with mean zero and
standard deviation 8; this is roughly the standard deviation of each of these economic
news indexes. These are then cumulated from time t             12 to time t to form the
augmenting variable, making this a Gaussian MA(12) process. The asymptotic and
bootstrap p-values for the Wald tests of           = 0 and   = 0 are then computed. We
report the simulated percentage of samples for which these p-values are less than 5
percent. Obviously, a well-calibrated test will give numbers near 5 percent; numbers
greater than 5 percent indicate that the true size is greater than the nominal size.
         As can be seen from Table A1, the e¤ective size of the asymptotic test of the
hypothesis that      = 0 is elevated, at around 25 percent, while the asymptotic test of
the hypothesis that     = 0 shows massive size distortions, with an actual rejection rate
of over 60 percent. In contrast, the bootstrap test is roughly correctly sized with the
e¤ective size of the test being between 2 and 5 percent. The third block in the table,
labelled RRMSPE=1, gives the percentage of times that the bootstrap p-value for
testing the hypothesis that RRMSPE=1 in the out-of-sample forecasting experiment
is less than 5 percent, using the unrestricted regression (there are no asymptotic p-
values for this test). We see that this test has a size that is very close to the nominal
level.




                                              29
References

Andersen, T., T. Bollerslev, F.X. Diebold and C. Vega (2007): Real-Time Price Dis-
    covery in Stock, Bond and Foreign Exchange Markets, Journal of International
    Economics, 73, pp.251-277.

Ang, A. and G. Bekaert (2007): Stock Return Predictability: Is it There?, Review
    of Financial Studies, 20, pp.651-707.

Ang, A., G. Bekaert and M. Wei (2007): Do Macro Variables, Asset Markets, or Sur-
    veys Forecast In‡ation Better?, Journal of Monetary Economics, 54, pp.1163-
    1212.

Balduzzi, P. Elton, E.J. and T.C. Green (2001): Economic News and Bond Prices:
    Evidence from the U.S. Treasury Market, Journal of Financial and Quantitative
    Analysis, 36, pp.523-543.

Bekker, P.A. (1994): Alternative Approximations to the Distributions of Instrumen-
    tal Variables Estimators, Econometrica, 62, pp.657-681.

Campbell, J.Y. and R. J. Shiller (1988): The Dividend-Price Ratio and Expecta-
    tions of Future Dividends and Discount Factors, Review of Financial Studies,
    1, pp.195-228.

Campbell, J.Y. and R. J. Shiller (1991): Yield Spreads and Interest Rate Movements:
    A Bird’s Eye View, Review of Economic Studies, 58, pp.495-514.

Campbell, J.Y. and M. Yogo (2006): E¢ cient Tests of Stock Return Predictability,
    Journal of Financial Economics, 81, pp.27-60.

Cochrane, J.H. and M. Piazzesi (2005): Bond Risk Premia, American Economic
    Review, 95, pp.138-160.

Eichenbaum, M.S., L.P. Hansen and K.J. Singleton (1988): A Time-Series Analy-
    sis of Representative Agent Models of Consumption and Leisure Choice under
    Uncertainty, Quarterly Journal of Economics, 103, pp.51-78.


                                       30
Elliott, G. and J.H. Stock (1994): Inference in Time Series Regression When the Or-
    der of Integration of a Regressor is Unknown, Econometric Theory, 10, pp.672-
    700.

Fama, E. and R.R. Bliss (1987): The Information in Long-Maturity Forward Rates,
    American Economic Review, 77, pp.680-692.

Fama, E. and K. French (1988): Dividend Yields and Expected Stock Returns, Jour-
    nal of Financial Economics, 22, pp.3-26.

Froot, K.A. (1989): New Hope for the Expectations Hypothesis of the Term Structure
    of Interest Rates, Journal of Finance, 44, pp.283-305.

Goetzmann, W. and P. Jorion (1993): Testing the Predictive Power of Dividend
    Yields, Journal of Finance, 48, pp.663-679.

Goldberger, A. (1970): Estimation of a Regression Coe¢ cient Matrix Containing a
    Block of Zeros, Working Paper, University of Wisconsin.

Goldberger, A. and I. Olkin (1971): A Minimum Distance Interpretation of Limited
    Information Estimation, Econometrica, 39, pp.635-639.

Hansen, B.E. (1995): Rethinking the Univariate Approach to Unit Root Testing:
    Using Covariates to Increase Power, Econometric Theory, 11, pp.1148-1171.

Hodrick, R.J. (1992): Dividend Yields and Expected Stock Returns: Alternative
    Procedures for Inference and Measurement, Review of Financial Studies, 5,
    pp.357-386.

Kilian L. (2006): Exogenous Oil Price Shocks: How Big are They and How Much do
    They Matter for the U.S. Economy, Working Paper, University of Michigan.

Koenig, E., S. Dolmas, and J. Piger (2003): The Use and Abuse of ‘Real-Time’Data
    in Economic Forecasting, Review of Economics and Statistics, August, 85(3),
    pp. 618–28.




                                       31
Mehra, Y.P. (2002): Survey Measures of Expected In‡ation: Revisiting the Issues
    of Predictive Content and Rationality, Federal Reserve Bank of Richmond Eco-
    nomic Quarterly, 88, pp.17-36.

Romer, C.D. and D.H. Romer (2000): Federal Reserve Information and the Behavior
    of Interest Rates, American Economic Review, 90, pp.429-457.

Stambaugh, R. (1999): Predictive Regressions, Journal of Financial Economics, 54,
    pp.375-421.

Thomas, L.B. (1999): Survey Measures of Expected U.S. In‡ation, Journal of Eco-
    nomic Perspectives, 13, pp.125-144.




                                      32
                                 Table 1: Simulated mean square errors of ~ relative to ^
                 2
                Rw           k=1 k=5 k=10 k=20 k=1 k=5 k=10 k=20 k=1 k=5 k=10                              k=20
                                     T=100                    T=500                      T=1000
                0.00   0     1.01 1.06 1.13     1.25 1.00 1.01 1.02      1.04 1.00 1.00 1.01                1.02
                0.01   0     0.99 1.04 1.11     1.23 0.99 0.99 1.00      1.02 0.98 1.00 1.00                1.01
                0.1    0     0.90 0.95 1.00     1.11 0.91 0.89 0.91      0.91 0.88 0.92 0.90                0.92
                0.3    0     0.76 0.80 0.84     0.93 0.76 0.75 0.76      0.75 0.73 0.78 0.75                0.78
                0.5    0     0.66 0.69 0.72     0.80 0.65 0.65 0.66      0.72 0.63 0.68 0.65                0.67
                0.00   0.5   1.00 1.05 1.11     1.21 1.00 1.00 1.01     1.03    1.00 1.01 1.01              1.03
                0.01   0.5   1.00 1.04 1.09     1.19 0.99 0.99 1.00     1.00    0.99 1.00 1.00              1.01
                0.1    0.5   0.92 0.96 0.98     1.08 0.92 0.92 0.92     0.90    0.90 0.93 0.90              0.91
                0.3    0.5   0.76 0.80 0.81     0.88 0.78 0.78 0.78     0.74    0.75 0.79 0.75              0.76
                0.5    0.5   0.65 0.69 0.69     0.74 0.67 0.68 0.68     0.64    0.64 0.68 0.64              0.66
                0.00   0.9   1.00 1.02 1.08     1.16 1.00 1.01 1.01      1.02 1.00 1.01 1.01                1.04
                0.01   0.9   1.00 1.01 1.07     1.15 0.99 1.00 1.00      1.00 0.99 1.00 0.99                1.02
                0.1    0.9   0.91 0.91 0.96     1.02 0.91 0.93 0.92      0.90 0.99 0.92 0.89                0.92
                0.3    0.9   0.72 0.74 0.76     0.81 0.78 0.78 0.78      0.76 0.77 0.76 0.74                0.76
                0.5    0.9   0.59 0.62 0.63     0.67 0.67 0.67 0.68      0.66 0.66 0.65 0.63                0.66

Notes. Relative mean square errors of baseline and augmented regressions obtained by Monte-Carlo simulations, as described in the
text. In each experiment, 1000 replications were conducted.
      Table 2-1: Regressions predicting       excess bond returns using forward rates and additional regressors A1
                            n = 24                  n = 36           n = 48           n = 60          Average
         Regression:  Baseline    Aug.        Baseline Aug. Baseline Aug. Baseline Aug. Baseline Aug.
         y12            -0.75     -1.34         -1.41     -2.44  -2.13     -3.47  -2.73     -4.37  -1.76     -2.91
                       (0.22)    (0.20)        (0.38)    (0.33) (0.50)    (0.44) (0.61)    (0.52) (0.43)    (0.37)
         f24             0.12      1.04         -0.25      1.35   0.05      2.10   0.25      2.75  0.05       1.81
                       (0.45)    (0.36)        (0.77)    (0.64) (1.01)    (0.85) (1.26)    (1.07) (0.86)    (0.72)
         f36             1.04      1.18          2.63      2.85   2.74      3.01   2.99      3.31  2.35       2.59
                       (0.38)    (0.32)        (0.64)    (0.58) (0.86)    (0.80) (1.10)    (1.02) (0.74)    (0.68)
         f48             0.80      0.08          1.48      0.24   2.83      1.25   3.43      1.51  2.14       0.77
                       (0.29)    (0.25)        (0.52)    (0.48) (0.67)    (0.65) (0.84)    (0.80) (0.58)    (0.54)
         f60            -0.94     -0.88         -2.02     -1.89  -2.92     -2.75  -3.23     -3.00  -2.28     -2.13
                       (0.37)    (0.28)        (0.63)    (0.48) (0.84)    (0.66) (1.07)    (0.84) (0.72)    (0.56)

           Wald                       64.33               47.05               43.33               43.37               46.79
           p-val (boot)               0.000               0.000               0.000               0.000               0.000
           p-val (asy)                0.000               0.000               0.000               0.000               0.000

           Wald                      120.88
           p-val (boot)              0.119
           p-val (asy)               0.000

Notes. The baseline regressions show the estimated coe¢ cients in regressions of excess 24-, 36-, 48- and 60-month bond returns on
the term structure of forward rates and of the average of these four excess returns on the term structure of forward rates.        The
regressions are run on data from the …rst month of each quarter from 1968Q4 to 2006Q4. Asymptotic standard errors are shown
in parentheses. All asymptotic standard errors and p-values are Newey-West with a lag length of 4. The augmented regressions
control for the additional variable A1. The estimated coe¢ cients on this additional variable are not reported, but the row Wald
denotes the Wald statistic testing the hypothesis that it is equal to zero along with the associated asymptotic and bootstrap p-values,
constructed as described in the text. The row Wald denotes the Wald statistic testing the hypothesis that = 0, again along with
the associated asymptotic and bootstrap p-values. These are the same for each regression in the table because they depend only on
fxt g and fwt g.
      Table 2-2: Regressions predicting excess bond returns using forward rates and additional regressors A2
                           n = 24            n = 36            n = 48            n = 60            Average
        Regression:  Baseline    Aug.  Baseline    Aug.  Baseline    Aug.  Baseline    Aug.  Baseline     Aug.
        y12            -0.09     -0.56   -0.36     -1.26   -0.83     -2.02    1.07     -2.52   -0.59      -1.59
                      (0.39)    (0.14)  (0.73)    (0.27)  (0.97)    (0.40)  (1.22)    (0.51)  (0.82)     (0.31)
        f24            -0.29      0.67   -0.81      1.04   -0.60      1.87   -0.57      2.48   -0.57       1.52
                      (0.60)    (0.26)  (1.10)    (0.50)  (1.45)    (0.75)  (1.84)    (1.06)  (1.23)     (0.62)
        f36             0.42      0.36    1.56      1.43   1.38       1.16    1.19      0.88    1.14       0.96
                      (0.40)    (0.15)  (0.79)    (0.33)  (1.13)    (0.56)  (1.39)    (0.78)  (0.92)     (0.44)
        f48             0.81     -0.33    1.50     -0.69   2.81      -0.09    3.45     -0.11    2.14      -0.30
                      (0.23)    (0.12)  (0.44)    (0.22)  (0.60)    (0.35)  (0.75)    (0.45)  (0.50)     (0.27)
        f60            -0.49     -0.01   -1.31     -0.38   -2.03     -0.76   -2.08     -0.51   -1.48      -0.41
                      (0.27)    (0.09)  (0.57)    (0.20)  (0.80)    (0.34)  (1.01)    (0.49)  (0.66)     (0.27)

         Wald                     419.60              520.30             383.26             321.19        415.27
         p-val (boot)             0.000               0.000              0.000              0.000         0.000
         p-val (asy)              0.000               0.000              0.000              0.000         0.000

         Wald                     121.16
         p-val (boot)             0.329
         p-val (asy)              0.000

Notes. As in Table 2-1, except using the additional variable A2 and the sample period 1981Q3 to 2006Q4.
      Table 2-3: Regressions predicting      excess bond returns using forward rates and additional regressors A3
                            n = 24                 n = 36           n = 48           n = 60          Average
         Regression:  Baseline Aug.          Baseline Aug. Baseline Aug. Baseline Aug. Baseline Aug.
         y12            -0.28     -0.60        -0.84     -1.45  -1.60     -2.43  -2.43     -3.41  -1.29     -1.97
                       (0.44)    (0.33)       (0.83)    (0.61) (1.09)    (0.80) (1.29)    (0.96) (0.91)    (0.67)
         f24            -0.55     -0.15        -0.99     -0.22  -0.63      0.40  0.19       1.41  -0.50      0.36
                       (0.77)    (0.54)       (1.37)    (0.95) (1.81)    (1.24) (2.17)    (1.50) (1.53)    (1.05)
         f36             1.32      1.46         3.12      3.38   3.56      3.91   3.74      4.15   2.94     3.23
                       (0.93)    (0.62)       (1.01)    (1.11) (2.31)    (1.51) (2.76)    (1.84) (1.92)    (1.26)
         f48             0.71      0.53         0.87      0.66   1.90      1.43   1.74      1.19   1.34     0.95
                       (0.45)    (0.30)       (0.59)    (0.62) (1.22)    (0.92) (1.54)    (1.25) (1.00)    (0.76)
         f60            -0.92     -0.88        -1.80     -1.72  -2.50     -2.39  -2.35     -2.22  -1.89     -1.80
                       (0.44)    (0.33)       (0.82)    (0.63) (1.13)    (0.89) (1.39)    (1.13) (0.94)    (0.74)

           Wald                      31.52               29.27              26.45               20.89               26.00
           p-val (boot)              0.005               0.006              0.006               0.008               0.006
           p-val (asy)               0.000               0.000              0.000               0.000               0.000

           Wald                      13.41
           p-val (boot)              0.567
           p-val (asy)               0.037

Notes. The baseline regressions show the estimated coe¢ cients in regressions of excess 24-, 36-, 48- and 60-month bond returns
on the term structure of forward rates and of the average of these four excess returns on the term structure of forward rates.
The regressions are run on monthly data from 1985:02 through 2006:12. Asymptotic standard errors are shown in parentheses.
All asymptotic standard errors and p-values are Newey-West with a lag length of 18. The augmented regressions control for the
additional variable A3. The estimated coe¢ cients on this additional variable are not reported, but the row Wald denotes the Wald
statistic testing the hypothesis that it is equal to zero along with the associated asymptotic and bootstrap p-values, constructed as
described in the text. The row Wald denotes the Wald statistic testing the hypothesis that = 0, again along with the associated
asymptotic and bootstrap p-values. These are the same for each regression in the table because they depend only on fxt g and fwt g.
      Table 2-4: Regressions predicting      excess bond returns using forward rates and additional regressors A4
                            n = 24                 n = 36           n = 48           n = 60          Average
         Regression:  Baseline    Aug.       Baseline Aug. Baseline Aug. Baseline Aug. Baseline Aug.
         y12            -0.17     -0.39        -0.74     -1.17  -1.57     -2.13  -2.54     -3.19  -1.25     -1.72
                       (0.48)    (0.39)       (0.88)    (0.71) (1.14)    (0.91) (1.34)    (1.06) (0.96)    (0.76)
         f24            -1.18     -0.57        -1.99     -0.83  -1.87     -0.34  -1.03      0.74  -1.52     -0.25
                       (0.82)    (0.62)       (1.43)    (1.07) (1.85)    (1.38) (2.24)    (1.67) (1.58)    (1.18)
         f36             2.46      1.96         5.34      4.39   6.74      5.49   7.63      6.17  5.54       4.50
                       (0.96)    (0.65)       (1.74)    (1.15) (2.26)    (1.49) (2.70)    (1.79) (1.90)    (1.26)
         f48             0.01      0.24        -0.68     –0.25  -0.78     -0.21  -1.90     -1.24  -0.84     -0.37
                       (0.54)    (0.41)       (1.01)    (0.74) (1.32)    (0.98) (1.58)    (1.22) (1.10)    (0.82)
         f60            -0.55     -0.85        -0.95     -1.51  -1.21     -1.95  -0.65     -1.51  -0.84     -1.45
                       (0.46)    (0.44)       (0.96)    (0.90) (1.36)    (1.29) (1.74)    (1.67) (1.12)    (1.07)

          Wald                      15.77               13.37              11.08               8.81         11.23
          p-val (boot)              0.030               0.039              0.053               0.071        0.054
          p-val (asy)               0.000               0.000              0.001               0.003        0.001

          Wald                      26.042
          p-val (boot)              0.541
          p-val (asy)               0.000

Notes. As in Table 2-3, except using the additional variable A4 and the sample period 1989:09 to 2006:12.
      Table 2-5: Regressions predicting      excess bond returns using forward rates and additional regressors A5
                            n = 24                 n = 36           n = 48           n = 60          Average
         Regression:  Baseline    Aug.       Baseline Aug. Baseline Aug. Baseline Aug. Baseline Aug.
         y12            -0.11     -0.20        -0.64     -0.82  -1.44     -1.68  -2.22     -2.51  -1.10     -1.30
                       (0.63)    (0.37)       (1.17)    (0.67) (1.50)    (0.84) (1.77)    (0.98) (1.26)    (0.71)
         f24            -1.38     -0.89        -2.27     -1.30  -2.20     -0.90  -1.57      0.00  -1.86     -0.77
                       (0.92)    (0.60)       (1.62)    (1.02) (2.06)    (1.32) (2.46)    (1.59) (1.76)    (1.12)
         f36             2.87      2.31         5.93      4.84   7.43      5.96   8.34      6.57  6.14       4.92
                       (0.96)    (0.63)       (1.78)    (1.13) (2.36)    (1.49) (2.86)    (1.82) (1.98)    (1.25)
         f48            -0.28      0.00        -1.13     -0.58  -1.33     -0.59  -2.56     -1.67  -1.32     -0.71
                       (0.58)    (0.35)       (1.08)    (0.62) (1.42)    (0.82) (1.70)    (1.04) (1.19)    (0.69)
         f60            -0.53     -0.89        -0.88     -1.59  -1.10     -2.06  -0.31     -1.46  -0.71     -1.50
                       (0.58)    (0.44)       (1.25)    (0.93) (1.76)    (1.34) (2.24)    (1.75) (1.45)    (1.10)

          Wald                      25.39               26.28              23.64               20.33        23.60
          p-val (boot)              0.011               0.010              0.012               0.016        0.011
          p-val (asy)               0.000               0.000              0.000               0.000        0.000

          Wald                      16.756
          p-val (boot)              0.704
          p-val (asy)               0.010

Notes. As in Table 2-3, except using the additional variable A5 and the sample period 1991:07 to 2006:12.
      Table 2-6: Regressions predicting      excess bond returns using forward rates and additional regressors A6
                            n = 24                 n = 36           n = 48           n = 60          Average
         Regression:  Baseline    Aug.       Baseline Aug. Baseline Aug. Baseline Aug. Baseline Aug.
         y12            -0.11     -0.22        -0.64     -0.86  -1.44     -1.75  -2.22     -2.61  -1.10     -1.36
                       (0.63)    (0.37)       (1.17)    (0.70) (1.50)    (0.91) (1.77)    (1.09) (1.26)    (0.76)
         f24            -1.38     -0.88        -2.27     -1.28  -2.20     -0.84  -1.57      0.07  -1.86     -0.73
                       (0.92)    (0.63)       (1.62)    (1.12) (2.06)    (1.46) (2.46)    (1.77) (1.76)    (1.24)
         f36             2.87      2.38         5.93      4.99   7.43      6.17   8.34      6.83  6.14       5.09
                       (0.96)    (0.61)       (1.78)    (1.05) (2.36)    (1.37) (2.86)    (1.65) (1.98)    (1.15)
         f48            -0.28      0.26        -1.13     -0.15  -1.33     -0.13  -2.56     -1.20  -1.32     -0.30
                       (0.58)    (0.36)       (1.08)    (0.58) (1.42)    (0.72) (1.70)    (0.84) (1.19)    (0.60)
         f60            -0.53     -1.00        -0.88     -1.74  -1.10     -2.12  -0.31     -1.45  -0.71     -1.58
                       (0.58)    (0.36)       (1.25)    (0.75) (1.76)    (1.04) (2.24)    (1.29) (1.45)    (0.85)

          Wald                       20.30              23.57               23.08               22.50              23.12
          p-val (boot)               0.218              0.180               0.161               0.139              0.167
          p-val (asy)                0.000              0.000               0.000               0.000              0.000

          Wald                      429.29
          p-val (boot)              0.208
          p-val (asy)               0.000

Notes. As in Table 2-3, except using the additional variables A4 and the sample period 1991:7 to 2006:12. In this case, the row Wald
 gives the Wald statistic testing the joint hypothesis that all the elements of are equal to zero.
     Table 3: Out-of-Sample RRMSPE for excess bond returns from augmented regression relative to baseline
                        Additional Regressors                     Bond Maturity
                                                           n = 24 n = 36 n = 48 n = 60
                        A1                    Unrestricted 0.913  0.910  0.914  0.902
                                                           0.059  0.041  0.041  0.030
                                              Restricted   0.948  0.925  0.900  0.892
                                                           0.124  0.060  0.030  0.016
                        A2                    Unrestricted 0.907  0.871  0.848  0.777
                                                           0.023  0.006  0.002  0.000
                                              Restricted   0.815  0.815  0.826  0.851
                                                           0.001  0.000  0.001  0.002
                        A3                    Unrestricted 0.910  0.925  0.940  0.957
                                                           0.059  0.103  0.146  0.189
                                              Restricted   0.960  0.948  0.938  0.931
                                                           0.182  0.147  0.147  0.130
                        A4                    Unrestricted 0.893  0.891  0.897  0.904
                                                           0.119  0.123  0.145  0.166
                                              Restricted   0.908  0.903  0.894  0.895
                                                           0.153  0.140  0.140  0.141
                        A5                    Unrestricted 0.685  0.690  0.707  0.723
                                                           0.008  0.009  0.019  0.027
                                              Restricted   0.730  0.719  0.699  0.698
                                                           0.016  0.015  0.017  0.018
                        A6                    Unrestricted 0.911  0.931  0.967  0.991
                                                           0.316  0.367  0.451  0.488
                                              Restricted   0.973  0.965  0.958  0.956
                                                           0.470  0.439  0.437  0.433

Notes. This table shows the root mean square prediction error for excess bond returns from the augmented model in which the
augmenting regressors are predicted to be zero divided by the root mean square prediction error from the baseline model. Bootstrap
p-values for one-sided tests testing the hypothesis of equality in root mean square prediction errors are shown in italics. The models
are as described in the notes to Table 2. Models are either restricted or unrestricted: Restricted predictions impose that there is a
single return forecasting factor with a di¤erent loading for each bond maturity n. Predictions are pseudo-out-of-sample with the …rst
prediction made half-way through the estimation period.
     Table 4-1: Regressions predicting excess stock returns using           log dividend-price ratio and/or stochastically
                          detrended short-term interest rate and            additional regressors A1
                         Regression:   Baseline  Aug.    Baseline              Aug.  Baseline   Aug.
                         dpt            0.018    0.031                                  0.016  0.031
                                       (0.018) (0.016)                                (0.019) (0.016)
                         r~tRF                            -0.749              -0.357   -0.550  -0.103
                                                         (1.037)             (0.920) (1.036) (0.903)

                             Wald                       38.104               39.676               39.352
                             p-val (boot)               0.000                0.000                0.000
                             p-val (asy)                0.000                0.000                0.000

                             Wald                       1.863                7.323                 8.063
                             p-val (boot)               0.750                0.197                 0.587
                             p-val (asy)                0.601                0.062                 0.234


Notes. The baseline regressions show the estimated coe¢ cients in regressions of 3 month excess stock returns (CRSP value-weighted
returns less the Fama-Bliss riskfree rate) on the corresponding log dividend-price ratio and/or the stochastically detrended short term
interest rate. The regressions are run on data from the …rst month of each quarter from 1968Q4 through 2006Q4. Asymptotic
standard errors are shown in parentheses. The augmented regressions control for the additional variable A1. The estimated
coe¢ cients on this additional variable are not reported, but the row Wald denotes the Wald statistic testing the hypothesis that it
is equal to zero along with the associated asymptotic and bootstrap p-values, constructed as described in the text. The row Wald
   denotes the Wald statistic testing the hypothesis that = 0, again along with the associated asymptotic and bootstrap p-values.
These are the same for each regression in the table because they depend only on fxt g and fwt g.
Table 4-2: Regressions predicting excess stock returns using   log dividend-price ratio and/or stochastically
                     detrended short-term interest rate and    additional regressors A2
                    Regression:   Baseline  Aug.    Baseline      Aug.  Baseline   Aug.
                    dpt            0.013    0.020                          0.013  0.020
                                  (0.015) (0.014)                        (0.015) (0.014)
                    r~tRF                            -1.336      -0.894   -1.343  -0.897
                                                    (0.668)     (0.615) (0.646) (0.596)

                    Wald                   23.77                16.68             18.66
                    p-val (boot)           0.002                0.004             0.003
                    p-val (asy)            0.000                0.001             0.000

                    Wald                   12.78                24.49             34.85
                    p-val (boot)           0.718                0.002             0.217
                    p-val (asy)            0.005                0.000             0.000
     Table 4-3: Regressions predicting excess stock returns using        log dividend-price ratio and/or stochastically
                          detrended short-term interest rate and         additional regressors A3
                         Regression:   Baseline  Aug.    Baseline          Aug.   Baseline   Aug.
                         dpt            0.034    0.036                              0.034   0.037
                                       (0.018) (0.019)                             (0.019) (0.019)
                         r~tRF                            0.221            0.270    0.386   0.476
                                                         (1.005)          (0.987) (0.983) (0.956)

                            Wald                      0.608                0.270               0.681
                            p-val (boot)              0.545                0.681               0.517
                            p-val (asy)               0.436                0.603               0.409

                            Wald                      0.125                1.237               1.540
                            p-val (boot)              0.782                0.365               0.633
                            p-val (asy)               0.724                0.266               0.463


Notes. The baseline regressions show the estimated coe¢ cients in regressions of 3 month excess stock returns (CRSP value-weighted
returns less the Fama-Bliss riskfree rate) on the corresponding log dividend-price ratio and/or the stochastically detrended short
term interest rate. The regressions are run on monthly data from 1985:02 through 2006:12. Asymptotic standard errors are shown
in parentheses. The augmented regressions control for the additional variable A3. The estimated coe¢ cients on this additional
variable are not reported, but the row Wald denotes the Wald statistic testing the hypothesis that it is equal to zero along with
the associated asymptotic and bootstrap p-values, constructed as described in the text. The row Wald denotes the Wald statistic
testing the hypothesis that = 0, again along with the associated asymptotic and bootstrap p-values. These are the same for each
regression in the table because they depend only on fxt g and fwt g.
     Table 4-4: Regressions predicting excess stock returns using         log dividend-price ratio and/or stochastically
                          detrended short-term interest rate and          additional regressors A4
                         Regression:   Baseline  Aug.    Baseline           Aug.   Baseline   Aug.
                         dpt            0.035    0.033                               0.038   0.036
                                       (0.025) (0.025)                              (0.025) (0.025)
                         r~tRF                            0.752             0.845    1.006   1.077
                                                         (1.126)           (1.119) (1.072) (1.069)

                            Wald                       1.943               2.783               2.132
                            p-val (boot)               0.242               0.157               0.223
                            p-val (asy)                0.163               0.095               0.144

                            Wald                       4.575               0.986               4.765
                            p-val (boot)               0.387               0.407               0.508
                            p-val (asy)                0.032               0.321               0.092

Notes. As in Table 4-3, except using the additional variable A4 and the sample period 1989:09 to 2006:12.
     Table 4-5: Regressions predicting excess stock returns using         log dividend-price ratio and/or stochastically
                          detrended short-term interest rate and          additional regressors A5
                         Regression:   Baseline  Aug.    Baseline           Aug.   Baseline   Aug.
                         dpt            0.066    0.065                               0.065   0.065
                                       (0.025) (0.025)                              (0.024) (0.025)
                         r~tRF                            1.060             1.076    0.981   0.990
                                                         (1.206)           (1.206) (1.113) (1.111)

                            Wald                       0.043                0.222               0.063
                            p-val (boot)               0.845                0.679               0.818
                            p-val (asy)                0.835                0.637               0.802

                            Wald                       2.001                0.193               2.067
                            p-val (boot)               0.467                0.737               0.660
                            p-val (asy)                0.157                0.660               0.356

Notes. As in Table 4-3, except using the additional variable A5 and the sample period 1991:07 to 2006:12.



     Table 4-6: Regressions predicting excess stock returns using         log dividend-price ratio and/or stochastically
                          detrended short-term interest rate and          additional regressors A6
                         Regression:   Baseline  Aug.    Baseline           Aug.   Baseline   Aug.
                         dpt            0.066    0.068                               0.065   0.068
                                       (0.025) (0.024)                              (0.024) (0.023)
                         r~tRF                            1.060             1.356    0.981   1.276
                                                         (1.206)           (1.176) (1.113) (1.057)

                            Wald                       6.753                7.128               7.460
                            p-val (boot)               0.230                0.197               0.189
                            p-val (asy)                0.080                0.068               0.059

                            Wald                       0.348                2.293               2.905
                            p-val (boot)               0.973                0.723               0.948
                            p-val (asy)                0.951                0.514               0.821

Notes. As in Table 4-3, except using the additional variables A6 and the sample period 1991:7 to 2006:12. In this case, the row Wald
 gives the Wald statistic testing the joint hypothesis that all the elements of are equal to zero.
      Table 5: Out-of-Sample RMSPE for excess stock returns from augmented regression relative to baseline
                                Additional Regressors         Predictors
                                                      dpt    r~tRF    dpt & r~tRF
                                A1                    0.960 0.947 0.917
                                                      0.010 0.001 0.002
                                A2                    0.974 0.978 0.975
                                                      0.093 0.000 0.089
                                A3                    0.999 1.000 0.998
                                                      0.357 0.398 0.284
                                A4                    1.028 0.995 1.033
                                                      0.955 0.189 0.967
                                A5                    1.014 0.999 1.014
                                                      0.876 0.390 0.862
                                A6                    1.013 0.996 1.005
                                                      0.736 0.339 0.555

Notes. This table shows the root mean square prediction error for excess stock returns from the augmented model in which the
augmenting regressors are predicted to be zero divided by the root mean square prediction error from the baseline model. Bootstrap
p-values for one-sided tests testing the hypothesis of equality in root mean square prediction errors are shown in italics. The models
are as described in the notes to Table 4.
                 Table A1: Simulated E¤ective Size of Nominal 5 Percent Tests of = 0,        = 0, and RRMSPE= 1
                                                   =0                       =0               RRMSPE=1
                               n=24 n=36 n=48 n=60 Average                       n=24        n=36 n=48 n=60
                  T=263
                  Asymptotic 23.9      23.2     21.6    20.5    21.7      62.5
                  Bootstrap     4.9     5.2      5.0     4.7     4.8       2.3    4.5         4.1     4.0      4.2

                  T=208
                  Asymptotic     26.2     25.1    24.9    24.2     24.9      69.4
                  Bootstrap       5.0      5.0     4.8     4.6      4.7       3.3     4.9     4.7     5.0      4.8

                  T=186
                  Asymptotic     29.0     28.9    28.0    27.6     27.8      75.3
                  Bootstrap       5.4      5.4     5.8     5.8      5.6       2.8     5.2     5.0     4.9      4.4

Notes. In these simulations, fyt g and fxt h g were …xed at their values in the excess bond return prediction application using sample
periods beginning in 1985:02, 1989:09 and 1991:07, respectively. These correspond to the regressions with three alternative economic
news indexes and give the sample sizes of T=263, T=208 and T=186, respectively. Normal random draws of the economic news
index with mean zero and standard deviation 8 were taken and were then cumulated from t-12 to t to form the augmenting variable
fwt g, making this a Gaussian MA(12). The Wald test statistics testing the hypothesis = 0 (for the di¤erent excess return series)
and = 0 were then computed and compared to the asymptotic and bootstrap critical values. The table reports the fraction of
simulations in which the test rejected using asymptotic and bootstrap critical values (nomional signi…cance level: 5 percent). The
table also reports the percentage of times that the bootstrap p-value for testing the hypothesis that RRMSPE=1 in the out-of-sample
forecasting experiment is less than 5 percent, using the unrestricted regression (there are no asymptotic p-values for this test).
          Fig. 1: 12-Month Moving Sum of Economic News Indexes Ending in Month Shown
100
                                                                                       A3
                                                                                       A4
                                                                                       A5


 50




  0




 -50




-100




-150
   1985          1990              1995              2000               2005            2010
