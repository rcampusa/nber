                                 NBER WORKING PAPER SERIES




                TESTING MODELS OF SOCIAL LEARNING ON NETWORKS:
                  EVIDENCE FROM A LAB EXPERIMENT IN THE FIELD

                                         Arun G. Chandrasekhar
                                           Horacio Larreguy
                                           Juan Pablo Xandri

                                          Working Paper 21468
                                  http://www.nber.org/papers/w21468


                       NATIONAL BUREAU OF ECONOMIC RESEARCH
                                1050 Massachusetts Avenue
                                  Cambridge, MA 02138
                                      August 2015




We are grateful to Daron Acemoglu, Abhijit Banerjee, Esther Duflo, Ben Golub, Matthew O. Jackson,
Markus Möbius, and Adam Szeidl for extremely helpful discussions. Essential feedback was provided
by Juan Dubra, Rema Hanna, Ben Olken, Evan Sadler, Rob Townsend, Xiao Yu Wang, Luis Zermeño
and participants at numerous seminars and conferences. We also thank Mounu Prem for excellent
research assistance. This project was possible thanks to the financial support of the Russell Sage Behavioral
Economics Grant. Chandrasekhar is grateful for support from the National Science Foundation GFRP.
Larreguy thanks the Bank of Spain and Caja Madrid for financial support. JPAL and CMF at IFMR
provided valuable logistical assistance. Gowri Nagraj and our field team were vital to the project’s
progress. All errors are our own. The views expressed herein are those of the authors and do not necessarily
reflect the views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2015 by Arun G. Chandrasekhar, Horacio Larreguy, and Juan Pablo Xandri. All rights reserved.
Short sections of text, not to exceed two paragraphs, may be quoted without explicit permission provided
that full credit, including © notice, is given to the source.
Testing Models of Social Learning on Networks: Evidence from a Lab Experiment in the
Field
Arun G. Chandrasekhar, Horacio Larreguy, and Juan Pablo Xandri
NBER Working Paper No. 21468
August 2015
JEL No. C91,C92,C93,D83

                                               ABSTRACT

Agents often use noisy signals from their neighbors to update their beliefs about a state of the world.
The effectiveness of social learning relies on the details of how agents aggregate information from
others. There are two prominent models of information aggregation in networks: (1) Bayesian learning,
where agents use Bayes' rule to assess the state of the world and (2) DeGroot learning, where agents
instead consider a weighted average of their neighbors' previous period opinions or actions. Agents
who engage in DeGroot learning often double-count information and may not converge in the long
run. We conduct a lab experiment in the field with 665 subjects across 19 villages in Karnataka, India,
designed to structurally test which model best describes social learning. Seven subjects were placed
into a network with common knowledge of the network structure. Subjects attempted to learn the underlying
(binary) state of the world, having received independent identically distributed signals in the first period.
Thereafter, in each period, subjects made guesses about the state of the world, and these guesses were
transmitted to their neighbors at the beginning of the following round. We structurally estimate a model
of Bayesian learning, relaxing common knowledge of Bayesian rationality by allowing agents to have
incomplete information as to whether others are Bayesian or DeGroot. Our estimates show that, despite
the flexibility in modeling learning in these networks, agents are robustly best described by DeGroot-learning
models wherein they take a simple majority of previous guesses in their neighborhood.


Arun G. Chandrasekhar                                 Juan Pablo Xandri
Department of Economics                               Department of Economics
Stanford University                                   Princeton University
579 Serra Mall                                        Fisher Hall, Office 212
Stanford, CA 94305                                    Princeton, NJ 08544
and NBER                                              jxandri@princeton.edu
arungc@stanford.edu

Horacio Larreguy
Harvard University
Department of Government
1737 Cambridge Street
CGIS Knafel Building 408
Cambridge, MA 02138
hlarreguy@fas.harvard.edu
      TESTING MODELS OF SOCIAL LEARNING ON NETWORKS:
        EVIDENCE FROM A LAB EXPERIMENT IN THE FIELD

     ARUN G. CHANDRASEKHAR‡ , HORACIO LARREGUY§ , AND JUAN PABLO XANDRIı



        Abstract. Agents often use noisy signals from their neighbors to update their beliefs
        about a state of the world. The effectiveness of social learning relies on the details of how
        agents aggregate information from others. There are two prominent models of information
        aggregation in networks: (1) Bayesian learning, where agents use Bayes’ rule to assess the
        state of the world and (2) DeGroot learning, where agents instead consider a weighted
        average of their neighbors’ previous period opinions or actions. Agents who engage in
        DeGroot learning often double-count information and may not converge in the long run.
        We conduct a lab experiment in the field with 665 subjects across 19 villages in Karnataka,
        India, designed to structurally test which model best describes social learning. Seven
        subjects were placed into a network with common knowledge of the network structure.
        Subjects attempted to learn the underlying (binary) state of the world, having received
        independent identically distributed signals in the first period. Thereafter, in each period,
        subjects made guesses about the state of the world, and these guesses were transmitted
        to their neighbors at the beginning of the following round. We structurally estimate
        a model of Bayesian learning, relaxing common knowledge of Bayesian rationality by
        allowing agents to have incomplete information as to whether others are Bayesian or
        DeGroot. Our estimates show that, despite the flexibility in modeling learning in these
        networks, agents are robustly best described by DeGroot-learning models wherein they
        take a simple majority of previous guesses in their neighborhood.
            Keywords: networks, social learning, Bayesian learning, DeGroot learning, experi-
        ments
            JEL Classification Codes: D83, C92, C91, C93




                                           1. Introduction

  The way in which individuals aggregate information is a critical feature of many eco-
nomic environments. Information and opinions about new technologies, job opportunities,
products, and political candidates, among other things, are largely transmitted through
Date: First Version: August 2011, This Version: August 2015.
We are grateful to Daron Acemoglu, Abhijit Banerjee, Esther Duflo, Ben Golub, Matthew O. Jackson,
Markus Möbius, and Adam Szeidl for extremely helpful discussions. Essential feedback was provided by
Juan Dubra, Rema Hanna, Ben Olken, Evan Sadler, Rob Townsend, Xiao Yu Wang, Luis Zermeño and
participants at numerous seminars and conferences. We also thank Mounu Prem for excellent research assis-
tance. This project was possible thanks to the financial support of the Russell Sage Behavioral Economics
Grant. Chandrasekhar is grateful for support from the National Science Foundation GFRP. Larreguy thanks
the Bank of Spain and Caja Madrid for financial support. JPAL and CMF at IFMR provided valuable
logistical assistance. Gowri Nagraj and our field team were vital to the project’s progress. All errors are
our own.
‡
  Stanford University, Department of Economics and NBER.
§
  Harvard University, Department of Government.
ı
  Princeton University, Department of Economics.
                                                     1
                   TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                             2

social networks. However, the signals individuals receive about the state of the world often
contain noise. A critical aspect of social learning, therefore, concerns how agents handle
and aggregate noisy information in order to resolve uncertainty and estimate the state of
the world.
   Consider the case where an individual A hears which of two choices each of her two
friends B and C intend to make in the coming period. For example, this could be which of
two fertilizers her friends intend to use or which of two candidates her friends are likely to
vote for. Both B and C have friends of their own, including some shared friends, and have
discussed their choices with them. Knowing all of this, when A updates her opinion as to
which choice is best, how should she incorporate B’s and C’s views? She may naively treat
them as independent clues about the optimal choice, or perhaps she is more sophisticated
and worries that there is a common component to their opinions because they have friends
in common. Or A may even worry about how B and C themselves make inferences about
the world, and thus, arrive at their opinions. In this paper, we conduct a lab experiment
in the field in order to carefully study the nature of social learning on a network.
   There are two broad paradigms of modeling social learning through networks. The first
is Bayesian learning, wherein individuals process information using Bayes’ rule (see, e.g.,
Banerjee (1992), Bikhchandani, Hirshleifer, and Welch (1992), Gale and Kariv (2003),
Acemoglu, Dahleh, Lobel, and Ozdaglar (2010), Mossel and Tamuz (2010), Lobel and
Sadler (forthcoming)). An environment often studied considers a situation where each
individual initially receives a signal about the state of the world and then subsequently
observes her neighbors’ guesses, before revising her own belief and offering an updated
guess in each period. A core result in this setting is that individuals eventually converge
in their beliefs as to which state of the world they are in; this guess is correct in large
networks when some regularity conditions hold (Gale and Kariv (2003), Mossel and Tamuz
(2010)). While Bayesian agents in large networks are able to come to the right opinion
in this environment, these results rely on sophisticated agents who are able to implicitly
discern between repeated and new information they receive through their neighbors over
time.
   This cognitive load that the Bayesian-learning model imposes over agents when these
learn in social networks led to a second paradigm: DeGroot learning (DeGroot, 1974). In
these models, agents are myopic and, after seeing the behavior or beliefs of their network
neighbors, they take a weighted average of these to construct their belief going into the
subsequent period. Ellison and Fudenberg (1993; 1995), DeMarzo, Vayanos, and Zwiebel
(2003), Eyster and Rabin (2008), Eyster and Rabin (2010), Golub and Jackson (2010), and
Jadbabaie, Molavi, and Tahbaz-Salehi (2012) among others, have studied related models of
this form. When agents communicate beliefs with their neighbors and engage in DeGroot
learning they converge to the truth in large networks (Golub and Jackson, 2010). However,
                      TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                                      3

this convergence is inefficient and agents located in networks that exhibit significant ho-
mophily – wherein agents tend to be connected more with similar others – converge slowly
to the truth (Golub and Jackson (2012)).
   The contrast between DeGroot and Bayesian learning grows when one moves away from
rich communication structures – where agents exchange beliefs with their neighbors – in De-
Groot learning models.1 In many environments of interest, agents observe their neighbors’
actions as opposed to their beliefs: for example, which fertilizer did a farmer use? Which
candidate did a neighbor support? The differences between the Bayesian and DeGroot
paradigms are particularly pronounced in these environments. While Bayesian learning
typically generates convergence to the truth in large societies, DeGroot learning may gen-
erate misinformation traps, wherein pockets of agents hang on to an incorrect opinion
for infinitely many periods. Thus, understanding which learning paradigm better predicts
group behavior is important both for understanding whether information transmission is
efficient and for thinking through policy designs that rely on information dissemination.2
   In this paper, we study whether Bayesian or DeGroot learning does a better job of de-
scribing learning on networks. We conducted a unique lab experiment in the field in 19
villages in rural Karnataka, India in 2010, with 665 subjects. We ran our experiments
directly in the villages so that we could study a population of interest to development
economists and policy makers – namely, those who could be potentially targeted by policy
that depends on social learning (e.g., the introduction of a new technology, health practices,
microfinance). We designed simple networks that maximized statistical power to distin-
guish between the different learning models in the Gale and Kariv (2003) environment. We
then conducted a lab experiment in the field, using these networks to study which learning
model better fits learning in social networks.
   We created networks of seven individuals, giving each individual a map of the entire
graph so that the full informational structure was comprehended. The underlying state
of the world was either 1 or 0 with equal probability. At t = 0 each individual received
an independent identically distributed (i.i.d.) signal about the underlying state of the
world and was informed that signals were correct with probability 5/7. After receiving
the signal, each individual privately made a guess about the state of the world, which was
communicated to each individual’s network neighbors. Thereafter, each individual made
guesses about the underlying state of the world, and these guesses were transmitted to her
neighbors at the beginning of the following round. Using this information, each individual
made a new guess about the state of the world, which in turn was communicated to
1For a recent review article, see Sadler (2014).
2Consider the case where the state of the world is either 0 or 1. In this binary environment, individuals
engaging in DeGroot learning often double-count information and may not reach consensus in the long
run even for extremely large graphs (as we show below). Meanwhile, in such an environment, Bayesian
learning mechanisms generically generate consensus in finite graphs and, moreover, in very large graphs
the populations’ limit belief coincides with the true state of the world (Gale and Kariv (2003), Mossel and
Tamuz (2010)). That is, if the world was truly 0, all individuals would eventually come to believe this.
                     TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                                   4

each of her network neighbors at the beginning of the following period. Individuals were
incentivized to guess correctly, as we discuss in further detail below.3
   In comparing Bayesian and DeGroot learning, the following subtle issue must be consid-
ered. The standard Bayesian-learning model typically encodes two important but distinct
features: (1) agents are Bayesian, and therefore apply Bayes’ rule to make inferences
about neighbors’ information sets; and (2) agents have common knowledge of the ways in
which their neighbors map information sets to actions. The first condition is that agents
are Bayesian, and the second condition is that they have correct beliefs about the other
agents’ types, i.e., whether they are Bayesian or DeGroot learners. The most extreme
version of this is a model is one of complete information where everyone is Bayesian and
this is common knowledge. Our approach is unique and relaxes both of these features when
studying the data. We structurally estimate a Bayesian learning model with incomplete
information, where agents need not know ex-ante how others are learning, and relax com-
mon knowledge, as well. We then ask which model generates the best fit of the data. Thus,
the key parameter in our estimation is ﬁ, which is the share of agents in the population
who are Bayesian, with the assumption that a 1 ≠ ﬁ share of agents are DeGroot learners
in their thinking. We are then interested in what ﬁ œ [0, 1] best describes the data from
the experiment.
   To assess how a particular model fits the experimental data, we look at the data from
two different perspectives: the network level and the individual level. The network-level
analysis considers the entire network and sequence of actions by agents as a single obser-
vation. That is, we consider the path of actions predicted by theory under a model of
social learning for each individual in each period. In individual-level analysis, instead of
focusing on the the social network itself as the observational unit, we consider the action
of an individual given a history of actions.
   Our core results are as follows. First consider the network-level analysis. We find that
the incomplete information model that best explains the data is one where the share of
Bayesian agents in the population (ﬁ) is 0. Thus, the data is best fit by a model where
all agents are DeGroot. Specifically, this model explains between 86% and 88% of the
the actions taken in the data. This is not to say that social learning does not resemble
Bayesian learning; a complete information Bayesian learning model explains 82% of the
experimental data. However, this fit largely originates from the fact that the predictions
of the Bayesian and DeGroot models often agree. Second, at the individual level, again we
find that incomplete information Bayesian models fit the data best when the proportion of
Bayesian agents (ﬁ) is 0 and there is common knowledge about this. In fact, the DeGroot-
learning models explain between 89% and 94% of the actions taken by agents given a

3While it could be the case that players were extremely sophisticated and engaged in experimentation in
early rounds, anecdotal evidence from participants suggests that this is not the case. In addition, the
theoretical and experimental literature assumes away experimentation (see, e.g., Choi et al. (2009)).
                      TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                                              5

history of actions. Meanwhile, the complete information Bayesian model only explains
74% of those actions.
   We also establish several supplementary results which may also be of independent inter-
est. First, we explore the DeGroot action model (also called the voter model) and demon-
strate that even in simple networks where Bayesian learners who observe their neighbors’
actions and DeGroot agents who fully communicate their beliefs would converge to the
truth, a non-vanishing and non-trivial share of agents could converge to the wrong beliefs.
In particular, we show that this pathology exists in social networks that have realistic fea-
tures (such as high triadic closure), which may have been formed for other reasons such as
to facilitate the sharing of risks (Jackson et al., 2012).
   Second, we develop a simple algorithm to compute incomplete information Bayesian
learning on networks that is the best network-independent algorithm. Specifically, it is
computationally tight in the sense that, asymptotically, there can be no faster algorithm
that is independent of network structure. This is a challenge that, to our knowledge,
previous work had not overcome, precluding a structural analysis of learning on networks
like ours. Namely, the algorithm is O(T ), where T is the number of rounds played.4
   Third, an approach taken in related work looks at learning models with trembles or, re-
latedly, quantal response equilibrium (QRE). We demonstrate that networks small enough
to avoid computational constraints are not large enough to separate between DeGroot
and Bayesian learning with trembles. Meanwhile those that are large enough to separate
between those models become computationally infeasible to study using trembles or QRE.
   Fourth, we discuss why model selection must be done through structural estimation
and in a lab setting. We show that natural examples of reduced form analyses, where
the intuitions of Bayesian and DeGroot learning are used to test for predictions in regres-
sion analysis of social learning data, may be problematic. Namely, the data generated by
Bayesian and DeGroot learning models do not necessarily conform to the intuition mo-
tivating the regressions. Thus, there much to be gained from a structural analysis. The
computational constraints for structural estimation of learning models in large networks,
however, suggests the importance of small networks in lab settings to separate between
models of social learning. Moreover, lab settings allow controlling priors of agents in the
network and the signal quality, as well as restricting the communication among individ-
uals and the social-network structure. Since structural estimation is often sensitive to
misspecification, it is difficult to cleanly identify which model best describes the data in a
non-laboratory context.
   In terms of related work, Gale and Kariv (2003) study the Bayesian learning environment
that we build upon. They only focus on Bayesian learning and extend the learning model to
a finite social network with multiple periods. At time t, each agent makes a decision given

4An algorithm is O (T ) if the number of computations as a function of T , f (T ), is such that   f (T )
                                                                                                           æ M for
                                                                                                    T
some constant M . In particular, this is true if f (T ) = M T , as it is in our algorithm.
                     TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                                      6

her information set, which includes the history of actions of each of her neighbors in the
network. Via the martingale convergence theorem, they point out that connected networks
with Bayesian agents yield uniform actions in finite time with probability one. Choi et al.
(2005, 2009) make a seminal contribution to the empirical literature of social learning
by testing the predictions derived by Gale and Kariv (2003) in a laboratory experiment.
They are able to show that features of the Bayesian social learning model fit the data well
for networks of three individuals. Note that they do not study the DeGroot models and
whether this could possibly explain the learning behavior, which is our aim. We show
that their networks do not allow for statistical power under the DeGroot alternatives. In
extremely simple networks, such as the ones studied in their paper, there are few (if any)
differences in the predicted individual learning behavior by the Bayesian and DeGroot-type
learning models.5
   The works most closely related to ours are Möbius et al. (2015), who study how informa-
tion decays as it spreads through networks, and Mueller-Frank and Neri (2013) and Mengel
and Grimm (2014), who conduct lab experiments similar to ours. Möbius et al. (2015) test
between DeGroot models and an alternative model that they develop in which individuals
“tag” information by describing its origin (called a “streams” model). Their experiment
uses network data from Harvard undergraduates in conjunction with a field experiment and
finds evidence in favor of the “streams” model model in which individuals “tag” information.
In our experiment, we shut down the possibility that individuals “tag” information. This
allows us to compare the Bayesian model to DeGroot alternatives since, as described above,
looking at Bayesian learning even in a complete information context (let alone incomplete
information) is impossible in such a field setting. As noted by Möbius et al. (2015), the
conclusions of our work and theirs suggest that in complicated environments where tagging
can be difficult, agents may exhibit more DeGroot-like learning behavior. Subsequent to
our work, and independently of us, both Mueller-Frank and Neri (2013) and Mengel and
Grimm (2014) conducted lab experiments to look at Bayesian versus DeGroot learning. A
nice feature of these studies is the way they offer some modified models of learning. Crucial
differences include the fact that they don’t reveal the information about the entire network
to their subjects, making the inference problem more complicated, and more importantly,
they do not allow for their agents to have incomplete information (which can greatly change
the benchmark Bayesian’s behavior). We are particularly interested relaxing the complete
information Bayesian model with common knowledge of Bayesian rationality since, in a
sense, it serves as a weak straw man. In theory, with a mix of Bayesian and naive agents,
a Bayesian agent understanding the heteroegeneity in the population would behave in a
manner such that the data would look very different from a world in which all agents are

5The literature on social learning experiments begins with Anderson and Holt (1997), Hung and Plott
(2001), and Kubler and Weizsacker (2004). Explicit network structures are considered in a series of papers
by Gale and Kariv (2003) , Choi et al. (2005, 2009), and Celen et al. (2010).
                          TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                                            7

Bayesian. Taken together, there is a vibrant literature interested in whether learning in
networks can be thought of as DeGroot or Bayesian.
   The rest of the paper is organized as follows. Section 2 develops the theoretical frame-
work. Section 3 contains the experimental setup. Section 4 describes the structural esti-
mation procedure and the main results of the estimation. Section 5 presents the discussion
of the difficulties of reduced form approaches. Section 6 concludes.

                                                 2. Framework

2.1. Notation. Let G = (V, E) be a graph with a set V of vertices and E of edges and
put n = |V | as the number of vertices. We denote by A = A (G) the adjacency matrix of G
and assume that the network is an undirected, unweighted graph, with Aij = 1 {ij œ E}.
Individuals in the network are attempting to learn about the underlying state of the world,
◊ œ {0, 1} . Time is discrete with an infinite horizon, so t œ N.
   At t = 0, and only at t = 0, agents receive iid signals si |◊, with P (si = ◊|◊) = p and
P (si = 1 ≠ ◊|◊) = 1≠p. The signal correctly reflects the state of the world with probability
p. In every subsequent period, the agent takes action ai,t œ {0, 1}, which is her guess of the
underlying state of the world. Figure 1 provides a graphical illustration of the timeline.




               t=0!                        t=1!                      t=2!           …!               t=T!
          nature!picks!         5!agents!receive!iid!     5!observe!network!             5!observe!network!
          binary!state!         signals!about!state!of!   neighbors’!t*1!guesses&        neighbors’!T*1!
          of!the!world!         the!world,!correct!       &                              guesses&
          θ ={0,1}!             with!probability!p&       5!update!beliefs!about!        &
          !                     &                         state!of!the!world!            5!update!beliefs!about!
                                5!every!i!guesses!the!    !                              state!of!the!world!
                                state!of!the!world,!      5!every!i!guesses!the!         !
                                given!by!ai1&             state!of!the!world,!           5!every!i!guesses!the!!
                                !                         given!by!ai2&                  state!of!the!world,!
                                                          !                              given!by!aiT&
                                                                                         !
                                                                                         5!u:lity!is!given!by!
                                                                                            u(aiT)!=!1{aiT!=!!θ}
                                                                                         !

                                             Figure 1. Timeline

   Through their interactions, agent try to learn about the initial signal configuration s =
(s1 , ...., sn ), with si œ {0, 1}.6 Note that the set of all such configurations, S := {0, 1}n , has
6In the complete information model, the signal configuration s completely determines how all other agents
play, and is therefore a sufficient statistic for one’s belief about ◊. Hence, the signal configurations s œ S
are the only relevant states that Bayesian agents need to learn about. In the incomplete information
model (where other players in the network may not be Bayesian) the signal configuration s no longer
determines how neighbors play; agents need to also learn how their neighbors process their information.
                       TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                                           8

                                          q
2n elements. Finally, we use di = j Aij to refer to the vector of degrees for i œ {1, ..., n}
and › for the eigenvector7 corresponding to the maximal eigenvalue of A.

2.2. Bayesian Learning. In our analysis, we consider a model of Bayesian learning with
incomplete information. Individuals have common priors over the relevant state spaces
(described below) and update according to Bayes’ rule in each period. We formalize the
model in Appendix A.
   Each agent is drawn i.i.d. from an infinite population which has a ﬁ share Bayesian
agents and a 1 ≠ ﬁ share DeGroot agents. This fact is common knowledge – obviously a
feature relevant only for the Bayesian agents – as is the structure of the entire network.
Since there is incomplete information about the types of the other agents in the network,
Bayesian individuals attempt to learn about the types of the other agents in the network
while attempting to learn about the underlying state of the world.
   The incomplete information setup is an important step beyond the complete information
Bayesian environment, which restricts ﬁ = 1. For instance, if an individual believes that
her neighbor does not act in a Bayesian manner, she processes the information about
observed decisions accordingly; as outside observers, the econometricians might think that
she is not acting as a Bayesian. This is a serious problem when testing Bayesian learning,
as we need to make very strong assumptions about common knowledge. A model in which
there is incomplete information about how other players behave attempts to address this
issue while only minimally adding parameters to be estimated in an already complicated
system.

2.3. DeGroot Learning. We now briefly discuss DeGroot learning (DeGroot (1974)).
DeMarzo et al. (2003), Golub and Jackson (2012), and Jackson (2008) contain an extensive
reviews of DeGroot models. In our experiment, we do not consider belief-communication
models; instead, we consider a DeGroot model where individuals observe each others’
actions in this binary environment. We call this a DeGroot action model. It has also been
called a voter model (Mossel and Tamuz, 2014). The basic idea is to maintain a parallel
structure with the Bayesian environment of Gale and Kariv (2003), where the state is
binary but agents can only pass on their best guesses, as in the Bayesian benchmark.
In action models, individuals observe the actions of their network neighbors, whereas in
communication models, individuals are able to communicate their beliefs to their neighbors.
One might also call these (weighted) majority models; individuals choose the action that
is supported by a (weighted) majority of their neighborhood.
   We are interested in action models for several reasons. First, observe that the corre-
sponding models of Bayesian learning on networks are action models, so it is the appropriate
comparison. A model with communication in Bayesian learning, where agents pass their
Here the sufficient statistics for one’s belief about ◊ is s as well as the configuration of types – whether each
individual is Bayesian or DeGroot. See Appendix A.2 for a formal discussion.
7Normalized in ¸ .
                  2
                     TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                                 9

posteriors or information sets, becomes fundamentally uninteresting in an environment
such as ours. Generically, if agents pass on information sets, for instance, within a number
of time periods equal to the diameter of the network, each individual learns everyone else’s
signals (Mueller-Frank, 2014). Second, it is extremely difficult to get reliable, measurable,
and believable data of beliefs in a communication model for a lab experiment conducted in
the field in rural villages. Third, as it is difficult to control and map into data exactly what
is (or is not) communicated by various agents in a more general communication model, we
are able to focus on the mechanics of the learning process by restricting communication to
observable actions. Fourth, this also fits with the motivating literature wherein individuals
may only observe the actions of their neighbors, such as technology usage, microfinance
adoption, statement of political preferences, etc.
   Let T = T (A) be a weighted matrix which parametrizes the weight that person i gives
to the action of person j. We study three natural parameterizations of the DeGroot model.
The first is uniform weighting wherein each individual weights each of her neighbors exactly
the same. The weight matrix T u (A) is given by
                                             Aij                 1
(2.1)                              Tiju =          and Tiiu =
                                            di + 1            di + 1
meaning that each individual puts (di + 1)≠1 weight on each of her di neighbors as well as
on herself.
  The second model we consider is degree weighting. Each individual weights her neighbors
by their relative popularity, given by degree. T d (A) is given by
                                      dj                       di
(2.2)                    Tijd = q               and Tiid = q             ,
                                   jœNi dj + di             jœNi dj + di

where Ni is the set of neighbors of individual i.
   The third model is eigenvector weighting.8 An individual places weight on her neighbor
proportional to the neighbor’s relative importance, given by eigenvector centrality. T e (A)
is given by
                                       ›j                       ›i
(2.3)                     Tije = q               and Tiie = q
                                    jœNi ›j + ›i             jœNi ›j + ›i

where › is the eigenvector corresponding to the maximal eigenvalue of A. This is moti-
vated by the idea that an individual may put greater weight on more information-central
neighbors, which eigenvector centrality captures.
   The behavior of individuals that learn according to the DeGroot model is as follows.
At time t = 0, individuals receive signals s = (s1 , s2 , ..., sn ), and accordingly, take actions
ai,0 = 1 {si = 1}. Let a0 = (a1,0 , a2,0 , ..., an,0 ) . At t = 1, beliefs are denoted by I1 = T a0 ,
and actions are chosen according to a1 =1 {I1 > 1/2}. Now consider time t = k + 1 with
lagged set of actions ak . Then, beliefs are formed as indicated by Ik+1 = T ak , and actions

8Thanks to Matt Jackson for suggesting this alternative.
                       TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                                      10

are chosen as denoted by ak+1 =1 {Ik+1 > 1/2} . While a very natural parametrization of
learning, the DeGroot model misses the inferential features that characterize Bayesian
learning. If the limit exists,

             aŒ =        lim 1 {T ak+1 > 1/2}
                        kæŒ
                   =     lim 1 {T · 1 {T ak > 1/2} > 1/2} , ak = 1 {T ak≠1 > 1/2} .
                        kæŒ

While we cannot easily analyze the limit exploiting the linear structure, as is done with
DeGroot communication models, we discuss its implications below.




2.4. An Illustrative Example: Social Quilt Trees. We present a simple setup which
yields asymptotic learning under communication DeGroot models and consensus under
action Bayesian models, but fails asymptotic learning and violates consensus with action
DeGroot models. Namely, if agents play according to the uniform weighting DeGroot
model, a fraction of local neighborhoods will become “stuck” in an information trap; i.e.,
some neighborhoods will have all agents eventually choosing ai = 1 ≠ ◊, and will keep
doing this forever.9 This demonstrates a wedge between DeGroot and Bayesian learning
in models with discrete actions.
   We say a network T = (V, E) is a binary tree if it is a rooted tree where every node
has either two children, or none (i.e., a terminal node).10 The level of a node j œ V is the
shortest path distance to the root node i0 . The set of nodes of level d is denoted by Ld .11
   Define the depth of a network d to be the maximum distance1 possible
                                                                    2    from the root node.
A complete binary tree of depth d is a binary tree Td = V̂d , Êd where every terminal
node has depth d. Note that in this network, every node (other than the root) has 1 sibling
(a node that has the same parent) to which it is not connected.
   Given Td , we define a complete binary quilt of depth d to be a network Qd = (Vd , Ed ),
super-graph of Td , where Vd = V̂d and Ed = Êd ﬁ {ij : i and j are siblings}, as illustrated
in Figure 2.4




9The wisdom of DeGroot learning hinges on the fact that an extensive amount of information is passed
along such a model relative to the action model of Bayesian learning. For a parallel, in Bayesian learning if
we introduced a communication model, then the filtering problem would be considerably simpler since an
agent would know her neighbors’ posteriors exactly.
10A terminal node, or leaf, is a node in a tree with only one neighbor (i.e., it has degree 1)
11Formally, L := {j œ V : d (i , j) = d} where d (i, j) is the length of the shortest path between i and j
              d                 0
(which exists, given that T is a connected graph)
                   TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                            11




        Figure 2. Complete binary tree Td and complete binary quilt Qd (d = 3)


   The motivation for the graph structure comes from Jackson et al. (2012), who study
network architecture that arise as equilibria in favor-exchange games. They show that
these networks are social quilts. A social quilt is a patchwork of substructures (e.g., trian-
gles) pasted together in specific ways: the substructures do not share edges, only vertices.
From the applied perspective, this speaks to how the incentives to overcome contractual
incompleteness or lack of formal insurance influence how well a society may engage in social
learning. If graphs are constructed as equilibria of risk-sharing or favor-exchange games,
then they may have such quilt-like substructures. However, at the same time, because of
fixed costs in building relationships, the resulting pattern of relationships will also be the
network on which information travels. We note that, if individuals are indeed DeGroot in a
discrete learning process, it may be the case that information does not transmit efficiently
through social quilts.

Definition 2.1. We say that node i œ Vd is stuck if there exists ti œ N such that for all
t Ø ti , ai,t = 1 ≠ ◊.
   A node is stuck if the node for all but finitely many periods takes the same (wrong)
action. Figure 3 provides two examples of nodes that get stuck despite the majority of
nodes in the network receiving the right signal, in the network Q2 :
   Panel A of Figure 3 illustrates the problem. Assume that for some subtree of level d ≠ 1
of Qd (i.e., where the siblings are terminal nodes), which connects to the rest of the network
through its parent node (as in Figure 3), we have the initial signal endowment shown. To
get a lower bound on the number of nodes that get stuck in the wrong action, we can
simply assume that the parent node of the subtree always chooses the right action for all
rounds. However, even in this case, the nodes in the lower right triangle act in the same
(wrong) manner for all periods, since they follow the DeGroot uniform weighting rule as
in equation 2.1.
   Take a sequence of networks {Qd }dœN of growing complete binary quilts. As d æ Œ,
there will be a non-vanishing fraction of subtrees with this initial configuration of signals.
Following the argument of Panel A of Figure 3, take the subgraph induced by levels d ≠
                       TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                                     12




                 (A)                             (B)

         Figure 3. In Panel A, two nodes are stuck for all periods t œ N, even
         though 5 of the 7 nodes have received the correct signal. In Panel B, in the
         first period, 4 nodes receive the correct signal, and after one node switches,
         3 are stuck.

2, d ≠ 1 and d of Qd ; i.e., subgraph QÕ induced by V Õ = Ld≠2 ﬁ Ld≠1 ﬁ Ld . It consists of the
union of 2d≠2 connected components of 7 players, like the ones in panel A and B of Figure
3. If d is big enough for the law of large numbers to kick in, there will be approximately
a fraction of p5 (1 ≠ p)2 of those components with signal configurations as in Panel A, and
each of these will have at least 2 players getting stuck choosing the wrong action. This
already gives a non-zero lower bound on the fraction of agents that will never match their
action to the true state of nature, even if the network is arbitrarily big. In this case, the
fraction of agents that get stuck in components like Panel A is at least f = 14 p5 (1 ≠ p)2 as
d æ Œ.12
   This example has demonstrated the following result. We say that a sequence of networks
exhibits asymptotic learning if all but a vanishing share of nodes correctly guess the state
of the world.

Proposition 2.1. For a sequence of complete binary quilts with i.i.d. signals with proba-
bility p, with probability approaching one:
    (1) under the Bayesian action model, there is asymptotic learning;
    (2) under the DeGroot communication model with uniform weighting there is asymp-
        totic learning13; but however,
    (3) under the DeGroot action model with uniform weighting a non-vanishing fraction
        of nodes get stuck and there is no asymptotic learning.

12To see this, one can show that (1) # {V } = 2d+1 ≠ 1 (see Appendix C); (2) there are 2d≠2 components
in V Õ , each of 7 nodes; and (3) At least
                                        ! 2 players
                                               " out of Panel A-type components get stuck. Then, if d is
large enough, we have that f ¥ 2 ◊ p5 1 ≠ p2 2d≠2 /(2d+1 ≠ 1) = 14 p5 (1 ≠ p)2 ◊ (2d+1 /2d+1 ≠ 1). Since the
second term converges to 1 as d æ Œ, we get the desired expression.
13Let µ = ◊p + (1 ≠ p)(1 ≠ ◊) and T is a sequence of convergent row-normalized matrices.
                                               n
As defined in Golub-       and Jackson
                                     -   (2010), there is asymptotic learning (the sequence is wise) if
plimnæŒ supiÆn -limtæŒ Tnt sn ≠ µ- = 0. In our context, there is asymptotic learning since in the limit
a share of nodes that have belief µ goes to one and therefore the nodes can distinguish µ > 0 or µ < 0, as
p is known.
                   TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                            13

Proof. All proofs are contained in Appendix D.                                              ⇤
That asymptotic learning occurs with the Bayesian action model follows from Mossel et al.
(Forthcoming), and that it occurs with DeGroot communication for this model follows
from Corollary 1 of Golub and Jackson (2010). The result for the DeGroot action model
is apparent from the previous example. To illustrate the severity of Proposition 2.1, in
Figure 4 we show lower bounds on the expected fraction of nodes that are stuck. These
bounds are calculated by the argument following Figure 3, replicating the bounds obtained
for the signal configurations of Panel A and B for all possible signal configurations of each
component. (Appendix C further details how to calculate these bounds.) Even with high
quality signals (p = 0.7), at least 16% of nodes become stuck and do not asymptotically
learn.




                           Figure 4. Bounds for fraction stuck

   In addition to motivating the study of DeGroot action models in our experiment, this ex-
ample is of independent interest. It raises a question as to what types of network structures
are better for asymptotic social learning. We conjecture that graphs with sufficiently good
expansion properties generate asymptotic learning, even when individuals only observe
their neighbors’ actions and use DeGroot rule of thumb to form their beliefs.
   To illustrate this, we consider data from Banerjee et al. (2013) consisting of detailed
network data in 75 villages in Karnataka, India. We consider the networks consisting of
information relationships, which in other work we have shown to correlate strongly with
favor exchange relationships. We show that a DeGroot action learning process is likely to
get stuck. In Panel (A) we examine simulations conducted on the 75 empirical networks
and quantify the degree of stuckness. Specifically, with a signal quality of 0.55, in the
median village, at least 78% of the nodes that initially received the wrong information stay
stuck at the wrong belief. Even with a signal quality of 0.66, in the median village, at least
37% of households that initially received the wrong information stay stuck. Furthermore,
                                      TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                                                                                       14

                      CDF of fraction who fail to learn across 75 Indian Villages                                    CDF of fraction who fail to learn across Erdos-Renyi Graphs
         1                                                                                            1
                                         p = 0.525                                                                                      p = 0.525
                                         p = 0.55                                                                                       p = 0.55
        0.9                              p = 0.575                                                   0.9                                p = 0.575
                                         p = 0.6                                                                                        p = 0.6
                                         p = 0.66                                                                                       p = 0.66
        0.8                                                                                          0.8


        0.7                                                                                          0.7


        0.6                                                                                          0.6
 F(x)




                                                                                              F(x)
        0.5                                                                                          0.5


        0.4                                                                                          0.4


        0.3                                                                                          0.3


        0.2                                                                                          0.2


        0.1                                                                                          0.1


         0                                                                                            0
          0.1   0.2       0.3      0.4        0.5        0.6         0.7   0.8      0.9   1                0   0.1      0.2     0.3      0.4        0.5     0.6     0.7        0.8   0.9   1
                                         Fraction failing to learn                                                                      Fraction failing to learn
                                (A)                                                                                                   (B)

                Figure 5. Both panels (A) and (B) present CDFs of the fraction of nodes
                that initially received the signal 1 ≠ ◊ that became stuck at the wrong
                belief for various levels of p. Panel (A) presents results where we conduct
                simulations using the 75 Indian village networks from Banerjee et al. (2013).
                Panel (B) presents the same results for Erdos-Renyi networks which have
                an average degree that matches the Indian network data.


in over 1/3 of the villages, at least 50% of households that initially received the wrong
information stay stuck. In Panel (B) we repeat the exercise but for Erdos-Renyi graphs
calibrated to have an average degree that matches the empirical data. We find, similarly,
that DeGroot action learning is likely to get stuck. But furthermore, by comparing the
simulations under the Indian village networks and the corresponding Erdos-Renyi graphs,
we can see that the problem is somewhat exacerbated for the empirical networks. For
instance, 90% of empirical network simulations have at least 35% of nodes failing of to
learn whereas the corresponding number is at least 18% for Erdos-Renyi graphs. This
suggests evidence that, as shown in Jackson et al. (2012), the networks organized to aid
informal transactions in the face of limited commitment have generated network structures
that are prone to misinformation traps.


                                                                             3. Experiment

3.1. Setting. We conducted 95 experimental sessions for each of the three chosen networks
across 19 villages in Karnataka, India. The experiments had 665 total subjects. The
villages range from 1.5 to 3.5 hours’ drive from Bangalore. We chose the village setting
because social learning through networks is of the utmost importance in rural environments;
information about new technologies (Conley and Udry, 2010), microfinance (Banerjee et al.,
2013), political candidates (Cruz, 2012), among other things, propagates regularly through
social networks.
                   TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                          15

3.2. Implementation and Game Structure. In every village, we run an average of 5
sessions, each with 7 participants, given that each of the networks of our experiment had
7 nodes. We recruited an average of 35 individuals from a random set of households from
each village. We brought the individuals to a public space (e.g., marriage hall, school,
dairy, barn, clusters of households) where we conducted the experiment. While individuals
were recruited, the public space was divided into “stations.” In each station there was a
single surveyor to monitor the single participant assigned to the station at random. This
ensured that participants could not observe each other nor could they communicate. Often
times, stations would be setup across several buildings.
   In each village, individuals anonymously played the social learning game three times,
each time with a different network structure. The three networks (see Figure 6) were played
with a random order in each village. At the beginning of each game, all participants were
shown two identical bags, one with five yellow balls and two blue balls and the other,
with five blue balls and two yellow balls. One of the two bags was chosen at random to
represent the state of the world. Since there was an equal probability that either bag could
be chosen, we induced priors of 1/2. As the selected bag contained five balls reflecting the
state of the world, participants anticipated receiving independent signals that were correct
with probability 5/7.
   After an initial explanation of the experiment and payments, the bag for the first game
was randomly chosen in front of the participants. The participants were then assigned to
stations where each was shown a sheet of paper with the entire network structure of seven
individuals for that game, as well as her own location in the network. The neighbors’ past
decisions were also communicated to subjects on sheets of paper that presented an image
of the network and colored in their neighbors’ guesses.
   Once in their stations, after receiving their signals in round zero, all participants si-
multaneously and independently made their best guesses about the underlying state of the
world (which bag had been selected). The game continued to the next round randomly and
on average lasted 6 rounds. If the game continued to the second round, at the beginning of
this round, each participant was shown the round one guesses of the other participants in
her neighborhood through the mentioned procedure. Agents updated their beliefs about
the state of the world and then again made their best guesses about it. Once again, the
game continued to the following round randomly. This process repeated until the game
came to an end. Notice that, after the time zero set of signals, no more signals were drawn
during the course of the game. Participants could only observe the historical decisions of
their neighbors and update their own beliefs accordingly. After each game, participants
were regrouped, the color of the randomly chosen bag was shown, and if appropriate, a
new bag was randomly chosen for the next game. Participants were then sent back to their
stations and the game continued as the previous one. After all three games were played,
individuals were paid Rs. 100 for a randomly chosen round from a randomly chosen game,
                   TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                                    16

as well as a Rs. 20 participation fee. Participants then faced non-trivial incentives to
submit a guess that reflected their belief about the underlying state of the world. The
incentive was about three-fourths of a daily agricultural wage.

3.3. Network Choice. We selected networks specifically so that we could separate be-
tween various DeGroot and Bayesian models considered in the paper.


                                                                                     1"

                         1"         5"


                                                                        2"           4"        5"

                    2"         4"              6"

                                                                                     3"


                         3"         7"

                                                                                6"        7"




                         (A) Network 1                                       (B) Network 2


                                                              1"




                                                         3"        4"
                                         2"                                    5"




                                                    6"                  7"




                                              (C) Network 3

                Figure 6. Network structures chosen for the experiment.

   The previous experimental literature on Bayesian learning on networks (Choi et al. (2005,
2009)) make use of several three-person networks. However, we are unable to borrow these
networks for our study as they were not designed for the purpose of separating between
DeGroot and Bayesian learning. In fact, the networks utilized in Choi et al. (2005, 2009)
lack power to pit Bayesian learning against the DeGroot alternatives posited above. Panel
A of Table 1 shows the fraction of observations that differ across complete information
Bayesian learning and the DeGroot alternatives for each of the three networks used in Choi
et al. (2005) and Choi et al. (2009). In two of the networks, there are no differences between
the equilibrium paths of Bayesian learning and the uniform and degree weighted DeGroot
alternatives (with a difference with eigenvector weighting of 6.8% in one of the networks).
While in the third network, the differences are on average 15.5% of the observations, these
are largely an artifact of many indifferences along equilibrium path for the DeGroot models,
which were broken by choosing the individuals’ past action.
   Given our goal of separating between Bayesian and DeGroot alternatives, we move to
an environment with seven agents as opposed to three, so that we obtain more power
                           TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                                          17



     1




                                                                    1
     .9




                                                                    .9
   Power




                                                                  Power
     .8




                                                                    .8
     .7




                                                                    .7
     .6




                                                                    .6
           .02       .04      .06                .08   .1   .12           .02    .04        .06        .08   .1
                                    Divergence                                            Divergence



                                          (A) Network 1                   (B) Network 2

                 Figure 7. (A) depicts the power and divergence frontier for degree weight-
                 ing DeGroot. (B) shows the power and divergence frontier for uniform
                 weighting

to distinguish between these models while still maintaining computational tractability.
Additionally, we consider a quality signal (p = 5/7) to prevent that the separation from
different learning models is driven by indifferences along equilibrium path, and thus, the
tie-breaking rule.14
   In order to select our three networks, we initially considered all connected and undi-
rected networks with seven nodes. Additionally, to prevent indifferences along equilibrium
path, we restricted to networks where the minimum degree of a given individual is two.
Further, to avoid confusing our experimental subjects, we restricted to planar networks
(networks that can be drawn so that their edges intersect only at their endpoints). Next,
we established a model selection criterion function. This criterion function depended on the
power to detect a DeGroot alternative against a complete information Bayesian null, using
our pilot data to generate an estimate of the noise, as well as a divergence function. The
divergence function measures the share of node-time observations for which the Bayesian
model (with ﬁ = 1) and a DeGroot model pick different actions,

                                   1     ÿÿ   T ÿ n -
                                                     - B
                                                                                 -
                                                                                 -
                   D (G) :=                          -ai,t (s | G) ≠ am
                                                                      i,t (s | G)- · P (s | ◊ = 1) ,
                               n (T ≠ 1) sœS t=2 i=1
where aBi,t (s | G) is the action predicted under the Bayesian model and ai,t (s | G) is
                                                                            m

the action predicted under DeGroot with m-weighting, where m is uniform, degree, or
eigenvector weighting.15
   Figure 7(A) depicts a scatter plot of power and divergence for network 1 in Figure 6.
We see that our network 1 is the best ex-ante choice to separate between the Bayesian
and DeGroot degree weighting models. Figure 7(B) illustrates the analogous figure for
14Moving to eight agents, for instance, would be exponentially more difficult for our structural estimation.
                   q              q
15P (s | ◊ = 1) = p i si (1 ≠ p)n≠ i si
                      TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                                        18

network 2 in Figure 6, and highlights that it is the best ex-ante choice to separate between
the Bayesian and DeGroot uniform weighting models. Lastly, the choice of network 3 in
Figure 6 was motivated by our exercise in section 2.4, and by the fact that it performed
very well in separating between the Bayesian and both the DeGroot uniform and degree
weighting models.16


                                      4. Testing the Theory

4.1. Estimation and Inference. In order to test how well model m fits the data in
session r, we will use the fraction of discrepancies between the actions taken by individuals
in the data and those predicted by the model. This is given by
                                                      1     ÿn ÿ Tr
                                D (m, r; ﬁ) :=                      Dm
                                                  n(Tr ≠ 1) i=1 t=2 i,t,r

where Di,t,r
         m = |aobs ≠ am | which computes the share of actions taken by players that are
                  i,t,r i,t,r
not predicted by the model m.17 To examine how poorly model m predicts behavior over
the entirety of the data set, we define the divergence function as
                                             1 ÿR
                                                       1     ÿn ÿ Tr
                             D (m; ﬁ) :=                             Dm .
                                             R r=1 (Tr ≠ 1)n i=1 t=2 i,t,r
This is simply the average discrepancy taken over all sessions. Model selection will be
based on the minimization of this divergence measure. Note that we include dependency
on ﬁ, the share of Bayesian agents believed to be in the population, since for the Bayesian
model, the prediction ami,t,r depends on ﬁ. The computation of the Bayesian actions are
described in Appendix A.
   While the divergence is the deviation of the observed data from the theory, we may define
the action prescribed by theory in one of two ways. First, we may look at the network level,
which considers the entire social learning process as the unit of observation; and second,
we may study the individual level wherein the unit of observation is an individual’s action
at an information set.
   When studying network level divergence, we consider the entire learning process as a
single observation. Theory predicts a path of actions under the true model for each indi-
vidual in each period given a network and a set of initial signals. This equilibrium path
that model m predicts is given the theoretical action am   i,t,v . When using this approach,
we try to assess how the social learning process as a whole is explained by a model. This

16Originally, we selected a different third network, which was the best ex-ante choice to separate between the
Bayesian and DeGroot eigenvector centrality weighting models. However, since that alternative of DeGroot
weighting model performed ex-ante very poorly at separating between models relative to the other DeGroot
weighting models, we decided to replace it for the social quilt network, which served a dual purpose.
17Since all models and all empirical data have a fixed first action (given by the signal endowment), the first
round should not enter into a divergence metric. Thus, we restrict attention to t Ø 2.
                   TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                            19

method maintains that the predicted action under m is not path-dependent and is fully
determined by the network structure and the set of initial signals.
   When we consider the individual level divergence, the observational unit is the individual.
The action prescribed by theory is conditional on the information set available to i at t ≠ 1
and the ex-ante probability that a given individual is a Bayesian learner as opposed to
some DeGroot-alternative learner: am    i,t,v is the action predicted for agent i at time t in
session r, given information set Pi,r,t and ﬁ.
   For every DeGroot alternative, we consider the probability that minimizes the divergence
function:
                                   ‚ m = argmin D (m; ﬁ)
                                   ﬁ
                                          ﬁœ[0,1]
where m indexes an incomplete information Bayesian model with each of the possible
DeGroot alternatives: uniform, degree, and eigenvector. In order to perform inference on
the parameter, we perform a Bayesian bootstrap (see a similar procedure used in Banerjee
et al. (2013)).
   Equipped with the minimizing value of ﬁ  ‚ m for m œ {u, d, e}, we are prepared to conduct
our analysis. Note that procedure is done both at the network level and the individual
level. In particular, in addition to simply identifying the best-fitting model, we can go one
step further and ask whether incorrect beliefs about others’ types can explain the data.
Specifically, given ﬁ
                    ‚ m , we can ask whether the divergence can be minimized further by,
for instance, drawing a population of all Bayesian agents who have heterogeneous priors
and lack common knowledge of Bayesian rationality, and therefore are employing ﬁ      ‚ m as a
mistaken belief. We are able to assess whether deviations from correct beliefs can rationalize
the data better than a singular DeGroot alternative. Finally, we can also look at a non-
nested hypothesis test of how the model with common knowledge of Bayesian rationality
compares to each of the DeGroot alternatives, and how the DeGroot models compare to
each other in terms of explaining the data.



4.2. Learning at the Network Level. We begin by treating the social network and the
entire path of actions as a single observation.



4.2.1. Comparing DeGroot and Complete Information Common Knowledge Bayesian Mod-
els. Before looking at the incomplete information estimation result, we begin by first look-
ing at comparisons of the three DeGroot models and the common knowledge, complete
information Bayesian model. Figure 8 presents the data in a graphical manner and Table
2 presents the results for non-nested hypothesis tests comparing each of the models in a
pairwise manner.
                      TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                                20


             0.25"


              0.2"

                                                                            Bayesian"
             0.15"
                                                                            Uniform"

               0.1"                                                         Degree"

                                                                            Eigenvector"
             0.05"


                 0"
                        All"      Network"1"   Network"2"   Network"3"


              Figure 8. Fraction of actions unexplained at the network level


   As seen in Figure 8, across all the networks, uniform weighting fails to explain 12% of the
data, degree weighting fails to explain 14% of the data, eigenvector-centrality weighting
fails to explain 13.5% of the data, and complete information Bayesian learning fails to
explain 18% of the data. This suggests that the DeGroot models, as well as the Bayesian
learning models each explain more than 80% of the observations, but the DeGroot models
do considerably better.
   Turning to the pairwise comparisons of fit, we conduct a non-nested hypothesis test
(Rivers and Vuong, 2002) using a nonparametric bootstrap at the session-game level,
wherein we draw, with replacement, 95 session-game blocks of observations and compute
the network level divergence.18 This procedure is analogous to clustering and, therefore, is
conservative exploiting only variation at the block level. We then create the appropriate
test statistic, which is a normalized difference of the divergence functions from the two
competing models.
   Our key hypothesis of interest is a one-sided test with the null of Bayesian learning
against the alternative of the DeGroot model. Table 2 presents the p-value results of the
inference procedure. Note that most of the values are essentially zero. First, looking
across all topologies both separately and jointly, we find evidence to reject the Bayesian
model in favor of all the DeGroot alternatives. Second, we find that uniform weighting
dominates every alternative across every topology both separately and jointly. Ultimately,
the bootstrap provides strong evidence that the uniform-weighting DeGroot model best
describes the data generating process when analyzed at the network level.

18We have 95 village-game blocks in networks 1 and 2, and 75 for network 3. We redraw with replacement
the same number that we have in our empirical data.
                      TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                                      21

4.2.2. Incomplete Information Bayesian Learning. We now present our main results using
the network level divergence. The estimation algorithm is described in Appendix A. Panel
A in Table 3 displays the minimizing parameter estimate ﬁ for in incomplete information
Bayesian learning model with each of the DeGroot alternatives. We robustly find that
the minimizing parameter value is ﬁ  ‚ m = 0 for every model m. This suggests that, if the
common knowledge parameter ﬁ truly describes the population distribution, essentially
0% of the population is Bayesian and any Bayesian agent believes 0% of the population is
Bayesian. Doing a session level bootstrap, we estimate the standard errors as 0.08, 0.12
and 0.09 respectively across uniform, degree and eigenvector alternatives. This shows that
despite the uncertainty, at best only a very small share of agents could likely be Bayesian.
   Our results then indicate that when we estimate a model of incomplete information
learning with potentially Bayesian agents, the model that best describes the data is one
that is equivalent to having no Bayesians whatsoever and instead describing each agent as
DeGroot. Moreover, the results of Table 2 indicate that that the best fitting such model
is one with uniform weighting.
   To give the Bayesian social learning model another shot at better describing the exper-
imental data, we conduct a second exercise. We consider the case where all agents are
Bayesian, but we relax the common prior assumption. Specifically, we allow for each agent
to be Bayesian, know that she is Bayesian, but be uncertain about whether others are
Bayesian or not. So each agent believes that a ﬁ share of the population is non-Bayesian,
despite the fact that everyone is indeed Bayesian. We then compute the divergence min-
imizing ﬁ for a model where all agents are Bayesian but there is no common knowledge
of Bayesian rationality but instead is a miscentered belief on the distribution of Bayesian
types via heterogenous priors. Here we find the best fitting parameters across all networks
to be ﬁ‚ = 0 for every model m (Panel C of Table 3). Unsurprisingly, however, the stan-
dard errors are larger in this case. By looking at the divergence at the optimal ﬁ, we can
see that drawing individuals from the distribution given by ﬁ m fits the data considerably
better than assuming all agents are Bayesian but incorrectly believing that others could
be DeGroot types (see Panels B versus D).
   To summarize, as illustrated by Figure 9, whether considering common knowledge of
Bayesian rationality or not, the robust best explanation of the data is the simplest model
with ﬁ = 0. Here every agent is DeGroot.

4.3. Learning at the Individual Level. Having looked at the network level divergence,
we turn our attention to individual level divergence. While this does not purely address the
mechanics of the social learning process as a whole, it does allow us to look at individual
learning patterns. Understanding the mechanics of the individual behavior may help us
microfound the social learning process.19
19It is certainly ex-ante possible that agents themselves do not each behave according to a particular model
while the aggregate social group may best be described by such a model.
                      TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                                22




                           ﬁ                                            ﬁ
                      (A) Uniform weighting                 (B) Degree weighting




                                                 ﬁ
                                      (C) Eigenvector weighting

         Figure 9. Fraction of actions unexplained by incomplete information
         model at various ﬁ. We show expected divergences (where we plot the share
         of actions unexplained under ﬁ share Bayesian agents where each agent is
         drawn Bayesian with probability ﬁ and DeGroot with probability 1≠ﬁ). We
         also show divergences when all agents are Bayesians but mistakenly think
         that other agents could be DeGroot with probability 1 ≠ ﬁ.


4.3.1. Complete Information Bayesian Learning. We begin by calculating the individual
level divergence for the DeGroot models and the model where all agents are Bayesian and
commonly know all are Bayesian.20 This is depicted in Figure 10.
   First, uniform weighting systematically outperforms degree weighting (0.0648 versus
0.1029), and degree weighting outperforms eigenvector weighting by a small margin (0.1029
versus 0.1097). Second, it is worth noting how well the all-DeGroot models perform in terms
of predicted individual behavior. Across all three networks, the uniform weighting model


20When an agent faces a tie, they stay with their previous action. We considered a random tie-breaking
alternative as well, which does not substantively change the results.
                   TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                           23




           Figure 10. Fraction of actions unexplained at the individual level

explains approximately 94% of all individual observations. Degree and eigenvector cen-
trality weighting models predict 90% and 89% of all individual observations, respectively.
Finally, the common knowledge of Bayesian rationality model performs rather poorly, pre-
dicting 74% of all individual observations, and consequently, significantly underperforms
at explaining the data relative to all-DeGroot models. Accordingly, Table 4 provides the
hypothesis tests for the non-nested model selection procedure to show that the complete
information Bayesian-learning model can be rejected in favor of DeGroot-learning alterna-
tives.
   We also provide a non-parametric test using just network 3 in our sample. Notice that
in this network peripheral nodes that behave according to the Bayesian-learning model
should follow the action of its parent node in the graph in any period t > 3. This is
because the peripheral nodes’ information sets are dominated by those of the parent node.
Table 5 shows that only 17% of the time when the Bayesian and DeGroot models predict
contradicting guesses do the agents actually take the Bayesian decision. This means that
around 83% of the time, agents are ignoring information dominance and going with a
(weighted) majority of their neighborhood.

4.3.2. Zero Probability Information Sets. When considering Bayesian learning, observe
that there is a possibility that, empirically, Bayesian agents may arrive to an informa-
tion set that has zero probability of occurrence. This is not a conceptual problem when
we study the network-level divergence, because any deviation from the empirical data, ir-
respective of the history, is penalized. However, this is problematic for identification when
we look at the individual level divergence, since the Bayesian learning model is mute when
agents have to condition their inference on zero probability events; any observed action
from then on would be admissible for a Bayesian learning agent.
                   TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                            24

   The degree to which this is a problem is highlighted in Table 6. We show the share of
individuals, the share of sessions, and the share of observations (a given individual in a
given round in a given game) that encounter zero probability information sets. In Panel A,
we look at the zero probability information sets reached under the complete information
common knowledge model of Bayesian learning. We find that about 45% of individuals,
70% of sessions and 40% of observations hit zero probability information sets. In Panels B,
C, and D we observe that these numbers modestly decline when we allow for incomplete
information about others’ types. While this certainly does not eliminate the problem, it
does provide immediate evidence of the potential value of allowing our Bayesian agents to
doubt the Bayes-rationality of her peers.
   One natural way to eliminate the zero probability information set problem entirely is to
introduce disturbances. In section 4.3.4, we explore the possibility of estimating a trembling
hand or quantal response equilibrium (QRE) style version of Bayesian learning in which
we introduce the possibility of making mistakes by all agents. In such a model, individuals
can make mistakes with some probability, and Bayesian agents, knowing the distribution
of these disturbances, integrate over this possibility when updating. We will show that this
approach is computationally infeasible in our context in Proposition 4.2.
   Since guaranteeing full support in this model by introducing trembles creates computa-
tional problems that we will describe below, let us start by discussing what we make of
the zero probability events we encounter. First, we argue that the fact that we repeat-
edly observe agents facing zero probability events – even when there is positive probability
that agents may be behaving in another manner – may be taken as prima facie evidence
supporting the idea that this model of Bayesian learning with incomplete information on
networks fails to explain the experimental data. Second, one could make the objection that
the considered incomplete information Bayesian model, is not sufficiently rich to capture
the characteristics of the data and that, perhaps, one needs a more nuanced model. This
could indeed be the case, but as demonstrated in Proposition 4.2, it would be computation-
ally infeasible to estimate a model generating full support. Third, it might be the case that
we have the right incomplete information Bayesian model, but we lack a theory of what
individuals do once they hit zero probability events. If this is the case we may take two
different approaches: we could be agnostic about the correct off equilibrium beliefs, or we
could consider the case of a richer Bayesian model that rationalizes the actions taken after
an agent hits a zero probability event and precisely matches the supposed off equilibrium
behavior. Such a model, of course, has the degree-of-freedom problem.
   Instead, we take what we believe to be an reasonable approach by considering two
alternatives. In the first case, we penalize each zero probability observation as an error. In
the second case, we only look at observations in the support of observations with positive
probability of occurrence under the Bayesian learning model (meaning we do not penalize
                   TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                          25

the Bayesian model for arriving at zero probability information sets). We report results
for both to paint a complete picture.


4.3.3. Incomplete Information Bayesian Learning. We now turn to studying which incom-
plete information Bayesian learning model best fits the data, taking these constraints into
account. We look at the deviation of each agent’s action, given the history that the agent
has observed at that time, from the predicted action by the model for that agent given the
history. The formalities are developed in Appendix A.
   Table 7 presents the results for the case where we use all rounds of data. We find that
‚u = ﬁ
ﬁ      ‚d = ﬁ‚ e = 0. Further, the standard errors are quite small: 0.014, 0.024, and 0.011,
respectively for uniform, degree and eigenvector weighting. In addition, in Panel B, we see
that at the optimum (as noted in Figure 10) the divergences are small.
   In Table 8, we show results where we constrain ourselves to the support of observations
with positive probability of occurrence under the Bayesian learning model. Here we lose
a tremendous amount of information. In Panel B, we see that the fraction of observa-
tions unexplained rises steeply (almost three-fold) relative to the case where we use all
observations. This means that the DeGroot model is explaining behavior precisely when
the Bayesian model, even with incomplete information, runs into a zero probability infor-
mation set. Nonetheless, we find that the divergence minimizing ﬁ to ﬁ    ‚u = ﬁ‚ d = 0 and
‚ e = 0.9. Unsurprisingly, the standard errors are considerably larger in this case (0.32,
ﬁ
0.28, and 0.31, respectively) and the bootstrapped confidence intervals practically cover
the entire interval. In sum, if we take the view that we must remain agnostic about the
zero probability information set observations, then we do not have enough information at
the individual level to distinguish the model for any ﬁ.
   Finally, as before, we explore what happens if we consider the Bayesian setting but re-
move common knowledge of Bayesian rationality. In this case, while the objective function
minimizing parameter estimates are typically far from 0, the divergences are significantly
higher from those in the incomplete information case with common knowledge of distribu-
tion of types.
   In summary, we have shown that when we consider the data at the individual level,
because even the incomplete information Bayesian model reaches zero probability informa-
tion sets so frequently, when we consider the data inclusive of all observations, the best
explanation is a simple model where all agents are DeGroot. At the same time, the share
of data unexplained rises steeply when we restrict attention to only observations where
we have not encountered a zero probability information set. This suggests that the DeG-
root model precisely explains the data where the Bayesian models are failing. However,
if we take the extreme view to not penalize the Bayesian model at all on zero probability
information sets, then, when conditioning on the history, we do not have the power to
distinguish between any model with any ﬁ.
                       TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                                          26

4.3.4. Bayesian Learning with Disturbances and Complexity Problems. Now we discuss
expanding the support by introducing disturbances. For expositional simplicity, we re-
strict attention to the case of a complete information Bayesian model where each agent is
Bayesian. In this environment, each agent makes a mistake with probability ‘ and chooses
the opposite action that a Bayesian agent would choose. This guarantees full support: any
agent can take any action given any history with positive probability.21
   Introducing disturbances comes at great computational cost in an environment where
agents learn on networks. The only sufficient statistic for the information set that each
agent sees is the information set itself, as there is no deterministic function between signal
endowments and information sets. This means that through time, the relevant state space
(the histories that each agent could have seen) grows exponentially. We show that this
makes the problem intractable for any practical purpose.
   First, we note that the algorithm that we use to simulate the Bayesian learning model
without trembles is computationally “tight” in the sense that, asymptotically in the number
of rounds, there is no faster algorithm.22 Because any algorithm would have to take order T
steps to print output for each of the T periods, an algorithm that is O(T ) is asymptotically
tight.

Proposition 4.1. The algorithm for computing Bayesian learning with no disturbances is
  (T ).23 Moreover, it is asymptotically tight; i.e., any algorithm implementing Bayesian
learning must have running time of at least (T ).
   Specifically, the algorithm is (n4n T ). Notice that if n was growing this algorithm
would be exponential time, but here, n is a constant. Second, we show that the extension
of this algorithm to an environment with disturbances is computationally intractable.

Proposition 4.2. Implementing the Bayesian learning algorithm with disturbances has
computational time complexity of (4nT ).
   In order to illustrate the complexity of these algorithms with trembles, we compare them
to their deterministic counterparts. For the complete information model, the algorithm
with trembles with T = 6 involves 1.19 ◊ 1016 more computations than the deterministic
model. With the same T , the incomplete information model involves 8.65 ◊ 1032 more cal-
culations than its deterministic counterpart. To see how severe the burden is, suppose that
the deterministic complete information model takes 1 second to run. Then the determinis-
tic incomplete information model (again, without trembles) takes 4 and a half hours. The

21Haile et al. (2008) show that QRE imposes no falsifiable restrictions and can rationalize any distribution
of behavior in normal form games. Relating this intuition to our context, one may be able to pick a
distribution of ‘ such that it rationalizes the incomplete information Bayesian model as describing the data
well.
22
   Our environment consists of finite graphs where n does not grow in T .
23Recall that we say f (n) œ (f (n)) if f is asymptotically bounded above and below by f , up to a
                         1           2         1                                                         2
multiplicative constant. Formally, if ÷c1 , c2 > 0, n such that ’n > n, c1 · |f2 (n)| < |f1 (n)| < c2 · |f2 (n)|.
                   TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                             27

trembling hand complete information model, however, takes approximately 377,346,524
years.
   Note that the above exercise looked at the number of computations for a specific algo-
rithm – one that was efficient for the case without disturbances. An objection could be
made that there may be, perhaps, a more efficient algorithm. In Appendix A.5, we discuss
the relationship between our current problem and work on computational complexity in
Bayesian networks, which gives the reader intuition as to why we believe it is likely impos-
sible, if not in the least very difficult, to derive efficient algorithms with trembles in this
context.

4.3.5. Summary. To summarize this section’s results, first we have presented evidence that
the considered model of Bayesian learning result often arrives at zero probability informa-
tion sets. This can be taken as evidence against these particular models. By relaxing the
model to incomplete information, we can recover some, but not all, of these events. Second,
we provide an argument to show that models with trembles, which would smooth out the
zero probability information set problem, are of little practical use to structurally evaluate
empirical data. In turn, methodologically, structural approaches must restrict themselves
to models which allow for zero probability information sets. Third, we take a pass at the
data by ignoring the off-equilibrium information sets. We lose considerable information
in this case. If we penalize the Bayesian model for zero probability events, the result is
particularly strong with robust estimates suggesting ﬁ = 0 irrespective of the DeGroot
alternative. Finally, we show that one cannot better fit the data by adding heterogeneous
priors where all agents are Bayesian but each incorrectly believes that each other may be
DeGroot with probability ﬁ. The results robustly support the results we also found in the
network level analysis: DeGroot models with simple weighting (such as uniform weighting)
provide a good explanation of the data against a large class of alternatives and information
structures, though in this case, eigenvector weighting does nearly as well.

             5. Why a Lab Experiment with Structural Estimation

   In this section, we discuss two reduced form approaches to study the experimental data.
The motivation is the following. Given the computational limits of the structural approach,
we are interested in seeing whether reduced form patterns of Bayesian learning (as opposed
to DeGroot learning) may be obtained from the data. Since larger networks, such as
those found in empirical data sets, do not lend themselves to structural approaches for
computational reasons, it is worth looking into the effectiveness of reduced form approaches
to address these questions.
   The central intuition on which we focus concerns double counting information and was
inspired by the work of Möbius et al. (2015). Under any of the aforementioned Bayesian
models, Bayesian agents should not double-count information. DeGroot agents do double-
count information, however, and it is on this intuition that we build the exercise. The work
                    TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                              28

of Möbius et al. (2015) explores an idea like this to look for tagged information, which is
different from the question of Bayesian versus DeGroot learning. In our setting, tags are
shut down.
   We provide two examples of regressions that researchers may run. The first set of
regressions explores whether individuals overweight the same information if they receive
it through multiple channels. The second set of regressions explores whether individuals
treat old information that cycles back to them as if it is new, additional information.
The naive null in these regressions is Bayesian model, since one would assume that the
relevant parameters ought to be zero. Thus, a rejection of a zero may provide evidence
in the direction of the DeGroot rules of thumb. The empirical data shows that both of
these reduced form analyses seem to provide support in favor of the DeGroot alternatives.
However, because we are able to simulate out the data under the null, we show that these
intuitions are wrong. Specifically, when we simulate social learning data under the Bayesian
null, the coefficients are not as one may have expected. We can also show, available upon
request, that we can generate DeGroot-like reduced form parameters for a number of
network topologies under true Bayesian learning, and vice versa.


5.1. Multiplicity. We define a variable which is a dummy for whether individual i makes a
guess of 1 in the final period T , yi := 1 {ai,T = 1}. As before, di is the degree of individual
i, and Ni is the set of (direct) neighbors Ni = {j œ V : ij œ E}. Note that di = |Ni | .
Moreover, let N2i be the set of second-neighbors of person i; that is, j œ N2i means that
there is at least one path of length two between i and j, but no path of length one. Finally,
we define N2i l to be the set of second neighbors to whom she has exactly l paths.

   The first regression we run is of the form
                                                         ÿ
(5.1)         yi = —0 + —1 si + —2 ENi [sj |j œ Ni ] +       —3l EN l [sj |j œ N2i
                                                                                l
                                                                                   ] + ‘i .
                                                                    2i
                                                         l

This is a regression of whether or not individual i ultimately makes a guess of 1 on whether
the individual’s signal is 1 (si ) the share of ones (ENi [sj |j œ Ni ]) in individual i’s neigh-
borhood, and the share of ones given to each subset of second neighbors to whom i has
exactly l paths (EN l [sj |j œ N2i
                                 l ]).
                     2i
   The interpretation is as follows. —2 measures the impact of her neighborhood receiving a
greater share of ones on an individual’s guess. We expect —2 > 0. Moreover, —3l measures
the impact of the subset of her second-neighborhood with multiplicity l. The intuition is
that as the signals of individuals with greater multiplicity ought not be double-counted
under a Bayesian frame, —3l+1 > —3l would be evidence of overweighting redundant infor-
mation that has arrived via multiple channels, while —3l+1 = —3l would provide evidence in
favor of the Bayesian hypothesis.
   Given the learning model, the network structure, and signal endowment, we simulated
out the learning path and then ran the relevant regressions. We present results when
                    TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                               29

simulating the learning process from the complete information Bayesian model (every agent
is Bayesian) as well as the two most intuitive DeGroot alternatives.
   Table 9 provides the simulation and empirical results. When looking at the empirical
results, we see that, as expected, an individual’s own signal being one and the share
of individuals in one’s neighborhood with signals of one increase the probability of the
final guess being one. However, we can reject that —3l+1 > —3l . While this seems to
be inconsistent with the intuition that agents engage in double-counting, the simulation
exercise shows that these patterns cannot necessarily be interpreted in that manner. First,
columns (3) and (4) indicate that the Bayesian null does not have coefficients that are
near identical across multiplicities 1 and 2. Moreover, columns (5) and (6) show that when
we look at DeGroot learning with degree weighting, it is not the case that —3,2 > —3,1 .
More generally, the increasing correlation with indirect friends of higher multiplicities is
also not uniformly found across the DeGroot models. Ultimately, the regressions suggest
that the linear projection of this learning process is complex and may depend crucially
on the network structure, set of initial signals, and the particular communication channels
involved.

5.2. Historical Information. Another reduced form analysis that one may conduct is
addressing whether individuals re-incorporate historical information that they have pre-
viously observed. Consider an individual at period 3. They have observed both their
own signals and the signals of their direct neighbors (insofar as the first period guesses of
their neighbors will be identical to their signals). In period three, therefore, a Bayesian
individual’s guess should not re-incorporate this information. Instead, it should only up-
date using information about second-neighbors and the like, about whom they have yet to
receive information.
   To examine this formally, we perform the following regression. We regress the period
three guess of individual i on her own signal (si ) and the average signal of her neighborhood
(ENi [sj |j œ Ni ]) which she would have seen in period three. We also include as regressors
the average signal of second neighbors (EN2i [sk |k œ N2i ]) which should be new information
in period three. Lastly, we include the average signal of direct neighbors whose signals
can cycle back via a path of length two back to individual i. Of course, we also include
the agent herself in this set. Formally, we use ECi [sj |j œ Ci ], where Ci = {j œ V ≠ {i} :
A2ij Aij > 0} ﬁ {i} . The regression is as follows.

(5.2)   yi = –0 + –1 si + –2 ENi [sj ] + –3 EN2i [sk |k œ N2i ] + –4 ECi [sj |j œ Ci ] + ‘i .

We test the hypothesis of whether –4 = 0, which is our naive Bayesian null. Notice that
–4 > 0 provides evidence that individuals reincorporate information that they already
knew as it cycles through the network.
   Table 10 presents the simulation and empirical results. When looking at the empirical
results, we see that as expected, an individual’s own signal being one and the share of
                   TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                           30

direct and new indirect neighbors with signals of one increase the probability of the final
guess being one. Also, the empirical results show that the share of repeated indirect
neighbors with signals of one increase the probability of the final guess being one, that is,
–4 > 0 . While this seems to provide suggestive evidence for the intuition that DeGroot
weighting reincorporates old information, the simulation results provide evidence that for
our environment, –4 > 0, even for the Bayesian model.

5.3. Reflection on Reduced Forms. Taken together, Tables 9 and 10 have shown that
natural reduced form approaches to test between these models may be misguided without
first checking whether the patterns by the learning processes actually match the intuitions.
We are able to study the reduced form projections of the Bayesian model using our simula-
tion algorithm and find evidence that, when projected onto a regression for these networks
with this environment, the Bayesian data suggests that the coefficients can deviate greatly
from our intuitions. This, we argue, provides a strong motivation for the structural ap-
proach to studying the models.

                                     6. Conclusions

   In this paper, we have investigated whether social learning patterns on small networks
are better fit by models where individuals construct beliefs in a Bayesian manner or by
models where individuals are myopic and instead follow DeGroot rules of thumb to compute
beliefs. To do so, we developed a simple experiment where we designed networks in order
to distinguish between these models, large enough to give us power on this dimension,
but small enough to ensure that simulating a Bayesian learning on networks model was
not computationally intractable. Given the experimental data, we were able to study the
social learning process as a whole by taking the network as the unit of observation and
studying the behavior of an individual, which addresses whether an agent acts in a Bayesian
manner. Furthermore, we are able to relax both common knowledge of Bayesian rationality,
by allowing agents to be either Bayesian or DeGroot in their learning, modeled as agents
having privately observed (epistemic) types (as in Harsanyi (1967)). Additionally, we relax
the common prior assumptions, allowing all agents to be Bayesian (and therefore know it)
yet remain unsure as to whether others are Bayesian or DeGroot.
   At the network level, we find evidence that the uniform weighting DeGroot model best
explains the data. The Bayesian learning null is rejected in favor of this alternative model.
However, we maintain that Bayesian learning did an adequate job of describing the exper-
imental data, largely owing to the fact that in many circumstances, the Bayesian prescrip-
tion did not differ from the DeGroot prescription.
   At the individual level, we find that uniform weighting DeGroot performs the best, out-
performing the Bayesian model. However, we show that the Bayesian model encounters
the problem that many individuals come across zero probability information set. First,
this provides suggestive evidence of the lack of fit of this incomplete information Bayesian
                   TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                           31

model. Second, we demonstrate that introducing disturbances to smooth out the distri-
bution cannot be a solution in this environment. The computational complexity of the
problem is damaging to the very approach of applying QRE or trembles to the Bayesian
learning on networks environment. As such, we recommend that researchers focus on
computationally tractable models, which will be easier to falsify.
   We also show that reduced form approaches may be problematic. We provide two natural
examples of regressions which build on intuitions separating DeGroot and Bayesian learning
patterns. Equipped with our Bayesian learning algorithm, we simulate learning data from
the Bayesian model, as well as from DeGroot models and show that the reduced form
regression outcomes do not conform to the intuitions.
   Ultimately, the findings suggest that agents and the learning process is rather consistent
with DeGroot action models where individuals myopically weight their neighbors’ actions
when updating their own beliefs rather from a Bayesian model. This may imply that social
learning processes, in contexts where agents observe each others’ actions, empirically may
be sub-optimal, with information often getting stuck in pockets of the network. Having
constructed an example of a network which satisfies asymptotic learning for DeGroot com-
munication models, but where asymptotic learning fails for DeGroot action models, we
argue that in action-learning environments DeGroot processes may be more damaging to
the wisdom of society than previously anticipated.



                                       References

Acemoglu, D., M. A. Dahleh, I. Lobel, and A. Ozdaglar (2010): “Bayesian Learn-
  ing in Social Networks,” .
Anderson, L. and C. Holt (1997): “Information cascades in the laboratory,” The
  American Economic Review, 87, 847–862.
Aumann, R. J. (1976): “Agreeing to Disagree,” The Annals of Statistics, 4, 1236–1239.
Banerjee, A. (1992): “A simple model of herd behavior,” The Quarterly Journal of
  Economics, 797–817.
Banerjee, A., A. G. Chandrasekhar, E. Duflo, and M. Jackson (2013): “The
  Difussion of Microfinance,” Science, 341, 1236498.
Bikhchandani, S., D. Hirshleifer, and I. Welch (1992): “A Theory of Fads, Fashion,
  Custom and Cultural Change as Information Cascades,” Journal of Political Economy,
  100, 992–1026.
Celen, B., S. Kariv, and A. Schotter (2010): “An Experimental Test of Advice and
  Social Learning,” Management Science, 56, 1–15.
Choi, S., D. Gale, and S. Kariv (2005): “Behavioral aspects of learning in social
  networks: An experimental study,” Advances in Applied Microeconomics: A Research
  Annual, 13, 25–61.
                 TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                       32

——— (2009): “Social Learning in Networks: A Quantal Response Equilibrium Analysis
  of Experimental Data,” WP, May.
Conley, T. and C. Udry (2010): “Learning about a new technology: Pineapple in
  Ghana,” The American Economic Review, 100, 35–69.
Cruz, C. (2012): “Social Networks and the Targeting of Illegal Electoral Strategies,” .
DeGroot, M. (1974): “Reaching a consensus,” Journal of the American Statistical Asso-
  ciation, 69, 118–121.
DeMarzo, P., D. Vayanos, and J. Zwiebel (2003): “Persuasion Bias, Social Influence,
  and Unidimensional Opinions*,” Quarterly journal of economics, 118, 909–968.
Ellison, G. and D. Fudenberg (1993): “Rules of thumb for social learning,” Journal
  of Political Economy, 101, 612–643.
——— (1995): “Word-of-mouth communication and social learning,” The Quarterly Jour-
  nal of Economics, 93–125.
Eyster, E. and M. Rabin (2008): “Naive herding,” Department of Economics, London
  School of Economics: London, UK.
——— (2010): “Naive herding in rich-information settings,” American economic journal:
  microeconomics, 2, 221–243.
Gale, D. and S. Kariv (2003): “Bayesian learning in social networks,” Games and
  Economic Behavior, 45, 329–346.
Geanakoplos, J. (1994): “Common Knowledge. chapter 40 of volume 2 of the Handbook
  of Game Theory, edited by R. Aumann and S. Hart,” .
Golub, B. and M. Jackson (2010): “Naive Learning in Social Networks and the Wisdom
  of Crowds,” American Economic Journal: Microeconomics, 2, 112–149.
——— (2012): “How homophily affects learning and diffusion in networks,” Quarterly
  Journal of Economics, 127, 1287–1338.
Haile, P., A. Hortaçsu, and G. Kosenok (2008): “On the empirical content of quantal
  response equilibrium,” The American Economic Review, 98, 180–200.
Harsanyi, J. C. (1967): “Games with Incomplete Information Played by "Bayesian"
  Players, I-III. Part I. The Basic Model,” Management Science, 14, pp. 159–182.
Hung, A. and C. Plott (2001): “Information cascades: Replication and an extension to
  majority rule and conformity-rewarding institutions,” American Economic Review, 91,
  1508–1520.
Jackson, M. O. (2008): Social and Economic Networks, Princeton University Press.
Jackson, M. O., T. Rodriguez-Barraquer, and X. Tan (2012): “Social Capital and
  Social Quilts: Network Patterns of Favor Exchange,” American Economic Review, 102,
  1857–1897.
Jadbabaie, A., S. A. Molavi, Pooya, and A. Tahbaz-Salehi (2012): “Non-Bayesian
  Social Learning,” Games and Economic Behavior, 76, 210–225.
                 TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                    33

Kubler, D. and G. Weizsacker (2004): “Limited depth of reasoning and failure of
  cascade formation in the laboratory,” Review of Economic Studies, 71, 425–441.
Lobel, I. and E. D. Sadler (forthcoming): “Information diffusion in networks through
  social learning,” Theoretical Economics.
Mengel, F. and V. Grimm (2014): “An Experiment on Learning in a Multiple Games
  Environment,” .
Möbius, M., T. Phan, and A. Szeidl (2015): “Treasure Hunt: Social Learning in the
  Field,” .
Mossel, E., A. Sly, and O. Tamuz (Forthcoming): “Asymptotic learning on Bayesian
  social networks,” Probability Theory and Related Fields.
Mossel, E. and O. Tamuz (2010): “Effcient Bayesian Learning in Social Networks with
  Gaussian Estimators,” arXiv:1002.0747.
——— (2014): “Opinion exchange dynamics,” arXiv preprint arXiv:1401.4770.
Mueller-Frank, M. (2014): “"Does one Bayesian Make a Difference?",” Journal of
  Economic Theory, 154, 423–452.
Mueller-Frank, M. and C. Neri (2013): “Social Learning in Networks: Theory and
  Experiments,” .
Osborne, M. and A. Rubinstein (1994): A Course in Game Theory, MIT Press Books.
Rivers and Vuong (2002): “Model Selection Tests for Nonlinear Dynamic Models,”
  Econometrics Journal, 5, 1–39.
Sadler, E. D. (2014): “Bounded Rationality and the Network Economist’s Dilemma,”
  Available at SSRN 2501690.
           TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                              34

                                        Tables


Table 1. Fraction of observations for which the complete information
Bayesian model differs with the DeGroot alternative
                    Panel A: Networks from Choi et al. (2005, 2009)
                                [1]                [2]                    [3]
          Network            Uniform              Degree              Eigenvector
             1                0.00%               0.00%                  0.00%
             2                 0.00%               0.00%                 6.80%
             3                11.34%              22.68%                12.47%


                       Panel B: Networks Selected in This Paper
                                 [1]                [2]                    [3]
          Network            Uniform             Degree               Eigenvector
             1                 6.95%             11.15%                  6.95%
             2                10.04%             10.39%                 10.39%
             3                 9.63%             10.69%                 10.69%


     Notes: Fraction of observations for which the complete information Bayesian
     model differs with the DeGroot alternative. In Panel A, network 1 is the
     “complete network”, network 2 is the “directed network”, and network 3 is the
     “incomplete network” of Choi et al. (2005, 2009). In Panel B, network 3 has
     identical paths for Bayesian and both uniform and degree DeGroot alternatives.
     We chose this network to pit Bayesian against eigenvector weighting.
                   TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                                            35

     Table 2. One-sided tests with the null of Bayesian learning against each
     alternative of the DeGroot model for the network-level analysis

                                                  [1]                 [2]                [3]                  [4]
         H0                   H1            All networks          Network 1          Network 2            Network 3
     Bayesian              Uniform           0.0000***            0.0000***          0.0000***            0.0001***
     Bayesian           Eigenvector          0.0000***            0.0000***          0.0001***             0.0151**
     Bayesian               Degree           0.0000***            0.0000***          0.0001***             0.0151**
     Uniform                Degree           0.9999***            0.9999***           0.9482*             0.9999***
     Uniform            Eigenvector          0.9999***                 -              0.9458*             0.9999***
      Degree            Eigenvector          0.0000***            0.0000***               -                    -
Notes: The test statistic is the normalized difference in the divergence function of the null and the alternative model.
We show the probability that the test statistic is less than zero, estimated via bootstrap with replacement at the
session level.
              TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                              36

  Table 3. Incomplete information parameter estimates and corresponding
  divergences for the network-level analysis


                     Panel A: Incomplete information point estimates
                                    [1]                  [2]                    [3]
                                      u
                                    π                    πd                     πe
      Point Estimate               0.00                 0.00                   0.00
      Standard Error              (0.08)               (0.12)                 (0.09)

                    Panel B: Incomplete information expected divergence
                                     [1]                   [2]                  [3]
        Divergence                 0.1198                0.1413               0.1343
      Standard Errors             (0.0096)              (0.0102)             (0.0099)

           Panel C: Incomplete information point estimates, no common knowledge
                                      [1]                    [2]               [3]
                                      πu                     πd                πe
       Point Estimate                0.00                   0.00              0.00
      Standard Errors              (0.2834)              (0.2620)           (0.3139)

              Panel D: Incomplete information divergence, no common knowledge
                                       [1]                   [2]                 [3]
          Divergence                 0.1798                0.1721              0.1831
       Standard Errors              (0.0107)              (0.0106)            (0.0105)
Notes: Columns 1, 2 and 3 refer to the alternative models of uniform, degree and eigenvector
centrality weighting, respectively. Standard errors are constructed by bootstrapping with
replacement at the session level.
                   TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                                   37

       Table 4. One-sided tests with the null of Bayesian learning against each
       alternative of the DeGroot model for the individual-level analysis

                                                  [1]                 [2]                [3]                  [4]
         H0                   H1            All networks          Network 1          Network 2            Network 3
     Bayesian              Uniform           0.0000***            0.0000***          0.0000***            0.0000***
     Bayesian           Eigenvector          0.0000***            0.0000***          0.0000***            0.0000***
     Bayesian               Degree           0.0000***            0.0000***          0.0000***            0.0000***
     Uniform                Degree           0.9999***            0.9999***           0.9757**            0.9999***
     Uniform            Eigenvector          0.9999***            0.9999***          0.9999***            0.9999***
      Degree            Eigenvector             0.8074              0.2460           0.9990***              0.6451
Notes: The test statistic is the normalized difference in the divergence function of the null and the alternative model.
We show the probability that the test statistic is less than zero, estimated via bootstrap with replacement at the
session level. When comparing DeGroot alternatives we use all observations, since there is no notion of zero
probability information sets. When comparing complete information Bayesian to a DeGroot alternative, we only use
histories on which a Bayesian agent has not reached a zero probability information set.
TESTING MODELS OF SOCIAL LEARNING ON NETWORKS            38

         Table 5. Information dominance

                       [1]                 [2]
     H1           Observations      Percent Bayesian
Uniform                98                17.35%
Degree                 46                17.39%
Eigenvector            98                17.35%
Notes: Observations are the number of cases where
there are discrepancies between the parent node action
(which, from t > 3 has the property that under the
Bayesian model the peripheral node should follow the
parent node) and the action that the alternative model
prescribes for the peripheral node. The percent
Bayesian presents the share of observations in which
the peripheral nodes followed the Bayesian node
action.
            TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                       39

      Table 6. Cases that encounter zero probability information sets


                            Panel A: Complete Information
                              [1]                   [2]                      [3]
      Network             % Individuals          % Sessions             % Observations
         1                  47.97%                72.63%                   43.90%
         2                  54.29%                73.68%                   46.15%
         3                  33.06%                64.29%                   34.66%


                        Panel B: Uniform Weighting Alternative
                               [1]                  [2]                      [3]
      Network            % Individuals          % Sessions              % Observations
         1                  35.79%                72.63%                   37.29%
         2                  37.14%                68.42%                   39.85%
         3                  26.94%                64.29%                   31.45%

                         Panel C: Degree Weighting Alternative
                               [1]                   [2]                     [3]
      Network            % Individuals           % Sessions             % Observations
         1                  34.14%                 67.37%                  37.02%
         2                  38.05%                 70.53%                  40.70%
         3                  25.92%                 62.86%                  30.89%

                       Panel D: Eigenvector Weighting Alternative
                               [1]                    [2]                    [3]
      Network             % Individuals          % Sessions             % Observations
         1                   36.09%                72.63%                  38.00%
         2                   38.05%                70.53%                  40.70%
         3                   25.92%                62.86%                  30.89%


Notes: All percentages computed using the π that minimizes the share of data units
(individuals, sessions or observations) that reach zero probability information sets.
                 TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                                     40

    Table 7. Incomplete information parameter estimates and corresponding
    divergences for the individual-level analysis on entire support


                      Panel A: Incomplete information point estimates
                                      [1]                 [2]                       [3]
                                        u
                                      π                   πd                        πe
       Point Estimate                0.00                0.00                      0.00
       Standard Error              (0.0140)            (0.0243)                  (0.0109)

                   Panel B: Incomplete information expected divergence
                                    [1]                   [2]                       [3]
        Divergence                0.0648                0.1029                    0.1097
      Standard Errors            (0.0064)              (0.0075)                  (0.0061)

                 Panel C: Incomplete information point estimates, no prior
                                    [1]                    [2]                      [3]
                                     πu                    πd                       πe
      Point Estimate             [0.4,0.5]               0.00                      0.00
 Confidence Interval (95%)        [0,0.8]               [0,0.6]                   [0,0.8]

                Panel D: Incomplete information divergence, no common prior
                                       [1]                    [2]                    [3]
          Divergence                 0.2656                 0.2554                 0.2672
       Standard Errors              (0.0142)               (0.0154)               (0.0142)
Notes: Columns 1, 2 and 3 refer to the alternative models of uniform, degree and eigenvector
centrality weighting, respectively. Standard errors are constructed by bootstrapping with
replacement at the session level over all histories where zero probability information sets are
penalized. We present confidence intervals when the parameter is set identified instead of point
identified.
               TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                                    41

  Table 8. Incomplete information parameter estimates and corresponding
  divergences for the individual-level analysis on Bayes-feasible histories


                      Panel A: Incomplete information point estimates
                                     [1]                  [2]                        [3]
                                       u
                                     π                    πd                         πe
       Point Estimate               0.00                 0.00                       0.90
       Standard Error             (0.3214)             (0.2765)                   (0.3102)

                    Panel B: Incomplete information expected divergence
                                     [1]                   [2]                       [3]
        Divergence                 0.0685                0.1068                    0.1177
      Standard Errors             (0.0155)              (0.0145)                  (0.0151)

             Panel C: Incomplete information point estimates, no common prior
                                     [1]                   [2]                 [3]
                                     πu                    πd                  πe
      Point Estimate               [0.9,1]                0.90                0.90
 Confidence Interval (95%)          [0,1]                 [0,1]               [0,1]

                 Panel D: Incomplete information divergence, no common prior
                                         [1]                    [2]                   [3]
          Divergence                  0.2750                  0.2762                0.2769
       Standard Errors               (0.0148)                (0.0148)              (0.0152)
Notes: Columns 1, 2 and 3 refer to the alternative models of uniform, degree and eigenvector
centrality weighting, respectively. Standard errors are constructed by bootstrapping with
replacement at the session level over all histories where any Bayesian agent has not yet reached a
zero probability information set. Therefore, zero probability information sets are not penalized.
We present confidence intervals when the parameter is set identified instead of point identified.
                        TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                                                      42

          Table 9. Overweighting information received through multiple channels


                              Data                            Bayesian                          Degree                          Uniform
                      All            Restricted        All          Restricted         All           Restricted         All           Restricted
                      [1]               [2]            [3]             [4]             [5]              [6]             [7]               [8]
Signal             .4974***          .5351***       .3965***        .4429***        .5725***         .6204***        .6017***          .677***
                    [.0218]           [.0279]        [.017]           [.0223]        [.0169]          [.0211]         [.0159]          [.0201]
Direct             .7735***          .7485***       .775***          .8048***       .9125***         .8913***        .8954***         .9078***
                    [.0311]           [.0438]        [.0323]          [.0384]        [.0335]             [.0425]      [.0381]             [.0475]
One Way            .3157***          .2609***       .7056***         .8189***        0.0461              -0.0614     .1306***              .077*
                    [.0405]           [.0542]        [.0363]          [.0486]          [.035]            [.0421]      [.0368]          [.0447]
Two Ways           .2177***          .1751***       .3582***         .4096***         .0797*              0.024      .1985***         .1841***
               [.0355]                [.0424]        [.0337]             [.0338]     [.0421]             [.0449]      [.0459]             [.0519]
TwoWays > OneWay
t-statistic      -1.97                -1.557         -8.653              -8.136       0.7675              1.925        1.481               2.215
Probability     0.9741                0.9386            1                   1         0.2224             0.0287        0.071              0.0146
N                1750                  1271           1750                1271         1750               1271         1750                1271
R -squared      0.4724                 0.373         0.5322              0.4632        0.588             0.5453       0.5974              0.5571
Note: Robust standard errors, clustered at the village by game level, in brackets. Ouctome variable is action in round 3. "Direct" is the average
signal of direct neighbors, "One Way" is the average signal of indirect neighbors only thorugh one direct neighbor, and "Two Ways" is the average
signal of indirect neighbors thorugh two direct neighbors. Column (1) is the regression with all data. Column (2) is the regression restricting to
treatments that are informative for the comparisons Bayesian - Degree and Bayesian - Uniform. * p<.1, ** p<.05, *** p<.01
                         TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                                                       43

         Table 10. Re-incorporation of previously-observed historical information


                                 Data                            Bayesian                          Degree                           Uniform
                         All            Restricted        All          Restricted          All          Restricted          All           Restricted
                         [1]               [2]            [3]             [4]              [5]             [6]              [7]              [8]
Signal                .4284***          .4573***       .4729***         .5264***        .4507***            .482***     .5694***          .6468***
                       [.0347]           [.0425]        [.0286]          [.0356]          [.032]         [.0359]         [.0307]           [.0357]
Direct                .7508***          .7011***       .8095***         .8007***        .8436***        .8167***        .7576***          .7275***
                       [.0363]           [.0471]        [.0285]             [.0346]      [.0402]            [.0489]       [.0391]             [.0492]
Indirect New          .2455***          .1803***       .3768***         .3489***        .1558***            .0883**     .2548***          .2063***
                       [.0331]           [.0377]        [.0261]             [.0317]      [.0333]            [.0384]       [.0283]             [.0338]
Indirect Repeated     .1715***          .1669***       .1029***             .0801*      .2463***        .2619***        .1724***          .1554***
                       [.0416]            [.048]        [.0354]             [.0427]      [.0398]         [.0447]         [.0417]           [.0492]
N                       1587               1250          1587                1250         1587            1250            1587              1250
R -squared             0.4628            0.3958         0.4953              0.4135       0.5819             0.5475        0.5945              0.5687
Note: Robust standard errors, clustered at the village by game level, in brackets. Ouctome variable is action in round 3. "Direct" is the average
signal of direct neighbors, "Indirect New" is the average signal of indirect neighbors that provide new information, and "Indirect Repeated" is the
average signal of indirect neighbors that do not provide new information. Column (1) is the regression with all data. Column (2) is the regression
restricting to treatments that are informative for the comparisons Bayesian - Degree and Bayesian - Uniform. * p<.1, ** p<.05, *** p<.01
                    TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                                                   44

  Appendix A. Bayesian Learning Algorithm in Complete and Incomplete
                                Information Models

  In this appendix, we describe the algorithm for computing the actions under the as-
sumption of complete information Bayesian agents.

A.1. Setup. We follow the notation on Osborne and Rubinstein (1994) and Geanakoplos
(1994), modeling agents’ information in the experiment by means of dynamically consis-
tent models of action and knowledge (DCMAK), a natural multi-period generalization of
Aumann (1976). Following Geanakoplos (1994), a DCMAK consists of a set of states of the
world Ê œ , information functions Pi,t : æ 2 , and action functions ai,t : æ {0, 1}.
In what follows, we will define these objects for our experimental setup, which we will use
to calculate the predicted behavior of Bayesian agents ai,t (Ê).

A.2. States of the world. In both the complete and incomplete information models, we
model agents information as partitions over Ê œ , where Ê = (Ê1 , Ê2 . . . . , Ên ) is the vector
of agents’ initial private information. In the complete information case, Êi œ {0, 1} denotes
whether they observe a yellow ball (Êi = 1) or blue (Êi = 0), ; i.e.       = S = {0, 1}n . In
the incomplete information case (where players don’t know if their neighbors are Bayesian
or DeGroot types) we model the state of the world as Êi = (si , ÷i ) where si œ {0, 1} is the
color of the observed ball, and ÷i œ {0, 1} denotes agent iÕ s type: she is either a Bayesian
type (÷i = 1) who guesses the most likely state following Bayes’ rule, or a DeGroot agent
(÷i = 0) who decides her guess based on a weighted average of her neighbors’ guesses (we
consider the 3 variants mentioned: uniform, degree and eigenvector centrality weighting).
   On both complete and incomplete information models, Bayesian agents have a common
prior belief over states Ê œ , conditional on the realization of ◊ œ {0, 1} (i.e., which bag
has been chosen), which we denote by ﬂ (Ê | ◊). In the complete information model Ê = s
and all agents are of the Bayesian type ÷i = 1 at all states, and hence

                                              q                           q
                                                    sj
(A.1)        ﬂ (Ê | ◊) = ﬂ (s | ◊) := p◊                 (1 ≠ p◊ )                     for ◊ œ {0, 1} , Ê œ
                                                j                   n≠        j
                                                                                  sj


where p◊ = P (si = 1 | ◊). In our experiment, p◊ = 5/7 if ◊ = 1 and p◊ = 2/7 if ◊ = 0. In the
incomplete information model, Ê = (s, ÷) where ÷ = (÷1 , ÷2 , . . . , ÷n ) is a type profile (the
complete description of every agent’s type), and we assume that agents have a common prior
over agents’ types as well, which are assumed to be i.i.d. across agents and independent of
observed signals;
                                 q                              q         5 q                          q        6
                                         sj
(A.2)           ﬂ (s, ÷ | ◊) := p◊            (1 ≠ p◊ )                                     (1 ≠ ﬁ)
                                     j                     n≠        sj                ÷j         n≠       ÷j
                                                                j         ﬁ        j                   j



where ﬁ := P (÷i = 1). The set of all type configurations is denoted by H = {0, 1}n , and
in this model, := S ◊ H = {0, 1}n ◊ {0, 1}n .
                         TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                                      45

A.3. Recursive definition of information and action functions. The function Pi,t (Ê) ™
   denotes the information set of agent i at round t, under state Ê. At round t = 1, agent
i only observes Êi out of state Ê, and hence, her information set is:
                                                          )                         *
(A.3)                                    Pi,1 (Ê) := Ê Õ œ             : ÊiÕ = Êi

i.e., the possible states of the world are those compatible with the private information she
has received (which includes her signal si œ {0, 1} and her type). Based on this information,
all agents choose to match their signal; i.e.,

(A.4)                                                ai,1 (Ê) := si

   For t > 1 we compute Pi,t (Ê) and ai,t (Ê) inductively, for each Ê œ . In our experi-
mental setup, at round t agent i observes all the actions taken by her neighbors j œ N (i)
(including herself) up to s = t ≠ 1. Therefore, the states of the world that are consistent
with agent i’s observations (her information set) are
                         )                                    !    "                                    *
(A.5)     Pi,t (Ê) := Ê Õ œ          : ÊiÕ = Êi and aj,s Ê Õ = aj,s (Ê) for all j œ N (i) , s Æ t ≠ 1

Clearly, we have Pi,t (Ê) ™ Pi,t≠1 (Ê) for all i, Ê œ (i.e., Pi,t (·) corresponds to a filtra-
tion24). The round t action function ai,t (Ê) is then given by:
                                           Y Ó             Ô
                                           ]1 Ii,t (Ê) > 1                 if Ii,t (Ê) ”=    1
                                                         2                                   2
(A.6)                          ai,t (Ê) :=
                                                                                             1
                                           [a     (Ê)
                                                  i,t≠1                    if Ii,t (Ê) =     2

where Ii,t (Ê) is the “belief index” at state Ê, which depends on the agents’ type. If agent
i is Bayesian (i.e., under the complete information model, or if ·i = 1 in the incomplete
one) then Ii,t (Ê) := P (◊ = 1 | Pi,t (Ê)), which is calculated using Bayes rule conditioning
on the event B = Pi,t (Ê):

                                                              q
                                                                  Ê Õ œPi,t (Ê) ﬂ (Ê    | ◊ = 1)
(A.7)              P (◊ = 1 | Pi,t (Ê)) := q
                                                    Ê Õ œPi,t (Ê) [ﬂ (Ê   | ◊ = 1) + ﬂ (Ê | ◊ = 0)]
                                                                  q
When i is not Bayesian at Ê, then Ii,t (Ê) := nj=1 Tij aj,t≠1 (Ê) where [Tij ]ij are the De-
Groot weights (which are T u , T d or T e as in equations 2.1,2.2, and 2.3, depending on the
assumed alternative type).

A.4. Numerical Implementation. The algorithm used is based on the inductive step
defined above, calculating iteratively the objects Pi,t (Ê) and ai,t (Ê) for all i, t and Ê, for
both the complete and incomplete information models.

Algorithm 1. Bayesian Learning Algorithm (Complete Information case)
  Input: An n≠person network G = (V, E) with adjacency matrix An◊n
24We can also define P recursively, starting at P
                      i,t                        i,1 as in A.3, and for t Ø 1 let Pi,t (Ê) := Pi,t≠1 (Ê) ﬂ
{Ê Õ œ   : aj,t≠1 (Ê Õ ) = aj,t≠1 (Ê Õ ) for all j œ N (i)}
                    TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                              46

  Outputs: Information and action functions Pi,t (Ê) and ai,t (Ê)
  Step 1: Initialize algorithm by defining:
   (1) State space = {0, 1}n
   (2) Measures ﬂ (Ê | ◊) according to A.2, for ◊ œ {0, 1}
   (3) Information functions Pi,1 (Ê) and actions ai,1 (Ê) according to A.3 and A.4 for all
       i = 1, . . . n and Ê œ .
Step t > 1: Given (Pi,s (Ê) , ai,s (Ê))i=1,...n,s=1,...t≠1,Êœ calculate Pi,t (Ê) and ai,t (Ê) for
all i and Ê œ according to A.5 and A.6, where Ii,t (Ê) = P (◊ = 1 | Pi,t (Ê)) as in equation
A.7



Algorithm 2. Bayesian Learning Algorithm (Incomplete Information case)
  Inputs:
   (1) An n≠person network G = (V, E) with adjacency matrix An◊n
   (2) A row stochastic matrix of DeGroot weights Tn◊n
   (3) Probability ﬁ œ [0, 1]
Output: Information and action functions Pi,t (Ê) and ai,t (Ê)
 Step 1: Initialize algorithm by defining:
   (1) State space = S ◊ H = {Ê = (s, ÷) where s œ S := {0, 1}n , ÷ œ H := {0, 1}n }
   (2) Measures ﬂ (Ê | ◊) = ﬂ (s, · | ◊) according to A.2, for ◊ œ {0, 1}
   (3) Information functions Pi,1 (Ê) and actions ai,1 (Ê) according to A.3 and A.4 for all
       i = 1, . . . n and Ê œ .
Step t > 1: Given (Pi,s (Ê) , ai,s (Ê))i=1,...n,s=1,...t≠1,Êœ calculate Pi,t (Ê) and ai,t (Ê) for
all i and Ê œ according to A.5 and A.6, where Ii,t (Ê) = P (◊ = 1 | Pi,t (Ê)) if ÷i = 1 and
          q
Ii,t (Ê) = j Tij aj,t (Ê) if ÷i = 0.

   It is worth noting that an alternative way of modeling the knowledge structure is by
including the true state ◊ in the description of the state of the world; i.e., define Ê = (◊, s)
in the complete information case, and Ê = (◊, s, ÷) in the incomplete information case,
which would need the definition of just one common prior ﬂ (Ê), instead of having to define
it conditional on ◊. While this would perhaps be a better fit for most epistemic models,
the description of the algorithm is slightly easier in our model, given the fact that Ê = s
in the complete information model, and Ê = (s, ÷) in the incomplete information models
are, respectively, sufficient statistics for the actions sequence of players, since ◊ is never in
any information set of any of the players, significantly reducing the relevant state space (in
fact, these are the minimal state spaces we can consider, exactly because of sufficiency).

A.5. Complexity. The decision problem we are interested in is determining whether an
agent i in time period t given a history always picks the same action under a proposed
                      TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                                         47

algorithm as under the Bayesian model with trembles. We conjecture that the problem is
NP-hard, which we are investigating in ongoing work. This means that the computational
problem is at least as hard as NP-complete problems.25 Whether there may or may not
be polynomial time solutions for NP-hard problems is open; if P ”= NP, then none would
exist. The computer science literature studying Bayesian learning networks shows that
obtaining the probabilities is NP-hard (Cooper, 1990) in any given network of events. In
this context, the networks are networks of events. Translating our framework into this
setup involves constructing a network of belief states for each individual in the network
and each time period, so a node in the Bayesian learning network would be a pair (i, t),
so the size of it would be N · T . Our ongoing work seeks to extend their argument to our
decision problem which involves checking that the action taking by each person in each time
period is identical when comparing a proposed algorithm with the true Bayesian learning
model. The intuition is that the associated learning network is growing exponentially in
the number of periods and individuals, and therefore, for any algorithm there can be some
action sequence such that to be able to decide whether individual i at time t, given the
history, needs to decide whether to guess 0 or 1, one needs all the probabilities. Based
on Cooper (1990), which applies to a broader class of networks (and therefore will have
weakly worse complexity), we conjecture that the argument for our sub-class of networks
will also be NP-hard.
   Let t be the the set of states that agent i has to integrate over at time t. The basic
algorithm (in this general version) involves two states: the indicator function of the set
Pi (Ê) for each Ê œ t and the action function ai,t (Ê). We define
                                                    Y
                                     !              ]1    if Ê Õ œ Pi,t (Ê)
                                             Õ"
                                  ‡t i, Ê, Ê :=
                                                    [0    otherwise
and

                                           –t (i, Ê, t) := ai,t (Ê)
to calculate the objects Pi,t (Ê) and ai,t (Ê) numerically, as in appendix A.4. To calculate
them, we then have to loop across # ( t ) ◊ # ( t ) states for each (i, t) to update ‡t to ‡t+1
                                                              q q !q           !    q         ""
and # ( t ) to update –t . The number of operations is then t i          wœWt k +     ŵœWt k
where k is the number of computations done in each step. In the deterministic complete
information model (without trembles), t = S = {0, 1}n and then

                                  Computations = nT (2n ) (1 + 2n ) k



25A problem is said to be NP-complete if (a) it is in NP, which is to say that a given solution can be verified
in polynomial time, and (b) it is NP-hard, so that any NP problem can be converted to this problem in
polynomial time.
                      TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                                             48

  In the incomplete information model, it is nT (4n ) (1 + 4n ) k. The ratio between the
complete and incomplete information models is then
                                   nT (4n ) (1 + 4n ) k      1 + 4n
                                                        = 2n        ¥ 4n
                                   nT (2 ) (1 + 2 ) k
                                        n         n          1 + 2n
   So, for a network of n = 7, the relative complexity of the incomplete information model
approximately 16, 258.
   The trembling hand complete information model needs agents to integrate over 2n(t≠1)
states, at least, in each round; since there is no longer a deterministic mapping between
information sets and signal profiles, agent i needs to integrate over the actions of other
agents. Although agent i actually does not observe the information of n ≠ di agents, for
rounds t Ø 3 we have to have to calculate her beliefs about those agents’ information
sets. The partitional model presented in appendix A.4 does not suffer this problem, by
calculating beliefs on all states, which we do here as well. Therefore # (Wt ) = 2n(t≠1) and
the number of calculations is
                                                         A ≠1                                      B
                   t≠1
                  ÿÿ               1            2       ÿ Tÿ                       ≠1
                                                                                  Tÿ
              k            2n(t≠1) k + 2n(t≠1) = k                    2n(t≠1) +         22n(t≠1)
                   i t=1                                    i   t=1               t=1
              CA              B                D        1             2
                   2nT ≠ 2n         22nT ≠ 22n    2nT ≠ 2n 3                   4
                                                                     n 2 ≠1
          ÿ                                                    1      2 n
      k                           +            =n            1+ 2 ≠2
                                                                 nT

          i
                    2n ≠ 1            2n ≠ 1       2n ≠ 1              22n ≠ 1
Therefore, the ratio between the two is approximately



                                         Ë              È
                              n2n(T ≠1) k + 2(T ≠1)
                                              ¥ T ◊ 22n(T ≠1)≠2n
                           nT (2n ) (k + 2n )
and for the incomplete information model, the equivalent ratio is
                                                    1                 2
                                         n4n(T ≠1) 1 + 4n(T ≠1)
                                                                          .
                                             nT (4n ) (1 + 4n )

                                       Appendix B. Filtering

   Here we describe a filter to estimate, in the incomplete information model, the probability
that an agent is Bayesian given the data that the researcher has in hand, under the network
level approach. The objective of this section is to estimate

                     Fi := P (i is Bayesian | ﬁ, experimental data in room r)

To simplify the exposition, we drop the subscripts for the particular room r, the alternative
model used, m and the prior probability of players being Bayesian, ﬁ œ [0,   Ó 1]. ÔLet [k] =
{1, 2, . . . , k} for any k œ N, T = Tr be the number of rounds played, and aobsi,t                    iœ[n],tœ[T ]
                    TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                                                          49

denote the (panel) data set of player choices, which include the signals drawn at t = 1
(since sobs
        i -© ai,1 ). Given a -type profile ÷ œ H, define the observation level divergence
              obs
                1       2
            -                       -
Di,t (÷) := -ai,t sobs , ÷ ≠ aobs
                              i,t - for all i œ [n] and t > 1. The room level divergence of type
                                                     q
profile ÷ is defined as D (÷) := n(T1≠1) i,t>1 Di,t (÷).
   In the network level approach, we treat the data generating process as follows: a
state Ê œ       = S ◊ H is drawn at t = 1, and agents play according to the equilib-
rium path sequence ai,t (Ê). The researcher knows the actual signal endowment s = sobs
(i.e., the color of the balls drawnÓ in the
                                          Ô experiment), and the panel of actions chosen by
all agents, which we denote by ai,t   obs         . Formally, the information set of the re-
                                                     iœ[n],tœ[T ]

       Ó P ™ , restricts the set of states to those consistent with
searcher,  R
                                                                  Ô the observed data; i.e.
P := Ê = (s, ÷) œ S ◊ H : ai,t (Ê) = ai,t for all i œ [n] , t Æ T . Therefore, the only re-
  R                                       obs

maining uncertainty is the type profile ÷ œ H, and the researcher information can be better
described as restrictions over possible type profiles, denoted by
                                    Ó                    1             2                                               Ô
          H R := projH P R = ÷ œ H : ai,t sobs , ÷ = aobs
                                                      i,t for all i œ [n] , t œ [T ]

In this appendix, we are particularly interested in calculating the posterior probability that
agent i is Bayesian, given a prior probability ﬁ;
                                                         1             Ó         Ô    2
                                    Fi (ﬁ) := P ÷i = 1 | aobs
                                                          i,t ,ﬁ

   If there exists some type profile ÷ ú that is consistent with the observed data (i.e. H R ”=
ÿ), then this can be easily calculated using Bayes’ rule:
                                                                           q                    1              2
                                    1                              2           ÷œH R :÷i =1 P       ÷ | sobs
                 Fi (ﬁ) = P ÷i = 1 | ÷ œ H R =                                 q
                                                                                  ÷œH R   P (÷ | sobs )
                                q
                                    ÷œH R :÷i =1 (ﬁ/1          ≠ ﬁ)n÷
                            =       q                                      .
                                        ÷œH R    (ﬁ/1 ≠ ﬁ)n÷
                    q                        q                             1       2n÷                             q
   Since P (÷) = ﬁ j j (1 ≠ ﬁ)        j j = (1 ≠ ﬁ)n              , where ÷ := n1 j ÷j . While
                        ÷               n≠       ÷        ﬁ
                                                         1≠ﬁ
tractable, this exercise is impossible in most of our experiments: as Table 6 shows, in at
least 63% of experiments, no type profile ÷ ú œ H exist that could rationalize the observed
data (i.e H R = ÿ) regardless of network topology chosen and the alternative DeGroot
action type for the incomplete information model.
   If we maintain the network level approach interpretation, an alternative is to add “trem-
bles” to ensure full support over possible action sequences {ai,t }i,t . Formally, let ai,t œ {0, 1}
be the (random) choice of agent i atÓ round   Ô t . The key object1we need    2 to model is the
probability of observing the dataset ai,t obs    if the state is Ê = s , ÷obs
                                                             i,t
                                1                                                               1         22
                 A (÷) := P ai,t = aobs
                                    i,t for all i œ [n] , t œ [T ]|Ê = s
                                                                        obs
                                                                            ,÷
                         TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                                                     50

so A (÷) is the likelihood function of type profile ÷ (treated as a parameter) given the
observed data. Given A (÷), we can calculate Fi by first getting the posterior distribution
over type profiles, using Bayes’ rule
                                          3                  Ô 4
                                                   Ó                           P (÷) A (÷)
                                      P ÷ | aobs                       =q
                                                                              ÷œH P (÷) A (÷)
                                             i,t
                                                                 i,t

and then calculating the probability of ÷i = 1 under it;
                                          3                  Ó         Ô 4         ÿ         3        Ó     Ô 4
                     Fi := P ÷i = 1 | aobs
                                       i,t                                   =             P ÷ | aobs
                                                                                                  i,t
                                                                       i,t                                       i,t
                                                                                 ÷:÷i =1
                                      q
                                          ÷:÷i =1 (ﬁ/1           ≠ ﬁ)n÷ A (÷)
                              =           q                                        .
                                              ÷   (ﬁ/1 ≠ ﬁ)n÷ A (÷)
  Our leading example of a model of trembles will be the independent trembles model,
parametrized by a constant ‘ œ (0, 1). Formally, given 1a type 2profile ÷ œ H, we assume
(1) ai,1 = sobs
             i   with probability 1 for all i (2) ai,t = ai,t sobs , ÷ with probability (1 ≠ ‘),
which is i.i.d across players and rounds. Formally,
                                              q                                             q
                                                            Di,t (÷)                                  Di,t (÷)
                              A (÷) := ‘                               (1 ≠ ‘)
                                                   i,t>1
                                                                                 n(T ≠1)≠     i,t>1

                     -                    1            2-
                     -                                  -
where Di,t (÷) := -aobs
                    i,t ≠ ai,t s
                                obs , ÷
                                        - is the theoretical observation divergence under state
    1        2
Ê = sobs , ÷ . It can be expressed in a more familiar way as
                         Ë                              È                                         3         4n(T ≠1)D(÷)
                                                  1≠D(÷) n(T ≠1)                                       ‘
            A (÷) = ‘        D(÷)
                                    (1 ≠ ‘)                              = (1 ≠ ‘)      n(T ≠1)
                                                                                                      1≠‘
A nice feature is that the posterior distribution over type profiles is directly related to its
measured divergence:
                                  3       Ó         Ô 4                        3           4n(T ≠1)D(÷)
                                                                                    ‘
                               P ÷ | aobs                        Ã P (÷) ◊
                                      i,t
                                                       i,t                         1≠‘
and hence
                                                                 3           4n÷ 3           4n(T ≠1)D(÷)
                                           1      ÿ       ﬁ                             ‘
(B.1)                        Fi =
                                          (ﬁ, ‘) ÷:÷ =1 1 ≠ ﬁ                          1≠‘
                                                        i

                     q
where (ﬁ, ‘) := ÷ (ﬁ/1 ≠ ﬁ)n÷ (‘/1 ≠ ‘)n(T ≠1)D(÷) is proportional to the probability of
the researcher’s information set over type profiles H R26. One may observe that type profiles
that do not explain the data well (if D (÷) is high) are given a lesser weight in the posterior
distribution if ‘ < 12 , so that the filtered probability of i being Bayesian depends crucially
on how much is this evidenced by the data.




            !
26Namely, P H R =
                 "                    #                      $n
                             (ﬁ, ‘) ◊ (1 ≠ ﬁ) ‘T ≠1
                   TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                                       51

             Appendix C. Stuck Nodes and Complete Binary Quilts

  This section expands on subsection 2.4, and describes how to get the learning bounds
presented.

C.1. Stuck Nodes. Given an undirected graph G = (V, E) and a subset of nodes v ™ V we
define Gv =(v, Ev ) as the induced subgraph for subset v, where Ev = {(ij) œ E : {i, j} ™ v}.
Given a subgraph Gv , let di (Gv ) be the degree of node i in subgraph Gv . Let ait œ {0, 1}
be the action that node i œ V takes at round    Ó t œq N, which we willÔ assume follows the
uniform DeGroot action model; i.e., ai,t = 1 di +1 jœNi aj,t≠1 > 12 . We allow for any
                                                   1

                           1     q
tie-breaking rule when   di +1    jœNi   aj,t≠1 = 12 .

Lemma C.1. Take a subset of individuals v ™ V such that there exists h œ N with

                            h Æ di (Gv ) Æ di < 2h + 1 for all i œ v

If agents behave according to the uniform weighting DeGroot action model, and at some
T œ N we have ai,T = a œ {0, 1} for all i œ v, then ai,t = a for all t Ø T .

Proof. The proof is by induction: without loss of generality, suppose ai,T = 1 for all i œ v.
Of course, for t = T the result is trivially true. Suppose now that ai,t = 1 for all i œ v and
                                                                    q
t Ø T , and we need to show that ai,t+1 = 1 too. Let Ii,t+1 = di1+1 jœNi aj,t be the index
of uniform weighting. We then know that Ii,t Ø 12 for all nodes in v, and it suffices to show
that this implies Ii,t+1 Ø 12 . Observe:
                                     q                   q
                                     jœvﬂNi     aj,t +     jœNi ≠v   aj,t
                  q                            ¸˚˙˝                                    q
                    jœNi aj,t                   =1                              h + 1 + jœNi ≠v aj,t
      Ii,t+1 =                =                                             Ø
                    di + 1                        di + 1                    (i)        di + 1
                  h+1         1
              Ø           > .
                  di + 1 (ii) 2
We have used in inequality (i) that di (Gv ) Ø h and aj,t = 1 for all j œ v. Inequality (ii)
comes from the fact that
                               h+1      1
                                     > ≈∆ di < 2h + 1.
                              di + 1    2
                                  1
Therefore, we have that Ii,t+1 > 2 for any i œ v, implying then that ai,t+1 = 0, as we
wanted to show.                                                                           ⇤

  This lemma indicates that whenever we find a subset of nodes v such that each node has
more connections to nodes in v than it has outside v, then whenever they reach consensus,
they would remain there forever. We present an useful corollary of Lemma C.1, which we
will use when studying the family Qd .

Corollary C.1 (Regular Subgraphs). Take a family of nodes v œ V such that there exists
k œ N such that
                   TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                           52

                                    Od(1)                            Od(2)




                         C1    C2        C3      C4             Ca             Cb




                      Figure C.1. Subgraphs Od (k) of Qd (d = 3).

   (1) Gv is a k-regular graph
   (2) di < 2k + 1 for all i œ v.
Then, if agents behave according to the uniform weighting DeGroot action model, and at
some T œ N ai,T = a œ {0, 1} for all i œ v, then ai,t = a for all t Ø T .

Proof. Simply take h = k and apply Lemma C.1.                                              ⇤

   See that any triangle in Qd is a 2≠ regular subgraph, and that each node in it has
di = 4 < 2 · 2 + 1 = 5. Applying Corollary C.1 with k = 2, we see that, whenever a triangle
achieves consensus, it remains there forever.

C.2. Stuckness in Complete Binary Quilts. We define Sd = {i œ Vd : i gets stuck}
and let Nd = # (Vd ), the number of nodes in Qd . Our object of interest is the random
variable
                                                                  #Sd
                   Fd = Fraction of nodes in Qd that gets stuck ©     .
                                                                  Nd
Our objective is to get an asymptotic bound on Fd . Since we do not yet know whether Fd
has a limit for almost every realization, we define F and Fas

(C.1)                         F = lim inf Fd and F = lim supFd
                                        dæŒ                          dæŒ

which is well defined for all realizations of the sequence Fd , and so it is a well defined
random variable. Namely, we want to get the tightest asymptotic lower and upper bounds
for the fraction of stuck nodes. Our
                                  Ó objective is to get
                                                     Ô a number F œ [0, 1] such that F Ø F
and F Æ F almost surely; i.e., P F Æ F Æ F Æ F = 1.
   Define
                                                         s=k
                                                          €
                                             Od (k) :=         Ld≠s
                                                         s=0
as the subgraph formed by the last k + 1 levels of Qd . This subgraph is disconnected, with
a lot of connected components, as pictured in Figure C.1. Let C be a generic connected
component of Od (k); i.e., the induced subgraph of one of the connected components of
Od (k). For example, Panels A and B of Figure 3 correspond to connected components of
Od (2). Notice that in this example, each component of Od (2) is itself a complete binary
quilt of level 2. It is easy to see that in fact, the connected components of Od (k) are also
                   TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                                                53


complete binary quilts; i.e., C = Q̂k (binary quilts of depth dÕ = k). The key property
of these subgraphs (and the reason of studying this family of networks) is that the only
connection between each component C and the rest of the graph is the parent node of the
component C, denoted by iC . We will see how this property allows us to study the local
behavior of the component, abstracting away from the rest of the network, in order to get
bounds on the fraction of players stuck.
   Define
                                           # {Od (k) ﬂ Sd }
                                   d (k) =
                                             # {Od (k)}
as the fraction of stuck nodes in Od (k),

                                                      # {Od (k) ﬂ Sd }
                                    (k) := lim inf                     ,
                                             dæŒ        # {Od (k)}
and
                                            # {Or (k) ﬂ Sr }
                                    (k) := lim sup           ,
                                      ræŒ      # {Or (k)}
which are also all well-defined random variables. These are the tightest asymptotic lower
and upper bounds on the fraction of nodes in Od (k) that get stuck.
                                             #{Od (k)}         2k+1 ≠1
Lemma C.2. For all k œ N, limd≠æŒ              Nd          =    2k+1
                                                                       .




Proof. Let Ld = number of nodes in level r. Because of Qd being a complete binary
network, we have the following recursion for Ld :

                                      Ld+1 = 2Ld and L0 = 1

from which it follows that

(C.2)                                           Ld = 2d .

Therefore, the number of nodes in # {Od (k)} is then
                                                                     A              B        A              B
                       k
                       ÿ                        k
                                                ÿ                        1 ≠ 2k+1                2k+1 ≠ 1
        # {Od (k)} =         # {Ld≠s } = 2d≠k
                                                      2 =2
                                                       s       d≠k
                                                                                        =2
                                                                                         d

                       s=0                      s=0
                                                                           1≠2                      2k
We also need to calculate Nd := # (Vd ). Again, because of how Qd grows, it can be also
easily shown that

(C.3)                                        Nd = 2d+1 ≠ 1

Now, we can state the result. Observe that
                                                 A                 B
                   # {Od (k)}     2d                  2k+1 ≠ 1                 2k+1 ≠ 1
                              = d+1                                    ædæŒ
                      Nd       2    ≠1                   2k                       2k
as we wanted to show.                                                                                           ⇤
                     TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                        54

  The following proposition is the key to understanding how to get bounds on F and F
by getting bounds on (k) and (k)

Proposition C.1. Suppose there exist functions Â, Â : N æ[0, 1] such that for all k we
have
                   Â (k) Æ (k) Æ (k) Æ Â (k) almost surely.
Then, for all k œ N almost surely,
                                            2k+1 ≠ 1
(C.4)                                FØ              Â (k)
                                              2k+1
and
                                       A           B
                                           2k+1 ≠ 1 Ë          È
(C.5)                          F Æ1≠                  1 ≠ Â (k)  .
                                             2k+1
Proof. Lets focus only on inequality C.4, since C.5 follows the same reasoning. See that

                                      3                      4
                            # {Od (k)} # {Od (k) ﬂ Sd }     # {Sd ≠ Od (k)}
                   Fd =                                   +
                               Nd        # {Od (k)}               Nd
                                      3                 4
                            # {Od (k)} # {Od (k) ﬂ Sd }
                     Ø
                               Nd        # {Od (k)}
so, for all realizations,
                                3                   43           3               44
                                     # {Od (k)}               # {Od (k) ﬂ Sd }
          F   = lim inf Fd Ø lim                   lim inf
                dæŒ           d≠æŒ       Nd       dæŒ           # {Od (k)}
                3                  4
                        # {Od (k)}           2k+1 ≠1
              =    lim                 (k) =            (k) .
                  r≠æŒ      Nd                 2k+1

This, together with the fact that     (k) Ø Â (k) almost surely, completes the proof.     ⇤

   Note that this proposition is true for any learning model (Bayesian or DeGroot). The
learning model plays a role when calculating the bounds Â and Â. See that condition C.4
and C.5 are bounds on F and F, which do not depend on k. Therefore, these are bounds
for all k. Moreover, the higher k, the tighter the bound we get.

C.3. Bounding stuck nodes in the Uniform Weighting model. Without loss of
generality, we will assume that the true state of nature is ◊ = 1, which implies that as d æ
Œ the fraction of nodes with true signals is p > 12 . The idea is simple: take a component
C = (VC , EC ) µ Od (k). As we mentioned before, the only connection between C and the
rest of the graph is through the parent node iC (as seen in Figure C.1). Let SC = {0, 1}NC
be the set of signal endowments for nodes in C. We look for a lower bound Â k (s) for each
signal endowment realization in C such that, when signal endowment is s, the fraction of
stuck nodes in C is larger that Âk (s) fraction of stuck nodes in C if the endowment is s Ø
Â k (s). If we can find such Â k (s), then we can use a law of large numbers to argue that
                   TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                          55


                                                  Ó      Ô
                        (k) Ø Â (k) © EsœSC Â k (s)          almost surely
because the realizations of s in each component C is independent of each other. Likewise,
if we can find a function Â k (s) to bound from above the fraction of stuck nodes,
                                                  Ó      Ô
                        (k) Ø Â (k) © EsœSC Â k (s)          almost surely

   Imagine first that the signal endowment of the upper triangle in C is (0, 0, 0). Then,
using Lemma C.1 we know that the upper triangle of C will get stuck from period t = 1 on,
and we can get the expected value of stuck nodes in C from there on. See that the fraction
of nodes that get stuck in this component is only a function of the realization of s œ SC ,
which is independent of the realization of the signal endowment of other components on
Od (k)
   When the signal endowment of the upper triangle in C is different from (0, 0, 0) , we
make use of the other property we know from C : the only connection to the rest of the
graph is through iC ,the uppermost node in C. Therefore, a way of getting a lower bound
on the number of nodes that get stuck, is assuming that from round t = 2 on, node iC
knows the truth, and plays aiC ,t = 1 for all t Ø 2. Intuitively, we are making the graph to
have the biggest effect possible in convincing nodes in C that ◊ = 1, which can only do by
making aiC ,t = 1 for all rounds other than t Ø 2. Once we have that, we can simulate the
learning model on C, and calculate Âk (s) and Â k (s).
                                              Ó        Ô
  There are two ways of calculating EsœSC Â k (s) :
   (1) Doing it explicitly: This can be done for k = 2 and k = 3, because # {SC (k = 4)} =
       128.
   (2) Monte-Carlo: As k goes bigger, it is computationally unfeasible to calculate the
       expected value of Â k (s) explicitly, since

                          # {SC } = 22            = O (exp (exp (k)))
                                         k+1 ≠1



       which grows super-exponentially. Ó
                                        However,
                                               Ô we can simulate random draws of s œ
       SC and get an estimate for EsœSC Â k (s) using law of large numbers.
The bounds presented in Figure 4 were calculated using the explicit approach in (1),
replicating the idea behind Panel A of Figure 3 for all possible signal configurations for
components in Od (3) (i.e., which components are complete binary quilts of depth k = 3).
Following Lemma C.2, as d æ Œ, nodes in Od (3) account for approximately 23+1 ≠
1/23+1 = 93.75% of all nodes, so we are looking at most nodes by just looking at the last
3 levels.

                                  Appendix D. Proofs

Proof of Proposition 2.1. The first part follows from Mossel et al. (Forthcoming). The
second part follows from Golub and Jackson (2010), since every node has degree 2 or 4,
                  TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                                    56


their Corollary 1 applies. Namely, max1ÆiÆn qdid æ 0 along our sequence and therefore
                                                   i
the social learning process is wise. The third part follows from Lemma C.2.        ⇤




Proof of Proposition 4.2. Let g(n, T ) be the number of calculations for the original
model (with no trembles). It is given by
                            t=T
                            ÿ
                g(n, t) =         n(22n + 2n+1 ) = nT (4n + 2n+1 ) =              (nT 4n ).
                            t=1

Meanwhile, let f (n, T ) be the number of calculations that need to be done for a network
of size n and played for T rounds.

                              T
                                                                A T             T
                                                                                           B
                              ÿ                                  ÿ              ÿ
              f (n, T ) =           n2   nt+1
                                                (1 + 2 ) = 2n
                                                     nt
                                                                       2
                                                                       nt
                                                                            +         4
                                                                                      nt

                              t=1                                t=1            t=1
                                   C                                        D
                            2n(T +1) ≠ 2n 4n(T +1) ≠ 4n                               1        2
                       = 2n              +              =                                 n4nT .
                               2n ≠ 1        4n ≠ 1
Thus, the complexity ratio between the model with trembles and the model with no trem-
bles is                       Ë n(T +1) n                È
                               2            4n(T +1) ≠4n
               f (n, T )   2n     2n ≠1
                                       ≠2
                                          +    4n ≠1
                                                             3
                                                               1 n(T ≠1)
                                                                         4
                         =                                 =     4
               g (n, T )          nT (4n + 2n+1 )              T
which completes the proof.                                                          ⇤
            TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                                         57

               Appendix E. Online Appendix: Figures




                    1
                    .9
                  Power
                    .8
                    .7
                    .6




                          .02         .04            .06              .08             .1
                                                   Divergence



 Panel A: Distinguishing between Bayesian and DeGroot uniform weighting
                    1
                    .9
                  Power
                    .8
                    .7
                    .6




                          .02   .04          .06                .08              .1         .12
                                                   Divergence



  Panel B: Distinguishing between Bayesian and DeGroot degree weighting
                    1
                    .9
                  Power
                    .8
                    .7
                    .6




                          .02   .04         .06             .08             .1        .12
                                                   Divergence



Panel C: Distinguishing between Bayesian and DeGroot eigenvector weighting

   Figure E.1. Divergence versus power frontier for network 1 selection
            TESTING MODELS OF SOCIAL LEARNING ON NETWORKS                                         58




                    1
                    .9
                  Power
                    .8
                    .7
                    .6
                          .02         .04            .06              .08             .1
                                                   Divergence



 Panel A: Distinguishing between Bayesian and DeGroot uniform weighting
                    1
                    .9
                  Power
                    .8
                    .7
                    .6




                          .02   .04          .06                .08              .1         .12
                                                   Divergence



  Panel B: Distinguishing between Bayesian and DeGroot degree weighting
                    1
                    .9
                  Power
                    .8
                    .7
                    .6




                          .02   .04         .06             .08             .1        .12
                                                   Divergence



Panel C: Distinguishing between Bayesian and DeGroot eigenvector weighting

   Figure E.2. Divergence versus power frontier for network 2 selection
