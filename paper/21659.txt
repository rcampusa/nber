                                NBER WORKING PAPER SERIES




                         EMPLOYMENT AND TRAINING PROGRAMS

                                           Burt S. Barnow
                                            Jeffrey Smith

                                        Working Paper 21659
                                http://www.nber.org/papers/w21659


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                     October 2015




The views expressed herein are those of the authors and do not necessarily reflect the views of the
National Bureau of Economic Research. Disclaimer on potential conflicts of interest: Both authors
have undertaken contract research sponsored by the U.S. Department of Labor as well as serving on
numerous technical advisory panels and providing comments on draft reports for various evaluations.
In particular, in regard to evaluations considered in detail in this chapter, both authors were part of
the NORC subcontract for the non-experimental component of the National JTPA Study, both authors
reviewed drafts and provided technical advice on the recent TAA evaluation, and both are members
of the technical advisory panel for the WIA experiment. It should, but does not, go without saying
that the views expressed represent those of the authors alone.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2015 by Burt S. Barnow and Jeffrey Smith. All rights reserved. Short sections of text, not to exceed
two paragraphs, may be quoted without explicit permission provided that full credit, including © notice,
is given to the source.
Employment and Training Programs
Burt S. Barnow and Jeffrey Smith
NBER Working Paper No. 21659
October 2015
JEL No. H11,I28,J24

                                               ABSTRACT

This chapter considers means-tested employment and training programs in the United States. We focus
in particular on large, means-tested federal programs, including the Job Training Partnership Act (JTPA),
its successor the Workforce Investment Act (WIA), that program’s recent replacement, the Workforce
Innovation and Opportunity Act (WIOA), the long-running Job Corps program, and the Trade Adjustment
Assistance (TAA) program. The first part of the chapter provides details on program history, organization,
expenditures, eligibility rules, services, and participant characteristics. In the second part of the chapter,
we discuss the applied econometric methods typically used to evaluate these programs, which in the
United States means primarily social experiments and methods such as matching that rely on an assumption
of “selection on observed variables.” The third part of the chapter reviews the literature evaluating
these programs, highlighting both methodological and substantive lessons learned as well as open
questions. The fourth part of the chapter considers what lessons the evaluation literature provides on
program operation, especially how to best allocate particular services to particular participants. The
final section concludes with the big picture lessons from this literature and discussion of promising
directions for future research.


Burt S. Barnow
George Washington University
805 21st St NW, Room 601T
Washington, DC 20052
barnow@gwu.edu

Jeffrey Smith
Department of Economics
University of Michigan
238 Lorch Hall
611 Tappan Street
Ann Arbor, MI 48109-1220
and NBER
econjeff@umich.edu
Introduction

The United States has used employment and training programs as a policy tool during two

periods, first during the Great Depression, when work relief and other employment-related

programs were used, and since 1961, when a broad array of employment and training programs

has been implemented. This chapter focuses on employment and training programs implemented

in the latter period, with emphasis on current means-tested programs and developments since

2000.1

         Although people often think of employment and training programs as synonymous with

vocational classroom training, workforce programs actually use a variety of approaches. Indeed,

in an analysis of data from the Workforce Investment Act (WIA) from Program Years 2002 to

2005 (July 1, 2002 through June 30, 2005), Trutko and Barnow (2007) found that less than half

(46.6 percent) of adults who exited the program received classroom training, with individual

states ranging from 14 percent to 96 percent of participants receiving classroom training.

         Both Butler and Hobbie (1976) and Perry et al. (1975) classify employment and training

programs into four broad categories: (1) skill development programs, which increase vocational

skills through classroom or on-the-job training; (2) job development programs, which consist of

public employment programs where jobs are specifically created for the participants; (3)

employability development programs, which, according to Butler and Hobbie (1976) emphasize

personal attitudes and attributes needed for employment (i.e. what we would now call “soft

skills”); and (4) work experience programs, which provide employment experiences intended to

help workers gain the same attitudes and attributes as employability development programs

through paid or unpaid work.


1
  For a discussion of programs during the Great Depression, see Kesselman (1978). Developments from 1961
through 2000 are discussed in more detail in LaLonde (2003).

                                                      2
         This classification system covers programs intended to increase human capital and create

new jobs, although individual programs may have other goals as well. What is missing are

categories that include programs intended to provide a better match between workers and jobs

(sometimes called labor exchange programs) and programs that provide job seekers with more

information about themselves (through counseling and assessment) and the jobs that are

available (labor market information or LMI).2, 3

         This chapter focuses on means-tested employment and training programs in the United

States. As such we generally exclude programs that do not do means testing. This set includes

vocational education programs (though we do briefly discuss Pell grants, which many students

use to attend vocational education, later in the chapter).4 We also exclude (1) the Unemployment

Insurance (UI) program; (2) the Employment Service (ES) funded by the Wagner-Peyser Act; (3)

the Worker Profiling and Reemployment Services (WPRS) and Reemployment and Eligibility

Assessment (REA) programs for UI claimants; (4) registered apprenticeship programs; and (5)

vocational rehabilitation and the Ticket to Work program, both of which provide services aimed

at returning people with disabilities to productive employment.

         To avoid duplication, we do not cover programs surveyed in other chapters, such as the

earned income tax credit (Chapter 2) and the welfare-to-work programs associated with

Temporary Assistance for Needy Families or TANF (Chapter 7). In addition, we exclude (1)
2
  LaLonde (2003) includes labor exchange programs as well as counseling and assessment and LMI in the job
development category.
3
  Barnow and Nightingale (2007) take a broad view of policies that affect the labor force, and they include several
other program categories: insurance and cash payments (e.g., unemployment insurance, disability insurance, and
workers’ compensation), regulations and mandates (e.g., minimum wage, living wage, occupational safety and
health, and discrimination statutes and executive orders), tax incentives and credits (e.g., lifelong earning credit,
earned income tax credit, and economic development programs such as empowerment zones and enterprise zones
that offer place-based tax credits), and social and support services and payments (e.g., need-based stipends,
transportation assistance, and subsidized or paid child care). Alternatively, labor market programs are sometimes
divided into active programs, which impose requirements on those who benefit such as job search requirements, and
passive programs, which provide cash or in-kind assistance with no requirements for the recipients.
4
  Vocational education can be classified as education rather than training, but many courses offered by community
colleges as vocational education also enroll individuals in employment and training programs in the same course.

                                                          3
programs that operate through mandates to employers, such as the minimum wage and civil

rights legislation; (2) place-based programs, such as economic development programs,

empowerment zones, and enterprise zones because they do not directly serve individual workers;

and (3) programs for in-school youth. Finally, among the programs that remain (there are a lot of

programs!) we devote the bulk of our attention to large (in persons served or budget or both)

programs operated by the federal government for which credible impact evaluations exist. In that

sense, we look where the light is, but there are a lot of keys there too.

       The remainder of the chapter is organized as follows. We begin in the next section by

briefly laying out the case for government involvement in the area of employment and training

programs. Existing theory provides a case for the standard interventions, though the case would

benefit from a stronger empirical foundation. Following that, we provide a history of U.S.

workforce development programs and then describe the current array of means tested programs.

In our view, some but by no means all of the wide diversity of existing programs can be justified

based on the need to target specific interventions to specific populations.

       We then describe the key issues involved in evaluating employment and training

programs and how the U.S. literature has addressed them, as a prelude to our discussion of

results from recent evaluations of major U.S. employment and training programs. The U.S.

literature largely relies on occasional social experiments and more frequent analyses that attempt

to solve the problem of non-random selection into programs or into particular services via

conditioning on observed participant characteristics, particularly past labor market outcomes.

The existing evidence makes it clear that some programs (in particular, the adult funding stream

of the Workforce Investment Act program) have positive impacts on labor market outcomes

sufficient to justify their costs, while many others do not. Explaining the differences in impacts



                                                  4
among programs (and between funding streams within programs) remains an important topic for

future research. In addition, we argue that the literature should shift its focus somewhat from

research that estimates the impacts of program participation to research on how to better operate

existing programs.

       The final section summarizes and offers some suggestions for future work and

institutional change. We emphasize the potential for generating additional policy-relevant

knowledge via (1) improvements in the quality of administrative data as well as more intensive

use of the administrative data already available and (2) “designing in” credible evaluation

designs such as regression discontinuity following the (very) successful example provided by the

education policy field.



Justifications for government involvement

This section considers what, if any, substantive economic justification underlies the types of

means-tested employment and training programs currently operated by the U.S. government and

considered in this chapter. Employment and training programs, sometimes referred to (by us and

by others) as workforce development programs, clearly do not meet the usual definition of public

goods as they are both excludable and rivalrous, so other explanations must be invoked.

       One straightforward view sees employment and training programs as what Richard

Musgrave (1959) termed “merit goods,” a good that although not meeting the criteria of a public

good, is so highly valued by society that it is provided publicly. Education is the most common

example of a merit good, and as noted above, the line between occupational training and

education is fuzzy. Musgrave and Musgrave (1973, p. 81) expand on the concept of merit goods,

noting that merit goods might be provided either as the imposition of the preferences of the



                                                 5
ruling elite on the poor or as a means of correcting market imperfections such as imperfect

information, externalities, and interpersonal utility preferences.5

         Another potential rationale for government intervention in the provision of employment

and training services is market failure due to imperfect access to capital, especially among the

poor. In some situations, training programs might be offered to achieve equity when some

segment of the population is harmed by unforeseen market events or by specific government

interventions. Programs targeted on workers who lose their jobs due to technical change, changes

in consumer preferences, or changes in trade patterns exemplify the general case, while programs

targeted on workers who lose their jobs due to trade agreements exemplify the second category.

         Finally, imperfect information on the current and future labor market, particularly on the

wages associated with occupations requiring training, could lead workers to systematically

underinvest in training or to invest in the wrong types of training. Government may have a

comparative advantage in collecting both labor market information and information about

training providers, as it can amortize the fixed costs of doing so over many individuals. These

arguments rationalize both the collection of the information and its distribution via caseworkers

and websites.

         Some of the rationales for government intervention in the employment and training field

call for means testing, but others do not. If, for example, it is the poor who mostly experience

challenges with access to capital and information, then it makes sense to have means testing for

such programs. However, to the extent the public (or ruling elite in Musgrave’s terminology)

views employment and training programs as a merit good that should be provided to all, then

means testing would not be required.


5
  In the latter cases, the provision of such goods does correspond to the traditional definition of a market failure. For
a discussion of the rationale for in-kind redistribution to increase social welfare, see Garfinkel (1973).

                                                            6
          The existence of a rationale for government involvement in workforce development does

not imply that most (or even very much) training should be financed by the government or that

the government should directly provide some (or even any) training or other services. In regard

to the first point, Mikelson and Nightingale (2004) conclude that private sector spending on

training may be 10 times as great as public sector contributions, and other researchers have

developed larger estimates. In regard to the second point, we detail below how the major U.S.

federal programs contract out to other providers (some of them other units of government, such

as community colleges) much of their service provision.




History of U.S. employment and training programs


Workforce programs in the United States began in the 1930s with several efforts to deal with the

high unemployment associated with the Great Depression. During the Great Depression eight

major work relief and public works programs were initiated.6 Under President Hoover, the

Reconstruction Finance Administration provided loans to state and local governments for

welfare and public employment. Although this function was in effect for less than one year, $300

million in nominal dollars was spent on work relief, and at its peak nearly two million people

were employed through the program. When Franklin Roosevelt assumed the presidency, a

number of public employment programs were enacted. All these programs ended by 1943, when

the unemployment rate dropped to 1.9 percent and there was no need for large-scale employment

creation programs. Public employment programs largely vanished until the 1970s.




6
    The discussion of programs during the Great depression is based on Kesselman (1978).


                                                         7
        After discussing the Wagner-Peyser Act, which began during the Great Depression, this

section focuses on the flagship U.S. Department of Labor programs beginning in the 1960s. We

then describe other U.S. Department of Labor programs as well as significant programs operated

by other federal agencies.7

        The Wagner-Peyser Act

        The Wagner-Peyser Act of 1933 established the Employment Service (ES, but sometimes

called the Job Service in some states), the longest continuously operating workforce program in

the United States.8 The ES is open to all job seekers and employers, so it is not a means-tested

program. The ES focuses on providing a variety of employment-related labor exchange services

including but not limited to job search assistance, job referral, placement assistance for job

seekers, reemployment services to unemployment insurance claimants, and recruitment services

to employers with job openings. Services are delivered in one of three modes: self-service,

facilitated self-help, and staff-assisted. Depending on the budget available and needs of the labor

market, other services such as assessment of skill levels, abilities, and aptitudes; career guidance;

job search workshops; and referral to training may be available.9 The services offered to

employers, in addition to referral of job seekers to available job openings, include assistance in

development of job order requirements; matching job seeker experience, skills and other

attributes with job requirements; assisting employers with special recruitment needs; arranging

for job fairs; helping employers analyze hard-to-fill job orders; assisting with job restructuring;




7
  For reviews of the employment and training programs between 1961 and 1973, see Clague and Kramer (1976),
Perry et al. (1975), Barnow (1993), King (1999), and O’Leary, Straits, and Wandner (2004).
8
  See Eberts and Holzer (2004) for a review of the ES history.
9
  In many states the ES administers other programs and services at the direction of the governor. For example, in
many states the ES administers the Trade Adjustment Assistance (TAA) program and the Supplemental Nutrition
Assistance Program (SNAP), formerly known as food stamps.

                                                         8
and helping employers deal with layoffs.10 With enactment of the Workforce Investment Act in

1998, the ES was named as a mandated partner in the One-Stop delivery system.

        The Area Redevelopment Act

        No major federal employment and training programs emerged following the Great

Depression until the 1960s.11 In 1961, the Area Redevelopment Act (ARA) was passed to

stimulate growth in areas with high unemployment by providing loans, financial assistance, and

technical assistance to firms developing new products, and training for workers who would be

employed by firms that expanded or relocated. The ARA was never a large program, and training

enrollments ranged from 8,600 in 1962 to a high of about 12,000 before the program ended in

1965.

        The Manpower Development and Training Act

        The Manpower Development and Training Act of 1962 (MDTA) was the first federal

program to provide training on a larger scale. The original intent of MDTA was to retrain

workers who lost their jobs due to “automation,” the term used at that time for technical change.

From the beginning, the program also served disadvantaged workers, and services to the

economically disadvantaged soon predominated, as job losses due to automation failed to

materialize. A total of approximately 1.9 million workers enrolled in MDTA between 1963 and

1972, with about two-thirds of the participants enrolled in classroom training and one-third

enrolled in on-the-job training (OJT, which is informal training by employers who receive

subsidies of up to 50 percent of wages for up to six months).12




10
   The description of the Employment Service is from http://www.doleta.gov/programs/wagner_peyser.cfm,
accessed on November 1, 2014.
11
   This discussion is based largely on Barnow (1993).
12
   See Mangum (1968).

                                                      9
         Administration of MDTA was complex. The original legislation called for states to

eventually pay for half the program, but these requirements were postponed and diluted, and

eventually states were only required to make a 10 percent match that could be an “in-kind”

contribution. Administration of the OJT component of the program was eventually shifted from

the U.S. Department of Labor to the Job Opportunities in the Business Sector (JOBS) program,

which was operated by the National Alliance of Business, a nonprofit business trade association.

The institutional classroom training was largely administered by the U.S. Department of Labor,

with relatively minor roles played by state and local governments.

         Although MDTA was by far the largest employment and training program in the 1960s

and early 1970s, there were many other workforce development programs in operation. Table 8.1

describes the major programs that operated during this period. Barnow (1993) describes 10

programs that operated during this period, and Perry et al. (1975) provide detailed information

about most of the programs as well as evidence on their effectiveness. Franklin and Ripley

(1984, p. 7) note the consequences of having so many programs available with similar intent:

“The need for coordination among manpower programs and agencies serving the poor became

increasingly apparent during the 1960s. A fearsome degree of fragmentation and rococo

complexity resulted from the large number of separate programs, each with its own target

groups, application procedures, funding cycles, and delivery mechanisms.”13

         The only U.S. Department of Labor program providing training still in operation from the

1960s is the Job Corps, a primarily residential program for disadvantaged youth that we describe

in more detail below.

13
  It is not clear what the optimal number of employment and training programs is, and there is still debate about
whether there are “too many” programs. We discuss this issue later in the chapter, but note here that to a large extent
there are many programs because they serve different populations or provide a different mix of services. Interesting
research opportunities exist on the issue of how many programs there should be and how they should be
differentiated in mission and targeting.

                                                          10
       The Comprehensive Employment and Training Act of 1973

       In 1973, MDTA was replaced by the Comprehensive Employment and Training Act

(CETA). The new program included a change in the mix of activities offered and in the

responsibilities of different levels of government. President Nixon was a strong advocate for the

“New Federalism,” which sought to give state and local governments more control over who was

served and how they were served. The CETA program represented a major departure from

MDTA in several ways. First, decisions about who would be served and how they would be

served were primarily made at the local level rather than at the federal or state level; in fact,

CETA was the high point for local authority compared to the MDTA program that preceded it

and the Job Training Partnership Act (JTPA), Workforce Investment Act (WIA), and Workforce

Innovation and Opportunity Act (WIOA) programs that succeeded it.

       Under CETA Title I prior to the 1978 amendments, funds were distributed by formula to

cities, counties, or consortia of local governments. Any jurisdiction with a population of 100,000

or more was entitled to be recognized as a “prime sponsor.” Areas that were ineligible for

designation on their own and failed to join a consortium were included in a “balance of state”

prime sponsor that was administered by the state. Prime sponsors were required to submit an

annual plan to the U.S. Department of Labor for approval, and they were also required to

establish a planning council with representatives of various constituencies, including the private

sector. Prime sponsors had significant latitude in determining their mix of activities and

participants under Title I; activities available included classroom and on-the-job training, public

service employment, and work experience.

       A concern under MDTA that persists to the present day is that states and local programs

would engage in “creaming” or “cream skimming” by selecting as participants those among the



                                                  11
eligible applicants most likely to do well after participation whether or not the program helps

them (Perry et al., 1975, p. 151; Mangum, 1968, p. 169; Mirengoff and Rindler, 1978, p. 176).

Several features of CETA were designed to mitigate this issue. First, categorical programs were

established for groups with severe barriers—Indians and Native Americans, and migrant and

seasonal farmwokers—and these categorical programs remain part of the program mix today.

Additionally, prime sponsors were required to make assurances in their annual plans that they

would serve those “most in need,” including “low-income persons of limited English-speaking

ability.” The original CETA statute included a public service employment program in Title II,

and 1974 legislation added a countercyclical public service employment program as Title VI.

Over time, the public service employment components grew to be the largest part of CETA.

       In 1977, the Youth Employment and Demonstration Projects Act (YEDPA) created two

new categorical youth programs for the prime sponsors to administer, the Youth Employment

and Training Program (YETP) and the Youth Community Conservation and Improvement

Projects (YCCIP); YETP provided training and work experience, primarily for in-school

disadvantaged youth, and YCCIP provided training and work experience primarily for out-of-

school disadvantaged youth. The legislation also established a large demonstration program, the

Youth Incentive Entitlement Program (YIEPP), to test the feasibility and impact of guaranteeing

part-time school-year and full-time summer jobs to disadvantaged youth to encourage them to

remain in school. A year later, the Young Adult Conservation Corps (YACC) was added to

provide participating youth a conservation experience. Although over $1 billion was spent on the

YEDPA programs, only a few of the programs were rigorously evaluated. A National Academy

of Sciences review of the evidence on the YEDPA programs concluded that “…despite the

magnitude of resources devoted to the objectives of research and demonstration, there is little



                                                12
reliable information on the effectiveness of the programs in solving youth employment

problems” (Betsey, Hollister, and Papageorgiou, 1985, p. 22).14

        Several other national programs were added to CETA during this period, including the

Skill Training Improvement Program (STIP), which was one of the first U.S. initiatives to offer

long-term training to dislocated workers through competitively funded projects; Help through

Industry Retraining and Employment (HIRE), which provided training to veterans though the

National Alliance of Business initially and later through prime sponsors; and the Private Sector

Initiative Program (PSIP), which provided training in conjunction with the newly established

private industry councils affiliated with the CETA prime sponsors. HIRE and PSIP were early

efforts to try to more effectively involve the private sector in federally sponsored training

programs, an effort whose goals have yet to be fully achieved.

        By 1976, concern had increased that the public service employment (PSE) slots were

allowing local governments to substitute federal funds for state and local funds to support

positions, a phenomenon known as fiscal substitution.15 As a result, several modifications were

made to PSE Title VI requirements. PSE positions that became vacant could only be used in

special projects that lasted for 12 or fewer months. In addition, individuals hired for new Title VI

positions and half the Title VI positions that became vacant were required to be individuals

unemployed for at least 15 weeks and a member of a low-income family.




14
   The director of the YEDPA program was more optimistic about what was learned from the experience. See
Taggart (1981).
15
   Butler and Hobbie (1976), a report prepared by the Congressional Budget Office, summarized the research by
Johnson and Tomola (1976), which concluded that fiscal substitution reached 100 percent within 18 months of a
PSE slot being funded. A reanalysis of the data by Borus and Hamermesh (1978) found that the amount of
substitution was very sensitive to the assumptions of the statistical model used. A qualitative field analysis
conducted at roughly the same time concluded that much of what appeared to be substitution was instead
maintenance, where PSE workers filled slots that would have been abolished in the absence of the PSE funding. See
Nathan et al. (1981).

                                                       13
           Amendments to CETA in 1978 were enacted to address concerns that the program was

not creating jobs but instead substituting federal funds for state and local funds. Among the

changes that were instituted, PSE wages in most places were capped at $10,000 annually, but in

high-wage areas salaries could be up to $12,000; average national wages were capped at $7,200,

lowering the national average by $600;16 new PSE participants could not have their wages

supplemented by the prime sponsor; and prime sponsors were required to establish independent

monitoring units to investigate violations of laws and regulations. Cook, Adams, and Rawlins

(1985, p. 13) refer to the 1978 amendments as “the beginning of the end for PSE.” All the

restrictions on qualifications, salaries, and project characteristics made PSE unattractive to prime

sponsors, so that when the Reagan Administration proposed barring PSE in the new Job Training

Partnership Act, there was little objection.

           The Job Training Partnership Act of 1982

           CETA was due to expire in 1982, and the replacement program, the Job Training

Partnership Act (JTPA) was a bipartisan effort sponsored by Senators Edward Kennedy and Dan

Quayle. The new law reflected President Reagan’s view of federalism, which included a larger

role for state government and smaller roles for the federal government and local government.

Public service employment, which had become increasingly unpopular with the public and less

desirable for local governments as restrictions on participants and activities were added, was

prohibited under JTPA. Some key features of JTPA included: programs for economically

disadvantaged youth and adults continued to be locally administered; states assumed a much

greater role in monitoring the performance of local programs; the private sector was given the

opportunity to play a major role in guiding and/or operating the local programs; and the system

was to be “performance driven,” with local programs rewarded or sanctioned based on their
16
     All these figures are in nominal dollars.

                                                 14
performance. As we describe below, both the role of the private sector and the performance

measurement system remain important but unsettled issues.

        JTPA included three categorical funding streams that were distributed by formula to the

states and then to local areas.17 The Title II-A program provided funding for economically

disadvantaged adults and youth, the Title II-B program was for summer youth employment and

training, and the Title III program served dislocated workers.18 National programs for Indians

and Native Americans, and migrant and seasonal farmworkers were authorized by Title IV of the

legislation. The Title II programs were conducted through local service delivery areas (SDAs),

which were similar in nature to the prime sponsors under CETA. The minimum population size

for automatic designation as an SDA was increased from 100,000 under CETA to 200,000 in an

effort to reduce the number of local programs from the over 450 prime sponsors under CETA;

the failure to have a provision for balance of state units actually led to an increase in the number

of local programs to over 600. Major activities provided under Title II-A were occupational and

basic skills training, OJT, job search assistance, and work experience.19

        JTPA focused on the poor. All out-of-school youth and at least 90 percent of those served

in the adult program had to meet income-based eligibility requirements. There were no income-

related eligibility requirements for those served in the dislocated worker program.20

        As noted above, JTPA attempted to increase the role of the private sector in guiding

employment and training programs. In 1978, CETA was amended to authorize the creation of

17
   Allocations for the adult and youth program to states and substate areas were based equally on the number
unemployed in areas of substantial unemployment (local areas with at least a 6.5 percent unemployment rate), the
number unemployed in excess of 4.5 percent of the labor force, and the number of economically disadvantaged
adults; allocations for the dislocated worker program distributed by formula were based equally on the number
unemployed, the number unemployed in excess of 4.5 percent, and the number unemployed for 15 weeks or longer.
See Johnston (1987).
18
   The 1992 JTPA amendments established a separate program, Title II-C, for services to youth.
19
   Reviews of the JTPA literature are found in Johnston (1987) and Levitan and Gallo (1988).
20
   Devine and Heckman (1996) analyze the eligibility requirements for JTPA from equity and efficiency
perspectives.

                                                       15
private industry councils (PICs), but the PICs gained much more authority under JTPA, where

the PICs served as boards of directors for the local programs and could operate the programs if

they voted to do so. PIC members were appointed by the chief local official(s) in the SDA, and a

majority of the members were required to be from the private sector.

         The Title III program for dislocated workers was originally a state-level program, and

states were required to match federal funding on a dollar-for-dollar basis. Congressional concern

about services to dislocated workers was high, and JTPA was modified in major ways in 1988

with the Economic Dislocation and Worker Adjustment Assistance Act (EDWAAA).21 This

legislation required governors to distribute at least 60 percent of the Title II funds to sub-state

areas.

         Major amendments to JTPA were enacted in August 1992.22 The amendments made the

program more prescriptive in terms of who could be served and what activities could be

undertaken. For example, the amendments required that at least 65 percent of the Title II-A

participants possess at least one characteristic that classified them as “hard to serve.”23



         The Workforce Investment Act of 1998

         The Workforce Investment Act (WIA) was enacted August 7, 1998 to replace JTPA.24

States had the option of being “early implementers,” but most states began implementing the


21
   In addition to modifying JTPA, Congress also passed the Worker Adjustment and Retraining Notification
(WARN) Act in 1988, which required employers under certain circumstances to provide workers with notice 60
days in advance of plant closings and major layoffs.
22
   For a summary of the amendments, see Barnow (1993), and for a thorough discussion of the amendments and
their impacts, see Trutko and Barnow (1997).
23
   These characteristics were basic skills deficient, high school dropout, welfare recipient, disabled, homeless, or an
offender. Youth could also meet the requirement if they were pregnant or a parent, or below the appropriate grade
level for their age.
24
   The Workforce Investment Act reauthorized several programs in addition to the workforce program commonly
referred to as WIA. Title I of WIA establishes the workforce program usually referred to as WIA; Title II authorizes
the federal adult education and literacy program; Title III amends the Wagner-Peyser Act to better integrate

                                                          16
new law July 1, 2000. The WIA program maintained the formulas used to distribute funds to

states and sub-state areas. WIA is based on seven guiding principles: 25

         (1) Streamlined services: Integrating multiple employment and training programs at the

“street level” through the One-Stop delivery system to simplify and expand access to services for

job seekers and employers.

         (2) Individual empowerment: Empowering individuals to obtain the services and skills

they need to enhance their employment opportunities through Individual Training Accounts

(ITAs), voucher-like instruments that enable eligible participants to choose the qualified training

program they prefer. Vendors were to meet performance criteria established by the states, and

vendors that met the criteria were to be included in an eligible training provider list.

         (3) Universal access: Granting access to all job seekers and others interested in learning

about the labor market through the One-Stop delivery system. The concept was that anyone

interested in what were termed core employment-related services could obtain job search

assistance as well as labor market information about job vacancies, the skills needed for

occupations in demand, wages paid, and other relevant employment trends in the local, regional,

and national economy.26

         (4) Increased accountability: Holding states, localities, and training providers

accountable for their performance. WIA was intended to improve the performance measurement

system established under JTPA by holding states accountable for their performance and building

continuous improvement into the system.

Wagner-Peyser labor exchange activities by requiring that Employment Service activities be integrated with the
One-Stop Career Center system; Title IV amends the Rehabilitation Act of 1973, which authorizes the state
vocational rehabilitation program for individuals with disabilities; and Title V includes general provisions dealing
with matters such as state unified plans and state incentive grants. See Bradley (2013).
25
   This is based on Barnow and King (2005), and the principles are described in the WIA White Paper available at
www.doleta.gov/usworkforce/documents/misc/wpaper3.cfm, accessed November 9, 2014.
26
   Access to staff-assisted services varied among local service delivery areas, depending on state and local policies
and funding availability.

                                                          17
        (5) A strengthened role for local Workforce Investment Boards (WIBs) and the

private sector: The framers of WIA envisioned that the local WIBs would have a stronger role in

administering the system than the PICs under JTPA and that employers would participate more in

administering the system than they had under JTPA.27

        (6) Enhanced state and local flexibility: Giving states and localities the flexibility to

build on existing reforms to implement innovative and comprehensive workforce investment

systems was a priority under WIA. Through such mechanisms as unified planning and waivers,

states and their local partners were provided flexibility to tailor delivery systems to meet the

particular needs of individual communities.

        (7) Improved youth programs: Linking youth programs more closely to local labor

market needs and the community as a whole, and providing a strong connection between

academic and occupational learning were envisioned under WIA.

        Many of the guiding principles do in fact reflect meaningful changes in the delivery

system for workforce investment services in the nation’s primary employment and training

program. The utilization of a one-stop system began several years prior to enactment of WIA on

a voluntary basis in local areas, but the 1998 statute required many workforce development

programs to co-locate and coordinate services in One-Stop Career Centers, which the U.S.

Department of Labor has recently rebranded as American Job Centers (AJCs).28 The One-Stop

centers were intended to provide the “core” and “intensive” services mandated by WIA

(described below); to provide access to workforce development programs and services offered by


27
   Although the Department of Labor envisioned a stronger role for the private sector under WIA, there is scant
evidence of this occurring. There were no major changes in the WIA statute that would have mandated a stronger
role for employers, and neither of the two studies of WIA implementation, D’Amico et al. (2004) and Barnow and
King (2005) found growth in the role of the private sector under WIA.
28
   Ironically, when local programs first formed one-stop centers on their own, state and federal officials sometimes
expressed concern that employees of one organization might provide services to customers who were supposed to be
served by a different program, and the practice was often discouraged.

                                                        18
One-Stop partners; and to provide access to the labor market information, job search, placement,

recruitment, and labor exchange services offered by the Employment Service (Bradley, 2013).

         The One-Stops were required to include over a dozen programs that provide services to

job seekers: WIA adult, youth, and dislocated worker programs; federal Department of Labor

programs authorized under WIA including Job Corps, the Native American program, and the

Migrant and Seasonal Farmworker program; Employment Service programs authorized by the

Wagner-Peyser Act; adult education and literacy programs; vocational rehabilitation; welfare-to-

work programs; the Senior Community Service Employment Program; postsecondary vocational

education; Trade Adjustment Assistance (TAA); programs administered by the Veterans’

Employment and Training Service; community services block grants; employment and training

activities operated by the U.S. Department of Housing and Urban Development; unemployment

insurance; and registered apprenticeship programs.29

         Depending on state and local policies, other relevant programs may be present at the

One-Stops. Optional partners noted by the Department of Labor include Temporary Assistance

for Needy Families (TANF), employment and training programs operated in conjunction with

the Food Stamps program (now the Supplemental Nutrition Assistance Program or SNAP),

Department of Transportation employment and training programs, and programs operated under

the National and Community Service Act of 1990.30

          Although the Department of Labor’s White Paper called for “streamlined services,” this

goal was hindered by another feature of WIA, namely the requirement that the program offer

services in sequence from core to intensive to training. D’Amico and Salzman (2004, p. 102)

29
   Retrieved from http://www.doleta.gov/usworkforce/onestop/partners.cfm on November 15, 2014; also available
from http://www.doleta.gov/programs/factsht/pdf/onestoppartners.pdf retrieved November 15, 2014. The programs
are described slightly differently at the two sites. Note that the welfare-to-work programs, which referred to special
programs for TANF recipients that were operated through local WIA programs, are no longer in operation.
30
   Ibid.

                                                          19
note that “JTPA was faulted for authorizing expensive training services as a first, rather than as a

last, resort.” As a result, WIA established three levels of service that customers were required to

access sequentially:31 (1) Core services, including outreach, job search and placement assistance,

and labor market information available to all job seekers; (2) intensive services, including more

comprehensive assessments, development of individual employment plans and counseling, and

career planning; and (3) training services, including both occupational training and training in

basic skills. Participants who reach the third step use an "individual training account" (ITA) to

select an appropriate training program from a qualified training provider.32

         Individual empowerment was an important feature of WIA, implemented largely through

the ITAs.33 Although vouchers were used by some local areas under JTPA, ITAs were the

default approach to training under WIA.34 Local areas had a great deal of flexibility in

administering the ITAs, and some local areas tended to give customers wide latitude in using

their ITA, while others restricted customers in terms of cost, past performance of the vendor, and

qualifications and aptitude of the customer for the course.35

         Universal access was envisioned as an important feature of WIA to avoid stigmatizing

the program due to its having poor and low-skilled customers. With the co-location of the

Employment Service in most One-Stop centers, it was anticipated that all adult job seekers, not

31
   Several reviewers questioned whether WIA specifically required sequencing of services or if the sequencing was
imposed by the Department of Labor. Section 134 of the statute reserves intensive services for those unable to obtain
or retain employment after receipt of core services, and training is reserved for individuals who are unable to obtain
or retain employment after receipt of intensive services. As the Department of Labor noted when it later emphasized
that WIA was not a “work first” program, there are no minimum time periods that a person must receive core or
intensive services before receiving training.
32
   http://www.doleta.gov/programs/general_info.cfm retrieved November 15, 2014.
33
   Although WIA required that ITAs be available to training customers in most circumstances, exceptions included
when on-the-job training and customized training are provided, when the local board determines that there are too
few providers available to meet the intent of vouchers, and when the local board determines that there is a local
program of demonstrated effectiveness for meeting the needs of special low-income participant populations that face
multiple barriers to employment (Patel and Savner, 2001, p. 1).
34
   For a review of the use of vouchers under JTPA, see Trutko and Barnow (1999).
35
   See D’Amico et al. (2004), Barnow (2009), and King and Barnow (2011). We discuss the ITA experiment later in
the chapter.

                                                         20
just the poor or unemployment insurance claimants who were required to search for work, would

use the One-Stops to obtain labor market information and search for work. Access to intensive

services and training was restricted, however, to public assistance recipients and other low-

income individuals when the local workforce area had insufficient funds to serve all potential

customers who might benefit from training.36

         The goal of increased accountability was addressed in two ways—changes were made to

the performance measurement system used under JTPA, and states and local areas were asked to

establish an eligible training provider (ETP) list of vendors with strong performance. Only

vendors on the list could accept ITAs from WIA participants.

         The performance measures varied over the existence of JTPA, with a trend toward longer

post-program follow-up for earnings measures.37 Changes in the performance system between

JTPA and WIA include the following:38 Under JTPA, only local areas were subject to

performance measures, but under WIA, the federal government sets standards for states, and the

states establish standards for local areas; under JTPA, after the initial few years, local standards

were adjusted by a regression model intended to hold areas harmless for differences in customer

characteristics and economic conditions, but under WIA standards were established through

negotiations.39 Performance was initially measured under JTPA at the time of program exit and

13 weeks after exit, and under WIA the employment and earnings measures used the second and

third quarters after program exit. JTPA did not specify a source for the data used to measure

performance, but WIA specified the use of unemployment insurance wage records.


36
   Later in the chapter we discuss the characteristics of WIA exiters. In Table 8.3 we note that among adult exiters
who left the program between April 2012 and March 2013, 60.9 percent received training.
37
   See Barnow (2011) for a discussion of the WIA performance measurement system and a comparison with the
JTPA system. In the same volume, Borden (2011) discusses the problems associated with measuring performance.
38
   This information is from Blank, Heald, and Fagnoni (2011), King and Barnow (2011), and Barnow (2011).
39
   In the last few years of WIA, the Department of Labor resurrected the idea of using statistical models to adjust
performance standards. Results of these efforts are described in Eberts, Bartik, and Huang (2011).

                                                         21
       During the WIA period, the Office of Management and Budget (OMB) sought to have all

workforce-oriented federally sponsored programs use “common measures” so that programs

could be compared, but only the Department of Labor complied. Originally, the WIA Adult

program had four measures—entered employment rate, employment retention rate, earnings

change, and the employment and credential rate; these measures are defined below when

program outcomes are provided. For dislocated workers, an earnings replacement rate was used

instead of an earnings change measure. Youth ages 19 to 21 had the same measures as adults,

and youth ages 14 to 18 had three core measures—the skill attainment rate, the diploma or

equivalent attainment rate, and the retention rate (D’Amico et al., 2004). In addition, there were

employer and participant customer satisfaction measures.

       The performance measures were modified somewhat in 2006, as described in Training

and Guidance Letter 17-05 (TEGL 17-05). The TEGL indicated three common measures to be

used for adults in the Adult and Dislocated Worker programs—entered employment rate,

employment retention (the proportion of adults employed in the first quarter after exit who were

employed in the second and third quarters after exit), and average earnings (total earnings in the

second and third quarters after exit for adults employed in each of the first three quarters after

exit). The three common youth measures beginning in 2006 are placement in employment or

education, attainment of a degree or certificate, and literacy and numeracy. The states using

common measures stopped using customer satisfaction measures beginning in 2006, as they were

not included in the common measures.

       The second effort to increase accountability in WIA was the use of an eligible training

provider (ETP) list. With customers having a greater role in selecting their field of training and

vendors through the use of ITAs, there was a risk that customers might select vendors based on



                                                 22
vendor claims rather than the performance of the programs and the customer’s suitability for the

program selected. Governors were thus given the opportunity to establish an ETP list that only

included vendors who had a good track record for that program. Evidence indicates that although

some states were able to develop satisfactory ETP lists, there were severe challenges in meeting

this requirement of WIA, and 35 states received waivers that permitted them to implement only a

portion of the ETP requirements or to delay implementation (Van Horn and Fichtner, 2011).40

         WIA included several changes to the eligibility requirements for youth participants as

well as changes in the programs themselves. There was separate funding for a summer youth

program and a year-round program under JTPA, while WIA included only a year-round

program; as D’Amico et al. (2004, p. VIII-1) note, the summer program was a major DOL

program for 36 years so this was a substantial change. The specific eligibility requirements

varied somewhat, but in both JTPA and WIA there was a heavy emphasis on serving poor youth;

a study by the U.S. General Accounting Office (GAO, 2002, p. 7) suggested that the eligibility

changes may have resulted in the youth served by WIA coming from poorer families than under

JTPA, and D’Amico et al. (2004, p. VIII-1) drew a similar conclusion. The new program also

required that at least 30 percent of the funds be spent on out-of-school youth. GAO (2002, p. 6)

notes that WIA’s intent was for longer-term and more comprehensive services than had been

provided under JTPA, and the statute required that 10 program elements be made available to

youth enrolled in the program.41



40
   Van Horn and Fichtner (2011. p. 155) note that “Education and training establishments and their trade
organizations marshaled opposition to performance reporting and undermined or quashed implementation
throughout the country.” D’Amico et al. (2004, p. I-12) also note the failure of the ETP list to achieve its expected
role in the system.
41
   The 10 required youth services are: (1) tutoring, study skills training, and instruction leading to completion of
secondary school; (2) alternative secondary school services; (3) summer employment linked to academic and
occupational learning; (4) paid and unpaid work experience; (5) occupational skills training; (6) leadership
development; (7) supportive services; (8) adult mentoring during the program and at least 12 months afterward; (9)

                                                          23
        There were two large-scale studies of the implementation of WIA in its early years:

D’Amico et al. (2004) and Barnow and King (2005). D’Amico et al. (2004) conducted their

study in 21 states and 38 local areas between 1999 and 2004. Barnow and King (2005) based

their analysis on eight states and 16 local areas visited in 2002. Below we briefly summarize the

major findings of the two studies, starting with D’Amico et al. (2004). D’Amico et al. (2004)

organize their summary of accomplishments and challenges by a slightly modified list of the

seven guiding principles listed above.

        Regarding the principle of streamlining services through integration, D’Amico et al.

(2004, p. I-4) conclude “Despite numerous challenges that have been encountered along the way

(and sometimes outright resistance), partnership formation represents a highly successful and, in

the long term, potentially critically important accomplishment engendered by WIA.” The authors

note that the system encountered a number of challenges, and the greatest challenge appeared to

be finding each partner’s share of financing the One-Stop infrastructure. Other challenges they

note include differing visions among partners on what service integration means, differences in

program goals and customer needs across partners, varying cultures among One-Stop partners,

logistical issues in arranging co-location, different management information systems for various

programs, and separate performance and reporting requirements among programs.

        D’Amico et al. (2004) found that states and local areas had made great progress in

promoting universal access through the One-Stop system. As evidence, they note that states and

local areas established nearly 2,000 One-Stop Centers by 2003, and that 40 percent of the local

areas had six or more access points to services. The authors observe that promoting universal

access creates some important tensions in the system; for example, by broadening services to the


at least 12-month follow-up after program completion; and (10) guidance and counseling. See U.S. General
Accounting Office (2002, p. 7).

                                                       24
entire population, fewer resources are available for the poor, and local areas must decide how to

balance provision of lower tier services with the desire to provide training to those who need

more skills.42

        The study found that the principle of empowering customers through choice has been

“enthusiastically embraced by One-Stop administrators and staff” (p. I-11). Specifically, they

point to the widespread use of ITAs to be evidence of the popularity of giving customers choice.

The authors note that many local areas cap the ITAs at levels as low as $1,500 so that resources

are spread across many participants.

        The goal of enhancing state and local flexibility is considered a major success by

D’Amico et al. (2004, p. I-12). The authors state:

        …our field researchers were struck by the enormous diversity in WIA service designs
        and delivery structures across the country. Thus, within the broad constraints of the
        legislation, local areas vary markedly in their governance and administrative structures,
        the way local boards operate, the procedures for designating One-Stop operators and the
        responsibilities with which the operator is charged, the ways partners work together to
        staff various services, how adult and dislocated workers move through the service levels,
        how priority for target groups is established, whether or not training is emphasized, caps
        placed on ITA amounts, and so forth.

Although states and local areas appreciated the freedom, the researchers felt that the states and

local areas would have benefited from technical assistance on promising practices.

        Although employment and training programs have had performance measurement

systems since the 1970s, D’Amico et al. (2004) found the principle of promoting performance

accountability was the most challenging of the WIA principles to implement. One aspect of the

accountability system is the requirement that states and local areas establish an ETP list of

training vendors. Problems cited regarding the ETP list include that high standards limit the

choice of vendors available to customers, many vendors (including a number of low-cost, high-

42
  As is shown in Tables 8.5 and 8.6, only about one-half of those served by the WIA adult program meet the
definition of low-income, and fewer than 10 percent of the customers received training.

                                                       25
quality community colleges) dislike the ETP application procedures and may not seek to be

listed, and the data used to compile the list is often of questionable reliability.

        States and local areas also expressed concern about the performance measurement system

used for the WIA program. The concerns included that the measures were too numerous and

complex, the definitions used for some of the measures (such as credentialing) were vague and

potentially unreliable, the system promoted “cream skimming” of the potential customers most

likely to look good on the performance measures, the states and local governments spent a

significant effort managing their numbers rather than focusing on providing appropriate services,

the system was not useful for program management because of the long lag between when

customers were served and when the results were measured, and the differing measures across

programs hindered partnership development.

        D’Amico et al. (2004) found that although WIA continued the requirement that business

representatives make up a majority of state and local boards and the Department of Labor

encouraged states to make increased use of business in shaping their programs, “in practice

[local workforce areas] are lagging in their ability to engage businesses seriously in strategic

planning or serve them as customers with high-quality services” (p. I-17).

        The final guiding principle for WIA was improving youth programs. D’Amico et al.

(2004) found that at the time of their site visits, states and local areas were “lagging badly behind

in their implementation of youth programming, partly because of the time delays inherent in

needing to appoint a Youth Council [one of the new requirements of WIA] and competitively

select service providers.” Other challenges for the WIA youth program included the abolition of

the summer youth program, the requirement that individual eligibility be documented rather than

being able to use presumptive measures such as participation in free and reduced price school



                                                  26
lunch programs, dealing with the statutory requirement that 10 program elements be included in

youth programs, and connecting WIA programs with the One-Stop system for older youth.

       Barnow and King (2005) organized their findings around five major topics: (1)

leadership, (2) system administration and funding, (3), organization and operation of One-Stop

Career Centers, (4) service orientation and mix, and (5) use of market mechanisms.

         The study states exhibited a range of leadership patterns in setting up, implementing,

 and operating their workforce development systems. In five of the eight states, the governor’s

 office played a strong leadership role, but in others the governor gave discretion to local

 workforce areas. The state legislature had a leadership role in three states, resulting in

 bipartisan state workforce legislation. Business’s role was strong at the state level in only a

 few of the states. At the local level, however, business engagement was found to be strong in

 half of the states.

       WIA’s administrative structure is complex, distinguishing between policy development,

program administration, and service delivery more explicitly than earlier workforce legislation. It

also requires states to balance state and local responsibilities and make decisions about how to

administer WIA in conjunction with other state employment security, economic development,

and related programs. The most common approach in the states in the study is that policy was

developed by the state and local WIBs, program administration was undertaken by agencies at the

state and local level, and service delivery was carried out by vendors. Some study states adopted

this separation of responsibilities several years prior to WIA. Some states and local areas found

that they did not have sufficient funding to provide training to all they believed would benefit

from the service, and they limited training by rationing it and/or by limiting the amount that

would be paid for training programs.



                                                 27
       Barnow and King (2005) found wide variation in how states and local areas interpreted

the requirement to operate programs through the One-Stop system. They found that challenges

arose related to how the mandatory and optional partners relate to each other at the centers and

regarding how the centers are operated and funded. In some states, key programs such as WIA,

the Employment Service, and TANF are highly integrated, but in others TANF, which is an

optional partner, has no presence at One-Stop Centers, and/or the Employment Service has a

separate office. Although Unemployment Insurance (UI) is a mandatory partner, the study found

its role in the One-Stop Career Centers to be minimal; this is because in the years prior to WIA,

UI staff in most states were located in call centers and primarily dealt with clients through

telephone and internet contact. The study found that TANF, Vocational Rehabilitation, and the

Veterans Employment and Training Service did not fit well in One-Stop Career Centers because

of conflicting goals, cultures, or other differences. There was variation in how the infrastructure

of One-Stop Centers was financed, and the issue of funding the centers was a source of

contention in most of the study sites.

       Barnow and King (2005) found that service orientation evolved significantly in the early

years of WIA implementation in many states. Initially, states and local areas interpreted the

statutory language to require a “work-first” or labor market attachment orientation based on early

guidance provided by the Department of Labor and the statutory requirement for sequencing of

core, intensive, and training services. Later, the Department of Labor made it clear that a work-

first orientation was not required and that states could place greater emphasis on training. After

that, states diverged in their orientation, with some still emphasizing finding work, and others

focusing more on human capital development through training, and still others leaving

orientation up to local areas.



                                                 28
         Market mechanisms were to play a major role under WIA, and Barnow and King (2005)

analyzed the ETP list requirement and the use of performance measures to reward and sanction

states and local areas based on how well they did in terms of customers’ employment and

earnings. They found that three states already had systems in place to monitor training provider

performance, and these states had little problems with the ETP list concept. Of the remaining

states in the study, three had problems initially with the ETP concept but were able to adapt, and

two states found the system to be burdensome for training providers and reported that some

vendors refused to participate in WIA because of the ETP requirements.43

         A second market mechanism implemented in WIA is the use of ITAs. Barnow and King

(2005) reached conclusions similar to D’Amico et al. (2004), finding that the ITAs were popular

with customers and accepted by local programs as a useful feature.44

         Barnow and King (2005) also reached conclusions similar to D’Amico et al. (2005) on

the WIA performance standards system. State and local areas were critical of the elimination of a

regression-based adjustment system to level the playing field and replacement of this approach

with negotiations between states and the Department of Labor, particularly because many states

believed that the Department of Labor representatives often did not negotiate fairly. Barnow and

King (2005) also found that a majority of states in their sample engaged in strategic behavior

designed to make their measured performance look good.




43
   This is one of the few areas where Barnow and King (2005) reach different conclusions from D’Amico et al.
(2004). We believe that the somewhat more positive conclusions regarding the ETP list for Barnow and King relate
to the nature of their sample of states, which included a relatively high proportion of states with something
resembling an ETP list in place prior to WIA. As noted earlier, Van Horn and Fichtner (2011) reported that a
majority of states now have waivers to some or all the ETP list requirements, indicating that this feature has not been
widely implemented.
44
   Perez-Johnson, Moore, and Santilano (2011) provide results from an experimental evaluation comparing three
models for administering ITAs. We discuss this experiment in detail below.

                                                          29
        The Workforce Innovation and Opportunity Act of 2014

        Although WIA was originally authorized for five years, 15 years passed before the two

houses of Congress and the Administration were able to agree on new legislation. In 2014,

working largely behind the scenes, the House and Senate reached agreement on the Workforce

Innovation and Opportunity Act (WIOA) as a replacement for WIA, again with broad bipartisan,

bicameral support. The bill was introduced in May 2014 with sponsorship by both parties in

both houses of Congress, and the bill was signed July 22, 2014. WIOA makes some significant

changes to the nation’s workforce development system and is authorized through 2020.

Highlights of the new law are described below. Proposed regulations were issued April 16, 2015

but are not discussed in this chapter, as they are subject to revision.45 Most of the new legislation

became effective July 1, 2015. The new legislation maintains much of the structure of WIA, with

states having a prominent administrative role and services delivered through local workforce

areas designated by the states. WIOA also maintains WIA’s funding streams for Adults,

Dislocated Workers, and Youth, and requires activities at the state and local levels to be overseen

by a board with a majority of the members from the private sector. Funds are distributed to the

state and sub-state levels using formulae similar to those used under WIA.

        States are required to establish unified strategic planning across core programs defined as

the WIOA Adult, Dislocated Worker, and Youth programs; Adult Education and Literacy

programs; the Wagner-Peyser Employment Service, and state Vocational Rehabilitation

programs. If taken seriously by the states this could be important, but a unified plan could simply

consist of separate plans attached to each other.



45
  Interpretation of the WIOA statute is based on the U.S. Department of Labor’s WIOA Fact Sheet, accessed at
http://www.doleta.gov/wioa/pdf/WIOA-Factsheet.pdf retrieved on November 16, 2014, and National Skills
Coalition (2014).

                                                       30
           The boards at the state and local levels have streamlined membership requirements,

which are expected to reduce their size; boards under JTPA sometimes included 50 members or

more. The desire to be inclusive of many interest groups is admirable, but such large bodies may

not (and often did not) function well. The boards also have new responsibilities to develop

strategies to meet worker and employer needs.

           The Act adds flexibility at the local level to provide incumbent worker training and

transitional jobs as allowable activities and promotes work-based training by increasing the

maximum reimbursement rate for on-the-job training from 50 percent to 75 percent; the law also

emphasizes training that leads to industry-recognized post-secondary credentials. These changes

are all efforts to make the program more attractive to employers and, it is hoped, increase their

participation.

           WIOA attempts to strengthen program accountability in several ways. The performance

measures for core workforce development programs are aligned, and new performance indicators

are added related to services to employers and post-secondary credential attainment.46 Data on

training providers’ outcomes must be made available, and programs are to be evaluated by third

parties.

           States are required to identify economic regions within the state, and local areas are

required to coordinate planning and service delivery on a regional basis. Prior legislation has also

mentioned regional coordination. Although perhaps laudable in concept, these efforts are

difficult to enforce. Also, these provisions cannot address issues of labor market areas that cross

state borders.



46
  The statute is more prescriptive than previous laws. For example, the law requires that median post-program
earnings be used as a performance measure and that statistical adjustment models be developed to adjust standards
for variations in customer characteristics and economic conditions.

                                                        31
        The statute seeks to provide better services to job seekers in a number of ways. First,

WIOA promotes the use of career pathways programs and sectoral partnerships for training

programs, two approaches that appear promising.47 Second, the statute allows states to transfer

unlimited amounts of their grant between the adult and dislocated worker programs.48 Third,

WIOA adds basic skills deficient as a priority category for participants, along with low income,

for Adult services. Fourth, WIOA requires that 75 percent of Youth funds be used for out-of-

school youth, a large increase over the 30 percent required under WIA. Fifth, WIOA combines

the core and intensive service categories under WIA into a new category called career services,

and it abolishes the requirement that customers pass through core and intensive services before

receiving training. WIOA also permits direct contracts with higher education institutions (rather

than placing participants on an individual basis or with ITAs), a practice that was commonly

used prior to WIA and was permitted with funds provided under the American Reconstruction

and Recovery Act (ARRA).

        Finally, WIOA changes the partners required to be in the American Job Centers. Under

WIOA, the Wagner-Peyser Employment Service is required to be co-located in the AJCs, and the

TANF program is made a mandatory partner instead of an optional partner. WIOA also

authorizes the use of performance-based contracting for training providers.49



47
   Career pathways are defined in Section 3 of the statute. Training and Employment Notice 39-11 (TEIN 39-11)
issued by the Employment and Training Administration states that “Career pathways programs offer a clear
sequence of education coursework and/or training credentials aligned with employer-validated work readiness
standards and competencies. TEIN 39-11 has links to information about career pathways programs. The approach
has been adopted by the U.S. Department of Labor, the U.S. Department of Education, and the U.S. Department of
Health and Human Services. Sectoral programs are programs that provide training for an industry sector,
presumably with significant input from sector employers.
48
   Under WIA, states had to receive permission from DOL to transfer funds among the Adult and Dislocated Worker
programs. Although such transfers used to be routinely approved, in recent years DOL was more rigid. See Barnow
and Hobbie (2013).
49
   The Department of Labor has changed policies on the use of performance-based contracting several times.
Although there is appeal to pay for performance, some abuses of performance-based contracting appear to have led
to large profits for some vendors, so the policy was tightened. See Spaulding (2001).

                                                      32
         Although there is currently hope in the workforce development community that WIOA

will improve the workforce development system, sometimes promising ideas, like the eligible

training provider list, prove more beneficial in theory than in practice.

         Employment and Training Program Expenditures and Enrollments over Time

         Table 8.2 shows estimated expenditures on Department of Labor employment and

training programs except the Wagner-Peyser Act from 1965 to 2012; Figure 8.1 shows the trend

in total funding in real 2012 dollars graphically, and Figure 8.2 shows the trend in funding for

Department of Labor programs as a percentage of gross domestic product (GDP). The data from

1984 on were compiled by the Employment and Training Administration Budget Office and are

believed to accurately reflect final budget authority for each year, including supplemental

appropriations, recissions, and transfers.50 Data from 1965 through 1983 were obtained primarily

from unpublished data from the ETA Budget Office, and the data are believed to be accurate but

may not reflect all recissions, supplemental appropriations, and transfers.51 Data on dislocated

workers is unavailable prior to 1984, as there was not a separate program for dislocated workers

before that, and from 1984 through 1992, JTPA youth and adult funding are not available

separately.

         In addition to the MDTA, JTPA, and WIA programs, the table includes other ETA

programs, such as the Senior Community Service Employment Program, the Indian and Native

50
   Budget authority is the amount of money available for spending, but actual expenditures in a year can reflect
carryovers of funds from prior years and amounts available but unspent. Transfers reflected in the table refer to
transfers among programs at the national level, but they do not reflect transfers of funds within states between the
WIA Adult and Dislocated Worker programs. Finally, the Job Corps was removed from the ETA in FY 2008 and
although it was later added back to the ETA budget, it was maintained in a separate account. We obtained Job Corps
data for FY 2008 and after from OMB budget documents for the Department of Labor. The detailed Employment
and Training Administration budget data was obtained from http://www.doleta.gov/budget/bahist.cfm accessed on
February 15, 2015.
51
   We are grateful to Anita Harvey of the ETA Budget Office for providing the data, but she is not responsible for
the analysis performed. Data on Job Corps was obtained from other budget documents, but we were not able to find
data for all individual years. For years where Jobs Corps data are missing, Job Corps budget authority is included in
the total column but was not available separately.

                                                         33
Americans program, the Migrant and Seasonal Farmworker program, and a number of other

national activities.52 The table must be read with caution because of the ways that programs were

funded at various times. For example, during the Great Recession, the American Recovery and

Reconstruction Act (ARRA) added $4.279 billion for the covered programs. The ARRA funds

were intended to be spent in a timely manner, and there were restrictions on how long the funds

were available for spending. Eberts and Wandner (2013) find that the WIA Adult program spent

72 percent of the ARRA funds available in the first five quarters, and the Dislocated Worker

program spent 60 percent. Thus, most of the ARRA funds were actually spent in PY 2009 and

PY 2010. Other examples of appropriations expected to fund programs over several years

include funding for public service employment (beyond what was provided for in the regular

CETA program) in 1977 and the Youth Employment and Demonstration Projects Act of 1977.53

        The table and graphs show that overall funding in nominal terms has increased over the

period covered, but the share of GDP devoted to workforce development programs has followed

an irregular course. In the 1960s, the programs constituted between 0.04 and 0.10 percent of

GDP. Employment and training programs peaked as a share of GDP in the 1970s, due in large

part to one-time efforts such as a large-scale public service employment appropriation of $6.8

billion in 1977 and the Youth Employment and Demonstration Projects Act, also in 1977. During

the 1970s, employment and training programs were consistently over 0.10 percent, and reached a

peak of 0.64 percent in 1977. During the WIA era in the 1980s and 1990s, funding as a share of

GDP gradually declined, from about 0.07 percent of GDP in the earlier years to the 0.04-0.05




52
   Programs are described on the Employment and Training Administration’s web site, http://www.doleta.gov/#
accessed March 2, 2015.
53
   We were unable to locate original budget documents for the 1977 fiscal year, but we did find several documents
that gave total YEDPA spending each year, so we have included YEDPA based on these spending figures.

                                                        34
range toward the end of the WIA era; as noted earlier, there was a temporary increase in

expenditures due to the ARRA during the Great Recession.

        Although the focus of this chapter is on U.S. programs, it is instructive to compare U.S.

expenditures on publicly-funded training with those of other nations. The Organization for

Economic Cooperation and Development (OECD) estimated the share of GDP devoted to

training in a number of countries. OECD estimates that the United States spent 0.04 percent of

GDP on training in 2012, which is substantially less than in most of the countries tracked,

including Austria (0.45 percent), Belgium (0.15 percent), Canada (0.08 percent), Denmark (0.74

percent), Estonia (0.17 percent), Finland (0.52 percent), France (0.34 percent), Germany (0.22

percent), Italy (0.15 percent), Japan (0.05 percent), Korea (0.07 percent), the Netherlands (0.11

percent), New Zealand (0.13 percent), Norway (0.15 percent), Portugal (0.27 percent), and

Sweden (0.09 percent). There were several countries that spent the same percentage of GDP or

less on training, including Chile, Czech Republic, Mexico, and the Slovak Republic.54

        The programs that have fared the best since the 1980s (in terms of funding, but not in

terms of program effectiveness) are the Dislocated Worker programs under JTPA and WIA.

Unlike the other programs, funding for Dislocated Workers has grown substantially since 1985.



Characteristics of Employment and Training Program Participants

This section describes the characteristics of employment and training program participants, or

“customers,” as they are sometimes called.




54
  OECD data on training as a share of GDP was obtained from
http://stats.oecd.org/Index.aspx?DatasetCode=LMPEXP accessed March 1, 2015.

                                                    35
         Characteristics of Recent WIA Exiters

         The most recent data available on the Adult, Dislocated Worker, and Youth WIA

participants are shown in Tables 8.3. Unfortunately, data on WIA enrollments are not easy to

interpret. As noted above, the WIA Adult and Dislocated Worker programs require participants

to receive core and intensive services before they can receive the more expensive training

services. Core services can be accessed with or without staff assistance (including, in the latter

case, via the internet), and states are asked to report only customers who receive staff assistance;

it is likely that states and localities vary in how they interpret the definition of “staff assisted,”

particularly since customers who are recorded as staff assisted count in calculating performance

while those who are recorded as self-service do not. Moreover, WIA core services include the

same types of services provided by the ES, whose staff members are co-located with WIA at the

One-Stop centers, and states vary in their policies regarding co-enrollment in the ES and WIA. In

addition to varying by state, all of these policies also vary over time in some states, leaving both

cross-sectional and longitudinal comparisons open to misinterpretation. For example, in PY

2006, New York adopted a policy of co-enrolling all Wagner-Peyser customers in WIA,

resulting in an increase in the number of Adult WIA exiters entering employment from 20,963 in

PY 2005 to 210,049 in PY 2007—more than a 900 percent increase.55 The general trend over

time was for increased co-enrollment of Wagner-Peyser participants in WIA, making

comparisons of enrollments over time difficult to assess.

         Table 8.3 shows the characteristics of exiters from the WIA Adult and Dislocated Worker

programs from April 2012 through March 2013.56 Nine percent of the exiters from the Adult


55
   See Trutko and Barnow (2010) for more examples in the variation in how customers are classified across states
and over time.
56
   The WIA data system is designed to provide data for performance measurement. Because the performance
measures track cohorts of exiters, data are provided on exit cohorts rather than all participants in a given period.

                                                          36
program are ages 18-21, and thus were also eligible for the Youth program.57 The program

served a slightly higher percentage of men than women, 52 percent compared to 48 percent.

Individuals with disabilities constitute nearly 4 percent of all Adult exiters and about 3 percent of

those who received training. A majority of the Adult exiters, 59 percent, are white, with black

non-Hispanics making up 25 percent of the exiters and Hispanics constituting 10.5 percent. Pre-

program quarterly earnings were about $6,000 for those with earnings for all exiters and about

$5,400 for Adult exiters who received training.58

         Because core services under WIA are open to all and access to training is only restricted

to low-income individuals if there is not sufficient funding available for all customers the

programs would like to enroll, the WIA customers are not as economically disadvantaged as one

might expect. Only one-half of the exiters from the Adult program are classified as low-income,

and only 61 percent of Adult exiters who received training are classified as low income, which is

somewhat surprising given the focus on low-income families and the relatively broad definition

of low income.59 About one-quarter of all Adult exiters and one-third of Adult exiters receiving

training are public assistance recipients.60




57
   Customers who are co-enrolled in two programs are reported for both programs. Thus, some adults are also
included in youth program data and dislocated worker program data.
58
   Quarterly earnings are derived from state unemployment insurance wage records and thus do not include self-
employment income or earnings from government, military, or informal employment. Earnings are the average for
the second and third quarters prior to entry if earnings were positive for both quarters. If earnings were positive for
only one of the second and third quarters, then the value used is earnings in that quarter. Individuals with zero
earnings in both quarters are not included in the average. See Appendix B in Social Policy Research Associates
(2013) for definitions of terms used.
59
   The WIA definition of “low income” is complex; see Social Policy Research Associates (2013, p. 299) for the full
definition. It is broader than being in poverty, and includes all recipients of cash assistance (such as TANF), SNAP
(food stamps), and individuals whose family income is less than 70 percent of the lower living standard income
level. In 2014, the poverty level for a family of four in the 48 contiguous states was $23,850 and 70 percent of the
lower living standard income level for a family of four ranged from $23,285 to $31,945, depending on the state of
residence and whether the family lived in a specific metro area or a non-metro area. See Federal Register (2014).
60
   Public assistance recipient for WIA reporting is broadly defined and includes TANF, general assistance, SNAP,
supplemental security income, and refugee cash assistance. See Social Policy Research Associates (2013).

                                                          37
       The educational attainment of the Adult exiters is fairly high. Less than 11 percent of the

exiters had not completed high school or passed the GED, 30 percent had some postsecondary

education, and 13 percent had at least a bachelor’s degree. Adult exiters who received training

had roughly equivalent levels of education.

       Characteristics of Dislocated Worker exiters are not markedly different from those of the

Adult exiters. Dislocated Worker exiters are less likely to be under 21 (3 percent compared to 9

percent for Adults), and they are more likely to be age 55 and above (20 percent compared to 14

percent). They are slightly more likely to be white (63 percent compared to 59 percent). Not

surprisingly, their quarterly pre-program earnings are substantially higher than Adult exiters

($8,566 compared to $6,006).

       Characteristics of WIA Youth exiters are presented in Table 8.4. In addition to presenting

the data for all youth, data are available for two categories of in-school youth (high school or

below and post-secondary) and two categories of out-of-school youth (high school dropouts and

high school graduates). Roughly 40 percent of the Youth exiters attended high school or a lower

level of school, and nearly one-quarter of the exiters were in each of the out-of-school categories

(high school dropouts and high school graduates); the balance, about 4 percent of the total,

attended a post-secondary school.

       The WIA Youth program is much more income targeted than the Adult or Dislocated

Worker programs, and 97 percent of the exiters were low-income youth (not shown in table).

The Youth participants differ in several other ways from those served by the Adult and

Dislocated Worker programs. Women made up a majority of the exiters in all categories, with

the smallest proportion among dropouts, most likely because young women are more likely to

stay in school than young men. The racial/ethnic mix of Youth exiters also differs from what we



                                                 38
see for Adults and Dislocated Workers. In contrast to the other two programs, whites comprised

only about 30 percent of exiters, with similar numbers of Hispanic non-blacks and non-Hispanic

blacks exiting the program.

       WIA Youth exiters had a high prevalence of conditions likely to serve as barriers to

employment. The proportion of participants with a disability was considerably higher among

youth than in the two other programs, running at 13 percent of all Youth exiters compared to 4

percent for the Adult program and 3 percent for the Dislocated Worker program. Nearly 5

percent of the Youth were classified as homeless or runaway youth, with a rate of nearly 7

percent for dropouts. Nearly 4 percent of the Youth exiters had been in foster care, with a higher

rate for those attending high school (4.7 percent) and a considerably lower rate for high school

graduates (2.6 percent). Nearly two-thirds of the Youth exiters were classified as being deficient

in basic literacy skills, with nearly 3 out of 4 deficient among high school dropouts.

       Services Received by WIA Exiters

       Table 8.5 summarizes the services received by WIA Adult and Dislocated Worker exiters

for recent years (Program years 2008 through 2012). Among the Adult customers, only 10 to 13

percent received training. Training was somewhat more common for Dislocated Workers,

ranging from 14 percent to 19 percent during this period. Some of the participants received

specialized training. Among those receiving training, on-the-job training was received by

between 7 percent and 13 percent of the adults and 6 percent and 12 percent of the Dislocated

Workers. Skill upgrading and retraining was slightly more common, with 12 percent to 15

percent of the Adults and 14 percent to 16 percent of the Dislocated Workers receiving this type

of training. The incidence of entrepreneurial training, adult basic education (ABE) or English as

a Second Language (ESL) in combination with training, and customized training were all less



                                                39
than 5 percent for both Adult and Dislocated Worker exiters.61 About three-quarters of the Adult

and Dislocated Worker exiters received other types of training, presumably mostly classroom

training.

        Outcomes for WIA Exiters

        The WIA statute requires that data on the satisfaction levels of employers and

participants be collected as part of the performance measurement system. However, when the

Department of Labor adopted the common measures for performance, most states were given

waivers from this requirement. Seven states still report satisfaction data, and collectively their

satisfaction scores (among respondents) averaged 84 for participants and 77 for employers.62

        Data on outcomes for recent exiters from WIA are shown in Table 8.6. There are three

common measures for Adults, Dislocated Workers, and Older Youth: (1) Entered Employment:

Of those who are not employed at the date of participation, [Number of participants who are

employed in the first quarter after the exit quarter]/[Number of participants who exit during the

quarter]; (2) Employment Retention: Of those who are employed in the first quarter after the exit

quarter, [Number of participants who are employed in both the second and third quarters after the

exit quarter]/[Number of participants who exit during the quarter]; Average Earnings63: Of those

who are employed in the first, second, and third quarters after the exit quarter, [Total earnings in




61
   Customized training refers to vocational training developed with input from employers regarding eligibility,
curriculum and requirements for successful completion. Also sometimes referred to as “employer-based training,”
customized training often includes provisions for employers to hire or give preference in hiring to individuals who
have successfully completed the training.
62
   Satisfaction is measured by a three-question survey called the American Consumer Satisfaction Instrument
(ACSI). Employers and participants respond to each question on a 1 to 10 scale, and the total score is a weighted
average of the three responses, scaled to range from 0 to 100. The ASCI measure and its use are described in the
Department of Labor TEGL 36-10, accessed at http://wdr.doleta.gov/directives/corr_doc.cfm?DOCN=3052
retrieved November 22, 2014. Satisfaction data are only available for Arizona, Hawaii, Michigan, Minnesota, Puerto
Rico, Rhode Island, and Vermont.
63
   Previously, instead of post-program earnings, the measure for Adults was change in earnings from pre-enrollment
earnings, and the measure for Dislocated Workers was the earnings replacement rate.

                                                        40
the second quarter plus total earnings in the third quarter]/[Number of participants who exit

during the quarter].

         The entered employment rate for Adult and Dislocated Worker exiters in PY 2012 was 60

percent, but there was a great deal of variation among subgroups. Subgroups with lower entered

employment rates include individuals with disabilities (41 percent for Adults and 46 percent for

Dislocated Workers), and older individuals64 (48 percent for both programs). Based on data from

Social Policy Research Associates (2013), which covers a slightly different period, there is little

variation in the entered employment rate by race/ethnicity and gender. Perhaps surprisingly,

Adults who were receiving public assistance at entry had an above average entered employment

rate. Another surprising finding is that the entered employment rate for Older Youth (70 percent)

is a full 10 percentage points higher than the rate for exiters from the Adult and Dislocated

Worker programs (60 Percent).65

         The entered employment rate differs by a fairly large amount for individuals who

received training (75 percent for Adults and 81 percent for Dislocated Workers) compared to

those who only received core and/or intensive services (59 percent for Adults and 56 percent for

Dislocated Workers). As noted earlier, under WIA, states vary greatly in the proportion of

participants that receives training. An analysis of 2002-2005 data found that the percentage

ranged from 14 percent to 96 percent (Trutko and Barnow 2007).66

         Subgroup outcomes varied by considerably less for employment retention. Most

subgroups fell within a range of 80 percent to 90 percent retention. The only exceptions were

individuals with disabilities (75 percent) and older youth veterans (60 percent). The range of

64
   The published data do not include a definition for “older individual,” but based on data in Social Policy Research
(2013) it is likely that this refers to participants age 55 and above.
65
   The older youth outcomes are reported only for six states that have a waiver from using the common measures.
66
   Training was much more prevalent in the period covered by Trutko and Barnow (2007), perhaps in part because
co-enrollment of all Employment Service customers became more common after 2005.

                                                         41
outcomes among subgroups was also not large for six month average earnings. In part, the

compression in the range in outcome results across subgroups is likely an artifact of the way the

measures are defined. The entered employment rate is based on data for all exiters, but the

employment retention measure only includes individuals who were employed in the first quarter

after exit, and the average earnings measure is based only on exiters who were employed in all

three quarters after exit; because these measures are based on customers with initial post-

program success, it is not surprising that customers included in the calculations tend to do well

on the measures.

       The three outcome measures used for youth are: (1) Placement in Employment or

Education: Of those who are not in postsecondary education or employment at the date of

participation, [Number of youth participants who are in employment or enrolled in

postsecondary education and/or advanced training/occupational skills training in the first quarter

after the exit quarter]/[Number of youth participants who exit during the quarter]; (2) Attainment

of a Degree or Certificate: Of those enrolled in education at the date of participation or at any

point during the program, [Number of youth participants who attain a diploma, GED, or

certificate by the end of the third quarter after the exit quarter]/[Number of youth participants

who exit during the quarter]; and (3) Literacy and Numeracy Gains: Of those out-of-school

youth who are basic skills deficient, [Number of youth participants who increase one or more

educational functioning levels]/[Number of youth participants who have completed a year in the

program plus the number of youth participants in the program who exit before completing a year

in the youth program].

       Although we report the youth common measure results in Table 8.6, we do not discuss

them here because we do not consider them of particular interest. We would prefer to see youth



                                                 42
measures based on school status at the time of entry, as employment and earnings measures are

of interest for out-of-school youth, but educational measures are of more interest for in-school

youth.67



Current Funding Levels

Table 8.7 lists major means-tested employment and training programs funded by the Department

of Labor and other federal agencies. Only programs with at least $30 million in budget authority

for 2014 are included. We also omit temporary programs, pilots, and demonstrations regardless

of their size. So, for example, we omit the Transition Assistance Program that provides

assistance to separating veterans so that they can reenter the civilian labor market because the

program is only funded at $14 million annually and the Trade Adjustment Assistance

Community College and Career Training (TAACCCT) grants, which included $464 million in

funding for FY 2014 because it is a temporary program with no funding for future years. For

each program, we provide a brief description of funding level, program eligibility, and activities.

        U. S. Department of Labor Programs

        Job Corps.68 Job Corps is a largely residential education and vocational training program

serving young people ages 16 through 24 through vocational and academic training. Through a

nationwide network of campuses, Job Corps offers a comprehensive array of career development

services to at-risk young women and men to prepare them for careers. Job Corps integrates the

teaching of academic, vocational, and employability skills, and social competencies through a

combination of classroom, practical, and work-based learning experiences. Enacted budget

authority for the Job Corps for PY 2014 is $1,684 million.

67
   The Department of Labor does report the outcomes used for Adults and Dislocated Workers for subgroups as well
as all older youth, but these results are only available for six states.
68
   Material on Job Corps is from http://www.jobcorps.gov/AboutJobCorps.aspx retrieved November 23, 2014.

                                                      43
         WIA Adult and Dislocated Worker Programs. The WIA Adult and Dislocated Worker

programs are “designed to provide quality employment and training services to assist eligible

individuals in finding and qualifying for meaningful employment, and to help employers find the

skilled workers they need to compete and succeed in business.”69 The program funds are

allocated to states by formula based on their unemployment rates and on their number of

economically disadvantaged individuals and then distributed to local areas by the same formula.

Participants can receive core, intensive, and training services at local American Job Centers,

which were described above. Core services are available to all, but intensive and training

services are reserved for low-income individuals if there are insufficient funds to serve everyone.

The Dislocated Worker program is restricted to individuals who have lost their job or are about

to lose their job; the definition also includes self-employed individuals (including farmers and

ranchers) who have lost their potential livelihood and displaced homemakers who have lost their

financial support from another family member. For PY 2014, the funds appropriated are $764 for

the Adult program and $1,219 million for the Dislocated Worker program.70

         WIA Youth Program.71 The WIA Youth program serves eligible low-income youth, ages

14-21, who face barriers to employment. Funds for youth services are allocated to state and local

areas based on a formula distribution. Service strategies, developed by workforce providers,

prepare youth for employment and/or post-secondary education through strong linkages between

academic and occupational learning. Local communities provide youth activities and services in

partnership with the WIA American Job Center system and under the direction of local



69
   http://www.doleta.gov/programs/general_info.cfm retrieved November 23, 2014.
70
   The Dislocated Worker appropriation includes formula grants to states and funds for the national reserve fund,
which is distributed by ETA based on proposals from the states.
71
   Material on the WIA Youth program is from http://www.doleta.gov/youth_services/wiaformula.cfm retrieved
November 23, 2014.

                                                         44
Workforce Investment Boards. To participate, youth must have low income and one or more

prescribed barriers to employment. Budget authority for PY 2014 is $818 million.

        Wagner-Peyser Employment Service.72 The Wagner-Peyser Act of 1933 established a

nationwide system of public employment offices known as the Employment Service (ES). The

Act was amended in 1998 to make the Employment Service part of the One-Stop services

delivery system. The Employment Service focuses on providing a variety of employment-related

labor exchange services including but not limited to job search assistance, job referral, and

placement assistance for job seekers; and reemployment services to unemployment insurance

claimants. As described earlier, the ES also provides a variety of services to employers. The

Employment Service is open to all, but veterans receive priority of service and are eligible for

other special services; although the Employment Service is not means tested, we include it here

because many WIA customers are co-enrolled in the Employment Service program. Budget

authority for PY 2014 is $664 million.

        Senior Community Service Employment Program.73 The Senior Community Service

Employment Program (SCSEP) is a community service and work-based job training program for

older Americans. Authorized by the Older Americans Act, the program provides training for

low-income, unemployed seniors. Participants also have access to employment assistance

through American Job Centers. SCSEP participants gain work experience in a variety of

community service activities at nonprofit and public facilities, including schools, hospitals, day-

care centers, and senior centers. The program provides over 40 million community service hours

to public and nonprofit agencies, allowing them to enhance and provide needed services.

Participants work an average of 20 hours a week, and are paid the maximum of the relevant

72
   Material on the Wagner-Peyser Act Employment Service is from
http://www.doleta.gov/programs/Wagner_Peyser.cfm retrieved November 23, 2014
73
   Material on the SCSEP is from http://www.doleta.gov/seniors/ retrieved November 24, 2014.

                                                      45
federal, state or local minimum wages. This training is intended to serve as a bridge to

unsubsidized employment opportunities for participants. Participants must be at least 55 years

old, be unemployed, and have a family income of no more than 125% of the federal poverty

level. Funding for PY 2014 is $433 million.

        Trade Adjustment Assistance (TAA).74 The Trade Adjustment Assistance (TAA)

Program is a federal entitlement program that assists U.S. workers who have lost or may lose

their jobs as a result of foreign trade. This program seeks to provide adversely affected workers

with opportunities to obtain the skills, credentials, resources, and support necessary to become

reemployed. Participants are eligible to receive employment and case management services,

training, cash payments called trade readjustment allowances (TRA) when unemployment

insurance is exhausted, and job search and relocation allowances. The program also includes a

wage subsidy for up to two years that is available to reemployed older workers and covers a

portion of the difference between a worker’s new wage and their old wage (up to a specified

maximum amount). Note that this program is not means tested. Enacted budget authority for PY

2014 is $306 million.

        Employment Services for Veterans.75 The Veterans' Employment and Training Service

(VETS) offers employment and training services to eligible veterans through the Jobs for

Veterans State Grants Program. Under this grant program, funds are allocated to State Workforce

Agencies in direct proportion to the number of veterans seeking employment within their state.

The grants support two principal state workforce agency staff positions: Disabled Veterans'

Outreach Program Specialists (DVOPs) and Local Veterans' Employment Representatives


74
   Material on Trade Adjustment Assistance is from
http://www.doleta.gov/tradeact/docs/program_brochure2014.pdf.
75
   Material on Employment Services for Veterans is from
http://www.dol.gov/vets/programs/empserv/employment_services_fs.htm.

                                                    46
(LVERs). DVOP and LVER staff provides services to all eligible veterans, but their efforts are

concentrated on outreach and the provision and facilitation of direct client services to those who

have been identified as most in need of intensive employment and training assistance. Disabled

Veterans Outreach Program (DVOP) specialists provide intensive services to meet the

employment needs of disabled veterans and other eligible veterans, with the maximum emphasis

directed toward serving those who are economically or educationally disadvantaged. Local

Veterans' Employment Representatives conduct outreach to employers and engage in advocacy

efforts with hiring executives to increase employment opportunities for veterans, encourage the

hiring of disabled veterans, and generally assist veterans to gain and retain employment. LVER

staff conducts seminars for employers and job search workshops for veterans seeking

employment, and facilitate priority of service in regard to employment, training, and placement

services furnished to veterans by all staff of the employment service delivery system. Combined

enacted budget authority in PY 2014 for DVOP and LVER positions is $175 million.

           H-1B Job Training Grants.76 The Job Training for Employment in High Growth

Industries Grants are designed to provide training for workers according to need in different

sectors of the economy. The funding for this program is provided from H-1B visa fees. The

Department’s long-term goal for the program is to decrease the need for these visas by helping

American workers develop the high level skills needed by these employers. The Department

intends to use this program to support training and education models that lead to highly-skilled

technical jobs. The fees collected for this program in FY 2014 totaled $166 million.




76
     Material on H-1B Job Training Grants is from U.S. Department of Labor (2014)

                                                        47
        Migrant and Seasonal Farmworker Program.77 The Employment and Training

Administration’s Migrant and Seasonal Farmworker Program, also sometimes called the

National Farmworker Jobs Program, provides services to the American farmworker population to

help combat the chronic underemployment experienced by workers who depend primarily on

agricultural labor jobs. The National Farmworker Jobs Program provides funding to community-

based organizations and public agencies to assist migrant and seasonal farmworkers and their

families attain greater economic stability. Farmworkers also receive training and employment

services through the nationwide network of American Job Centers. Funding for PY 2014 is $82

million, which is expected to serve approximately 19,000 participants.78

        Reintegration of Ex-Offenders (RExO).79 The RExO program provides funding to pilots

and demonstration projects designed to test the effectiveness of successful models and practices

found in community and faith-based environments and other government systems that have not

been tested for their adaptability to the public workforce system. RExO is designed to strengthen

communities through projects that incorporate mentoring, job training, education, legal aid

services, and other comprehensive transitional services. Grants are awarded through a

competitive process open to any nonprofit organization with 501(c)(3) status, unit of state or

local government, or any Indian and Native American entity eligible for grants under Workforce

Investment Act Section 166 in areas with high poverty and crime rates that meet the

requirements of the solicitations. Enacted budget authority for the program in PY 2014 is $80

million.




77
   Material on the Migrant and Seasonal Farmworker Program is from http://www.doleta.gov/Farmworker/ retrieved
November 23, 2014.
78
   Number of participants expected and funding level are from U.S. Department of Labor (2014).
79
   Material on RExO is from http://www.doleta.gov/RExO/.

                                                     48
        YouthBuild.80 YouthBuild is a community-based alternative education program that

provides job training and educational opportunities for at-risk youth ages 16-24. The program

was transferred from the Department of Housing and Urban Development to the Department of

Labor in 2006. Youth learn construction skills while constructing or rehabilitating affordable

housing for low-income or homeless families in their own neighborhoods. Youth split their time

between the construction site and the classroom, where they earn their GED or high school

diploma, learn to be community leaders, and prepare for college and other postsecondary training

opportunities. YouthBuild includes significant support systems, such as a mentoring, follow-up

education, employment, and personal counseling services; and participation in community

service and civic engagement. There are over 250 DOL funded YouthBuild programs in 45 states

serving over 10,000 youth per year.81 Funding for YouthBuild in PY 2014 is $78 million.

        The Indian and Native American Program.82 The Indian and Native American Program

serves American Indians and Native Americans through a network of 178 grantees. To meet the

employment and training needs of the Indian, Alaskan Natives, and Native Hawaiian

populations, the enacted budget authority for PY 2014 is $46 million. At this funding level, the

program serves approximately 28,000 unemployed and under-skilled Indian, Alaskan Native,

and Native Hawaiian adults and youth.

        Homeless Veterans Reintegration Program.83 The purpose of the Homeless Veterans'

Reintegration Program (HVRP) is to provide services to assist in reintegrating homeless veterans

into meaningful employment within the labor force and to stimulate the development of effective


80
   Material on YouthBuild is from http://www.doleta.gov/Youth_services/Youth_Build.cfm retrieved November 23,
2014
81
   The data on enrollment and the number of program sites is from YouthBuild’s web site, https://youthbuild.org/;
the numbers are higher than what is found on the Department of Labor web site.
82
   Material on the Indian and Native American Program is from U.S. department of Labor (2014).
83
   Material on the Homeless Veterans Reintegration Program is from
http://www.dol.gov/vets/programs/fact/Homeless_veterans_fs04.html.

                                                       49
service delivery systems that will address the complex problems facing homeless veterans. Funds

are awarded on a competitive basis to eligible applicants. Grantees provide an array of services

utilizing a case management approach that directly assists homeless veterans as well as critical

linkages for a variety of supportive services available in their local communities. The program is

"employment focused," and veterans receive the employment and training services they need in

order to re-enter the labor force. Job placement, training, job development, career counseling,

and resume preparation, are among the services that are provided. Supportive services such as

clothing; provision of or referral to temporary, transitional, and permanent housing; referral to

medical and substance abuse treatment; and transportation assistance are also provided to meet

the needs of this target group. Budget authority for the program in FY 2014 is $38 million.

         Employment and Training Programs Operated by Other Agencies

         Pell Grants (U.S. Department of Education).84 The Pell Grant program provides need-

based grants to low-income undergraduate and certain post-baccalaureate students to promote

access to postsecondary education.85 Students may use their grants at any one of approximately

5,400 participating postsecondary institutions. Grant amounts are determined by the student's

expected family contribution, the cost of attendance (as determined by the institution), the

student's enrollment status (full-time or part-time), and whether the student attends for a full

academic year or less. The maximum Pell Grant for the award year beginning July 1, 2015 is

$5,775.86 Financial need is determined by the U.S. Department of Education. Pell Grants may be

used at two-year and four-year institutions, and at public and private institutions. Students can

receive Pell Grants for study toward a degree or eligible certificate programs. The funds can be


84
   Information on Pell grants is from http://www2.ed.gov/programs/fpg/index.html.
85
   Pell Grants to students pursuing post-baccalaureate study are rare and restricted to students pursuing a teaching
certificate.
86
   From https://studentaid.ed.gov/types/grants-scholarships/pell accessed February 1, 2015.

                                                          50
used to pay for tuition, fees, and other expenses, or to help pay for the living expenses of the

student. Although the program is not an entitlement, sufficient funds are generally appropriated

to meet the needs of all eligible applicants. For the 2013-2014 school year, the latest period for

which data are available, an estimated $33.7 billion was spent on Pell Grants, according to

Baum, Elliott, and Ma (2014). Analysis of National Postsecondary Student Aid Survey (NPSAS)

indicates that $5,060 million was used for occupational degrees and another $3,121 million was

used for certificate programs in the 2011-2012 school year.87 Thus, Pell Grant spending is the

largest single source of funding for means-tested employment and training programs, and it is

roughly twice the spending for the WIA Adult, Dislocated Worker, and Youth programs

combined.88

        Temporary Assistance for Needy Families (TANF) (U.S. Department of Health and

Human Services).89 TANF is the federal welfare program for families with children. Under

TANF, the federal government provides block grants to the states, which use these funds to

operate their own programs. In order to receive federal funds, states must also spend some of

their own dollars on programs for needy families. States can use federal TANF and state

maintenance of effort (MOE) dollars to meet any of the four goals set out in the 1996 law: “(1)

provide assistance to needy families so that children may be cared for in their own homes or in

the homes of relatives; (2) end the dependence of needy parents on government benefits by

promoting job preparation, work, and marriage; (3) prevent and reduce the incidence of out–of-

87
   We are extremely grateful to Sandy Baum for performing the analyses generating these estimates. The
Department of Education’s programs that are classified as career and technical education can be found at
http://nces.ed.gov/surveys/ctes/tables/postsec_tax.asp accessed March 1, 2015.
88
   Pell Grants can be used for WIA participants if the income and program requirements are met, and the WIA
statute required that funding sources such as Pell Grants be used when possible. However, training programs offered
by WIA are often too short to qualify for Pell Grants, and dislocated workers enrolled in WIA training programs
may not meet the income requirements. For the period from April 1, 2012 through March 31, 2013, Social Policy
Research Associates (2013) reports that out of 1.6 million exiters, 17,246 Adult programs participants, 10,836
Dislocated Worker participants, and 14,115 youth received Pell Grants.
89
   Material on TANF is from Center on Budget and Policy Priorities (2012).

                                                        51
wedlock pregnancies and establish annual numerical goals for preventing and reducing the

incidence of these pregnancies; and (4) encourage the formation and maintenance of two parent

families.” The 1996 law sets forth 12 categories of work activities that can count toward the

required work rates. Nine of these 12 categories are core categories that can count toward any

hours of participation; participation in the three non-core categories can only count if the

individual also participates in core activities for at least 20 hours per week (30 hours for two-

parent families). The nine core activities are: Unsubsidized employment, subsidized private-

sector employment, subsidized public-sector employment, work experience, on-the-job training,

job search and job readiness assistance, community service programs, vocational educational

training (for up to 12 months), and providing child care services to an individual who is

participating in a community service program. The three non-core activities are: Job skills

training directly related to employment, education directly related to employment, and

satisfactory attendance at secondary school or in a course of study leading to a GED. Federal

expenditures for work-related TANF activities for FY 2013 were $1,517 million.

       Adult Education Basic Grants to States (U.S. Department of Education). This program

provides grants to states to fund local programs of adult education and literacy services,

including workplace literacy services, family literacy services, English literacy programs, and

integrated English literacy-civics education programs. Participation in these programs is limited

to adults and out-of-school youths age 16 and older who are not enrolled or required to be

enrolled in secondary school under state law. More than 2,500 programs deliver instruction

through public schools, community colleges, libraries, and community-based organizations, and

other providers. The programs provide instruction in reading, numeracy, GED preparation, and




                                                 52
English literacy. More than 1.8 million adults participated in programs in program year 2011-12.

The appropriation for the program for FY 2014 is $564 million.

          Supplemental Nutrition Assistance Program Employment and Training (SNAP E&T)

(U.S. Department of Agriculture).90 SNAP E&T is a funding source that allows states to provide

employment and training and related supportive services to individuals receiving Supplemental

Nutrition Assistance Program (SNAP) benefits. These services are intended to assist recipients in

gaining skills, training, work, or experience that will increase their employment and earnings and

reduce their need for SNAP. In an average month in FY 2013, more than 47 million individuals

received SNAP benefits; however, in 2012, the most recent year for which data are available,

only 15.3 percent of non-elderly adult SNAP recipients participated in SNAP E&T activities.

SNAP E&T supports a range of employment and training activities for SNAP recipients. Such

activities can include job search, job search training, work experience or workfare, and education

and training including basic skills instruction. Employability assessments and case management

services can be part of a component but cannot be stand-alone activities. SNAP E&T can also be

used to provide job retention services for up to 90 days after an individual who received other

services under SNAP E&T gains employment. The 2013 appropriation for the program was $416

million.



Program evaluation issues

Employment and training programs in the United States have received more attention from

evaluators than many programs far larger in budgetary terms, such as SNAP or Social Security

Disability Insurance (SSDI). Relatively large participant populations as well as available

administrative data plus the absence of a constituency powerful enough to block serious
90
     Material on SNAP E&T is from Lower-Basch (2014).

                                                        53
evaluation conspire to make this so.91 We have (much) more to say about the substantive

findings from that evaluative activity later on. In this section, we lay the foundation for our

substantive discussion by describing the fundamental evaluation problem, along with the usual

approaches to solving it, both in the broad sense of evaluation policy and in the narrow sense of

applied econometrics. Our discussion presumes voluntary rather than mandatory participation

because almost all participants in the major (and most minor) US employment and training

programs volunteer for the privilege.

         To fix ideas, it helps to adopt some formal notation. We use the standard potential

outcomes framework, in which          Y1i denotes the outcome that individual “i” would experience if

she received program services (and so is “treated” in the jargon of the literature), while                Y0i

denotes the outcome that same individual would receive if she did not receive program services

(and so is not “treated”).92 The outcome remains generic at this point; it could be earnings,

employment, an indicator for obtaining a job with employer-provided health insurance, etc. The

term “potential” outcomes refers to the fact that each individual will actually experience only the

outcome associated with their program participation status, while the other outcome remains an

unrealized, and thus unobserved, potential. If we define            Di as an indicator variable for program

participation, then we can write the observed outcome             Yi as a function of the potential

outcomes:

Yi  DY
      i i1  (1  Di )Y0i

91
   Perusing the last print version of the Digest of the Social Experiments, Greenberg and Shroder (2004), suggests a
strong revealed preference for experimenting on disadvantaged people and criminals and a strong revealed
preference against experimenting on the middle class. Greenberg, Shroder and Onstott (1999) provide some
quantitative confirmation of these patterns based on an earlier edition of the book.
92
   The potential outcomes framework is variously attributed to Frost (1920), Neyman (1923), Fisher (1935), Roy
(1951), Quandt (1972) and Rubin (1974). Disciplinary affiliation and academic genealogy strongly predict
attributions.

                                                         54
The impact of the program on individual “i” equals the difference between their treated and

untreated potential outcomes, or   i  Y1i  Y0i .

       Interest in impact evaluations centers on various means of these individual impacts. The

most common parameter of interest in practice is the average impact of the treatment on the

treated, given by E (Y1  Y0 | D  1) . In words, this parameter captures the mean difference

between the outcome with treatment and the outcome without treatment for those who receive

the treatment. If the policy question at hand concerns whether to keep or eliminate a program as

it currently operates, versions of this parameter for different outcomes lie on the benefit side of

the relevant cost-benefit calculation. A second parameter of interest is the average treatment

effect in the population, or ATE. In notation, we have     E (Y1  Y0 ) . Typically, interest lies in the

ATE for some actual or potential eligible population. Versions of this parameter for various

outcomes figure in a cost-benefit calculation designed to answer the question of whether or not a

mandatory version of program makes sense from an (economic) efficiency point of view.

       Less frequently, evaluations of job training programs consider other parameters of

interest, which in turn address other substantive questions of interest. Quantile treatment effects

(QTEs) reveal how a treatment affects the entire distribution of outcomes. In practice, they

consist of differences between the corresponding quantiles of the outcome distributions for the

treated and (corrected for selection, if required) untreated units, i.e. differences of quantiles of

F (Y1 | D = 1) and F (Y0 | D = 1) . Thus, for example, with experimental data, the QTE at the

median is the difference between the median outcome in the treatment group and the median

outcome in the control group. They also provide information about how the program affects

inequality within the treated population and, as in Bitler, Gelbach, and Hoynes (2006), they can


                                                      55
even play a role in testing theoretical models of participant behavior. We find them surprisingly

underutilized in practice in evaluations of job training programs.93

        Another question of frequent policy interest concerns the impact of programs on

participants at the margin of participation, where the margin may depend on the choices of

would-be participants, of program staff, or both. The effect on marginal participants informs

choices about whether to modestly expand or contract the program. Pinning down effects on

marginal participants requires additional work at the design stage and/or additional measurement.

That additional work enables estimation of impacts at the margin as in the discontinuity design

exploited in Black, Smith, Berger, and Noel (2003), or of local average treatment effect in a

randomized encouragement design, or a subgroup analysis of impacts on likely marginal

participants as identified by program staff or the participants themselves. In our view, such work

gets done far too infrequently in this literature, given that the relevant policy question (at least

implicitly) is almost always expansion or contraction rather than eliminating the program or

making it mandatory.

        The final parameters of interest require the joint distribution of the treated and untreated

outcomes rather than just their marginal distributions. Examples of such parameters include the

variance of impacts, the fraction of impacts that are positive, and quantiles of the distribution of

impacts (which are not the same thing as impacts on quantiles of the outcome distribution).

Because (perhaps wrongly) the literature rarely analyzes these parameters in practice, we deem

them beyond the scope of our chapter; see Heckman, Smith, and Clements (1997) and Djebbari

and Smith (2008) for more.

        One path to avoid all of the conceptual and econometric complications associated with


93
  For more on QTEs see e.g. Koenker and Bassett (1978), Heckman, Smith and Clements (1997), Abadie, Angrist
and Imbens (2002), Bitler, Gelbach and Hoynes (2005), and Djebbari and Smith (2008).

                                                     56
alternative treatment effect parameters leads to the common effect model. Much of the applied

literature, especially the older applied literature, implicitly assumes a common effect world, in

which “the effect” of training is the same for all participants. A more sophisticated version of

this view allows that the effect of training varies, but assumes that neither potential participants

nor program gatekeepers can predict the variation, with the result that it plays no role in program

participation decisions. In our view, the available evidence militates strongly against the

common effect view, particularly in the context of training programs as operated in the United

States.

          One very compelling reason for thinking that there are heterogeneous treatment effects is

that there are, almost always in the U.S. program context, heterogeneous treatments. In this

sense, the programs covered in this chapter differ from both the budgetary treatments (e.g. the

Earned Income Tax Credit) and the cash and in-kind transfer programs (e.g. Temporary

Assistance for Needy Families) considered in the other chapters of this volume. Coding up an

indicator variable for receipt of training or, even more dramatically, receipt of any services from

some employment and training program, implicitly disguises a substantial amount of

heterogeneity in the program as experienced by participants. One trainee may take a community

college course in cosmetology, while another takes a course from the Salvation Army in

computer repair, and still another receives subsidized on-the-job training at Whataburger. More

broadly, some participants may receive instruction designed to prepare them to obtain a GED,

while others receive only job search assistance. Some evaluations distinguish among broad

categories of services, such as classroom training or job search assistance94, but as the examples


94
  For instance, Hotz, Imbens, and Klerman (2006) apply non-experimental methods to the experimental data from
the California Greater Avenues to Independence experiment in order to disentangle the effects of particular service
types.


                                                         57
just listed illustrate, most programs embody substantial heterogeneity even within broad service

categories.

       In addition to heterogeneous services, programs operate in heterogeneous contexts in

terms of aggregate labor market conditions, industry and occupation mix, and so on. Lechner and

Wunsch (2009) and Heinrich and Mueser (2014), among others, provide evidence that the effect

of training varies with local labor market conditions.

       The literature also offers a long history of estimated differences in impacts between

different demographic groups and participants at different sites. For instance, LaLonde (2003)

describes the durable finding (in the US literature) that adult women benefit the most from

training, followed by adult men, followed by male and female youth. We have more to say about

subgroup effects below; for our purposes here, differences by age, sex, and site suggest the

presence of differences on other unmeasured dimensions as well.

       As another argument for heterogeneity in treatment effects, think about the relationship

between impacts on earnings and employment over some period during and after training. If

some participants have zero earnings, but the program has a non-zero mean impact, then some

heterogeneity in impacts must exist, as the participants with zero earnings must have had a zero

or negative impact (because earnings are bounded below by zero) while some other participants

had positive impacts. Finally, and a bit more technically, as noted in Heckman, Smith, and

Clements (1997) it is possible to place an empirical lower bound on the impact variance. This

lower bound corresponds to the variance of the quantile treatment effects described above.

Heckman, Smith and Clements (1997) calculate this lower bound for the adult women in the Job

Training Partnership Act experiment and find that it is statistically and (what is more important)

substantively different from zero. Taken together, we find the case for heterogeneous treatment



                                                58
effects that substantively matter quite compelling, and assume them in all that follows.

         To motivate the problem of non-random selection into programs, it helps to think about a

simple model of program participation. We draw here on the models in Heckman and Robb

(1985) and Heckman, LaLonde, and Smith (1999). We begin with a simple model in which

training is available in only one period, lasts exactly one period, and is not announced in

advance. This allows us to view the participation choice as a static problem, though one with

dynamic implications. Call the period of program availability period k.

         In periods prior to k, all individuals have the outcome function

 Y0it  X it  X  i   it for t  k .

Here   X it denotes various determinants of outcomes unaffected by treatment, with an associated

vector of coefficients         X , i   denotes the time-invariant unobserved component of outcomes for

individual “i,” and     it   denotes the transitory component of outcomes for person “i” in period “t.”

After period k, the same function persists, but with the addition of an additive treatment effect

received only by participants. In notation

Yit  X it  X  Di  Di  i   it for t  k ,

with

Y1it  Y0it   Di .

The “i” subscript on       Di    captures heterogeneity in the treatment effect; we assume for

simplicity that the heterogeneous effect persists indefinitely. Potential trainees may know their

treatment effect, or not know it, or something in between.95 Extending the model to allow the


95
  See Carniero, Hansen, and Heckman (2003) for an analysis that estimates the fraction of the variation in treatment
effects known ex ante in an educational context.

                                                         59
treatment effect to vary with observed characteristics – to capture systematic heterogeneity in

treatment effects in the terminology of Djebbari and Smith (2008) – via interaction terms follows

easily.

          As unobserved variables that affect outcomes may vary over time at lower frequency than

the observed data, we allow for serial correlation in the “error” term; in particular, we assume for

convenience an autoregressive form, with

 it   i ,t 1  vit

where     vit is an independently and identically distributed (over time and people) shock and

1    1 to keep the process from diverging.

          The participation decision depends on a comparison of costs and benefits. The benefit

comes in the form of the discounted present value of the stream of future treatment effects. The

costs come in the form of direct costs   Ci , which may include tuition, transportation, or books as

well as negative direct costs in the form of subsidies to participation, and the opportunity cost of

having    Yik = 0 during the training period. Formally, the potential participant calculates

         Di
Di*            Ci  Y0ik  ui
          r

where r denotes the interest rate,   ui denotes unobserved factors affecting the net utility of

training, and we make the simplifying assumption that the potential trainee lives forever. We do

not make costs a function of observed characteristics, but it would be easy and reasonable to do

so. If D *  0 then the individual chooses to participate in period k while if D *  0 the individual

forgoes the opportunity and continues to receive     Y0it in all future periods.

          What do we learn from this model? First, opportunity costs play a key role. The fact that


                                                   60
the untreated outcome in period k enters the decision problem along with the direct costs and the

discounted impacts means that individuals who choose to train will have differentially low

values of   Y0k . Those low values in period k can result from a low value of the time invariant

unobserved component of earnings, a low value of the transitory component, and/or values of X

associated with low earnings. Thus, to the extent that the time-invariant unobserved component

accounts for a substantial amount of the variation in earnings for the relevant population, we

would expect substantively important selection on it, with trainees having a lower average value

than non-trainees. This selection will lead to persistent differences in the mean earnings of

participants and non-participants both before and after period k. If this were the only source of

selection into training, researchers would naturally gravitate toward difference-in-differences and

related longitudinal estimators of program impact. In practice (more about this below) such

estimators play only a minor role in this literature, because of empirically important selection on

the transitory unobserved component as well. Due to the assumed serial correlation in the

transitory unobserved component, selection on this component operating via selection on the

opportunity cost of participation leads to the empirically ubiquitous “Ashenfelter dip,” first

identified in this literature in Ashenfelter’s (1978) paper on MDTA, the programmatic great-

great-grandparent of WIOA. This combination of selection on both transitory and more

permanent components of the outcome process complicates credible estimation of the causal

effects of training.

        In addition to opportunity costs, participation depends on direct costs and on the person-

specific impact of training. Though some literatures make intensive use of direct cost measures

as sources of exogenous variation in participation, the training literature has not seen much work

along these lines. The training literature does pay close attention to heterogeneous treatment


                                                  61
effects. As noted in Heckman, LaLonde, and Smith (1999), heterogeneous treatment effects

uncorrelated with other factors affecting participation and outcomes act like (partial) random

assignment and so reduce the difficulty of the selection problem; the empirical relevance of that

observation remains largely unexplored. The simple model also illustrates why we would expect

participants to have different impacts than non-participants. Indeed, with impacts known in

advance, and conditional on particular values of direct costs and opportunity costs, everyone who

participates has a higher impact than everyone who does not participate. Even impacts estimated

without bias but with some uncertainty prior to the training choice imply that the ATET > ATE >

ATNT, where the ATNT is the average treatment effect on the non-treated. The ATE is, of

course, just a weighted average of the ATET and ATNT and so must lie between them. More ex

ante uncertainty about individual impacts weakens this pattern. Also clear from the model is the

fact that the ATET provides an upper bound, and possibly a distant upper bound, on the

treatment effects that would be experienced by marginal participants enticed into the program by

a small reduction in the direct costs C.96

        Of course, as we intend the name “simple model” to signal, this model leaves a lot out,

even as it also captures quite clearly the key factors that make compelling non-experimental

estimation of the impacts of training programs challenging to evaluators. One omission from the

model concerns the underlying behavioral foundations of the Ashenfelter dip. In the model, the

dip results from the fact that individuals select into training based in part on having a low

opportunity cost; this results, via the serial correlation in  , in many individual participants

having a gentle decline in their mean earnings in the periods prior to period k. In reality, almost

no one experiences this gentle decline. Instead, most individuals have a sharp drop in earnings at


96
  See the Monte Carlo study that builds on this model in Section 8 of Heckman, LaLonde, and Smith (1999) for
further analyses and intuition.

                                                      62
a discrete point in time due to job loss. The smooth dip observed in the aggregate results from

the averaging of these sharp falls, which become more common as period k approaches.

         Another omission from the model concerns selection on earnings trajectories, as opposed

to just selection on earnings shocks (i.e., on  ) and on persistent differences in earnings levels

(i.e., on and/or X). By way of illustration, consider two scenarios. In one scenario, some

individuals select into training when they decide to get serious about life, or at least about the

labor market. In another scenario, some individuals select into training because they have lost a

job in which their pay well exceeded the value of their marginal product in the other jobs

available to them. In many industries the number of such jobs has declined over time due to freer

trade or deregulation; as a result those who lose such jobs often experience persistent earnings

decreases, as in Jacobson, LaLonde, and Sullivan (1993).97 We will return to this scenario below

when we discuss the evidence on the WIA dislocated worker funding stream.98

         Our simple model also omits the justifications for government intervention in the training

market discussed at the beginning of the chapter. One could easily have the direct costs C reflect

a government subsidy, which would capture the justification from credit constraints but the

model still ignores any of the informational justifications for government intervention. First, the

government may have better information on the state of the labor market (the “labor market

information” aspect of active labor market programs) for particular occupations. This can help

the would-be trainee choose between skill upgrading in their existing occupation (or just more

effective search in that market) and investing in human capital associated with a new occupation.

The government, via knowledgeable and experienced caseworkers armed with standardized tests,


97
   See Krolikowski (2014) for a new look at displaced workers through the lens of the dynamic treatment effect
literature.
98
   The data from the JTPA experiment (described below) imply little selection on earnings trajectories for adults, but
modest amounts for youth. See Heckman and Smith (1999).

                                                         63
may have a better sense of how a given participant’s skills and interests match up to particular

occupations. This allows more effective investments in training and more effective job search.

Finally, the government, again via the caseworkers, may provide quality signals to firms looking

for workers to hire into subsidized on-the-job training slots. Because programs recommend only

a subset of their participants for these slots, and because they are engaged in a repeated game

with individual employers in which reputation matters on both sides, it can make these signals

credible in a way that the workers themselves cannot. The literature on government sponsored

job-training lacks formal models capturing these aspects of the process, though the directed

search model in Plesca’s (2010) equilibrium analysis of the Employment Service implicitly

captures some of them by having different matching technologies for workers who search via the

ES than for workers who search on their own.

       The simple model above also completely ignores the supply side of the “market” for

services provided by government employment and training programs. Instead, it focuses solely

on the participation decision facing the potential trainee. We rectify this omission in the model

later in our discussion of performance management, which plays a very important role in shaping

the supply responses of the major U.S. employment and training programs. Finally, the model

discussed here ignores general equilibrium effects (i.e. effects on those not participating in the

program); we discuss those later in the context of cost-benefit analysis.

       The standard theory, along with empirical evidence from both experimental and non-

experimental studies, strongly indicates selection into employment and training programs based

on both transitory and relatively more permanent components of outcomes. The literature that

evaluates employment and training programs in the United States has adopted a variety of data

sources, identification strategies, and econometric estimators to deal with the problem of non-



                                                 64
random selection into programs. Indeed, as we will have some occasion to note, this literature

has played an important role in the evolution of applied econometric methods more broadly. We

turn now to a limited methodological review, emphasizing those identification strategies and

related estimators and data sets, most commonly used in the U.S. literature, namely random

assignment and “selection on observed variables.” We focus almost exclusively on impact

evaluations (we are economists after all) but note that, in our experience, well-designed and

executed process and implementation evaluations are important complements to econometric

impact evaluations.

        Unlike every other country, at least until the last decade or so, the United States

sometimes evaluates its employment and training programs using random assignment designs.99

In terms of our notation, random assignment as typically implemented involves taking a sample

of would-be participants, i.e. D  1 individuals, and randomly forcing some of them to

experience the untreated outcome        Y0 (the experimental control group) while randomly allowing

others to experience the treated outcome         Y1 (the experimental treatment group). Randomly

assigning treatment assures (statistically) equivalent distributions of all the relevant variables

(i.e. X,  ,  , C, and u) in the two groups. As a result, a simple comparison of means provides a

consistent (in the statistical sense) and compelling estimate of the ATET.

        Running experimental evaluations and meaningfully interpreting the resulting data in the

real world differs from the pleasant but over-simplified description in the preceding paragraph.

We briefly note a subset of the issues here, focusing on those most important to the literature


99
  Experimental evaluations of labor market programs outside the U.S. include the Self-Sufficiency Project in
Canada described in Ford et al. (2003), the UK Employment Retention and Advancement Demonstration
documented in Hendra et al. (2011), caseworker experiments in Denmark evaluated in Pederson, Rosholm and
Svarer (2012), and a very impressive multi-level randomized evaluation in France recounted in Crépon et al. (2013).
White and Lakey (1992), who evaluate the UK RESTART program, provide a rare exception to our general claim.

                                                        65
whose evidence we review later in this chapter, and starting with randomization bias.100

        Randomization bias means bias induced by the presence of an experimental evaluation. It

is bias relative to the population value of the impact parameter of interest in a world without

randomization, i.e. in the world of the program as it normally operates. Consider the following

examples: First, the presence of random assignment may change the participant population

because potential participants on the margin of participation may find it optimal to pay the fixed

cost of attempting to participate in the absence of random assignment, but not in its presence,

because the possibility of randomization into the control group reduces the expected value of the

attempt. Second, as noted in Heckman and Smith (1995), the presence of randomization may

lead individuals to change their behavior even if they do still choose to participate, as when

participants reduce pre-random-assignment investments complementary to the treatment due to

the uncertainty of receiving it. Third, the institutional trappings associated with randomized

evaluations, but not generally with non-experimental evaluations, may lead to differences

between the participant population of interest and that in the evaluation, due, for example to

selective removal of those put off by signing consent forms. Sianesi (2014) documents the

empirical importance of this behavior, and of the resulting bias, in the context of the evaluation

of a program providing on-going support for unemployed workers who find a job in the United

Kingdom. Fourth, randomization will affect the scale of program intake and thereby lead to

differences between the population served by the program as it normally operates and during the

randomized evaluation. For example, in the Job Training Partnership Act experiment (described

in more detail in the results section), sites were instructed to keep the number of individuals they

served the same during the evaluation, so as to avoid randomization bias due to a change in


100
  For more on social experiments see e.g. Ferber and Hirsch (1981), Heckman (1992), Burtless and Orr (1995),
Heckman and Smith (1995), Orr (1998), and Heckman, LaLonde, and Smith (1999, Section 5).

                                                      66
program scale. But this stricture, coupled with a 2:1 random assignment ratio, meant that sites

had to recruit a substantially larger number of potential participants than they normally would.

Indeed, this requirement played a role in the site selection difficulties we discuss in the next

paragraph, because many sites worried about the quality of the marginal participants drawn in as

part of the larger pool of potential participants.101

        In multi-site programs (like JTPA, WIA, WIOA, and the Job Corps), random assignment

can increase the per-site costs of the evaluation and can complicate external validity by making

sites more reluctant to participate due to the disruptions in normal program operation

necessitated by random assignment. For example, as detailed in e.g. Hotz (1992) and Doolittle

and Traeger (1990), the JTPA experimental evaluation’s attempt to recruit a random sample of

sites that would allow compelling generalization to the population of sites failed miserably. In

the end, and after non-trivial side payments plus some design compromises, a non-random

sample of 16 sites was obtained.102, 103 The result was controversy regarding the external validity

of the experimental findings; see e.g. Heckman and Smith (2000) or Heckman and Krueger

(2003). While the literature offers various strategies for generalizing from non-random samples

of sites, these strategies remain controversial and thus inferior to including all sites or evaluating

at a random sample of sites.104

        Finally, and the best documented (if not necessarily the most important) empirically, we




101
    One member of the design team (Barnow) for the JTPA evaluation suggested having sites identify the marginal
participants; this was not done in that study but is being done in the WIA experimental evaluation described below.
102
    The least attractive design compromise allowed the experimental sites to provide control group members with a
list of alternative service providers in the community, thereby increasing substitution and muddying the
interpretation of the counterfactual. At one site, this list ran to over 10 pages!
103
    Section 5.II of Doolittle and Traeger (1990) makes the positive (but weak in our view) case for the
representativeness of the sites in the JTPA evaluation.
104
    See e.g. Hotz, Imbens, and Mortimer (2005), Gechter (2014), Muller (2015), or Vivalt (2015).

                                                        67
may have treatment group dropout105 and control group substitution. In an ideal experiment,

everyone randomized into the experimental treatment group would receive treatment, and no one

in the experimental control group would receive treatment. In this pure case, the experimental

contrast clearly represents the causal effect of receipt of treatment rather than no treatment for

the relevant population. In practice, because of institutional factors, as well as evaluation design

choices and the sometimes chaotic lives of the individuals who participate, or consider

participating, in active labor market programs, real experiments rarely look this clean. Heckman,

Hohmann, Smith, and Khoo (2000) document the empirical relevance of dropout and substitution

for a variety of experimental evaluations of active labor market programs, with a particular focus

on the JTPA evaluation.

        Three factors appear particularly important in explaining the extent of dropout and

substitution in evaluations of employment and training programs in the United States. The first

factor is the extremely decentralized nature of the provision of employment and training

programs. Many federal government programs have an employment and training component, as

do some state programs and many nonprofit social service organizations. This means that would-

be trainees who get randomized out of one training opportunity can easily find others. For

instance, because community colleges provide much of the training in WIA (and now WIOA)

control group members can easily enroll in the same or similar courses on their own. Second,

high intensity, expensive programs tend to have low rates of dropout, presumably because they


105
   Our usage follows that of Heckman, Smith, and Taber (1998), who denote as “dropouts” those individuals
randomly assigned to the treatment group in the JTPA experiment who never enroll in the program. This usage
makes more sense in their context than it might seem at first blush because, as documented in Kemple, Doolittle and
Wallace (1993), many treatment group members received (typically low intensity) services without formal
enrollment for reasons related to the JTPA performance management system. More generally, the literature tries to
capture variation in the extent of treatment among those with some contact with a program in a variety of ways, such
as categorizing individuals who receive no substantive services as “no-shows” or by estimating a “dose-response”
function that links outcomes to the amount of service received, or via the related notion of different impacts for
different combinations or sequences of services.

                                                        68
appear valuable to potential participants, and low rates of substitution, because the supply of

substitutes is far smaller for expensive services than for inexpensive ones. For example, both the

Supported Work Demonstration and the Job Corps evaluation have very low rates of dropout and

substitution: 0.05 and 0.11 for the former and 0.28 and 0.02 for the latter.106 Both are quite

expensive. In contrast, the more modest services on offer in the JTPA evaluation elicited

substantial rates of both substitution and dropout. For example, according to Table II of

Heckman, Hohmann, Smith, and Khoo (2000), among adult women recommended to receive

classroom training in occupational skills, 48.8 percent of the treatment group actually did so,

compared to 27.4 percent of the control group (who received it from other sources, or from the

same sources with different funding). Finally, the experimental design itself can affect the

amount of treatment group dropout via its interaction with the process of program participation.

For instance, random assignment in the JTPA evaluation took place at the JTPA office rather

than at the service provider locations for cost reasons, but doing so introduced a temporal wedge

between assignment and service receipt that allowed some treatment group members time to find

a job or wind up in jail or just get dissatisfied with the services offered to them.

        Researchers typically adopt one of two strategies in the presence of dropout and/or

substitution. The first strategy redefines the parameter of interest to represent the average effect

of the offer of treatment (sometimes called the “intention to treat” or ITT), relative to a (possibly

complicated) counterfactual, rather than the ATET. For example, one can think of the

experimental contrast in the JTPA study as between a treatment group with access to all of the

various treatment options in the community including JTPA, and a control group with access to


106
   See Table I of Heckman, Hohmann, Smith, and Khoo (2000) for NSW and Table 2 of Schochet, Burghardt, and
McConnell (2008) for the Job Corps study. For the Job Corps, the substitution number includes only crossovers who
actually received Job Corps despite randomization into the control group; the fraction of the control group that
received some sort of educational treatment was about 72 percent.

                                                       69
just the options other than JTPA. This represents a reasonable causal parameter, but also one

quite different in substantive interpretation from treatment versus no treatment. The second

strategy rescales the experimental difference in outcomes by the difference in the probability of

treatment between the treatment group and the control group. The resulting estimand represents

the mean impact of treatment on the treated when the experiment features dropout but not

substitution, and a Local Average Treatment Effect (LATE) when both are present.107 The LATE

gives the mean effect of treatment on those induced to participate as a result of ending up in the

treatment group rather than the control group, whom the literature calls “compliers.” It says

nothing about the mean impact on those who would get treated in either state, the so-called

“always-takers” in the language of Angrist, Imbens, and Rubin (1996). The LATE is also a

reasonable and often interesting causal estimand, but it differs from both the ATET and from the

ITT. Again, comparisons with non-experimental estimates of the ATET require care.108

        The decade since LaLonde (2003) has seen a combination of triumphalism and humility

among advocates for greater social experimentation. The triumphalism comes from the rapid

movement of policy-relevant random assignment designs into development economics and into

education, and the broader “credibility revolution” described by Angrist and Pischke (2010) and

the related enthusiasm for “evidence-based policy.”109 See, e.g., Gueron and Ralston (2013) and

Institute for Education Sciences (2008) for more of this view. At the same time, the practice of

randomized evaluations has become much more nuanced, with greater attention to the role of

dropout and substitution, to the importance of careful definition and interpretation of the

estimand in the context of heterogeneous treatment effects, and to the fact that random

107
    The LATE is called the “Complier Average Causal Effect” or CACE in some literatures.
108
    For more on these issues see e.g. Bloom (1984), Heckman, Smith, and Taber (1998), or Heckman, Hohmann,
Smith, and Khoo (2000).
109
    One always wonders what it was they were doing before “evidence-based policy.” It is probably best not to think
too hard about that.

                                                        70
assignment does not magically overcome the general problems of either empirical research (e.g.

outliers and so on, see Heckman and Smith, 2000) or of partial equilibrium evaluation (see our

discussion of general equilibrium issues below). Also relevant in this sense is the pendulum in

economics swinging back toward a balanced approach that emphasizes both the depth of the

economics and the quality of the identification strategy, a view that naturally sees experiments

and “structure” as complements as in, e.g., Todd and Wolpin (2006), rather than as substitutes.

       We aim here to walk down a middle road on random assignment, avoiding excessive

cheerleading and excessive cynicism, both of which one can find in e.g. the current

methodological debate in development economics. Instead, we view the deliberate creation of

high-quality exogenous variation as an important complement to the other activities that

economists and other evaluators of programs undertake. We think the literature needs more, and

more thoughtful, use of randomization.

       The continued flourishing of experimental evaluation has coincided with ongoing

progress in non-experimental evaluation, spurred on in part by improvements in the available

data (particularly in Europe, but also to some extent in the United States) and in part by

developments (and rediscoveries) in the realm of applied econometrics. Methods for solving the

selection problem via conditioning on observed covariates consume most of our attention, as

they do most of this literature. We then briefly remark on developments and applications of other

identification strategies, such as discontinuity designs.

       Selection on observed variables identification strategies attempt to solve the problem of

non-random selection into training (or a program more generally) via conditioning on a

sufficiently rich set of observed covariates. Put differently, under this strategy the researcher tries

to make the case that they have observed all the variables, or good proxies for all the variables,



                                                  71
that affect both participation in training and outcomes in the absence of training. In formal

notation, the researcher assumes either E (    | X , D)  0 in the case of parametric linear

regression, or E (    | X , D  1)  E (    | X , D  0) for matching and weighting estimators.

Depending on the researcher, this assumption might get called the “conditional independence

assumption” (CIA), “exogeneity” or, to use the awkward term contributed by the statistics

literature, “unconfoundedness.”

        Making these assumptions is easy; making a compelling case for them is not. The

literature has responded to the task of learning what variables suffice for the conditional

independence assumption in three ways. First, some studies implicitly adopt the view that,

perhaps because of some benevolent identification deity, the data at hand necessarily include

some set of conditioning variables that suffice for the CIA. Indeed, some writers implicitly hold

this faith with such fervor that they see no need even to attempt an explicit case for the CIA.

        More serious researchers make an explicit case for the CIA based on theory, institutions

(sometimes helpfully embodied in high-quality process analyses), and existing empirical

knowledge. Theory, like our simple model above, suggests the importance of transitory outcome

shocks and of fixed characteristics that affect both outcomes and participation. The former

signals the importance of conditioning on histories of labor market outcomes in the period prior

to the decision to take training or not, and of doing so flexibly and at a relatively fine level of

temporal detail. The latter signals the importance of conditioning on things like ability and

motivation, or at least for compelling proxies for them. Longer lags of labor market outcomes

(i.e. before the “dip”) often assume this proxy role.

        Existing evidence relevant to the justification of conditional independence assumptions

takes a number of forms. One very common form in this literature arises from “within-study”


                                                  72
comparisons that use experiments as benchmarks to learn which conditioning variables lead to

non-experimental estimates based on the CIA that replicate (up to statistical variation) the

experimental estimates and which do not. A long series of papers starting with LaLonde (1986)

and Fraker and Maynard (1987) and continuing through Dehejia and Wahba (1999, 2002),

Heckman, Ichimura, Smith and Todd (1998), and Smith and Todd (2005a, b) embodies this

idea.110 These papers also highlight the importance of conditioning flexibly on labor market

outcomes in the period prior to participation.

         A second form of evidence on conditioning variables comes from studies that take a

relatively compelling set of conditioning variables and adding in an additional set of more novel

conditioning variables. If the impact estimates move upon adding the new variables, they matter

and future evaluations should include them. If estimates do not change much, then the new

variables do not aid in solving the selection problem at the margin.111 That is very useful

knowledge as well, as it helps avoid spending resources collecting data on variables not

necessary to solve the selection problem and (not unrelated) increases the credibility of future

CIA-based evaluations that do not include them. Lecher and Wunsch (2013) provide a thorough

analysis along these lines using the (very) rich German administrative data. Andersson et al.

(2013) use the U.S. Longitudinal Employer Household Dynamics (LEHD) data to examine the

value of conditioning on the characteristics of the firm at which WIA participants last worked in

addition to the usual flexible form in earnings and employment. They find, to their and our

surprise, that the firm characteristics do not matter.


110
    These papers do not always frame their analysis as we do here. Instead, some studies frame the question as “does
matching work?” which in our view represents a very silly question indeed. Matching “works” when you match on
variables that suffice for the CIA and it does not work when you do not. What matters is the conditioning set.
111
    Note that having the estimates not move (much) when adding new variables does not imply that the old variables
suffice for the CIA, though it suggests they do. The key is the absence of an additional unobserved factor,
uncorrelated with all of the included covariates, that affects both participation and outcomes. See e.g. the discussion
in Heckman and Navarro (2004).

                                                          73
        A final way to think about justifying the CIA centers on the so-called support condition.

Semi-parametric and non-parametric estimators based on the CIA require variation in training

status conditional on observed characteristics. Put another way, for any given set of observed

characteristics, the data must include non-trainees to compare to the trainees. Lurking in the

background, some unobserved instruments generate this conditional variation in training.

Thinking about the nature of these instruments (random information shocks, distance from the

training provider, and so on) can aid in making the case for the CIA in a given context.

        The econometric literature provides a wealth of semi-parametric and non-parametric

estimators that build on the CIA and complement the traditional parametric linear regression

model. The most commonly used estimators in applied work in economics undertake non-

parametric matching on the conditional probability of training – the so-called propensity score

given by Pr( D  1| X ) . With a parametric (though ideally relatively flexible) propensity score

model (typically a logit or probit) this general class of semi-parametric estimators balances

parametric assumptions with uni-dimensional non-parametric flexibility. The economics

literature sometimes frames this class of estimators as non-parametric regression estimators.

Essentially the matching implicitly estimates a non-parametric regression of Y0 on the estimated

propensity score and uses predicted values from that estimated regression as estimates of the

expected counterfactual for each trainee. A lively Monte Carlo literature that includes Frölich

(2004), Huber, Lechner, and Wunsch (2013), and Busso, Dinardo, and McCrary (2014) guides

the applied researcher in choosing among the many available estimators.112

        A variety of other identification strategies allow the evaluation of active labor market

programs but have not attracted wide use in the recent empirical literature on training programs.
112
   An odd history of applied econometrics aside: some of the CETA evaluations summarized in Barnow (1987) and
reconciled in Dickinson, Johnson, and West (1987) anticipate the “Coarsened Exact Matching” (CEM) of Iacus,
King, and Porro (2012) that has gained some traction in literatures outside of economics.

                                                     74
These include the bivariate normal selection model, instrumental variables, regression

discontinuity, and the bias stability assumption that justifies the difference-in-differences

estimator. The bivariate normal model has fallen out of favor with labor economists in recent

years for several reasons, including a growing aversion to difficult-to-justify functional form

assumptions and the realization that sensible application of the model required a hard-to-find

exclusion restriction (i.e. a variable affecting outcomes only via its effect on training

participation).113 Similarly, instrumental variables methods have seen little use in the training

literature due to a paucity of plausible instruments.114

        Discontinuity based methods have run rampant in many quarters of applied economics in

the past decade – see Cook (2008) for their history – but they play almost no role in the training

literature. We know of only two examples. One is the Urban Institute’s evaluation of the High

Growth Job Training Initiative (HGJTI) in Eyster et al. (2010) which lacks compelling results

due to the deadly combination of a modest sample size and a high variance outcome variable.

The other is the analysis of the WPRS in Black, Galdo, and Smith (2007). This astounding lack

of discontinuity designs relative to, say, the evaluation literature in K-12 education, results from,

in our view, two factors. First, even before researchers started thinking along these lines,

educational institutions had a lot of policy discontinuities built in. We do not have a compelling

technological explanation for this difference across substantive domains, but it meant that

researchers had lots of low-hanging fruit to pick when the design became salient in economics.

Second, the employment and training world has seen little in the way of attempts to “design in”


113
    See Puhani (2000) for a survey of the literature on the bivariate normal model and Bushway, Johnson and Slocum
(2007) for examples of how things can go wrong in practice.
114
    Frölich and Lechner (2010), which studies Swiss active labor market programs, is the only example of which we
are aware in the recent literature. Back in the dinosaur days, Mallar et al. (1982) used distance to the Job Corps
center as an instrument. The marginal treatment effect approach, a semi-parametric generalization of the classic
bivariate normal selection model well described and illustrated in Carniero, Heckman, and Vytlacil (2011), also
awaits application in the training literature.

                                                       75
discontinuities that can serve as the foundation for causal research, even though they seem quite

natural for courses that require some level of academic preparation as measured by a test score.

        Another design common in other literatures and much less common in the evaluation of

employment and training programs builds “difference-in-differences” or other panel estimators

atop assumptions about bias stability. Bias stability holds that, possibly conditional on observed

characteristics, the difference in untreated outcomes between participants and non-participants is

constant over time. As a result of the temporal stability, differencing takes care of bias due to

selection on unobserved variables. At the individual level, the bias stability assumption runs

afoul of Ashenfelter’s dip, which clearly indicates selection on both time-varying and time-

invariant unobserved variables. Heckman and Smith (1999) show the trouble this causes; in

particular, they emphasize that the choice of the “pre” period matters tremendously in the

presence of the dip.

        Another style of difference-in-differences study operates at the jurisdictional level rather

than the individual level, and exploits policy changes that occur in some but not all jurisdictions.

This type of study faces two difficulties in the training context. First, very little of the policy

action in the training world occurs at jurisdictional levels that correspond to standard data sets,

such as the state-level analyses often used to study things like the minimum wage or (in the old

days) the minimum legal drinking age. Second, data detailing the variation in policies across

jurisdictions requires a lot of digging, because no one collects it and disseminates it in easy-to-

use form, as Huber, Kassabian, and Cohen (2014) do for the TANF program. With new

programs, a staged roll-out, ideally with the jurisdictional timing randomized, allows the

application of this design. We know of one attempt along these lines, namely the Social Security

Administration’s evaluation of the Ticket-to-Work voucher program for disability recipients; see



                                                  76
Stapleton et al. (2008) for details.



Data and measurement issues

Though it might come as a surprise to some, evaluations of training programs have many

dimensions besides the quality of their causal identification. These attributes affect the quality of

the impact estimates, in the sense that they affect the amount of error they contain relative to the

parameter of interest that arises from sources other than sampling variation and selection bias.

They also affect the interpretation of the obtained impact estimates and the value of the cost-

benefit analysis built around those estimates. This section discusses issues related to the

measurement of training and of labor market outcomes, while the following section discusses

various issues that arise in the context of cost-benefit analysis.

        Data on service receipt, service type, and service timing play an important role in

evaluating training programs, yet we know very little about how best to measure these variables

or about the quality of existing measures. Typically, data on services come from one of two

sources: surveys or administrative data. These sources have differing strengths and weaknesses

that vary somewhat across contexts. Administrative data avoid issues with a failure to recall

receipt of services, particularly low intensity services, by survey respondents. In some cases, the

use of administrative measures of enrollment in performance management schemes (we say more

about these below) may increase the quality of these data, which caseworkers might otherwise

have little incentive to enter with care. But use in performance management can cut both ways.

In the JTPA evaluation, as documented in Table 4.5 of Kemple, Doolittle, and Wallace (1993),

many treatment group members not formally enrolled in JTPA nevertheless received JTPA

services, because caseworkers avoided enrolling potential participants as long as possible



                                                  77
because only those actually enrolled counted for performance management purposes. On the

other hand, survey data may catch services received at programs other than the one under study,

as well as services explicitly not recorded in administrative data systems, such as public core

services (e.g. free computers to use to search for jobs) in some WIA programs and the

Employment Service. And in contexts where control or comparison group members may receive

training or other services from (often quite numerous) other programs and providers, surveys of

the individuals in the evaluation may represent the only cost-effective way to characterize the

counterfactual.

       Smith and Whalley (2015) compare data from surveys and administrative records for

treatment group members in the JTPA experiment. They find a substantial amount of

underreporting of services received in the survey data relative to the administrative data. Looking

at particular services, they find that respondents appear to do a better job of reporting services

that happen in classrooms, such as formal training in occupational skills or adult basic education,

than they do at reporting services such as job search assistance or subsidized on-the-job training

at private firms. Reported start and stop dates of training also often differ substantially between

the two sources, though here it is less clear which source should be viewed a priori as containing

less measurement error. More broadly, their paper suggests the value of both additional research

on training measurement and of paying more attention to the quality of administrative data.

       The same choice between survey data and administrative data arises when considering

how best to measure labor market outcomes such as earnings and employment. As with the

measurement of the timing and incidence of training, neither source strictly dominates the other.

This is particularly true in the context of the disadvantaged populations served by means-tested

government training programs. For example, administrative data typically miss sources of labor



                                                 78
income outside the formal labor market and thus not reported to the authorities. These sources

may include illegal activities like drug-dealing or prostitution, as well as legal but informal

activities such as childcare, hair care, automotive repairs, and so on. To the extent that training

programs move their trainees from such informal work into formal sector jobs, reliance on

administrative data on earnings and employment overstate program impacts due to

undercounting informal earnings and employment among non-trainees. At the same time,

administrative data likely measure earnings and employment in the formal sector with less error

than do survey data, particularly as the recall period lengthens. Kornfeld and Bloom (1999) show

that measurement differences between survey and administrative data (from state Unemployment

Insurance records) matter for the impact estimates obtained for the male youth sub-group in the

JTPA experiment. More recently, Barnow and Greenberg (2015) show that measurement

differences between survey and administrative data (usually from Unemployment Insurance

wage records) often have large effects on estimated earnings impacts in the eight randomized

controlled trials examined.

        Earnings and employment measures within the broad categories of survey and

administrative data differ as well. For example, among administrative data sources, state

Unemployment Insurance (UI) wage record data do not include the earnings of many

government employees or of the self-employed, while IRS earnings data do. Neither includes the

value of fringe benefits, which Hollenbeck and Huang (2014) estimate at about 20 percent of

earnings for this population. Smith (1997, Table 11) shows non-trivial differences in self-

reported annual earnings from a simple summary question versus earnings built up from more

detailed information about wages, hours, and weeks worked on individual jobs.115 Whether or


115
  The exact wording of the question from the background information form is “In the past year (12 months), how
much did you earn (before taxes and deductions)?”

                                                      79
not measurement error matters for impact estimates depends on its correlation with treatment

status, as with the example above where training moves trainees away from informal work. See

e.g. Angrist and Krueger (1999); Bound, Brown, and Mathiowetz (2001); and Hotz and Scholz

(2002) for more on measuring labor market outcomes in general and Kornfeld and Bloom

(1999); Wallace and Haveman (2007); Schochet, Burghardt, and McConnell (2008); and Barnow

and Greenberg (2015) for discussions specific to an evaluation context.



Issues for cost-benefit analysis

Cost-benefit analysis provides a framework for combining impacts on a variety of outcomes,

expressing them in common (i.e., dollar) units, and comparing their discounted present values to

the present costs of training. Such analyses add substantial value to impact estimates for that

subset of programs that produce positive impacts on at least some outcomes of interest. In our

view, the cost-benefit analysis produced as part of the National Job Corps Study and documented

in McConnell and Glazerman (2001) represents the best among the evaluations we survey here;

we also draw inspiration from the analyses in Section 10.1 of Heckman, LaLonde, and Smith

(1999) and in Section 12 of Andersson et al. (2013). All of these exercises compare average costs

to average impacts of treatment on the treated; the literature would also benefit from attempts to

compare marginal costs to benefits on marginal participants.

       In many cost-benefit analyses, the magnitude, and sometimes even the sign, of the net

present value will depend on a number of important choices about which the researcher may

have only limited knowledge. Like Heckman, LaLonde, and Smith (1999), we favor thoughtful

sensitivity analysis in such cases, so that the consumer of the cost-benefit analysis comes away

with a clear understanding of the amount and sources of sensitivity in the calculations.



                                                80
       One common limitation concerns the duration of follow-up data. Because training

programs typically exhibit “lock-in” effects – negative impacts during the training period due to

labor market withdrawal – any hope of finding enough positive impacts to pass a cost-benefit test

depends on having follow-up for a reasonably long period after training. At the same time,

evaluation delayed may mean policy influence denied, which argues for not waiting around too

long for more follow-up. If the evidence suggested that positive impacts always persisted once

they started, this issue would become less important, as researchers could feel relatively

confident when projecting impacts out beyond the data. Sadly, what evidence we have provides a

mixed picture about impact persistence. For instance, Couch (1992) shows that impacts on

earnings from the National Supported Work Demonstration remain rock solid for many years;

similarly, the U.S. General Accounting Office (1996) finds that earnings impact from the JTPA

experiment also appear relatively consistent over five years. In contrast, Schochet, Burghardt,

and McConnell (2008) show that earnings impacts from the Job Corps experiment fade out over

time. Greenberg, Michalopoulos, and Robins (2004) find some evidence of sex differences in

impact persistence between men and women in their meta-analysis but lament, as do we, the

absence of many evaluations with more than three years of follow-up data. Longer-term follow-

up using administrative data for both past and future experiments would add to our knowledge

base on this dimension at relatively low cost.

       Another common limitation concerns impacts on outcomes other than employment and

earnings and/or on household members other than the trainee. We might, for example, expect job

training to affect criminal activity, both by consuming the time of the trainee (idle hands …) and,

in the event that training leads to employment, by increasing the opportunity cost of getting

caught. We might also expect job training to affect health. For female participants, training might



                                                 81
affect the timing or incidence of fertility. Finally, we might expect training to have effects on

other household members’ choices regarding schooling and work, and possibly regarding divorce

or co-residence as well.

        Though often hypothesized, we know of only two U.S. studies of general training

programs (as opposed to programs specifically for ex-convicts, for example) that have actually

attempted to measure such effects, namely the National Job Corps Study experimental evaluation

(discussed in more detail below) and the earlier non-experimental evaluation of the same

program. Both of these studies devote a fair amount of effort to estimating the impact of the Job

Corps on the criminal activities of participants, monetizing the resulting impacts, and then

incorporating them into their cost-benefit analyses; see Mallar et al. (1982) and McConnell and

Glazerman (2001). In both cases, they find that a substantively important component of program

benefits, particularly in the first year, comes from reductions in criminal behavior, reductions

that presumably result from the residential nature of the program, which separates participants

from both dubious friends and opportunities for profitable misbehavior. Elsewhere in the world,

Lechner and Wiehler (2011) find some effect of Austrian training programs on the fertility of

female participants. In our view, further work along these lines, whether via survey data or

matched administrative data, would provide a richer view of the overall effects of training

programs.116

        A dollar of government revenue to spend on training costs society more than a dollar in

lost output due to costs associated with collecting the revenue. These include the direct costs of



116
   In addition to their value in a thorough cost-benefit analysis, examination of outcomes beyond just earnings and
employment levels also informs our understanding of the mechanisms by which programs bring about any impacts
on earnings and employment. For instance, studies that examine the effects of employment and training programs on
the durations of subsequent employment and unemployment spells, such as Ham and LaLonde (1996) and Eberwein,
Ham, and LaLonde (1997) both illuminate causal mechanisms and provide guidance on the likely persistence of
impacts on employment and earnings beyond the available data.

                                                        82
operating the tax collection system (e.g. the Internal Revenue Service and all the tax preparers

and accountants) as well as the indirect costs due to the use of distortionary taxes. For example,

income taxes distort choices between labor and leisure in ways that reduce welfare relative to a

world with (non-distortionary) lump sum taxes. Not surprisingly, calculating the direct cost per

dollar of government revenue proves relatively uncontroversial, while estimating the indirect

costs proves quite complex and controversial, enough to generate a large literature and even a

book, namely Dahlby (2008). The public finance literature calls one plus the sum of the direct

and indirect costs of the marginal tax dollar the marginal social cost of public funds (MSCPF).

We do not take a stand here on the correct value for the MSCPF other than that it exceeds one.

Rather, we note that even otherwise very nice cost-benefit analyses such as that in the National

Job Corps Study err by leaving it out, and we recommend the sort of sensitivity analysis using a

range of MSCPF values drawn from the literature that appears in Andersson et al. (2013).

       Another puzzling lacuna in many cost-benefit analyses concerns the value of the “leisure”

of the participants. Consider an individual who would receive training for six months and then

work for 18 months in the first two years after random assignment to the treatment group in an

evaluation of a training program but who would stay at home and care for their children for two

years if assigned to the control group. The standard analysis values the employment based on the

earnings received and implicitly assigns a value of zero to caring for the children at home. The

latter appears in the cost-benefit analysis only indirectly if the childcare used in the treated state

receives a government subsidy (and the analysis sweats such details). As discussed in Greenberg

and Robins (2008), the standard analysis gets the economics wrong by omitting the value of the

participants’ counterfactual non-market time. This omission leads to a systemic overstatement of

programs’ cost-benefit worthiness, as illustrated by Greenberg and Robins (2008) for the case of



                                                  83
the Canadian Self-Sufficiency Project earnings supplement.

        Finally, doing a good cost-benefit analysis requires good data on costs. In the case of

evaluations estimating the ATET of the program as a whole, this means data on average per-

participant costs. In the case of evaluations comparing difference services, it requires data broken

down by service type. For evaluations that focus on marginal participants, it requires data on

marginal costs. As discussed e.g. in Andersson et al. (2013) for WIA, most U.S. programs lack

any useable data on marginal costs as well as lacking data on average costs broken down by

service type, client type or geographic location. 117



General equilibrium effects

None of the evaluations we consider in this chapter accounts for general equilibrium effects. In

the context of training programs, equilibrium effects typically take two forms: displacement and

changes in relative skill prices.118 Displacement, the focus of most papers in this literature that

attend to equilibrium effects at all, occurs when program participants take jobs that others would

have taken in the absence of the program. This could result from their leaping ahead in the queue

due to enhanced qualifications or due to changes in optimal search effort. In either case the

control or comparison group used in a partial equilibrium evaluation will likely contain only a

very small fraction of those displaced, meaning that such an evaluation will overstate the social

impacts of the program. Changes in skill prices result when training programs increase and

decrease the supply of particular types of skills in local labor markets. For example, if a training

program trains many nail technicians in a particular locality, we expect the relative wages of nail

117
    Heinberg et al. (2005) and Barnow and Trutko (2015) document the conceptual and empirical challenges
associated with cost measurement in the context of employment and training programs.
118
    Deterrent effects may matter for mandatory programs; see e.g. Black et al. (2003) and the broader European
literature surveyed in McCall, Smith and Wunsch (2015). We follow the Office of Management and Budget (1992)
in passing on “magic” multiplier effects.

                                                      84
technicians to fall due to increased supply (and doubters of this sort of scenario should read Boo,

2004). Again, such effects will lead a partial equilibrium evaluation, whether experimental or

non-experimental, to overstate the overall economic benefits to the training program.

           General equilibrium evaluations typically take one of two approaches. The first makes

use of spatially distinct local labor markets that have plausibly exogenous variation in program

scale. Different outcomes for the non-treated in localities with a large program presence relative

to those with a small program presence indicate equilibrium effects. Examples of such studies

outside the US include Forslund and Krueger (1997) in Sweden and the astounding two-level

random assignment study in France by Crépon et al. (2013). We know of no such studies for

U.S. programs.

           The second strategy writes down a complete equilibrium model and estimates or

calibrates the model to obtain estimates of the size and nature of any equilibrium effects. Though

we know of no U.S. training programs evaluated using this strategy119, Davidson and Woodbury

(1993) uses a calibrated search model to estimate the equilibrium effects of UI bonuses (lump

sum payments to UI claimants who end their claim early) on the search effort of unemployed

workers not eligible for the bonuses. Along similar lines, Lise, Seitz, and Smith (2004) calibrate

a search model to examine the equilibrium effects of the Canadian Self-Sufficiency Project. In

contrast, Heckman, Lochner, and Taber (1998) estimate a dynamic, stochastic, general

equilibrium model in their study of the equilibrium effects of a $500 subsidy to university

tuition. In their model, the equilibrium effects work through changes in the relative skill prices of

high school educated and university educated labor. All three studies find substantively

important equilibrium effects; in the Lise, Seitz, and Smith (2004) paper they suffice to overturn

the positive verdict of a partial-equilibrium cost-benefit calculation. More work along these lines,
119
      Johnson (1979) considers displacement effects in an early (i.e. pre-search) equilibrium framework.

                                                           85
including greater emphasis on the potential for equilibrium effects and some thinking about the

contexts where equilibrium effects will and will not matter very much, would improve our

understanding of the effects of training programs and of their fiscal worthiness.




Systematic evaluation and aggregation of evaluations

Another important development since the publication of LaLonde’s (2003) survey centers on the

systematic evaluation and aggregation of evidence across evaluation studies. The meta-analyses

of evaluations of active labor market programs from many developed countries summarized in

Card, Kluve, and Weber (2010, 2015) provide a fine example of this. Greenberg, Michalopoulos,

and Robins (2003) undertake a similar meta-analysis restricted to U.S. evaluations. Meta-

analysis in this context means estimating so-called “meta-regressions” in which impact estimates

from various evaluation studies (often for particular subgroups) form the dependent variable and

various characteristics of the evaluation (e.g. was it experimental or not) of the program (e.g.

classroom training or job search assistance, program duration), of the participants (e.g. men or

women, youth or adults) and of the context (e.g. the unemployment rate) comprise the

independent variables. This differs from the original use of meta-analytic techniques in the

medical literature to combine multiple under-powered studies of the same treatment applied to

the same population. Here the (quite different) goal consists in accounting for the variation

across studies. One perhaps surprising result from the Card, Kluve, and Weber (2010) meta-

analysis is that, conditional on controlling for other features of the evaluation, the estimates

provided by experimental and non-experimental methods do not differ very much on average.

       The Clearinghouse for Labor Evaluation and Research (CLEAR) website represents



                                                 86
another flavor of evaluative aggregation. Inspired by the U.S. Department of Education’s What

Works Clearinghouse (colloquially known, with some justification, as the “Nothing Works

Clearinghouse”), and thus indirectly by the Cochrane Collaboration in health and the Campbell

Collaboration in social policy, CLEAR grades evaluations of labor market programs relative to

fixed standards of quality, and also provides summaries of evidence.120 The latter take two main

forms: one comprises quality-weighted reviews of the literature related to specific programs or

classes of programs and the other represents, essentially, summary translations (from research-

speak into regular English) of evaluations for practitioner and policymaker audiences. CLEAR

differs from the WWC along several dimensions, some of which result from the much smaller

size of the relevant literature, some from the generally lower financial stakes facing researchers

who write the evaluations evaluated on the site, and some from the fact that the quality of the

literature on employment and training programs has historically far exceeded that on educational

interventions, with the implication that CLEAR does not, unlike WWC, explicitly see part of its

mission as raising an entire field out of the research muck. While we acknowledge the

difficulties in coming up with generally applicable and reasonably objective standards for

evaluations, we think that CLEAR plays a very useful role in publicly grading studies relative to

a good shot at such standards. Importantly, both CLEAR and WWC include mechanisms for

updating the grading standards as applied econometrics moves forward over time, though neither

site has successfully dealt with the problem of studies that exceeded the methodological

standards of their day but fall short of the standards of the present.



Review of research on program impacts


120
  See clear.dol.gov for CLEAR, ies.ed.gov/ncee/wwc for the WWC, www.cochrane.org for the Cochrane
Collaboration, and www.campbellcollaboration.org for the Campbell Collaboration.

                                                   87
Rather than repeat earlier summaries in the literature of pre-2000 evaluations such as those in

LaLonde (2003) and Heckman, LaLonde, and Smith (1999), we focus our energies here

primarily on recent, high quality evaluations of major federal programs, namely WIA, the Job

Corps, and TAA.



Workforce Investment Act

We consider the four closely related papers that examine the Workforce Investment Act (WIA):

Hollenbeck (2009), Heinrich et al. (2013), Andersson et al. (2013) and Heinrich and Mueser

(2014). These evaluations share a common basic design, in part because they share a common

foundation of administrative data sources. Each of these papers combines administrative data

from the WIA program – formally the WIA Standard Record Database (WIASRD) data – with

data on earnings by calendar quarter from state Unemployment Insurance records. The WIASRD

data, in addition to program-related information on enrollment and termination dates and

services received, also include basic demographic information as well as limited information on

schooling.121

        All four papers focus their estimation energies on one or both of two parameters: the

mean effect of receiving any WIA services relative to not receiving any WIA services on those

who receive them (hereinafter the “W-ATET”) and the mean effect of receiving WIA training,

and possibly other WIA services, compared to receiving one or more WIA core or intensive

services, but not training, on those who receive the training (hereinafter the “T-ATET”). Both

parameters answer interesting policy questions, though we note the absence (necessarily given

the data) of any attempts to estimate impacts on marginal participants, those most relevant to


121
   See Decker and Berk (2011) and Van Horn, Krepcio, and Wandner (2015) for broader surveys of recent research
on WIA.

                                                      88
thinking about the effects of small expansions or contractions in the WIA budget.

       In the WIA context, the two parameters present somewhat different challenges to the

researcher. Andersson et al. (2013) argue that the T-ATET estimand embodies an easier selection

problem than the W-ATET. We can think of two versions of this argument. First, due to the

interplay of the economics and the institutions, WIA participants may differ from WIA non-

participants more strongly in terms of observed and unobserved characteristics than do WIA

trainees and WIA non-trainees. Second, we may just know more about how the WIA trainees

differ from the WIA non-trainees via institutional knowledge about the service assignment

process. Andersson et al. (2013) present evidence for the first claim by showing that pre-program

mean earnings patterns differ only very modestly between the WIA trainees and non-trainees in

their data relative to the differences found in other papers for WIA participants versus WIA non-

participants. Bell et al. (1995) advance a closely related view in making the case for program

dropouts as a comparison group for program participants; see also Section 15 of Heckman,

Ichimura, and Todd (1997).

       Another difference between the T-ATET and W-ATET estimands concerns comparison

group selection and the related problem of temporal alignment: i.e. what time period to use as a

baseline when coding up time-varying conditioning variables. The comparison group for the T-

ATET is clear: it is the WIA participants who do not receive training. The temporal alignment

problem for the T-ATET has a similarly straightforward solution: the natural choice aligns the

WIA trainees and the WIA non-trainees based on their dates of WIA enrollment. All of the

papers that estimate the T-ATET follow this course.

       In contrast to the T-ATET, the choice of comparison group for the W-ATET requires

some thought and some tradeoffs. Due to their reliance on administrative data, the WIA papers



                                                89
lack a version of the “ideal” comparison group of eligible non-participants collected as part of

the JTPA experiment. Instead, data limitations require choosing among various candidate

comparison groups based on their participation in other programs, as administrative data become

available only via such participation. Rhetorically, the choice gets presented either as a practical

alternative to the desired but too-expensive-to-obtain (because of the large number of screening

interviews required) sample of eligible non-participants, with a case then made about the nature

and size of the resulting bias, or as a particular way of defining the counterfactual of interest, so

that the treatment contrast becomes WIA versus another program rather than WIA versus no

WIA. Neither contrast necessarily dominates in terms of policy interest, but they do differ in

terms of the mix of related services received by comparison group members, a difference that

affects interpretation and comparisons with other studies.

       In practice, the choice for researchers seeking to estimate the W-ATET boils down to

either Employment Service (ES) participants or Unemployment Insurance (UI) claimants.

Consider UI claimants first. This comparison group has the disadvantage that many WIA

participants lack UI eligibility because they lack sufficient work experience to qualify for UI.

This problem can be (and is, in these papers) “solved” by comparing UI claimant WIA

participants to UI claimant non-participants. This is not an uninteresting comparison, but it does

leave aside many important components of the WIA participant population, including welfare

recipients and low-skill workers with spotty employment histories. Using UI claimants has the

advantage that it simplifies the problem of temporal alignment as WIA participant and WIA non-

participant UI claimants can be aligned based on their UI claim’s start date.

       As described in more detail earlier in the chapter, the ES dates back to the Wagner-Peyser

Act of 1933 and provides labor exchange services. The UI program requires virtually all



                                                  90
claimants (other than those awaiting recall) to register with the ES.122 The ES also serves many

other job-seekers, including some currently employed but looking for a better match. The extent

of ES integration with WIA varies substantially across states. Relative to using UI claimants as a

comparison group, using ES registrants has the advantage of capturing a broader population, one

that overlaps with more of the WIA participant population. The costs are two-fold. First, the

process that leads some job seekers who are not UI claimants to register for the ES and others not

to do so is not well understood, but has implications for the interpretation of the ES comparison

group counterfactual. Second, and not unrelated, while UI claimants typically register for the ES

shortly after becoming unemployed, other job seekers may wait until initial job search efforts fail

before seeking help from the ES. This process complicates temporal alignment, as aligning WIA

non-participant ES registrants with WIA participants using the ES registration date may do a bad

job of implicitly conditioning on the duration of job search, something the literature suggests

matters because it proxies for otherwise unobserved characteristics.123

         The four non-experimental WIA papers also share common identification strategies, as

they all assume one or both of the conditional independence assumption and the bias stability

assumption. The available data and institutional variation essentially force these choices. Unlike

many educational institutions, WIA does not provide helpful discontinuities in treatment

assignment that depend on observed, difficult-to-manipulate running variables like test scores.

Hence, an RD analysis would require purposive institutional changes. One could imagine using

variation in services received due to exogenous variation in caseworker assignment (whether

explicitly random or just “first available”), but the data typically available lack information on



122
   For more on the ES, see e.g. Balducchi, Johnson, and Gritz (1997).
123
   In many European countries, centralized labor market institutions that link formal registration as unemployed to
benefit receipt greatly simplify the temporal alignment problem.

                                                         91
caseworkers and on the process that matches clients to caseworkers.124 Similarly, one can

imagine an analysis that attempts to use distance to the One-Stop as an instrument in an analysis

of WIA versus no WIA, but the available data lack residential addresses for comparison group

members. No other credible instruments suggest themselves.

        At the same time, as discussed above, the literature provides some support for the idea

that the available conditioning variables, particularly the lagged labor market outcomes provided

by the UI data, may suffice to make identification of causal effects based on the Conditional

Independence Assumption (CIA) or the Bias Stability Assumption (BSA) plausible. That is,

these papers can, and sometimes do, make a positive case for a causal interpretation of impact

estimates based on the CIA or BSA.

        Consider the case for the CIA first. As mentioned above, this case rests on claims about

having a sufficiently rich set of exogenous conditioning variables to make it plausible that

participation (i.e. in WIA or in training within WIA) is conditionally unrelated to the untreated

outcome. To make this case, all four papers start out by forcing exact matches on particularly

important covariates. Hollenbeck (2009) employs exact matching by sex and by region within

Indiana. Heinrich et al. (2013) match exactly on sex and on state. Andersson et al. (2013) match

exactly on state but find similar estimates for men and women and so pool them in their

preferred specifications. Heinrich and Mueser (2014) match exactly on sex and on calendar time.

Exact matching identifies particular conditioning variables thought to have such a strong effect

on both treatment choice and outcomes that allowing the inexact matches implicit in the

application of propensity score methods in finite samples could lead to non-trivial bias. As

discussed in LaLonde (2003), the earlier literature found consistent differences in the mean


124
   Such a strategy would mimic the literature in criminology and the economics of crime that relies on randomly
assigned judges as instruments for aspects of punishment severity. See e.g. Mueller-Smith (2015).

                                                        92
impacts of employment and training programs on men and women; combined with the broader

evidence that men and women experience the labor market differently, this motivates exact

matching by sex. The clear finding that local labor markets matter in Heckman, Ichimura, Smith,

and Todd (1998) motivates exact matching on geography.125 For the reasons just noted, all the

studies we consider include sex, calendar time, and geography at the sub-state level as

conditioning variables, even if they do not match on them exactly.

         All the studies also include education (categories for years of schooling), veteran status,

and disability status. Education has an extremely well-documented correlation with labor market

outcomes, and also should affect participation via the opportunity cost. It should also matter for

whether or not enrollees train or not as it signals the ability to successfully absorb complicated

material presented in a classroom format as well as proxying for the participant’s taste, or

distaste, for such activities.

         The two remaining major categories of conditioning variables represent recent histories

of labor market outcomes (earnings and employment) in all four papers and recent histories of

participation in various programs, including some or all of the ES, WIA, UI, and TANF in the

Heinrich et al. (2013) and Andersson et al. (2013) papers. Heinrich et al. (2013) have the richest

specification of recent program participation. Hollenbeck (2009) has a somewhat less flexible

specification in terms of earnings and employment than the other two papers.126 The flexibility in

all the papers builds on the notions that, first, zero earnings is different, so that indicators for zero

earnings in a quarter should be included, second, that dynamics matter, so that strings of zeros

and/or job loss just prior to participation matter, and, third, that variability in earnings likely


125
   In some cases, such as sex, a desire to present subgroup estimates also motivates the exact matching.
126
   The full list of conditioning variables appears in Table A-1 for both Hollenbeck (2009) and Andersson et al.
(2013). The conditioning variables for Heinrich et al. (2013) appear in Table A-1 of the report that underlies their
published paper, Heinrich et al. (2008).

                                                          93
matters, which motivates inclusion of the earnings variance directly or of measures of particular

types of changes in employment and earnings. Andersson et al. (2013) compare conditioning sets

that include eight and 12 quarters of pre-program earnings information and find little difference

in their T-ATET estimates, though given the modest differences in pre-program mean earnings

they find for WIA trainees and WIA non-trainees we would hesitate to generalize this finding to

the W-ATET.

       The UI administrative data do not allow these researchers to distinguish between zero

earnings due to unemployment and zero earnings due to absence from the labor force, which

Heckman and Smith (1999) find important. They also do not allow the finer level of temporal

detail – namely monthly rather than calendar quarter labor market outcomes - in the “pre” period

emphasized in that paper. The empirical importance of these (relative) weaknesses in the data

remains unknown. Andersson et al. (2013) do examine the value of conditioning on a set of

variables related to the firm at which WIA participants most recently worked in their estimation

of the T-ATET and find, to their and our surprise, that they add essentially nothing in terms of

reducing selection bias (as indicated by the fact that the estimates hardly budge).

       Relative to the CIA, the BSA allows for the existence of selection into WIA, or into WIA

training, based on time-invariant unobserved variables. The simple model we presented above

comports with the BSA, but a more general model of selection on outcome trends would not. The

JTPA experimental data suggest selection on trends for some demographic groups. Coincidence

between estimates based on the CSA and estimates based on BSA suggests that the available

conditioning variables suffice to solve the problem of selection on time-invariant characteristics.

       The four WIA evaluation papers apply somewhat different econometric estimators.

Heinrich et al. (2013) apply many-to-one caliper matching followed by a linear regression bias-



                                                94
correction step. Andersson et al. (2013) use inverse propensity weighting (IPW) and single

nearest-neighbor matching with replacement. Hollenbeck (2009) uses single nearest-neighbor

matching with replacement and a caliper. Heinrich and Mueser (2014) also use IPW. The papers

that assume both the CIA and the BSA simply replace the outcome level as the dependent

variable under the CIA case with the before-after outcome difference as the dependent variable

under the BSA. The methodological literature provides reasons to prefer some estimators over

others. For example, Hirano, Imbens, and Ridder (2003) show conditions under which IPW

attains the semi-parametric efficiency bound. IPW also avoids the troublesome bandwidth

choices associated with nearest neighbor and kernel matching estimators. The Monte Carlo

literature, e.g. Frölich (2004), Huber, Lechner, and Wunsch (2013), and Busso, DiNardo, and

McCrary (2014), reveals that single nearest neighbor matching with replacement typically has

very low bias but a high enough variance that it typically performs poorly in mean-squared-error

horse races. Bias correction via ex post linear regression using the IPW or matching weights can,

but need not, improve finite sample performance. At the same time, in actual applications,

variation in estimates due to different econometric estimators typically pales in comparison to

variation due to e.g. changes in the conditioning set; see e.g. Table 7 of Plesca and Smith (2007).

       The four non-experimental WIA evaluations do vary on two important dimensions: the

states included in their data and the calendar time period during which the WIA participants they

study participated in the program. Heinrich et al. (2013) attempted, with assistance from the US

Department of Labor, to recruit all 50 states. They ended up with a non-random sample of 12.

Andersson et al. (2013) attempted to recruit nine states (selected based on size and ex ante

likelihood of cooperation) and ended up with just two. In both studies, the states declined to have

their names attached to state-specific impact estimates, which of course makes it difficult to even



                                                95
casually link those impacts to features of state programs and economic contexts. The

unwillingness of many states to provide data for high-quality evaluations provided at very low

cost, or to have their state-specific impacts identified when they do, provides stark evidence of

the importance of issues of monitoring and control between taxpayers as principals and state

program administrators as their misbehaving agents. It also limits what studies such as these can

add to our store of policy-relevant knowledge. In contrast, Hollenbeck (2009) and Heinrich and

Mueser (2014) examine single, identified states, namely Indiana and Missouri, respectively.

        The Andersson et al. (2013) paper has the earliest sample, which includes WIA

registrants from calendar years 1999 to 2005, inclusive, with the bulk in 2000-2004. Their study

thus includes the “dot com” recession of the early 2000s. Heinrich et al. (2013) study WIA

registrants from July 2003 to June 2005, and Hollenbeck studies program exiters from July 2003

to June 2005; both papers thus focus exclusively on program performance in good economic

times. Finally, Heinrich and Mueser (2014) focus by design on the Great Recession period by

studying WIA registrants from June 2007 to June 2010. There is some European evidence from

Lechner and Wunsch (2009) indicating that training programs have larger impacts in slack labor

markets (due to worse comparison group outcomes), while the meta-analysis of U.S. programs in

Greenberg, Michalopoulos, and Robins (2003) suggests the reverse. Either way, the time period

may matter in comparing estimates among the WIA studies.

        We now summarize the estimated earnings impacts from three of the four WIA non-

experimental studies.127 Given the focus of this chapter on training, the T-ATET impacts occupy

most of our attention. We begin with those.

        Heinrich et al. (2013) present separate estimates for men and women and, within those


127
   We do not present numerical estimates from Heinrich and Mueser (2014) as it has not yet been published or
appeared in a formal working paper series.

                                                       96
groups, for the adult and dislocated worker funding streams. They produce separate estimates by

state and quarter; within each quarter they produce an overall impact estimate by weighting the

state-specific estimates by each state’s overall contribution to the trainee sample. As shown in

their Figure 5, for women in the adult stream, they find a modest lock-in effect that lasts for three

quarters followed by impacts that increase to around $800 per quarter and persist until the end of

their 16 quarters of post-enrollment data. For men in the adult stream, they find essentially no

lock-in effect, perhaps because men who receive subsidized on-the-job training at private firms

have positive impacts in early quarters that cancel out the lock-in effect (on average) of the men

receiving classroom training. In later quarters, positive impacts stabilize at around $500 per

quarter; this lower absolute impact represents a much lower impact in percentage terms due to

the higher average earnings of men in this population. Their Figure 8 shows that both men and

women in the dislocated worker stream have large and long-lasting lock-in effects and no clear

positive impacts even at the end of the sample period. All estimates of any magnitude attain

conventional levels of statistical significance. Guided by a specification test, Heinrich et al.

(2013) report cross-sectional matching estimates of the T-ATET; the difference-in-differences

estimates of the T-ATET in report – Heinrich et al. (2008) – tell the same story.

       The findings from Andersson et al. (2013) turn out similar in the large but differ in

important ways in the small. Unlike Heinrich et al. (2013), they pool men and women but report

separate estimates for their two states. Like them, they also separate out the adult and dislocated

worker funding streams within states. The relevant estimates appear in their Tables 4A to 4D. In

their state A adults experience a three-quarter lock-in effect and then see impacts that gradually

rise, stabilizing at around $300 per quarter by the time the data end at 12 calendar quarters after

enrollment. In contrast, displaced workers in State A (a medium-sized state on the Atlantic



                                                 97
seaboard) experience earnings losses of around $900 per quarter initially, trailing off to “only”

about $125 per quarter. In their State B (a large, Midwestern state), the adults experience a quite

similar pattern of impacts, but stabilizing at around $400 per quarter, while the displaced workers

do much better: following a very long lock-in period their impacts rise to about $300 per quarter

at the very end of the data. In addition to not finding clear differences in impacts between men

and women, Andersson et al. (2013) also report looking for differential impacts by race /

ethnicity and by years of schooling and not finding much differences on those dimensions either.

They find that quite similar estimates emerge from their cross-sectional and difference-in-

differences estimators; like them, we highlight the cross-sectional estimates.128

         We can compare, in a very broad sense, the estimates of the T-ATET from these two

studies to the estimates of the effect of training obtained by Heckman, Hohmann, Smith, and

Khoo (2000) by applying various non-experimental estimators to the experimental data from the

JTPA experiment on individuals recommended prior to random assignment to receive classroom

training in occupational skills (and possibly other services not including subsidized on-the-job

training), the so-called “classroom training treatment stream.” The JTPA experiment randomized

adult participants and not dislocated workers (JTPA having the same distinction between these as

WIA). Their Table IV presents instrumental variables estimates while their Table V presents

cross-sectional and before-after estimates. In a very broad sense, and one should not push farther

than that given the differences in programs, geographic locations, and identification strategies,

they tell the same story here of substantively important but not completely implausible impacts

of training on earnings following a lock-in effect.

         The W-ATET estimates in Heinrich et al. (2013) for the adult stream show positive

128
   Most of the impact estimates of more than $300 in absolute value in Andersson et al. (2013) easily attain
conventional levels of statistical significance but with imperfect (and likely somewhat too small) standard errors.
See their note 11 for additional details.

                                                          98
impacts for women that start around $500 per quarter and rise to about $600 per quarter, and

impacts for men that start around $800 per quarter and then sink fairly rapidly to around $500

per quarter. In stark contrast, the results for the dislocated worker stream reveal large and

persistent lock-in effects that last about two years, followed by approximate zero impacts for

men and approximately $100 per quarter impacts for women. All of the estimates not

approximately zero attain conventional statistical significance. Based on specification tests

looking at differences in pre-period earnings, the authors present cross-sectional matching

estimates for the adults and difference-in-differences matching estimates for the dislocated

workers, though the cross-sectional results in Heinrich et al. (2008) exhibit the same basic

patterns.

       Hollenbeck (2009) presents estimates of the W-ATET from Indiana using ES registrants

as the comparison group. Besides being from a different state, these estimates differ in their

construction from those in Heinrich et al. (2013) because Hollenbeck (2009) measures outcomes

from program exit (whether WIA or ES) rather than from program start. This not only omits an

important part of the lock-in period for the WIA participants – one would not expect lock-in

from the employment-focused ES – but also changes relative timing, as ES tends to have shorter

enrollment spells than WIA. Hollenbeck’s (2009) analysis shows similar post-lock-in W-ATET

impacts for adults as found in Heinrich et al. (2013), with relatively precise point estimates of

$549 in the third quarter after exit and $463 in the seventh quarter after exit. In contrast, dramatic

differences emerge in regard to the W-ATET for participants served under the dislocated worker

funding stream. Here Hollenbeck (2009) finds relatively precise (the reader, unfortunately,

receives stars rather than standard errors) estimates of $410 in the third quarter after exit and

$310 in the seventh quarter after exit (the last quarter available for the full sample). Hollenbeck



                                                 99
reports that in his analysis, as in Andersson et al. (2013), the conditional difference-in-

differences estimates closely resembled those from cross-sectional matching; it is the latter that

he anoints as his preferred estimates and which we highlight here.

       Where do the earnings impacts estimated in these studies come from? Do they result from

increases in wages, from “intensive margin” increases in hours worked or from “extensive

margin” increases in employment? What about increases in the duration of employment spells

via higher match quality and/or matches to “better” firms? The administrative outcome data used

in the WIA studies allow only modest insights into the mechanisms underlying realized earnings

impacts. Basically, they only allow the construction of impacts on employment, defined as non-

zero earnings, and then only at the level of the calendar quarter. In each of the studies considered

here, the employment estimates parallel the earnings estimates in the sense that positive earnings

impacts coincide with positive employment impacts. The magnitudes relative to the earnings

impacts do vary somewhat, with particularly large employment impacts relative to earnings

impacts for the displaced worker W-ATET in Heinrich et al. (2013) and for both funding

streams’ W-ATET in Hollenbeck (2009).

       Linking the usual administrative data to the Census Bureau’s Longitudinal Employer-

Household Dynamics (LEHD) data allows Andersson et al. (2013) to estimate impacts of WIA

training on the characteristics of the firms at which participants end up. They consider standard

characteristics from the literature including the LEHD firm “fixed effect” (bigger is better), firm

turnover (less is better) and firm size (bigger is again better). They find (see their Table 6)

impacts of modest size that parallel the earnings impacts discussed above. Thus, for state by

funding stream combinations with positive earnings impacts, trainees have a net improvement in

employer quality in the 12th quarter after WIA registration.



                                                 100
       The non-experimental literature on WIA offers the reader methodological insights, useful

findings for policy, and (at least) two puzzles. For adults, both W-ATET and T-ATET turn out

positive and of reasonable magnitude in every study that presents them. Those findings justify

continuing to provide similar services to a similar clientele under WIOA. In contrast, the

literature offers heterogeneous findings for displaced workers. This leads to the first of the two

puzzles: in the Heinrich et al. (2013) and Andersson et al. (2013) papers, why do the adult and

dislocated worker programs have such astoundingly different impact estimates? The puzzle only

becomes more complicated upon noting that almost all dislocated worker participants could have

received services under the adult stream, while many adult participants could have received

services under the dislocated worker stream. Second, whence the positive impacts for dislocated

workers in Indiana in Hollenbeck (2009)? A Hoosier might argue that Indiana is just special, or

perhaps especially well-run, but the fact that Hollenbeck (2009) obtains similar results in two

other state analyses not discussed in detail here – see his Table 5 - suggests some feature of his

methodology as the culprit. Aligning participants and comparison group members relative to the

timing of exit rather than the timing of enrollment represents an obvious candidate, but Table 6

of Hollenbeck (2011) yields no smoking gun. Satisfactory resolution of both puzzles awaits

future research.

       In addition to the four non-experimental evaluations, the US Department of Labor

presently has an experimental evaluation of WIA, called the “WIA Adult and Dislocated Worker

Programs Gold Standard Experiment,” in the field. This evaluation compares three treatment

arms for participants in the adult and dislocated worker funding streams: eligible just for core

services, eligible for core and intensive services, and eligible for all services including training.

The comparison between the second and third arms will provide a benchmark of sorts for the



                                                 101
non-experimental evaluations that estimate the T-ATET, once adjusted for whatever level of

treatment group dropout (from WIA) arises in the experiment. The WIA experiment will also

provide the first experimental impact estimates for dislocated workers, who were omitted from

the JTPA evaluation. As a result, it should shed some light on the puzzling difference in impacts

between participants in the two funding streams in the non-experimental studies.

        In sharp contrast to the site recruitment difficulties in the JTPA experiment that led to

serious concerns regarding external validity, the WIA experiment has done quite well on this

dimension, apparently because it imposes a lower burden on sites by randomly assigning a

smaller fraction of the intake to the control group. Its 28 sites include 26 from an initial random

sample of 30 plus two additional randomly chosen replacement sites. Taken together, the sites

will provide a sample size of around 35,000, substantially more than the 20,601 in the JTPA

experimental sample. Results from the experiment, for which follow-up data collection is in

progress as we write, should become public in 2016. When they do, they will contribute to both

our substantive and methodological knowledge in important ways.129



Job Corps

We have very good evidence on the labor market effects of the Job Corps program thanks to an

extensive experimental evaluation conducted in the mid-1990s. In particular, the experiment

randomly assigned eligible applicants at (almost) all Job Corps centers around the U.S. to either

a treatment group eligible to receive Job Corps or to a control group excluded from Job Corps for

three years. Random assignment took place from November 1994 through December 1995. The

design of the (formally titled) National Job Corps Study (NJCS) overcomes two of the main


129
   See http://www.mathematica-mpr.com/our-capabilities/case-studies/evaluating-the-effectiveness-of-employment-
and-training-services for more.

                                                     102
issues that raised concerns about external validity in the JTPA experiment. First, by conducting

random assignment at (almost) every Job Corps center, it removed concerns about non-random

site selection; the fact that the Job Corps, unlike JTPA or WIA, is run directly at the federal level

enabled this strategy. Second, on average the experiment assigned only about seven percent of

applicants to the control group. As a result, sites did not have to recruit many additional potential

participants in order to maintain the size of their operation while still filling in the control group.

This reduces site burden and also reduces concerns about external validity; put differently, the

NJCS can make a credible claim that the experimental impact estimates apply to the program as

it normally operates. The research sample in the NJCS includes about 6,000 in the control group

and about 9,400 in the treatment group; for cost reasons the evaluation collected data on only a

random subset of those randomly assigned to the treatment group.

        The NJCS presents an interesting treatment contrast and, in so doing, highlights issues

that arise in dealing with control group substitution. Around 73 percent of the treatment group

enrolled in the Job Corps, with an average enrollment duration of about eight months. Only 1.4

percent of the control group defeated the experimental protocol by enrolling in the program

during the embargo period. At the same time, and not at all surprisingly given the age of the

applicants and their expressed interest in programs to improve their human capital, 71.7 percent

of the control group enrolled in some sort of education or training program during the 48 months

after random assignment. Some treatment group members also enrolled in programs other than

the Job Corps, so that in total 92.5 percent received some sort of education and training. Thus,

focusing strictly on incidence, the treatment increases receipt of some education and training by

about 21 percentage points. At the same time, incidence misses much of the story here due to the

substantial difference in intensity. The options facing control group members do not include



                                                  103
long-duration residential programs like Job Corps. As a result, the difference in mean hours of

education and training between the treatment and control groups (including all the zeros) equals

710, or about 18 weeks of full time activity.

        We focus here on the “intent to treat” (ITT) impacts estimated using matched earnings

records from the Social Security Administration. Schochet, Burghardt, and McConnell (2008)

document non-trivial differences between these estimates and those obtained using survey data

and using administrative data from state UI systems. The ITT require careful interpretation in

light of the nature of the treatment contrast presented by the experiment as described above. As

expected given the timing of random assignment, estimated annual impacts for calendar years

1995 and 1996 equal -$270 and -$179, respectively, reflecting a “lock-in” effect due to reduced

job search, and thus reduced employment, while treatment group members engage with the Job

Corps. The estimated annual impacts turn positive in 1997 and 1998, equaling $173 and $218,

respectively.130 All four estimates achieve conventional levels of statistical significance.

Consistent with the earnings impacts, the evaluation finds positive impacts on measures of job

quality as of the 16th quarter after random assignment. Finally, the Job Corps also affected

criminal behavior, measured as arrest and conviction rates.131 The headline: the Job Corps, nearly

alone among employment and training programs for youth, has positive and substantial impacts

on labor market outcomes. Comparison with the JOBSTART program found ineffective by Cave,

Bos, Doolittle, and Toussaint (1993), which provided (more or less) a non-residential version of

the Job Corps, suggests the importance of the residential aspect of the program.



130
    See Schochet, Burghardt and Glazerman (2001) for discussion of the finding of larger impacts for older
participants and Flores-Lagunes, Gonzales, and Neumann (2008) for discussion of the lack of strong impacts among
Hispanic participants.
131
    The big picture findings from the NJCS echo those of the earlier non-experimental evaluation documented in
Long, Mallar, and Thornton (1981) and Mallar, Kerachsky, Thorton, and Long (1982).

                                                      104
         McConnell and Glazerman (2001) present a careful and comprehensive cost-benefit

analysis. Job Corps costs a lot: about $16,500 per participant in 1995 dollars. As a result, because

the earnings impacts fade out over time as control group earnings catch up to treatment group

earnings, it fails to pass a social cost-benefit test despite having positive impacts on both labor

market and criminal justice outcomes. It does (easily) pass a cost-benefit test from the

perspective of participants. Thus, the Job Corps presents a glass half full, but in a desert of

dismal evaluation results for youth, that means something, and at the least suggests directions for

future innovations in program design.



Trade Adjustment Assistance (TAA)

TAA recently received a thorough non-experimental evaluation using a “selection on observed

variables” identification strategy building on a combination of survey and administrative data.

The survey data allow a (somewhat) richer, and thus more compelling, set of conditioning

variables than those in the WIA evaluations. On the other hand, the complicated structure of the

TAA program makes the non-experimental evaluation task substantially more challenging than

for WIA. In the end, Schochet et al. (2012) have produced valuable evidence by optimizing

within the design constraints, but substantial uncertainty remains.

         The evaluation focuses primarily on the impact of receiving “significant TAA services”

for a sample of workers certified under TAA between November 1, 2005 and October 31, 2006

from 26 states and with UI claims starting in a wider window around that year as allowed in the

law in effect at that time.132 UI claimants from the same time periods and the same local labor

markets not certified under TAA constitute the comparison group. Significant TAA services


132
  The evaluation calls this the “certified-worker participant sample.” Analyses using alternative definitions of the
TAA treatment (and thus alternative samples of treated individuals) reach similar substantive conclusions.

                                                         105
means more than just “light-touch TAA services or One-Stop core services provided through

WIA or ES”; the evaluation measures service receipt using both administrative data and survey

reports.

           Not surprisingly given that TAA provides UI benefits and trade readjustment allowances

(TRA) over a longer time period than for the comparison group and encourages longer-term

training, TAA participants experience relatively long-lasting lock-in effects. In particular, in the

first four quarters, Table 1 of Schochet et al. (2012) shows that the matched comparison group

averaged 19.4 weeks of employment and $12,674 in earnings more than the participants. The

negative impacts fade out over time but never entirely disappear during the four-year follow-up

period. For example, in quarters 13-16, the matched comparison group averages 2.0 more weeks

of work and $3,273 more in earnings than the participants. Subgroup analyses reveal less

negative effects for younger TAA participants and no substantive difference between men and

women. While the evaluation includes a (truly) extensive collection of sensitivity analyses on

many dimensions, the question of whether the job loss that leads the participants into TAA might

have more persistent consequences than the job losses among the comparison group lingers,

though it would require an implausibly large difference to save the TAA program in a cost-

benefit sense.



Other programs

A variety of other programs, some large and most small, exist and have received some evaluative

attention.133 We have chosen to focus on larger programs with relatively high quality evaluations

and on programs operated via the Department of Labor. Our focus leaves out the many welfare-



133
      See http://wdr.doleta.gov/research/keyword.cfm for a partial list as well as the discussion around Table 8.7.

                                                           106
to-work programs discussed in Ziliak (2015) and in Greenberg and Robins (2011) as well as the

Food Stamp / SNAP employment and training programs evaluated in Puma and Burstein (1994).

It also omits “sectoral training” programs under which taxpayers provide training for particular

firms or small groups of firms, as in Maguire et al. (2010) as well as studies of vocational

training provided by the community college system not financed by WIA or TAA, as in

Jacobson, LaLonde, and Sullivan (2005). Finally, we also omit many evaluations with

methodological, data, or sample size issues such as the Eyster et al. (2010) evaluation of the

High Growth Job Training Initiative (HGJTI).134



Program operation issues

As we have noted along the way, in our view the literature spends relatively too much effort on

estimating the ATET for programs that will, for various political reasons, never go away no

matter what their ATET looks like, and relatively too little time providing compelling evidence

on ways to operate the programs so that they will have larger ATETs than they presently do. In

this section, we discuss some of what we do and do not know about program operation issues

under three broad headings: performance management, program participation (i.e. how potential

participants find their way to programs), and how participants get matched to particular services

within programs and to jobs after they finish programs.



Performance management

The Department of Labor’s flagship employment and training programs have played an

important role in the intellectual and institutional development of federal performance


134
   Of course, the authors of these evaluations typically have a very clear sense of these issues, which often arise
from institutional, political and data limitations beyond their control.

                                                         107
management, starting with initial efforts under the CETA program. JTPA and WIA featured

quantitative performance management systems operating at both the state and local levels that

included financial incentives for good performance as well as potential penalties for poor

performance; WIOA retains the WIA system with some modest modifications.135 Courty and

Marschke (2011a) provide a detailed description of the JTPA system, and Heinrich (2004) does

the same for WIA.

        One can think about the performance management systems for U.S. government-

sponsored training programs as trying to accomplish two things: (1) provide quick and

inexpensive proxies for impact estimates that would otherwise take a long time and cost a lot of

money; (2) motivate program staff to work harder (i.e. apply more effort) and to work smarter

(i.e. to figure out how to make a given amount of effort yield a higher payoff via changes in how

the program operates). Success on the second task requires success on the first, for if the

performance measures do not proxy effectively for impacts (i.e. changes in labor market

outcomes relative to a counterfactual) then pressing programs to do well on them may reduce,

rather than increase, their economic efficiency.

        Concerns about performance measures in the economics literature center on three issues.

The first is the correlation between the performance measures and program impacts. Here, the

available evidence suggests concern, if not alarm, as the literature provides essentially no

evidence of such a correlation; see in particular Barnow (2000) and Heckman, Heinrich, and

Smith (2002) for studies that make use of the data from the JTPA experiment and Schochet and

Burghardt (2008) for evidence from the Job Corps experiment.

135
   The most notable change concerns the reinstatement of regression adjustment of the performance measures based
on participant characteristics. JTPA used such adjustments but WIA did not. Intuitively, regression adjustment aims
to present local training centers with a level playing field, though one might argue that conditioning on the
characteristics of the eligible population, rather than of the chosen participants, would do this better. See the
discussion in Eberts, Bartik, and Huang (2011).

                                                       108
        The second concern springs from the literature on principal agent models when agents

have multiple tasks; see, e.g., Dixit (2002) for an overview in a public sector context. This

literature teaches that what gets rewarded gets done. If the government, acting on behalf of the

taxpayer, wants training program staff to do five tasks, but the performance management system

rewards only two of them, then we would expect to see training centers do a lot of those two and

not much of the other three. Thus, for example, performance measures based on labor market

outcomes in the relatively short run (e.g., anytime in the first year after participation) should lead

programs away from services that have long-run impacts at the cost of short run reductions in

outcomes, such as training in a new occupation, and toward services that improve short-run

outcomes, such as job search assistance, regardless of their effect on long-run impacts.

        The third concern centers on strategic responses to performance management. These

include cream-skimming, the literature’s term for selecting participants based on their expected

outcome with training (i.e., Y1 ) rather than based on expected impacts from training (i.e., Y1  Y0 ),

where the latter maximizes the (economic) efficiency of the program. This concern follows

immediately from the fact that, as described earlier, existing performance measures consist

entirely of variants of Y1 .

        Other potential strategic responses include manipulating the timing and incidence of

formal enrollment as well as the timing of formal termination from the program in response to

performance measures that include only those formally enrolled and which measure outcomes

over defined program years. Under the nonlinear reward functions common in job training

programs, it can make sense to reallocate weak trainees over time to particular periods by

manipulating the timing of enrollment and termination. Suppose, for example, that a training

center gets rewarded for an entered employment rate that exceeds 0.80 by any amount in a given

                                                  109
program year, but that, absent a strategic response, it has a rate of 0.78 in every program year. If

it can manipulate the program year in which marginal trainees count toward the performance

measure so as to alternate its entered employment rate between 0.76 and 0.80, it becomes better

off under the performance management system, but without actually improving labor market

outcomes in any way (and perhaps with an expenditure of real resources on the strategic

response). The literature provides a wealth of compelling empirical evidence on both crude and

also remarkably subtle responses to the incentives implicit in the performance management

systems of U.S. job training programs: see Courty and Marschke (2011b) for an overview as well

as Barnow and King (2005). See Barnow and Smith (2004) and Heckman et al. (2011) for more

extensive summaries of the literature on performance management in U.S. employment and

training programs, Radin (2006) for a critique from outside economics that emphasizes different

concerns than we do here, and Wilson (1989) for a thoughtful presentation of the underlying

problems of public management that motivate performance management.



Program participation

Studies of program participation consider how individuals come to participate in social

programs. Such studies have interest for several reasons. First, program participation represents a

choice, and economists (and other social scientists) like to understand the choices individuals

make. Second, understanding how individuals choose to participate in programs aids in program

design and targeting. Third, an understanding of the participation process provides the

foundation for credible non-experimental evaluation. Fourth, it also informs discussions of

external validity to the set of eligible non-participants. Fifth, program operators (and voters) may

care about the equity with which programs services get distributed to particular identifiable



                                                110
groups within the eligible population. Currie (2006) reviews the literature on program

participation.

       In an institutional sense, participants find government-sponsored training programs in a

variety of ways. They may get a referral from a friend or neighbor or from a social worker or

caseworker in another program. They may, as the government hoped when it mandated co-

location, head to the One-Stop center for some other purpose and, once there, find the lure of the

current employment and training program impossible to resist. They may get referred by service

providers, as when individuals seeking vocational training at their local community college get

sent to the WIOA office to try to obtain funding for that endeavor. In contrast, some participants

participate due to a requirement rather than a choice. For example, 9.5 percent of those

randomized in the JTPA experiment report that a welfare program required them to participate

and 0.5 percent report a court doing so. The Worker Profiling and Reemployment Services

(WPRS) program and the Reemployment and Eligibility Assessment (REA) program require

some UI claimants to participate in reemployment services (sometimes but not often including

training) or risk disqualification for benefits. Finally, among those who make it into a program,

actual enrollment depends in part on caseworker behavior. They may, perhaps out of goodwill

and perhaps out of a desire to improve their measured performance, discourage some potential

participants from enrolling by requiring additional visits to the one-stop center or by referring

them to alternative services, while encouraging others. Heckman, Smith, and Taber (1996) find

that caseworkers at the JTPA center in Corpus Christi, Texas appear to emphasize equity

concerns rather than performance concerns in the process by which applicants became enrollees,

a process that also includes applicant self-selection.

       Standard economic models of participation tend not to emphasize these institutional



                                                 111
features. Instead, as with the simple model we discussed earlier, they focus on more abstract

notions of opportunity costs and expected benefits. Individuals participate when they face low

costs to doing so, due to either ongoing skills deficits or transitory labor market shocks such as

job loss, and when they expect large impacts from doing so.136 They may also view participation

as a form of assisted job search, either literally, as when receiving job search assistance or

subsidized on-the-job training at a firm, or figuratively, as when new skills learned in classroom

training improve the frequency or quality of job offers. The literature also includes some

informal discussion of the possible importance of credit constraints due to the absence of

stipends or other payments for training participants in most current programs – the Job Corps is

an exception in providing room and board – and the resulting value of alternative sources of

financial support such as transfers or family support during training. The potentially crucial role

of information, both in making the possibility of participation salient enough to induce explicit

choice and in the sense of forming ideas about potential benefits, has played little role in the

theoretical literature on training participation and only a very modest role in the empirical

literature, as we describe next.

         The empirical literature consists primarily of multivariate studies of the observed

determinants of participation, with the determinants including demographics, human capital

variables, past labor market outcomes and so on. The estimated reduced form effects of these

variables then get interpreted in light of the sorts of theories just described. For example, a

negative coefficient on age would suggest that younger workers perceive a higher benefit to

participation due to more time over which to realize any earnings gain the training provides. In


136
    Ashenfelter (1983) emphasizes that for particularly attractive means-tested programs, potential participants may
choose to reduce their opportunity cost of participation (e.g. by quitting a job) in order to qualify while Moffitt
(1983) adds stigma to the participation cost-benefit calculation. We suspect that neither factor plays much role in the
training context.

                                                         112
some cases, the participation model functions mainly as an input into estimation of treatment

effects via some estimator based on the propensity score, rather than as the primary object of

interest in the study.

        Several such studies look at the JTPA program. Anderson et al. (1993) examine

participation in JTPA in Tennessee by comparing program records on enrollees with a sample of

eligibles constructed from the Current Population Survey – a very imperfect enterprise for

reasons outlined in Devine and Heckman (1996)’s study of the JTPA-eligible population. Their

multivariate analysis reveals blacks, high school dropouts, and individuals with disabilities as

underrepresented among participants, which they interpret as evidence of cream-skimming

resulting from the JTPA performance standards.

        Heckman and Smith (1999) study the JTPA participation process using rich data on

experimental control group members and eligible non-participants at four of the sites in the JTPA

experiment. Their headline findings concern the importance of labor force status transitions in

the months leading up to the participation decision in determining participation, especially

transitions to unemployment. These transitions need not entail a simultaneous change in

earnings, as when an individual goes from “out of the labor force” to “unemployed” by initiating

job search. This finding in turn suggests that analyses that rely solely on earnings and

employment may miss an important part of the participation picture (and so may end up with

biased impact estimates as well). Their analysis also highlights the importance of family factors,

including marital status and family income, in determining participation, along with the usual

suspects identified in other studies, such as age (declining) and education (hill-shaped).

        Finally, Heckman and Smith (2004) combine the data from the National JTPA Study with

data from the Survey of Income and Program Participation (SIPP) to decompose the process that



                                                113
leads from JTPA eligibility to JTPA enrollment into a series of stages: eligibility, awareness,

application, acceptance (defined to mean reaching random assignment), and enrollment. Though

descriptive in nature, the analysis reveals a number of important findings. First, decomposing the

steps from eligibility to enrollment reveals that for some groups the key stage is program

awareness, rather than enrollment conditional on application or acceptance. This adds nuance to

the findings in the Anderson et al. (1993) paper and signals that substantively important

differences in participation conditional on eligibility among groups arise from factors other than

the incentives implicit in the performance management system. Second, looking at the stage

from acceptance to enrollment – the stage over which program staff has the most influence –

does suggest some role for the performance management system as individuals with

characteristics that predict relatively weak labor market outcomes have lower probabilities of

enrollment. Finally, simply making a particular group eligible for a program does not mean that

they will take it up.

        We know of only one such study for WIA, namely the analysis in Andersson et al.

(2013). Andersson et al. (2013) present both univariate and multivariate analyses (see their

Table 3) of the characteristics of WIA enrollees that predict receipt of training. In particular, they

find that younger enrollees have a greater chance of receiving training, which makes sense in

terms of the basic advice of the lifecycle human capital model. They also find a hill-shaped

conditional pattern by years of schooling, with those in the middle of the distribution, i.e. those

with a high school diploma or some college, having the highest probability of training. This

makes sense as well. Many training courses require high school completion and, even if they do

not, they may require mastery of relatively technical written material. At the upper end of the

distribution, college graduates likely have little need for further training in general (or may have



                                                 114
other issues that training will not fix). Finally, while Andersson et al. (2013) find differences in

univariate training chances between whites and non-whites, these largely disappear in the

multivariate analyses.

       In our view, participation in both employment and training programs in general and in the

training components of those programs in particular remains fertile ground for additional

research. In particular, the role of information in leading to program awareness and then to

participation, the formation of ex ante beliefs about likely program impacts, the determinants of

the timing of training within spells of unemployment or non-employment, and the role of other

family members merit further researcher attention.



Matching participants to services

Large general employment and training programs such as JTPA, WIA, and WIOA face the

complicated problem of matching particular participants to particular services. Even within broad

service types, such as classroom training or subsidized on-the-job training, this represents a non-

trivial problem. A given program office may have several different classroom training providers

offering programs of varying lengths and varying skill prerequisites that aim to prepare trainees

for a variety of different occupations as well as an array of heterogeneous employers willing to

consider program participants for subsidized on-the-job training slots. This section briefly

reviews the (remarkably) small extant literature in economics that considers different ways to

match participants with services.

       Caseworkers play a pivotal role in matching participants to services in the major U.S.

employment and training programs (as they do elsewhere in the developed world). Typical

motivations for this practice revolve around information asymmetries between the caseworker



                                                 115
and the participant due to the caseworker’s superior knowledge of local service providers, of

local labor market conditions (e.g. occupations in demand), and (more speculatively) of the best

matches, in terms of earnings and employment impacts, between participant characteristics and

preferences and particular services and occupations.137

         We have only very limited evidence in the United States (and not much more elsewhere)

regarding how and how well caseworkers assign participants to services. The JTPA experiment

and Andersson et al.’s (2013) WIA observational study both provide some information regarding

what caseworkers believe about optimal service assignment rules. For example, Kemple,

Doolittle, and Wallace (2003) find a number of ex ante reasonable patterns in univariate analyses

for adults using the JTPA experimental data: (1) participants without a high school diploma or

GED have a higher probability of assignment to adult basic education and a lower probability of

assignment to classroom training in occupational skills; (2) participants receiving cash

assistance, who thus have a source of income during training other than work, are more likely to

receive classroom-based services; and (3) participants with limited work experience have a lower

probability of assignment to job search assistance and subsidized on-the-job training (the latter of

which requires a willing employer). Smith (1992) and Plesca and Smith (2007) provide further

analyses using the JTPA data, while the Andersson et al. (2013) findings described in detail in

our discussion of the determinants of participation in training provide evidence for the WIA

program. Taken together, the analyses from the JTPA and WIA programs suggest that

caseworkers have some reasonable ideas about service assignment as a function of participant

characteristics, with the caveat that in both programs caseworkers take client interests and


137
   Caseworkers also perform a number of other functions, including referring participants to other services such as
substance abuse programs, transfer programs and so on, helping participants clarify their interests and abilities,
providing informal instruction in job search, monitoring eligibility and search intensity, and so on. Bloom et al.
(2003) investigates some of these other aspects of the caseworker role.

                                                        116
preferences into account, so that the observed patterns reflect the views of both groups.

       A different line of research estimates heterogeneous treatment effects as a function of

observed participant characteristics using experimental or observational variation and then uses

those estimates to examine how well, or how poorly, existing caseworker service assignment

patterns do relative to the minimum and maximum impacts possible given the estimates. Plesca

and Smith (2005) undertake this exercise using the JTPA experimental data and consider

assignment to the three experimental “treatment streams” based on services recommended prior

to random assignment. They find benefits from assigning treatment stream using a statistical

treatment rule based on estimated impacts relative to caseworker assignment. Lechner and Smith

(2007) perform a similar exercise using observational data (with larger samples) from

Switzerland and find that caseworkers do about as well as random assignment to treatment, and

thus leave substantial potential gains on the table. Their paper emphasizes the importance of

respecting capacity constraints under alternative allocation schemes. McCall, Smith, and Wunsch

(2015) summarize the broader European literature, which reaches an overall conclusion similar

to that of Lechner and Smith (2007).

       A pair of experiments provides further evidence on caseworker performance at the

service assignment task. Bell and Orr (2002) analyze the AFDC Homemaker-Home Health Aide

Demonstrations. In that study, caseworkers predicted both the untreated outcome and the impact

for each experimental sample member prior to random assignment. Interacting the treatment

indicator with the impact prediction in the impact estimation reveals that caseworkers in this

context have no idea who will benefit from training as a homemaker / home health aide. They do

a much better job at predicting untreated outcome levels. This experiment shows what

caseworkers know about the impact of one particular treatment, which is related to, but not the



                                               117
same as, picking the service with the highest expected impact. We think more experiments

should undertake exercises like this one.

           The second experiment, reported in Perez-Johnson, Moore, and Santillano (2011)

compares alternative administrative models for delivering ITAs using a sample of WIA enrollees

determined eligible for ITAs in eight sites in six states.138 The experiment included three

treatment arms: structured choice, guided choice, and maximum choice, which differed primarily

on three dimensions. First, under structured choice, but not the other two arms, the caseworker

had veto power over training choices. Second, under structured choice, but not the other arms,

the caseworker had discretion over the dollar value of the ITA. Third, the amount of counseling

regarding the training choice varied from mandatory and substantial under structured choice, to

mandatory and less intensive under guided choice, to optional under maximum choice. In all

treatment arms, the eligible training provider list and any local rules about in-demand

occupations constrained the training choices.

           Operationally, the caseworkers were reluctant to be as directive regarding client training

choices as envisioned in the original design for the structured choice treatment arm. Instead,

according to Perez-Johnson, Moore, and Santilano (2011, pg. xxvii) caseworkers “tended to

award Structured Choice customers’ ITAs that enabled them to attend their preferred training

programs.” For this reason, program costs for the structured choice arm proved higher than for

the other two arms. Potential trainees in the maximum choice arm largely opted out of

counseling, providing a revealed preference evaluation of that service at the margin. A larger

fraction of those in the maximum choice arm used ITAs, but overall training rates (including

both ITA-funded and other training) and the occupational mix of training differed little across the

three treatment arms. Enrollees in the structured choice and maximum choice arms had
138
      See also the earlier reports by McConnell et al. (2006) and McConnell, Decker and Perez-Johnson (2006).

                                                         118
substantively and statistically larger probabilities of completing a training course and of earning

a credential.

           Earnings and employment outcomes differ somewhat between the survey data and the

administrative data from state UI records. The report gives (somewhat unusually, relative to the

literature) greater weight to the survey data, while we lean toward giving them equal weight. In

the survey data, the constrained choice arm shows the highest earnings over all post-program

periods, with a difference of about $500 per quarter in the final two years of follow-up (roughly

2008-2009) relative to the guided choice arm and about $250 per month relative to the maximum

choice arm, though the latter difference fails to attain traditional levels of statistical significance.

In contrast, in the administrative data reveal only small differences in labor market outcomes: for

example, in the final two years of the follow-up period (calendar years 2008-2009), average

quarterly earnings equal $4818, $4713, and $4734 for the structured choice, guided choice and

maximum choice arms, respectively, with none of the differences statistically significant.

Overall, Perez-Johnson, Moore, and Santilano (2011) conclude that the stronger impact

performance of the structured choice arm has more to do with the larger dollar value of the ITAs

in that arm than with caseworker value-added. At the same time, the marginally better

performance of the maximum choice arm relative to the guided choice arm, a contrast that

highlights the value-added of the caseworkers as these arms both included the same relatively

low cap on ITA value, suggests that caseworkers add little if any value in their informational

role.139

           The leading alternative to having caseworkers assign participants to services consists of

allowing participants to assign themselves to services, typically via some form of voucher, such

139
    An additional, less direct, way to evaluate the match between trainees and training measures the extent to which
trainees end up in jobs directly related to their training as in Park (2012). The key issue in this approach relates to
the benchmark – how much mismatch is too much, given that the optimum is not 100 percent?

                                                          119
as the ITAs under WIA. The literature refers to this as demand-driven assignment. Arguments in

favor of demand-driven assignment include (1) participants likely have private information about

their tastes and abilities that allow them to make better matches than caseworkers; (2)

participants may work harder and be more likely to complete programs and courses they choose

for themselves; and (3) participant choice may put more competitive pressure on providers to do

a good job. As noted in our discussion of the ITA experiment just above, ITAs under WIA

typically embody a combination of caseworker input and participant choice, within the

constraints of the eligible provider list.

        The literature offers only limited evidence regarding vouchers in the training context. The

ITA experiment described just above represents the best we have. Reframed from the voucher

perspective, it shows that more flexible vouchers (i.e. vouchers less constrained by caseworkers

and program rules) increase training incidence somewhat, do not change the mix of training very

much, and marginally improve outcomes relative to the status quo of guided choice. Barnow

(2009) provides a survey of the older U.S. literature that emphasizes thinking about a continuum

of options with varying degrees of customer control and program guidance and limitation.

McCall, Smith, and Wunsch (2015) include the somewhat larger European literature in their

survey. Based on our reading, the literature suggests surprisingly modest effects of additional

customer choice on impacts but some impact on customer satisfaction. Additional research on

how participants use information in making choices, and on the effects of additional types of

information on choices and outcomes, represents a logical next step.

        In addition to participants and caseworkers, institutional factors also play an important

role in determining service assignments. First, the law typically encourages programs to offer

training in occupations actually in demand in the local labor market; under WIA, local programs



                                                120
vary in how, and how enthusiastically, they implement this aspect of the law. Second, the

availability of local service providers constrains the set of available options; as a result, for

example, WIA programs in urban areas typically offer a broader array of training options than

those in rural areas. The reluctance of some providers to jump through the hoops required to get

on the eligible provider list described earlier in our discussion of WIA implementation further

limits the available options in some areas. Finally, broader institutional enthusiasm for particular

services or service sequences, as with the “core then intensive then training” sequence in the

WIA program, have an influence on service patterns.140

         The literature suggests that caseworkers do not add much value in directing participants

into particular services or trainees into particular training courses. This does not mean that they

could not do better and it could just mean that they seek to maximize something else, such as

equity or measured performance, instead of value-added. It also does not mean that they do not

add value in their other roles – see, e.g., Rosholm (2014) and the broader discussion in McCall,

Smith and Wunsch (2015). We still have much to learn regarding this dimension of the training

provision process.



Summary and conclusions

The United States continues to spend relatively little on employment and training programs in

general or on government-sponsored training more narrowly than most other developed

countries. It remains unclear which countries (if any) have found the optimum. The years since

LaLonde (2003) have seen some valuable research on employment and training programs in the

U.S., but the quantity of high quality work remains low. We conjecture that this lack results from

140
   A small literature considers, with a combination of theory and calibration, the optimal mix of broad service
categories and their interaction with the design of social insurance and transfer programs. See e.g. Wunsch (2013)
and the references therein.

                                                        121
both the relatively small budgetary footprint of this program category as well as from data and

data access limitations. Taken together, the recent evidence presents a mixed but somewhat

disheartening picture. WIA training and WIA overall have fairly robust positive earnings effects

for both men and women served under the adult funding stream, effects that tend to pass cost-

benefit tests under reasonable assumptions. In contrast, WIA training and WIA overall appear to

have a negative effect on individuals served under the dislocated worker funding stream. We find

the available non-experimental evidence a bit more compelling for WIA training versus WIA

without training than for WIA versus no-WIA and the findings for adults appear more robust to

mildly different design decisions and/or to the set of states studied than the findings for

dislocated workers. More attention to explaining the differences across states and streams would

have great value; perhaps the ongoing WIA experimental evaluation will shed some light.

       The TAA analysis reveals that we should perhaps seek a more efficient way to

compensate workers who suffer individually while the public benefits from reduced trade

barriers. The Job Corps experiment highlights the potential value of immersive, residential

treatments in changing the outcomes of youth, while at the same time the fact that any positive

impacts, even ones that end up not passing a cost-benefit test, elicit cheers from the audience

reinforces the difficulty of the underlying task.

       Given the demonstrated inability of the U.S. political system to kill even programs with

dismal evaluation track records stretching over decades, future evaluation research should focus

relatively more on impacts on marginal participants, which would inform decisions to increase or

decrease program budgets at the margin, and on ways to improve program design,

implementation and performance management, as with the WIA ITA experiment.

       The last two decades have seen a major “data gap” emerge between the United States and



                                                    122
various central and northern European countries. The administrative data available for research

on government-subsidized training programs in the United States pales in comparison to that

available in e.g. Germany, Sweden, or Denmark in its quality (i.e. richness of individual

characteristics, temporal fineness of outcome variables, lack of measurement error in the timing

and incidence of service receipt and enrollment, etc.), the ease with which serious researchers

can gain access to it, and the ease with which they can use it if they do gain access. These

limitations associated with administrative data in the United States mean that much policy-

relevant research that would improve our understanding of training programs does not get done.

This research would often cost the government little or nothing as graduate students and

professors would do it in order to generate publishable papers for which they receive indirect

compensation.

       At the same time, it remains essentially impossible to undertake evaluations of job

training programs using standard social science data sets in the United States due to sample size

issues in the major panel data sets (e.g. the Panel Study of Income Dynamics) and due to

measurement issues (especially poor measurement of program participation) in both the cross-

sectional data sets and the panel data sets. Matching of administrative data on participants to one

or more of the major surveys – we suggest the SIPP, which combines relatively large sample

sizes with a short panel and detailed information on earnings and program participation - could

address the measurement issues at relatively low cost, and allow the generation of important new

knowledge about how the citizenry interacts with these programs.

       Other areas where data remain weak in the U.S. context could be addressed with less

controversy. While the Department of Labor provides some information about variation in state

UI programs over time, similar (and, ideally, more comprehensive) information on many other



                                                123
programs such as WIA (and now WIOA), the Worker Profiling and Reemployment Services

System (WPRS), and the Reemployment and Eligibility Assessment (REA) program does not

exist to our knowledge.141 Providing it would facilitate research on these specific programs and

on the system of active and passive labor market programs as a whole. Also valuable, as noted

earlier, would be improved information on program costs, on average and at the margin, for

different types of services, for different types of clients, and in different locations. The

intersection between community colleges and employment and training programs would also

benefit from improved data; at present community college data do not indicate which students

have their courses paid for by programs such as WIA and neither the aggregated WIA data

available to the public nor the WIA administrative records typically provided to researchers

indicate the identity of individual service providers. The intersection between workforce

development programs and the community college system has great substantive importance;

having the data required for serious research would allow evidence-based policy to improve it.

        On the methods side, the United States continues to lead the world in the evaluation of

government-sponsored training programs via large scale social experiments. Both the Job Corps

experiment and the WIA experiment solve important problems regarding site selection and

external validity that arose in the earlier JTPA experiment. The non-experimental evaluations of

WIA and TAA reflect, to the extent allowed by the data, recent advances in the literature on non-

parametric and semi-parametric estimation of treatment effects. European studies of the value of

particular conditioning variables have served to make these U.S. studies more credible by

showing that some of the variables absent in the United States do not add that much in terms of

bias reduction. On the negative side, the tidal wave of compelling studies of educational


141
   See http://www.unemploymentinsurance.doleta.gov/unemploy/statelaws.asp for the DOL information on state UI
laws.

                                                    124
interventions using regression discontinuity designs over the past decade has no analogue in the

job training literature due to the on-going failure to “design in” usable discontinuities in this

policy domain. Similarly, the federal government often misses opportunities for staged roll-outs

of programs, which would allow the application of standard panel data estimation methods.

           Finally, we note the potential for institutional reform in the broad sense, designed to

embody an alternative vision of what Smith (2011) calls “evaluation policy.” The success of the

Department of Education’s Institute for Education Sciences (IES) at generating truly remarkable

improvements in the quality of official evaluations of educational interventions (and, indeed, in

the entire academic literature that evaluates educational interventions) suggests consideration of

a similar institution in the world of active labor market programs.142 Similarly, the success of the

requirement that tied rigorous evaluation to the granting of waivers under the old AFDC program

in the 1980s and 1990s suggests a similar scheme for allow states to innovate in their workforce

systems in exchange for providing the public good of high-quality evidence.




142
      See Institute of Education Sciences (2008) for more on the IES success story.

                                                           125
References

Abadie, Alberto, Joshua Angrist, and Guido Imbens. 2002. “Instrumental Variables Estimates of
the Effects of Subsidized Training on the Quantiles of Trainee Earnings.” Econometrica 70(1):
91-117.

Anderson, Kathryn, Richard Burkhauser, and Jennie Raymond. 1993. “The Effect of Creaming
on Placement Rates under the Job Training Partnership Act.” Industrial and Labor Relations
Review 46: 613–24.

Andersson, Fredrik, Harry Holzer, Julia Lane, David Rosenblum, and Jeffrey Smith. 2013.
“Does Federally-Funded Job Training Work? Nonexperimental Estimates of WIA Training
Impacts Using Longitudinal Data on Workers and Firms.” NBER Working Paper No. 19446.

Angrist, Joshua, Guido Imbens, and Donald Rubin. 1996. “Identification of Causal Effects Using
Instrumental Variables.” Journal of the American Statistical Association 91(434): 444-455.

Angrist, Joshua and Alan Krueger. 1999. “Empirical Strategies in Labor Economics.” In Orley
Ashenfelter and David Card (eds.) Handbook of Labor Economics, Volume 3A. Amsterdam:
North-Holland. 1277-1366.

Angrist, Joshua and Jorn-Steffen Pischke. 2010. “The Credibility Revolution in Empirical
Economics: How Better Research Design Is Taking the Con out of Econometrics.” Journal of
Economic Perspectives 24(2): 3-30.

Ashenfeltcr, Orley. 1978. “Estimating the Effect of Training Programs on Earnings.” Review of
Economics and Statistics 6(1): 47-57

Ashenfelter, Orley. 1983. “Determining Participation in Income-Tested Social Programs,”
Journal of the American Statistical Association 78: 517-525.

Balducchi, David, Terry Johnson ,and Mark Gritz. 1997. “The Role of the Employment Service.”
In Christopher O’Leary and Stephen Wandner (eds.) Unemployment Insurance in the United
States: Analysis of Policy Issues. Kalamazoo, MI: W.E. Upjohn Institute for Employment
Research. 457-504.

Barnow, Burt. 1987. “The Impact of CETA Programs on Earnings: A Review of the Literature.”
Journal of Human Resources 22(2): 157-193.

Barnow, Burt. 1993. “Thirty Years of Changing Federal, State, and Local Relationships in
Employment and Training Programs.” Publius: The Journal of Federalism 23(3): 75-94.

Barnow, Burt. 2000. “Exploring the Relationship between Performance Management and
Program Impact: A Case Study of the Job Training Partnership Act.” Journal of Policy Analysis
and Management 19(1): 118-141.



                                             126
Barnow, Burt. 2009. “Vouchers in U.S. Vocational Training Programs: An Overview of What
We Have Learned.” Journal for Labor Market Research 42: 71-84.

Barnow, Burt. 2011. “Lessons from the WIA Performance Measures.” In Douglas Besharov and
Phoebe Cottingham (eds.) The Workforce Investment Act: Implementation Experiences and
Evaluation Findings. Kalamazoo, MI: W.E. Upjohn Institute for Employment Research. 209-
231.

Barnow, Burt, and David Greenberg. 2015. “Do Estimated Impacts on Earnings Depend on the
Source of the Data Used to Measure Them? Evidence from Previous Social Experiments.”
Evaluation Review 39(2): 179-228.

Barnow, Burt and Hobbie, Richard (eds.). 2013. The American Recovery and Reinvestment Act:
The Role of Workforce Programs. Kalamazoo, MI: W.E. Upjohn Institute for Employment
Research.

Barnow, Burt and Christopher King. 2005. The Workforce Investment Act in Eight States. Report
Prepared for U.S. Department of Labor, Employment and Training Administration.

Barnow, Burt and Demetra Nightingale. 2007. “An Overview of U.S. Workforce Development
Policy in 2005.” In Harry Holzer and Demetra Nightengale (eds.) Reshaping the American
Workforce in a Changing Economy. Washington, DC: Urban Institute Press.
Barnow, Burt, and Jeffrey Smith. 2004. “Performance Management of U.S. Job Training
Programs” in Christopher O’Leary, Robert Straits and Stephen Wandner (eds.), Job Training
Policy in the United States, Kalamazoo, MI: W.E. Upjohn Institute for Employment Research,
21-56.

Barnow, Burt and John Trutko. 2015. “The Value of Efficiency Measures: Lessons from
Workforce Development Programs.” Public Performance and Management Review 38: 487-513.

Baum, Sandy, Diane Cardenas Elliott, and Jennifer Ma. 2014. Trends in Student Aid 2014. New
York, NY: College Board.

Bell, Stephen and Larry Orr. 2002. “Screening (and Creaming?) Applicants to Job Training
Programs: the AFDC Homemaker–Home Health Aide Demonstrations.” Labour Economics 9:
279-301.

Bell, Stephen, Larry Orr, John Blomquist, and Glen Cain. 1995. Program Applicants as a
Comparison Group in Evaluating Training Programs. Kalamazoo, MI: W.E. Upjohn Institute
for Employment Research.

Betsey, Charles, Robinson Hollister, and Mary Papageorgiou (Eds.). 1985. Youth Employment
and Training Programs: The YEDPA Years. Washington, DC: National Academies Press

Bitler, Marianne, Jonah Gelbach, and Hilary Hoynes. 2005. “Distributional Impacts of the Self-
Sufficiency Project.” NBER Working Paper No.11626.
                                              127
Bitler, Marianne, Jonah Gelbach, and Hilary Hoynes. 2006. “What Mean Impacts Miss:
Distributional Effects of Welfare Reform Experiments.” American Economic Review 96:
988–1012.

Black, Dan, Jose Galdo and Jeffrey Smith. 2007. “Evaluating the Worker Profiling and
Reemployment Services System Using a Regression Discontinuity Design.” American Economic
Review Papers and Proceedings 97(2): 104-107.

Black, Dan, Jeffrey Smith, Mark Berger, and Brett Noel. 2003. “Is the Threat of Reemployment
Services More Effective than the Services Themselves? Evidence from Random Assignment in
the UI System.” American Economic Review 93(4): 1313-1327.

Blank, Diane, Laura Heald, and Cynthia Fagnoni. 2011. “An Overview of WIA.” In Douglas
Besharov and Phoebe Cottingham (eds.) The Workforce Investment Act: Implementation
Experiences and Evaluation Findings. Kalamazoo, MI: W.E. Upjohn Institute for Employment
Research. 49-78.

Bloom, Howard. 1984. “Accounting for No-Shows in Experimental Evaluation Designs.”
Evaluation Review 8(2): 225-46.

Bloom, Howard, Carolyn Hill, and James Riccio. 2003. “Linking Program Implementation and
Effectiveness: Lessons from a Pooled Sample of Welfare-to-Work Experiments.” Journal of
Poicy Analysis and Management 22(4): 551-575.

Boo, Katherine. 2004. “Letter from South Texas: The Churn.” New Yorker. March 29.

Borden, William. 2011. “The Challenges of Measuring Performance.” In Douglas Besharov and
Phoebe Cottingham (eds.) The Workforce Investment Act: Implementation Experiences and
Evaluation Findings. Kalamazoo, MI: W.E. Upjohn Institute for Employment Research. 177-
208.

Borus, Michael and Daniel Hamermesh. 1978. “Estimating Fiscal Substitution by Public Service
Employment Programs.” Journal of Human Resources 13(4): 561-565.

Bound, John, Charles Brown and Nancy Mathiowetz. 2001. “Measurement Error in Survey
Data.” In James Heckman and Edward Leamer (eds.) Handbook of Econometrics, Volume 5.
Amsterdam: North-Holland. 3705-3843.

Bradley, David. 2013. The Workforce Investment Act and the One-Stop Delivery System. CRS
Report for Congress. Washington, DC: Congressional Research Service.

Burtless, Gary and Larry Orr. 1995. “The Case for Randomized Field Trials in Economic and
Policy Research.” Journal of Economic Perspectives 9(2): 63-84.

Bushway, Shawn, Brian Johnson and Lee Ann Slocum. 2007. “Is the Magic Still There? The Use

                                            128
of the Heckman Two-Step Correction for Selection Bias in Criminology.” Journal of
Quantitative Criminology 23: 151-178..

Busso, Matias, John DiNardo and Justin McCrary. 2014. “New Evidence on the Finite Sample
Properties of Propensity Score Reweighting and Matching Estimators.” Review of Economics
and Statistics 96(5): 885-897.

Butler, Wendell and Richard Hobbie. 1976. Employment and Training Programs. Congress of
the United States, Congressional Budget Office.

Card, David, Jochen Kluve, and Andrea Weber. 2010. “Active Labour Market Policy
Evaluations: A Meta-Analysis.” Economic Journal 120: F452-F477.

Card, David, Jochen Kluve, and Andrea Weber. 2015. “What Works? A Meta Analysis of
Recent Active Labor Market Program Evaluations.” NBER Working Paper No. 21431.

Carniero, Pedro, Karsten Hansen and James Heckman. 2003. “Estimating Distributions of
Treatment Effects with an Application to the Returns to Schooling and Measurement of the
Effects of Uncertainty on College Choice.” International Economic Review 44(2): 361-422.

Carneiro, Pedro, James Heckman, and Edward Vytlacil. 2011. “Estimating Marginal Returns to
Education.” American Economic Review 101(6): 2754-2781.

Cave, George, Hans Bos, Fred Doolittle and Cyril Toussaint. 1993. JOBSTART: Final Report on
a Program for School Dropouts. New York, NY: MDRC.

Center on Budget and Policy Priorities. 2012. Policy Basics: An Introduction to TANF.
Washington, DC: Center on Budget and Policy Priorities.

Clague, Ewan and Leo Kramer. 1976. Manpower Policies and Programs: A Review. Kalamazoo,
MI: W.E. Upjohn Institute for Employment Research.

Cook, Robert, Charles Adams, and Lane Rawlins.1985. Public Service Employment: The
Experience of a Decade. Kalamazoo, MI: W.E. Upjohn Institute for Employment Research.

Cook, Thomas. 2008. “`Waiting for Life to Arrive’: A History of the Regression-Discontinuity
Design in Psychology, Statistics and Economics.” Journal of Econometrics 142(2): 636–654.

Couch, Kenneth. 1992. “New Evidence on the Long-Term Effects of Employment Training
Programs.” Journal of Labor Economics 10(4): 380-388.

Courty, Pascal and Gerald Marschke. 2011a. “The JTPA Incentive System.” In James Heckman,
Carolyn Heinrich, Pascal Courty, Gerald Marschke and Jeffrey Smith (eds.) The Performance of
Performance Standards. Kalamazoo, MI: W.E. Upjohn Institute for Employment Research. 65-
93.



                                             129
Courty, Pascal and Gerald Marschke. 2011b. “Measuring Government Performance: An
Overview of Dysfunctional Responses.” In James Heckman, Carolyn Heinrich, Pascal Courty,
Gerald Marschke and Jeffrey Smith (eds.) The Performance of Performance Standards.
Kalamazoo, MI: W.E. Upjohn Institute for Employment Research. 203-229.

Crépon, Bruno, Esther Duflo, Marc Gurgand, Roland Rathelot, and Philippe Zamora. 2013. “Do
Labor Market Policies have Displacement Effects? Evidence from a Clustered Randomized
Experiment.” Quarterly Journal of Economics 128(2): 531-580.

Currie, Janet. 2006. “The Take-up of Social Benefits.” In Alan Auerbach, David Card, and John
Quigley (eds.) Poverty, the Distribution of Income, and Public Policy. New York: Russell Sage.
80-148.

Dahlby, Bev. 2008. The Marginal Social Cost of Public Funds. Cambridge, MA: MIT Press.

D’Amico, Ronald, Kate Dunham, Jennifer Henderson-Frakes, Deborah Kogan, Vinz Koller,
Melissa Mack, Micheline Magnotta, Jeffrey Salzman, Andrew Wiegand, Gardner Carrick, and
Dan Weissbein. 2004. The Workforce Investment Act after Five Years: Results from the National
Evaluation of the Implementation of WIA. Oakland, California: Social Policy Research
Associates

D’Amico, Ronald and Jeffrey Salzman. 2004. “Implementation Issues in Delivering Training
Services to Adults under WIA.” In Christopher O’Leary, Robert Straits and Stephen Wandner
(eds.) Job Training Policy in the United States. Kalamazoo, MI: W.E. Upjohn Institute for
Employment Research. 101-134.

Davidson, Carl and Stephen Woodbury. 1993. “The Displacement Effect of Reemployment
Bonus Programs.” Journal of Labor Economics 11(4): 575-605.

Decker, Paul, and Gillian Berk. 2011. “Ten Years of the Workforce Investment Act (WIA):
Interpreting the Research on WIA and Related Programs.” Journal of Policy Analysis and
Management 30(4): 906-926.

Dehejia, Rajeev and Sadek Wahba. 1999. “Causal Effects in Nonexperimental Studies:
Reevaluating the Evaluation of Training Programs.” Journal of the American Statistical
Association. 94(448): 1053-1062.

Dehejia, Rajeev and Sadek Wahba. 2002. “Propensity Score Matching Methods for Non-
Experimental Causal Studies.” Review of Economics and Statistics 84(1): 151-161.

DeRocco, Emily. 2006. Training and Guidance Letter 17-05 (TEGL 17-05): Common Measures
Policy for the Employment and Training Administration’s (ETA) Performance Accountability
System and Related Performance Issues. Washington, DC: U.S. Department of Labor
Employment and Training Administration.

Devine, Theresa, and James Heckman. 1996. “The Structure and Consequences of Eligibility

                                             130
Rules for a Social Program.” In Solomon Polachek (ed.) Research in Labor Economics, Volume
15. Greenwich, CT: JAI Press. 111-170.

Dickinson, Katherine, Terry Johnson, and Richard West. 1987. “An Analysis of the Sensitivity
of Quasi-Experimental Net Impact Estimates of CETA Programs.” Evaluation Review 11(4):
452-472.

Dixit, Avinash. 2002. “Incentives and Organizations in the Public Sector: An Interpretative
Review.” Journal of Human Resources 37(4): 696-727.

Djebbari, Habiba and Jeffrey Smith. 2008. “Heterogeneous Program Impacts: Experimental
Evidence from the PROGRESA Program.” Journal of Econometrics 145(1-2): 64-80.

Doolittle, Fred and Linda Traeger. 1990. Implementing the National JTPA Study. New York,
NY: MDRC.

Eberts, Randall, Timothy Bartik, and Wei-Jang Huang. 2011. “Recent Advances in Performance
Measurement of Federal Workforce Development Programs.” In Douglas Besharov and Phoebe
Cottingham (eds.) The Workforce Investment Act: Implementation Experiences and Evaluation
Findings. Kalamazoo, MI: W.E. Upjohn Institute for Employment Research. 233-275.

Eberts, Randall and Harry Holzer. 2004. “Overview of Labor Exchange Policies and Services.”
In David Balducchi, Randall Eberts, and Christopher O'Leary (eds.) Labor Exchange Policy in
the United States. Kalamazoo, MI: W.E. Upjohn Institute for Employment Research. 1-31.
Eberts, Randall and Stephen Wandner. 2013. “Data Analysis of the Implementation of the
Recovery Act Workforce Development and Unemployment Insurance Provisions.” In Burt
Barnow and Richard Hobbie (eds.) The American Recovery and Reinvestment Act: The Role of
Workforce Programs. Kalamazoo, MI: W.E. Upjohn Institute for Employment Research. 267-
307.

Eberwein, Curtis, John Ham and Robert LaLonde. 1997. “The Impact of Classroom Training on
the Employment Histories of Disadvantaged Women: Evidence from Experimental Data.”
Review of Economic Studies 64(4): 655-682.

Eyster, Lauren, Demetra Smith Nightengale, Burt Barnow, Carolyn O’Brien, John Trutko and
Daniel Keuhn. 2010. Implementation and Early Training Outcomes of the High Growth Job
Training Initiative. Washington, DC: The Urban Institute.

Federal Register. 2014. March 27. 79(59): 17184-17188.

Ferber, Robert and Werner Hirsch. 1981. Social Experimentation and Economic Policy. New
York: Cambridge University Press.

Fisher, Ronald. 1935. The Design of Experiments. Edinburgh: Oliver and Boyd.



                                              131
Flores-Lagunes, Alfonso, Arturo Gonzalez, and Todd Neumann. 2008. “Learning But Not
Earning? The Impact of Job Corps Training on Hispanic Youth.” Economic Inquiry 48(3): 651-
667.

Ford, Reuben, David Gyarmati, Kelly Foley, Doug Tattrie, and Liza Jimenez. 2003. Can
Work Incentives Pay for Themselves? Final Report on the Self-Sufficiency Project for
Welfare Applicants. Ottawa: Social Research and Demonstration Corporation.

Forslund, Anders and Alan Krueger. 1997. “An Evaluation of Swedish Active Labor Market
Policy: New and Received Wisdom.” In Richard Freeman, Robert Topel and Birgitta
Swedenburg (eds.) The Welfare State in Transition: Reforming the Swedish Model. Chicago, IL:
University of Chicago Press for NBER. 267-298.

Fraker, Thomas and Rebecca Maynard. 1987. “The Adequacy of Comparison Group Designs for
Evaluations of Employment Related Programs.” Journal of Human Resources 22(2): 194-227.

Franklin, Grace and Randall Ripley. 1984. CETA Politics and Policy, 1973-1982. Knoxville:
TN: University of Tennessee Press.

Frölich, Markus. 2004. “Finite-sample Properties of Propensity-score Matching and
Weighting Estimators.” Review of Economics and Statistics 86: 77–90.

Frölich, Markus and Michael Lechner. 2010. “Exploiting Regional Treatment Intensity for the
Evaluation of Labor Market Policies.” Journal of the American Statistical Association 105(491):
1014-1029.

Frost, Robert. 1920. “The Road Not Taken.” In: Robert Frost (ed.), Mountain Interval. New
York: Henry Holt.

Garfinkel, Irwin. 1973. “Is In-Kind Redistribution Efficient?” Quarterly Journal of Economics
87(2): 320-330.

Gechter, Michael. 2014. “Generalizing the Results from Social Experiments.” Unpublished
manuscript, Boston University.

Greenberg, David, Charles Michalopoulos, and Philip Robins. 2003. “A Meta-Analysis of
Government-Sponsored Training Programs.” Industrial and Labor Relations Review 57(1): 31-
53.

Greenberg, David, Charles Michalopoulos, and Philip Robins. 2004. “What Happens to the
Effects of Government-Funded Training Programs Over Time?” Journal of Human Resources
39(1): 277-293.

Greenberg, David and Philip Robins. 2008. “Incorporating Nonmarket Time into Benefit–Cost
Analyses of Social Programs: An Application to the Self-Sufficiency Project.” Journal of Public
Economics 92: 766–794.

                                              132
Greenberg, David and Philip Robins. 2011. “Have Welfare-to-Work Programs Improved Over
Time in Putting Welfare Recipients to Work?” Industrial and Labor Relations Review 64(5):
920-930.

Greenberg, David and Mark Shroder. 2004. The Digest of Social Experiments, Third Edition.
Washington, DC: Urban Institute Press.

Greenberg, David, Mark Shroder and Matthew Onstott. 1999. “The Social Experiment Market.”
Journal of Economic Perspectives 13(3): 157-172.

Gueron, Judith and Howard Ralston. 2013. Fighting for Reliable Evidence. New York, NY:
Russell Sage Foundation.

Ham, John and Robert LaLonde. 1996. “The Effect of Sample Selection and Initial Conditions in
Duration Models: Evidence from Experimental Data.” Econometrica 64(1): 175-205.

Heckman, James. 1992. “Randomization and Social Policy Evaluation.” In Charles Manski and
Irwin Garfinkel (eds.) Evaluating Welfare and Training Programs. Cambridge, MA: Harvard
University Press. 201-230.

Heckman, James, Carolyn Heinrich, Pascal Courty, Gerald Marschke and Jeffrey Smith. 2011.
The Performance of Performance Standards. Kalamazoo, MI: W.E. Upjohn Institute for
Employment Research.

Heckman, James, Carolyn Heinrich, and Jeffrey Smith. 2002. “The Performance of Performance
Standards.” Journal of Human Resources 37(4): 778-811.

Heckman, James, Neil Hohmann, and Jeffrey Smith, with the assistance of Michael Khoo. 2000.
“Substitution and Dropout Bias in Social Experiments: A Study of an Influential Social
Experiment.” Quarterly Journal of Economics 115(2): 651-694.

Heckman, James, Hidehiko Ichimura, and Petra Todd. 1997. “Matching as an Econometric
Evaluation Estimator: Evidence from Evaluating a Job Training Programme.” Review of
Economic Studies 64(4): 605-654.

Heckman, James, Hidehiko Ichimura, Jeffrey Smith, and Petra Todd. 1998. “Characterizing
Selection Bias Using Experimental Data.” Econometrica 66(5): 1017-1098.

Heckman, James and Alan Krueger. 2003. Inequality in America. Cambridge, MA: MIT Press.

Heckman, James, Robert LaLonde, and Jeffrey Smith. 1999. “The Economics and Econometrics
of Active Labor Market Programs.” In Orley Ashenfelter and David Card (eds.) Handbook of
Labor Economics, Volume 3A. Amsterdam: North-Holland, 1865-2097.

Heckman, James, Lance Lochner and Christopher Taber. 1998. “General-Equilibrium Treatment

                                            133
Effects: A Study of Tuition Policy.” American Economic Review 88(2): 381-386.

Heckman, James and Salvador Navarro. 2004. “Using Matching, Instrumental Variables, and
Control Functions to Estimate Economic Choice Models.” Review of Economics and Statistics
86(1): 30-57.

Heckman, James and Richard Robb. 1985. “Alternative Methods for Evaluating the Impacts of
Interventions: An Overview.” Journal of Econometrics 30(1-2): 239-267.

Heckman, James and Jeffrey Smith. 1995. “Assessing the Case for Social Experiments.” Journal
of Economic Perspectives 9(2): 85-110.

Heckman, James and Jeffrey Smith. 1999. “The Pre-Program Earnings Dip and the Determinants
of Participation in a Social Program: Implications for Simple Program Evaluation Strategies.”
Economic Journal 109(457): 313-348.

Heckman, James and Jeffrey Smith. 2000. “The Sensitivity of Experimental Impact Estimates:
Evidence from the National JTPA Study.” In David Blanchflower and Richard Freeman (eds.),
Youth Employment and Joblessness in Advanced Countries. Chicago: University of Chicago
Press for NBER, 331-356.

Heckman, James and Jeffrey Smith. 2004. “The Determinants of Participation in a Social
Program: Evidence from the Job Training Partnership Act.” Journal of Labor Economics 22(2):
243-298.

Heckman, James, Jeffrey Smith. and Nancy Clements. 1997. “Making the Most Out of
Programme Evaluations and Social Experiments: Accounting for Heterogeneity in Programme
Impacts.” Review of Economic Studies 64(4): 487-535.

Heckman, James, Jeffrey Smith, and Christopher Taber. 1996. “What Do Bureaucrats Do? The
Effects of Performance Standards and Bureaucratic Preferences on Acceptance into the JTPA
Program.” In Gary Libecap (ed.) Advances in the Study of Entrepreneurship, Innovation and
Economic Growth: Reinventing Government and the Problem of Bureaucracy, Volume 7.
Greenwich, CT: JAI Press, 191-217.

Heckman, James, Jeffrey Smith, and Christopher Taber. 1998. “Accounting for Dropouts in
Evaluations of Social Programs.” Review of Economics and Statistics 80(1): 1-14

Heinberg, John, John Trutko, Burt Barnow, Mary Farrell, and Asaph Glosser. 2005. Unit Costs
of Intensive and Training Services for WIA Adults and Dislocated Workers: An Exploratory
Study of Methodologies and Estimates in Selected States and Localities: Final Report. Report
Prepared for U.S. Department of Labor, Employment and Training Administration.

Heinrich, Carolyn. 2004. “Improving Public-Sector Performance Management: One Step
Forward, Two Steps Back?” Public Finance and Management 4(3): 317-351.



                                             134
Heinrich, Carolyn, Peter Mueser, and Kenneth Troske. 2008. Workforce Investment Act Non-
Experimental Net Impact Evaluation: Final Report. Washington, DC: IMPAQ International.

Heinrich, Carolyn, Peter Mueser, Kenneth Troske, Kyung-Seong Jeon, and Daver Kahvecioglu.
2013. “Do Public Employment and Training Programs Work?” IZA Journal of Labor Economics
2:6.

Heinrich, Carolyn and Peter Mueser. 2014. “Training Program Impacts and the Onset of the
Great Recession.” Unpublished manuscript, University of Missouri.

Hendra, Richard, James Riccio, Richard Dorsett, David Greenberg, Genevieve Knight, Joan
Phillips, Philip Robins, Sandra Vegeris, and Johana Walter. 2011. Breaking the Low-pay, No-pay
Cycle: Final Evidence from the UK Employment Retention and Advancement (ERA)
Demonstration. UK Department for Work and Pensions Research Report No. 765.

Herman, Alexis. 1998. Implementing the Workforce Investment Act of 1998. US Department of
Labor, Employment and Training Administration. Retrieved November 9, 2014, from
www.doleta.gov/usworkforce/documents/misc/wpaper3.cfm.

Hirano, Keisuke, Guido Imbens, and Geert Ridder. 2003. “Efficient Estimation of Average
Treatment Effects Using the Estimated Propensity Score.” Econometrica 71(4): 1161-1189.

Hollenbeck, Kevin. 2009. “Return on Investment Analysis of a Selected Set of Workforce
System Programs in Indiana.” Report submitted to the Indiana Chamber of Commerce
Foundation, Indianapolis, Indiana. http://research.upjohn.org/reports/15.

Hollenbeck, Kevin. 2011. “Short-Term Net Impact Estimates and Rates of Return.” In Douglas
Besharov and Phoebe Cottingham (eds.) The Workforce Investment Act: Implementation
Experiences and Evaluation Findings. Kalamazoo, MI: W.E. Upjohn Institute for Employment
Research. 347-370.

Hollenbeck, Kevin, and Wei-Jang Huang. 2014. “Net Impact and Benefit-Cost Estimates of the
Workforce Development System in Washington State.” Upjohn Institute Technical Report No.
13-029.

Hotz, V. Joseph. 1992. “Designing an Evaluation of the Job Training Partnership Act.” In
Charles Manski and Irwin Garfinkel (eds.) Evaluating Welfare and Training Programs.
Cambridge, MA: Harvard University Press. 76-114.

Hotz, V. Joseph, Guido Imbens, and Jacob Klerman. 2006. “Evaluating the Differential Effects
of Alternative Welfare-to-Work Training Components: A Reanalysis of the California GAIN
Program.” Journal of Labor Economics 24(3): 521-566.

Hotz, V. Joseph, Guido Imbens and Julie Mortimer. 2005. “Predicting the Efficacy of Future
Training Programs Using Past Experiences at Other Locations.” Journal of Econometrics 125:
241-270.

                                              135
Hotz, V. Joseph and Karl Scholz. 2002. “Measuring Employment and Income Outcomes
for Low-Income Populations with Administrative and Survey Data.” In Studies of
Welfare Populations: Data Collection and Research Issues. National Research Council:
National Academy Press. 275-315.

Huber, Erika, David Kassabian, and Elissa Cohen. 2014. Welfare Rules Databook: State TANF
Policies as of July 2013, OPRE Report 2014-52. Washington, DC: Office of Planning, Research
and Evaluation, Administration for Children and Families, US Department of Health and Human
Services.

Huber, Martin, Michael Lechner and Conny Wunsch. 2013. “The Performance of Estimators
Based on the Propensity Score.” Journal of Econometrics 175: 1-21.

Iacus, Stefano, Gary King, and Giuseppe Porro. 2012. “Causal Inference without Balance
Checking: Coarsened Exact Matching.” Political Analysis 20(1): 1-24.

Institute of Education Sciences (IES), U.S. Department of Education. 2008. Rigor and Relevance
Redux: Director’s Biennial Report to Congress. IES 2009-6010. Washington, DC: IES.

Jacobson, Louis, Robert LaLonde and Daniel Sullivan. 1993. “Earnings Losses of Displaced
Workers.” American Economic Review 83(4): 685-709.

Johnson, George. 1979. “The Labor Market Displacement Effect in the Analysis of the Net
Impact of Manpower Training Programs.” In F.E. Bloch (ed.) Evaluating Manpower Training
Programs: Research in Labor Economics (Supplement 1): 227-254.

Johnson, George and James Tomola. 1977. “The Fiscal Substitution Effect of Alternative
Approaches to Public Service Employment Policy.” Journal of Human Resources 12(1): 3-26.

Johnston, Janet. 1987. The Job Training Partnership Act: A Report by the National Commission
for Employment Policy. Washington, DC: Government Printing Office.

Kemple, James, Fred Doolittle, and John Wallace. 1993. The National JTPA Study: Site
Characteristics and Participation Patterns. New York, NY: MDRC.

Kesselman, Jonathon. 1978. “Work Relief Programs in the Great Depression.” In John Palmer
(ed.) Creating Jobs: Public Employment Programs and Wage Subsidies. Washington, DC: The
Brookings Institution.

King, Christopher. 1999. “Federalism and Workforce Policy Reform.” Publius: The Journal of
Federalism 29(2): 53-71.

King, Christopher T., and Burt S. Barnow. 2011. “The Use of Market Mechanisms.” In Douglas
Besharov and Phoebe Cottingham (eds.) The Workforce Investment Act: Implementation
Experiences and Evaluation Findings. Kalamazoo, MI: W.E. Upjohn Institute for Employment

                                             136
Research. 81-111.

Koenker, Roger and Gilbert Basset. 1978. “Regression Quantiles.” Econometrica 46: 33–50.

Kornfeld, Robert and Howard Bloom. 1999. “Measuring Program Impacts on Earnings and
Employment: Do Unemployment Insurance Wage Reports of Employers Agree with Surveys of
Individuals?” Journal of Labor Economics 17(1): 168-197.

Krolikowski, Pawel. 2014. “Reassessing the Experience of Displaced Workers.” Unpublished
manuscript, University of Michigan.

LaLonde, Robert. 1986. “Evaluating the Econometric Evaluations of Training Programs
with Experimental Data.” American Economic Review 76(4): 604-620.

LaLonde, Robert. 2003. “Employment and Training Programs.” In Robert Moffitt (ed.) Means-
Tested Transfer Programs in the United States. Chicago: University of Chicago Press. 517-585.

Lechner, Michael and Smith, Jeffrey. 2007. “What is the Value Added by Caseworkers?” Labour
Economics 14(2): 135–151.

Lechner, Michael and Stephan Wiehler. 2011. “Kids or Courses? Gender Differences in the
Effects of Active Labor Market Policies.” Journal of Population Economics 24(3): 783-812.

Lechner, Michael and Conny Wunsch. 2009. “Are Training Programs More Effective When
Unemployment is High?” Journal of Labor Economics 27(4): 653-692.

Lechner, Michael and Conny Wunsch. 2013. “Sensitivity of Matching-Based Program
Evaluations to the Availability of Control Variables.” Labour Economics 21: 111-121.

Levitan, Sar and Frank Gallo.1988. A Second Chance: Training for Jobs. Kalamazoo, MI: W.E.
Upjohn Institute for Employment Research.

Lise, Jeremy, Shannon Seitz, and Jeffrey Smith. 2004. “Equilibrium Policy Experiments and the
Evaluation of Social Programs.” NBER Working Paper No. 10283.

Long, David, Charles Maller and Craig Thornton. 1981. “Evaluating the Benefits and Costs of
the Job Corps.” Journal of Policy Analysis and Management 1(1): 55-76.

Lower-Basch, Elizabeth. 2014. SNAP E&T. Washington, DC: Center for Law and Social
Programs.

Maguire, Sheila, Joshua Freely, Carol Clymer, Maureen Conway and Deena Schwartz. 2010.
Tuning In to Local Labor Markets: Findings From the Sectoral Employment Impact Study.
Philadelphia, PA: P/PV.

Mallar, Charles, Stuart Kerachsky, Craig Thornton, and David Long. 1982. Evaluation of the

                                             137
Economic Impact of the Job Corps Program: Third Follow-up Report. Princeton, NJ:
Mathematica Policy Research.

Mangum, Garth. 1968. MDTA: The Foundation of Federal Manpower Policy. Baltimore, MD:
Johns Hopkins University Press.

McCall, Brian, Conny Wunsch, and Jeffrey Smith. 2015. “Government-Sponsored Vocational
Training.” Unpublished manuscript, University of Michigan.

McConnell, Sheena, Paul Decker, and Irma Perez-Johnson. 2006. “The Role of Counseling in
Voucher Programs: Findings from the Individual Training Account Experiment.” Unpublished
manuscript, Mathematica Policy Research.

McConnell, Sheena and Steven Glazerman. 2001. National Job Corps Study: The Benefits and
Costs of Job Corps. Princeton, NJ: Mathematica Policy Research.

McConnell, Sheena, Elizabeth Stuart, Kenneth Fortson, Paul Decker, Irma Perez-Johnson,
Barbara Harris, and Jeffrey Salzman. 2006. Managing Customers’ Training Choices: Findings
from the Individual Training Account Experiment: Final Report. Princeton, NJ: Mathematica
Policy Research.

Mikelson, Kelly and Demetra Nightingale. 2004. Estimating Public and Private Expenditures on
Occupational Training in the United States. Washington, DC: The Urban Institute.

Mirengoff, William and Lester Rindler. 1978. CETA: Manpower Programs under Local Control.
Washington, DC: National Academy of Sciences.

Moffitt, Robert. 1983. “An Economic Model of Welfare Stigma.” American Economic Review
73(5): 1023-1035.

Mueller-Smith, Michael. 2015. “The Criminal and Labor Market Impacts of Incarceration.”
Unpublished manuscript, University of Michigan.

Muller, Seán. 2015. “Interaction and External Validity: Obstacles to the Policy Relevance of
Randomized Evaluations.” World Bank Economic Review. 29 (Suppl 1): S226-S237.

Musgrave, Richard. 1959. The Theory of Public Finance: A Study in Public Economy. New
York, NY: McGraw-Hill.

Musgrave, Richard and Peggy Musgrave. 1976. Public Finance in Theory and Practice. New
York: McGraw-Hill.

Nathan, Richard, Robert Cook, Lane Rawlins, and Associates. 1981. Public Service
Employment: A Field Evaluation. Washington, DC: The Brookings Institution.

National Skills Coalition. 2014. Side-by-Side Comparison of Occupational Training and Adult

                                              138
Education & Family Literacy Provisions in the Workforce Investment Act and the Workforce
Innovation and Opportunity Act. Washington, DC: National Skills Coalition.

Neyman, Jerzy. 1923. “Statistical Problems in Agricultural Experiments.” Journal of the Royal
Statistical Society 2:107-180.

Oates, Jane. 2012. Training and Employment Notice 39-11 (TEIN 39-11): Release and
Availability of Career Pathways Technical Assistance Resources. Washington, DC: U.S.
Department of Labor Employment and Training Administration.

O’Leary, Christopher, Robert Straits, and Stephen Wandner. 2004. “U.S. Job Training: Types,
Participants, and History.” In Christopher O’Leary, Robert Straits, and Stephen Wandner (eds.)
Job Training Policy in the United States. Kalamazoo, MI: W.E. Upjohn Institute for
Employment Research. 1-20.

Orr, Larry. 1998. Social Experiments: Evaluating Public Programs with Experimental Methods.
New York: Sage Publications.

Park, Jooyoun. 2012. “Does Occupational Training by the Trade Adjustment Assistance Program
Really Help Reemployment? Success Measured as Matching.” Review of International
Economics 20(5): 999-1016.

Patel, Nisha and Steve Savner. 2001. Implementation of Individual Training Account Policies
under the Workforce Investment Act: Early Information from Local Areas. Washington, DC:
Center for Law and Social Policy.

Pederson, Jonas, Michael Rosholm and Michael Svarer. 2012. “Experimental Evidence on the
Effects of Early Meetings and Activation.” IZA Discussion Paper No. 6970.

Perez-Johnson, Irma, Quinn Moore, and Robert Santilano. 2011. Improving the Effectiveness of
Individual Training Accounts: Long-term Findings from an Experimental Evaluation of Three
Service Delivery Models: Final Report. Princeton, New Jersey: Mathematica Policy Research.

Perry, Charles, Bernard Anderson, Richard Rowan, and Herbert Northrup, 1975. The Impact of
Government Manpower Programs in General, and on Minorities and Women. Philadelphia, PA:
Industrial Research Unit, the Wharton School, University of Pennsylvania.

Plesca, Miana. 2010. “A General Equilibrium Analysis of the Employment Service.” Journal of
Human Capital 4(3): 274-329.

Plesca, Miana and Jeffrey Smith. 2005. “Rules Versus Discretion in Social Programs: Empirical
Evidence on Profiling in Employment and Training Programs.” Unpublished manuscript,
University of Maryland.

Plesca, Miana and Jeffrey Smith. 2007. “Evaluating Multi-treatment Programs: Theory and
Evidence from the U.S. Job Training Partnership Act Experiment.” Empirical Economics 32(2-3):
491-528.

                                              139
Puhani, Patrick. 2000. “The Heckman Correction for Sample Selection and its Critique.” Journal
of Economic Surveys 14(1): 53-68.

Puma, Michael and Nancy Burstein. 1994. “The National Evaluation of the Food Stamp
Employment and Training Program.” Journal of Policy Analysis and Management. 13(2): 311-
330.

Quandt, Richard. 1972. “Methods of Estimating Switching Regressions.” Journal of the
American Statistical Association 67: 306-310.

Radin, Beryl. 2006. Challenging the Performance Movement: Accountability, Complexity and
Democratic Values. Washington, DC: Georgetown University Press.

Rosholm, Michael. 2014. “Do Caseworkers Help the Unemployed? Evidence for Making a
Cheap and Effective Twist to Labor Market Policies for Unemployed Workers.” IZA World of
Labor 72.

Roy, A.D. 1951. “Some Thoughts on the Distribution of Earnings.” Oxford Economic
Papers 3:135-146.

Rubin, Donald. 1974. “Estimating Causal Effects of Treatments in Randomized and Non-
Randomized Studies.” Journal of Educational Psychology 66:688-701.

Schochet, Peter, Ronald D’Amico, Jillian Berk, Sarah Dolfin and Nathan Wozny. 2012.
Estimated Impacts for Participants in the Trade Adjustment Assistance (TAA) Program Under
the 2002 Amendments. Princeton, NJ: Mathematica Policy Research.

Schochet, Peter and John Burghardt. 2008. “Do Job Corps Performance Measures Track
Program Impacts?” Journal of Policy Analysis and Management 27(3): 556-576.

Schochet, Peter, John Burghardt, and Steven Glazerman. 2001. National Job Corps Study: The
Impacts of Job Corps on Participants’ Employment and Related Outcomes. Princeton, NJ:
Mathematica Policy Research.

Schochet, Peter, John Burghardt, and Sheena McConnell. 2008. “Does Job Corps Work?” Impact
Findings from the National Job Corps Study.” American Economic Review 98(5): 1864-1886.

Sianesi, Barbara, 2014. “Dealing with Randomisation Bias in a Social Experiment: the Case of
ERA.” IFS Working Papers W14/10, Institute for Fiscal Studies.

Smith, Jeffrey. 1992. The JTPA Selection Process: A Descriptive Analysis. Report Submitted to
the U.S. Department of Labor as part of the National JTPA Study.

Smith, Jeffrey. 1997. “Measuring Earnings Levels Among the Poor: Evidence from Two
Samples of JTPA Eligibles.” Unpublished manuscript, University of Western Ontario.

                                             140
Smith, Jeffrey. 2011. “Improving Impact Evaluation in Europe.” In Douglas Besharov and
Phoebe Cottingham, eds., The Workforce Investment Act: Implementation Experiences and
Evaluation Findings. Kalamazoo, MI: W.E. Upjohn Institute for Employment Research. 473-
494.

Smith, Jeffrey and Petra Todd. 2005a. “Does Matching Overcome LaLonde’s Critique of
Nonexperimental Methods?” Journal of Econometrics 125(1-2): 305-353.

Smith, Jeffrey and Petra Todd. 2005b. “Rejoinder.” Journal of Econometrics 125(1-2): 365-375.

Smith, Jeffrey and Alexander Whalley. 2015. “How Well Do We Measure Public Job Training?”
Unpublished manuscript, University of Michigan.

Social Policy Research Associates. 2013. PY 2012 WIASRD Data Book. Oakland, CA: Social
Policy Research Associates.

Spaulding, Shayne. 2001. Performance-Based Contracting under the Job Training Partnership
Act. Baltimore, MD: Johns Hopkins University Master’s Thesis.

Stapleton, David, Gina Livermore, Craig Thornton, Bonnie O’Day, Robert Weathers, Krista
Harrison, So O’Neil, Emily Sama Martin, David Wittenburg, and Debra Wrig. 2008. Ticket to
Work at the Crossroads: A Solid Foundation with an Uncertain Future. Princeton, NJ:
Mathematica Policy Research.

Taggart, Robert. 1981. A Fisherman's Guide: An Assessment of Training and Remediation
Strategies. Kalamazoo, MI: W.E. Upjohn Institute for Employment Research.

Todd, Petra and Kenneth Wolpin. 2006. “Assessing the Impact of a School Subsidy Program in
Mexico Using a Social Experiment to Validate a Dynamic Behavioral Model of Child Schooling
and Fertility.” American Economic Review 96(5): 1384-1417.

Trutko, John and Burt Barnow. 1997. Implementation of the 1992 Job Training Partnership Act
(JTPA) Amendments: Report to Congress. Washington, D.C.: U.S. Department of Labor,
Employment and Training Administration.

Trutko, John and Burt Barnow. 1999. Vouchers under JTPA: Lessons for Implementation of the
Workforce Investment Act. Arlington, VA: James Bell Associates.

Trutko, John and Burt Barnow. 2007. Variation in Training Rates across States and Local
Workforce Investment Boards: Final Report. Arlington, VA: Capital Research Corporation.

Trutko, John and Burt Barnow. 2010. Implementing Efficiency Measures for Employment and
Training Programs: Final Report. Arlington, VA: Capital Research Corporation.

U.S. Bureau of the Budget. 1966. Appendix of the Budget of the United States Government for

                                             141
Fiscal Year 1967. Washington, DC: U.S. Government Printing Office.

U.S. Bureau of the Budget. 1967. Appendix of the Budget of the United States Government for
Fiscal Year 1968. Washington, DC: U.S. Government Printing Office.

U.S. Bureau of the Budget. 1968. Appendix of the Budget of the United States Government for
Fiscal Year 1969. Washington, DC: U.S. Government Printing Office.

U.S. Bureau of the Budget. 1969. Appendix of the Budget of the United States Government for
Fiscal Year 1970. Washington, DC: U.S. Government Printing Office.

U.S. Bureau of the Budget. 1970. Appendix of the Budget of the United States Government for
Fiscal Year 1971. Washington, DC: U.S. Government Printing Office.

U.S. Department of Agriculture. 2014. Food and Nutrition Service: 2015 Explanatory Notes.
Retrieved March 29, 2015 from http://www.obpa.usda.gov/32fns2015notes.pdf.

U.S. Department of Education. 2015. Federal Student Aid: Federal Pell Grants. Retrieved
February 1, 2015 from https://studentaid.ed.gov/types/grants-scholarships/pell.

U.S. Department of Education. 2014. Programs: Federal Pell Grant Program. Retrieved
February 1, 2015 from http://www2.ed.gov/programs/fpg/index.html.

U.S. Department of Education. 2014. Programs: Adult Education: Basic Grants to States.
Retrieved January 28, 2015 from http://www2.ed.gov/programs/adultedbasic/funding.html.

U.S. Department of Health and Human Services. 2014. TANF Financial Data - FY 2013.
Retrieved January 28, 2015 from http://www.acf.hhs.gov/programs/ofa/resource/tanf-financial-
data-fy-2013.

U.S. Department of Labor. 1973. Manpower Report of the President: A Report on Manpower
Requirements, Resources, Utilization, and Training. Washington, DC: U.S. Government Printing
Office.

U.S. Department of Labor. 2013. About Job Corps. Retrieved November 23, 2014, from
http://www.jobcorps.gov/AboutJobCorps.aspx.

U.S. Department of Labor. 2014. FY 2015 Department of Labor Budget in Brief. Washington,
DC: U.S. Department of Labor.

U.S. Department of Labor. 2000. One-Stop Partners. Retrieved November 15, 2014, from
http://www.doleta.gov/programs/factsht/pdf/onestoppartners.pdf.

U.S. Department of Labor. 2014. Workforce Investment Act: Adults and Dislocated Workers
Program. Retrieved November 15, 2014, from
http://www.doleta.gov/programs/general_info.cfm.

                                             142
U.S. Department of Labor. 2010. Wagner-Peyser/Labor Exchange. Retrieved November 1, 2014,
from http://www.doleta.gov/programs/wagner_peyser.cfm.

U.S. Department of Labor. 2010. Workforce Investment Act One-Stop Partners. Retrieved
November 15, 2014, from http://www.doleta.gov/usworkforce/onestop/partners.cfm.

U.S. Department of Labor. 2014. Workforce Innovation and Opportunity Act (WIOA) Factsheet.
Retrieved November 16, 2014, from http://www.doleta.gov/wioa/pdf/WIOA-Factsheet.pdf.

U.S. Department of Labor. 2014. Budget Authority Tables: Training and Employment Programs.
Retrieved February 15, 2015, from http://www.doleta.gov/budget/bahist.cfm.

U.S. Department of Labor. 2015. Budget Authority Tables: Training and Employment Programs.
Retrieved May 15, 2015, from http://www.doleta.gov/budget/bahist.cfm.

U.S. Department of Labor. 2014. Training and Employment Guidance Letter No. 36-10 (TEGL
36-10). Retrieved November 22, 2014, from
http://wdr.doleta.gov/directives/corr_doc.cfm?DOCN=3052.

U.S. Department of Labor. 2014. WIA Youth Formula Funded Program. Retrieved November
23, 2014 from http://www.doleta.gov/youth_services/wiaformula.cfm.

U.S. Department of Labor. 2014. Senior Community Service Employment Program. Retrieved
November 24, 2014 from http://www.doleta.gov/seniors/.

U.S. Department of Labor. 2014. The Trade Adjustment Assistance Program Brochure.
Retrieved November 24, 2014 from
http://www.doleta.gov/tradeact/docs/program_brochure2014.pdf.

U.S. Department of Labor. 2014. ETA Programs for Migrant and Seasonal Farmworkers.
Retrieved November 23, 2014 from http://www.doleta.gov/Farmworker/.

U.S. Department of Labor. 2014. Reintegration of Ex-Offenders (RExO). Retrieved November
23, 2014 from http://www.doleta.gov/RExO/.

U.S. Department of Labor. 2014. YouthBuild. Retrieved November 24, 2014 from
http://www.doleta.gov/Youth_services/Youth_Build.cfm.

U.S. Department of Labor. No date. Budget Authority from 1948-1989. Unpublished internal
document.

U.S. Department of Labor. 2014. Quarterly Workforce System Results. Retrieved November 22,
2014 from http://www.doleta.gov/performance/results/eta_default.cfm#wiastann.

U.S. Department of Labor. 2014. VETS Employment Services Fact Sheet 1. Retrieved November

                                            143
23, 2014 from http://www.dol.gov/vets/programs/empserv/employment_services_fs.htm.

U.S. Department of Labor. 2014. VETS HVRP Fact Sheet. Retrieved November 23, 2014 from
http://www.dol.gov/vets/programs/fact/Homeless_veterans_fs04.htm.

U.S. Department of Labor. 2015. Summary of Appropriation Budget Authority, Fiscal Year 2014.
Retrieved April 18, 2015, from
http://www.doleta.gov/budget/docs/14_final_appropriation_action.pdf

U.S. General Accounting Office. 1996. Job Training Partnership Act: Long Term Earnings and
Employment Outcomes. Report HEHS-96-40. Washington, DC: US General Accounting Office.

U.S. General Accounting Office. 2002. Workforce Investment Act: Youth Provisions Promote
New Service Strategies, But Additional Guidance Would Enhance Program Development. Report
GAO-02-413. Washington, DC: US General Accounting Office

U.S. Office of Management and Budget. 1992. Guidelines and Discount Rates for Benefit-Cost
Analysis of Federal Programs. Circular No. A-94 Revised.
https://www.whitehouse.gov/omb/circulars_a094/

Van Horn, Carl and Aaron Fichter. 2011. “Eligible Training Provider Lists and Consumer Report
Cards.” In Douglas Besharov and Phoebe Cottingham (eds.) The Workforce Investment Act:
Implementation Experiences and Evaluation Findings. Kalamazoo, MI: W.E. Upjohn Institute
for Employment Research. 153-172.

Van Horn, Carl, Kathy Krepcio, and Stephen Wandner. 2015. Identifying Gaps and Setting
Strategic Priorities for Employment and Training Research (2014-2019). Report prepared for
U.S. Department of Labor, Employment and Training Administration.

Wallace, Geoffrey and Robert Haveman. 2007. “The Implications of Differences between
Employer and Worker Employment/earnings Reports for Policy Evaluation.” Journal of Policy
Analysis and Management 26(4): 737–754.

White, Michael, and Jane Lakey. 1992. The Restart Effect: Does Active Labour Market Policy
Reduce Unemployment? London, UK: Policy Studies Institute.

Wilson, James. 1989. Bureaucracy: What Government Agencies Do and Why They Do It. New
York: Basic Books.

Wunsch, Conny. 2013. “Optimal Use of Labor Market Policies: The Role of Job Search
Assistance.” Review of Economics and Statistics 95(3): 1030-1045.

YouthBuild. https://youthbuild.org/. Accessed November 23, 2014.

Ziliak, James. 2015. “Temporary Assistance for Needy Families.” NBER Working Paper No.
21038.

                                             144
                                                                                Table 8.1
                                                             Major Employment and Training Programs 1963-1973


Name and Authorizing             Dates of Operation                General Purpose              Administrative Agencies           Target Groups             Average Annual Enrollment
Legislation                      (FY 1963-1974)
Manpower Development and                   1963-1974   Vocational training in a classroom       National: Department of Labor     Economically              Economically disadvantaged
Training Act                                           setting and subsidized training by       (DOL) and Department of           disadvantaged and         126,200
                                                       employers on the job                     Health, Education, and Welfare    dislocated workers        Dislocated workers 83,700
                                                                                                (HEW)
                                                                                                Local: Employment Service
                                                                                                (ES), school districts, skills
                                                                                                centers
Vocational education (Smith               1964-1974    Occupational training in public          National: HEW                     General not seeking       6,674,0000
Hughes Act of 1917)                                    schools                                  Local: School districts           academic degree
                                                                                                                                  population
Neighborhood Youth Corps in-              1965-1974    Subsidized work experience in            National: DOL                     Disadvantaged youth       In school 129,400
school, summer, out-of-school                          public and nonprofit agencies            Local: Community action                                     Out of school 84,300
(Economic Opportunity Act of                                                                    agencies (CAAs), local                                      Summer 362,500
1964)                                                                                           governments, schools, ES
Job Corps (EOA)                           1965-1974    Vocational skills training and           National: Office of Economic      Disadvantaged youth       42,100
                                                       basic skills in a residential setting    Opportunity (OEO); DOL after
                                                                                                1969
Operation Mainstream (EOA)                1967-1974    Subsidized employment in                 National: OEO, DOL after 1967     Disadvantaged adults      20,000
                                                       paraprofessional positions in            Local: CAAs and public
                                                       public and nonprofit agencies            agencies
Job Opportunities in the                  1968-1974    Subsidized on-the-job training in        National: DOL and National        Disadvantaged adults      49,300
Business Sector (Presidential                          the private sector                       Alliance of Business (NAB)
initiative)                                                                                     Local: NAB offices

Work Incentive Program (Social            1967-1974    Vocational training, work                National: DOL                     Aid to Families with      124,700
Security Act)                                          experience, support services,            Local: welfare offices, ES, and   Dependent Children
                                                       placement                                WIN offices                       (AFDC) recipients
Concentrated Employment                   1968-1974    Coordinates employment and               National: DOL                     Disadvantaged youth and   92,900
Program (EOA)                                          training services of other               Local: CAAs, local government     adults
                                                       programs
Public Employment Program                 1972-1974    Subsidized public employment             National: DOL                     Unemployed adults         234,300
(Emergency Employment Act)                                                                      Local: Chief elected officials
Source: Barnow (1993)




                                                                                               145
                                             Table 8.2
                                 Historical Budget Authority
             U.S. Department of Labor Employment and Training Programs
                              (in thousands of nominal dollars)

                  Total                                          Youth
               Employment     Dislocated                       Except Job               E&T Programs
               and Training    Workers         Adults            Corps      Job Corps   as pct of GDP
                                        MDTA era (1962-1972)a
  1965           529,406          x           266,505           127,742      52,523        0.07%
  1966           671,095          x           339,649           263,337     303,527        0.09%
  1967           861,044          x           296,247           348,833     209,000        0.10%
  1968           398,497          x           296,418           281,864     282,300        0.04%
  1969           409,992          x           272,616           320,696     278,400        0.04%
  1970          1,451,215         x           336,380           356,589     169,782        0.14%
  1971          1,622,997         x           335,752           426,458     160,187        0.15%
  1972          2,682,066         x           424,368           517,244     202,185        0.22%
                                         CETA era (1973-1982)b
  1973          1,549,416        n/a            n/a               n/a          n/a         0.11%
  1974          2,275,584        n/a            n/a               n/a          n/a         0.15%
  1975          3,739,450        n/a            n/a               n/a          n/a         0.23%
  1976          5,827,720        n/a            n/a               n/a          n/a         0.33%
Transition
 Quarter         597,500         n/a            n/a               n/a          n/a         0.13%
  1977          17,200,830       n/a            n/a               n/a          n/a         0.85%
  1978          3,652,630        n/a            n/a               n/a       280,000        0.16%
  1979          10,510,312       n/a            n/a               n/a       380,000        0.41%
  1980          8,387,193        n/a            n/a               n/a       470,000        0.30%
  1981          8,100,887        n/a            n/a               n/a       465,000        0.26%
  1982          3,300,301        n/a            n/a               n/a          n/a         0.10%
                                                                  c
                                         JTPA era (1983-2000)
  1983          4,329,876        n/a                     n/a                   n/a         0.12%
  1984          6,863,525      317,250                4,849,862             1,014,100      0.17%
  1985          4,100,662      222,500                2,710,700             617,000        0.09%
  1986          3,649,194      95,702                 2,419,061             612,480        0.08%
  1987          4,041,913      200,000                2,590,000             656,350        0.08%
  1988          4,138,911      287,220                2,527,536             716,135        0.08%
  1989          4,140,485      283,773                2,497,205             755,317        0.07%
  1990          4,283,975      463,603                2,444,585             789,122        0.07%
  1991          4,968,253      526,979                2,961,364             867,486        0.08%
  1992          4,555,331      576,986                2,435,196             919,533        0.07%
  1993          4,843,266      651,246       1,015,021         1,535,056    966,075        0.07%
  1994          5,410,010     1,151,000       988,021          1,496,964    1,040,469      0.07%
  1995          4,352,602     1,228,550       996,813           311,460     1,089,222      0.06%
  1996          4,513,678     1,091,900       850,000           751,672     1,093,942      0.06%
  1997          5,178,903     1,286,200       895,000           997,672     1,153,509      0.06%
  1998          6,837,464     1,345,510       955,000          1,000,965    1,246,217      0.08%


                                                 146
   1999         7,018,662       1,403,510       954,000       1,250,965      1,307,947           0.07%
                                                                 d
                                            WIA era (2000-2014)
   2000         5,969,155       1,589,025       950,000       1,250,965      1,357,776           0.06%
   2001         6,041,678       1,433,951       950,000       1,377,965      1,399,148           0.06%
   2002         6,417,023       1,602,110       945,372       1,353,065      1,454,241           0.06%
   2003         5,713,068       1,454,891       894,577       1,038,669      1,509,094           0.05%
   2004         5,566,051       1,445,939       893,195        995,059       1,535,623           0.05%
   2005         5,680,372       1,303,918       882,486        980,801       1,544,951           0.04%
   2006         5,736,193       1,528,549       840,588        928,716       1,573,270           0.04%
   2007         5,595,655       1,390,434       826,105        964,930       1,566,205           0.04%
   2008         5,147,987       1,464,707       861,540        983,021        919,506            0.04%
   2009         9,581,432       2,902,391      1,356,540      2,231,569      1,242,938           0.04%
   2010         7,337,268       1,410,880       860,116       1,026,569      1,680,626           0.05%
   2011         7,170,341       1,283,303       769,576        905,754       1,734,150           0.05%
   2012         7,699,612       1,210,536       770,811        904,042       1,702,946           0.05%
Notes
x indicates not applicable; n/a indicates not available. Budget Authority figures unless otherwise noted.
Table excludes funding for Wagner-Peyser Act.
Footnotes
a. MDTA era:
   The "Total Employment and Training" budget may seem unusually large in 1972 due to Emergency
Employment Assistance, a temporary program.
     During the MDTA era, "Youth Except Job Corps" is Neighborhood Youth Corps.

    Budget data in the following categories for the following years are obligations: "Adults" (1965-1972);
"Youth except Job Corps" (1965-1972); "Job Corps" (1970-1972).
     Budget data for "Job Corps" for 1965-1969 are appropriations.
b. CETA era:
     The "Total Employment and Training" budget is large in some years due to the following programs:
Community Service Employment for Older Americans (1974-1981); Temporary Employment Assistance
(1975-1981); YEDPA (1977 total employment and training budget includes appropriations for YEDPA,
which were disbursed over four years, 1978-1981).
     "Total Employment and Training" budget data for 1977 is a combination of Budget Authority and
outlays.
     Budget data for "Job Corps" for years 1978-1981 are outlays.
c. JTPA era:
     From 1983-1992: JTPA IIA included both Adult and Youth activities, so the funds cannot be divided
into separate categories; combined Adults and Youth budget includes JTPA Summer Youth Employment
and Training.
d. WIA era:
    Budget figures for 2009 may seem unusually large due to the following: all categories in 2009 include
appropriations for ARRA, which were disbursed over several years.
Sources
DOL Budget Authority from 1948-1989; DOL (2014) Budget Authority Tables; DOL (2015) Budget
Authority Tables; 1967, 1968, 1969, 1970, and 1971 Appendices of US Government Budgets; 1973
Manpower Report of the President; Betsey et al. (1985).




                                                   147
                                       Table 8.3
               Characteristics of WIA Adult and Dislocated Worker Exiters
                       by Training Status April 2012-March 2013


                                         Adults                 Dislocated Workers
                                  All             Training      All         Training
  Age
    18-21                                9.3            11.0        3.2            2.2
    22-54                               76.9            81.7       77.2           83.6
    55 and over                         13.8             7.3       19.6           14.2
  Gender
    Female                              47.6            54.5       48.5           47.6
    Male                                52.4            45.5       51.5           52.4
  Individual with                        3.9             3.3        3.1            2.4
  Disability
  Race/Ethnicity
    Hispanic                          10.5              15.2       12.8           12.8
    Black, not Hispanic               23.6              23.5       18.0           18.8
    White not Hispanic                59.2              54.0       62.8           63.1
    Other                              6.7                7.3        6.4            5.3
  Veteran                              7.8                7.3        7.6            8.8
  Average Pre-program               $6,006            $5,432     $8,566         $8,295
  Quarterly Earnings
  Low Income                            50.2            60.9        NA             NA
  Limited English                        3.0             3.3        NA             NA
  Single Parent                         15.1            20.3        NA             NA
  Public Assistance                     27.4            32.0        NA             NA
  Highest Grade /
  Education
    Less than 12                        10.8             8.1        NA             NA
    High School Grad                    37.8            40.7        NA             NA
    High School Equiv.                   8.0             9.3        NA             NA
    Some Postsecondry                   30.2            31.3        NA             NA
    College Graduate BA                 13.2            10.5        NA             NA


Source: Social Policy Research Associates (2013)




                                               148
                                             Table 8.4
                           Characteristics of WIA Youth Exiters
                        by Education Status April 2012-March 2013


                           All              Attending School         Not Attending School
                                          High      Postsecondary     High      High School
                                        School or                    School       Graduate
                                         Below                       Dropout
Number of Exiters          112,386         52,954            4,630      28,087       26,706
Age
  14-15                           6.6         13.7            0.1          0.4          0.0
  16-17                          36.6         60.5            4.4         26.4          5.8
  18                             21.9         19.9           19.7         24.8         23.4
  19-21                          34.8          5.9           75.8         48.4         70.8
Gender
  Female                         54.6         54.1           63.6         51.2         57.4
  Male                           45.4         45.9           36.4         48.8         42.6
Individual with                  13.2         19.0            5.9          7.7          8.8
Disability
Race/Ethnicity
  Hispanic                       32.5         35.1           43.5         27.2         31.1
  Black, not                     32.5         32.3           23.4         33.1         33.8
Hispanic
  White not Hispanic             29.8         27.1           29.2         34.9         30.1
  Other                           5.2          5.5            3.9          4.8          5.0
Veteran (among 19-                0.3          0.1            0.3          0.1          0.5
21)
Homeless or                       4.5          2.6             3.4         6.7          5.6
Runaway Youth
Offender                          9.5          6.2            5.2         15.3          9.5
Pregnant or Parenting            24.0         19.5           40.5         26.0         28.2
Youth
Basic Literacy Skills            64.3         61.0           56.4         74.5         61.3
Deficient
Ever in Foster Care               3.7          4.7             3.1         3.0          2.6


Source: Social Policy Research Associates (2013)




                                              149
                                      Table 8.5
    Services Received by WIA Adult and Dislocated Worker Exiters, PY 2008-PY 2012

                   WIA - SERVICES RECEIVED BY ADULT EXITERS
              Year               2008       2009     2010      2011      2012
                                 General Information
   Total Number of Exiters    1,040,676 1,187,450 1,252,411 1,144,947 1,111,555
   Did Not Receive Training       89.1%      86.8%    86.7%     89.3%     89.6%
   Received Training              10.9%      13.2%    13.3%     10.7%     10.4%
                                   Types of Training
   On-the-job training             9.0%       7.4%     8.9%     10.8%     12.6%

   Skill upgrading & retraining     12.4%       14.5%       13.1%       13.1%      13.0%
   Entrepreneurial training          0.4%        0.1%        0.3%        0.3%       0.2%
   ABE or ESL in combination
   with training                     2.5%           2.9%     4.3%        3.4%        3.1%
   Customized training               6.5%           7.5%     6.8%        5.7%        5.7%
   Other occupational skills
   training                         72.5%       70.7%       71.0%       70.4%      69.2%


         WIA - SERVICES RECEIVED BY DISLOCATED WORKER EXITERS
                 Year              2008       2009     2010    2011    2012
                                  General Information
     Total Number of Exiters      364,044 581,985     760,853 750,409 705,706
     Did Not Receive Training       83.8%      80.8%    81.8%  84.5%   86.0%
     Received Training              16.2%      19.2%    18.2%  15.5%   14.0%
                                   Types of Training
     On-the-job training             7.5%       5.9%     6.8%  10.1%   11.8%
     Skill upgrading & retraining   13.6%      16.3%    14.6%  15.2%   14.7%
     Entrepreneurial training        1.5%       0.3%     0.3%   0.4%    0.3%
     ABE or ESL in combination       2.1%       2.3%     1.8%   1.7%    1.4%
     with training
     Customized training               1.5%          1.4%    1.3%       1.0%       0.7%
     Other occupational skills        77.2%         76.4%   78.2%      74.7%      74.4%
     training


Notes: Years 2008 through 2011 are program years, e.g., PY 2008 is July 1, 2008 through June
30, 2009; 2012 is April 1, 2012 through March 31, 2013. Types of training received may not
sum to 100% due to enrollment in more than one type of training and rounding.
Source: Social Policy Research Associates (2013).
                                              150
                                              Table 8.6
                       Outcomes for PY 2012 WIA Exiters for Selected
                     Subgroups of Adults, Dislocated Workers, and Youth


                                      Entered          Employment           Six-Month
                                    Employment        Retention Rate         Average
                                       Rate                                  Earnings
           Adults                          59.9%                81.9%            $13,335
            Veterans                       56.5%                81.0%            $15,726
            Public assistance              62.7%                80.4%            $10,447
            Individuals with               41.2%                75.4%            $11,086
            Disabilities
            Older individuals               47.9%               81.4%            $14,437
            With Training                   74.5%               87.3%            $15,986
            With only core                  58.6%               81.1%            $12,935
            and intensive
           Dislocated Workers               60.0%               84.3%            $15,930
            Veterans                        56.6%               82.4%            $17,073
            Displaced                       54.8%               80.0%            $11,049
            Homemakers
            Individuals with                45.5%               78.7%            $13,152
            Disabilities
            Older individuals               48.1%               81.4%            $16,221
            With Training                   81.2%               90.0%            $16,965
            With only core                  56.4%               82.8%            $15,653
            and intensive
           Older Youth                      69.7%               87.7%                 NA
            Veterans                        54.8%               60.0%                 NA
            Public Assistance               64.8%               85.0%                 NA
            Individuals with                70.0%               87.9%                 NA
            Disabilities
            Out-of-school                 69.9%                 87.5%                 NA
           All Youth               Placement in       Attainment of a      Literacy and
                                  Employment or          Degree or          Numeracy
                                    Education           Certificate           Gains
                                          66.0%                 62.3%              47.5%


Source: Data at http://www.doleta.gov/performance/results/eta_default.cfm#wiastann retrieved
November 22, 2014. Note that the outcome data is for Program Year 2012, July 1, 2012 through
June 30, 2013, while the participant data in the two prior tables covers a slightly different period.
The data for older youth is based on seven jurisdictions: Arizona, Hawaii, Michigan, Minnesota,
Puerto Rico, Rhode Island, and Vermont.



                                                151
                                      Table 8.7
    List of Selected Means Tested Employment and Training Programs and Budgets

                                                                                                         Funding - FY
                                                                                                        2014 in Millions
Program Title                                          Agency / Office                                    of Dollarsa
                                             Department of Labor Programs

Job Corps                                              DOL / Employment Training Administration                    $1,684

WIA Dislocated Workers                                 DOL / Employment Training Administration                   $1,219b

WIA Youth                                              DOL / Employment Training Administration                      $818

WIA Adults                                             DOL / Employment Training Administration                      $764

Wagner-Peyser Funded Employment Service                DOL / Employment Training Administration                      $664

Senior Community Service Employment Program            DOL / Employment Training Administration                      $433
Trade Adjustment Assistance (TAA)                      DOL / Employment Training Administration                     $306c
Disabled Veterans Outreach Program (DVOP) and
Local Veterans’ Employment Representative              DOL / Veterans' Employment and Training
Program (LVER)                                         Service                                                       $175
H-1B Job Training Grants                               DOL / Employment Training Administration                     $166d
National Farmworker Jobs Program (NFJP)                DOL / Employment Training Administration                       $82
Reintegration of Ex-Offenders (RExO)                   DOL / Employment Training Administration                       $80
YouthBuild                                             DOL / Employment Training Administration                       $78
Indian and Native American Employment and
Training                                               DOL / Employment Training Administration                       $46
Homeless Veterans Reintegration Program                DOL / Veterans' Employment and Training
(HVRP)                                                 Service                                                        $38
                                          Programs of Other Federal Agencies
                                                       Ed / Office of Vocational and Adult
Pell Grants                                            Education                                                  $8,181e
Temporary Assistance for Needy Families (TANF)         HHS / Administration for Children &
Grants                                                 Families                                                   $1,517f
                                                       Ed / Office of Vocational and Adult
Adult Education - Grants to States                     Education                                                     $564
SNAP Employment & Training                             USDA / Food and Nutrition Service                            $416g
Footnotes
a. Unless otherwise noted. Figures rounded to nearest million. Appropriations unless otherwise noted. All figures are in
nominal dollars. DOL ETA figures reflect budgets after the evaluations set aside.
b. Includes National Emergency Grants
c. Budget for training only (does not include cash payments)
d. Actual collected through fees
e. Estimated expenditures for 2011-2012 school year for postsecondary vocational training
f. Expenditures, FY 2013
g. Appropriations, FY 2013

Sources
Food and Nutrition Service - 2015 Explanatory Notes; DOL Budget in Brief FY2015;
http://www2.ed.gov/programs/adultedbasic/funding.html; http://www.acf.hhs.gov/programs/ofa/resource/tanf-financial-
data-fy-2013; http://www.doleta.gov/budget/docs/14_final_appropriation_action.pdf




                                                           152
Figure 8.1




   153
Figure 8.2




   154
