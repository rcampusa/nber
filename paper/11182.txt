                                 NBER WORKING PAPER SERIES




                       COHORT TURNOVER AND PRODUCTIVITY:
                    THE JULY PHENOMENON IN TEACHING HOSPITALS

                                          Robert S. Huckman
                                            Jason R. Barro

                                         Working Paper 11182
                                 http://www.nber.org/papers/w11182


                       NATIONAL BUREAU OF ECONOMIC RESEARCH
                                1050 Massachusetts Avenue
                                  Cambridge, MA 02138
                                      March 2005




We thank David Cutler, Arnold Epstein, LeRoi Hicks, Barbara McNeil, Paul Oyer, Douglas Staiger, and
seminar participants at Harvard Business School, the Stanford Strategy Conference, the 14th Annual Health
Economics Conference, and the National Bureau of Economic Research’s Summer Insitute for helpful
comments. We also thank Juliana Pakes for information regarding the Charlson Index. We acknowledge
financial support from the Division of Research at the Harvard Business School. The views expressed herein
are those of the author(s) and do not necessarily reflect the views of the National Bureau of Economic
Research.

 © 2005 by Robert S. Huckman and Jason R. Barro. All rights reserved. Short sections of text, not to exceed
two paragraphs, may be quoted without explicit permission provided that full credit, including © notice, is
given to the source.
Cohort Turnover and Productivity: The July Phenomenon in Teaching Hospitals
Robert S. Huckman and Jason R. Barro
NBER Working Paper No. 11182
March 2005
JEL No. I1, J0

                                             ABSTRACT

We consider the impact of cohort turnover—the simultaneous exit of a large number of experienced

employees and a similarly sized entry of new workers—on productivity in the context of teaching

hospitals. In particular, we examine the impact of the annual July turnover of house staff (i.e.,

residents and fellows) in American teaching hospitals on levels of resource utilization (measured by

risk-adjusted length of hospital stay) and quality (measured by risk-adjusted mortality rates). Using

patient-level data from roughly 700 hospitals per year over the period from 1993 to 2001, we

compare monthly trends in length of stay and mortality for teaching hospitals to those for

non-teaching hospitals, which, by definition, do not experience systematic turnover in July. We find

that the annual house-staff turnover results in increased resource utilization (i.e., higher risk-adjusted

length of hospital stay) for both minor and major teaching hospitals and decreased quality (i.e.,

higher risk-adjusted mortality rates) for major teaching hospitals. Further, these effects with respect

to mortality are not monotonically increasing in a hospital’s reliance on residents for the provision

of care. In fact, the most-intensive teaching hospitals manage to avoid significant effects on

mortality following this turnover. We provide a preliminary examination of the roles of supervision

and worker ability in explaining why the most-intensive teaching hospitals appear able to reduce

turnover’s negative effect on performance.

Robert S. Huckman                                        Jason Barro
T17 Morgan Hall                                          280 Beacon St . #5
Harvard Business School                                  Boston, MA 02116
Boston, MA 02163                                         and NBER
and NBER                                                 jason.barro@post.harvard.edu
rhuckman@hbs.edu
       Nearly all managers must deal with the consequences of employee turnover

within their organizations. Despite the importance of this issue, several authors have

observed that academic attention has been disproportionately focused on the causes

rather than consequences of turnover (Staw, 1980; Mobley, 1982; Glebbeek and Bax,

2004). One possible explanation for the dearth of studies on the effects of turnover is the

difficulty in answering this question empirically. Turnover is an endogenous

phenomenon that may occur for a wide variety of reasons that are not observed by the

researcher. For example, more productive workers may be more likely to remain with a

given company longer than less productive ones (Jovanovic, 1979). Under such

circumstances, it is difficult to make causal inferences concerning turnover'
                                                                            s effect on

productivity and performance using firm-level data.

       A second issue concerning the effect of turnover on firm performance is that

turnover, itself, appears in multiple forms. Many firms face a continuous stream of

individual turnover in which employees leave and are replaced by new workers at various

points throughout the year. In such settings, there is no one particular time during the

year when managers are required to train and orient a large portion of their workforces.

       In contrast, other firms bring on new employees in large numbers at discrete

points in the year. For example, law and consulting firms tend to start most of their new

employees in late summer or early fall. These new employees must all be trained and

integrated into the firm at one time. In the law and consulting examples, the potential

negative effects of the large inflow of new workers may be buffered by the fact that firms

do not face the simultaneous exist of large portions of their experience workers. Rather,

these departures occur in a roughly continuous manner throughout the year.




                                             1
        An extreme, though not uncommon, form of this discrete scenario is what we

term cohort turnover—the simultaneous exit of a large number of experienced employees

and a similarly sized entry of new workers—and serves as the focus of this study.

Examples of cohort turnover can be found in changeovers that occur between military

units in combat, political administrations,1 and residents and fellows in teaching

hospitals. The use of replacement workers during large-scale labor strikes represents yet

another case of cohort turnover. Given the number of individuals transitioning either into

or out of employment at a specific point in time, cohort turnover raises concerns about

adverse effects on productivity due to factors such as operational disruption (Krueger and

Mas, 2004) or the loss of the tacit knowledge (Polanyi, 1966) held by departing workers.

        In this paper, we consider cohort turnover among house staffs (i.e., residents and

fellows) in teaching hospitals. Residency represents a new physician'
                                                                    s first assignment

following medical school and typically lasts from three-to-five years depending on the

doctor'
      s area of specialization. In certain specialties, residency will be followed by a one-

or two-year fellowship, during which the doctor receives further training in a sub-

specialty. At the beginning of every July, the most senior residents move on to

permanent medical positions or fellowships at other hospitals, and recent medical school

graduates arrive as first-year residents, also known as interns. This turnover leads to a

significant lack of continuity and a discrete reduction in the average experience of the

labor force at teaching hospitals every summer. In addition, this changeover may disrupt

established teams of doctors and other caregivers within hospitals. Either of these effects



        1
          For example, Boylan (2004) examines turnover in the United States Attorney’s Office and finds
that 40% of turnover between 1969 and 1999 occurred during a President’s first year in office.



                                                   2
may have potentially troubling consequences for the two determinants of hospital

productivity—resource utilization and clinical quality.

       This “July phenomenon” is often mentioned in the lore of medical professionals.

Many physicians have, perhaps jokingly, counseled patients not to get sick in July. As of

yet in the medical literature, however, any identified July phenomenon has been limited

to declines in hospital efficiency (i.e. higher costs or lengths of hospital stay) without any

significant impact on clinical outcomes, such as mortality. As we discuss below, several

of these studies are limited either by relatively small sample sizes or issues concerning

their empirical strategy for identifying effects.

       We examine the impact of the July turnover on hospital productivity using data on

all patient admissions from a large, multi-state sample of American hospitals over a five-

year period. By comparing trends in teaching hospitals to those for non-teaching (i.e.,

control) hospitals over the course of the year, we find significant negative effects of the

residency turnover on both hospital efficiency (as measured by average length of stay

(LOS)) and clinical quality (as measured by risk-adjusted mortality rates). Over some

range, these effects appear to be increasing in the degree to which a hospital relies on

residents (as measured by the number of residents per hospital bed). Nevertheless, those

hospitals with the highest levels of residents per bed (i.e., high teaching intensity) appear

to be less affected—in terms of mortality—by the July phenomenon than facilities with

medium teaching intensities. We find initial evidence suggesting that this non-linearity

in the July phenomenon may be due to higher levels of supervision for new residents at

the most intensive teaching hospitals.




                                               3
EMPLOYEE TURNOVER AND PRODUCTIVITY

        As noted by several authors, (Staw, 1980; Mobley, 1982; Glebbeek and Bax,

2004), there is significantly more literature examining the causes, rather than

consequences, of turnover. Further, the literature that does address the consequences of

turnover is split over the direction of these effects.

        In his balanced review of the potential consequences of turnover, Staw (1980)

notes several theoretical, negative effects of turnover on organizations. These include

selection, recruitment, and training costs for replacement workers, operational disruption,

and demoralization of remaining workers. In a sample of nearly 1,000 firms, Huselid

(1995) found a negative relationship between turnover and productivity (measured by

sales per employee) and corporate financial performance (as measured by Tobin'
                                                                             s q).

        One explanation for the hypothesized negative effect of turnover on productivity

and performance is that job exits interfere with learning by individuals or teams. Several

studies find that worker productivity improves with experience2 (Levhari and Sheshinski,

1973; Newell and Rosenbloom, 1981; Maranto and Rodgers, 1984, Hellerstein and

Neumark, 1995). While experience with a given firm likely leads to higher productivity,

the converse may also be true—more productive workers may be less likely to leave a

firm (Jovanovic, 1979). Further, Price (1977) notes that younger workers exhibit higher

rates of turnover than older workers. To the extent that instances of turnover are not

randomly distributed across workers with different levels of underlying productivity, the

turnover of individual employees may be an endogenous event.


        2
          Staw (1980) suggests that the opposite may be true. In many settings, the relationship between
tenure and performance may assume an inverted-U shape, with performance initially increasing and later
decreasing with tenure.



                                                    4
        To address this issue, several studies have used levels of union presence as a

proxy for the workforce stability that firms experience in the absence of turnover. Brown

and Medoff (1978) suggest that this stability accounts for some portion of the positive

relationship they observe between unionization and productivity. Similarly, Clark (1980)

finds a positive relationship between unionization and productivity, though he suggests

that additional evidence is required to establish the degree to which this relationship is

explained by lower turnover. Freeman and Medoff (1984) provide a summary of the

factors—including, but not limited to, lower turnover—that may explain this relationship.

        While these negative effects have received significant attention, several studies

discuss turnover as a contingent, or even positive, phenomenon.3 For example, Jovanovic

(1979) presents turnover as a key element in the process of improving matches between

employers and employees over time. Staw (1980) describes performance as a function of

both skill and effort. As a result, in many settings—particularly high-stress professions

that may lead to employee burnout—the relationship between tenure and performance

may assume an inverted-U shape. Employee skill will initially increase faster than effort

decreases; as burnout begins, however, effort will decline faster than skill improves.

Assuming that performance follows an inverted-U shape over time, turnover may thus

improve average performance. Beyond the replacement of less productive with more

productive workers, turnover of poor performing employees may also serve to improve

the morale and motivation of workers who remain with the firm (Staw, 1980).

        The replacement of low performing workers and the increased motivation of

remaining employees both suggest that turnover could have particularly strong positive


        3
            Dalton and Todor (1979) and Staw (1980) provide reviews of both the positive and negative
effects of turnover on performance.


                                                    5
effects in settings requiring substantial levels of innovation and adaptation (Dalton and

Todor, 1979; Staw, 1980; Mobley, 1982). In support of this claim, experimental work

by Argote et al. (1995) finds that while turnover has a negative effect, on average, on

group performance, this effect is more pronounced for simple tasks than for complex

activities requiring innovation. Argote and Epple (1990), however, also find that

turnover does not appear to have a negative effect on firm productivity in settings where

work is relatively standardized, as the knowledge required to perform a task is codified

and can be easily transferred to new workers. Together, these studies suggest that tasks

requiring intermediate levels of innovation may be the most susceptible to performance

declines following turnover.

       Beyond the nature of the task, several other factors may affect degree and

direction of turnover'
                     s impact on performance. These include the degree of hierarchy

within an organization (Carley, 1992), whether turnover itself is voluntary or involuntary

(Price, 1977), whether turnover occurs in a predictable manner (Staw, 1980), and the

absolute level of turnover in the organization. With respect to this final factor, some have

suggested that the relationship between turnover and performance exhibits an inverted-U

shape with a medium level of turnover being preferred to both low and high levels

(Abelson and Baysinger, 1984).

       Beyond its mixed findings, much of the prior work on turnover either explicitly or

implicitly considers only individual turnover; significantly less attention has been

devoted to cohort turnover, the phenomenon we consider in this paper. One analog to

cohort turnover whose performance implications have been considered in the literature is




                                             6
the turnover of management teams (e.g., Tushman and Rosenkopf, 1996).4 It is not clear,

however, how one should expect findings on management turnover to generalize to the

performance of line workers. While not a direct study of cohort turnover among line

workers, Krueger and Mas (2004) find evidence that a period of significant labor

unrest—including a strike and the large-scale use of replacement workers—in a Firestone

tire plant was associated with reduced product quality. The effects identified in their

work speak to the impact of labor unrest on performance, but do not separate the effects

of worker discontent from the impact of the cohort turnover that occurs when striking

employees are replaced.

        One reason for the lack of attention to cohort turnover may be the fact that it

occurs less frequently than individual turnover. Nevertheless, it takes place in several

important settings including military deployments, changes of political administrations,

labor strikes, and, of course, annual house staff turnover in teaching hospitals. A second

reason for the lack of attention placed on cohort turnover may be the belief that the

answers to questions concerning its effects are obvious—given the sheer magnitude of

the change it brings, cohort turnover must have a detrimental impact on performance.

Despite the magnitude of cohort turnover, however, it often occurs in a predictable

fashion and the affected organizations should, theoretically, have time to prepare for its

occurrence. For example, attending physicians in teaching hospitals—being aware of the

turnover that occurs each July—may focus intently on supervising new residents at that

time of the year. As a result, the impact of worker turnover in settings where the


        4
            Much of the literature on the turnover of management teams focuses on the determinants of such
activity (e.g., Fee and Hadlock, 2004; Hayes, Oyer, and Schaefer, 2005), thereby treating turnover as the
dependent, rather than independent, variable.



                                                    7
supervisory staff (in this case, attending physicians) does not change over is not a priori

obvious.5 The presence of a stable cadre of potential supervisors in teaching hospitals

allows us to examine the impact of supervision as a potential moderator of turnover’s

effect on performance. We return to this issue at the end of our empirical analysis.

         Perhaps the most attractive empirical characteristic of cohort turnover is that it

typically occurs for exogenous reasons that are independent of the performance of

individual workers. Such changeovers are mandated as a matter of policy, as is the case

with teaching hospitals where the annual turnover occurs regardless of the underlying

productivity of the physicians and hospitals involved. In contrast, instances of individual

turnover may occur due to particular characteristics of the departing or entering worker—

such as motivation or ability—that remain unobserved by the researcher and may bias

statistical estimates of turnover’s impact.



COHORT TURNOVER IN TEACHING HOSPITALS

         It is widely agreed that teaching hospitals have two primary objectives—the

provision of high quality medical care and the training of new doctors. These related but

distinct objectives overlap within medical residency programs. Medical school graduates

in the United States apply for residencies at any of the roughly 800 teaching hospitals in

the country. Depending on a physician’s specialty, residencies typically last for three-to-

five years, during which time residents represent an important piece of a hospital’s

system for delivering care.



         5
           We also note that the non-physician clinical staff (e.g., nurses, technicians) in teaching hospitals
does not turnover each July. These individuals may provide informal supervision to new residents.



                                                       8
       Patient care in teaching hospitals is provided by teams of medical professionals

that include attending physicians, fellows, residents, and medical students. Much of the

care for patients is provided by a resident, who supervises medical students and is

supervised by the chief (i.e., most senior) resident in that field and an attending physician.

The daily activities of residents include admitting, treating, and discharging patients. In

some departments, fellows (i.e., physicians who have completed their residencies) my

provide an intermediate level of supervision between the residents and attending

physicians.

       Residency programs in the United States are structured like schools. Each class

of residents enters together at the beginning of the academic year, and the senior

members of the program all graduate together. For residency programs, the year begins

and ends on July 1st. The annual transition, however, does not occur all on one day.

Typically, hospitals will complete the transition over a two-to-three week period, lasting

from the middle of June through the first week of July.

       One might imagine that hospitals would transition the new interns into their

positions slowly. Anecdotal evidence, however, suggests that each cohort of residents

typically moves up one level and covers the entire set of responsibilities of the group it is

replacing. As a result, on day one new interns may have the same responsibilities that the

now-second-year residents had at the end of June (i.e., after they had a full year of

experience).

       The changeover creates the potential for turmoil in teaching hospitals as each

cohort of doctors becomes comfortable with new roles and responsibilities. With respect

to this changeover, Claridge et al. (2001) note, “During this time of year, there is clearly




                                              9
a feeling of apprehension among providers of health care, as well as among many

patients.” Gawande (2002) echoes these concerns:

        In medicine we have long faced a conflict between the imperative to give
        patients the best possible care and the need to provide novices with
        experience. Residencies attempt to mitigate potential harm through
        supervision and graduated responsibility…But there is still no getting
        around those first few unsteady times a young physician tries to put in a
        central line, remove a breast cancer, or sew together two segments of
        colon. No matter how many protections we put in place, on average these
        cases go less well with the novice than with someone experienced.

These anecdotal observations suggest the need for systematic analysis of the implications

of this annual turnover for medical productivity.

        Most of the medical literature on staffing and performance in teaching hospitals

deals with issues concerning limitations on resident work hours6 or differences in

outcomes on weekends and weekdays7—two periods when the average level of on-duty-

physician experience is expected to differ substantially. There exists a limited set of

previous studies in the medical literature dealing with the July phenomenon.

        Barry and Rosenthal (2003) test for a July phenomenon in a sample of 28

hospitals in northeast Ohio. Similar to our approach, they compare teaching and non-

teaching hospitals in terms of LOS and risk-adjusted mortality, though they focus solely

on patients in intensive care units (ICUs). They do not find evidence of the July

phenomenon in this population, but do note that their findings may not generalize to non-

ICU patients. Specifically, they suggest that, given the severity of patients in this setting,

ICU residents may receive higher levels of supervision than their non-ICU counterparts.


        6
          Examples include Thorpe (1990), Laine et al. (1993), Leach (2000), Gaba and Howard (2002),
Steinbrook (2002), and Weinstein (2002).
        7
            Examples are Hendry (1981), Bell and Redelmeier (2001), and Dobkin (2002).



                                                   10
       Some studies find a link between the July turnover and hospital inefficiencies.

Rich et al. (1993) examine several teaching hospitals in the Minneapolis area and find

that doctors spend less money on diagnostic tests and pharmaceuticals as their experience

increases throughout the academic year. These results, however, applied only to medical

(i.e., non-surgical) patients. Rich et al. (1993) use a difference-in-differences approach,

much as we do in this paper, to control for seasonal patterns. They utilize patient

outcomes in non-teaching hospitals as a baseline from which to estimate the impact of the

July turnover for teaching hospitals. They are able to identify some changes in efficiency

but are unable to find any evidence of mortality differences. Compared to our study, both

Rich et al. (1993) and Barry and Rosenthal (2004) rely on data from a small number of

hospitals, which may explain why they do not find significant effects on outcomes.

       While they do not directly test for the presence of a July phenomenon, Griffith,

Wilson et al. (1997) examine patterns in test ordering among physicians in the neonatal

intensive care unit at a single hospital. They find that first-year interns are more likely to

incur higher charges than their more experienced colleagues. In addition to its small

sample size, this study is limited by the fact that it does not consider effects on medical

outcomes. As such, it is difficult to determine whether these higher charges reflect

beneficial attention to detail or unnecessary utilization due to inexperience.

       A third study claims to reject the existence of a July phenomenon on any

dimension for the trauma unit at one particular hospital (Claridge et al., 2001). This

paper compares patient outcomes in April and May with those in July and August and

does not identify any significant differences between the two periods. Given their study

design, however, Claridge et al. (2001) are unable to control for seasonal variations in




                                              11
patient outcomes that could affect outcomes at all hospitals regardless of teaching status.

For example, as we will illustrate later, patients admitted to hospitals in the winter have

higher mortality rates than those admitted in the summer. Without some baseline to

adjust for exogenous changes in patient outcomes, a comparison of outcomes for one

hospital at two times of the year may be confounded by these omitted variables.



DATA


         The primary source of data for this analysis is the Healthcare Cost and Utilization

Project (HCUP) National Inpatient Sample (NIS) for each year from 1993 to 2001.8 NIS

contains discharge-level data for all inpatient cases at a sample of roughly 20% of the

community hospitals9 in the United States. Depending on the year, NIS includes

information for hospitals from between 17 and 33 states.

         For each patient, NIS provides information on patient age and gender, expected

primary payer (i.e., Medicare, Medicaid, private including HMO, self pay, no charge, and

other), length of stay (LOS), total charges, and in-hospital mortality. In addition, NIS

includes detailed data on a patient’s principal and secondary diagnoses, principal and

secondary procedures, and diagnosis-related group (DRG).




         8
          The NIS database is administered by the Agency for Healthcare Research and Quality (AHRQ),
previously known as the Agency for Health Care Policy and Research (AHCPR).
         9
           The NIS definition of “community hospital” is the same as that used by the American Hospital
Association (AHA): “…‘all nonfederal, short-term, general, and other specialty hospitals, excluding
hospital units of institutions.’ Included among community hospitals are specialty hospitals such as
obstertrics-gynecology, ear-nose-throat, short-term rehabilitation, orthopedic, and pediatric. Excluded are
long-term hospitals, psychiatric hospitals, and alcoholism/chemical dependency treatment facilities
(Healthcare Cost and Utilization Project, 1999).”



                                                     12
       We link the NIS data with information from the AHA Annual Survey of

Hospitals, which includes data on the operating and financial characteristics for more

than 6,000 hospitals each year. In addition to several other items, the AHA database

provides information on the number of hospital beds and full-time residents (including

interns) at each facility in a given year. Using this information, we are able to construct

our measure of teaching intensity—full-time residents per hospital bed.

     Our final sample of facilities is limited to those that appear in both the NIS and

AHA databases. The appendix presents the number of hospitals that appear in our

sample and in the NIS by year. For each year and state, the table provides the number of

hospitals appearing in the NIS and in our matched NIS-AHA sample. Most of the

discrepancies between the matched sample and the NIS are due to the fact that certain

states opted not to provide the hospital identifiers required to match NIS and AHA data in

certain years. In other rare cases, a hospital may appear in the NIS but not the matched

sample because that facility did not appear in the AHA data for a given year.



EMPIRICAL METHODOLOGY


Hospital Categories

       The source of identification in our empirical analysis is the varying degree to

which certain types of hospitals rely on residents. Initially, we divide hospitals into three

categories—non-teaching hospitals, minor teaching hospitals, and major teaching

hospitals. Non-teaching hospitals are those that are not listed as teaching hospitals in the

NIS. These facilities have few, if any, residents. As such, we would not expect them to

be affected by the July changeover. Those hospitals that are listed as teaching hospitals



                                             13
in the NIS data are subdivided into two categories. Minor teaching hospitals are those

teaching hospitals that have resident intensities (i.e., full-time residents per inpatient

hospital bed) that are less than 0.25, while major teaching hospitals are those facilities

with teaching intensities equal to or greater than 0.25. This threshold for resident

intensity is used by the Medicare Payment Advisory Commission (MedPAC) to

distinguish minor and major teaching facilities (Medicare Payment Advisory

Commission, 2002).

        Figure 1 illustrates that, in the aggregate, both LOS and mortality vary quite

substantially throughout the calendar year. Productivity appears to decline in the winter

months, as evidenced by increases in both LOS and mortality during that period. This

pattern has been noted by epidemiologists (e.g., Gemmell et al., 2000) and has been

attributed to a range of factors including the impact of seasonal disease (e.g., influenza

and respiratory illness) and weather. Key to the empirical strategy in our paper is the use

of non-teaching hospitals as a control for these seasonal changes in outcomes, which

should affect all hospitals regarding of teaching status. We can thus calculate “de-

seasoned” trends in LOS and mortality for teaching hospitals to determine the extent of

potential effects around the July turnover.

        Table 1 presents descriptive statistics for each of the three hospital categories as

well as for the entire sample. The first row illustrates the differences in average teaching

intensity across the three groups. This average measure increases from 0.01 for non-

teaching facilities to 0.10 and 0.53 for minor and major teaching hospitals, respectively.

In terms of both measures of facility size—hospital beds and admissions per year—the

hospitals get progressively larger as the level of teaching intensity increases. Teaching




                                              14
intensity is also correlated with the demographics of a hospital’s patient base. In

particular, non-teaching hospitals attract older patients than either type of teaching

hospital. The average age for patients at non-teaching facilities is 49.0 versus 45.0 and

40.9 for minor and major teaching hospitals, respectively. In addition to having younger

patients, the major teaching hospitals in our sample also have a higher percentage of

Medicaid patients than the other groups. Moving from non-teaching to minor teaching to

major teaching, this percentage increases from 15% to 17% to 26%. This relationship is

consistent with the fact that many teaching hospitals are located in densely populated

cities.

          The bottom portion of Table 1 presents information on the mortality rate and

average length of stay (LOS) for each type of hospital. The values are not adjusted for

differences in the severity of the case mix at each type of facility. While we perform

more sophisticated risk-adjustment in our later analysis, here we simply present each rate

for the entire population, as well as separately for patients younger than age 65 and those

65 and older. Average LOS increases with teaching intensity both for the entire

population and each of the age groups. This trend is consistent with the claim that major

teaching hospitals tend to attract the most complex cases among the three groups.

Overall mortality, however, is highest for the non-teaching facilities—2.7% versus 2.4%

for the minor and major teaching group. At first glance, this finding seems puzzling

given the fact that the LOS data suggests that major teaching hospitals were attracting the

most severe cases. Analysis of mortality by age category, however, reveals that, within

each group, the mortality rate either increases (age<65) or remains relatively flat (age

65+) with teaching intensity. These latter results suggest that the higher overall observed




                                              15
mortality rate for non-teaching hospitals may be due to the higher average age of their

patients.



Basic Specification

             Our multivariate analysis relies on a difference-in-differences framework that

follows the relative changes in risk-adjusted LOS and mortality for the three groups of

hospitals over the course of the year. The basic specification takes the following form:



Yh ,m ,t =        h   +   t   +    m   +   1   ⋅ MIN _ TCH h ,m ,t +       2   ⋅ MAJ _ TCH h ,m ,t +
                  6                                               6                                                          (1)
                      3m      ⋅(   m   × MIN _ TCH h ,m ,t ) +             4m   ⋅(   m   × MAJ _ TCH h ,m ,t ) +   h ,m ,t
             m =1                                                m =1




where Y represents the dependent variable of interest (i.e., risk-adjusted average LOS or

risk-adjusted mortality).

             The first two terms on the right-hand side of (1) are vectors of fixed effects for

hospital and year, respectively. The third term, µm, represents a vector of fixed effects for

six multi-month periods during the year—January through March, April through May,

June, July through August, September through October, and November through

December. Given that the residency changeover begins in late June for many hospitals,

we isolate that month and then compare the change in the dependent variable from April-

May to July-August for teaching hospitals to the similar change for non-teaching

hospitals to measure the impact of the July turnover.10


             10
             Due to the fact that the residency changeover begins in 3rd and 4th weeks of June at several
hospitals, mortality and LOS results for that month represent a mixture of outcomes from both before and
after the transition. We thus use the comparison of July-August to April-May to measure the July


                                                                      16
         MIN_TCH and MAJ_TCH are indicators for minor and major teaching hospitals,

respectively.11 The next two terms on the right-hand side of (1) are vectors of

interactions between the teaching hospital categories and the month effects. The

coefficients on the MIN_TCH (MAJ_TCH) interactions thus capture the extent to which

any seasonal pattern that is found for minor (major) teaching hospitals differs from that

for the non-teaching controls. Each of the observations in (1) is weighted by the total

number of cases for the hospital-month pair to account for the fact that all of the

dependent variables are averages. Finally, the standard errors are clustered by hospital to

address potential lack of independence in the error term, εh,m,t.



Risk-Adjustment of Dependent Variables

         As suggested in Table 1, the average severity of patients likely differs across the

three types of hospitals. To the extent that the differences in patient severity for major

teaching, minor teaching, and non-teaching hospitals vary systematically over the course

of the year, risk adjustment is required to ensure proper identification of any July

phenomenon. For example, to the degree that relatively healthy individuals in the

population aged 65 and older move from cold climates in northeastern states—which tend

to have a high concentration of teaching hospitals—to warmer southern and western




phenomenon. This difference captures the change in the dependent variables from the two complete
months that precede the beginning of the changeover for any hospital to the two complete months that fall
after its conclusion for all hospitals.
         11
            Despite the inclusion of hospital fixed effects, the uniteracted coefficients on minor (β1)and
major (β2) teaching status are identified by those facilities that change between minor and major teaching
status across years. For example, a hospital may have a teaching intensity of 0.25 (making it a minor
teaching hospital) in one year and 0.30 (making it a major teaching hospital) in the next.



                                                     17
states during the winter months, the mortality risk for the hospitalized population in the

northeast will increase ceteris paribus during this period of the year.

         The covariates in our risk-adjustment equation are patient age; age squared;

gender; an indicator for Medicaid as the primary payment source; indicators for a

patient’s state of residence; interactions of the state indicators with both the linear and

quadratic age terms; and the Charlson index—a measure of comorbidities that increase a

patient’s risk of mortality (Charlson et al., 1987). The Medicaid variable is included as a

proxy for the patient’s socioeconomic status.12 The interactions of the state-of-residence

and age terms are included to control for the fact that the average severity of patients,

conditional on age, may vary across geography.

         Given that the in-hospital mortality variable is binary, we use logistic regression

to obtain estimated probability of death for each patient discharge. For LOS, we use a

simple linear regression to calculate predicted values. The risk-adjustment equations are

run separately for each calendar year. The observed and expected values for mortality

and LOS are then averaged by hospital and month. The risk-adjusted value of each

dependent variable is calculated as the ratio of the observed-to-expected rate for a given

hospital-year. For example, the risk-adjusted mortality rate (RAMRh,m,t) is:

                         OMRh ,m ,t
         RAMRh ,m ,t =                 ∗ OMRt                                                       (2)
                          EMRh ,m ,t




         12
            With linear and quadratic terms for patient age included in the regression, we do not include a
separate term for Medicare status. While it would be useful to include an indicator for HMO patients—
who may be healthier, on average, than patients in other payer categories—the HCUP data does not
distinguish HMO patients from those with other forms of private insurance (e.g., indemnity).


                                                     18
where OMRh,m,t and EMRh,m,t are the observed and expected mortality rates, respectively,

for hospital h in month m of year t. OMRt is the average mortality rate for the entire

sample in year t and is used simply to normalize the value of RAMRh,m,t.



RESULTS AND DISCUSSION


Base Results

       Table 2 presents results from our estimation of (1), the basic regression using

three discrete categories of teaching status. The coefficients in this table represent the

change in the dependent variable for minor and major teaching hospitals relative to the

change for non-teaching hospitals over the same period. As noted earlier, we use the

period just prior to the resident turnover (April - May) as the baseline. A positive

coefficient thus indicates that, on average, the hospital group in question experiences a

larger increase in the outcome measure than does the non-teaching group over the same

period of time. For example, the value of 0.040 for the September-October coefficient

for the minor teaching group (Column 1) suggests that the change in LOS from April-

May to September-October is 0.040 days greater for minor teaching than for non-

teaching hospitals. Similarly, the November-December coefficient for the same group

suggests that change in LOS from April-May to November-December is 0.036 days

greater for minor than for non-teaching hospitals.

       For our purposes, the coefficients of greatest interest are those in the period just

following the resident turnover (i.e., July-August). In terms of LOS (Column 1), the

July-August coefficient for minor teaching hospitals is 0.049 and is significant at the 1%

level. To provide a perspective on the magnitude of this effect, we note that the average



                                             19
risk-adjusted LOS for minor teaching hospitals is 5.3 days. If we assume that LOS is

proportional to hospital costs, these results suggest that costs increase by roughly 0.9%

following the July turnover.

         The estimated coefficients on LOS for minor teaching hospitals decline somewhat

in magnitude during the months from September to December and remain significantly

different from the April-May baseline. By January-March, LOS falls back to its value in

the April-May period. Nevertheless, the coefficients for September-October and

November-December are not significantly different than that for July-August, so we are

not able to reject the hypothesis that LOS for minor teaching hospitals increases in July-

August and remains at that higher level for the final four months of the calendar year.

The January-March coefficient, while not significantly different from April-May, is

significantly lower than the July-August value at the 1% level. This reduction in the

estimated coefficient over the course of the academic year suggests that house staffs may

benefit from experience-based improvement in performance over time.

         Consistent with the view that the July phenomenon should increase with the

intensity of a hospital'
                       s teaching program, we find that, relative to minor teaching

hospitals, major teaching facilities show even stronger evidence of such a trend in LOS.

Specifically, these hospitals experience a positive and significant increase in LOS relative

to non-teaching hospitals following the July turnover, and the effect remains for

approximately six months. This increase appears to begin in June, and the estimated

coefficient for that month (0.057) is significant at the 5% level.13 The effect, however,

appears to strengthen in terms of both magnitude (0.111) and significance (1%) in the


         13
           As noted earlier, June represents a mixture of days before and after the turnover at many
hospitals. The coefficient on June may thus underestimate the immediate impact of the turnover.


                                                    20
July-August period. This effect represents a 1.9% increase relative to the average LOS

for major teaching hospitals (5.80 days). As with minor teaching hospitals, the effects

for September through December are significantly different from the April-May baseline

and decline in estimated magnitude over time. Again, however, these coefficients are not

statistically distinguishable from the July-August estimate. By January-March, LOS falls

to the point where it is insignificantly different from April-May, but significantly lower

than the July-August coefficient. These results provide additional support for the

contention that house staffs learn over the course of the academic year.

       The fact that LOS, and, in turn, resource utilization, increase following the July

turnover does not provide conclusive evidence concerning its effects on medical

productivity. To address this issue, one also needs to consider the impact of this turnover

on medical quality. Column 2 presents results using risk-adjusted mortality—a proxy for

quality—as the dependent variable. While minor teaching hospitals do not experience

significant changes in mortality during the course of the academic year, major teaching

facilities do show evidence of a July phenomenon with respect to this outcome measure.

As with LOS, major teaching hospitals experience an increase in their risk-adjusted

mortality rate in June. This increase of 0.063 percentage points is significant at the 10%

level. For the July-August period, the magnitude of this effect nearly doubles (to 0.122

percentage points) and becomes significant at the 1% level. The magnitude of this July

phenomenon represents a 4.3% increase relative to the average mortality rate of 2.82%

for major teaching hospitals. Evidence of learning is again present in the coefficients for

the remainder of the academic year; the levels for September-October and November-

December are both significantly different from April-May; the level for January-March—




                                             21
though not distinguishable from April-May—is significantly lower than that for July-

August. While suggestive of learning, these results also imply that the negative effects of

the July turnover linger—albeit at slightly lower magnitudes—for the months from

August through December.



Testing for Patient Self-Selection and Increased Transfers

        While consistent with declines in medical productivity following cohort turnover,

our results are also consistent with alternate explanations. One leading hypothesis is that

patients recognize July to be a time of turmoil for teaching hospitals and that those with

choice (i.e., elective patients) decide to avoid those facilities at that time of the year. Of

course, these elective patients are likely to be relatively healthier than those who lack

choice regarding their admission to the hospital. This self-selection of the patient base

could thus leave teaching hospitals with relatively sicker patient populations at precisely

the time we estimate their resource use to be increasing and their outcomes to be

declining. If such selection were occurring, we would be mistaken to attribute the effects

we observe to a decline in productivity.

        We offer a test of the selection hypothesis in Column 3 of Table 2. If patients are

in fact self-selecting away from teaching hospitals in July and August, then teaching

hospitals should experience a decline in their number of admissions relative to non-

teaching facilities during those months. We estimate a regression of the same form as

(1), but with the number of hospital admissions on the left-hand side. The results are not

consistent with a self-selection story. In particular, the coefficient in the July-August

time period for major teaching hospitals—the period most critical for the analysis of the




                                              22
July phenomenon—is positive. This effect is actually in the opposite direction of that

which one would expect under the self-selection hypothesis.

        The positive coefficient on admissions for major teaching hospitals in July-

August is related to a second potential explanation for our base results: that major

teaching hospitals are receiving a higher percentage of patients transferred from minor

teaching or non-teaching hospitals during the summer months due to the presence of

excess hospital capacity during warmer months. In Column 4, we thus repeat our

analysis using the percentage of cases transferred from another hospital as the dependent

variable. Again, we find no systematic change in transfer rates for either minor or major

teaching hospitals around the July turnover.



Robustness and Extensions

Results Using Medicaid Patients

        To test the robustness of our base results, we take advantage of the empirical

regularity that Medicaid recipients are more likely to be treated by house staff physicians

than are patients with private health insurance. This phenomenon is typically attributed

to the fact that Medicaid recipients are less likely to have a regular source of physician

care than those with private insurance. James Tallon, president of the United Hospital

Fund, noted this fact in reaction to his organization'
                                                     s study of Medicaid in New York

City:

        ‘These findings indicate the medical residents play a major role in
        providing primary care for low-income people who rely on Medicaid…
        As these Medicaid beneficiaries are enrolled into managed care, we need
        to evaluate whether we can accomplish the goals of managed care—
        providing coordinated and continuous care—while relying so heavily on
        this workforce of doctors in training.’ (United Hospital Fund, 1999)



                                               23
To the extent that residents play a disproportionately large role in the treatment of

Medicaid patients, we would expect that the July phenomenon should be even more

pronounced for these individuals than for the rest of the population.

         Table 3 presents results for LOS and mortality separately for the Medicaid and

non-Medicaid populations. In terms of LOS, both the Medicaid and non-Medicaid

populations show evidence of an increase for major teaching hospitals following the July

turnover. While the July-August coefficient is slightly bigger in absolute magnitude for

Medicaid (0.126) than for non-Medicaid (0.097) patients, the two coefficients are nearly

identical in terms of magnitude relative to the mean LOS for their respective populations.

Further, the coefficient for the Medicaid population is significant at 10% versus 1% for

the non-Medicaid group. Another difference between the Medicaid and non-Medicaid

populations with respect to LOS is that the latter does show evidence of a July

phenomenon for minor teaching hospitals that is not present for the former.

         With respect to mortality, the July-August coefficient for major teaching hospitals

is substantially larger for the Medicaid (0.251) than for the non-Medicaid population

(0.093). Relative to the mean mortality rate for each population at major teaching

hospitals, the coefficient for the Medicaid group represents a 7.3% increase while that for

the non-Medicaid group translates into a 3.4% increase. While each coefficient is

significantly different from zero, we are not able to reject the null hypothesis that these

two coefficients are the same.14 Nonetheless, the fact that the coefficient for the


         14
            To test whether these coefficients are the same, we pooled the Medicaid and non-Medicaid
observations and interaction the time period indicators for each type of hospital with a Medicaid indicator.
We then examined the significance of the coefficient on these interaction terms. The coefficient on this
interaction for major teaching hospitals in July-August was significant at the 16% level, suggesting that we
cannot statistically distinguish the coefficients for the Medicaid and non-Medicaid populations.


                                                     24
Medicaid population is notably larger in both absolute and relative terms than that for the

non-Medicaid group is consistent with the story that the effects we are seeing are due to

the July turnover.



Results Using Narrower Categories of Teaching Intensity

         Our base findings suggest that the negative impact of the July turnover on

productivity is increasing in teaching intensity (i.e. major teaching hospitals are more

affected than minor teaching hospitals). Intuitively, this makes sense, as teaching

intensity captures the degree to which a hospital relies on residents and, therefore, should

be correlated with the magnitude of the turmoil created by resident turnover.

Nevertheless, it is not theoretically clear that the relationship between teaching intensity

and the magnitude of the July phenomenon need be either linear or even monotonic. To

more directly test for potential non-linearity in this relationship, we estimate versions of

(1) in which we further divide the major teaching category into thirds on the basis of

teaching intensity.15

         Table 4 presents the results of these regressions. We focus on the coefficients for

the three thirds of major teaching hospitals, as the results for the minor teaching group are

the same as in Table 2. For LOS, the July-August coefficient is positive and significant

for all three thirds. The magnitude of this coefficient increases from 0.086 for the lower

third to 0.125 for the middle third and slightly decreases to 0.120 for the upper third




         15
           Teaching intensities for the lower third of major teaching hospitals range from 0.25 to 0.34
residents per bed, the middle third ranges from 0.34 to 0.55, and the upper third includes all values greater
than 0.55.



                                                      25
(Column 1). None of these coefficients are statistically different from the other two at

conventional levels.

        The results for risk-adjusted mortality (Column 2), however, differ somewhat

from those for LOS. Specifically, the July phenomenon appears to be focused in the

lower and middle thirds, but is not present in the upper third. The effects for the first two

groups (0.172 and 0.152, respectively) are both significantly different from zero and from

the coefficient for the minor teaching category, but are not significantly different from

each other. The effect for the upper third is substantially smaller in magnitude (0.054)

than that for the lower and middle thirds and is not significantly different from zero at

conventional levels. While the decline from the middle to upper thirds is only significant

at the 16% level, it is not clear if this reflects a true lack of significance or if it is simply

due to the relatively small sample size within each third of the major teaching category.

Nonetheless, these results suggest that the July phenomenon with respect to risk-adjusted

mortality does appear to diminish for the most-intensive teaching hospitals. Column 3

again suggests that the observed July phenomenon is not due to patient self-selection

away from major teaching hospitals around the July turnover.



Results Using Supervision Proxies

        One can imagine two leading explanations for the non-linear July phenomenon (at

least with respect to mortality) observed in Table 4. First, it is possible that the most-

intensive teaching programs (e.g., the upper third of major teaching) are more aware of or

sensitive to the possibility of performance declines around the July turnover. As a result,

these programs may provide higher levels of supervision to their house staffs in an effort




                                                26
to avoid major disruptions. Alternatively, the most-intensive programs may be more

likely to be highly prestigious and attract residents and fellows who are capable of

learning more rapidly than those at lower-intensity programs. We refer to these two

explanations as the supervision and worker ability stories, respectively.

        While we cannot definitively distinguish between these two explanations, we do

provide some suggestive evidence in Table 5. We supplement our existing data with

information from the American Medical Association’s FREIDA database, which provides

annual information on the number of residents and attending physicians for individual

residency programs within teaching hospitals. We were able to obtain this data for each

year from 1994 through 2000. For each hospital and year, we calculate the ratio of full-

time attending physicians to residents as a weighted average across all of a hospital’s

medical and surgical teaching programs. We use this ratio as a proxy for the level of

supervision provided to house staff at a given teaching hospital. This ratio is a not a

perfect proxy, as it reflects only potential (not actual) supervision levels and does not

account for unobserved factors. Nevertheless, it represents a reasonable approximation

of our variable of interest.

        As a proxy for worker (i.e., house staff) ability, we use an indicator for whether

that facility was ranked as a top hospital in the United States by U.S. News & World

Report in at least one major specialty category in 2002. In that year, 205 acute care

hospitals (3.4% of the 6,045 considered) in the United States were included in this group

(Comarow, 2002). Though being included in this list is not a perfect measure of the

prestige of a hospital’s teaching programs, it does represent a reasonable proxy.




                                             27
        In the first two columns of Table 5, we divide major teaching hospitals into low-

and high-supervision groups. We assign major teaching hospitals to high- and low-

supervision groups based on their attending-to-resident ratio in each year.16 Table 5

presents the results of these regressions using different thresholds to define major

teaching hospitals with high supervision levels. The first two rows show the July-August

coefficients of interest from regressions where high-supervision hospitals are defined as

the top 50% of major teaching hospitals with respect to the attending-to-resident ratio.

Thus, the “Major Teaching” coefficient suggests that major teaching hospitals with

supervision levels in the lower 50% had an increase in LOS of 0.108 days in July-August.

For high-supervision, major teaching hospitals, the estimated increase is smaller at 0.066

(=0.108-0.042) days. While the July-August increase for high-supervision hospitals

remains significantly different from zero at the 10% level, it is not significantly different

from the 0.108 increase for low-supervision hospitals.

        The first two rows of Column 2 show similar results with respect to risk-adjusted

mortality. Low-supervision hospitals in the major teaching group show a July-August

increase of 0.156 (significant at the 10% level). The magnitude of this increase declines

to 0.090 (=0.156-0.066) for high-supervision hospitals and is significantly different from

zero at the 5% level. As seen by the coefficient on “Major Teaching x High

Supervision”, the July-August increase is not significantly different for the high- and

low-supervision groups.

        The next four rows of Table 5 show analogous results for regressions where high-

supervision hospitals are defined as those in the top 33% and top 20% of major teaching

        16
           A given hospital may thus be in the high-supervision group in one year and in the low-
supervision group in the following year.



                                                    28
hospitals, respectively, in terms of their supervision ratios. With respect to LOS, these

more stringent definitions of “high-supervision” hospitals do not have much of an effect

on the results reported above—low- and high-supervision hospitals in the major teaching

group experience similar increases in LOS in the July-August period.

       The results for mortality are also consistent across the various definitions of high

supervision with one notable exception. The final two rows of Column 2 suggest that,

when high-supervision is defined as the top 20% of major teaching hospitals, high-

supervision hospitals experience a significantly smaller increase in mortality during the

July-August period. This decline in the July phenomenon of 0.146 percentage points is

significant at the 10% level. Further, it reduces the overall July phenomenon for high-

supervision hospitals to 0.004 percentage points (insignificantly different from zero at

conventional levels). This result suggests that major teaching hospitals with very high-

levels of supervision appear to be able to avoid the adverse impact of the July changeover

on mortality performance.

       Of course, it is possible that those hospitals with the highest levels of supervision

are also those with the highest levels of worker ability. In the next two columns of Table

5, we also split major teaching hospitals into two groups based on whether or not they

were ranked as a top hospital in at least one major specialty. The results of the LOS

regressions in Column 3 are similar to those in Column 1. While the ranked, major

teaching hospitals did have higher estimated increases in LOS than their unranked

counterparts, the coefficients for the two groups were not significantly different from

each other. The mortality results in Column 4 are also similar to those in Column 2. We

note that, when high supervision is defined as the top 20% of hospitals, the decline of




                                             29
0.123 percentage points in the July-August coefficient is no longer significant at

conventional levels; it is, however, significant at the 15% level, and the overall effect for

high-supervision hospitals of 0.039 (=0.162-0.123) is insignificantly different from zero

(as in Column 2).

       One might be concerned that the high supervision and ranking variables are so

highly correlated that it is not possible to identify independent effects. We address this

issue in two ways. First, we note that the weighted correlation between high supervision

(defined by the 20% threshold) and being ranked is only 0.31. Second, the final two

columns of Table 5 illustrate the effect of hospitals being ranked without controlling for

supervision levels. We find that ranked, major teaching hospitals experience a July

phenomenon that is 0.10 percentage points smaller than that for their unranked

counterparts. Nevertheless, this difference is not significant at conventional levels,

though it is significant at the 15% level. It is possible that, with more refined data, we

might find that higher levels of house-staff ability reduce the magnitude of the July

phenomenon. That said, our initial results suggest that supervision—particularly at very

high levels—may play a role in mitigating the disruptive effects of turnover for major

teaching hospitals, even after controlling for differences in worker ability.



CONCLUSION


       This study examines the relationship between cohort turnover and productivity

using a unique setting in which turnover occurs exogenously—the July turnover of house

staffs in teaching hospitals. We find that both minor and major teaching hospitals

experience a significant increase in resource utilization—measured by average LOS—



                                              30
immediately following the July turnover, and that the effect appears to last for several

months. We also find that teaching hospitals with medium teaching intensity experience

a significant increase in patient mortality over the same period. The confluence of

increased resource utilization and increased mortality (i.e., decreased quality) during the

July-August period implies that this cohort turnover reduces medical productivity.

Nevertheless, those hospitals with the highest teaching intensities (i.e., the greatest

reliance on residents for the provision of care), seem to avoid the disruption of the July

phenomenon with respect to changes in their average mortality rates. We provide

preliminary evidence suggesting that higher supervision levels play a role in mitigating

the impact of the July turnover in major teaching facilities.

        The magnitude of the estimated effects is substantial and appears to last for

roughly six months. We find that average LOS—our proxy for resource utilization and

cost—for the average, major teaching hospital increases by roughly 2% following the

July turnover and remains between 1% and 2% higher throughout the final six months of

the calendar year. Similarly, the average, major teaching hospital experiences an increase

in risk-adjusted mortality of roughly 4% (not percentage points) in the July-August

period. This effect also remains at levels between 2% and 4% for the last six months of

the calendar year. For the average major teaching hospital, this translates into between

7.8 and 13.8 “accelerated” deaths (i.e., deaths that occur earlier than they would have

occurred in the absence of the July turnover) per year. Based on a total of roughly 200

major teaching hospitals in the United States, the July phenomenon is thus associated

with roughly 1,500 to 2,750 accelerated deaths per year in the United States.17


        17
           To the extent that this effect is sizeable, one might ask why it may go unnoticed by teaching
hospitals. A possible answer to this question is that the July phenomenon occurs at a time of the year when


                                                    31
Determining the social cost of this increase in mortality requires assumptions about the

expected longevity of these individuals in the absence of the July turnover. Such

assumptions are beyond the scope of this paper.

         Beyond its findings with respect to the July phenomenon, this paper has broader

implications for the study of the effects of labor turnover on organizations. It provides

empirical support for the contention that cohort turnover has negative implications for

productivity on average, though these effects do not increase linearly with the intensity of

turnover. We find initial evidence suggesting that supervision can mitigate this negative

effect. This latter finding implies that, even if firms are not able to reduce the levels of

turnover they face, they may be able to manage its effects.

         One question that is not addressed by our study is the degree to which managers

should be concerned about turnover-related declines in productivity. On one hand, these

declines likely reflect the costs associated with valuable on-the-job training. On the

other, they may be larger than necessary to obtain the desired training benefit for new

employees. In the case of teaching hospitals, we thus are not arguing that an optimal

residency system would result in no systematic change in productivity throughout the

year. Presumably, no system can guarantee that residents will be as productive at the

beginning of their tenure as they will be at its end. Ultimately, the important question to

answer is whether declines in productivity are higher than necessary to train new workers

efficiently. The question of optimal supervision levels in the face of significant on-the-

job training is an interesting issue for further study in contexts both within and outside of

the hospital industry.


the overall trend in mortality is declining. As a result, an increase in mortality relative to non-teaching
hospitals may not appear as an increase in absolute mortality.



                                                      32
                                   REFERENCES



Abelson, Michael and Barry Baysinger (1984). “Optimal and Dysfunctional Turnover:
Toward an Organizational Level Model,” Academy of Management Review, 9(2), 331-
341.

Argote, Linda, Chester Insko, Nancy Yovetich, and Anna Romero (1995). "Group
Learning Curves: The Effects of Turnover and Task Complexity on Group Performance,"
Journal of Applied Social Psychology, 25(6), 512-529.

Argote, Linda and Dennis Epple (1990). “Learning Curves in Manufacturing,” Science,
247, 920-924.

Barry, William and Gary Rosenthal (2004). “Is There a July Phenomenon? The Effect of
July Admission on Intensive Care Mortality and Length of Stay in Teaching Hospitals,”
Journal of General Internal Medicine, 18, 639-645.

Bell, Chaim and Donald Redelmeier (2001). “Mortality Among Patients Admitted to
Hospitals on Weekends as Compared With Weekdays,” New England Journal of
Medicine, 345(9), 663-668.

Brown, Charles and James Medoff (1978). “Trade Unions in the Production Process,”
Journal of Political Economy, 86(3), 355-378.

Boylan, Richard (2004). “Salaries, Turnover, and Performance in the Federal Criminal
Justice System,” Journal of Law and Economics, 47, 75-92.

Carley, Kathleen (1992). "Organizational Learning and Personnel Turnover,"
Organization Science, 3(1), 20-46.

Charlson, Mary, Peter Pompei, Kathy Ales, and C. Ronald MacKenzie (1987). “A New
Method of Classifying Prognostic Comorbidity in Longitudinal Studies: Development
and Validation,” Journal of Chronic Diseases, 40(5), 373-383.

Claridge, Jeffrey, Andrew Schulman, Robert Sawyer, Anousheh Ghezel-Ayagh, and
Jeffrey Young (2001). “‘The July Phenomenon’ and the Care of the Severely Injured
Patient: Fact or Fiction?” Surgery, 130(2), 346-353.

Clark, Kim (1980). “The Impact of Unionization on Productivity: A Case Study,”
Industrial and Labor Relations Review, 33, 451-469.

Comarow, Avery (2002). “America’s Best Hospital,” U.S. News & World Report, 133(3),
45.




                                          33
Dalton, Dan and William Todor (1979). “Turnover Turned Over: An Expanded and
Positive Perspective,” Academy of Management Review, 4(2), 225-235.

Dobkin, Carlos (2002). “Hospital Staffing and Inpatient Mortality,” mimeograph.

Fee, C. Edward and Charles Hadlock (2004). “Management Turnover Across the
Corporate Hierarchy,” Journal of Accounting and Economics, 37, 3-38.

Freeman, Richard and James Medoff (1982). What Do Unions Do?, New York, NY:
Basic Books.

Gaba, David and Steven Howard (2002). “Fatigue Among Clinicians and the Safety of
Patients,” New England Journal of Medicine, 347(16), 1249-1255.

Gawande, Atul (2002). Complications: A Surgeon’s Notes on an Imperfect Science, New
York, NY: Metropolitan Books.

Gemmell, Islay, Philip McLoone, F.A. Boddy, Gordon Dickinson, and G.C.M. Watt
(2000). "Seasonal Variation in Mortality in Scotland," International Journal of
Epidemiology, 29, 274-279.

Glebbeek, Arie and Erik Bax (2004). "Is High Employee Turnover Really Harmful? An
Empirical Test Using Company Records," Academy of Management Journal, 47(2), 277-
286.

Griffith, Charles, John Wilson, Nirmala Desai, and Eugene Rich (1997). “Does Pediatric
Housestaff Experience Influence Tests Ordered for Infants in the Neonatal Intensive Care
Unit?” Critical Care Medicine, 25(4), 704-709.

Hayes, Rachel, Paul Oyer, and Scott Schaefer (2005). “Co-Worker Complementarity and
the Stability of Top Management Teams,” working paper.

Healthcare Cost and Utilization Project (1999). The HCUP Nationwide Inpatient Sample
(NIS), Release 6, 1997, Rockville, MD: Agency for Healthcare Research and Quality.

Hellerstein, Judith and David Neumark (1995). “Are Earnings Profiles Steeper than
Productivity Profiles? Evidence from Israeli Firm-Level Data,” Journal of Human
Resources, 30(1), 89-112.

Hendry, R. (1981). “The Weekend—A Dangerous Time to be Born?” British Journal of
Obstetrics and Gynaecology, 88(12), 1200-1203.

Huselid, Mark (1995). "The Impact of Human Resource Management Practices on
Turnover, Productivity, and Corporate Financial Performance," Academy of Management
Journal, 38(3), 635-672.



                                           34
Jovanovic, Boyan (1979). “Job Matching and the Theory of Turnover,” Journal of
Political Economy, 87(5), 972-990.

Krueger, Alan and Alexandre Mas (2004). "Strikes, Scabs, and Tread Separations: Labor
Strife and the Production of Defective Bridgestone/Firestone Tires," Journal of Political
Economy, 112(2), 253-289.

Laine, Christine, Lee Goldman, Jane Soukup, and Joseph Hayes (1993). “The Impact of a
Regulation Restricting Medical House Staff Working Hours on the Quality of Patient
Care,” Journal of the American Medical Association, 269(3), 374-378

Leach, David (2000). “Residents’ Work Hours: The Achilles Heel of the Profession?”
Academic Medicine, 75(12), 1156-1157.

Levhari, David and Eytan Sheshinski (1973). “Experience and Productivity in the Israel
Diamond Industry,” Econometrica, 41(2): 239-253.

Maranto, Cheryl and Robert Rodgers (1984). “Does Work Experience Increase
Productivity? A Test of the On-the-Job Training Hypothesis,” Journal of Human
Resources, 19(3): 341-357.

Medicare Payment Advisory Commission (2002). Report to the Congress: Medicare
Payment Policy (Washington, DC: Medicare Payment Advisory Commission).

Medoff, James and Katherine Abraham (1985). “Experience, Performance, and
Earnings,” Quarterly Journal of Economics, 95(4): 703-736.

Mobley, William (1982). Employee Turnover: Causes, Consequences, and Control.
Reading, MA: Addison-Wesley.

Newell, Allen and Paul Rosenbloom (1981). “Mechanisms of Skill Acquisition and the
Power Law of Practice,” in John Anderson, ed., Cognitive Skills and Their Acquisition.
Hillsdale, NJ: Erlbaum, 1-55.

Polanyi, Michael (1966). The Tacit Dimension, New York, NY: Anchor Day Books.

Price, James (1977). The Study of Turnover, Ames, IA: Iowa State University Press.

Rich, Eugene, Steven Hillson, Bryan Dowd, and Nora Morris (1993). “Specialty
Differences in the ‘July Phenomenon’ for Twin Cities Teaching Hospitals,” Medical
Care, 31(1): 73-83.

Staw, Barry (1980). "The Consequences of Turnover," Journal of Occupational
Behaviour, 1(4): 253-273.




                                           35
Steinbrook, Robert (2002). “The Debate Over Residents’ Work Hours,” New England
Journal of Medicine, 347(16): 1296-1302.

Thorpe, Kenneth (1990). “House Staff Supervision and Working Hours: Implications of a
Regulatory Change in New York State,” Journal of the American Medical Association,
263(23): 3177-3181.

Tushman, Michael and Lori Rosenkopf (1996). “Executive Succession, Strategic
Reorientation and Performance Growth: A Longitudinal Study in the U.S. Cement
Industry,” Management Science, 42(7), 939-953.

United Hospital Fund (1999). “Medical Residents Provide Large Share of Primary Care
at NYC Ambulatory Care Facilities,” press release,
http://www.uhfnyc.org/press_release3159/press_release_show.htm?doc_id=98095,
accessed December 13, 2004.

Weinstein, Debra (2002). “Duty Hours for Resident Physicians—Tough Choices for
Teaching Hospitals,” New England Journal of Medicine, 347(16): 1275-1278.




                                         36
                                                      Figure 1: Average Length of Stay and Mortality Rate by Month, 1993-2001

                                          5.5                                                                                                    3


                                          5.4

                                                                                                                                                 2.5
                                          5.3


                                          5.2




                                                                                                                                                       Average Mortality Rate (Percent)
          Average Length of Stay (Days)




                                                                                                                                                 2

                                          5.1


                                           5                                                                                                     1.5


                                          4.9

                                                                                                                                                 1
                                          4.8


                                          4.7
                                                                                                                                                 0.5

                                          4.6


                                          4.5                                                                                                    0
                                                Jan    Feb    Mar     Apr    May    Jun           Jul         Aug        Sep   Oct   Nov   Dec
                                                                                          Month

                                                                               Average LOS              Mortality Rate

Source: NIS, 1993-2001.




                                                                                           37
                                   Table 1: Descriptive Statistics by Hospital Type, 1993-2001


                                                Non-Teaching            Minor Teaching         Major Teaching            Full Sample
                                                       Standard                 Standard               Standard                 Standard
                                               Mean    Deviation       Mean     Deviation     Mean     Deviation       Mean     Deviation
Residents Per Inpatient Bed                       0.01        0.04       0.07         0.08       0.53           0.28     0.10        0.19
Inpatient Hospital Beds                           135         115        337          206        493            260      187         177
Inpatient Admissions/Year                       4,824        5,016    13,923         8,562    21,222          10,049   7,181        7,684
Patient Age                                       49.0         9.2       45.0          9.0       40.9            8.2     46.5         9.4
Medicaid Admissions/Total Admissions              15%         13%        17%          14%        26%            16%      17%         14%
Medicare Admissions/Total Admissions              39%         14%        33%          12%        26%            10%      35%         14%
Observed Average Length of Stay
      Total                                        4.8         2.2         5.2         1.4        5.9            1.4      5.1         1.9
      Age<65                                       3.7         1.9         4.4         1.3        5.4            1.3      4.2         1.7
      Age 65+                                      6.5         3.0         6.9         1.9        7.5            2.2      6.7         2.7
Observed Mortality
      Total                                      2.6%         1.6%       2.4%         0.7%      2.4%            0.7%    2.5%         1.3%
      Age<65                                     0.8%         1.0%       1.0%         0.4%      1.4%            0.5%    1.0%         0.8%
      Age 65+                                    5.3%         2.2%       5.2%         1.3%      5.4%            1.5%    5.3%         1.9%
Observations (hospital-years)                        4,873                   1,041                      318                 6,232

Percentage of Total Sample                           78.2%                   16.7%                     5.1%



Note: Observations are at the hospital-year level and cover the five-year period from 1993 to 2001.

Source: NIS, 1993-2001.



                                                                 38
                   Table 2: Base Regressions Using Minor and Major Teaching Categories


                                               Change in Dependent Variable Relative to Non-Teaching Baseline (Reference
                                                                          Period=April-May)
                                                                                                                          Transfer Rate
                                                                           Risk-Adjusted                                 (Transfers/Total
                                          Risk-Adjusted LOS                   Mortality             Total Admissions     Admissions*100)

Minor Teaching
     Jan-Mar                                -0.003 (0.012)                 -0.012 (0.018)              1.5   (3.4)        -0.02 (0.04)
     Apr-May
     June                                    0.010    (0.016)               0.006   (0.024)           -8.0   (2.8) ***    -0.02   (0.04)
     Jul-Aug                                 0.049    (0.013) ***           0.014   (0.019)           -0.5   (3.2)        -0.03   (0.04)
     Sep-Oct                                 0.040    (0.012) ***           0.003   (0.020)           -5.2   (3.1) *      -0.03   (0.05)
     Nov-Dec                                 0.036    (0.014) ***          -0.008   (0.020)          -25.9   (3.9) ***     0.02   (0.06)

Major Teaching
     Jan-Mar                                 0.018 (0.021)                 -0.037 (0.027)              4.7   (8.2)         0.03 (0.06)
     Apr-May
     June                                    0.057    (0.028)   **         0.063    (0.034)   *       -5.6   (8.2)         0.05   (0.06)
     Jul-Aug                                 0.111    (0.024)   ***        0.122    (0.039)   ***     30.6   (8.8) ***    -0.09   (0.08)
     Sep-Oct                                 0.092    (0.021)   ***        0.088    (0.043)   **       2.9   (8.9)        -0.01   (0.08)
     Nov-Dec                                 0.073    (0.020)   ***        0.074    (0.037)   **     -31.6   (9.1) ***     0.14   (0.07) **

Mean of Dependent Variable
     Minor Teaching                            5.28                           2.60                     1,259                 4.32
     Major Teaching                            5.80                           2.82                     1,855                 5.25


Observations                                  74,521                         74,521                    74,521              74,521
           2
Adjusted R                                    0.738                          0.465                     0.983               0.822


*,**, and *** denote statistical signficance at the 10%, 5%, and 1% levels, respectively.

Note: The level of observation is the hospital-month. All regressions include fixed effects for hospital, year, and multi-month period,
though these coefficients are not shown in the table for ease of presentation. Standard errors (in parentheses) are heteroskedasticity
robust and clustered by hospital. In regressions with risk-adjusted LOS and risk-adjusted mortality as the dependent variable,
observations are weighted by the total number of cases for the relevant hospital-month.




                                                                      39
                     Table 3: Regressions on Medicaid and Non-Medicaid Populations

                                            Change in Dependent Variable Relative to Non-Teaching Baseline (Reference
                                                                       Period=April-May)

                                                           Medicaid                                       Non-Medicaid
                                                                      Risk-Adjusted                                    Risk-Adjusted
                                        Risk-Adjusted LOS                Mortality          Risk-Adjusted LOS             Mortality

Minor Teaching
     Jan-Mar                              -0.059 (0.052)              -0.182 (0.083) **      0.004 (0.012)            0.003 (0.017)
     Apr-May
     June                                  0.091    (0.062)           -0.011   (0.115)       -0.003   (0.016)         0.009   (0.024)
     Jul-Aug                               0.059    (0.051)           -0.103   (0.093)        0.045   (0.012) ***     0.026   (0.019)
     Sep-Oct                               0.074    (0.048)           -0.134   (0.086)        0.035   (0.012) ***     0.018   (0.020)
     Nov-Dec                               0.100    (0.051) **        -0.207   (0.092) **     0.026   (0.014) **      0.010   (0.019)

Major Teaching
     Jan-Mar                              -0.014 (0.058)              0.014 (0.084)          0.027 (0.024)            -0.032 (0.028)
     Apr-May
     June                                  0.087    (0.091)           0.194    (0.115) *     0.048    (0.024)   **    0.027   (0.036)
     Jul-Aug                               0.126    (0.070) *         0.251    (0.110) **    0.097    (0.019)   ***   0.093   (0.038) **
     Sep-Oct                               0.135    (0.076) *         0.142    (0.096)       0.078    (0.017)   ***   0.072   (0.040) *
     Nov-Dec                               0.119    (0.068) *         0.082    (0.111)       0.063    (0.022)   ***   0.070   (0.039) *

Mean of Dependent Variable
     Minor Teaching                          6.12                        3.25                   5.14                      2.54
     Major Teaching                          6.71                        3.42                   5.59                      2.73


Observations                                71,486                      71,491                 74,500                    74,500
          2
Adjusted R                                  0.407                        0.148                  0.789                    0.430


*,**, and *** denote statistical signficance at the 10%, 5%, and 1% levels, respectively.

Note: The level of observation is the hospital-month. All regressions include fixed effects for hospital, year, and multi-month period,
though these coefficients are not shown in the table for ease of presentation. Standard errors (in parentheses) are
heteroskedasticity robust and clustered by hospital. In regressions with risk-adjusted LOS and risk-adjusted mortality as the
dependent variable, observations are weighted by the total number of cases for the relevant hospital-month.




                                                                  40
            Table 4: Regressions Using Major-Teaching Subcategories

                                         Change in Dependent Variable Relative to Non-Teaching
                                                  Baseline (Reference Period=April-May)
                                                              Risk-Adjusted
                                      Risk-Adjusted LOS          Mortality          Admissions

Minor Teaching

          Jan-Mar                    -0.003   (0.012)         -0.012   (0.018)         1.537    (3.437)
          Apr-May
          June                        0.010   (0.016)          0.006   (0.024)        -7.978    (2.762) ***
          Jul-Aug                     0.049   (0.013) ***      0.014   (0.019)        -0.454    (3.234)
          Sep-Oct                     0.040   (0.012) ***      0.003   (0.020)        -5.217    (3.084) *
          Nov-Dec                     0.036   (0.014) **      -0.009   (0.020)       -25.919    (3.905) ***

Major Teaching

Lower Third
          Jan-Mar                     0.005   (0.029)         -0.026   (0.050)        25.548 (12.836) **
          Apr-May
          June                        0.051   (0.034)         0.032    (0.050)       -15.866   (20.378)
          Jul-Aug                     0.086   (0.028) ***     0.172    (0.087) **     18.759   (14.384)
          Sep-Oct                     0.074   (0.029) **      0.096    (0.096)        -4.515   (11.793)
          Nov-Dec                     0.075   (0.033) **      0.142    (0.072) **    -14.258   (13.929)

Middle Third
          Jan-Mar                     0.085   (0.039) **      -0.042   (0.045)       -10.918 (13.541)
          Apr-May
          June                        0.027   (0.055)         0.106    (0.053) **     -3.480 (9.388)
          Jul-Aug                     0.125   (0.058) **      0.152    (0.056) ***    23.981 (12.723) *
          Sep-Oct                     0.102   (0.056) *       0.086    (0.061)        -1.929 (11.254)
          Nov-Dec                     0.089   (0.047) *       0.028    (0.074)       -27.852 (15.348) *

Upper Third
          Jan-Mar                    -0.025   (0.024)         -0.043   (0.036)         -0.790 (14.165)
          Apr-May
          June                        0.088   (0.039)   **    0.055    (0.057)         2.400   (11.215)
          Jul-Aug                     0.120   (0.031)   ***   0.054    (0.044)        48.835   (16.910) ***
          Sep-Oct                     0.100   (0.024)   ***   0.084    (0.050) *      15.018   (20.280)
          Nov-Dec                     0.059   (0.024)   **    0.053    (0.032)       -52.478   (15.186) ***

Mean of Dependent Variable
         Minor Teaching                    5.28                    2.60                     1,259
         Major Teaching                    5.80                    2.82                     1,855


Observations                              74,521                  74,521                    74,521
           2
Adjusted R                                 0.738                  0.465                      0.984

*,**, and *** denote statistical signficance at the 10%, 5%, and 1% levels, respectively.

Note: The level of observation is the hospital-month. All regressions include fixed effects for hospital,
year, and multi-month period, though these coefficients are not shown in the table for ease of
presentation. Standard errors (in parentheses) are heteroskedasticity robust and clustered by hospital.
In regressions with risk-adjusted LOS and risk-adjusted mortality as the dependent variable,
observations are weighted by the total number of cases for the relevant hospital-month.




                                                    41
                                   Table 5: July-August Coefficients for Categories of Major Teaching Hospitals


                                                         Change in Dependent Variable Relative to Non-Teaching Baseline (Reference Period=April-May)
                                                                     Risk-Adjusted                                 Risk-Adjusted       Risk-Adjusted      Risk-Adjusted
                                           Risk-Adjusted LOS            Mortality           Risk-Adjusted LOS         Mortality            LOS               Mortality

High Supervision = Top 50%
   Major Teaching                            0.108 (0.032) ***      0.156 (0.094) *          0.103 (0.031) ***    0.162 (0.094) *     0.101 (0.028) ***   0.145 (0.047) ***
   Major Teaching x High Supervision        -0.042 (0.049)         -0.066 (0.105)           -0.060 (0.057)       -0.038 (0.107)
   Major Teaching x Ranked                                                                   0.058 (0.057)       -0.091 (0.074)       0.045 (0.043)       -0.100 (0.064)

High Supervision = Top 33%
   Major Teaching                            0.086 (0.036) **       0.133 (0.064) **         0.080 (0.038) **     0.148 (0.068) **
   Major Teaching x High Supervision        -0.006 (0.044)         -0.040 (0.083)           -0.016 (0.042)       -0.012 (0.081)
   Major Teaching x Ranked                                                                   0.038 (0.043)       -0.102 (0.075)

High Supervision = Top 20%
   Major Teaching                           0.083 (0.030) ***       0.150 (0.054) ***        0.077 (0.034) **     0.162 (0.059) ***
   Major Teaching x High Supervision        0.005 (0.050)          -0.146 (0.082) *         -0.005 (0.049)       -0.123 (0.084)
   Major Teaching x Ranked                                                                   0.034 (0.046)       -0.070 (0.080)

Observations                                    73,287                73,287                   73,287               73,287               74,521              74,521


*,**, and *** denote statistical signficance at the 10%, 5%, and 1% levels, respectively.


Note: The level of observation is the hospital-month. All regressions include fixed effects for hospital, year, and multi-month period, though these coefficients are not
shown in the table for ease of presentation. Regressions also include a full set of interactions with multi-month periods for each of the following variables: minor
teaching, major teaching, major teaching x high supervision, and (in Columns 3 and 4) major teaching x ranked. For ease of presentation, only the July-August
coefficients for the various types of major teaching hospitals appear in the table. Standard errors (in parentheses) are heteroskedasticity robust and clustered by
hospital. Observations are weighted by the total number of cases for the relevant hospital-month. The smaller sample size in the first four columns is due to the fact
that data on supervision levels are not available for 1993 and 2000. In these years, all major teaching hospitals are thus excluded from the sample.




                                                                                       42
                         Appendix: Number of Hospitals in the Sample and the NIS by State and Year, 1993-2001

                     1993            1994            1995             1996                 1997             1998             1999             2000             2001
                         Full            Full               Full             Full                 Full             Full             Full             Full             Full
     State        Sample NIS      Sample NIS      Sample    NIS    Sample    NIS     Sample       NIS    Sample    NIS    Sample    NIS    Sample    NIS    Sample    NIS
Arizona            13       13     12       12     15        15     15        15          14      14       14      14       13      13       14      14       12      12
California         95       96     101      102    104      105     102      103         106      107      97      97       95      95       93      93       94      94
Colorado           27       28     21        22     21       22      21       21          18       18      20      20       18      18       22      22       18      18
Connecticut         7        7      7        7       9        9      8         8           9        9       7       7        6       6        6       6       7       7
Florida            165      166    162      163    140      141     137      138                  117              109              98               55               57
Georgia                                                                                           115              114              100              60               60
Hawaii                                                                                              3               4                4                3               3
Illinois            75      75      77      77      73      73       72      72          73        73      75       75      70       70      69      69       65      65
Iowa                70      70      64      64      54      54       53      53          52        52      53       53      54       54      54      54       38      38
Kansas                      72              71              61               60                    62               56               58              57               36
Kentucky                                                                                                                                     31      31       30      30
Maine                                                                                                                       11      11       10      10       9       9
Maryland            40      40      42      42      39      39       39      39          35        35      32      32       23      23       13      13       12      12
Massachusetts       30      30      27      27      25      25       19      19          18        18      17      17       15      15       16      16       16      16
Michigan                                                                                                                                                              30
Minnesota                                                                                                                                                     37      37
Missouri                                            48      49       46      47          44        44      38      39       38      38       39      40       21      22
Nebraska                                                                                                                                                              21
New Jersey          20      20      19      19      18      18       17      17          19        19      17      17       17      17       16      16       14      14
New York            60      60      62      62      59      59       58      58          56        56      52      52       46      46       49      49       43      43
North Carolina                                                                                                                               36      36       34      34
Oregon              19      19      19      19      17      17       17      17          16        16      18      18       18      18       18      18       19      19
Pennsylvania        57      57      53      53      51      51       50      50          52        52      47      47       43      43       42      42       42      42
Rhode Island                                                                                                                                                  1       1
South Carolina              52              51              46               41                    34              34               34               20               18
Tennessee                                                   52               50                    64              72               68               31               36
Texas                                                                                                                                                93               90
Utah                                                                                     13        13      16      16       17      17       14      14       16      16
Vermont                                                                                                                                                       4       4
Virginia                                                                                                                    47      47       21      21       24      24
Washington          23      23      21      21      22      22       22      22          20        20      24      24       25      25       24      24       25      25
West Virginia                                                                                                                                21      21       18      18
Wisconsin           85      85      92      92      80      80       76      76          71        71      67      67       66      66       66      66       35      35

Total Hospitals    786      913    779      904    775      938     752      906         616      1012    594      984     622      984     674      994     634      986
Total States       15        17    15        17    16        19     16        19          16       22      16      22      18       24      21       28       24       33




                                                                                    43
