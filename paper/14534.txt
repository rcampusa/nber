                                NBER WORKING PAPER SERIES




                  INFLATION DETERMINATION WITH TAYLOR RULES:
                 IS NEW KEYNESIAN ANALYSIS CRITICALLY FLAWED?

                                        Bennett T. McCallum

                                        Working Paper 14534
                                http://www.nber.org/papers/w14534


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                    December 2008




I am indebted to Miguel Casares, Marvin Goodfriend, and Edward Nelson for many helpful discussions
and comments on an earlier draft. The views expressed herein are those of the author(s) and do not
necessarily reflect the views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2008 by Bennett T. McCallum. All rights reserved. Short sections of text, not to exceed two paragraphs,
may be quoted without explicit permission provided that full credit, including © notice, is given to
the source.
Inflation Determination with Taylor Rules: Is New Keynesian Analysis Critically Flawed?
Bennett T. McCallum
NBER Working Paper No. 14534
December 2008
JEL No. E4,E5,E52

                                             ABSTRACT

Cochrane (2007) has strongly questioned the basic economic logic of current mainstream monetary
policy analysis, arguing that the standard notion --that "determinacy" of a rational expectations (RE)
equilibrium suffices to imply that stable inflation behavior will be generated -- is incorrect. This is
because New Keynesian (NK) models are typically consistent with the existence of RE paths with
explosive inflation rates (in addition to one or more stable paths) that normally do not imply explosions
in real variables relevant for transversality conditions. Consequently, the usual logic does not imply
the absence of explosive inflation. That result does not, however, justify negative conclusions about
NK analysis. For there is a different criterion that is logically satisfactory for the purpose at hand.
 This is the requirement that, to be plausible, a RE solution must satisfy the property of least-squares
learnability. Adoption of this criterion, which should be attractive to analysts concerned with actual
monetary policy, serves to justify in principle the bulk of current mainstream analysis.


Bennett T. McCallum
Tepper School of Business, Posner 256
Carnegie Mellon University
Pittsburgh, PA 15213
and NBER
bm05@andrew.cmu.edu
1. Introduction

         Quite recently, John Cochrane (2007) has strongly questioned the basic economic

logic of current mainstream monetary policy analysis as widely practiced and described

by Clarida, Gali, and Gertler (2000), King (2000), Svensson and Woodford (2005),

Taylor (1999), Woodford (2003a), and many other prominent writers.1 A central

ingredient of this analysis is the assumption that monetary policy is conducted by means

of a central-bank policy rule for period-by-period adjustment of a short-term nominal

interest rate, with the rule calling for adjustments of this policy rate by more than one-for-

one in response to incipient movements in inflation—thereby satisfying a condition that

is widely referred to as the Taylor Principle.2 Cochrane’s contention is that the standard

notion—that “determinacy” of a rational expectations (RE) equilibrium serves to

guarantee that stable inflation behavior around target will be generated—is incorrect. His

basic reasoning is expressed in the following quote:

         “I argue that the Taylor principle, in the context of new-Keynesian models, does

         not, in fact, determine inflation or the price level. Nothing in economics rules out

         explosive or “non-local” nominal paths. Transversality conditions can rule out

         real explosions, but not nominal ones” (Cochrane, 2007, p. 2).

Consideration of Cochrane’s argument indicates that there is much merit in this point.

The concept of determinacy relied upon in the standard analysis is uniqueness of a

dynamically stable (i.e., non-explosive) RE solution in the model under study (which is



1
  Other notable items in the literature include Taylor (1993), Clarida, Gali, and Gertler (1999), Goodfriend
and King (1997), King and Wolman (1996), and Rotemberg and Woodford (1997). Goodfriend (2007)
presents a recent overview that links formal analysis and actual central bank practive.
2
  The principle is often extended to cases in which policy responds also to incipient movements in the the
output gap. See, e.g., Woodford (2003a, pp. 90-94, 252-261), Taylor (1999).


                                                      1
of course intended to depict reality). But the equations of New Keynesian (NK)3 models

are typically consistent with the existence of RE paths with explosive inflation rates, in

addition to one or more stable paths, under the usual specifications. These are “nominal

explosions” that bring about paths along which real money balances tend to decrease in

magnitude, rather than growing without limit. More generally, explosive paths for

inflation rates—or nominal interest rates—do not normally imply explosions in real

variables relevant for transversality conditions, which are crucial for individual agent

optimization. Consequently, the usual logic, in the usual models, does not imply the

absence of explosive inflation.

         It is my conclusion that this position of Cochrane’s is in principle correct. That

position as just stated does not, however, justify Cochrane’s negative conclusions about

NK analysis.4 For there is a different criterion—in many (but not all) cases implied by

determinacy and not generally implying determinacy—that is logically satisfactory for

the purpose at hand. This is the requirement that, to be plausible, a RE solution should

satisfy the property of learnability, of the type made prominent in the work of Evans and

Honkapohja (1999, 2001).5 Adoption of this criterion, which should be attractive to any

analyst concerned with actual monetary policy issues, serves to justify the bulk of current

mainstream analysis in principle. Indeed, as argued in McCallum (2003, 2007), it

eliminates some other problematic aspects of the NK analysis.


3
  New-Keynesian (NK) models are basically the same as those referred to by Goodfriend and King (1997)
as “New Neoclassical Synthesis” (NNS) models. Some would consider the latter label to be more apt, from
a historical perspective. Nevertheless, I will follow Cochrane in useing the more standard label NK to refer
to the models of current mainstream analysis.
4
  It should be said that Cochrane does not use the term “critically flawed” that appears in my title. I would
think, however, that most exponents of NK analysis would regard a model/policy-rule combination, that
does not rule out explosive inflation, to be critically flawed.
5
  More precisely, the requirement is least-squares (LS) learnability, as a necessary condition for a RE
equilibrium to be plausible and therefore of potential economic relevance.


                                                     2
2. Cochrane’s Critique

         Let me begin by outlining the central aspects of Cochrane’s argument in the

context of the basic NK model that he uses for his main presentation. It is common, in

the NK literature, to utilize a three-equation structure that includes an expectational IS

function,6 a Calvo-type price adjustment relation, and a Taylor-style policy rule in a

system that could be written as

(1)      yt = b0 + b1(Rt − Etπt+1) + Etyt+1 + vt                                   b1 < 0

(2)      πt = βEtπt+1 + κ(yt − y t )                                                κ>0

(3)      Rt = µ0 + (1 + µ1)πt + µ2(yt − y t ) + et

where yt represents output/consumption, y t is its “natural rate” flexible-price value, πt is

inflation, and Rt is the one-period (nominal) rate of interest.7 Here vt and et are

preference and policy shocks while y t is exogenously generated by an autocorrelated

technology process. Cochrane simplifies by assuming full price flexibility so that yt = y t

in each period, which eliminates the Calvo equation (2) and the output gap term in (3). If

we also let y t be a constant, then the IS relation (1) becomes

(4)      0 = b0 + b1(Rt − Etπt+1) + vt.

Then if the shock term is neglected and r = −b0/b1 is recognized as a constant real rate of

interest, we have the relation that Cochrane calls a “Fisher equation.” From the

perspective of monetary policy, I think it better to describe it as is done here, but there is




6
  This function represents a consumption Euler equation together with the economy’s overall resource
constraint.
7
  The rule is written assuming a zero target rate of inflation. If it were non-zero, then a non-zero constant
term would appear in the solution equation (7) below.


                                                       3
no substantive difference relative to Cochrane.8 Thus the system at hand reduces to (3)

and (4), identical to Cochrane’s (1) and (2) if we delete the shock vt from ours while also

setting µ0 = r in the policy rule, as a sensible central bank would do.

        To solve this simple system we combine (3) and (4) to obtain

(5)     0 = b0 + b1[µ0 + (1+µ1)πt + et − Etπt+1] + vt

and conjecture that with vt = 0 and et = ρet-1 + white noise ( ρ < 1) , as in Cochrane

(2007), there will be a solution of the form

(6)     πt = φ0 + φ1et

with expectations therefore obeying Etπt+1 = φ0 + φ1ρet. Substitution in (5) then implies

that this solution is

                       1
(7)     πt = 0 −             et .
                   1+ µ1 − ρ

        The latter is, of course, the “fundamentals” or “MSV” solution. With ρ < 1 , it

implies that πt is negatively related to et and that larger values of µ1 serve to reduce the

variability of πt around its target. Now suppose, however, that instead of (6) one looks

for a solution of the form

(8)     πt = φ0 + φ1et + φ2πt-1.

Then Etπt+1 = φ0 + φ1ρet + φ2(φ1et + φ2πt-1) and a second solution, in addition to (7), is

(9)     πt = (1/ρ)et + (1+µ1)πt-1.

Clearly, with µ1 > 0, as specified by the Taylor Principle, this expression (9) implies an


8
  My preference is to think of the “Fisher equation” as an identity, one which defines the one-period real
rate of interest as a nominal rate minus expected inflation. The item under discussion here is a model of
individual saving behavior together with a market-clearing condition that consumption equals output in the
aggregate plus the assumption that output is constant.


                                                    4
explosive process for the inflation rate.9 That is the type of solution referred to by

Cochrane, and it is clear that in the model at hand there is no transversality condition that

would rule out this explosive solution for the inflation rate. Further investigation,

pertaining to models in which the medium-of-exchange properties of money are

explicitly recognized, will appear below in Section 4. Pending that discussion, it appears

appropriate to accept, on a preliminary basis, the validity of Cochrane’s critique of

mainstream monetary policy analysis, with NK models and Taylor-style policy rules.

3. LS Learnability

         Several analysts have argued, however, that for any RE solution to be considered

plausible, and thereby relevant for policy analysis, it should be learnable.10 The basic

idea is simple and powerful. It is that in any RE model intended to represent behavior in

an actual market economy, the individual agents should be able to learn quantitative

details concerning the behavior of variables—which they must forecast for decision-

making purposes—from data generated by the economy itself.11 Cochrane (2007, p. 44)

briefly disputes that contention, as follows: “... a wide variety of almost philosophical

principles have been advocated to prune equilibria. For example, Evans and Honkapohja

(2001) advocate criteria based on least-squares (LS) learnability, and McCallum (2003)

advocates a ‘minimum state variable criterion,’ which he relates to learnability. These

refinements go beyond the standard definitions of economic equilibria. One may argue

that when a model gives multiple equilibria, we need additional selection criteria. I argue


9
  It appears from (9) that this solution will not be defined in the measure-zero case with ρ = 0. But in that
case one can add et-1 as an additional state variable in (8) and obtain an infinity of explosive solutions that
could be indexed by the start-up value of πt-1.
10
   These writers include Bullard (2006), Evans and Honkapohja (2001, p. 12-13), McCallum (2003, 2007),
and (arguably) Woodford (2003a, pp. 261-276; 2003b, p. 1183).
11
   For RE to obtain, the implied forecasting relationships must be quantitatively accurate.


                                                       5
instead that we need a different model.” This argument of Cochrane’s has two parts: (i) a

dismissal of learnability (justified only by his use of the word “philosophical”) and (ii)

advocacy of a “different model,” by which he evidently means the fiscal theory of the

price level. I will discuss the latter below, in Section 5. Here I wish to disagree with the

suggestion that LS learnability is a spurious principle.

       Let me begin by quoting myself: “The position that learnability (and thus E-

stability) should be regarded as a necessary condition for the relevance of a RE

equilibrium [sic?] begins with the presumption that individual agents must somehow

learn the magnitudes of parameters describing the economy’s law of motion from

observations generated by the economy; they cannot be endowed with such knowledge

by magic. Of course any particular learning scheme might be incorrect in its depiction of

actual learning behavior. But in this regard it is important to note that the LS learning

process in question assumes that (i) agents are collecting an ever-increasing number of

observations on all relevant variables while (ii) the structure is remaining unchanged.

Furthermore, (iii) the agents are estimating the relevant unknown parameters (iv) with an

appropriate estimator (v) in a properly specified model. Thus if a proposed RE solution

is not learnable by the process in question—the one to which the E&H results pertain—

then it would seem highly implausible that it could prevail in practice” (McCallum, 2007,

p. 1378). Cochrane suggests, to the contrary, that the notion that individual agents should

have some way of obtaining the information, necessary to form expectations in the

manner hypothesized in the analysis, is in some way “additional” to “standard definitions

of economic equilibria.” To me, this suggestion seems misplaced. In standard (i.e., RE)

analysis, for the determination of current endogenous variables we specify information




                                              6
sets that include quantitative features of the system plus, at a maximum, current and past

values of relevant variables.12 That is basically what LS learnability does for current

knowledge of the economy’s structure. Here, I am assuming that our analysis is based on

a model in which there are many individual agents interacting on markets, each agent

acting individually and without knowledge of other agents’ preferences and technologies

(therefore, their demand and supply functions), which can be different from his own—an

assumption that is not relaxed even if the analyst assumes (for simplicity) that all agents

are alike. I seriously doubt that John Cochrane, a dedicated (as well as skillful)

economist, would object to this position, even though it is perhaps “philosophical.” 13

         The point, of course, is that application of the LS learnability criterion does, in the

model at hand, eliminate the explosive solution of equation (9) above. That conclusion is

well known from the analyses of Bullard and Mitra (2002) and Honkapohja and Mitra

(2001), and is discussed on p. 1161 of McCallum (2003). The relevant analysis can be

summarized as follows. Consider linear models that can be written as

(10)     x t = AE t x t +1 + Cx t −1 + Dz t

where xt is a n×1 vector of endogenous variables, the system’s exogenous variables zt

being generated by a first-order autoregressive process

(11)     zt = Rzt-1 + εt

with εt white noise and R being a stable matrix. Here A and C are n×n, D is n×n1, and R

is n1×n1. Considering fundamental solutions of the form



12
   For some purposes perfect-foresight analysis is useful, of course, but one would not use that assumption
in an analysis that is concerned with (e.g.) the variability of asset prices or macroeconomic variables.
13
   My position could alternatively be expressed as admitting that learnability is not part of “standard
definitions” and then arguing that standard definitions are flawed in the context of models appropriate for
monetary policy analysis.


                                                     7
(12)     x t = Ωx t −1 + Γz t ,

standard undetermined-coefficient reasoning establishes that Ω must satisfy the quadratic

(13)    AΩ2 − Ω + C = 0.

There are many matrices that satisfy this quadratic; they result from different orderings of

the generalized eigenvalues of a matrix pencil involving parameter matrices A and C; see

McCallum (2007, p. 1380-2). If more than one of the Ωs that satisfies (13) has all its

eigenvalues less than 1 in modulus there are multiple stable solutions, i.e., indeterminacy.

        For such a system, Evans and Honkapohja (2001, p. 238) report that E-stability

(and thus LS learnability) obtains if and only if the following matrices have all

eigenvalues with real parts less than 1:

(13a)    F = (I − AΩ) −1 A

(13b) Ω '⊗ F

(13c)    R '⊗ F .

It will be noted that, if there are no lagged endogenous variables in the system, then C = 0

implying that Ω = 0 and F = A. Then the first two conditions amount to the requirement

that the eigenvalues of A all have real parts less than 1. In the basic system summarized

in (5) above, the fundamentals solution has Ω = 0 and A = 1/(1 + µ1). Thus it is clear that

the ES learnability requirements (13a-c) are satisfied. By contrast, the non-fundamentals

solution (9) yields πt = (1/ρ)et + (1+µ1)πt-1, implying that Ω = 1 + µ1 and thus that

F = (I − AΩ) −1 A = [1 − (1 + µ1 ) /(1 + µ1 )]−1 (1/(1 + µ1 )) = [1−1]-1 (1/(1 + µ1 )) = (1/(1 + µ1 )) /0.

Thus in this case F > 1 + µ1 (an understatement) and at least two of the three conditions

(13) are violated. Accordingly, the explosive solution is not learnable. Although it




                                                     8
satisfies the orthogonality conditions for a RE solution, it is implausible that an economic

system matching the specification would generate outcomes of the type described by (9).

       It should be added explicitly, perhaps, that the analysis of Evans and Honkapohja

(2001, pp. 138-9, 235, 238) includes results implying that, with the addition of a few

appropriate supplementary conditions, the LS learning process will converge to an

explosive path satisfying (9) with probability zero. In that sense, such paths cannot

represent plausible outcomes.

4. Medium of Exchange Money

       To this point there has been no explicit recognition of money, i.e., an asset that

serves as a medium of exchange and thus facilitates transactions in the economy under

discussion. It might be thought that such recognition could itself overturn Cochrane’s

basic argument since recognition of a monetary asset would give rise to a transversality

condition pertaining to real money holdings—and transversality conditions tend to rule

out certain explosive paths as equilibria because they violate conditions necessary for

individual optimality. Cochrane is fully aware of the relevant literature, however, which

rules out paths in which the discounted increment to utility provided by future real money

balances fails to approach zero as the horizon considered increases indefinitely. His

presumption, apparently, is that an explosive inflation rate would tend to reduce real

money holdings and, with the T−1 power of the discount factor approaching zero, thus to

induce the relevant product to approach zero. A basic analysis cited by Cochrane is that

of Obstfeld and Rogoff (1983), in which a model with medium-of-exchange money tends

to rule out paths along which the price level approaches zero but not paths along which

the price level explodes.




                                             9
        It is possible to object to the model used by Obstfeld and Rogoff on the grounds

that its money-in-the-utility-function specification does not represent money’s

transaction-facilitating properties as adequately as would a shopping-time or transaction-

cost model in which time or physical resources used up by transactions depend on both

real money holdings and real quantities of transactions. Additional analysis in that

direction has been conducted by Gray (1984). This paper does not consider every

specification that one might desire, but it goes some substantial way toward justifying the

conclusion of Obstfeld and Rogoff (1983, p. 686) that under a regime of pure fiat money

“… explosive price-level paths—speculative hyperinflations—can be ruled out only

when severe restrictions are placed on individual preferences.” 14

        Gray’s analysis includes model specifications in which the medium-of-exchange

role of money is represented by an explicit transaction-cost function. The main weakness

that I can see in her argument is that this function specifies that costs depend only upon

real money holdings; I would instead specify that the costs depend also on the quantity of

transactions conducted (per unit of time, of course). To represent that type of extension,

consider the following setup, which is based in part on McCallum (2001).15

        The economy consists of a large number of similar but independently-acting

households, each of which has an intertemporal utility function

(14)    u(ct) + βu(ct+1) + β2u(ct+2) + ....

where ct is consumption in period t. Each household supplies one unit of labor

inelastically each period and uses nt units of labor (the excess, if any, coming from the

14
   Obstfeld and Rogoff say “preferences” because their way of incorporating transaction costs is to include
real money balances in agents’ utility function. In Gray’s models both preference and transaction-cost
specifications are relevant.
15
   In McCallum (2001) there are a few “typos” including the following: In equations (26) and (27), ψ1(c,m)
should be ψ2(c,m).


                                                    10
labor market) to produce f(nt, k) units of output. Its budget constraint for period t is

(15)    f(nt, k) − txt – wt(nt−1) = ct + mt – (1+ πt)-1mt-1 + (1+ rt)-1bt+1 – bt + ψ(ct,mt).

Here txt is lump-sum taxes, wt is the real wage, mt is end-of-period real money balances,

1 + πt = Pt/Pt-1 where Pt is the price level in t, bt+1 is real private bonds purchased in t, and

rt is the real rate of interest on these bonds. Finally, transaction costs are given by

ψ(ct, mt), where ψ1 > 0, ψ11 < 0, ψ2 < 0, and ψ22 > 0 Both f and ψ have the Inada

properties, as does u.

        In this setting, a household’s first-order optimality conditions include

(16a)   u '(c t ) − λt[1 + ψ1(ct, mt)] = 0

(16b) −λtwt + λtf1(nt, k) = 0

(16c) −λt[1 + ψ2(ct, mt)] + βEtλt+1(1 + πt+1)-1 = 0

(16d) −λt(1 + rt)-1 + βEtλt+1 = 0

where λt is the Lagrangian multiplier attached to (15). There is a transversality condition

(TC) for bonds,

(17)     lim b T +1βT −1λ T = 0
        T →∞



and another for money:

(18)     lim m TβT −1λ T = 0 .
        T →∞



The latter two will be taken as necessary for individual-household optimality.16

        For general equilibrium, we must also have

(19)    nt = 1


16
  The TC (18) can be thought of as arising, from a T-period version of the optimization problem, as a
limiting version as T →∞ of the second part of the Kuhn-Tucker condition ∂L/∂mT ≤ 0 plus mT∂L/∂mT = 0,
since ∂L/∂mT = −βT-1λT. (Here L refers to the problem’s Lagrangian expression.)


                                                 11
(20)     bt+1 = 0

(21)     mt = Mt/Pt

and the government budget constraint17

(22)     txt + mt – (1+ πt)-1mt-1 = 0.

In the latter we take government consumption to equal zero, for simplicity. Thus money

is injected via lump-sum transfers (negative taxes). With the government exogenously

setting values of Mt, equilibrium paths for the nine endogenous variables ct, mt, λt, nt, wt,

Pt, πt, rt, and bt+1 are given by equations (15), (16a), (16b), (16c), (16d), (19), (20), (21),

and the definition 1 + πt = Pt+1/Pt .

         In this economy, the demand for money relation is, from equations (16c) and

(16d),

                                    1
(23)     1 + ψ 2 (c t , m t ) =
                                  1+ R t

where the nominal interest rate Rt satisfies the Fisher identity

(24)     1 + R t = (1 + rt )(1 + π t +1 ) .

(I ask the reader to please include expectation operators where necessary.) With flexible

prices, rt will be a constant and so as πt+1 explodes, 1 + ψ 2 (c t , m t ) will approach zero.

That is, − ψ 2 (c t , m t ) , the marginal benefit from holding money, will approach 1 from

below as mt/ct declines with the increasing cost of holding money.

         To get a feel for the situation, let’s consider the specific transaction-cost function

used in McCallum (2001), viz.,



17
  By excluding bonds from (22) I am implicitly assuming that bonds are private loans from one household
to another.


                                                  12
(25)     ψ (c, m) = a1c(c / m)a 2 ,                                      a1 > 0, a2 > 0

in which the average transaction cost has a constant-elasticity relationship to c/m. Note

that this function is one in which the average transaction cost grows without limit as real

money balances approach zero. But as inflation explodes, 1 + ψ 2 (c t , m t ) =

1 − a 2 a1 (c / m)1+ a 2 approaches zero and the limiting condition has c / m = (1/ a 2 a1 )1/(1+ a 2 ) so

m does not approach zero; it approaches a limiting value (say) m* > 0. The calibration

adopted in McCallum (2001) has a1 = 0.00102 and a2 = 4, in which case c/m* equals

3.96. This calibration is designed to pertain to the quarterly frequency, suggesting that

real money holdings in this situation are about 1/4 of quarterly spending. In the recent

U.S. data, by contrast, the M1 value of c/m is of the order of magnitude of 1, about four

times this limiting value.

        The relevance of all this for the issue at hand is that the transversality condition

(TC) for money holdings does not fail in the model at hand as inflation explodes. The

value of λt in our model is, from (16a),

                    u '(c t )                  u '(c t )
(26)     λt =                      =                                 ,
                1 + ψ1 (c t , m t ) 1 + (1 + a 2 )a1 (c t / m t )a 2

But from our calculations in above, we see that the latter does not grow without bound as

inflation explodes; the right-hand side of (26) approaches a positive limit. Then with λT

remaining finite, violation of the TC (18) will not occur, since both mT and λT remain

finite while βT-1 approaches zero as the horizon T extends indefinitely.

        A heuristic but more general argument can be made without reference to any

specific transaction technology, as follows. Transversality conditions for money holdings

are in general limiting values (as T → ∞) of the product of three terms: βT-1, mT, and λT,


                                                           13
where the latter is a Lagrange multiplier on the budget constraint for period T. If this

limit is zero, the TC is satisfied; if it is not for some path, that path is not optimal for the

agent. Now clearly βT-1 approaches zero as T grows, so for the TC to fail, either mT or λT

must explode. Exploding inflation tends, however, to drive the former toward small

values. So the only way that the TC can fail is for λT → ∞ as T → ∞.

        But since λT is a multiplier on the budget constraint, with each term in the latter

expressed in real terms, λT represents the increase in utility made possible by a one-unit

decrease in period-T real money holdings, at the margin. This magnitude will tend to

increase as money holdings decrease, but it cannot plausibly increase without bound.

Marginal utility of consumption may → ∞ as ct → 0, but a model specification that drives

consumption to zero (as real money holdings decrease) implies that a barter economy

would necessarily feature zero consumption. That seems to be an inadmissible

assumption, however, even to an enthusiast for the MOE role of money. This, I believe,

is what lies behind Cochrane’s assumption that exploding inflation does not imply the

failure of any TC.

5. Fiscal Theory of the Price Level

        Cochrane, to his credit, does not end his paper with a conclusion that “anything

can happen.” Instead, he invokes the “fiscal theory of the price level,” henceforth FTPL.

Specifically, he says “... I conclude that the fiscal theory is the only currently-available

economic model that can do so,” that is, “can determine the price level or inflation rate in

a fiat-money economy with the sort of interest rate targets that we observe central banks

to follow” (2007, p. 44). To do this he must, of course, introduce government bonds.

That necessity is somewhat unfortunate for several reasons, the most important being that



                                               14
one should be able to construct a theory of what happens with a Taylor rule in an

economy in which there is no government debt other than money. But let us nevertheless

take his suggestion and include government bonds. To be distinguishable from money

there must be some transaction-facilitating services provided by the latter but not the

former (or more such services provided by the latter). Probably Cochrane would not

object to that idea.18

         Continuing, then, let us consider the equilibrium that obtains in an economy in

which the central bank follows a Taylor rule that satisfies the Taylor Principle—the case

with which we are here concerned—while the fiscal authority is setting government

spending exogenously and setting (lump sum) taxes in response to the quantity of

outstanding bonds according to a rule of the type discussed by Leeper (1991), Sims

(1994), Woodford (1995), and Cochrane (2005). The fiscal authority’s rule may be

“active” or “passive,” in Leeper’s terminology. In the latter case the rule is a sensible

one, i.e., is one that collects enough (lump sum) taxes to decrease the (per capita) debt

[assuming no real growth in output] by some positive fraction from one period to the

next. An “active” policy, by contrast, either taxes so aggresively that debt is reduced to

zero in the “first” period or else so weakly that the debt grows continually from period to

period.19 Neither of these extremes is, I would contend, relevant to actual economies of

decently governed nations, not even the profligate USA of today.20

         What happens in such a situation, with an active monetary policy and passive

18
   He has, however, discussed the FTPL in the context of an economy with no money. I have argued
elsewhere that this approach negates the whole raison d’etre of the FTPL, which is to provide a logical
mechanism of price level determination that contrasts with the monetary approach, a contrast that is ruled
out by assumption if the model includes no money. On this topic, see McCallum and Nelson (2005).
19
   It should be noted that Leeper’s terminology terms monetary policy as “active” when it is sensible, and
fiscal policy as “active” when it is extremely expansionary or extremely contractionary. Thus it is not the
case that there is a symmetry between AM-PF and PM-AF, AM meaning active money, etc.
20
   But they will, nevertheless, be considered momentarily.


                                                     15
fiscal policy? Evans and Honkapohja (2007) have examined that case, with results

summarized in McCallum and Nelson (2005). In the relevant region, with sensible

monetary and fiscal policy, there is a single stable solution and it is learnable. It features

exactly the equilibrium assumed in standard analysis of the NK type.

           If, however, the fiscal authority adopts (for some perverse reason) an active—i.e.,

extreme and non-sensible—rule, then there are two cases. First, with overly aggressive

tax collections there is no learnable equilibrium. The analysis predicts that this

combination of policies would lead to disorderly behavior that rational analysis is poorly

equipped to explain. Secondly, with sensible monetary policy and insufficiently strong

tax policy, there is no stable solution. (There is a small region in which bonds explode

but money and prices are stationary in a learnable equilibrium, and also a still smaller

region in which money, bonds, and prices all explode in a learnable equilibrium21). In

both of these cases, as McCallum and Nelson (2005) explain, the outcome is exactly what

traditional monetarist analysis would predict. Thus the FTPL is not, I contend, helpful in

explaining price level behavior in a fiat money economy.

5. Conclusions

           It is clearly important that the logical foundations of the dynamic models used in

current mainstream monetary policy analysis be clearly understood. Accordingly,

Cochrane’s recent paper makes a substantial contribution by pointing out that

determinacy, in the sense of a unique solution that is dynamically stable, does not provide

an adequate foundation for analyses of the standard type. It is the case, however, that a

more satisfactory criterion, that of least-squares learnability, is available. Basic

informational requirements suggest that learnability should be considered necessary for
21
     See Evans and Honkapohja (2007, pp. 681-682).


                                                     16
the plausibility of any rational expectations equilibrium,22 and acceptance of this view

rules out explosive solutions of the type discussed by Cochrane.23 A useful by-product of

such acceptance is that “indeterminacy” ceases to be a problem in other circumstances,

such as strenuous inflation-forecast targeting, not discussed by Cochrane (2007).24




22
   Indeed, probably, for the definition of an equilibrium.
23
   In this paper I have focused on Cochrane’s argument and accordingly have not considered more esoteric
classes of sunspot solutions.
24
   This position is argued by McCallum (2003). Woodford’s (2003b) comment stresses objections to
McCallum’s “minimal state variable” solution concept, but features very little disagreement with his
paper’s substantive positions (which do not rely upon that concept).


                                                   17
                                      References



Bullard, J.B. (2006) “The Learnability Criterion and Monetary Policy.” Federal Reserve

       Bank of St. Louis Review 88 (3): 203-217.

Bullard, J.B., and K. Mitra (2002) “Learning about Monetary Policy Rules,” Journal of

       Monetary Economics 49, 1105-1129.

Clarida, R., J. Gali, and M. Gertler (1999) “The Science of Monetary Policy: A New-

       Keynesian Perspective,” Journal of Economic Literature 37, 1661-1707.

Cochrane, J. H. (2005) “Money as Stock,” Journal of Monetary Economics 52, 501-528.

____________ (2007) “Inflation Determination with Taylor Rules: A Critical Review.”

       NBER Working Paper 13409.

Evans, G.W., and S. Honkapohja (2001) Learning and Expectations in Macroeconomics.

       Princeton University Press.

__________________________ (2007) “Policy Interaction, Learning, and the Fiscal

       Theory of Prices,” Macroeconomic Dynamics 11, 665-690.

Goodfriend, M. (2006) “How the World Achieved Consensus on Monetary Policy.”

       Journal of Economic Perspectives 21(4), 47-68.

Goodfriend, M., and R.G. King (1997) “The New Neoclassical Synthesis and the Role of

       Monetary Policy,” NBER Macroeconomics Annual 12, 231-283.

Gray, J.A. (1984) “Dynamic Instability in Rational Expectations Models: An Attempt to

       Clarify,” International Economic Review 25, 93-122.

King, R.G. (2000) “The New IS-LM Model: Language, Logic, and Limits,” Federal

       Reserve Bank of Richmond Economic Quarterly 86(3), 45-103.




                                          18
King, R.G., and A.L. Wolman (1996) “Inflation Targeting in a St. Louis Model of the

       21st Century,” Federal Reserve Bank of St. Louis Review 78(3), 83-107.

Leeper, E.M. (1991) “Equilibria under ‘Active’ and ‘Passive’ Monetary and Fiscal

       Policies,” Journal of Monetary Economics 27, 129-147.

McCallum, B.T. (2001) “Monetary Policy Analysis in Models without Money,” Federal

       Reserve Bank of St. Louis Review 83(4), 145-160.

______________ (2003) “Multiple-Solution Indeterminacies in Monetary Policy

       Analysis,” Journal of Monetary Economics 50, 1153-1175.

______________ (2007) “E-Stability vis-a-vis Determinacy Results for a Broad Class of

       Linear Rational Expectations Models,” Journal of Economic Dynamics and

       Control 31, 1376-1391.

McCallum, B.T., and E. Nelson (2005) “Monetary and Fiscal Theories of the Price Level:

       The Irreconcilable Differences,” Oxford Review of Economic Policy 21, 565-583.

Obstfeld, M., and K.. Rogoff (1983) “Speculative Hyperinflations in Maximizing

       Models: Can We Rule Them Out?” Journal of Political Economy 91, 675-87.

Rotemberg, J.J., and M. Woodford (1997) “An Optimization-Based Econometric Frame-

       work for the Evaluation of Monetary Policy,” NBER Macroeconomics Annual 12,

       297-346.

Sims, C.A. (1994) “A Simple Model for Study of the Determination of the Price Level

       and the Interaction of Monetary and Fiscal Policy,” Economic Theory 4, 381-399.

Svensson, L.E.O., and M. Woodford (2005) “Implementing Optimal Policy Through

       Inflation-Forecast Targeting,” in B.S. Bernanke and M. Woodford, eds., Inflation

       Targeting. University of Chicago Press.




                                          19
Taylor, John B. (1993) “Discretion versus Policy Rules in Practice,” Carnegie-

       Rochester Conference Series on Public Policy 39, 195-214.

____________ (1999) “The Robustness and Efficiency of Monetary Policy Rules as

       Guidelines for Interest Rate Setting by the European Central Bank,” Journal of

       Monetary Economics 43, 655-679.

Woodford, M. (1995) “Price Level Determinacy without Control of a Monetary

       Aggregate,” Carnegie-Rochester Conference Series on Public Policy 43-1-46.

____________ (2003a) Interest and Prices: Foundations for a Theory of Monetary

       Policy. Princeton University Press.

____________ (2003b) “Comment,” Journal of Monetary Economics 50, 1177-1188.




                                             20
