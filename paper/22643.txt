                                 NBER WORKING PAPER SERIES




        CREDIBLE ECOLOGICAL INFERENCE FOR PERSONALIZED MEDICINE:
                     FORMALIZING CLINICAL JUDGMENT

                                          Charles F. Manski

                                         Working Paper 22643
                                 http://www.nber.org/papers/w22643


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                    September 2016




I have benefitted from the opportunity to present this work in a seminar at the Federal Reserve Bank
of Cleveland and from the comments of Pamela Giustinelli and Max Tabord-Meehan. The views expressed
herein are those of the author and do not necessarily reflect the views of the National Bureau of Economic
Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2016 by Charles F. Manski. All rights reserved. Short sections of text, not to exceed two paragraphs,
may be quoted without explicit permission provided that full credit, including © notice, is given to
the source.
Credible Ecological Inference for Personalized Medicine: Formalizing Clinical Judgment
Charles F. Manski
NBER Working Paper No. 22643
September 2016
JEL No. C18,I10

                                           ABSTRACT

This paper studies the ecological inference problem that arises when clinicians seek to
personalize patient care by making health risk assessments conditional on observed patient
attributes. Let y be a patient outcome of interest and let (x = k, w = j) be patient attributes that a
clinician observes. The clinician may want to choose a care option that maximizes the patient's
expected utility conditional on the observed attributes. To accomplish this, the clinician needs to
know the conditional probability distribution P(y|x = k, w = j). In practice, it is common to have
a trustworthy evidence-based risk assessment that predicts y conditional on a subset of the
observed attributes, say x, but not conditional on (x, w). Then the clinician knows P(y|x = k) but
not P(y|x = k, w = j). Partial conclusions about P(y|x = k, w = j) may be drawn if the clinician
also knows P(w = j|x = k). Tighter conclusions may be possible if he combines knowledge of
P(y|x) and P(w|x) with credible structural assumptions embodying some a priori knowledge of
P(y|x, w). This is the ecological inference problem studied here. A substantial psychological
literature comparing actuarial predictions and informal clinical judgments has concluded that
clinicians should not attempt to subjectively predict patient outcomes conditional on attributes
such as w that are not utilized in evidence-based risk assessments. The analysis in this paper
suggests that formalizing clinical judgment through analysis of the inferential problem may
enable clinicians to make more informative personalized risk assessments.


Charles F. Manski
Department of Economics
Northwestern University
2001 Sheridan Road
Evanston, IL 60208
and NBER
cfmanski@northwestern.edu
1. Introduction



        Suppose that each member of a population is characterized by a triple (y, x, w) whose value lies in

a set Y × X × W. Let P(y, x, w) denote the population distribution of these variables. A well-known

inferential problem is identification of the long conditional distribution P(y*x, w) given knowledge of the

short conditional distributions P(y*x) and P(w*x). Aspects of the problem have been studied in several

literatures with varying substantive concerns and terminology, including those on ecological inference,

contaminated sampling, and Simpson's paradox. Manski (2007, Chapter 5) gives a textbook exposition. The

term ecological inference is not particularly evocative, but it is prominent and I use it here.

        This paper studies the ecological inference problem that arises when clinicians seek to personalize

patient care by making health risk assessments or predictions of treatment response conditional on observed

patient attributes. Let y be a patient outcome of interest. In risk assessments, y commonly is a binary variable

indicating whether the patient will develop a specified disease or a real variable denoting remaining life span.

When predicting treatment response, y is an outcome assuming patient receipt of a potential treatment. Let

(x = k, w = j) be patient attributes that a clinician observes and wants to condition on when choosing a care

option. These attributes may include demographic traits, documentation of medical history, and the results

of screening and diagnostic tests. Suppose that the clinician wants to choose a care option that maximizes

the patient's expected utility conditional on the observed attributes. To accomplish this, the clinician wants

to know P(y|x = k, w = j).

        In practice, it is common to have a trustworthy evidence-based risk assessment that predicts y

conditional on a subset of the observed patient attributes, say x, but not conditional on (x, w). Thus, the

clinician may know P(y|x = k) but not P(y|x = k, w = j). For example, readily available life tables enable

prediction of remaining life span conditional on basic demographic traits (age, sex, race) but not conditional

on attributes characterizing a patient's health status. Available tools assessing the risk of developing specific

diseases condition their predictions on some patient attributes that clinicians observe but not on others.
                                                       2

        The Law of Total Probability gives the algebraic relationship between the short and long predictive

distributions for the patient under care, namely



(1)   P(y*x = k) = P(w = j*x = k)P(y*x = k, w = j) + P(w         j*x = k)P(y*x = k, w     j).



Equation (1) shows that knowledge of P(y|x = k) per se reveals nothing about P(y*x = k, w = j). Any

distribution P(y*x = k, w = j) satisfies the equation if P(w = j*x = k) = 0. On the other hand, partial

conclusions about P(y*x = k, w = j) may be drawn if one knows both P(y|x = k) and P(w = j|x = k), provided

that the latter is positive. Tighter conclusions may be possible if one combines knowledge of P(y|x) and

P(w|x) with credible structural assumptions; that is, assumptions embodying some a priori knowledge of

P(y|x, w). This is the ecological inference problem studied here.

        The analysis in the paper applies both to risk assessment and to prediction of treatment response. In

both cases, the objective is to predict patient outcomes conditional on observed attributes. Whereas risk

assessments explicitly or implicitly assume clinical use of some conventional patient care strategy, predictions

of treatment response forecast the outcomes that would occur if patients were to receive alternative forms of

care. For simplicity, I focus on risk assessment.

        Section 2 provides background. I explain the term personalized medicine and give a prominent

illustration of risk assessment that conditions on a subset of the patient attributes that a clinician typically

observes, this being prediction of risk of breast cancer. I discuss the psychological literature comparing

actuarial predictions and informal clinical judgments. This literature has concluded that clinicians should not

attempt to subjectively predict patient outcomes conditional on attributes not utilized in evidence-based risk

assessments. I suggest that formalizing clinical judgment through analysis of the inferential problem may

enable clinicians to make more informative personalized risk assessments.

        Section 3 presents the identification analysis, bringing to bear aspects of my previous research. I first

summarize findings that hold when one has no information beyond knowledge of P(y|x) and P(w|x).
                                                      3

Depending on the values of P(y|x) and P(w|x), the identification region for P(y|x = k, w = j) may be

sufficiently small for a clinician to make useful outcome predictions conditional on (x, w), but it may be so

large that predictions conditional on these attributes are uninformative.

        Tighter predictions become possible if one imposes structural assumptions. Structural assumptions

express judgment in a formal manner that contrasts with informal clinical judgment. Point identification of

P(y|x, w) can be achieved if sufficiently strong structural assumptions are imposed. Research on ecological

inference has considered various such assumptions. A central issue is how to resolve the tension between the

strength and credibility of maintained assumptions. Strong structural assumptions may have substantial

identifying power but little credibility. I show how to predict outcomes using weaker bounded-variation

assumptions, defined in Section 3.3, that have less power but greater credibility.

        Section 4 considers the use of risk assessment in medical decision making when the structural

assumptions (if any) that a clinician deems credible are not strong enough to point identify P(y|x = k, w = j).

In some situations the clinician may be able to make a decision that is sure to be optimal for patients with

attributes (k, j), even though knowledge of P(y|x = k, w = j) is incomplete. In other situations the available

information is too limited to enable optimization. Then patient care is a problem of decision making under

ambiguity, in which various decisions may be reasonable. I explain the general problem of patient care under

ambiguity and study it in a relatively simple setting where the clinician chooses between active surveillance

of a patient and prophylactic treatment. This part of the paper adds new findings to my program of work on

medical decision making under ambiguity.

        Although this paper reports some new findings on identification and decision making under

ambiguity in Sections 3 and 4, the primary contribution of the paper is not to develop new methodology. It

is rather to show how simple applications and extensions of existing methodology may potentially be used

to improve patient care, a substantive matter of considerable importance. Although research on medical

decision making has long distinguished between actuarial prediction and informal clinical judgment, to the

best of my knowledge it has not until now formally studied personalized risk assessment as an identification
                                                       4

problem.

        Readers who are not specifically concerned with the practice of personalized medicine may

nonetheless find aspects of the paper of interest. Many decision makers in realms other than health care face

identification problems similar to that studied here as they attempt to make personalized risk assessments.




2. Background



2.1. Personalized Medicine



        The term personalized medicine is sometimes defined to mean health care that is literally specific to

the individual, as in this definition by Ginsburg and Willard (2009, p. 278), which was adopted by American

Medical Association (2010): "Personalized medicine is . . . . . health care that is informed by each person’s

unique clinical, genetic, genomic, and environmental information." However, evidence to support complete

personalization of care is rarely available. Hence, the term is commonly used to mean care that varies with

some individual characteristics. President's Council of Advisors on Science and Technology (2008, p. 7) puts

it this way:

        " 'Personalized medicine' refers to the tailoring of medical treatment to the specific characteristics of
        each patient. In an operational sense, however, personalized medicine does not literally mean the
        creation of drugs or medical devices that are unique to a patient. Rather, it involves the ability to
        classify individuals into subpopulations that are uniquely or disproportionately susceptible to a
        particular disease or responsive to a specific treatment."

Similarly, Academy of Medical Sciences (2015, p. 4) states "The terms 'stratified', 'personalised' or 'precision'

medicine all refer to the grouping of patients based on risk of disease, or response to therapy, using diagnostic

tests or techniques."

        Thus, personalized medicine is a matter of degree rather than an all-or-nothing proposition.
                                                       5

Clinicians classify patients into groups based on information about medical history and findings obtained

through screening and diagnostic tests. Clinical practice guidelines (CPGs) recommend classifications that

aim to be well-grounded in evidence on risk of disease and treatment response. The classification rules used

depend on the evidence on group outcomes that is available.

        Normative studies of personalized medicine have commonly assumed that the clinician has evidence

enabling accurate probabilistic risk assessments and predictions of treatment response conditional on observed

patient attributes. For example, Phelps and Mushlin (1988) studied optimal diagnostic testing as a prelude

to treatment. They assumed that the clinician knows the actual probability distributions of test results and

of patient outcomes under alternative treatments, conditional on observed patient attributes. They assumed

that the objective is to maximize the patient's expected utility. In this context, the usefulness of performing

tests or making other efforts to learn patient attributes is expressed by the expected value of information,

defined succinctly by Meltzer (2001) as (p. 119) "the change in expected utility with the collection of

information."

        In practice, clinicians commonly observe patient attributes other than those used as predictors in

evidence-based risk assessments and studies of treatment response. They often use informal clinical judgment

to predict how patient outcomes vary with these attributes. There has long been concern that exercise of

clinical judgment may reduce rather than improve the quality of medical decision making (e.g., Dawes, Faust,

and Meehl, 1989).

        Section 2.2 gives a prominent illustration of evidence-based risk assessment that utilizes only a subset

of the patient attributes that clinicians typically observe. I next discuss the longstanding concern with use

of informal clinical judgment to predict patient outcomes (Section 2.3). I then introduce the analysis

undertaken here (Section 2.4).
                                                        6

2.2. Illustration: Predicting Risk of Breast Cancer



        An apt illustration of how available evidence affects risk assessment is the Breast Cancer Risk

Assessment (BCRA) Tool of the National Cancer Institute (2016). The risk assessment yielded by this tool

has become widely used in clinical practice (see Susan G. Komen, 2016) and is an important input to the CPG

for breast cancer screening issued by the National Comprehensive Cancer Network (2016).

        The BCRA Tool gives a predicted probability that a woman will develop invasive breast cancer

conditional on eight personal attributes: (1) history of breast cancer or chest radiation therapy for Hodgkin

Lymphoma (yes/no); (2) presence of a BRCA mutation or diagnosis of a genetic syndrome associated with

risk of breast cancer (yes/no/unknown); (3) current age, in years; (4) age of first menstrual period (7-11, 12-

13, $ 14, unknown); (5) age of first live birth of a child (no births, < 20, 20-24, 25-29, $30, unknown); (6)

number of first-degree female relatives with breast cancer (0, 1, >1, unknown); (7) number of breast biopsies

(0, 1, > 1, unknown); and (8) race/ethnicity (White, African American, Hispanic, Asian American, American

Indian or Alaskan Native, unknown).

        In terms of the notation introduced in Section 1, y denotes whether a patient will develop invasive

breast cancer, x are the attributes that the BCRA tool uses to predict y, and w are additional patient attributes

that the clinician observes but that are not used by the tool. The BRCA tool yields an evidence-based

estimate of the probability P(y = 1|x) that a woman with attributes x will develop invasive cancer. The reason

that the tool assesses risk conditional on eight attributes and not others is that the tool uses a modified version

of the "Gail Model," based on the empirical research of Gail et al. (1989). The Gail et al. article reported

predicted probabilities of breast cancer for white women who have annual breast examinations, conditional

on attributes (1) through (7). Scientists at the National Cancer Institute later modified the model to predict

invasive cancer within a wider population of women.

        The BCRA Tool personalizes predicted risk of breast cancer in multiple respects, but it does not

condition on further personal attributes that a clinician can observe and that may be associated with risk of
                                                     7

cancer. For example, when considering the number of first-degree relatives with breast cancer (item 6), the

BCRA Tool does not take into account the number and ages of a woman's first-degree relatives, which

logically should matter when interpreting the response to the item. Nor does it condition on the prevalence

of breast cancer among second-degree relatives, a consideration that figures prominently in another risk

assessment model due to Claus, Risch, and Thompson (1994). When considering race/ethnicity (item 8), the

BCRA Tool groups all white woman together and does not distinguish ethnic subgroups such as Ashkenazi

Jews, who are thought to have considerably higher risk of a BRCA mutation than other white subgroups, a

potentially important matter when the answer to item (2) is "unknown." Moreover, the BCRA Tool does not

condition on behavioral attributes such as excessive drinking of alcohol, which has been associated with

substantial increased risk of breast cancer (Singletary and Gapstur, 2001).



2.3. Evidence-Based Care and Informal Clinical Judgment



        The BCRA Tool exemplifies a common question in patient care. Evidence from medical research

enables one to assess risk of disease conditional on certain patient attributes. A clinician observes these

attributes and also observes additional attributes that may be informative predictors of future disease.

However, the available evidence does not show how patient outcomes vary with these additional attributes.

How should the clinician assess risk?

        A similar question arises when choosing treatments. Research articles reporting on clinical trials or

epidemiological studies provide evidence on treatment response for groups of patients who share certain

attributes. Clinicians commonly observe not only these attributes but also others that may predict treatment

response. In the absence of evidence of how treatment response varies with these additional attributes, how

should clinicians make treatment decisions?

        One option is to ignore the additional attributes w and base care only on the attributes x used in

available evidence-based assessment tools or reports on treatment response. Thus, a clinician recommending
                                                        8

a breast cancer screening plan to a woman might compute her BCRA risk assessment, disregarding attributes

that he observes but that the BCRA Tool does not utilize. Another option is to make predictions conditional

on (x, w). Such predictions are typically made informally, by a process called clinical judgment.

        A substantial body of empirical psychological research comparing evidence-based statistical

predictions with ones made by clinical judgment has concluded that the former consistently outperforms the

latter when the predictions are made using the same patient attributes. Moreover, the gap in performance

appears to persist even when clinical judgment uses additional attributes as predictors. This research began

in mid-twentieth century, early contributions including Sarbin (1943, 1944) and Meehl (1954). To describe

the conclusions of the literature, I rely mainly on the influential review article of Dawes, Faust, and Meehl

(1989). See also Camerer and Johnson (1997).

        Dawes et al. distinguish actuarial prediction and clinical judgment as follows (p. 1668):

        "In the clinical method the decision-maker combines or processes information in her or her head.
        In the actuarial or statistical method the human judge is eliminated and conclusions rest solely on
        empirically established relations between data and the condition or event of interest."

Comparing the two in circumstances where a clinician observes patient attributes that are not utilized in

available actuarial prediction, they state (p. 1670):

        "Might the clinician attain superiority if given an informational edge? For example, suppose the
        clinician lacks an actuarial formula for interpreting certain interview results and must choose between
        an impression based on both interview and test scores and a contrary actuarial interpretation based
        on only the test scores. The research addressing this question has yielded consistent results . . . .
        Even when given an information edge, the clinical judge still fails to surpass the actuarial method;
        in fact, access to additional information often does nothing to close the gap between the two
        methods."
        Seeking to explain this empirical finding, they discuss an example in which the additional observed

attribute is that a patient has a broken leg and then write (p. 1670-1671):

        "The broken leg possibility is easily studied by providing clinicians with both the available data and
        the actuarial conclusion and allowing them to use or countervail the latter at their discretion. The
                                                        9

        limited research examining this possibility, however, all shows that greater overall accuracy is
        achieved when clinicians rely uniformly on actuarial conclusions and avoid discretionary judgments
        . . . . When operating freely, clinicians apparently identify too many 'exceptions,' that is, the actuarial
        conclusions correctly modified are outnumbered by those incorrectly modified. If clinicians were
        more conservative in overriding actuarial conclusions they might gain an advantage, but this
        conjecture remains to be studied adequately."

Here and elsewhere, Dawes, Faust, and Meehl caution against use of clinical judgment to informally predict

disease risk or treatment response conditional on patient attributes that are not utilized in evidence-based

assessment tools or research reports. They attribute the weak performance of informal clinical judgment to

clinician failure to adequately grasp the logic of the prediction problem and to their use of decision rules that

place too much emphasis on exceptions such as broken legs.



2.4. Prediction Formalizing Clinical Judgment



        Suppose that Dawes, Faust, and Meehl are correct to advise against use of informal clinical judgment

to predict patient outcomes. This does not foreclose the possibility of making well-grounded predictions that

combine evidence with judgment. The authors suggest this when they conjecture (p. 1671) that clinicians

might gain an advantage if they were more conservative in overriding actuarial conclusions. They also

conjecture (p. 1670) that theory-mediated judgments may potentially be superior to conclusions reached

solely on the basis of empirical frequencies. However, they mention these ideas only briefly and do not

propose specific approaches. Other authors have offered broad qualitative suggestions for integration of

actuarial prediction and clinical judgment (e.g., Shlonsky and Wagner, 2005). Yet, as far as I am aware,

psychologists have not studied the matter formally as a mathematical problem of conditional prediction.

        This paper studies prediction of patient outcomes that coherently combines evidence and judgment.

As shown in Section 1, knowledge of P(y|x) by itself reveals nothing about P(y|x = k, w = j). To draw

conclusions about the long predictive distribution requires further information. Throughout the paper I
                                                      10

suppose that the available other information includes at least knowledge of P(w|x), the w-composition of the

group of patients with attribute value x. One may also pose structural assumptions that restrict the form of

P(y|x, w).

        Evidence on group composition is often available in practice, so analysis of its implications should

be broadly useful to patient care. For example, in the breast cancer context, w might measure the number of

first-degree relatives that a woman has or whether she is an excessive drinker of alcohol. In these cases,

knowing P(w|x) means that one knows the distribution of number of first-degree relatives or the prevalence

of excessive drinking among women with attributes x. The availability of credible structural assumptions

varies across settings. Illustrations are given in Section 3.




3. Risk Assessment Using Evidence and Structural Assumptions



        I analyze personalized risk assessment in stages. Section 3.1 considers prediction of patient outcomes

using evidence on group evidence and group composition, but without structural assumptions. Section 3.2

considers two types of strong structural assumptions, using instrumental variables and parametric models,

that can point-identify the long predictive distribution. Section 3.3 studies weaker but potentially more

credible bounded-variation assumptions.



3.1. Assessment with Evidence on Group Outcomes and Group Composition



        As previously stated, let each member of a population of patients be characterized by a triple (y, x,

w) and let P(y, x, w) denote the distribution of (y, x, w). Here y measures a patient outcome of interest,

taking values in an outcome space Y. Patient attributes x, which take values in a finite space X, are used to

predict y in an available evidence-based risk assessment tool. Attributes w, which take values in a finite set
                                                        11

W, are not used in the risk assessment tool.

        I assume that the evidence-based assessment tool is accurate, in the sense that it correctly reveals the

short predictive distribution P(y|x). This assumption simplifies analysis and clinicians often maintain it in

practice. Nevertheless, one should keep in mind that actual assessment tools may not be fully accurate. For

example, the Gail Model underlying the BCRA Tool maintains various structural assumptions and was

estimated using particular (outcome, attribute) data. The predictions made by the BCRA Tool may be suspect

if the assumptions of the Gail model were not realistic, if the data used to estimate the model suffered from

measurement problems, or if the predictive distribution that prevailed when the model was estimated does

not accurately describe the risk of breast cancer today. The parameter estimates of the Gail model are also

subject to ordinary finite-sample imprecision.

        Consider a patient with attributes (x = k, w = j). Equation (1) gave the algebraic relationship between

the short and long predictive distributions for this patient. Abstractly, the joint identification region for P(y|x

= k, w = j) and P(y*x = k, w    j) given knowledge of P(y|x = k) and P(w = j|x = k) is the set of pairs of long

outcome distributions that satisfy (1). The equation holds if both long distributions equal the short

distribution P(y|x = k). This is the only feasible pair of equal long distributions. All other feasible pairs have

P(y|x = k, w = j)   P(y*x = k, w     j).

        Social scientists have used the term ecological inference to describe the problem of inference on

P(y*x, w) given knowledge of P(y*x) and P(w*x). Important early contributions include Robinson (1950),

Duncan and Davis (1953), and Goodman (1953). A prominent instance arises in analysis of the geographic

and demographic variation in voting across the population. Surveys yielding information on individual

attributes and voting behavior may not be available and, when they are, the credibility of self-reports of

voting behavior may be suspect. Hence, social scientists have often sought to infer voting patterns from two

data sources that are readily available and credible: (a) administrative records on voting by electoral district

and (b) census data on the attributes of persons in each district.

        Formally, let y denote the voting outcome of interest, let x denote an electoral district, and let w
                                                      12

denote personal attributes thought to be associated with voting behavior. The objective is to learn P(y*x, w),

the distribution of voting outcomes among persons in district x with attributes w. Voting records may reveal

P(y*x) and census data may reveal P(w*x). The problem then is to use knowledge of P(y*x) and P(w*x) to

learn P(y*x, w).

        Inference on P(y*x, w) given knowledge of P(y*x) and P(w*x) has also been studied in research on

estimation with contaminated sampling, which began with Huber (1964). Here the object of interest is P(y*x,

w = j) for a specified value of j. Values of (y, x, w) with w = j are said to be error-free, whereas those with

w j are said to be erroneous. The researcher only observes (y, x) pairs, not (y, x, w) triples, and so does not

know which observations are error free. The researcher is, however, assumed to know the conditional

probability P(w = j*x) that an observation is error-free, or at least to know a lower bound on this probability.

        Whether the terminology be that of ecological inference or contaminated sampling, the mathematical

problem is to characterize the restrictions on P(y|x, w) implied by knowledge of P(y|x) and P(w|x), via the

Law of Total Probability (1). I first consider the simple case in which y is a binary outcome and then the case

of a general real-valued outcome.



3.1.1. Predicting Binary Outcomes

        When outcome y is binary, the identification region for P(y = 1|x = k, w = j) is an easily computed

interval, namely



                                      P(y = 1|x = k) ! P(w j|x = k) P(y = 1|x = k)
(2) P(y = 1|x = k, w = j) 0 [0, 1] 1 [————————————–, ——————].
                                               P(w = j|x = k)       P(w = j|x = k)



It is unclear when this result first appeared. It was sketched by Duncan and Davis (1953) in their concise

seminal study of ecological inference. They attributed it to the early statistician Yule. The first formal proof

appears to be in Horowitz and Manski (1995, Corollary 1.2), in their study of identification under
                                                       13

contaminated sampling.

        Proof of (2) is easy. Solving the linear equation (1) for P(y = 1*x = k, w = j) yields



(3)      P(y = 1*x = k, w = j) = [P(y = 1|x = k) ! P(w        j|x = k)@P(y = 1*x = k, w     j)]/P(w = j|x = k).



Letting P(y = 1*x = k, w     j) take all values in the interval [0, 1] yields a tentative identification region for

P(y = 1*x = k, w = j), this being the interval



                             P(y = 1|x = k) ! P(w j|x = k) P(y = 1|x = k)
                           [ ————————————–, ——————].
                                     P(w = j|x = k)        P(w = j|x = k)



However, not all values of P(y = 1*x = k, w      j) may be feasible; some values may make the lower bound of

the above interval less than zero or the upper bound greater than one. Shrinking the interval to include only

proper probabilities yields the interval on the right-hand side of (2).

        The lower bound in (2) is greater than zero, hence informative, when P(y = 1|x = k) > P(w         j|x = k).

The upper bound is less than one, hence informative, when P(y = 1|x = k) < P(w = j|x = k). When both

conditions hold, the interval has width P(w j|x = k)/P(w = j|x = k). A necessary but not sufficient condition

for both bounds to be informative is that P(w = j|x = k) > ½.



Illustration: Consider application of the BCRA Tool to a woman with these attributes (x = k): (1) no history

of breast cancer or chest radiation therapy; (2) unknown presence of a BRCA mutation; (3) 40 years old; (4)

age of first menstrual period in interval 12-13; (5) age of first live birth in interval 20-24; (6) 0 first-degree

female relatives with breast cancer; (7) 0 breast biopsies; (8) white race/ethnicity. The BCRA predicted

lifetime risk that such a woman will develop invasive breast cancer is P(y = 1|x = k) = 0.090.

        Suppose that the clinician asks the woman about her alcohol consumption, specifically whether or
                                                      14

not she is a heavy drinker, defined as drinking five or more drinks on the same occasion on each of five or

more days in the past thirty days. Let w = 1 if the patient is a heavy drinker and w = 0 otherwise. Data

collected in the 2014 National Survey on Drug Use and Health (NSDUH) shows that the fraction of adult

women who are heavy drinkers by this definition is 0.034 (Substance Abuse and Mental Health Services

Administration, 2014). Thus, P(w = 1) = 0.034.

        Suppose the clinician assumes that P(w = 1|x = k) = P(w = 1). Given the specified values for P(y|x)

and P(w|x), application of (1) yields these findings for P(y|x, w): P(y = 1|x = k, w = 0) 0 [0.058, 0.093] and

P(y = 1|x = k, w = 1) 0 [0, 1]. Thus, combining evidence on group outcomes and on group composition yields

a tight bound on P(y = 1|x = k, w = 0) but reveals nothing about P(y = 1|x = k, w = 1). The source of this

extreme difference in informativeness is that the fraction of women who are heavy drinkers is so small

(0.034). ~



3.1.2. Predicting Mean and Quantile Outcomes

        When y is a real-valued outcome taking more than two values, there is no characterization of the

identification region for P(y|x, w) of simplicity comparable to (2). However, Horowitz and Manski (1995)

derive relatively simple expressions for the identification regions of the mean and quantiles of P(y|x, w).

Thus, consider the long conditional mean E(y*x = k, w = j) or the á-quantile Qá(y*x = k, w = j), where á 0

(0, 1). Identification of these parameters can be studied directly, but Horowitz and Manski find it easier to

prove a general result for the class of parameters that respect stochastic dominance and then apply this result

to the mean and quantiles.

        To simplify notation, let p / P(w    j|x = k). It can be shown that the identification region for P(y*x

= k, w = j) contains a “smallest” member L that is stochastically dominated by all feasible values of P(y*x

= k, w = j) and a “largest” member U that stochastically dominates all feasible values of P(y*x = k, w = j).

These distributions are truncated versions of the observed distribution P(y|x = k): L right-truncates P(y|x =

k) at its (1 ! p)–quantile and U left-truncates P(y|x = k) at its p–quantile. Formally, L and U are defined as
                                                      15

follows:



(4a)                 L[!4, t] / P(y # t|x = k)/(1 ! p)        for t < Q(1!p)(y|x = k),

                              / 1                             for t $ Q(1!p)(y|x = k).

(4b)                 U[!4, t] / 0                             for t < Q p(y|x = k),

                              / [P(y # t|x = k) !p]/(1 ! p)    for t $ Qp(y|x = k).



           With this background, it follows immediately that if D(@) is a parameter that respects stochastic

dominance, the smallest feasible value of D[P(y*x = k, w = j)] is D(L) and the largest feasible value is D(U).

Hence, sharp lower and upper bounds on E(y*x = k, w = j) are the means of L and U. Similarly, sharp bounds

on Qá(y*x = k, w = j) are the á–quantiles of L and U.

           The above result determines sharp lower and upper bounds on D[P(y*x = k, w = j)], but it does not

assert that the identification region is the entire interval connecting these bounds. It can be shown that the

identification region is the entire interval if D(@) is the mean. However, the interior of the interval may

contain non-feasible values if y is discrete and D(@) is a quantile. Quantiles must be elements of the set Y of

logically possible values of y. Hence, the identification region for D[P(y*x = k, w = j)] can only include the

elements of the interval [D(L), D(U)] that belong to Y.



3.2. Risk Assessment with Strong Structural Assumptions



           The above summarizes findings on inference on P(y*x, w) using only knowledge of P(y*x) and

P(w*x). Tighter inferences may be feasible if structural assumptions are imposed. The literature has

developed two approaches imposing assumptions strong enough to point-identify P(y|x, w). I review these

here.
                                                      16

3.2.1. Instrumental Variables

        I first consider an assumption proposed by Goodman (1953) in the context of ecological inference

when y is real-valued. Goodman took the objective to be inference on the conditional means E(y*x, w). This

objective is the same as inference on the distribution P(y|x, w) when y is binary, as E(y|x, w) = P(y = 1|x, w)

in that case.

        The assumption uses x as an instrumental variable, asserting that y is mean-independent of x,

conditional on w. That is,



(5)             E(y*x = k, w = j) = E(y*w = j),      all (k, j) 0 X × W.



In the classical voting application studied by social scientists, (5) assumes that persons who have the same

demographic attributes but who reside in different districts vote the same way, on average.

        To begin, Goodman observed that the Law of Iterated Expectations gives



(6)               E(y*x = k) =     3 E(y*x = k, w = j)P(w = j*x = k),       k 0 X.
                                  j0W



For each k 0 X, the data reveal E(y*x = k) and [P(w = j*x = k), j 0 W], but not [E(y*x = k, w = j), j 0 W]. So

(6) is a system of *X* linear equations with the *X* × *W* unknowns E(y*x = k, w = j), (k, j) 0 X × W.

        Now impose assumption (5). Then (6) becomes



(7)                  E(y*x = k) =     3 E(y*w = j)P(w = j*x = k),          k 0 X.
                                     j0W



This is a system of *X* equations with the *W* unknowns E(y*w = j), j 0 W. Applying elementary linear

algebra, Goodman concluded that these equations have a unique solution if *X* $ *W* and if the *X* × *W*

dimensional matrix of conditional probabilities [P(w = j*x = k), (k, j) 0 X × W] has full rank *W*. Then
                                                        17

assumption (5) point-identifies E(y*w = j), j 0 W.

        Goodman also observed that assumption (5) is refutable. Equation system (7) may have no solution,

or its solution may lie outside the logical range of y. In both cases, it follows that assumption (5) is incorrect.

        Goodman’s remarkably simple analysis fully resolves the identification problem when equation

system (7) has one solution. It does not show how to use assumption (5) when (7) has multiple solutions, as

is generically the case when *X* < *W*. Extending Goodman's analysis to cover this case, Cross and Manski

(2002) characterize the identification region for [E(y*w = j), j 0 W] under assumption (5).

        It is important to mention that although Goodman (1953) demonstrated the identifying power of

assumption (5), he did not advocate its regular use in practice. He cautioned that the assumption holds (p.

663) "in very special circumstances." When he studied assumption (5), Goodman had in mind applications

to social science rather than to medicine. I find it difficult to conjecture instances in assessment of health risk

where the assumption may be credible. In risk assessment, x are attributes used to predict outcomes by

evidence-based assessment tools and w are clinician-observed attributes that are not used by these tools. If

(5) holds, the attributes x used by assessment tools have no predictive power once one conditions prediction

on the additional attributes w. It is mathematically possible for this to occur, but it seems unlikely to occur

in practice.



3.2.2. Parametric Models

        The second approach used to point-identify P(y|x, w) is to assert a parametric model that places these

conditional distributions in a finite-dimensional family. Thus, let È be a specified subset of L-dimensional

real space, let F(@, @, @) be a specified function mapping X × W × È into probability distributions on the

outcome space Y, and assume that there exists a è 0 È such that



(8)    P(y|x = k, w = j) = F(k, j, è), all (k, j) 0 X × W.
                                                       18

Combining the Law of Total Probability with assumption (8) yields



(9)               P(y*x = k) =     3 F(k, j, è)P(w = j*x = k),     k 0 X.
                                  j0W



For each k 0 X, the data reveal P(y*x = k) and [P(w = j*x = k), j 0 W]. Hence, (9) is a system of *X*

distributional equations restricting the L-dimensional parameter è.

        Analysis of distributional equations is difficult, but progress can be made by considering the

implications for prediction of mean outcomes. Let e(k, j, è) denote the mean of the random variable with

distribution F(k, j, è). Insertion of e(k, j, è) into the Law of Iterated Expectations (6) yields



(10)               E(y*x = k) =     3 e(k, j, è)P(w = j*x = k),    k 0 X.
                                   j0W



The system (10) of equations is similar to Goodman's system (7) except that e(k, j, è) generally varies

nonlinearly with è. Nonlinearity in è implies that solution of (10) is more complex than was the case with

Goodman's instrumental variable assumption. Nevertheless, the equations have a unique solution if *X* $

L and if sufficient regularity conditions hold. Then assumption (8) point-identifies P(y*x, w).

        There are innumerable alternative parametric models for P(y|x, w) and, hence, innumerable potential

implementations of this approach to inference. Twenty years ago, a particular model was proposed with

enthusiasm by King (1997), who asserted that he had achieved “a solution to the ecological inference

problem” in a book of that name. However, his assumptions immediately drew criticism, as evidenced in a

dispute played out in the Journal of the American Statistical Association (Freedman, Klein, Ostland, and

Roberts, 1998, 1999; King, 1999) and elsewhere (Cho, 1998, and Cho and Gaines, 2004). Wakefield (2008)

cautions against application of the King model or other parametric models to public health research.
                                                       19

Illustration: A common problem in risk assessment is to predict a patient's remaining life span conditional

on observed attributes. Let y denote remaining life span. Life tables provide actuarial predictions of mean

life span conditional on age, race, and sex; see, for example, Centers for Disease Control and Prevention

(2015). However, existing life tables do not predict mean life span conditional on the patient attributes that

clinicians commonly observe. The administrative data contained in death records may state a person's

immediate "cause of death," but they do not document the person's medical history. Population surveys may

provide richer personal data but most do not follow sample members until death.

        Organizations developing clinical practice guidelines for breast cancer screening have had to

acknowledge the absence of evidence-based predictions of life spans under alternative screening regimes.

For example, in a review of knowledge of the benefits and harms of screening commissioned by the American

Cancer Society, Myers et al. (2015) state (p. 1616):

        "We did not identify any direct evidence on the association between mammographic screening and
        life expectancy, which would require following up all participants in an RCT or cohort study until
        death from any cause. . . . . Because estimates of life expectancy gains from screening are by
        definition indirect and there is considerable uncertainty about the value of several parameters
        important for estimating these gains (in particular the magnitude of mortality reduction associated
        with screening at different ages and different intervals), we judged the quality of evidence for the
        magnitude of the association between screening and life expectancy to be LOW."

        Myers et al. summarize model-based predictions of life expectancy reported by Mandelblatt et al.

(2009). The latter authors describe six parametric models developed by different research teams. Each team

had access to a shared database that characterized the life trajectories from age 25 to age 40 of American

women born in 1960. The six teams made varying predictions of life expectancy after age 40 by combining

these shared data with auxiliary data and with alternative structural assumptions strong enough to yield point

identification.   ~
                                                        20

3.3. Bounded-Variation Assumptions



         A clinician contemplating risk assessment conditioning on patient attributes not used in evidence-

based assessment tools may be discomforted by the analysis in Sections 3.1 and 3.2. Without structural

assumptions, drawing informative conclusions about P(y|x = k, w= j) requires a relatively high prevalence

of attribute w = j in the group with attributes x = k. Strong structural assumptions may point-identify P(y|x

= k, w= j), but the conclusion drawn may have low credibility.

         There is a substantial middle ground between the polar cases of no structural assumptions and

assumptions strong enough to yield point identification. This section considers a class of bounded-variation

assumptions that clinicians should find easy to contemplate and apply. These assumptions flexibly restrict

the magnitudes of risk assessments and the degree to which they vary with patient attributes, enabling

clinicians to express quantitative judgments in a structured way. Bounded-variation assumptions have

previously been used to provide identifying power in other settings. See Manski and Pepper (2000, 2013,

2015).

         Section 3.3.1 analyzes some particularly simple bounded-variation assumptions when outcome y is

binary. Section 3.3.2 considers assumptions that place more general bounds on mean outcomes.



3.3.1. Binary Outcomes

         Recall the derivation of identification region (2) for P(y = 1|x = k, w = j) in the absence of structural

assumptions. Applying the Law of Total Probability (1) to P(y = 1|x = k) and solving this linear equation for

P(y = 1*x = k, w = j) yielded equation (3). Result (2) emerged by imposing only the logical constraints that

P(y = 1*x = k, w    j) and P(y = 1*x = k, w = j) both lie in the unit interval.

         A clinician may find it credible to assume that these long conditional probabilities lie within specified

informative bounds within the unit interval, say [a(k, j), b(k, j)] and [a(k, j), b(k, j)]. Thus, let the clinician

assume that
                                                        21

(11a)             a(k, j) # P(y = 1*x = k, w     j) # b(k, j),

(11b)             a(k, j) # P(y = 1*x = k, w = j) # b(k, j).



Using these bounds in (3) yields a bounded-variation identification region for P(y = 1*x = k, w = j), namely



(12) P(y = 1*x = k, w = j) 0


                          P(y = 1|x = k) ! b(k, j)@P(w j|x = k) P(y = 1|x = k) ! a(k, j)@P(w j|x = k)
   [a(k, j), b(k, j)] 1 [ ————————————–———, ————————————————].
                                         P(w = j|x = k)                   P(w = j|x = k)



        This interval has a simple and flexible form. It shrinks to a point as the width of either bound (11a)

or (11b) approaches zero. It widens to interval (2) as the widths of bounds (11a) and (11b) both approach

one. An important feature of assumptions (11a) and (11b) is that each bound helps to identify both long

predictive probabilities. That is, bound (11a) on P(y = 1*x = k, w     j) helps to identify P(y = 1*x = k, w = j)

and vice versa. This occurs because equation (3) connects the two probabilities to one another. Assumptions

that restrict one imply restrictions on the other.



Illustration: It often is credible to assume that risk of illness varies monotonically with the value of a patient

attribute. For example, consider the earlier illustration of risk of breast cancer in which w = 1 denotes a heavy

drinker and w = 0 a non-heavy drinker. A body of epidemiological research indicates that the risk of breast

cancer increases with alcohol consumption (e.g., Singletary and Gapstur, 2001). Under this assumption, the

bounds (11) are



(13a)             0 # P(y = 1*x = k, w = 0) # P(y = 1*x = k),

(13b)             P(y = 1*x = k) # P(y = 1*x = k, w = 1) # 1.
                                                      22

The resulting identification regions for P(y = 1*x = k, w = 0) and P(y = 1*x = k, w = 1) are



                                                      P(y = 1|x = k) ! P(w = 1|x = k)
(14a) P(y = 1*x = k, w = 0) 0 [0, P(y = 1*x = k)] 1 [ —————————–———, P(y = 1|x = k)],
                                                                P(w = 0|x = k)



                                                                     P(y = 1|x = k)
(14b) P(y = 1*x = k, w = 1) 0 [P(y = 1*x = k), 1] 1 [P(y = 1|x = k), ——————].
                                                                     P(w = 1|x = k)



        Recall from the earlier illustration that P(y = 1|x = k) = 0.090, P(w = 1|x = k) = 0.034, and P(w = 0|x

= k) = 0.966. Inserting these values into (14) yields P(y = 1|x = k, w = 0) 0 [0.058, 0.09] and P(y = 1|x = k,

w = 1) 0 [0.09, 1]. These identification regions modestly tighten those reported earlier without any structural

assumption.

        Stronger findings emerge if a clinician assumes more than (13). One might, for example, believe that

not being a heavy drinker can at most reduce the risk of breast cancer to 0.08. The identification regions

combining assumption (13) with this lower bound on P(y = 1|x = k, w = 0) are P(y = 1|x = k, w = 0) 0 [0.08,

0.09] and P(y = 1|x = k, w = 1) 0 [0.09, 0.37]. Thus, asserting a lower bound on the cancer risk of women

who are not heavy drinkers tightens the upper bound on the risk for heavy drinkers.       ~



3.3.2. Bounds on Mean Outcomes

        Recall the Goodman (1953) use of instrumental variables to identify conditional mean outcomes.

The derivation began with the Law of Iterated Expectations (6). Goodman studied solution of (6) under an

invariance assumption, namely that E(y*x = k, w = j) does not vary with x, conditional on w.

        In place of Goodman's invariance assumption, one might assert a set of bounded-variation

assumptions that impose M > 0 linear inequalities restricting E(y*x = k, w = j), (k, j) 0 X × W. In abstraction,

these inequalities have the form
                                                       23

(15)               a(m) #     3 c(m, k, j)@E(y*x = k, w = j) # b(m),      m = 1, . . . , M,
                          (k, j) 0 X × W



where [a(m), b(m), c(m, k, j), m = 1, . . . . , M; (k, j) 0 X × W] are specified constants.

        The identification region for E(y*x = k, w = j) is the interval whose lower (upper) bound minimizes

(maximizes) E(y*x = k, w = j) subject to (6) and (15). These lower and upper bounds solve linear

programming problems. Hence, they should be easy to compute with modern algorithms and hardware. It

should be straightforward to develop a prediction support tool that queries the clinician to input values for

E(y|x), P(w|x), and [a(m), b(m), c(m, k, j), m = 1, . . . . , M; (k, j) 0 X × W]. This done, the tool would

compute the lower and upper bounds on E(y*x = k, w = j) for any specified value of (k, j).

        Three special cases of the inequalities (15) may be particularly useful in clinical practice. First, one

may impose inequalities of the form



(16)          a(m) # E(y*x = k, w = j) # b(m).



Such inequalities bound the magnitudes of mean risk assessments. Thus, (16) generalizes the bounds (11)

on magnitudes posed earlier in the context of binary outcomes to settings with real-valued outcomes.

        Second, one may impose inequalities of the form



(17)   a(m) # E(y*x = k, w = j) ! E(y*x = kN, w = j) # b(m),



where k and kN are two values of attribute x. Such inequalities bound the variation of mean risk assessments

with x, holding w fixed. Goodman's invariance assumption is the special case in which a(m) = b(m) = 0.

        Third, one may impose inequalities of the form



(18)        a(m) # E(y*x = k, w = j) ! E(y*x = k, w = jN) # b(m),
                                                      24

where j and jN are two values of attribute j. Such inequalities bound the variation of mean risk assessments

across values of w rather than across values of x.




4. Patient Care with Partial Personalized Risk Assessment



4.1. Optimal and Reasonable Care



        Section 3 characterized risk assessment that combines evidence on group outcomes and composition

with structural assumptions. The basic lesson was that one may often draw some credible conclusions about

the long predictive distribution P(y|x = k, w = j) but one can rarely learn it precisely. Thus, partial

personalized risk assessment would seem the norm in clinical practice.

        This section considers medical decision making. As mentioned in Section 2, normative studies such

as Phelps and Mushlin (1988) have assumed that clinicians maximize expected utility with accurate

probabilistic risk assessments conditional on observed patient attributes. Our concern is decision making with

less information.

        In some circumstances, a clinician with partial knowledge may have sufficient information to choose

a care option that maximizes expected utility. Consider choice of a breast cancer care strategy for a woman

who has thus far not been diagnosed with the disease. The optimal strategy may be (A) periodic screening

if the woman's risk of developing the disease is below a certain threshold or (B) prophylactic treatment if her

risk is above the threshold. If so, determination of the optimal strategy does not require precise knowledge

of the woman's risk. It suffices to know whether risk is below or above the threshold.

        How might a clinician choose patient care when credible risk assessment is not sufficiently

informative to maximize expected utility? Bayesian decision theorists, citing axioms for decision making

under uncertainty proposed and studied by Savage (1954), suggest maximization of subjective expected
                                                     25

utility, using clinical judgment to make a subjective probabilistic risk assessment. Bayesian patient care may

be attractive if subjective probabilistic risk assessment has a credible foundation, but it may be harmful

otherwise.

        A clinician who acts without making a subjective probabilistic risk assessment is said to face a

problem of decision making under ambiguity (Ellsberg, 1961). There exists no optimal strategy for patient

care under ambiguity. Nevertheless, one can usefully pose alternative decision criteria and compare their

properties, the aim being to provide options that a decision maker may view as reasonable.

        A broadly reasonable idea is to use a criterion that achieves uniformly satisfactory results, whatever

the truth may be. There are multiple ways to formalize the idea of uniformly satisfactory results. Two that

have long been prominent in the literature on decision theory are the maximin (von Neumann and

Morgenstern, 1944; Wald, 1950) and minimax-regret (Savage, 1951) criteria. It appears that these criteria

have not been applied to medical decision making until recently. Manski (2009) studied maximin and

minimax-regret treatment choice in an abstract setting. Manski (2010, 2016) used these criteria to consider

how society might reasonably choose a vaccination policy under ambiguity. Manski (2013) considered the

decision to perform diagnostic testing as a prelude to treatment.

        Here I add to this small recent literature by considering choice under ambiguity between active

surveillance of a patient (aka watchful waiting) and prophylactic treatment. An apt example is choice

between periodic screening for breast cancer and prophylactic treatment. Similar choices are made regularly

by clinicians who care for patients at risk of aggressive prostate cancer, heart disease, and many other

illnesses. I pose a relatively simple version of the decision problem, for which it is easy to determine the

maximin and minimax-regret choices.

        The present analysis differs from my earlier work on medical decision making under ambiguity in

two important respects. First, this paper maintains a patient-centric perspective in which a clinician wants

to care as well as possible for a specific patient. In contrast, the earlier work presumed a population-wide

perspective in which a health planner wants to maximize a social welfare function that aggregates outcomes
                                                         26

across the population of patients. Second, the ecological inference problem studied here has a different

structure than the identification problems that I have considered earlier. The aspect of my earlier work most

closely related to the present analysis is a short examination in Manski (2000) of treatment choice by a social

planner who observes the aggregate outcomes of a classical randomized trial but who does not observe

outcomes within sub-populations of subjects.



4.2. Choice under Ambiguity Between Active Surveillance and Prophylactic Treatment



           As above let y = 1 if a patient will develop a specified disease and y = 0 if not. Let Pjk / P(y = 1|x

= k, w = j) denote the objective personalized probability that the patient will develop the disease, conditional

on the observed attributes (x = k, w = j). Using the available evidence and credible structural assumptions,

suppose that the clinician treating the patient is able to credibly conclude that Pjk lies in some interval [PL, PH];

thus, PL and PH are the lowest and highest feasible values of the patient's risk of illness.

           Suppose that the clinician chooses between two care options, c = A denoting active surveillance and

c = B denoting prophylactic treatment. The utility of each option depends on whether the patient will or will

not develop the disease. Let U(c, y) denote the utility of option c in the presence of illness outcome y. The

utility function U(@, @) expresses patient preferences and may be specific to the patient under consideration.

           Choice between c = A and c = B is a non-trivial decision problem if the relative merits of surveillance

and treatment vary with the illness outcome. It often is reasonable to suppose that prophylactic treatment is

the better option if the patient will develop the disease and that surveillance is the better option otherwise.

That is,



(19)   U(B, 1) > U(A, 1) and U(A, 0) > U(B, 0).



It is also often reasonable to suppose that, whatever care option is used, it is better to be healthy than ill. That
                                                      27

is,



(20)   U(A, 0) > U(A, 1) and U(B, 0) > U(B, 1).



I assume that these inequalities hold in parts of the analysis below.



4.2.1. Care Maximizing Objective or Subjective Expected Utility

        The clinician chooses a care option without knowing the illness outcome. The normative literature

on medical decision making has supposed that the clinician knows the utility function U(@, @) and the

personalized illness probability Pjk, and that he chooses a care option that maximizes objective expected

utility. Thus, the clinician acts as follows:



(21a) Choose c = A if Pjk@U(A, 1) + (1 ! Pjk)@U(A, 0) $ Pjk@U(B, 1) + (1 ! Pjk)@U(B, 0),

(21b) Choose c = B if Pjk@U(B, 1) + (1 ! Pjk)@U(B, 0) $ Pjk@U(A, 1) + (1 ! Pjk)@U(A, 0).



        The solution to (21) is easy to characterize when inequalities (19) hold. Let P* denote the threshold

value of Pjk that makes options A and B have the same expected utility. This value is



                             U(A, 0) ! U(B, 0)
(22)         P* = ————————————————— .
                  [U(A, 0) ! U(B, 0)] + [U(B, 1) ! U(A, 1)]



Option A is optimal if Pjk # P* and option B if Pjk $ P*.

        Our concern is decision making when the clinician does not know Pjk. He only knows that Pjk 0 [PL,

PH]. To focus on the implications of partial personalized risk assessment and to keep the analysis simple, I

will maintain the traditional normative assumption that the clinician knows U(@, @).
                                                       28

        A clinician with partial knowledge of Pjk can maximize objective expected utility if the threshold

probability P* is not interior to the interval [PL, PH]. Option A is sure to be optimal if PH # P* and B is sure

to be optimal if P* # PL. The clinician cannot maximize objective expected utility if P* is interior to [PL, PH].

Then there exist feasible values of Pjk that make only option A optimal and other values that make only B

optimal.

        The Bayesian prescription is to place a subjective distribution on Pjk and to maximize subjective

expected utility. The Bayesian prescription is easy to characterize in the present decision problem because

objective expected utility is linear in Pjk. Let ðjk denote the subjective mean that a Bayesian clinician holds

for Pjk. The Bayesian clinician acts as if Pjk = ðjk. Thus, option A maximizes subjective expected utility if

ðjk # P* and B if ðjk $ P*.

        In what follows, I suppose that the clinician does not place a subjective distribution on Pjk. Sections

4.2.2 and 4.2.3 study maximin and minimax-regret care respectively.



4.2.2. Maximin Care

        The maximin criterion evaluates each action by the worst welfare that it may yield and it chooses an

action with the least-bad worst welfare. In the present setting, there are two ways that one might reasonably

define the worst welfare of a care option. Hence, there are two ways to implement the maximin criterion.

        One approach considers the two possible illness outcomes, y = 0 and y = 1. Then the worst welfare

under option A is min[U(A, 0), U(A, 1)] and the worst under B is min[U(B,0), U(B, 1)]. With this definition

of worst welfare, option A is a maximin choice if min [U(A, 0), U(A, 1)] $ min [U(B, 0), U(B, 1)] and option

B if min [U(B, 0), U(B, 1)] $ min [U(A, 0), U(A, 1)]. When inequalities (19) and (20) hold, option B is the

maximin choice.

        The other approach considers the possible values for the objective expected utility of each option.

When inequalities (20) hold, the worst feasible expected utility under options A and B both occur when Pjk

equals its upper bound PH. Then options A and B have objective expected utilities PH@U(A, 1) + (1 ! PH)@U(A,
                                                        29

0) and PH@U(B, 1) + (1 ! PH)@U(B, 0) respectively. A clinician using this version of the maximin criterion acts

as follows:



(23a) Choose c = A if PH@U(A, 1) + (1 ! PH)@U(A, 0) $ PH@U(B, 1) + (1 ! PH)@U(B, 0),

(23b) Choose c = B if PH@U(B, 1) + (1 ! PH)@U(B, 0) $ PH@U(A, 1) + (1 ! PH)@U(A, 0).



Thus, this maximin choice is option A if PH # P* and B if PH $ P*.

        The maximin criterion has a deserved reputation for conservatism. Among the two versions of

maximin considered here, the former is more conservative than the latter. A clinician using the former

version acts as if the patient will become ill for sure. A clinician using the latter version acts as if the patient

will become ill with probability PH, the upper bound on his risk assessment.



4.2.3. Minimax-Regret Care

        The minimax-regret (MMR) criterion evaluates each action by the worst reduction in welfare that it

may yield relative to the highest welfare achievable. The term regret connotes reduction in welfare relative

to the highest achievable. Maximum regret is the worst reduction possible, considering all feasible risk

assessments. The criterion chooses an action that minimizes maximum regret.

        As with maximin, there are two ways that one might reasonably define maximum regret when

considering patient care. Hence, there are two ways to implement the criterion. In what follows, I assume

that inequalities (19) and (20) hold.

        Again, one approach considers the two possible illness outcomes, y = 0 and y = 1. Inequalities (19)

state that prophylactic treatment is the better option when illness occurs and surveillance is better otherwise.

Hence, option A has zero regret when y = 0 and positive regret U(B, 1) ! U(A, 1) when y = 1; thus, maximum

regret is U(B, 1) ! U(A, 1). Symmetrically, option B has zero regret when y = 1 and positive regret U(A, 0)

! U(B, 0) when y = 0; thus, its maximum regret is U(A, 0) ! U(B, 0). It follows that option A is a minimax-
                                                       30

regret choice if U(B, 1) ! U(A, 1) # U(A, 0) ! U(B, 0) and option B if U(B, 1) ! U(A, 1) $ U(A, 0) ! U(B,

0). The MMR choice is the same as a clinician maximizing objective expected utility would make if he were

to know that the probability of illness is Pjk = ½.

           The other approach considers the possible values for the objective expected utility of each option.

Given inequalities (19), maximum regret under option A occurs when Pjk equals its upper bound PH and

equals the expected utility difference



(24)         [PH@U(B, 1) + (1 ! PH)@U(B, 0)] ! [PH@U(A, 1) + (1 ! PH)@U(A, 0)].



Symmetrically, maximum regret under option B occurs when P jk equals its lower bound PL and equals



(25)          [PL@U(A, 1) + (1 ! PL)@U(A, 0)] ! [PL@U(B, 1) + (1 ! PL)@U(B, 0)].



Option A is an MMR choice if the difference in expected utilities shown in (24) is less than or equal to that

in (25).

           A more transparent representation of this finding emerges if we define PM to be the midpoint of

interval [PL, PH]. Then rearrangement of terms in (24) and (25) shows that a clinician using this version of

the MMR criterion acts as follows:



(26a) Choose c = A if PM@U(A, 1) + (1 ! PM)@U(A, 0) $ PM@U(B, 1) + (1 ! PM)@U(B, 0),

(26b) Choose c = B if PM@U(B, 1) + (1 ! PM)@U(B, 0) $ PM@U(A, 1) + (1 ! PM)@U(A, 0).



Thus, the MMR choice is the same as a clinician maximizing objective expected utility would make if he

were to know that the probability of illness is Pjk = PM.

           The maximin and MMR criteria are sometimes confused with one another. The above derivations
                                                       31

and findings make clear that they differ. A clinician using the maximin criterion chooses a care option that

maximizes the minimum utility or expected utility that might possibly occur. A clinician using the MMR

criterion chooses an option that minimizes the maximum reduction in utility or expected utility that can

possibly result from incomplete knowledge.

        We have found that the care choices yielded by the two maximin criteria are those that a clinician

maximizing expected utility would make if he were to adopt the pessimistic perspective that illness will occur

for sure or with probability PH. In contrast, the choices yielded by the two MMR criteria are those that a

clinician maximizing expected utility would choose if he were to adopt the middle-ground perspective that

illness will occur with probability ½ or PM.



4.3. Conclusion: Rethinking Care with Actuarial Prediction



        The psychological literature on clinical judgment exemplified by Dawes, Faust, and Meehl (1989)

does not recommend clinical use of any of the decision criteria discussed here—not maximization of

subjective expected utility, nor maximin, nor MMR. Instead it recommends that the clinician suppress his

knowledge of patient attributes w = j and act as if P jk = P(y = 1|x = k).

        Acting as if Pjk = P(y = 1|x = k) is clearly inappropriate if the value of this short probability lies

outside the interval [PL, PH] of credible potential values of Pjk. One might rationalize behaving in this manner

if P(y = 1|x = k) 0 [PL, PH] by asserting that the short probability is a possible value of Pjk. However, the same

assertion can be made for any element of [PL, PH]. I am unaware of any formal argument that justifies

singling out P(y = 1|x = k).

        Sections 4.2.2 and 4.2.3 showed that decision making with the second version of the maximin or

MMR criterion is equivalent to acting as if Pjk takes particular values in [PL, PH], namely PH for maximin and

PM for MMR. Singling out these values has a firmer justification because they yield care choices that are

uniformly satisfactory in the maximin or MMR sense.
                                                     32

        The negative conclusion reached here regarding acting as if Pjk = P(y = 1|x = k) does not contradict

the longstanding conclusion of psychological research that actuarial prediction outperforms informal clinical

judgment. Psychologists may be correct that clinician failure to adequately grasp the logic of the prediction

problem generates a broad empirical finding in favor of actuarial prediction. Coherent combination of

evidence and judgment is a subtle matter. It may be unrealistic to expect clinicians to understand the

mathematics of ecological inference or to accurately perform it in their heads, without the assistance of

decision support tools.

        What my analysis does suggest is that it may be possible to improve on both actuarial prediction and

informal clinical judgment by formalizing clinical judgment.
                                                   33

References


Academy of Medical Sciences (2015), Stratified, Personalised or P4 Medicine: a New Direction for Placing
the Patient at the Centre of Healthcare and Health Education,
www.acmedsci.ac.uk/viewFile/56e6d483e1d21.pdf, accessed July 4, 2016.

American Medical Association (2010), Genomic-based Personalized Medicine, Report 4 of the Council on
Science and Public Health, download.ama-assn.org/resources/doc/csaph/x-pub/a10csaph4.pdf, accessed July
4, 2016.

Camerer, C. and E. Johnson (1997), "The Process-Performance Paradox in Expert Judgment: How Can
Experts Know so Much and Predict so Badly," in Research on Judgment and Decision Making, W. Goldstein
and R. Hogarth (editors), Cambridge: Cambridge University Press.

Centers for Disease Control and Prevention (2015), United States Life Tables, 2011,
www.cdc.gov/nchs/data/nvsr/nvsr64/nvsr64_11.pdf, accessed August 18, 2016.

Cho, W. (1998), "Iff the Assumption Fits . . . : a Comment on the King Ecological Inference Solution,"
Political Analysis, 7, 143-163.

Cho, W. and B. Gaines (2004), "The Limits of Ecological Inference: the Case of Split-ticket Voting,"
American Journal of Political Science, 48, 52–71.

Claus, E, N. Risch, and W. Thompson (1994), "Autosomal Dominant Inheritance of Early-onset Breast
Cancer. Implications for Risk Prediction," Cancer, 73, 643-651.

Cross, P. and C. Manski (2002), “Regressions, Short and Long,” Econometrica, 70, 357-368.

Dawes, R., R. Faust, and P. Meehl (1989), "Clinical Versus Actuarial Judgment," Science, 243, 1668-1674.

Duncan, O. and B. Davis (1953), “An Alternative to Ecological Correlation,” American Sociological Review,
18, 665-666.

Ellsberg D. (1961), "Risk, Ambiguity, and the Savage Axioms," Quarterly Journal of Economics, 75, 643-
669.

Freedman, D., S. Klein, M. Ostland, and M. Roberts (1998), “Review of A Solution to the Ecological
Inference Problem, by G. King,” Journal of the American Statistical Association, 93, 1518-1522.

Freedman, D., S. Klein, M. Ostland, and M. Roberts (1999), “Response to King’s Comment,” Journal of the
American Statistical Association, 94, 355-357.

Gail, M, L. Brinton, D. Byar, D. Corle, S. Green, C. Shairer, and J. Mulvihill (1989), "Projecting
Individualized Probabilities of Developing Breast Cancer for White Females Who Are Being Examined
Annually," Journal of the National Cancer Institute, 81,1879-86.

Ginsburg, G. and H. Willard (2009), "Genomic and Personalized Medicine: Foundations and Applications,"
Translational Research, 154, 277-287.
                                                     34

Goodman, L. (1953), “Ecological Regressions and Behavior of Individuals,” American Sociological Review,
18, 663-664.

Horowitz, J. and C. Manski (1995), “Identification and Robustness with Contaminated and Corrupted Data,”
Econometrica, 63, 281-302.

Huber, P. (1964), “Robust Estimation of a Location Parameter,” Annals of Mathematical Statistics, 35, 73-
101.

King, G. (1997), A Solution to the Ecological Inference Problem: Reconstructing Individual Behavior from
Aggregate Data, Princeton: Princeton University Press.

King, G. (1999), “The Future of Ecological Inference Research: A Comment on Freedman et al.,” Journal
of the American Statistical Association, 94, 352-355.

Mandelblatt J., K. Cronin, S. Bailey et al., Breast Cancer Working Group of the Cancer Intervention and
Surveillance Modeling Network (2009), "Effects of Mammography Screening under Different Screening
Schedules: Model Estimates of Potential Benefits and Harms," Annals of Internal Medicine, 151, 738-747.

Manski, C. (2000), “Identification Problems and Decisions Under Ambiguity: Empirical Analysis of
Treatment Response and Normative Analysis of Treatment Choice,” Journal of Econometrics, 95, 415!442.

Manski, C. (2007), Identification for Prediction and Decision, Cambridge, Harvard University Press.

Manski C. (2009), "Diversified Treatment under Ambiguity," International Economic Review, 50,1013-1041.

Manski C. (2010), "Vaccination with Partial Knowledge of External Effectiveness," Proceedings of the
National Academy of Sciences, 107, 3953-3960.

Manski, C. (2013), "Diagnostic Testing and Treatment under Ambiguity: Using Decision Analysis to Inform
Clinical Practice," Proceedings of the National Academy of Sciences, 110, 2064-2069.

Manski, C. (2016), "Mandating Vaccination with Unknown Indirect Effects," Journal of Public Economic
Theory, forthcoming.

Manski, C. and J. Pepper (2000), “Monotone Instrumental Variables: With an Application to the Returns to
Schooling,” Econometrica, 68, 997-1010.

Manski, C. and J. Pepper (2013), “Deterrence and the Death Penalty: Partial Identification Analysis Using
Repeated Cross Sections,” Journal of Quantitative Criminology, 29, 123-141.

Manski, C. and J. Pepper (2015), "How Do Right-to-Carry Laws Affect Crime Rates? Coping with Ambiguity
Using Bounded-Variation Assumptions," National Bureau of Economic Research Working Paper No. 21701.

Meehl, P. (1954), Clinical Versus Statistical Prediction: a Theoretical Analysis and a Review of the Evidence,
Minneapolis: University of Minnesota Press.

Meltzer, D. (2001), "Addressing Uncertainty in Medical Cost-effectiveness: Implications of Expected Utility
Maximization for Methods to Perform Sensitivity Analysis and the Use of Cost-effectiveness Analysis to Set
                                                     35

Priorities for Medical Research," Journal of Health Economics, 20, 109-129.

Myers, E, P. Moorman, J. Gierisch, L. Havrilesky, L. Grimm, S. Ghate, B. Davidson, R. Mongtomery, M.
Crowley, D. McCrory, A. And G. Sanders (2015), "Benefits and Harms of Breast Cancer Screening: A
Systematic Review," Journa of the American Medical Association, 314, 1615-1634.

National Cancer Institute (2016), Breast Cancer Risk Assessment Tool, http://www.cancer.gov/bcrisktool/,
accessed July 4, 2016.

National Comprehensive Cancer Network (2016), Breast Cancer Screening and Diagnosis, Version 1.2016,
www.nccn.org/professionals/physician_gls/pdf/breast-screening.pdf, [login required], accessed August 14,
2016.

Phelps, C. and A. Mushlin (1988), "Focusing Technology Assessment Using Medical Decision Theory,"
Medical Decision Making, 8, 279-289.

President's Council of Advisors on Science and Technology (2008), "Priorities for Personalized Medicine,"
www.whitehouse.gov/files/documents/ostp/PCAST/pcast_report_v2.pdf, accessed July 3, 2016.

Robinson, W. (1950), “Ecological Correlation and the Behavior of Individuals,” American Sociological
Review, 15, 351-357.

Sarbin, T. (1943), "A Contribution to the Study of Actuarial and Individual Methods of Prediction," American
Journal of Sociology, 48, 593– 602.

Sarbin, T. (1944), "The Logic of Prediction in Psychology," Psychological Review, 51, 210-228.

Savage, L. (1951), “The Theory of Statistical Decision,” Journal of the American Statistical Association, 46,
55!67.

Savage, L. (1954), The Foundations of Statistics, New York: Wiley.

Shlonsky, A. and D. Wagner (2005), "The next Step: Integrating Actuarial Risk Assessment and Clinical
Judgment into an Evidence-based Practice Framework in CPS Case Management," Children and Youth
Services Review, 27, 409-427.

Singletary, K. and S. Gapstur (2001), "Alcohol and Breast Cancer: Review of Epidemiologic and
Experimental Evidence and Potential Mechanisms," Journal of the American Medical Association, 286,
2143-2151.

Substance Abuse and Mental Health Services Administration (2014), 2014 National Survey on Drug Use and
Health (NSDUH). Table 2.46B—Alcohol use, binge alcohol use, and heavy alcohol use in the past month
among persons aged 18 or older, by demographic characteristics: Percentages, 2013 and 2014,
www.samhsa.gov/data/sites/default/files/NSDUH-DetTabs2014/NSDUH-DetTabs2014.htm#tab2-46b,
accessed July 17, 2016.

Susan G. Komen (2016), Estimating Breast Cancer Risk,
ww5.komen.org/BreastCancer/GailAssessmentModel.html, accessed July 9, 2016.
                                                   36

Von Neumann, J. and O. Morgenstern (1944), Theory of Games and Economic Behavior, Princeton: Princeton
University Press.

Wakefield, J. (2008), "Ecological Studies Revisited," Annual Review of Public Health, 29, 75-90.

Wald, A. (1950), Statistical Decision Functions, New York: Wiley.
