                                NBER WORKING PAPER SERIES




    CLUSTERING, SPATIAL CORRELATIONS AND RANDOMIZATION INFERENCE

                                           Thomas Barrios
                                          Rebecca Diamond
                                          Guido W. Imbens
                                           Michal Kolesar

                                        Working Paper 15760
                                http://www.nber.org/papers/w15760


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                     February 2010




Financial support for this research was generously provided through NSF grant 0820361. We are
grateful for comments by participants in the econometrics lunch seminar at Harvard University, and
in particular for discussions with Gary Chamberlain. The views expressed herein are those of the authors
and do not necessarily reflect the views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2010 by Thomas Barrios, Rebecca Diamond, Guido W. Imbens, and Michal Kolesar. All rights
reserved. Short sections of text, not to exceed two paragraphs, may be quoted without explicit permission
provided that full credit, including © notice, is given to the source.
Clustering, Spatial Correlations and Randomization Inference
Thomas Barrios, Rebecca Diamond, Guido W. Imbens, and Michal Kolesar
NBER Working Paper No. 15760
February 2010
JEL No. C01,C1,C31

                                                ABSTRACT

It is standard practice in empirical work to allow for clustering in the error covariance matrix if the
explanatory variables of interest vary at a more aggregate level than the units of observation. Often,
however, the structure of the error covariance matrix is more complex, with correlations varying in
magnitude within clusters, and not vanishing between clusters. Here we explore the implications of
such correlations for the actual and estimated precision of least squares estimators. We show that
with equal sized clusters, if the covariate of interest is randomly assigned at the cluster level, only
accounting for non-zero covariances at the cluster level, and ignoring correlations between clusters,
leads to valid standard errors and confidence intervals. However, in many cases this may not suffice.
For example, state policies exhibit substantial spatial correlations. As a result, ignoring spatial correlations
in outcomes beyond that accounted for by the clustering at the state level, may well bias standard errors.
We illustrate our findings using the 5% public use census data. Based on these results we recommend
researchers assess the extent of spatial correlations in explanatory variables beyond state level clustering,
and if such correlations are present, take into account spatial correlations beyond the clustering correlations
typically accounted for.


Thomas Barrios                                         Guido W. Imbens
Department of Economics                                Department of Economics
Harvard University                                     Littauer Center
tbarrios@fas.harvard.edu                               Harvard University
                                                       1805 Cambridge Street
Rebecca Diamond                                        Cambridge, MA 02138
Department of Economics                                and NBER
Harvard University                                     imbens@fas.harvard.edu
rdiamond@fas.harvard.edu
                                                       Michal Kolesar
                                                       Department of Economics
                                                       Harvard University
                                                       kolesarm@nber.org
1       Introduction
Many economic studies analyze the effects of interventions on economic behavior, using out-
come data that are measured on units at a more disaggregate level than that of the intervention
of interest. For example, outcomes may be measured at the individual level, whereas interven-
tions or treatments vary only at the state level, or outcomes may be measured at the firm
level, whereas regulations apply at the industry level. Often the program effects are estimated
using least squares regression. Since the work by Moulton (Moulton, 1986, 1990; Moulton and
Randolph, 1989), empirical researchers in economics are generally aware of the potential impli-
cations of within-cluster correlations in the outcomes on the precision of such estimates, and
often incorporate within-cluster correlations in the specification of the error covariance matrix.
However, there may well be more complex correlations patterns in the data. Correlations in
outcomes between individuals may extend beyond state boundaries, or correlations between
firm outcomes may extend beyond firms within the same narrowly defined industry groups.
    In this paper we investigate the implications, for the sampling variation in least squares
estimators, of the presence of correlation structures beyond those which are constant within
clusters, and which vanish between clusters. Using for illustrative purposes census data with
states as the clusters and individuals as the units,1 we allow for spatial correlation patterns that
vary in magnitude within states, as well as allow for positive correlations between individual
level outcomes for individuals in different, but nearby, states. The first question addressed in
this paper is whether such correlations are present to a substantially meaningful degree. We
address this question by estimating spatial correlations for various individual level variables,
including log earnings, years of education and hours worked. We find that, indeed, such cor-
relations are present, with substantial correlations within divisions of states, and correlations
within puma’s (public use microdata area) considerably larger than within-state correlations.
Second, we investigate whether accounting for correlations of such magnitude is important for
the properties of confidence intervals for the effects of state-level regulations. Note that whereas
clustering corrections in settings where the covariates vary only at the cluster level always in-
crease standard errors, general spatial correlations can improve precision. In fact, in settings
where smooth spatial correlations in outcomes are strong, regression discontinuity designs can
exploit the presence of covariates which vary only at the cluster level. See Black (1999) for an
application of regression discontinuity designs in such a setting, and Imbens and Lemieux (2008)
and Lee and Lemieux (2009) for recent surveys. In our discussion of the second question we
report both theoretical results, as well as demonstrate their relevance using illustrations based
on earnings data and state regulations. We show that if regulations are as good as randomly
assigned to clusters, implying there is no spatial correlation in the regulations beyond the clus-
ters, some variance estimators that incorporate only cluster-level outcome correlations, remain
valid despite the misspecification of the error-covariance matrix. Whether this theoretical result
is useful in practice depends on the actual spatial correlation patterns of the regulations. We
    1
    We are grateful to Ruggles, Sobek, Alexander, Fitch, Goeken, Hall, King, and Ronnander for making the
PUMS data avaiable. See Ruggles, Sobek, Alexander, Fitch, Goeken, Hall, King, and Ronnander (2008) for
details.



                                                   [1]
provide some illustrations that show that, given the spatial correlation patterns we find in the
individual level variables, spatial correlations in regulations may or may not have a substantial
impact on the precision of estimates of treatment effects.
    The paper draws on three literatures that have largely evolved separately. First, the liter-
ature on clustering and difference-in-differences estimation, where one focus is on adjustments
to standard errors to take into account clustering of explanatory variables. See, e.g., Moul-
ton (1986, 1990), Bertrand, Duflo, and Mullainathan (2004), Donald and Lang (2007), Hansen
(2009), and the textbook discussions in Angrist and Pischke (2009) and Wooldridge (2002). Sec-
ond, the literature on spatial statistics. Here a major focus is on the specification and estimation
of the covariance structure of spatially linked data. For discussions in the econometric liter-
ature see Conley (1999), and for textbook discussions see Schabenberger and Gotway (2004),
Arbia (2006), Cressie (1993), and Anselin, Florax, and Rey (2004). In interesting recent work
Bester, Conley and Hansen (2009) and Ibragimov and Müller (2009) link some of the inferential
issues in the spatial and clustering literatures. Finally, we use results from the literature on
randomization inference, most directly on the calculation of exact p-values going back to Fisher
(1925), and results on the exact bias and variance of estimators under randomization (Neyman,
1923, reprinted, 1990). For a recent general discussion of randomization inference see Rosen-
baum (2002). Although the calculation of exact p-values based on randomization inference is
frequently used in the spatial statistics literature (e.g., Schabenberger and Gotway, 2004), and
sometimes in the clustering literature (Bertrand, Duflo and Mullainathan, 2004; Abadie, Dia-
mond, and Hainmueller, 2009; Small, Ten Have, and Rosenbaum, 2008), Neyman’s approach
to constructing confidence intervals using the randomization distribution is rarely used in these
settings. We will argue that the randomization perspective provides useful insights into the
interpretation of confidence intervals in the context of spatially linked data.
    The paper is organized as follows. In Section 2 we introduce the basic setting. Next,
in Section 3, we establish, using census data on earnings, education and hours worked, the
presence of spatial correlation patterns beyond the within-state correlations typically allowed
for. Specifically, we show that for these variables the constant-within-state component of the
total variance that is often the only one explicitly taken into account, is only a small part of
the overall covariance structure. Nevertheless, we find that in some cases standard confidence
intervals for regression parameters based on incorporating only within-state correlations are
quite similar to those based on more accurate approximations to the actual covariance matrix.
In Section 4 we introduce an alternative approach to constructing confidence intervals, based
on randomization inference and originating in the work by Neyman (1923), that sheds light
on these results. Initially we focus on the case with randomization at the unit level. Section
5 extends this to the case with cluster-level randomization. In Section 6, we present some
theoretical results on the implications of correlation structures. We show that if cluster-level
covariates are as good as randomly assigned to the clusters, the standard variance estimator
based on within-cluster correlations is robust to misspecification of the error-covariance matrix,
as long as the clusters are of equal size. Next, in Section 7 we investigate the presence of
spatial correlations of some state regulations using Mantel-type tests from the spatial statistics
literature. We find that a number of regulations exhibit substantial regional correlations. As


                                                [2]
a practical recommendation we suggest that researchers carry out such tests to investigate
whether ignoring between state correlations in the specification of covariance structures may
be misleading. If substantial spatial correlations between covariates are present, the error
correlation structure must be modelled carefully. Section 9 concludes.


2     Set Up
Consider a setting where we have information on N units, say individuals, indexed by i =
1, . . . , N . Associated with each unit is a location Zi . For most of the paper we focus on exact
repeated sampling results, keeping the number of units as well as their locations fixed. We think
of Zi as the pair of variables measuring latitude and longitude for individual i. Associated with
the locations z is a clustering structure. In general we can think of z taking on values in
a set Z, and the clustering forming a partition of the set Z. In our application to data on
individuals, the clustering structure comes from the partition of the United States into areas,
at various levels of disaggregation. Specifically, in that case we consider, in increasing order of
aggregation, pumas, states, and divisions.2 Thus, associated with a location z is a unique puma
P (z). Associated with a puma p is a unique state S(p), and, finally, associated with a state s is
a unique division D(s). For individual i, with location Zi , let Pi , Si , and Di, denote the puma,
state, and division associated with the location Zi . The distance d(z, z 0) between two locations
z and z 0 is defined as the shortest distance, in miles, on the earth’s surface connecting the two
points.3
     In this paper we focus primarily on estimating the slope coefficient β in a linear regression
of of some outcome Yi (e.g., individual level earnings) on an intervention Wi (e.g., a state level
regulation), of the form

       Yi = α + β · Wi + εi .                                                                                 (2.1)

The explanatory variable Wi may vary only at a more aggregate level than the outcome Yi . For
ease of exposition, and for some of the theoretical results, we focus on the case with Wi binary,
and varying at the state level, and we abstract from the presence of additional covariates.
   Let ε denote the N -vector with typical element εi , and let Y, W, P, C, and D, denote the
N vectors with typical elements Yi , Wi , Pi , Si , and Di . Let θ = (α, β)0 , Xi = (1, Wi), and X
and Z the N × 2 matrix with ith rows equal to Xi and Zi respectively, so that we can write in
matrix notation

     Y = ι · α + W · β + ε = Xθ + ε.                                                                          (2.2)
        P
Let N1 = N i=1 Wi , N0 = N − N1 , and W = N1 /N .
   2
     There    are      2,057     pumas,      49   states,    and    9    divisions  in   our    sample.         See
http://www.census.gov/geo/www/us regdiv.pdf for the definitions of the divisions.
   3
     Let z = (zlat , zlong ) be the latitude and longitude of a location. Then the formula for the distance in miles
between two locations z and z 0 we use is
       d(z, z 0) = 3, 959 × arccos(cos(zlong − zlong
                                                0                          0
                                                     ) · cos(zlat ) · cos(zlat                      0
                                                                               ) + sin(zlat) · sin(zlat )).




                                                               [3]
   We are interested in the distribution of the ordinary least squares estimator for β̂, equal to:
            PN
                 (Yi − Y ) · (Wi − W )
    β̂ols = i=1                        ,
                   (Wi − W )2
             P
where Y = N     i=1 Yi /N . For completeness let α̂ols be the least squares estimator for α, α̂ols =
Y − β̂ols · W .
    The starting point is the following model for the conditional distribution of Y given the
location Z and the covariate W:

Assumption 1. (Model)

        Y    W = w, Z = z ∼ N (ι · α + w · β, Ω(z)).

   Alternatively, we can write this assumption as

        ε   W = w, Z = z ∼ N (0, Ω(z)).

Under this assumption we can infer the exact (finite sample) distribution of the least squares
estimator, conditional on the covariates X, and the locations Z.

Lemma 1. (Distribution of Least Squares Estimator) Suppose Assumption 1 holds.
Then, (i), β̂ols is unbiased,
       h             i
     E β̂ols W, Z = β,                                                   (2.3)

and, (ii), its exact distribution is,

        β̂ols W, Z ∼ N (β, VM (W, Z)) ,                                                               (2.4)

where

        VM (W, Z) = VM (β̂ols|W, Z)
                                                                                             
                             1                              0                          W
              =          2              (W − 1)   ιN    W        Ω(Z)   ιN   W                    .
                  N 2 · W · (1 − W )2                                                    −1


    We write the variance VM (W, Z) as a function of W and Z to make explicit that this
variance is conditional on both the treatment indicators W and the locations Z. We refer to
this variance VM (W, Z) as the model-based variance for the least squares estimator.
    This lemma follows directly from the standard results on least squares estimation, and is
given without proof. Given Assumption 1, the exact distribution for the full vector of least
squares coefficients (α̂ols, β̂ols)0 is
                                                             
         α̂ols                       α       0
                                               −1  0
                                                           0 −1
                 X, Z ∼ N                 , XX     X Ω(Z)X X X      .                   (2.5)
         β̂ols                       β


                                                  [4]
We can then obtain (2.4) by writing out the component matrices of the joint variance of
(α̂ols , β̂ols)0 . The uncertainty in the estimates β̂ols arises from the randomness of the residu-
als εi postulated in Assumption 1. One interpretation of this randomness, perhaps the most
natural one, is to think of a large (infinite) population of individuals, so that we can resample
from the set of individuals with exactly the same location Zi and the same values for Wi . Al-
ternatively we can view the randomness arising from variation in the measures of the individual
level outcomes, e.g., measurement error.
     Finally, it will be useful to explicitly consider the variance of β̂ols, conditional on the locations
                           P
Z, and conditional on N      i=1 Wi = N1 , but not conditioning on the entire vector W. With some
abuse of language, we refer to this as the unconditional variance VU (Z) (although it is still
conditional on Z and N0 and N1 ). Because the conditional and unconditional expectation of
β̂ols are identical (and equal to β), it follows that the marginal (over W) variance is simply the
expected value of the conditional variance. Thus:

                                           N2                                    
      VU (Z) = E[VM (W, Z)|Z] =                  · E (W − N1 /N )0 Ω(W − N1 /N ) Z .                (2.6)
                                        N02    2
                                            · N1

There is typically no reason to use the unconditional variance. In fact, there are two reasons
what it would be inappropriate to use VU (Z) instead of VM (W, Z) to construct confidence
intervals. First, one would lose the exact nature of the distribution, because it is no longer
the case that the exact distribution of β̂ols is centered around β with variance equal to VU (Z).
Second, W is ancillary, and there are general arguments that inference should be conditional
on ancillary statistics (e.g., Cox and Hinkley, 1974).


3    Spatial Correlation Patterns in Individual Level Variables
In this section we show some evidence for the presence and structure of spatial correlations
in various individuals level variables, that is, how Ω varies with Z. We use data from the 5%
public use sample from the 2000 census. Our sample consists of 2,590,190 men at least 20,
and at most 50 years old, with positive earnings. We exclude individuals from Alaska, Hawaii,
and Puerto Rico (these states share no boundaries with other states, and as a result spatial
correlations may be very different than those for other states), and treats DC as a separate
state, for a total of 49 “states”. Table 1 and 2 present some summary statistics for the sample.
Our primary outcome variable is the logarithm of yearly earnings, in deviations from the overall
mean, denoted by Yi . The overall mean of log earnings is 10.17, the overall standard deviation is
0.97. We do not have individual level locations. Instead we know for each individual only puma
(public use microdata area) of residence, and so we take Zi to be the latitude and longitude of
the center of the puma of residence.
    First we look at simple descriptive masures of the correlation patterns separately within-
state, and out-of-state, as a function of distance. For a distance d (in miles), define the overall,
within-state, and out-of-state covariances as

      C(d) = E [ Yi · Yj | d(Zi , Zj ) = d] ,


                                                   [5]
      CS (d) = E [Yi · Yj | Si = Sj , d(Zi, Zj ) = d] ,
and

      CS (d) = E [Yi · Yj | Si 6= Sj , d(Zi, Zj ) = d] .

We estimate these using averages of the products of individual level outcomes for pairs of
individuals whose distance is within some bandwidth h of the distance d:
                    X
      [= 1 ·
      C(d)             1|d(Zi ,Zj )−d|≤h · Yi · Yj ,
             Nd,h
                           i<j

                     1           X
      \
      CS (d) =               ·         1Si =Sj · 1|d(Zi,Zj )−d|≤h · Yi · Yj ,
                 NS,d,h
                                 i<j

and
                     1       X
      \
      C                            1Si 6=Sj · 1|d(Zi ,Zj )−d|≤h · Yi · Yj ,
       S (d) =   NS,d,h
                             i<j

where
               X                                                      X
      Nd,h =         1|d(Zi ,Zj )−d|≤h ,             and NS,d,h =             1Si =Sj · 1|d(Zi ,Zj )−d|≤h ,
               i<j                                                     i<j

and
                 X
      NS,d,h =           1Si 6=Sj · 1|d(Zi ,Zj )−d|≤h .
                 i<j

Figures 1abc and 2abc show the covariance functions for two choices of the bandwidth, h = 20
and h = 50 miles, for the overall, within-state, and out-of-state covariances. If the standard
assumption of zero correlations between individuals in different states were true, one would
have expected the curve in out-of-state figures to be close to zero with no upward or downward
trend. The main conclusion from these figures is that out-of-state correlations are substantial,
and of a magnitude similar to the within-state correlations. Moreover, these correlations appear
to decrease with distance, as one would expect if there were out-of-state spatial correlations.
    Next, we consider various parametric structures that can be imposed on the covariance
matrix Ω(Z). Let Y be the variable of interest, log earnings in deviations from the overall
mean. Here we model the vector Y as

      Y|Z ∼ N (0, Ω(Z, γ)).

At the most general level, we specify the following form for the (i, j)th element of Ω, denoted
by Ωij :

      Ωij (Z, γ) = σε2 · 1i=j + σP2 · 1Pi =Pj + σS2 · 1Si =Sj + σD
                                                                 2              2
                                                                   · 1Di =Dj + σdist · exp(−α · d(Zi , Zj ))



                                                               [6]
                2                                 2 + σ2 + σ2 + σ2
               
                σdist · exp(−α · d(Zi, Zj )) + σD      S    P    ε              if   i = j,
               
               
                σdist · exp(−α · d(Zi, Zj )) + σD + σS2 + σP2
                   2                               2
                                                                                 if   i 6= j, Pi = Pj
             =     2
                 σdist · exp(−α · d(Zi, Zj )) + σD 2 + σ2                        if   Pi 6= Pj , Si = Sj
                                                        S
               
                  2                               2
                σ
                dist  · exp(−α · d(Zi , Zj )) + σ D                             if   Si 6= Sj , Di = Dj
                  2
                 σdist · exp(−α · d(Zi, Zj ))                                    if   Di 6= Dj .
In other words, we allow for clustering at the puma, state, and division level, with in addition
spatial correlation based on geographical distance, declining at an exponential rate. Note that
pumas and divisions are generally not associated with administrative jurisdictions. Findings of
correlations within such groupings are therefore not to be interpreted as arising from institu-
tional differences between pumas or divisions. As a result, it is unlikely that such correlations
in fact sharply decline at puma or division borders. The estimates based on models allowing for
within-puma and within-division correlations are to be interpreted as indicative of correlation
patterns that extend beyond state boundaries. What their precise interpretation is, for exam-
ple arising from geographical features, or from local labor markets, is a question not directly
addressed here.
                                                         2
    In this specification Ω(Z, γ) is a function of γ = (σD                2
                                                           , σS2 , σP2 , σdist , α, σε2)0 . As a result the
log likelihood function, leaving aside terms that do not involve the unknown parameters, is
                1
      L(γ|Y) = − ln (det(Ω(Z, γ))) − Y 0 Ω−1 (Z, γ)Y/2.
                2
The matrix Ω(Z, γ) is large in our illustrations, with dimension 2,590,190 by 2,590,190. Direct
maximization of the log likelihood function is therefore not feasible. However, because locations
are measured by puma locations, Ω(Z, γ) has a block structure, and calculations of the log
likelihood simplify and can be written in terms of first and second moments by puma.
    Table 3 gives maximum likelihood estimates for the covariance parameters γ based on the
log earnings data, with standard errors based on the second derivatives of the log likelihood
function. To put these numbers in perspective, the estimated value for α in the most general
                                                                  2
model, α̂ = 0.0468, implies that the pure spatial component, σdist    · exp(−α · d) dies out fairly
                              0                                                               2
quickly: at a distance d(z, z ) of about fifteen miles the spatial covariance due to the σdist    ·
                0
exp(−α · d(z, z )) component is half what it is at zero miles. The correlation for two individuals
in the same puma is 0.0888/0.9572 = 0.0928. To put this in perspective, the covariance between
log earnings and years of education is approximately 0.3, so the within-pum covariance is about
a third of the log earnings and education covariance. For two individual in the same state, but
in different puma’s and ignoring the spatial component, the total covariance is 0.0093. The
estimates suggest that much of what shows up as within-state correlations in a model that
incorporates only within-state correlations, in fact captures much more local, within-puma,
correlations.
    To show that these results are typical for the type of correlations found in individual level
economic data, we report in Tables 4, 5, and 6 results for the same model, and the same set
of individuals, for three other variables. First, in Table 4, we look at correlation patterns in
earnings residuals, based on a regression of log earnings on years of education, experience and
the square of experience, where experience is defined as age minus six minus years of education.
Next, in Table 5 we report results for years of education. Finally, in Table 6 we report results

                                                    [7]
for hours worked. In all cases puma-level correlations are a magnitude larger than within-
state, out-of-puma level correlations, and within-division correlations are of the same order of
magnitude as within-state correlations.
    The two sets of results, the covariances by distance and the model-based estimates of cluster
contributions to the variance, both suggest that the simple model that assumes zero covariances
between states is at odds with the data. Covariances vary substantially within states, and do
not vanish at state boundaries. The results do raise some questions regarding the interpre-
tation of the covariance structure. Whereas there are many regulations and institutions that
vary by state, there are no institutions associated with divisions of states, or with pumas. It is
possible that positive covariances between states within divisions arise from spatial correlations
in institutions. States that are close together, or states that share borders, may have similar
institutions. This could explain covariance structures that have discontinuities at state bound-
aries, but which still exhibit positive correlations extending beyond state boundaries. Within
puma correlations may arise from variation in local labor market conditions. Both explanations
make it unlikely that covariances change discontinuously at division or puma boundaries, and
therefore the models estimated here are at best approximations to the covariance structure.
We do not explore the interpretations for these correlations further here, but focus on their
implications for inference.
    To put these results in perspective, we look at the implications of these models for the
precision of least squares estimates. To make this specific, we focus on the model in (2.1),
with log earnings as the outcome Yi , and Wi equal to an indicator that individual i lives in a
state with a minimum wage that is higher than the federal minimum wage in the year 2000.
This indicator takes on the value 1 for individuals living in nine states, California, Connecticut,
Delaware, Massachusetts, Oregon, Rhode Island, Vermont, Washington, and DC (the state
minimum wage is also higher than the federal minimum wage in Alaska and Hawaii, but these
states are excluded from our basic sample).4 In the second to last column in Tables 3-6,
under the label “Min Wage,” we report in each row the standard error for β̂ols based on the
specification for Ω(Z, γ) in that row. To be specific, if Ω̂ = Ω(Z, γ̂) is the estimate for Ω(Z, γ)
in a particular specification, the standard error is
       q        p
         d
         V M =    VM (Z, γ̂)

                                                                                                !1/2
                            1                              0                              W
            =           2
                                       (W − 1)    ιN   W        Ω(Z, γ̂)   ιN   W                        .
                 N 2 · W · (1 − W )2                                                        −1

Despite the fact that the spatial correlations are quite far from being consistent with the con-
ventional specification of constant within-state and zero out-of-state correlations, the standard
errors for β̂ols implied by the various covariance specifications are fairly similar for the set of
models that incorporate state level correlations. For example, using the earnings variable, the
one specification that does not include state level clustering leads to a standard error of 0.001,
   4
     The data come from the website http://www.dol.gov/whd/state/stateMinWageHis.htm. Note that to be
consistent with the 2000 census, we use the information from 2000, not the current state of the law.



                                                 [8]
and the five specifications that do include state level clustering lead to a range of standard er-
rors from 0.068 to 0.091. Using the years of education variable, the one specification that does
not include state level clustering leads to a standard error of 0.004, and the five specifications
that do include state level clustering lead to a range of standard errors from 0.185 to 0.236.
In the next three sections we develop a theoretical argument to provide some insights into this
finding.


4    Randomization Inference with Complete Randomization
In this section we consider a different approach to analyzing the distribution of the least squares
estimator, based on randomization inference. See for a general discussion Rosenbaum (2002).
Recall the linear model given in equation (2.1),
      Yi = α + β · Wi + εi ,
combined with Assumption 1,
      ε|W, Z ∼ N (0, Ω(Z)).
In Section 2 we analyzed the properties of the least squares estimator β̂ols under repeated
sampling. To be precise, the sampling distribution for β̂ols was defined by repeated sampling
in which we keep both the vector of treatments W and the location Z fixed on all draws, and
redraw only the vector of residuals ε for each sample. Under this repeated sampling thought
experiment, the exact variance of β̂ols is, as given in Lemma 1:
                                                                                   
                             1                              0                W
     VM (W, Z) =           2
                                         (W − 1) ιN W Ω(Z) ιN W                        . (4.1)
                   N 2 · W · (1 − W )2                                           −1

We can use this variance in combination with normality to construct confidence intervals for β̂.
The standard 95% confidence interval is
                               p                   p  
     CI0.95 (β) = β̂ols − 1.96 · VM , β̂ols + 1.96 · VM .
With the true value for Ω in this expression for the confidence interval, this confidence interval
has the actual coverage equal to the nominal coverage. If we do not know Ω, we would substitute
a consistent estimator Ω̂ into this expression. In that case the confidence interval is approximate,
with the approximation becoming more accurate in large samples.
     It is possible to do construct confidence intervals in a different way, based on a different
repeated sampling thought experiment. Instead of conditioning on the vector W and Z, and
resampling the ε, we can condition on ε and Z, and resample the vector W. To be precise, let
Yi (0) and Yi (1) denote the potential outcomes under the two levels of the treatment Wi , and let
Y(0) and Y(1) denote the N -vectors of these potential outcomes. Then let Yi = Yi (Wi ) be the
realized outcome. We assume that the effect of the treatment is constant, Yi (1) − Yi (0) = β.
Defining α = E[Yi (0)], the residuals is εi = Yi − α − β · Wi .
     In this section we focus on the simplest case, where the covariate of interest Wi is completely
                                      P
randomly assigned, conditional on N     i=1 Wi = N1 . In the next section we look at the case where
Wi is randomly assigned to clusters. Formally, we assume

                                                [9]
Assumption 2. Randomization
                                                                          N
                                                                             X
                                          N
      pr(W = w|Y(0), Y(1), Z) = 1                  ,       for all w, s.t.         wi = N1 .
                                          N1
                                                                             i=1


    Under this assumption we can infer the exact (finite sample) variance for the least squares
estimator for β̂ols conditional on Z and (Y(0), Y(1)):
Lemma 2. Suppose Assumption 2 holds, and the treatment effect Yi (1) − Yi (0) = β is constant
for all individuals. Then (i), β̂ols is unbiased for β, conditional on Z and (Y(0), Y(1)),
         h                  i
      E β̂ols Y(0), Y(1), Z = β,                                                           (4.2)

and, (ii), its exact conditional (randomization-based) variance is
                                                                N                            
                                                       1 X                              1   1
      VR (Y(0), Y(1), Z) = V β̂ols   Y(0), Y(1), Z =       (εi − ε)2 ·                    +         , (4.3)
                                                     N −1                               N0 N1
                                                                  i=1
            PN
where ε =    i=1 εi /N .

     This follows from results by Neyman (1923) on randomization inference for average treat-
ment effects. In the Appendix we provide some details. Note that although the variance is
exact, we do not have exact normality, unlike the result in Lemma 1.
     In the remainder of this section we explore the differences between this conditional vari-
ance and the conditional variance given W and Z and its interpretation. We make four spe-
cific commments, interpreting the model and randomization based variances VM (W, Z) and
VR (Y(0), Y(1), Z), clarifying their relation, and the relative merits of both. First of all, al-
though both variances VM and VR are exact, the two are valid under different, complementary
assumptions. To see this, let us consider the bias and variance under a third repeated sam-
pling thought experiment, without conditioning on either W or ε, just conditioning on the
locations Z and (N0 , N1), maintaining both the model assumption and the randomization as-
sumption. In other words, we assume the existence of a large population. Each unit in the
population is characterized by two potential outcomes, (Yi (0), Yi(1)), with the difference con-
stant, Yi (1) − Yi (0) = β, and with the mean of Yi (0) in this infinite population equal to α.
Then, conditional on the N locations Z, we repeatedly randomly draw one unit from each the
set of units in each location, randomly assign each unit a value for Wi , and observe Wi , Zi , and
Yi (Wi ) for each unit.
Lemma 3. Suppose Assumptions 1 and 2 hold. Then (i), β̂ols is unbiased for β,
    h                i
   E β̂ols Z, N0 , N1 = β,                                                                            (4.4)

and, (ii), its exact unconditional variance is
                               1                           1
                                                                                
                                                                                   1   1
                                                                                         
 VU (Z) = V β̂ols Z, N0, N1 =              trace(Ω(Z)) −             ι0N Ω(Z)ιN ·    +     .
                                    N −1                 N · (N − 1)               N0 N1
                                                                                       (4.5)

                                               [10]
   Given Assumption 1, but without Assumption 2, the unconditional variance is given in
expression (2.6). Random assignment of W simplifies this to (4.5).
   By the law of interated expectations, it follows that
                  h                               i       h              i           
     VU (Z) = E V(β̂ols|Z, Y(0), Y(1)) Z, N0, N1 + V E β̂ols|Z, Y(0), Y(1) Z, N0 , N1
                 h                                 i
              = E V(β̂ols|Z, Y(0), Y(1)) Z, N0 , N1 ,

where the last equality follows because β̂ols is unbiased conditional on (Y(0), Y(1)) and Z. By
the same argument
                 h                 i      h             i        h               i
      VU (Z) = E V(β̂ols|Z, W) Z + V E β̂ols|Z, W Z = E V(β̂ols|Z, W) Z ,

because, again, β̂ols is unbiased, now conditional on Z and W. Hence, in expectation the two
variances, one based on randomization of W, and one based on resampling the (Y(0), Y(1)),
are both equal to the unconditional variance:

     VU (Z) = E [VR (Y(0), Y(1), Z)|Z, N0, N1 ] = E [VM (W, Z)|Z, N0, N1] .

In finite samples, however, VR (Y(0), Y(1), Z) will generally be different from VM (W, Z).
    The second point is that there is no reason to expect one of the confidence intervals to
have better properties. Because of ancillarity arguments one would expect the confidence in-
terval based on either of the conditional variances, either VR (Y(0), Y(1), Z) or VM (W, Z), to
perform better than confidence intervals based on the unconditional variance VU (Z), but we
are not aware of a general repeated sampling argument that would support a preference for
VR (Y(0), Y(1), Z) over VM (W, Z), or the other way around.
    Third, as a practical matter, the randomization variance VR (Y(0), Y(1), Z) is much easier
to estimate than the model-based variance VM (W, Z). Given the point estimates β̂ols and α̂ols ,
                          P
the natural estimator for i (εi − ε)2 /(N − 1) is

                1 X                       2
                             N
     σ̂ε2   =       Yi − α̂ols − β̂ols · Wi .
              N −1
                             i=1

This leads to
                                    
                             1   1
     V̂R =    σ̂ε2   ·         +         ,                                                 (4.6)
                             N0 N1

with corresponding confidence interval. Estimating the variance conditional on W and Z, on
the other hand is not easy. One would need to impose some structure on the covariance matrix
Ω in order to estimate it (or at least on X0 ΩX) consistently. The resulting estimator for the
variance will be sensitive to the specification unless Wi is randomly assigned. If, therefore,
one relies on random assignment of Wi anyway, there appears to be little reason for using the
model-based variance at all.



                                               [11]
   For the fourth point, suppose we had focused on the repeated sampling variance for β̂ols
conditional on W and Z, but under a model with independent and homoskedastic errors,

     ε|W, Z ∼ N (0, σ 2 · IN ).                                                              (4.7)

Under that independent and homoskedastic model (4.7), the exact sampling distribution for
β̂ols would be

        β̂ols W, Z ∼ N (β, VH (W, Z)) ,                                                      (4.8)

where
                                      
                      2        1   1
     VH (W, Z) = σ ·             +         .
                               N0 N1

Given this model, the natural estimator for the variance is
                                                             
               1 X                       2
                     N
                                                      1   1
     V̂H   =       Yi − α̂ols − β̂ols · Wi ·            +         .                          (4.9)
             N −1                                     N0 N1
                     i=1

This estimated variance V̂H is identical to the estimated variance under the randomization
distribution, V̂R given in (4.6). Hence, and this is the main insight of this section, if the
assignment W is completely random, and the treatment effect is constant, one can ignore
the off-diagonal elements of Ω(Z), and (mis-)specify Ω(Z) as σ 2 · IN . Although the resulting
variance estimator V̂H will not be estimating the variance under the repeated sampling thought
experiment that one may have in mind, conditional on Z and W, V̂H is consistent for the
variance under the randomization distribution, conditional on Z and (Y(0), Y(1), and for the
unconditional one.
    The result that the mis-specification of the covariance matrix need not lead to inconsistent
standard errors if the covariate of interest is randomly assigned has been noted previously.
Greenwald (1983) writes: “when the correlation patterns of the independent variables are un-
related to those across the errors, then the least squares variance estimates are consistent,” and
points out that the interpretation changes slightly, because the variance is no longer conditional
on the explanatory variables. Angrist and Pischke (2009) write, in the context of clustering,
that: “if the [covariate] values are uncorrelated within the groups, the grouped error structure
does not matter for standard errors.” The preceeding discussion interprets this result from a
randomization perspective.


5    Randomization Inference with Cluster-level Randomization
Now let us return to the setting that is the main focus of the current paper. The covariate of
interest, Wi , varies only between clusters, and is constant within clusters. Instead of assuming
that Wi is randomly assigned at the individual level, we now assume that it is randomly
assigned at the cluster level. Let M be the number of clusters, M1 the number of clusters with


                                               [12]
all individuals assigned Wi = 1, and M0 the number of clusters with all individuals assigned to
Wi = 0. The cluster indicator is
             
                 1       if individual i is in cluster m,
       Cim =
                 0       otherwise,

with C the N ×M matrix with typical element Cim . For randomization inference we condition on
Z, ε, and M1 . Let Nm be the number of individuals in cluster m. We now look at the properties
of β̂ols over the randomization distribution induced by this assignment mechanism. To keep the
notation clear, let W̃ be the M -vector of assignments at the cluster level, with typical element
                                                                               P
W̃m . Let Ỹ(0) and Ỹ(1) be M -vectors, with m-th element equal to Ỹm (0) = i:Cij =m Yi (0)/Nm,
               P
and Ỹs (1) = i:Cij =1 Yi (1)/Nm respectively. Similarly, let ε̃ be an M -vector with m-th element
                 P                            P
equal to ε̃m = i:Cij =1 εi /Nm , and let ε̃ = M m=1 ε̃m /M .
     Formally the assumption on the assignment mechanism is now:

Assumption 3. (Cluster Randomization)
                                                                M
                                                                   X
                                  M
     pr(W̃ = w̃|Z = z) = 1                 ,    for all w̃, s.t.         w̃m = M1 , and 0 otherwise.
                                  M1
                                                                   m=1



   We also consider the assumption that all clusters are the same size:

Assumption 4. (Equal Cluster Size) Nm = N/M for all clusters m = 1, . . . , M .

Lemma 4. Suppose Assumptions 3 and 4 hold, and the treatment effect Yi (1) − Yi (0) = β
is constant. Then the exact sampling variance of β̂ols, conditional on Z and ε, under the
randomization distribution is
                                                         M
                                                         X                      
                                                     1               2   1    1
    VCR (Y(0), Y(1), Z) = V(β̂ols|Y(0), Y(1), Z) =           ε̃m − ε̃ ·     +      . (5.1)
                                                   M −1                   M0 M1
                                                             m=1

    The proof is given in the Appendix. If we also make the model assumption (Assumption
1), we can derive the unconditional variance:

Lemma 5. (Marginal Variance Under Cluster Randomization)
Suppose that Assumptions 1, 3, and 4 hold. Then
                                                                         
                      M2               0
                                                    M       0        1   1
 VU (Ω(Z)) =                  · trace C Ω(Z)C − 2           ι Ω(Z)ι ·   +     . (5.2)
                N 2 · (M − 1)                   N · (M − 1)           M0 M1


    This unconditional variance is a special case of the expected value of the unconditional
variance in (2.6), with the expectation taken over W given the cluster-level randomization. A
special case of the result in this Lemma is that with each unit its own cluster, so that M = N ,
M0 = N0 , M1 = N1 , and C = IN , in which case the unconditional variance under clustering,
(5.2), reduces to the unconditional variance under complete randomization, (4.5).

                                               [13]
6    Variance Estimation Under Misspecification
In this section we present the main theoretical result in the paper. It extends the result in
Section 4 on the robustness of variance estimators ignoring clustering under complete random-
ization to the case where the model-based variance estimator accounts for clustering, but not
necessarily for all spatial correlations, under cluster randomization of the treatment.
    Suppose the model generating the data is the linear model in (2.1), with a general covariance
matrix Ω, and Assumption 1 holds. The researcher estimates a parametric model that imposes
a potentially incorrect structure on the covariance matrix. Let Σ(Z, γ) be the parametric model
for the error covariance matrix. The example we are most interested in is characterized by a
clustering structure. In that case Σ(Z, σε2, σC  2
                                                   ) is the N × N matrix with γ = (σε2 , σC
                                                                                          2 0
                                                                                            ) , where
                        2      2
                        σε + σC          if i = j
            2    2         2
      Σij (σε , σC ) =   σC               if i 6= j, Cim = Cjm , for all m = 1, . . ., M,        (6.1)
                       
                         0                otherwise.
Initially, however, we allow for any parametric structure Σ(Z, γ).
    Under the parametric model Σ(Z, γ), let γ̃ be the pseudo true value, defined as the value of
γ that maximizes the expectation of the logarithm of the likelihood function,
                                                             
                         1                  1     0    −1
      γ̃ = arg max E − · ln (det (Σ(γ))) − · Y Σ(γ) Y Z .
                γ        2                  2
Given the pseudo true error covariance matrix Σ(γ̃), the corresponding pseudo-true model-based
variance of the least squares estimator, conditional on W and Z, is
                                                                                           
                                    1                            0                  W
  VM (Σ(Z, γ̃), W, Z) =           2           (W −1)    ιN   W      Σ(Z, γ̃) ιN W             .
                          N 2 · W · (1 − W )2                                           −1

In general this pseudo-true conditional variance will differ from VM (W, Z) = VM (Ω, W, Z),
based on the correct error-covariance matrix Ω(Z). However, we need not have equality for
every value of W, if we have equality on average. Here we focus on the expected value of
VM (Σ(Z, γ̃), W, Z), conditional on Z, under assumptions on the distribution of W. Let us
denote this expectation by VU (Σ(Z, γ̃), Z) = E[VM (Σ(Z, γ̃), W, Z)|Z]. The question is under
what conditions on the specification of the error-covariance matrix Σ(Z, γ), in combination
with assumptions on the assignment process, this unconditional variance is is equal to the ex-
pected variance with the expectation of the variance under the correct error-covariance matrix,
VU (Ω, Z) = E[VM (Ω, W, Z)|Z].
    The following lemma shows that if the randomization of W is at the cluster level, then
solely accounting for cluster level correlations is sufficient to get valid confidence intervals.
Theorem 1. (Clustering with Misspecified Error-Covariance Matrix)
Suppose that Assumptions 1, 3, and 4 hold, and suppose that that Σ is specified as in (6.1)
Then

      VU (Σ(Z, γ̃), Z) = VU (Ω, Z).



                                                 [14]
     For the proof, see the Appendix. This is the main theoretical result in the paper. It implies
that if cluster level explanatory variables are randomly allocated to clusters, there is no need
to consider covariance structures beyond those that allow for cluster level correlations. The
limitation of the result to equal sized clusters does not play a major conceptual role, although
it is important for the specific results. One could establish similar results if the estimator was
the simple average of cluster averages, instead of the average of all individuals. The weighting of
cluster averages induced by the focus on the average over all individuals creates complications
for the Neyman framework. See Dylan, Ten Have and Rosenbaum (2008) for some discussion
of cluster randomized experiments.
     In many econometric analyses we specify the conditional distribution of the outcome given
some explanatory variables, and we pay no attention to the joint distribution of the explanatory
variables. The result in Theorem 1 shows that it may be useful to do so. Depending on the
joint distribution of the explanatory variables, the analyses may be robust to mis-specification of
particular aspects of the conditional distribution. In the next section we discuss some methods
for assessing the relevance of this result.


7    Spatial Correlation in State Averages
The results in the previous sections imply that inference is substantially simpler if the ex-
planatory variable of interest is randomly assigned, either at the unit or cluster level. Here
we discuss tests originally introduced by Mantel (1967) (see, e.g., Schabenberger and Gotway,
2004) to analyze whether random assignment is consistent with the data, against the alter-
native hypothesis of some spatial correlation. These tests allow for the calculation of exact,
finite sample, p-values. To implement these tests we use the location of the units. To make
the discussion more specific, we test the random assignment of state-level variables against the
alternative of spatial correlation.
    Let Ys be the variable of interest for state s, for s = 1, . . ., S, where state s has location Zs
(the centroid of the state). In the illustrations of the tests we use an indicator for a state-level
regulation, or the average of individual level outcomes, e.g., the logarithm of earnings, years
of education, or hours worked per week. The null hypothesis of no spatial correlation in the
Ys can be formalized as stating that conditional on the locations Z, each permutation of the
values (Y1 , . . . , YS ) is equally likely. With S states, there are S! permutations. We assess the
null hypothesis by comparing, for a given statistic M (Y, Z), the value of the statistic given the
actual Y and Z, with the distribution of the statistic generated by randomly permuting the Y
vector.
    The tests we focus on in the current paper are based on Mantel statistics (e.g., Mantel,
1967; Schabenberger and Gotway, 2004). These general form of the statistics we use is a
proximity-weighted average of squared pairwise differences:
                   S−1
                   X     S
                         X
      M (Y, Z) =               (Ys − Yt )2 · dst,                                               (7.1)
                   s=1 t=s+1

where dst = d(Zs , Zt) is a non-negative weight monotonically related to the proximity of the

                                                    [15]
states s and t.
    Given a statistic, we test the null hypothesis of no spatial correlation by comparing the value
of the statistic in the actual data set, M obs , to the distribution of the statistic under random
permutations of the Ys . The latter distribution is defined as follows. Taking the S units, with
values for the variable Y1 , . . . , YS , we randomly permute the values Y1 , . . . , YS over the S units.
For each of the S! permutations m we re-calculate the Mantel statistic, say Mm . This defines
a discrete distribution with S! different values, one for each allocation. The one-sided exact
p-value is defined as the fraction of allocations m (out of the set of S! allocations) such that
the associated Mantel statistic Mm is less than or equal to the observed Mantel statistic M obs:
              S!
           1 X
      p=         1M obs ≥Mm .                                                                       (7.2)
           S!
              m=1

A low value of the p-value suggests rejecting the null hypothesis of no spatial correlation in the
variable of interest. In practice the number of allocations is often too large to calculate the exact
p-value. In that case we approximate the p-value by drawing a large number of allocations, and
calculating the proportion of statistics less than or equal to the observed Mantel statistic. In
the calculations below we use 10, 000, 000 draws from the randomization distribution.
    We use six different measures of proximity. First, we define the proximity dst as states s
and t sharing a border:
             
       B        1         if s, t share a border,
      dst =                                                                                      (7.3)
                0         otherwise.

Second, we define dst as an indicator for states s and t belonging to the same census division
of states (recall that the US is divided into 9 divisions):
             
               1         if Ds = Dt ,
      dD
       st =                                                                              (7.4)
               0         otherwise.

Third, we define proximity dst as minus the geographical distance between states s and t:

      dG
       st = −d (Zs , Zt ) ,                                                                         (7.5)

where d(z, z 0) is the distance in miles between two locations z and z 0 , and Zs is the latitude
and longitude of state s, measured as the latitude and longitude of the centroid for each state.
   The last three proximity measures are based on transformations of geographical distance:

      dαst = exp (−α · d (Zs , Zt)) ,                                                               (7.6)

for α = 0.00138, α = 0.00276, and α = 0.00693. For these values the proximity index declines
by 50% at distances of 500, 250, and 100 miles.
    We calculate the p-values for the Mantel test statistic based on five variables. First, an
indicator for having a state minimum wage higher than the federal minimum wage. This
indicator takes on the value 1 in eleven out of the forty nine states in our sample, with these
eleven states mainly concentrated in the North East. Second, the average of the logarithm

                                                   [16]
of yearly earnings. Third, average years of education. Fourth, average hours worked. Fifth,
average weeks worked. The results for the five variables and three statistics are presented in
Table 7.
    All five variables exhibit considerable spatial correlation. Interestingly the results are fairly
sensitive to the measure of proximity. From these limited calculations, it appears that sharing
a border is a measure of proximity that is sensitive to the type of spatial correlations in the
data.


8    Does the Spatial Correlation Matter?
The theoretical results in Section 6 show that spatial correlations beyond state borders do not
matter if the interventions of interest are randomly assigned to states. The empirical results
in Section 7 suggest that there are in fact statistically significant spatial correlation patterns
in state-level regulations. Hence the results in Section 6 need not apply in practice. Here we
assess the practical implications of the combination of these findings.
    We focus on two examples. First, we return to the explanatory variable we looked at in
Section 3, the indicator whether a state has a minimum wage exceeding the federal level. Nine
states in our sample fit that criteria, and in the previous section it was shown that this indicator
variable exhibits a statistically significant degree of spatial clustering. Second, we consider an
artificial intervention, applying only to the six states making up the New England division and
the five states making up the East North Central division (so that we again have 11 states with
the intervention). The last two columns in Tables 3-6 present standard errors under different
specifications for the error covariance matrix. We find that although the minimum wage variable
exhibits substantial spatial correlation, allowing for within-state correlation leads to standard
errors that are fairly close to those for more general models. On the other hand, with the
artificial regulation applying only to NE/ENC states, ignoring division level correlations leads
to underestimation of the standard errors almost by a substantial amount. For the earnings
outcome, the range of values for standard errors within specifications for the variance that all
allow for state-level clustering is [0.494, 0.854], and for years of education it the range of standard
errors [0.136, 0.227]. Thus, if the state level regulation is clustered to a substantial degree, taking
into account between state correlations for the outcomes is important for inference.


9    Conclusion
In empirical studies with individual level outcomes and state level explanatory variables, re-
searchers often calculate standard errors allowing for within-state correlations between individual-
level outcomes. In many cases, however, the correlations may extend beyond state boundaries.
Here we explore the presence of such correlations, and investigate the implications of their
presence for the calculation of standard errors. In theoretical calculations we show that under
some conditions, including random assignment of regulations, correlations in outcomes between
individuals in different states can be ignored. However, state level variables often exhibit consid-
erable spatial correlation, and depending on the form of that correlation, using a more flexible


                                                 [17]
specification of the error covariance structure may be important.
    In practice we recommend that researchers explicitly explore the spatial correlation structure
of both the outcomes as well as the explanatory variables. Statistical tests based on Mantel
statistics, with the proximity based on shared borders, or belonging to a common division, are
straightforward to calculate and lead to exact p-values. If these test suggest that both outcomes
and explanatory variables exhibit substantial spatial correlation, one should explicitly account
for the spatial correlation, either by allowing for a more flexible specification than one that only
accounts for state level clustering, or for using robust variance estimators that allow for general
spatial correlation structures by relying more heavily on large sample approximations, using,
for exampe, the methods discussed in Conley (1999), Bester, Conley and Hansen (2009), and
Ibragimov and Müller (2009).




                                                [18]
                                                               Appendix

We first give a couple of preliminary results.

Theorem A.1. (Sylvester’s Determinant Theorem) Let A and B be arbitrary M × N matrices.
Then:

      det(IN + A0 B) = det(IM + BA0 )



For a proof see XXXX
Proof of Theorem A.1:                        
Consider a block matrix M
                        M3
                          1         M2
                                    M4        . Then:
                                                                       
              M1 M2               M1 0                 I    M1−1 M2
      det     M3 M4       = det   M3 I       det       0 M4 −M3M1−1 M2
                                                                              = det M1 det(M4 − M3 M1−1 M2 )
            similarly
                                                                       
              M1 M2               I M2                 M1 −M2 M4−1 M3 0
      det     M3 M4       = det   0 M4       det          M4−1 M3     I
                                                                              = det M4 det(M1 − M2 M4−1 M3 )

Letting M1 = IM , M2 = −B, M3 = A0 , M4 = IN yields the result. 

Lemma A.1. (Determinant of Cluster Covariance Matrix) Suppose C is an N × M matrix
of binary cluster indicators, with C0 C equal to a M × M diagonal matrix, Σ is an arbitrary M × M
matrix, and IN is the N -dimensional identity matrix. Then, for scalar σε2 , and

      Ω = σ2 IN + CΣC0                  ΩC = Σ + σ2 (C0 C)−1 ,

we have

      det(Ω) = (σ2 )N−M det(C0 C) det(ΩC ).



Proof of Lemma A.1: By Sylvester’s theorem:

      det(Ω) = (σ2 )N det(IN + CΣ/σ2 C0 )
                = (σ2 )N det(IM + C0 CΣ/σ2 ) = (σ2 )N det(IM + C0 CΩC /σ2 − IM )
                = (σ2 )N det(C0 C) det(ΩC /σ2 )
                             Y 
                = (σ2 )N−M       Np det(ΩC ).



Lemma A.2. (Neyman) Suppose we have N triples (Wi , Yi (0), Yi (1), with Wi ∈ {0, 1}. Define
               N
             1 X
      β=         (Yi (1) − Yi (0)) ,
             N
                i=1

                   N                                         N
                1 X                                       1 X
      Y (0) =         Yi (0),                Y (1) =            Yi (1),
                N i=1                                     N i=1
                     N                                                       N
                1 X                   2                                1 X                   2
      S02 =             Yi (0) − Y (0)                        S12 =             Yi (1) − Y (1) ,
              N − 1 i=1                                               N − 1 i=1


                                                                   [19]
                       N
        2         1 X                                    2
       S01 =              Yi (1) − Yi (0) − Y (1) − Y (0)    ,
                N − 1 i=1
               N
               X                           N
                                           X
       N1 =          Wi ,        N0 =            (1 − Wi )
               i=1                         i=1
and
                  N                    N
              1 X                  1 X
       β̂ =          Wi · Yi (1) −        (1 − Wi ) · Yi (0).
              N1 i=1               N0 i=1
                                                                                    PN
Suppose that Wi is randomly assigned to the N units, subject to                       i=1   Wi = N1 . Then
       h              i
     E β̂|Y(0), Y(1) = β,

and
                                 S02  S2  S2
       V(β̂|Y(0), Y(1)) =            + 1 − 01 .
                                 N0   N1   N
Lemma A.3. (i), Suppose Assumption 2 holds, then for any N × N matrix Ω,
                            N1 · (N1 − 1) 0          N1 · N0
       E [W0 ΩW] =                       · ιN ΩιN +             · trace(Ω),
                            N · (N − 1)             N · (N − 1)
and (ii), suppose Assumptions 3 and 4 hold, then for any N × N matrix Ω,
                            M1 · (M1 − 1) 0          M1 · M0
       E [W0 ΩW] =                       · ιN ΩιN +             · trace (C0 ΩC) .
                            M · (M − 1)             M · (M − 1)
Proof of Lemma A.3: First, consider part (i): Because
                    N1
                    N               if i = j,
     E[Wi · Wj ] =
                    N1 ·(N1 −1)
                      N·(N−1)
                                     if i 6= j,

it follows that
                                                                               
                N1 · (N1 − 1)                              N1   N1 · (N1 − 1)
       E[WW ] =  0
                              · ιN ι0N +                      −                     · IN
                N · (N − 1)                                N    N · (N − 1)
                     N1 · (N1 − 1)             N1 · N0
               =                   · ιN ι0N +             · IN .
                     N · (N − 1)              N · (N − 1)
Thus

       E[W0 ΩW] = trace (E[ΩWW0 ])
                                                                  
                          N1 · (N1 − 1)             N1 · N0
             = trace Ω ·                · ιN ι0N +             · IN
                           N · (N − 1)             N · (N − 1)
                     N1 · (N1 − 1) 0          N1 · N0
               =                  · ιN ΩιN +             · trace(Ω).
                     N · (N − 1)             N · (N − 1)
Next, consider part (ii). Now
                     M1
                     M                                if ∀m, Cim = Cjm,
      E[Wi · Wj ] =
                     M1·(M1 −1)
                               M ·(M −1)               otherwise.

                                                                [20]
it follows that
                                                                          
                      M1 · (M1 − 1)                   M1   M1 · (M1 − 1)
       E[WW0 ] =                    · ιN ι0N +           −                     · CC0
                      M · (M − 1)                     M    M · (M − 1)
                   M1 · (M1 − 1)             M1 · M0
               =                 · ιN ι0N +             · CC0 .
                   M · (M − 1)              M · (M − 1)
Thus

       E[W0 ΩW] = trace (E[ΩWW0 ])
                                                                  
                          M1 · (M1 − 1)       0    M1 · M0        0
             = trace Ω ·                · ιN ιN +             · CC
                           M · (M − 1)            M · (M − 1)
                   M1 · (M1 − 1) 0          M1 · M0
               =                · ιN ΩιN +             · trace (C0 ΩC) .
                   M · (M − 1)             M · (M − 1)

Lemma A.4. Suppose the N × N matrix Ω satisfies

       Σ = σε2 · IN + σC
                       2
                         · CC0 ,

where IN is the N × N identity       matrix, and C is an N × M matrix of zeros and ones, with CιM = ιN
and C0 ιN = (N/M )ιM , so that,
             2      2
             σε + σC                if i = j
                 2
      Ωij =    σ                     if i 6= j, ∀m, Cim = Cjm ,                                  (A.1)
             C
               0                     otherwise,

Then, (i)
                                                  
                                                2
                                              N σC
       ln (det (Ω)) = N · ln σε2 + M · ln 1 +  · 2 ,
                                              M σε
and, (ii)
                                         2
                                        σC
       Ω−1 = σε−2 · IN −                   2 · N/M ) · CC
                                                         0
                            σε2 · (σε2 + σC
or,
                                      2
                                     σC
                    σε − σε2 ·(σε2 +σS2 ·N/M )
                      −2
                                                           if i = j
             
          Ω−1 ij =              σ2
                    − σ2 ·(σ2 +σC2 ·N/M )                 if i 6= j, ∀m, Cim = Cjm,
                   
                    0  ε    ε    C
                                                           otherwise,

Proof of Lemma A.4: First, consider the first part. Apply Lemma A.1 with

            2                               N
       Σ = σC · IM ,          and C0 C =      · IM ,
                                            M
so that
                         
              2     2 M
       ΩS = σ S + σ ε ·     · IM .
                        N
Then, by Lemma A.1, we have

       ln det(Ω) = (N − M ) · ln(σε2 ) + M · ln(N/M ) + ln det(ΩC )

                                                          [21]
                                                                        
                                                                   2 M
               = (N − M ) ·    ln(σε2 )                       2
                                     + M · ln(N/M ) + M · ln σC + σε ·
                                                                       N
                                                         
                                                N 2
               = (N − M ) · ln(σε2 ) + M · ln     σ + σε2
                                                M C
                                                 2
                                                    
                                             N σC
               = N · ln(σε2 ) + M · ln 1 +      · 2 .
                                            M σε
Next, consider part (ii). We need to show that
                                                                     
        2         2       0
                             −2                    2
                                                   σC               0
       σε · IN + σC · CC      σε · IN − 2             2 · N/M ) · CC    = IN ,
                                        σε · (σε2 + σC

which amounts to showing that

                    σε2 · σC
                           2
                                      0     2     0 −2     0
                                                                            4
                                                                           σC
      −                   2       · CC  + σ C · CC σ ε − CC  ·                2         · CC0 = 0.
          σε2 · (σε2 + σC · N/M )                              σε2 · (σε2 + σC  · N/M )

This follows directly from the fact that C0 C = (N/M ) · IM and collecting the terms. 
Proof of Lemma 2: By Assumption 2, we can apply Lemma A.2. This directly implies the unbiasedness
result. It also implies that the conditional variance is
                           S2  S2  S2
      V β̂ols Y(0), Y(1), Z = 0 + 1 − 01 .
                             N0  N1   N
                                                                   2
The assumption that the treatment effect is constant implies that S01 = 0, and that εi − ε = Yi (0) −
Y (0) = Yi (1) − Y (1), which in turn implies that
                           PN (ε − ε)2 /(N − 1) PN (ε − ε)2 /(N − 1)
                                  i                    i
      V β̂ols Y(0), Y(1), Z = i=1                + i=1
                                    N0                   N1
                           N                           
                   1 X             2          1    1
               =           (εi − ε) ·            +          .
                 N − 1 i=1                    N0   N1

Proof of Lemma 3: The unbiasedness result directly follows from the conditional unbiasedness estab-
lished in Lemma 2. To establish the second part of the Lemma, we need to show that the expectation
of the conditional variance is equal to the marginal variance, or:
   "        N                                   #                                                    
       1 X             2    1     1                       1                      1                1    1
E              (εi − ε) ·      +        Z, N0 , N1 =          trace(Ω(Z)) −             0
                                                                                       ι Ω(Z)ιN ·    +      ,
     N −1                   N0    N1                   N −1                 N · (N − 1) N         N0   N1
            i=1

or, equivalently,
         " N                     #
          X           2                       1
      E       (εi − ε) Z, N0 , N1 = trace(Ω) − ι0N Ω(Z)ιN ,                                          (A.2)
          i=1
                                              N

where E[εε0 |Z] = Ω(Z). First,
      N
      X             2
            (εi − ε) = (ε − ιN ι0N ε/N )0 (ε − ιN ι0N ε/N )
      i=1

               = ε0 ε − 2ε0 ιN ι0N ε/N + ε0 ιN ιN ιN ι0N ε/N 2
               = ε0 ε − ε0 ιN ι0N ε/N.

                                                            [22]
Thus
            "   N
                                              #
                X                2
        E             (εi − ε) Z, N0 , N1 = E [ ε0 ε − ε0 ιN ι0N ε/N | Z, N0 , N1 ]
                i=1

                  = trace (E [ εε0 − ι0N εε0 ιN /N | Z, N0 , N1 ])
                  = trace (Ω(Z)) − ι0N Ω(Z)ιN /N,
which proves (A.2), and thus the result in the Lemma. 
Proof of Lemma 4: Applying Lemma A.2 to the clusters implies that
       h                  i
      E β̂ols Ỹ(0), Ỹ(1) = β,

and
                             S̃ 2 S̃ 2 S̃ 2
        V β̂ols |Ỹ(0), Ỹ(1) = 0 + 1 − 01 ,
                               S0   S1    S
where

                    1 X                    2                        1 X                    2
                         S                                                 S
        S̃02 =              Ỹs (0) − Ỹ (0) ,             S̃12 =             Ỹs (1) − Ỹ (1) ,
                  S − 1 s=1                                         S − 1 s=1

and

                        1 X                                      2
                             S
          2
        S̃01 =                  Ỹs (1) − Ỹs (0) − Ỹ (1) − Ỹ (0)    .
                      S − 1 s=1

                                                                   2
                                                                                                PS
Under the constant treatment effect assumption, it follows that S̃01 = 0, and that S̃02 = S̃12 = s=1 (ε̃s −
ε̃s )2 /(S − 1), and the result follows. 
Proof of Lemma 5: For proving the claim in the Lemma it is sufficient to show that the expectation
of the conditional variance is equal to the marginal variance, or:
           "         M                                        #
                1   X            2     1    1
         E               ε̃m − ε̃ ·        +      Z, C, N0 , N1
             M − 1 m=1                 M0    M1
                                                                                         
                                  M2                0             M       0         1    1
                  =                       · trace (C Ω(Z)C) − 2          ι Ω(Z)ι ·     +      ,
                            N 2 · (M − 1)                    N · (M − 1)            M0   M1
or, equivalently,
         " M                         #                                  
          X            2                M2                    M 0
      E        ε̃s − ε̃ Z, C, N0 , N1 =               0
                                            · trace (C Ω(Z)C) − 2 ι Ω(Z)ι .                          (A.3)
          s=1
                                         N2                    N

Note that in general

        CιM = ιN ,

and under Assumption 4, it follows that
                       N
        C0 C =           · IM .
                       M
We can write
                            −1            M 0
        ε̃s = (C0 C)             C0 ε =     C ε,
                                          N

                                                               [23]
and
               1 0        −1     1
       ε̃ =      ιM (C0 C) C0 ε = ι0N ε,
               M                 N
so that
       M
       X                                                0                   
                           2            M 0   1              M 0    1
                ε̃s − ε̃        =          Cε−   ιM ι0N ε       C ε−   ιM ι0N ε
       m=1
                                         N     M              N      M
                                    0                
                       M 0   1               M 0   1
                 =        C − ιM ι0N ε          C − ιM ι0N ε .
                        N    N                N    N
                                    0              
                        M    1            M 0    1
                 = ε0     C − ιN ι0M        C − ιM ι0N ε.
                        N    N            N     N
Thus
           "   M
                                                    #       "                         0                                          #
               X                2                                      M    1                M 0  1
       E             ε̃s − ε̃           Z, C, N0 , N1 = E ε     0
                                                                          C − ιN ι0M            C − ιM ι0N         ε Z, C, N0 , N1
               m=1
                                                                        N    N                N    N
                                        "                  0                                              #!
                                             M    1                 M 0  1
                 = trace E                     C − ιN ι0M             C − ιM ι0N             0
                                                                                           εε Z, C, N0 , N1
                                             N    N                 N    N
                                                       0                                !
                                        M    1                  M 0  1
                 = trace                  C − ιN ι0M              C − ιM ι0N         Ω(Z)
                                        N    N                  N    N
                                                                                     0 !
                                        M 0  1                           M    1
                 = trace                  C − ιM ι0N        Ω(Z)           C − ιN ι0M
                                        N    N                           N    N
                               2
                         M                          M 0
                 =                   · C0 Ω(Z)C −     · ι Ω(Z)ιN .
                         N                          N2 N

Proof of Theorem 1: For equality of VU (Σ) and VU (Ω) we need equality of trace(C0 ΩC) and
trace(C0 Σ(σ̃ε , σ̃S2 )C).
The log likelihood function based on the specification (A.1) is
                            1                    1
       L(σε2 , σC
                2
                  |Y, Z) = − · ln Σ Z, σε2 , σC
                                              2
                                                  − · Y0 Σ(σε2 , σS2 )−1 Y.
                            2                      2
The expected value of the log likelihood function is
                              1                 1                      
      E L(σε2 , σC
                 2
                   |Y, Z) Z = − ln Σ Z, σε2 , σC
                                               2
                                                   − · E Y0 Σ(σε2 , σC2 −1
                                                                       ) Y
                                2                    2
                  1                 1           h                    i
                                                                2 −1
           = − · ln Σ Z, σε2 , σC2
                                      − · trace E Σ Z, σε2 , σC      YY0
                  2                    2
                  1                  1                         
                                                             2 −1
           = − · ln Σ Z, σε2 , σC2
                                      − · trace Σ Z, σε2 , σC     Ω .
                  2                    2
Using Lemma A.4, this is equal to
                        1          M                                               2
                                                                                     σC
E L(σε2 , σS2 |Y, Z) Z = − ·N ·(σε )− ·ln 1 + Nm · σC
                                                    2
                                                      /σε2 −σε−2 ·trace(Ω)+ 2           2 ·N )
                                                                                               ·trace (C0 ΩC) .
                          2          2                                     σε · (σε2 + σC   m




                                                                        [24]
                                                                                        2
The first derivative of the expected log likelihood function with respect to σC            is
                                                                                                                                 
 ∂          2     2
                                        S · Ns /σε2                                      1                  (N/M ) · σC2
                                                                                                                          · σε2
   2 E   L(σ ε , σ C |Y, Z) Z  = −                 2        +trace (C0
                                                                       ΩC)·                   2       −              2
∂σC                                2 · (1 + Ns · σC  /σε2 )                   σε2 · (σε2 + σC   · Nm ) σε4 · (σε2 + σC · (N/M ))2

                                N                      0                 1
              =−                         2 ) + trace (C ΩC) · (σ 2 + σ 2 · (N/M ))2 .
                    2 · (σε2 + (N/M ) · σC                      ε     C
                                      2
Hence the first order condition for σ̃C implies that

       trace (C0 ΩC) = N · (σε2 + σC
                                   2
                                     · (N/M )).

For the misspecified error-covariance matrix Σ we have
                          M
                          X
                                2    2
                                                 
                0
       trace (C ΣC) =          Nm · σC + Nm · σε2 .
                         m=1

By equality of the cluster sizes this simplifies to
                                                                             
       trace (C0 ΣC) = M · (N/M )2 · σC
                                      2
                                        + (N/M ) · σε2 = N · σε2 + σC
                                                                    2
                                                                      · (N/M ) .






                                                       [25]
                                            References

Abadie, A., A. Diamond, and J. Hainmueller, (2009), “Synthetic Control Methods for Compara-
   tive Case Studies: Estimating the Effect of California’s Tobacco Control Program,” forthcoming,
   Journal of the American Statistical Association.
Angrist, J., and S. Pischke, (2009), Mostly Harmless Econometrics, Princeton University Press,
   Princeton, NJ.
Anselin, L., R. Florax, and S. Rey, (2004), Advances in Spatial Econometrics, Springer.
Arbia, G., (2006), Spatial Econometrics: Statistical Foundations and Applications to Regional Con-
    vergence, Springer.
Bester, C., T. Conley, and C. Hansen, (2009), “Inference with Dependent Data Using Clustering
    Covariance Matrix Estimators,” Unpublished Manuscript, University of Chicago Business School.
Bertrand, M., E. Duflo, and S. Mullainathan, (2004), “How Much Should We Trust Difference-
    in-Differences Estimates,” Quarterly Journal of Economics, 119: 249-275.
Black, S., (1999), “Do Better Schools Matter? Parental Valuation of Elementary Education,”Quarterly
    Journal of Economics, Vol. CXIV, 577.
Conley, T., (1996) “Econometric Modelling of Cross-Sectional Dependence,” Ph.D. Dissertation,
   University of Chicago.
Conley, T., (1999), “GMM Estimation with Cross-sectional Dependence,” Journal of Econometrics,
   92: 1-45.
Cox, D., and D. Hinkley, (1974), Theoretical Statistics, Chapman and Hall.
Cressie, N., (1993), Statistics for Spatial Data, Wiley.
Donald, S. and K. Lang, (2007), “Inference with Difference in Differences and Other Panel Data,”Review
   of Economics and Statistics, Vol. 89(2): 221-233.
Fisher, R. A., (1925), The Design of Experiments, 1st ed, Oliver and Boyd, London.
Greenwald, B.., (1983), “A General Analysis of the Bias in Estimated Standard Errors of Least
   Squares Coefficients,” Journal of Econometrics, 22, 323-338.
Hansen, C., (2009), “Generalized Least Squares Inference in Panel and Multilevel Models with Serial
   Correlation and Fixed Effects,” Journal of Econometrics, 140(2), 670-694.
Ibragimov, R., and U. Müller, (2009), “t-statistic based correlation and heterogeneity robust
    inference” Unpublished Manuscript, Department of Economics, Harvard University.
Imbens, G., and T. Lemieux, (2008) “Regression Discontinuity Designs: A Guide to Practice,”
    Journal of Econometrics Vol 142(2):615-635.
Kloek, T., (1981), “OLS Estimation in a Model where a Microvariable is Explained by Aggregates
   and Contemporaneous Disturbances are Equicorrelated,” Econometrica, Vol. 49, No. 1, 205-207.
Lee, D., and T. Lemieux, (2009), “Regression Discontinuity Designs in Economics,” Working Paper,
    Dept of Economics, Princeton University.
Liang, K., and S. Zeger, (1986), “Longitudinal Data Analysis Using Generalized Linear Models,”
    Biometrika, 73(1): 13-22.
Mantel, N., (1967), “The Detection of Disease Clustering and a Generalized Regression Approach,”
   Cancer Research, 27(2):209-220.
Moulton, B., (1986), “Random Group Effects and the Precision of Regression Estimates,” Journal
   of Econometrics, 32, 385-397.

                                                [26]
Moulton, B., (1990), “An Illustration of a Pitfall in Estimating the Effects of Aggregate Variables
   on Micro Units,” Review of Economics and Statistics, 334-338.
Moulton, B., and W. Randolph, (1989) “Alternative Tests of the Error Component Model,”
   Econometrica, Vol. 57, No. 3, 685-693.
Neyman, J., (1923), “On the Application of Probability Theory to Agricultural Experiments. Essay
   on Principles. Section 9,”translated in Statistical Science, (with discussion), Vol 5, No 4, 465–480,
   1990.
Rosenbaum, P., (2002), Observational Studies, 2nd edition, Springer Verlag, New York.
Ruggles, S., M. Sobek, T. Alexander, C. Fitch, R. Goeken, P. Hall, M. King, and C.
   Ronnander, (2008), “Integrated Public Use Microdata Series: Version 4.0 [Machine-readable
   database]” Minneapolis, MN: Minnesota Population Center [producer and distributor].
Schabenberger, O., and C. Gotway, (2004), Statistical Methods for Spatial Data Analysis, Chap-
    man and Hall.
Small, D., T. Ten Have, and P. Rosenbaum., (2008), “Randomization Inference in a Group-
   Randomized Trial of Treatments for Depression: Covariate Adjustment, Noncompliance, and
   Quantile Effects,” Journal of the American Statistical Association, 103(481), 271-279.
William S. (1998), Comparison of standard errors for robust, cluster, and standard estimators, Stat-
    aCorp http://www.stata.com/support/faqs/stat/cluster.html
Wooldridge, J., (2002), Econometric Analysis of Cross Section and Panel Data, MIT Press, Cam-
   bridge, MA.




                                                 [27]
                      Table 1: Summary Statistics

                                 log earnings   years of educ   hours worked

Average                             10.17          13.05           43.76
Stand Dev                           0.97           2.81            11.00

Average of PUMA Averages            10.17          13.06           43.69
Stand Dev of PUMA Averages          0.27           0.95            1.63

Average of State Averages           10.14          13.12           43.94
Stand Dev of State Averages         0.12           0.33            0.75

Average of Division Averages        10.17          13.08           43.80
Stand Dev of Division Averages      0.09           0.31            0.48



                          Table 2: Sample Sizes


  Number of observation in the sample                            2,590,190

  Number of PUMAs in the sample                                     2,057
  Average number of observations per PUMA                           1,259
  Standard deviation of number of observations per PUMA               409

  Number of states (incl DC, excl AK, HA, PR) in the sample            49
  Average number of observations per state                         52,861
  Standard deviation of number of observations per state           58,069

  Number of divisions in the sample                                     9
  Average number of observations per division                     287,798
  Standard deviation of number of observations per division       134,912




                                    [28]
Table 3: Maximum likelihood Estimates for Clustering Variances for Demeaned Log Earnings

   σε2          2
               σD           σS2             σP2        2
                                                      σdis                a         LLH              c β̂)
                                                                                                    s.e.(
                                                                                              Min Wage NE/ENC

 0.9388         0              0              0          0                0     1213298.132    0.00145    0.0015
[0.0008 ]

 0.9294         0         0.01610             0          0                0     1200406.963    0.07996    0.0568
[0.0008 ]                [0.0018 ]

 0.8683         0         0.0111        0.0659           0                0     1116976.379    0.06794    0.0494
[0.0008 ]                [0.0029]      [0.0022]

 0.9294      0.0056       0.0108              0          0                0     1200403.093    0.09091    0.0810
[0.0008 ]   [0.0020 ]    [0.0020 ]

 0.8683      0.0056       0.0058        0.0660           0                0     1116971.990    0.08054    0.0760
[0.0008 ]   [0.0033 ]    [0.0021]      [0.0021]

 0.8683      0.0080       0.0008        0.0331       0.0324          0.0468     1603400.922    0.08602    0.0854
[0.0008 ]   [0.0049]     [0.0012]      [0.0021]     [0.0030]        [0.0051]




Table 4: Maximum likelihood Estimates for Clustering Variances for Log Earnings Residuals

   σ̂2         2
              σ̂D         σ̂S2         σ̂P2          2
                                                   σ̂dist            α̂          Log Lik             c β̂)
                                                                                                    s.e.(
                                                                                              Min Wage NE/ENC

  0.7125       0           0            0            0               0         -856182.0271    0.0013    0.0013
 [0.0006]

  0.7071       0         0.0124         0            0               0         -846374.7763    0.0702    0.0499
 [0.0006]               [0.0025]

  0.6810       0         0.0087       0.0284         0               0         -801566.9926    0.0597    0.0430
 [0.0006]               [0.0022]     [0.0009]

  0.7071     0.0031      0.0091         0            0               0         -846371.8921    0.0765    0.0655
 [0.0006]   [0.0020]    [0.0023]

  0.6810     0.0028      0.0059       0.0284         0               0         -801563.9898    0.0661    0.0585
 [0.0006]   [0.0020]    [0.0018]     [0.0009]

  0.6810     0.0027      0.0060       0.0183       0.0120        0.0271        -801460.8862    0.0710    0.0619
 [0.0006]   [0.0020]    [0.0021]     [0.0011]     [0.0015]      [0.0057]




                                                             [29]
Table 5: Maximum likelihood Estimates for Clustering Variances for Demeaned Years of Edu-
cation

   σ̂2         2
              σ̂D        σ̂S2       σ̂P2        2
                                              σ̂dist             α̂         Log Lik             c β̂)
                                                                                               s.e.(
                                                                                         Min Wage NE/ENC

  8.0400       0          0          0          0                0        -3994627.262    0.0043   0.0043
 [0.0071]

  7.9337       0        0.1068       0          0                0        -3977534.893    0.2060   0.1464
 [0.0070]              [0.0219]

  7.1264       0        0.0792     0.8319       0                0        -3843542.281    0.1846   0.1361
 [0.0063]              [0.0218]   [0.0264]

  7.9337     0.0393     0.0702       0          0                0        -3977530.761    0.2363   0.2121
 [0.0070]   [0.0257]   [0.0162]

  7.1264     0.0531     0.0362     0.8313       0                0        -3843537.315    0.2339   0.2271
 [0.0063]   [0.0328]   [0.0142]   [0.0264]

  7.1264     0.0466     0.0321     0.6956     0.1355        0.0343        -3843496.376    0.2334   0.2222
 [0.0063]   [0.0158]   [0.0332]   [0.0307]   [0.0289]      [0.0123]




Table 6: Maximum likelihood Estimates for Clustering Variances for Demeaned Hours Worked
Per Week

   σ̂2          2
               σ̂D        σ̂S2       σ̂P2        2
                                               σ̂dist             α̂         Log Lik            c β̂)
                                                                                               s.e.(
                                                                                         Min Wage NE/ENC

 120.9749       0          0          0          0                0       -7505830.603    0.0165   0.0165
 [0.1036]

 120.5942       0       0.5697        0          0                0       -7501871.878    0.4759   0.3384
 [0.1061]              [0.1176]

 118.4649       0       0.2556     2.2410        0                0       -7482236.836    0.3298   0.2421
 [0.1040]              [0.0758]   [0.0743]

 120.5942    0.1161     0.4573        0          0                0        -7501870.39    0.5140   0.4279
 [0.1060]   [0.1022]   [0.1056]

 118.4650    0.1177     0.1335     2.2451        0                0       -7482235.194    0.3794   0.3557
 [0.1038]   [0.0823]   [0.0595]   [0.0745]

 118.5308    0.0701     0.1257     1.6442     0.6658            0.0496    -7482652.492    0.3574   0.3165
 [0.1042]   [0.0631]   [0.0601]   [0.1282]   [0.1321]          [0.0172]



                                                        [30]
Table 7: p-values for Mantel Statistics, based on 10,000,000 draws and one-sided alternatives

Proximity −→       Border    Divison   −d(Zs , Zt)             exp(−α · d(Zs , Zt))
                                                     α = 0.00138 α = 0.00276 α = 0.00693

Minimum wage       0.0002     0.0032     0.0087        0.2674         0.0324        0.0033

Log wage           0.0005     0.0239     0.0692        0.0001       < 0.0001       < 0.0001

Education         < 0.0001    0.0314     0.0028       < 0.0001      < 0.0001       < 0.0001

Hours Worked       0.0055     0.8922     0.0950        0.0243         0.0086        0.0182

Weeks Worked       0.0018     0.5155     0.1285        0.0217         0.0533        0.3717




                                            [31]
           Figure 1: Covariance of Demeaned Log(Earnings) by Distance Between Individuals.
0.04

0.03

0.02

0.01

  0
       0   10       20        30       40        50        60       70       80        90    100
                                               Overall

0.04

0.03

0.02

0.01

  0
       0   10       20        30       40        50        60       70       80        90    100
                                            Within State

0.04

0.03

0.02

0.01

  0
       0   10       20        30      40        50       60       70         80        90    100
                                Between States. Bandwidth=20 miles.
           Figure 2: Covariance of Demeaned Log(Earnings) by Distance Between Individuals.
0.04

0.03

0.02

0.01

  0
       0   10       20        30       40        50        60       70       80        90    100
                                               Overall

0.04

0.03

0.02

0.01

  0
       0   10       20        30       40        50        60       70       80        90    100
                                            Within State

0.04

0.03

0.02

0.01

  0
       0   10       20        30      40        50       60       70         80        90    100
                                Between States. Bandwidth=50 miles.
