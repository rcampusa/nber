                                 NBER WORKING PAPER SERIES




                      USING MATCHING, INSTRUMENTAL VARIABLES
                         AND CONTROL FUNCTIONS TO ESTIMATE
                              ECONOMIC CHOICE MODELS

                                           James Heckman
                                       Salvador Navarro-Lozano

                                          Working Paper 9497
                                  http://www.nber.org/papers/w9497


                       NATIONAL BUREAU OF ECONOMIC RESEARCH
                                1050 Massachusetts Avenue
                                  Cambridge, MA 02138
                                      February 2003




This research was supported by NSF SES-0099195, NIH HD34958-04 and the American Bar Foundation.
Navarro-Lozano acknowledges financial support from CONACYT, Mexico. We thank Alberto Abadie, Pedro
Carneiro, Michael Lechner and Costas Meghir for helpful comments. The views expressed herein are those
of the authors and not necessarily those of the National Bureau of Economic Research.

©2003 by James Heckman and Salvador Navarro-Lozano. All rights reserved. Short sections of text not to
exceed two paragraphs, may be quoted without explicit permission provided that full credit including notice,
is given to the source.
Using Matching, Instrumental Variables and Control Functions
to Estimate Economic Choice Models
James Heckman and Salvador Navarro-Lozano
NBER Working Paper No. 9497
February 2003
JEL No. C31

                                            ABSTRACT

This paper investigates four topics. (1) It examines the different roles played by the propensity score

(probability of selection) in matching, instrumental variable and control functions methods. (2) It
contrasts the roles of exclusion restrictions in matching and selection models. (3) It characterizes
the sensitivity of matching to the choice of conditioning variables and demonstrates the greater
robustness of control function methods to misspecification of the conditioning variables. (4) It
demonstrates the problem of choosing the conditioning variables in matching and the failure of
conventional model selection criteria when candidate conditioning variables are not exogenous.



James J. Heckman                                       Salvador Navarro-Lozano
Department of Economics                                Department of Economics
The University of Chicago                              The University of Chicago
1126 East 59th Street                                  1126 East 59th Street
Chicago, IL 60637                                      Chicago, IL 60637
and NBER                                               snavarro@uchicago.edu
jjh@uchicago.edu
1     Introduction

The method of matching has become popular in evaluating social programs because it is easy to understand
and easy to apply. It uses observed explanatory variables to adjust for diﬀerences in outcomes unrelated
to treatment that give rise to selection bias. Propensity score matching as developed by Rosenbaum
and Rubin (1983) is particularly simple to apply. The propensity score is the probability that an agent
takes treatment. If the analyst knows (without having to estimate) the probability that a person takes
treatment, and the assumptions of matching are fulÞlled, he can condition on that known probability and
avoid selection in means and marginal distributions. This choice probability also plays a central role in
econometric selection models based on the principle of control functions (Heckman, 1980; Heckman and
Robb, 1986, reprinted 2000; Heckman and Hotz, 1989; Ahn and Powell, 1993) and in instrumental variable
models (see e.g. Heckman and Vytlacil, 1999, 2001, 2003 or Heckman, 2001).
    The multiple use of the propensity score in diﬀerent statistical methods has given rise to some confusion
in the applied literature.1 This paper seeks to clarify the diﬀerent assumptions that justify the propensity
score in selection, matching and instrumental variables methods. We develop the following topics:

    1. We orient the discussion of the selection of alternative estimators around the economic theory of
      choice. We compare the diﬀerent roles that the propensity score plays in three widely used econo-
      metric methods, and the implicit economic assumptions that underlie applications of these methods.

    2. Conventional matching methods do not distinguish between excluded and included variables.2 We
      show that matching breaks down when there are variables that predict the choice of treatment
      perfectly whereas control function methods take advantage of exclusion restrictions and use the
      information available from perfect prediction to obtain identiÞcation. Matching assumes away the
      possibility of perfect prediction while selection models rely on this property in limit sets.

    3. We deÞne the concepts of “relevant” information and “minimal relevant” information, and distinguish
      agent and analyst information sets. We state clearly what information is required to identify diﬀerent
      treatment parameters. In particular we show that when the analyst does not have access to the
      “minimal relevant” information, matching estimates of diﬀerent treatment parameters are biased.
      Having more information, but not all of the “minimal relevant” information, can increase the bias
      compared to having less information. Enlarging the analyst’s information set with variables that do
      not belong in the relevant information set may either increase or decrease the bias from matching.
      Because the method of control functions explicitly models omitted relevant variables, rather than


                                                      3
       assuming that there are none, it is more robust to omitted conditioning variables.

    4. The method of matching oﬀers no guidance as to which variables to include or exclude in conditioning
       sets. Such choices can greatly aﬀect inference. There is no support for the commonly used rules
       of selecting matching variables by choosing the set of variables that maximizes the probability of
       successful prediction into treatment or by including variables in conditioning sets that are statistically
       signiÞcant in choice equations. This weakness is shared by many econometric procedures but is not
       fully appreciated in recent applications of matching which apply these selection rules when choosing
       conditioning sets.

    To simplify the exposition, throughout this paper we consider a one-treatment, two-outcome model.
Our main points apply more generally.


2      A Prototypical Model of Economic Choice

To focus the discussion, and interpret the implicit assumptions underlying the diﬀerent estimators presented
in this paper, we present a benchmark model of economic choice. For simplicity we consider two potential
outcomes (Y0 , Y1 ). D = 1 if Y1 is selected. D = 0 if Y0 is selected. Agents pick their outcome based on
utility maximization. Let V be utility. We write

                                       V = µV (Z, UV ) D = 1 (V > 0),                                        (1)

where the Z are factors (observed by the analyst) determining choices, UV are the unobserved (by the
analyst) factors determining choice and 1 is an indicator function (1(A) = 1 if A is true; 1(A) = 0
otherwise). We consider diﬀerences between agent information sets and analyst information sets in Section
(6).
    Potential outcomes are written in terms of observed variables (X) and unobserved (by the analyst)
outcome-speciÞc variables
                                                Y1 = µ1 (X, U1 )                                            (2a)

                                                Y0 = µ0 (X, U0 ).                                           (2b)

We assume throughout that U0 , U1 , UV are (absolutely) continuous random variables and that all means
are Þnite. The individual level treatment eﬀect is

                                                  ∆ = Y1 − Y0 .


                                                        4
    More familiar forms of (1), (2a) and (2b) are additively separable:

                                         V = µV (Z) + UV    E(UV ) = 0                                  (10 )

                                         Y1 = µ1 (X) + U1   E(U1 ) = 0                                 (2a0 )

                                         Y0 = µ0 (X) + U0   E(U0 ) = 0.                                (2b0 )

Additive separability is not strictly required in matching, or most versions of selection (control function)
models. However, we use the additively separable representation throughout most of this paper because
of its familiarity noting when it is a convenience and when it is an essential part of a method.
    The distinction between X and Z is crucial to the validity of many econometric procedures. In matching
as conventionally formulated there is no distinction between X and Z. The roles of X and Z in alternative
estimators are explored in this paper.


3    Parameters of Interest in this Paper

There are many parameters of interest that can be derived from this model if U1 6= U0 and agents use
some or all of the U0 , U1 in making their decisions (see Heckman and Robb, 1985, 1986; Heckman, 1992;
Heckman, Smith and Clements, 1997 ; Heckman and Vytlacil, 2001 and Heckman, 2001). Here we focus
on certain means because they are traditional. As noted by Heckman and Vytlacil (2000) and Heckman
(2001), the traditional means do not answer many interesting economic questions.
    The traditional means are:



                      AT E : E(Y1 − Y0 |X) (Average Treatment Eﬀect)

                        TT   : E(Y1 − Y0 |X, D = 1) (Treatment on the Treated)

                     M T E : E(Y1 − Y0 |X, Z, V = 0) (Marginal Treatment Eﬀect).

    The M T E is the marginal treatment eﬀect introduced into the evaluation literature by Björklund and
Moﬃtt (1987). It is the average gain to persons who are indiﬀerent to participating in sector 1 or sector
0 given X, Z. These are persons at the margin, deÞned by X and Z. Heckman and Vytlacil (1999, 2000)
show how the M T E can be used to construct all mean treatment parameters, including the policy relevant
treatment parameters, under the conditions speciÞed in their papers.




                                                      5
4     The Selection Problem

Let Y = DY1 + (1 − D)Y0 . Samples generated by choices have the following means which are assumed to
be known:


                                  E(Y |X, Z, D = 1) = E(Y1 |X, Z, D = 1)

and
                                  E(Y |X, Z, D = 0) = E(Y0 |X, Z, D = 0)

for outcomes of Y1 for participants and the outcomes of Y0 for non-participants, respectively. In addition,
choices are observed so that in large samples Pr (D = 1|X, Z), i.e., the probability of choosing treatment
is known. From the means we can integrate out Z given X and D to construct

                                   E(Y1 |X, D = 1) and E(Y0 |X, D = 0).

     The biases from using the diﬀerence of these means to construct various counterfactuals are, for the
three parameters studied in this paper:

                 Bias T T   = [E(Y |X, D = 1) − E(Y |X, D = 0)] − [E(Y1 − Y0 |X, D = 1)]

                            = [E(Y0 |X, D = 1) − E(Y0 |X, D = 0)].

In the case of additive separability

                               Bias T T = E[U0 |X, D = 1] − E[U0 |X, D = 0].

For AT E,
                     Bias AT E = E[Y |X, D = 1] − E(Y |X, D = 0) − [E(Y1 − Y0 |X)].

In the case of additive separability

                Bias AT E = [E (U1 |X, D = 1) − E (U1 |X)] − [E(U0 |X, D = 0) − E(U0 |X)].

For M T E,

    Bias M T E = E(Y |X, Z, D = 1) − E(Y |X, Z, D = 0) − E(Y1 − Y0 |X, Z, V = 0)

                = [E(U1 |X, Z, D = 1) − E(U1 |X, Z, V = 0)] − [E(U0 |X, Z, D = 0) − E(U0 |X, Z, V = 0)]

in the case of additive separability. The M T E is deÞned for a subset of persons indiﬀerent between the
two sectors and so is deÞned for X and Z. The bias is the diﬀerence between average U1 for participants
and marginal U1 minus the diﬀerence between average U0 for nonparticipants and marginal U0 . Each of
these terms is a bias which can be called a selection bias.

                                                     6
5     How Diﬀerent Methods Solve the Bias Problem

In this section we consider the identiÞcation conditions that underlie matching, control functions and
instrumental variable methods to identify the three parameters using the data on mean outcomes. We
start with the method of matching.


5.1   Matching

The method of matching as conventionally formulated makes no distinction between X and Z. DeÞne the
conditioning set as W = (X, Z). The strong form of matching advocated by Rosenbaum and Rubin (and
numerous predecessor papers) assumes that

                                             (Y1, Y0 ) ⊥⊥ D|W                                       (M-1)

and
                                     0 < Pr(D = 1|W ) = P (W ) < 1,                                 (M-2)

where “⊥
       ⊥” denotes independence given the conditioning variables after “|”. Condition (M-2) implies that
the treatment parameters can be deÞned for all values of W (i.e., for each W , in very large samples there
are observations for which we observe a Y0 and other observations for which we observe a Y1 ). Rosenbaum
and Rubin show that under (M-1) and (M-2)


                                          (Y1 , Y0 ) ⊥⊥ D|P (W ).                                   (M-3)

    This reduces the dimensionality of the matching problem. They assume that P is known.3 Under these
assumptions, conditioning on P eliminates all three biases deÞned in section (4) because

                    E (Y1 |D = 0, P (W )) = E (Y1 |D = 1, P (W )) = E (Y1 |P (W ))

                    E (Y0 |D = 1, P (W )) = E (Y0 |D = 0, P (W )) = E (Y0 |P (W )) .

Thus for T T we can identify E (Y0 |D = 1, P (W )) from E (Y0 |D = 0, P (W )) . In fact, we only need the
                    ⊥ D|P (W ) to remove the bias4 because E (Y1 |P (W ) , D = 1) is known, and only
weaker condition Y0 ⊥
E (Y0 |P (W ) , D = 1) is unknown. From the observed conditional means we can form AT E. Observe
that since AT E = T T for all X, Z under (M-1) and (M-2), the average person equals the marginal
person, conditional on W, and there is no bias in estimating M T E.5 The strong implicit assumption that
the marginal participant in a program gets the same return as the average participant in the program,
conditional on W , is an unattractive implication of these assumptions (see Heckman, 2001 and Heckman

                                                    7
and Vytlacil, 2003). The method assumes that all of the dependence between UV and (U1 , U0 ) is eliminated
by conditioning on W :


                                             UV ⊥
                                                ⊥ (U1, U0 )|W.

This motivates the term “selection on observables” introduced in Heckman and Robb (1985; 1986, reprinted
2000).
   Assumption (M-2) has the unattractive feature that if the analyst has too much information about
the decision of who takes treatment so that P (W ) = 1 or 0 the method breaks down because people
cannot be compared at a common W . The method of matching assumes that, given W , some unspeciÞed
randomization device allocates people to treatment.
   Introducing the distinction between X and Z allows the analyst to overcome the problem of perfect
prediction if there are some variables Z not in X so that, for certain values of these variables, and for each
X either P (X, Z) = 1 or P (X, Z) = 0. If P is a nontrivial function of Z (so P (X, Z) varies with Z for all
X) and X can be varied independently of Z,6 and outcomes are deÞned solely in terms of X, this diﬃculty
with matching disappears and treatment parameters can be deÞned for all values of X in its support (see
Heckman, Ichimura and Todd, 1997).
   Oﬀsetting the disadvantages of matching, the method of matching with a known conditioning set that
produces (M-1) does not require separability of outcome or choice equations, exogeneity of conditioning
variables, exclusion restrictions or adoption of speciÞc functional forms of outcome equations. Such fea-
tures are common in conventional selection (control function) methods and conventional IV formulations
although recent work in semiparametric estimation relaxes many of these assumptions, as we note below.
Moreover, the method does not strictly require (M-1). One can get by with weaker mean independence
assumptions:

                                      E (Y1 |W, D = 1) = E (Y1 |W )                                    (M-10 )

                                      E (Y0 |W, D = 0) = E (Y0 |W ) ,

in the place of the stronger (M-1) conditions. However, if (M-10 ) is involved, the assumption that we can
replace W by P (W ) does not follow from the analysis of Rosenbaum and Rubin, and is an additional new
assumption.
   In the recent literature, the claim is sometimes made that matching is “for free” (see, e.g., Gill
and Robins, 2001). The idea is that since E (Y0 |D = 1, W ) is not observed, we might as well set it to



                                                      8
E (Y0 |D = 0, W ), an implication of (M-1). This argument is correct so far as data description goes. Match-
ing imposes just-identifying restrictions and in this sense —at a purely empirical level— is as good as any
other just-identifying assumption in describing the data.
   However, the implied economic restrictions are not “for free”. Imposing that, conditional on X and
Z, the marginal person is the same as the average person is a strong and restrictive feature of these
assumptions and is not a “for free” assumption in terms of economic content.7


5.2   Control Functions

The principle motivating the method of control functions is diﬀerent. (See Heckman, 1980 and Heckman
and Robb, 1985, 1986, reprinted 2000, where this principle was developed). Like matching, it works
with conditional expectations of (Y1 , Y0 ) given (X, Z and D). Conventional applications of the control
function method assume additive separability which is not required in matching. Strictly speaking, additive
separability is not required in the application of control functions either.8 What is required is a model
relating the outcome unobservables to the observables, including the choice of treatment. The method of
matching assumes that, conditional on the observables (X, Z), the unobservables are independent of D.9
For the additively separable case, control functions are based on the principle of modeling the conditional
expectations given X, Z and D :

                          E (Y1 |X, Z, D = 1) = µ1 (X) + E(U1 |X, Z, D = 1)

                          E (Y0 |X, Z, D = 0) = µ0 (X) + E(U0 |X, Z, D = 0).

   The idea underlying the method of control functions is to explicitly model the stochastic dependence of
the unobservables in the outcome equations on the observables. This is unnecessary under the assumptions
of matching because conditional on (X, Z) there is no dependence between (U1 , U0 ) and D. Thus, if one can
model E (U1 |X, Z, D = 1) and E (U0 |X, Z, D = 0) and these functions can be independently varied against
µ1 (X) and µ0 (X) respectively, one can identify µ1 (X) and µ0 (X) up to constant terms.10 Nothing in
the method intrinsically requires that X, Z, or D be stochastically independent of U1 or U0 , although
conventional methods often assume that (U1 , U0 , UV ) ⊥⊥ (X, Z) .
                             ⊥ (X, Z) and adopt (10 ) as the choice model,
   If we assume that U1 , UV ⊥

                      E (U1 |X, Z, D = 1) = E (U1 |UV ≥ −µV (Z)) = K1 (P (X, Z)) ,

so the control function only depends on P (X, Z). By similar reasoning, if U0 , UV ⊥⊥ (X, Z) ,

                       E (U0 |X, Z, D = 0) = E (U0 |UV < −µV (Z)) = K0 (P (X, Z))

                                                     9
and the control function only depends on the propensity score. The key assumption needed to represent
the control function solely as a function of P (X, Z) is thus

                                              (U1 , U0 , UV ) ⊥⊥ (X, Z) .

Under these conditions

                             E (Y1 |X, Z, D = 1) = µ1 (X) + K1 (P (X, Z))

                             E (Y0 |X, Z, D = 0) = µ0 (X) + K0 (P (X, Z))

with lim K1 (P ) = 0 and lim K0 (P ) = 0 where it is assumed that Z can be independently varied for all
     P →1                  P →0
X, and the limits are obtained by changing Z while holding X Þxed.11 These limit results just say that
when the probability of being in a sample in one there is no selection bias.
   If K1 (P (X, Z)) can be independently varied from µ1 (X) and K0 (P (X, Z)) can be independently
varied from µ0 (X), we can identify µ1 (X) and µ0 (X) up to constants. If there are limit sets Z0 and Z1
such that lim P (X, Z) = 0 and lim P (X, Z) = 1, then we can identify the constants, since in those limit
            Z→Z0                   Z→Z1
sets we identify µ1 (X) and µ0    (X) .12   Under these conditions, it is possible to nonparametrically identify
all three treatment parameters:


                                              AT E = µ1 (X) − µ0 (X)

                         T T = µ1 (X) − µ0 (X) + E (U1 − U0 |X, Z, D = 1)                13
                                                              ¡     ¢
                         = µ1 (X) − µ0 (X) + K1 (P (X, Z)) + 1−P P    K0 (P (X, Z))

                                          ∂ [E (U1 − U0 |X, Z, D = 1) P (X, Z)]
              M T E = µ1 (X) − µ0 (X) +
                                                      ∂ (P (X, Z))
                                            £         ©                                ª¤
                                          ∂ P (X, Z) K1 (P (X, Z) + 1−P  P K0 (P (X, Z)
                      = µ1 (X) − µ0 (X) +                                                 .
                                                            ∂ (P (X, Z))

   Unlike the method of matching, the method of control functions allows the marginal treatment eﬀect
to be diﬀerent from the average treatment eﬀect or from treatment on the treated. Although conventional
practice is to derive the functional forms of K0 (P ), K1 (P ) by making distributional assumptions (e.g.,
normality, see Heckman, Tobias and Vytlacil (2001)), this is not an intrinsic feature of the method and
there are many non normal and semiparametric versions of this method (see Heckman and Vytlacil, 2003
for a survey).
   In its semiparametric implementation, the method of control functions requires an exclusion restriction
(a Z not in X) to achieve nonparametric identiÞcation.14 The method of matching does not. The method of

                                                          10
control functions requires that P (X, Z) = 1 and P (X, Z) = 0 to achieve full nonparametric identiÞcation.
The conventional method of matching excludes this case. Both methods require that treatment parameters
can only be deÞned on a common support:

                                    support (X|D = 1) ∩ support (X|D = 0)

A similar requirement is imposed on the generalization of matching with exclusion restrictions introduced
in Heckman, Ichimura and Todd (1997). Exclusion, both in matching and selection models, makes it more
likely to satisfy this condition.
   In the method of control functions, P (X, Z) is a conditioning variable used to predict U1 conditional
on D and U0 conditional on D. In the method of matching, it is used to generate stochastic independence
between (U0 , U1 ) and D. In the method of control functions, as conventionally applied, (U0 , U1 ) ⊥
                                                                                                    ⊥ (X, Z),
but this is not intrinsic to the method.15 This assumption plays no role in matching if the correct condition-
ing set is known (i.e., one that satisÞes (M-1) and (M-2)). However, as noted in section (6.6), exogeneity
plays a key role in the selection of conditioning variables. The method of control functions does not require
that (U0 , U1 ) ⊥⊥ D| (X, Z) , which is a central requirement of matching. Equivalently, the method of control
functions does not require
                                            (U0 , U1 ) ⊥
                                                       ⊥ UV | (X, Z)

whereas matching does. Thus matching assumes access to a richer set of conditioning variables than is
assumed in the method of control functions.
   The method of control functions is more robust than the method of matching, in the sense that it
allows for outcome unobservables to be dependent on D even conditioning on (X, Z) , and it models this
dependence, whereas the method of matching assumes no such dependence. Matching is thus a special
case of the method of control functions in which under assumptions (M-1) and (M-2),

                           E (U1 |X, Z, D = 1) = E (U1 |X, Z) = E (U1 |P (W ))

                           E (U0 |X, Z, D = 0) = E (U0 |X, Z) = E (U0 |P (W )) .

   In the method of control functions in the case when (X, Z) ⊥⊥ (U0 , U1 , UV )

E (Y |X, Z, D) = E (Y1 |X, Z, D = 1) D + E (Y0 |X, Z, D = 0) (1 − D)

                 = µ0 (X) + (µ1 (X) − µ0 (X)) D + E (U1 |X, Z, D = 1) D + E (U0 |P (X, Z) , D = 0) (1 − D)

                 = µ0 (X) + (µ1 (X) − µ0 (X)) D + E (U1 |P (X, Z) , D = 1) D + E (U0 |P (X, Z) , D = 0) (1 − D)

                 = µ0 (X) + [µ1 (X) − µ0 (X) + K1 (P (X, Z)) − K0 (P (X, Z))] D + K0 (P (X, Z)) .

                                                      11
    Under assumptions (M-1) and (M-2) of the method of matching, we may write

E (Y |P (W ) , D) = µ0 (P (W ))+[(µ1 (P (W )) − µ0 (P (W ))) + E (U1 |P (W )) − E (U0 |P (W ))] D+{E (U0 |P (W ))} .

Notice that
                         E (Y |P (W ) , D) = µ0 (P (W )) + [µ1 (P (W )) − µ0 (P (W ))] D,

since E (U1 |P (W )) = E (U0 |P (W )) = 0.
    The treatment eﬀect is identiÞed from the coeﬃcient on D. Condition (M-2) guarantees that D is not
perfectly predictable by W so the variation in D identiÞes this parameter. Since µ1 (P (W ))−µ0 (P (W )) =
AT E and AT E = T T = M T E, the method of matching identiÞes all of the mean treatment parameters.
Under the assumptions of matching, when means of Y1 and Y0 are the parameters of interest, the bias
terms vanish. They do not in the more general case considered by the method of control functions. This
is the mathematical counterpart of the randomization implicit in matching: conditional on W or P (W ) ,
(U1 , U0 ) are random with respect to D. The method of control functions allows them to be nonrandom
with respect to D. In the absence of functional form assumptions, it requires an exclusion restriction to
separate out K0 (P (X, Z)) from the coeﬃcient on D. Matching produces identiÞcation without exclusion
restrictions whereas identiÞcation with exclusion restrictions is a central feature of the control function
method in the absence of functional form assumptions.
    The fact that the control function approach is more general than the matching approach is implicitly
recognized in the work of Rosenbaum (1995) and Robins (1997). Their sensitivity analyses for matching
when there are unobserved conditioning variables are, in their essence, sensitivity analyses using control
functions.16
    Tables 1 and 2 perform sensitivity analysis under diﬀerent assumptions about the parameters of the
underlying selection model. In particular, we assume that the data are generated by the model of equations
(10 ), (2a0 ) and (2b0 ) and that

                                       (U1 , U0 , UV )0 ∼ N (0, Σ)

                                      corr (Uj , UV ) = ρjV

                                           var (Uj ) = σ 2j ;   j = {0, 1} .

Using the formulae derived in the Appendix, we can write the biases of section (4) as

                            Bias T T (P (Z) = p) = σ0 ρ0V M (p)

                          Bias AT E (P (Z) = p) = M (p) [σ1 ρ1V (1 − p) + σ 0 ρ0V p]

                                                       12
                  φ(Φ−1 (1−p))
where M (p) =        p(1−p)    ,   φ (·) and Φ (·) are the pdf and cdf of a standard normal random variable and
p is the propensity score. We assume that µ1 = µ0 so that the true average treatment eﬀect is zero.
      We simulate the bias for diﬀerent values of the ρjV and σ j . The results in the tables show that, as we let
the variances of the outcome equations grow, the value of the bias that we obtain can become substantial.
With large variances there are large biases. With larger correlations come larger biases. These tables
demonstrate the greater generality of the control function approach given the assumption of separability
                                                                                                  ¡    ¢
between model and errors. Even if the correlation between the observables and the unobservables ρjV
is small, so that one might think that selection on unobservables is relatively unimportant, we still get
substantial biases if we do not control for relevant omitted conditioning variables. Only for special values
of the parameters do we avoid the bias by matching. These examples also demonstrate that sensitivity
analyses can be conducted for control function models even when they are not fully identiÞed.


5.3      Instrumental Variables

Both the method of matching and the method of control functions work with E (Y |X, Z, D) and Pr (D = 1|X, Z).
The method of instrumental variables works with E (Y |X, Z) and Pr (D = 1|X, Z) . There are two versions
of the method of instrumental variables: (a) conventional linear instrumental variables and (b) local instru-
mental variables (LIV ) (Heckman and Vytlacil, 1999, 2000, 2003; Heckman, 2001). LIV is equivalent to
a semiparametric selection model (See Vytlacil, 2002). It is an alternative way to implement the principle
of control functions. LAT E (Imbens and Angrist, 1994) is a special case of LIV under the conditions we
specify below.
      We Þrst consider the conventional method of instrumental variables. In this framework, P (X, Z) arises
less naturally than it does in the matching and control function approaches. Z is the instrument and
P (X, Z) is a function of the instrument.
      Rewrite the model of equations (2a0 ) and (2b0 ) as

                              Y      = DY1 + (1 − D) Y0

                                     = µ0 (X) + (µ1 (X) − µ0 (X) + U1 − U0 ) D + U0

                                     = µ0 (X) + ∆ (X) D + U0

where ∆ (X) = µ1 (X) − µ0 (X) + U1 − U0 . When U1 = U0 , this is a conventional IV model with D
correlated with U0 . Standard instrumental variables conditions apply and P (X, Z) is a valid instrument
if:
                                           E (U0 |P (X, Z) , X) = E (U0 |X)                                (IV-1)

                                                          13
and
                         Pr (D = 1|X, Z) is a nontrivial function of Z for each X.                     (IV-2)

When U1 6= U0 but D ⊥⊥ (U1 − U0 ) |X (or alternatively UV ⊥⊥ (U1 − U0 ) |X) then the same two conditions
identify

                             AT E = E (Y1 − Y0 |X) = E (∆ (X) |X)

                              TT    = E (Y1 − Y0 |X, D = 1) = E (Y1 − Y0 |X)

                                    = MT E

                                                                           ⊥ (U1 − U0 ) |X is strong and
and marginal equals average conditional on X and Z. The requirement that D ⊥
assumes that agents do not participate in the program on the basis of any information about unobservables
in gross gains (Heckman and Robb, 1985, 1986; Heckman, 1997).
   The analytically more interesting case arises when U1 6= U0 and D 6⊥
                                                                      ⊥ (U1 − U0 ) . To identify AT E, we
require
                      E (U0 + D (U1 − U0 ) |P (X, Z) , X) = E (U0 + D (U1 − U0 ) |X)                   (IV-3)

and condition (IV-2) (Heckman and Robb, 1985, 1986; Heckman, 1997). To identify T T , we require

                         E (U0 + D (U1 − U0 ) − E (U0 + D (U1 − U0 ) |X) |P (X, Z) , X)

                     = E (U0 + D (U1 − U0 ) − E (U0 + D (U1 − U0 ) |X) |X)

and condition (IV-2). No simple conditions exist to identify the M T E using linear instrumental variables
                                     ⊥ (U1 − U0 ) |X, Z (Heckman and Vytlacil, 2000, 2003 characterize
methods in the general case where D 6⊥
what conventional IV estimates in terms of a weighted average of M T Es).
   The conditions required to identify AT E using P as an instrument, may be written in the following
alternative form:

                         E (U0 |P (X, Z) , X) + E (U1 − U0 |D = 1, P (X, Z) , X) P (X, Z)

                     = E (U0 |X) + E (U1 − U0 |D = 1, X) P (X, Z)

If U1 = U0 (everyone with the same X responds to treatment in the same way) or (U1 − U0 ) ⊥
                                                                                          ⊥ D|P (X, Z) , X
(people do not participate in treatment on the basis of unobserved gains), then these conditions are satisÞed.
In general, the conditions are not satisÞed by economic choice models, except under special cancellations
that are not generic. If Z is a determinant of choices, and U1 − U0 is in the agent’s choice set (or is
correlated only partly with information in the agent’s choice set), then this condition is not likely to be
satisÞed.

                                                     14
   These identiÞcation conditions are fundamentally diﬀerent from the matching and control function
identiÞcation conditions. In matching, the essential condition for means is

                          E (U0 |X, D = 0, P (X, Z)) = E (U0 |X, P (X, Z)) and

                          E (U1 |X, D = 1, P (X, Z)) = E (U1 |X, P (X, Z))

These require that, conditional on P (X, Z) and X, U1 and U0 are mean independent of UV (or D). When
µ1 (W ) and µ0 (W ) are the conditional means of Y1 and Y0 respectively, these terms are zero.
   The method of control functions models and estimates this dependence rather than assuming it vanishes.
The method of linear instrumental variables requires that the composite error term U0 + D (U1 − U0 ) be
mean independent of Z (or P (X, Z)), given X. Essentially, the conditions require that the dependence
of U0 and D (U1 − U0 ) on Z vanish through conditioning on X. Matching requires that U1 and U0 are
independent of D given (X, Z). These conditions are logically distinct. One set of conditions does not
imply the other set. Conventional IV in the general case does not answer a well posed economic question
(see Carneiro, Heckman and Vytlacil, 2001).
   Local instrumental variables methods developed by Heckman and Vytlacil (1999, 2000, 2003) estimate
all three treatment parameters in the general case where (U1 − U0 ) 6⊥⊥ D| (X, Z) under the following
additional conditions

                         µD (Z)       is a non-degenerate random variable given X                (LIV-1)

                                      (existence of an exclusion restriction)


                        (U0 , U1 , UV ) ⊥
                                        ⊥ Z|X                                                    (LIV-2)

                        0 < Pr (D|X) < 1                                                         (LIV-3)

                        Support P (D| (X, Z)) = [0, 1]                                           (LIV-4)

Under these conditions

                                  ∂E(Y |X,P (Z))                                  17
                                    ∂(P (Z))       = M T E (X, P (Z) , V = 0) .

Only (LIV-1) - (LIV-3) are required to identify this parameter.
   As demonstrated by Heckman and Vytlacil (1999, 2000, 2003) and Heckman (2001), over the support
of (X, Z), M T E can be used to construct (under LIV-4) or bound (in the case of partial support) AT E
and T T . Policy relevant treatment eﬀects can be deÞned, LAT E is a special case of this method. Table


                                                          15
3 summarizes the alternative assumptions used in matching, control functions and instrumental variables
to identify treatment parameters. For the rest of the paper, we discuss matching, the topic of this special
issue. We Þrst turn to consider the informational requirements of matching.


6    The Informational Requirements of Matching and the Bias When
     They are not SatisÞed

This section considers the informational requirements for matching.18 We introduce Þve distinct infor-
mation sets and establish relationships among them: (1) An information set that satisÞes conditional
independence (M-1), σ (IR∗ ), a “relevant” information set; (2) the minimal information set needed to sat-
isfy conditional independence (M-1), σ (IR ), the “minimal relevant” information set; (3) the information
set available to the agent at the time decisions to participate are made, σ (IA ) ; (4) the information avail-
able to the economist σ (IE ∗ ) and (5) the information used by the economist (σ (IE )) . We will deÞne the
random variables generated by these sets as IR∗ , IR , IA , IE ∗ , IE respectively.19
    After deÞning these information sets, we show the biases that result when econometricians use infor-
mation other than the relevant information set. More information does not necessarily reduce the bias in
matching. Standard algorithms for selecting conditioning variables are not guaranteed to pick the relevant
conditioning variables or reduce bias compared to conditioning sets not selected by these algorithms.
    First we deÞne the information sets more precisely.

DeÞnition 1 We say that σ (IR∗ ) is a relevant information set if its associated random variable, IR∗ ,
satisÞes (M-1) so
                                                (Y1 , Y0 ) ⊥
                                                           ⊥ D|IR∗

DeÞnition 2 We say that σ (IR ) is a minimal relevant information set if it is the intersection of all
sets σ (IR∗ ). The associated random variable IR is the minimum amount of information that guarantees
that (M-1) is satisÞed.

    If we deÞne the minimal relevant information set as one that satisÞes conditional independence, it might
not be unique. If the set σ (IR1 ) satisÞes the conditional independence condition, then the set σ (IR1 , Q)
such that Q ⊥⊥ (Y1 , Y0 ) | IR1 would also guarantee conditional independence. For this reason, we deÞne
the relevant information set to be the minimal; i.e., to be the intersection of all such sets.

DeÞnition 3 The agent’s information set, σ (IA ), is deÞned by the information IA used by the agent when
choosing among treatments. Accordingly, we call IA the agent’s information.

                                                        16
DeÞnition 4 The econometrician’s full information set, σ (IE ∗ ), is deÞned as all of the information
available to the econometrician, IE ∗ .

DeÞnition 5 The econometrician’s information set, σ (IE ) , is deÞned by the information used by the
econometrician when analyzing the agent’s choice of treatment, IE .

   Only three restrictions are imposed on the structure of these sets: σ (IR ) ⊆ σ (IR∗ ) , σ (IR ) ⊆ σ (IA ) and
σ (IE ) ⊆ σ (IE ∗ ) .20 The Þrst we have already discussed. The second one requires that the minimal relevant
information set must be part of the information the agent uses when deciding whether to take treatment.
The third requires that the information used by the econometrician must be part of the information he
observes. Other than these obvious orderings, the econometrician’s information set may be diﬀerent from
the agent’s or the relevant information set. The econometrician may know something the agent doesn’t
know since typically he is observing events after the decision is made. At the same time, there may be
private information known to the agent. The matching assumptions (M-1) or (M-3) imply that

                                                 σ (IR ) ⊆ σ (IE )

so that the econometrician uses the minimal relevant information set.
   In order to have a concrete example of these information sets and their associated random variables,
we assume that the economic model generating the data is a generalized Roy model of the form

                                        V   = Zγ + UV           where

                                      UV    = αV 1 f1 + αV 2 f2 + εV

                                       D = 1 if V ≥ 0, = 0 otherwise

and

                              Y1 = µ1 + U1        where U1 = α11 f1 + α12 f2 + ε1 ,

                              Y0 = µ0 + U0        where U0 = α01 f1 + α02 f2 + ε0 ,

where (f1 , f2 , εV , ε1 , ε0 ) are assumed to be mean zero random variables that are mutually independent of
each other and Z so that all the correlation among the elements of (U0 , U1 , UV ) is captured by f = (f1 , f2 ) .21
We keep implicit any dependence on X which may be general. The minimal relevant information for this
model when the factor loadings are not zero (αij 6= 0) is

                                                  IR = {f1 , f2 } .

                                                         17
   The agent’s information set may include diﬀerent variables. If we assume that ε0 , ε1 are shocks to
outcomes not known to the agent at the time decisions are made, the agent’s information is

                                                IA = {f1 , f2 , Z, εV } .

Under perfect certainty on the part of the agent

                                            IA = {f1 , f2 , Z, εV , ε1 , ε0 } .

In either case, all of the information available to the agent is not required to obtain conditional independence
(M-1). All three information sets guarantee conditional independence, but only the Þrst is minimal relevant.
   The observing economist may know some variables not in IA , IR∗ or IR but may not know all of the
variables in IR . In the following subsections, we address the question of what happens when the matching
assumption that σ (IE ) ⊇ σ (IR ) does not hold. That is, we analyze what happens to the matching bias as
the amount of information used by the econometrician is changed. In order to get closed form expressions
for the biases of the treatment parameters we add the additional assumption that

                                          (f1 , f2 , εV , ε1 , ε0 ) ∼ N (0, Σ) ,
                        ³                                  ´
where Σ is a matrix with σ 2f1 , σ2f2 , σ 2εV , σ2ε1 , σ2ε0 in the diagonal and zero in all the non-diagonal elements.
This assumption links matching models to conventional normal selection models. We next analyze various
cases.


6.1      The economist uses the minimal relevant information: σ (IR ) ⊆ σ (IE )

We begin by analyzing the case in which the information used by the analyst is IE = {Z, f1 , f2 } , so that
the econometrician has access to the relevant information set and it is larger than the minimal relevant
information set. In this case it is straightforward to show that matching identiÞes all of the mean treatment
parameters with no bias. The matching estimator is

              E (Y1 |D = 1, IE ) − E (Y0 |D = 0, IE ) = µ1 − µ0 + (α11 − α01 ) f1 + (α12 − α02 ) f2

and all of the treatment parameters collapse to this same expression since, conditional on knowing f there
is no selection because (ε1 , ε0 ) ⊥⊥ UV . Recall that IR = {f1 , f2 } and the economist needs less information
to achieve (M-1).
   The analysis of Rosenbaum and Rubin (1983) tells us that knowledge of (Z, f1 , f2 ) and knowledge of
P (Z, f1 , f2 ) are equivalent so that matching on the propensity score also identiÞes all of the treatment

                                                           18
parameters. If we write the propensity score as
                      µ                                 ¶      µ                         ¶
                         εV     −Zγ − αV 1 f1 − αV 2 f2          −Zγ − αV 1 f1 − αV 2 f2
         P (IE ) = Pr         >                           =1−Φ                             = p,
                         σ εV            σ εV                           σ εV
          ³                    ´
the event V S 0, P (f, Z) = p can be written as σεεV S Φ−1 (1 − p), where Φ is the cdf of a standard
                                                            V

normal random variable and φ is its density and f = (f1 , f2 ) . The population matching condition is

                      E (Y1 |D = 1, P (IE ) = p) − E (Y0 |D = 0, P (IE ) = p)

                  = µ1 − µ0 + E (U1 |D = 1, P (IE ) = p) − E (U0 |D = 0, P (IE ) = p)
                                µ                        ¶     µ                      ¶
                                      εV      −1                     εV       −1
                  = µ1 − µ0 + E U1 |       > Φ (1 − p) − E U0 |          ≤ Φ (1 − p)
                                      σ εV                          σ εV
                  = µ1 − µ0

and it is equal to all of the treatment parameters since
                                 µ                     ¶
                                     εV      −1          Cov (U1 , εV )
                              E U1 |      > Φ (1 − p) =                 M1 (p)
                                     σ εV                    σ εV

and                          µ                       ¶
                                   εV                   Cov (U0 , εV )
                            E U0 |      ≤ Φ−1 (1 − p) =                M0 (p) ,
                                   σ εV                     σ εV
where

                                                 φ(Φ−1 (1 − p))
                                        M1 (p) =
                                                       p
                                                      −1
                                                   φ(Φ (1 − p))
                                        M0 (p) = −
                                                       1−p

As a consequence of the assumptions about mutual independence of the errors

                        Cov (Ui , εV ) = Cov (αi1 f1 + αi2 f2 + εi , εV ) = 0,   i = 0, 1.

   In the context of this model, the case considered in this subsection is the one matching is designed to
solve. Even though a selection model generates the data, the fact that the information used by the econo-
metrician includes the minimal relevant information makes matching equivalent to the selection model.
We can estimate the treatment parameters with no bias since, as a consequence of the assumptions made
(U1 , U0 ) ⊥⊥ D| (f, Z), which is exactly what matching requires. The minimal relevant information set is
even smaller. We only need to know (f1 , f2 ) to secure this result, and we can deÞne the propensity score
solely in terms of f1 and f2 , and the Rosenbaum-Rubin result still goes through.




                                                       19
6.2     The Economist does not Use All of the Minimal Relevant Information

Now, suppose that the information used by the econometrician is

                                                    IE = {Z}

but there is selection on the unobservable (to the analyst) f1 , f2 , i.e., the factor loadings αij are all non
                                                                                           ³                  ´
zero. Recall that we assume that Z and the f are independent. In this case the event V S 0, P (Z) = p
is
                                    α f + αV 2 f2 + εV
                                   q V1 1                         S Φ−1 (1 − p) .
                                     2     2     2     2
                                    αV 1 σ f1 + αV 2 σ f2 + σ 2εV
Using the analysis presented in the Appendix, the bias for the diﬀerent treatment parameters is given by

                                        Bias T T (P (Z) = p) = β 0 M (p),                                  (3)

where M (p) = M1 (p) − M0 (p).

                       Bias AT E (P (Z) = p) = M (p) [β 1 (1 − p) + β 0 p]                                 (4)
                                                        ·                  ¸
                                                              β
                                             = β 0 M (p) p + 1 (1 − p) ; β 0 6= 0
                                                              β0
                  Bias M T E (P (Z) = p) = M (p) [β 1 (1 − p) + β 0 p] − Φ−1 (1 − p) [β 1 − β 0 ]          (5)

where
                                               αV 1 α11 σ 2f1 + αV 2 α12 σ 2f2
                                      β1 =     q
                                                α2V 1 σ 2f1 + α2V 2 σ 2f2 + σ 2εV
                                               αV 1 α01 σ 2f1 + αV 2 α02 σ 2f2
                                      β0 =     q                                  .
                                                α2V 1 σ 2f1 + α2V 2 σ 2f2 + σ 2εV

     It is not surprising that matching on variables that exclude the relevant conditioning variables produces
bias. The advantage of working with a closed form expression for the bias is that it allows us to answer
questions about the magnitude of this bias under diﬀerent assumptions about the information available to
the analyst, and to present some simple examples. We next use expressions (3), (4) and (5) as benchmarks
against which to compare the relative size of the bias when we enlarge the econometrician’s information
set beyond Z.


6.3     Adding information to the Econometrician’s Information Set IE : Using Some but
        not All the Information from the Minimal Relevant Information Set IR

Suppose next that the econometrician uses more information but not all of the information in the minimal
relevant information set. Possibly, the data set assumed in the preceding section is augmented or else the

                                                         20
econometrician decides to use information previously available. In particular, assume that

                                                     IE0 = {Z, f2 } .

Under conditions 1, 2 and 3 presented below the biases for the treatment parameters of section (6.2) are
reduced by changing the conditioning set in this way. We deÞne expressions comparable to β 1 and β 0 for
this case:

                                                             αV 1 α11 σ 2f1
                                              β 01   =     q
                                                            α2V 1 σ 2f1 + σ 2εV
                                                             αV 1 α01 σ 2f1
                                              β 00 =       q                    .
                                                            α2V 1 σ 2f1 + σ 2εV

Then, we just compare the biases under the two cases using formulae (3) - (5) suitably modiÞed but keeping
p Þxed.
Condition 1 The bias produced by using matching to estimate T T is smaller in absolute value for any
given p when the new information set σ (IE0 ) is used if
                                                                  ¯ ¯
                                                         |β 0 | > ¯β 00 ¯ .

Condition 2 The bias produced by using matching to estimate AT E is smaller in absolute value for any
given p when the new information set σ (IE0 ) is used if

                                                              ¯                     ¯
                                      |β 1 (1 − p) + β 0 p| > ¯β 01 (1 − p) + β 00 p¯ .

Condition 3 The bias produced by using matching to estimate M T E is smaller in absolute value for any
given p when the new information set σ (IE0 ) is used if
  ¯                                                      ¯ ¯       £                     ¤             £           ¤¯
  ¯M (p) [β 1 (1 − p) + β 0 p] − Φ−1 (1 − p) [β 1 − β 0 ]¯ > ¯M (p) β 01 (1 − p) + β 00 p − Φ−1 (1 − p) β 01 − β 00 ¯ .


Proof. These are straightforward applications of formulae (3)-(5), modiÞed to account for the diﬀerent
                                                                                                                          0
covariance structure produced by the information structure assumed in this Section (replacing β 0 with β 0 ,
             0
β 1 with β 1 ).
It is important to notice that we condition on the same p in deriving these expressions.
    These conditions do not always hold. In general, whether or not the bias will be reduced by adding
additional conditioning variables depends on the relative importance of the additional information in both
the outcome equations and on the signs of the terms inside the absolute value.


                                                                21
    Consider whether Condition (1) is satisÞed and assume β 0 > 0 for all α02 , αV 2 . Then β 0 > β 00 if
                                              ¡      ¢³       ´
                            αV 1 α01 σ 2f1 + α2V 2 ααV022 σ 2f2       αV 1 α11 σ 2f1
                      β0 = q                                       >q                    = β 00 .
                                 α2V 1 σ 2f1 + α2V 2 σ 2f2 + σ 2εV   α2V 1 σ 2f1 + σ 2εV
        ³  ´
When ααV022 = 0, clearly β 0 < β 00 . Adding information to the conditioning set increases bias. We can vary
³     ´
  α02
  αV 2 holding all other parameters constant. A direct computation shows that

                                                            2
                                                     αV 2 σ f2  2
                                  ∂β
                                  ³ 0 ´=q                                   > 0.
                                 ∂ ααV022 α2V 1 σ 2f1 + α2V 2 σ 2f2 + σ 2εV

As α02 increases, there is some critical value α∗02 beyond which β 0 > β 00 .
     If we assumed that β 0 < 0 however, the exact opposite conclusion would hold and the conditions would
be harder to meet as the relative importance of the new information is increased. Similar expressions can
be derived for AT E and M T E in which the direction of the eﬀect depends on the signs of the terms in the
absolute value.
     Figures 1, 2 and 3 illustrate the point that adding some but not all information from the minimal
relevant set might increase the bias for all treatment parameters. In these Þgures we let the variances of
the factors and the error terms be equal to one and set

                                           α01 = αV 1 = αV 2 = 1

                                           α02 = α12 = 0.1

                                           α11 = 2

so that we have a case in which the information being added is relatively unimportant in terms of outcomes.
     The fact that the bias might increase when adding some but not all information from IR is a feature
that is not shared by the method of control functions. Since the method of control functions models the
stochastic dependence of the unobservables in the outcome equations on the observables, changing the
variables observed by the econometrician to include f2 does not generate bias, it only changes the control
function used. That is, by adding f2 we simply change the control function from

                                        K1 (P (Z) = p) = β 1 M1 (p)

                                        K0 (P (Z) = p) = β 0 M0 (p)

to

                                       K10 (P (Z, f2 ) = p) = β 01 M1 (p)

                                       K00 (P (Z, f2 ) = p) = β 00 M0 (p)

                                                       22
but do not generate any bias. This is a major advantage of this method. It controls for the bias of the
omitted conditioning variables by modelling it. Of course, if the model for the bias is not valid, neither is
the correction for the bias. Matching evades this problem by assuming that the analyst always knows the
correct conditioning variables and they satisfy (M-1).


6.4   Adding information to the econometrician’s information set: using proxies for the
      relevant information

Suppose that instead of knowing some part of the minimal relevant information set, such as f2 , the analyst
                                                                                       e that is correlated
has access to a proxy for it.22 In particular, assume that he has access to a variable Z
with f2 but that is not the full minimal relevant information set. That is, deÞne the econometrician’s
information to be
                                                       n     o
                                                           e .
                                                IeE ∗ = Z, Z

and suppose that he uses it so IeE = IeE ∗ . In order to obtain closed form expressions for the biases we
further assume that
                                                 ³     ´
                                         Ze ∼ N 0, σ 2
                                                     e
                                                     Z
                                   ³      ´
                                     e f2
                               corr Z,               e⊥
                                            = ρ, and Z   ⊥ (ε0 , ε1 , εV , f1 ) .

We deÞne expressions comparable to β and β 0 :
                                                                      ¡       ¢
                                         α11 αV 1 σ 2f1 + α12 αV 2 1 − ρ2 σ 2f2
                                e =
                                β        q
                                  1
                                          α2V 1 σ 2f1 + α2V 2 σ 2f2 (1 − ρ2 ) + σ 2εV
                                                                      ¡       ¢
                                         α01 αV 1 σ 2f1 + α02 αV 2 1 − ρ2 σ 2f2
                                e0 =
                                β        q                                            .
                                          α2V 1 σ 2f1 + α2V 2 σ 2f2 (1 − ρ2 ) + σ 2εV

                                            ej (j = 0, 1) into Conditions (1), (2) and (3) of section (6.3) we
   By substituting IE0 for IeE and β 0j for β
obtain equivalent results for this case. Whether IeE will be bias reducing depends on how well it spans IR
and on the signs of the terms in the absolute values.
                                                                                          e and f2 . If
   In this case, however, there is another parameter to consider: the correlation between Z
                                                     e is a perfect proxy for f2 . If ρ = 0 we are essentially
|ρ| = 1 we are back to the case of IeE = IE0 because Z
back to the case analyzed in section (6.3). Since we know that the bias might either increase or decrease
when f2 is used as a conditioning variable but f1 is not, we know that it is not possible to determine
                                                                                    e That is, we know
whether the bias increases or decreases as we change the correlation between f2 and Z.



                                                        23
that going from ρ = 0 to |ρ| = 1 might change the bias in any direction. Use of a better proxy in this
correlational sense may produce a more biased estimate.
   >From the analysis of section (6.3), it is straightforward to derive conditions under which the bias
generated when the econometrician’s information is IeE is smaller than when it is IE0 . That is, it can be
                                         e is better than knowing the actual variable f2 . Take again the
the case that knowing the proxy variable Z
                                                                                                  e is
treatment on the treated case as a simple example (i.e., Condition (1)). The bias is reduced when Z
used instead of f2 if
                        ¯                           ¡       ¢         ¯ ¯                      ¯
                        ¯                                             ¯ ¯                      ¯
                        ¯ α01 αV 1 σ 2f1 + α02 αV 2 1 − ρ2 σ 2f2      ¯ ¯ α01 αV 1 σ 2f1       ¯
                        ¯q                                            ¯ < ¯q                   ¯.
                        ¯                                             ¯ ¯                      ¯
                        ¯ α2V 1 σ 2f + α2V 2 σ 2f (1 − ρ2 ) + σ 2εV   ¯ ¯ α2V 1 σ 2f + σ 2εV   ¯
                                    1            2                                  1


Figures 4, 5 and 6 use the same example of the previous section to illustrate the two points being made
here. Namely, that using a proxy for an unobserved relevant variable might increase the bias. On the
other hand, it might be better in terms of bias to use a proxy than to use the actual variable, f2 .


6.5   The case of a discrete treatment

The points that we have made so far do not strictly depend on all of the assumptions we have made
to produce simple examples. In particular, we require neither normality nor additive separability of the
outcomes. The proposition that if the econometrician’s information set includes all the minimal relevant
information, matching identiÞes the correct treatment, is true more generally provided that any additional
extraneous information used is “exogenous” in a sense to be precisely deÞned in the next section. In this
subsection, we present a simple analysis of a discrete treatment that does not rely on either normality or
separability of outcome equations.23
   Suppose that outcomes (Yj ) are binary random variables generated by the following model:

                                     Yj∗ = µj + Uj                                                     (6)

                                     Uj = αj1 f1 + αj2 f2 + εj , j = 0, 1

                                     Yj = 1 if Yj∗ ≥ 0, = 0 otherwise,

where j = 1 corresponds to treatment and j = 0 corresponds to no treatment. People receive treatment
according to the rule

                                       V    = µV + UV                                                  (7)

                                     UV     = αV1 f1 + αV2 f2 + εV

                                       D = 1 if V ≥ 0, = 0 otherwise;

                                                          24
and we assume that
                                        f1 ⊥⊥ f2 ⊥⊥ ε0 ⊥
                                                       ⊥ ε1 ⊥⊥ εV .

Each of these error components has a zero mean, the observed outcome is either zero or one and is given
by
                                          Y = DY1 + (1 − D) Y0 .

An example of such a model arises when we observe whether a person is working or not and when the
probability of being employed might be diﬀerent if the person has participated in a training program.
     There are many ways in which the eﬀect of treatment can be deÞned in this model. (see Aakvik,
Heckman and Vytlacil, 2003) One way is given by the ratio of the probabilities of observing Y1 = 1 given
that the person receives treatment and the counterfactual probability of observing Y0 = 1 given that the
person chooses treatment but does not receive it. That is, the eﬀect of treatment is given by:

                                                     Pr (Y1 = 1, D = 1|IE )
                                        ∆1 (IE ) =                          .
                                                     Pr (Y0 = 1, D = 1|IE )

A second deÞnition works with odds ratios:
                                                     Pr(Y1 =1,D=1|IE )
                                                     Pr(Y1 =0,D=1|IE )
                                        ∆2 (IE ) =   Pr(Y0 =1,D=1|IE )
                                                                       .
                                                     Pr(Y0 =0,D=1|IE )

One could also work with logs:

                                          ∆3 (IE ) = log (∆1 )

                                          ∆4 (IE ) = log (∆2 ) .

Under the null hypothesis of no eﬀect of treatment ∆1 = ∆2 = 1. More generally these ratios can be either
smaller or greater than one depending on whether there is a positive or negative eﬀect of treatment. In
order to Þx ideas, we will call ∆1 the eﬀect of treatment under the understanding that equivalent results
can be obtained for other deÞnitions.
     The econometrician measures the eﬀect of treatment by “matching” the observed distributions according
to some variables that he observes. Since Y0 is only observed when D = 0 the analyst attempts to identify
the eﬀect of treatment by
                                     b 1 (IE ) = Pr (Y1 = 1, D = 1|IE ) .
                                     ∆
                                                 Pr (Y0 = 1, D = 0|IE )
The denominator replaces the desired probability Pr (Y0 = 1, D = 1|IE ) by the available information Pr (Y0 = 1, D = 0|
Let there be no real eﬀect of treatment so that, in terms of the model given by equations (6) and (7) we


                                                      25
have that ∆1 = 1 and ∆2 = 1 so

                                               µ1 = µ0 = µ

                                              FU1 = FU0 = FU

which can be generated by setting

                                               α11 = α01 = α1

                                               α12 = α02 = α2

                                               Fε1 = Fε0 = Fε

where FX denotes the cdf of X.
   We initially assume that the analyst has access to the minimal relevant information set and uses it.
That is, we assume that
                                                 IE = {f1 , f2 } .

In this case, in large samples the estimated eﬀect of treatment is

                   b 1 (IE ) = Pr (Y1 = 1, D = 1|f1 , f2 ) = Pr (Y1 = 1|f1 , f2 ) = ∆1 (IE ).
                   ∆
                               Pr (Y0 = 1, D = 0|f1 , f2 )   Pr (Y0 = 1|f1 , f2 )
   Under the null of no treatment eﬀect, ∆1 = ∆2 = 1. Conditioning on (f1 , f2 ) removes any dependence
on D, and we can replace the denominator of ∆1 by Pr (Y0 = 1, D = 0|f1 , f2 ). If we do not condition on
information that contains the minimal relevant information set, this is no longer true. In general:
                               Pr (Y1 = 1, D = 1|IE )    Pr (Y1 = 1, D = 1|IE )  b 1 (IE ) .
                  ∆1 (IE ) =                          6=                        =∆
                               Pr (Y0 = 1, D = 1|IE )    Pr (Y0 = 1, D = 0|IE )
The biases can be substantial. Suppose that IE00 = {f2 } and consider the following simulations. Assume
that the true model is

                                                   α11 = α01 = αV 1 = 1

                                                   α12 = α02 = 1

                                                    µ1 = µ0 = µV = −1

                                    (ε1 , ε0 , εV , f1 , f2 ) ∼ N (0, Σ)

where Σ is the identity matrix. Values of αV 2 are speciÞed in the examples presented below. Given these
assumptions, there is no eﬀect of treatment so ∆1 = 1. In Þgures 7 and 8 we show what happens when the
analyst uses the population counterpart to the matching estimator:
                                         ¡ ¢                         00
                                      b 1 IE00 = Pr ¡(Y1 = 1, D = 1|IE )¢
                                      ∆
                                                 Pr Y0 = 1, D = 0|IE00

                                                        26
to measure the eﬀect of treatment. Figure 7 illustrates the case in which we assume that αV 2 = 1
whereas Þgure 8 shows the case of αV 2 = −1. In both cases matching does not estimate the true eﬀect of
treatment when the analyst uses information that does not contain the full minimal relevant information
set. Furthermore, the discrepancy between the estimate and the true eﬀect of treatment changes as we
change the level of f2 on which we are conditioning. Depending on the choice of f2 , we get either positive
or negative estimated treatment eﬀects. This result is again analogous to the continuous case result stating
that matching estimates are biased when the analyst does not use the minimal relevant information set.
Figures 9-14 show that equivalent results hold for the case in which the eﬀect of treatment is deÞned by
odds ratios
                                                        Pr(Y1 =1,D=1|IE )
                                                        Pr(Y1 =0,D=1|IE )
                                           ∆2 (IE ) =   Pr(Y0 =1,D=1|IE )
                                                        Pr(Y0 =0,D=1|IE )

and the analyst uses
                                                                      00
                                                        Pr(Y1 =1,D=1|IE  )
                                              ¡ ¢       Pr(Y1 =0,D=1|IE
                                                                      00
                                                                         )
                                           b 2 IE00 =
                                           ∆
                                                        Pr(Y0 =1,D=0|IE
                                                                      00
                                                                         )
                                                        Pr(Y0 =0,D=0|IE
                                                                      00
                                                                         )
                            b 1 and ∆
or the log versions of both ∆       b 2.


6.6   On the use of model selection criteria to choose matching variables

We have just shown that adding more variables from the minimal relevant information set, but not all
variables in it, may increase bias. There are no rigorously justiÞed algorithms for identifying a relevant
information set. Adding variables that are statistically signiÞcant in the treatment choice equation is not
guaranteed to select a set of conditioning variables that satisÞes condition (M-1). This is demonstrated
by the analysis of section (6.3) that shows that adding f2 when it determines D may increase bias. The
existing literature (e.g., Heckman, Ichimura and Todd, 1997) proposes other criteria based on selecting the
set of variables that maximizes some goodness of Þt criteria (λ) where a lower λ means a better Þt. The
intuition behind such criteria is that by using some measure of goodness of Þt as a guiding principle one is
using information relevant to the decision process. It is clear that knowing f2 improves goodness of Þt so
that in general such a rule is deÞcient if f1 is not known.
   An implicit assumption underlying such procedures is that the added conditioning variables C are
exogenous in the following sense


                                              (Y0 , Y1 ) ⊥⊥ D|IE , C                                  (M-4)



                                                        27
where IE is interpreted as the variables initially used as conditioning variables before C is added. Failure
of exogeneity is a failure of (M-1), and matching estimators are biased.
   In the literature, the use of such rules of thumb is justiÞed in two diﬀerent ways. Sometimes it is claimed
that they provide a relative guide. Sets of variables with lower λ (better goodness of Þt) are alleged to be
better than sets of variables with higher λ in the sense that they generate lower biases. However, we have
already shown that this is not true. We know that enlarging the analyst’s information from IE = {Z} to
IE0 = {Z, f2 } will improve Þt since f2 is also in IA . But, going from IE to IE0 might increase the bias. So, it
is not true that combinations of variables that decrease some measure of discrepancy λ necessarily reduce
the bias. Table 4 illustrates this point using a normal example. Going from row 1 to row 2, adding f2
improves goodness of Þt and increases bias for all three treatment parameters, because (M-4) is violated.
   A rule of thumb is sometimes invoked as an absolute standard against which to compare. The argument
is as follows. The analyst asserts that there is a combination of variables I 00 that satisfy (M-1) and hence
produces zero bias and a value of λ = λ00 smaller than that of any other I. Now we know that conditioning
on {Z, f1 , f2 } generates zero bias. However, we can exclude Z and still get zero bias. Since Z is a
determinant of D this shows immediately that the best Þtting model does not necessarily identify the
minimal relevant information set. In this example including Z is innocuous because there is still zero bias
and the add conditioning variables satisÞes (M-4). In general, such a rule is not innocuous. If goodness of
Þt is used as a rule to choose variables on which to match, there is no guarantee it produces a desirable
conditioning set. If we include in the conditioning set variables C that violate (M-4), they may improve
the Þt of predicted probabilities but worsen bias.
                                                                  e
   We can always construct a collection of conditioning variables IeE with a better Þt and a larger bias
than can be obtained from just conditioning on {f1 , f2 }. Let

                                                  ee
                                                  I E = {Z, S}

where
                                            S = V − Zγ + η
                                                  ¡      ¢
                                            η ∼ N 0, σ2η
                                            η⊥
                                             ⊥ (f1 , f2 , ε0 , ε1 , εV ) .




                                                         28
                                                                            e
                                                                            ej (j = 0, 1) instead of β j where:
The expressions for the biases are the same as in equations (3) - (5) using β
                                             ³                                   ´
                                    e       π α11 αV 1 σ 2f1 + α12 αV 2 σ 2f2
                                    e1 =
                                    β        q
                                               α2V 1 σ 2f1 + α2V 2 σ 2f2 + σ 2εV
                                             ³                                   ´
                                    e       π α01 αV 1 σ 2f1 + α02 αV 2 σ 2f2
                                    e0 =
                                    β        q
                                               α2V 1 σ 2f1 + α2V 2 σ 2f2 + σ 2εV
                                                               ση
                                    π =     q                                          .
                                              α2V 1 σ 2f1 + α2V 2 σ 2f2 + σ 2εV + σ 2η

   In general, these expressions are not zero so that using propensity score matching will generate a bias.
The source of the bias is the measurement error in S for V. Now, to prove that this combination of variables
has a better Þt all we need do is arbitrarily reduce σ 2η . In particular, when σ 2η = 0 we can perfectly predict
D. That is, for
                                                 2ε > σ2η > ε > 0

then

                               lim Pr (D = 1|V − Zγ + η, Z) = 1 for V > 0
                              ε→0

                               lim Pr (D = 1|V − Zγ + η, Z) = 0 for V < 0.
                              ε→0


However, when the limit is attained assumption (M-2) is violated and matching breaks down. Making σ 2η
arbitrarily small, we can predict D arbitrarily well so we can always decrease λ enough to get a combination
of variables with better Þt for predicted probabilities and larger bias than a model that conditions only on
the minimal relevant information f1 and f2 .
   Table 4 illustrates this point by generating two such variables (S1 , S2 ) and showing that, by reducing σ 2η ,
we are able to increase either of two goodness of Þt criteria (the percentage of correct in sample predictions
of D and the pseudo R2 ) above those of the model with IE = IR . Adding a model based on S2 and Z
(bottom row) increases the successful prediction rate over the case when the true model is used (the model
based on {Z, f1 , f2 }) but it is biased for all parameters and substantially biased for AT E and M T E.
   The essential feature of this example is that the selected conditioning variables are endogenous with
respect to the outcome equation (they violate (M-4)). If all candidate conditioning variables were restricted
to be exogenous, our example could not be constructed. This underscores the importance of the econometric
concept of endogeneity which is sometimes viewed as an inessential distinction in matching. Although it
is irrelevant for deÞning parameters, it is essential when selecting conditioning variables.



                                                          29
7    Concluding remarks

This paper considers three main points regarding the use of the propensity score in econometric evaluation
methods. The Þrst point is that the economic and statistical assumptions required to justify the use of the
propensity score are diﬀerent in selection, matching and instrumental variables models. In general, one set
of assumptions neither implies nor is implied by the other. In the case of additive separability of outcome
equations, matching models are a special case of selection models that assumes that conditioning eliminates
bias whereas control function methods model selection bias. Matching makes strong assumptions that are
not required in the method of control functions. It assumes that conditional on observables the marginal
return is the average return. One beneÞt of such strong assumptions is weaker assumptions about other
features of the underlying economic model. Matching does not require separability of outcomes, exogeneity
of regressors or exclusion restrictions provided valid conditioning sets are known.
    The second main point is that the literature on matching provides no guidance on the choice of the
conditioning variables that generate identiÞcation. We deÞne the concept of the “minimum relevant”
conditioning set that is assumed in matching. In general, it diﬀers from the information set available to
the analyst. Adding more “minimum relevant” variables but not all is not guaranteed to reduce bias and
we oﬀer examples of this point.
    Our third main point is that the model selection criteria advocated to pick the variables in the condi-
tioning set are not guaranteed to work. We oﬀer examples where goodness of Þt criteria advocated in the
literature select conditioning sets that generate more bias than conditioning sets that are less successful in
terms of model selection criterion. The methods work for choice among exogenous conditioning variables.
This highlights the point that the econometric distinctions of exogeneity and endogeneity play crucial roles
in the application of matching in the choice of conditioning sets.
    The sensitivity of estimates obtained from matching to the choice of conditioning variables, the inability
of the method to model omitted relevant conditioning variables and the lack of any clear rule for selecting
conditioning variables should give pause to economists who embrace this method.24 More robust methods
based on the control function approach are more sensitive to problems of omitted conditioning variables.
Recent semiparametric advances in the development of control functions make these procedures less vul-
nerable to the distributional assumptions that plagued the earlier literature on the topic (see Powell, 1994,
and Heckman and Vytlacil, 2003).




                                                     30
Appendix

Consider a general model of the form:

                                    Y1 = µ1 + U1

                                    Y0 = µ0 + U0

                                    V   = µV (Z) + UV

                                    D = 1 if V ≥ 0, = 0 otherwise

                                    Y   = DY1 + (1 − D) Y0 .

where

                                        (U1 , U0 , UV )0 ∼ N (0, Σ)

                                            var (Ui ) = σ 2i

                                         cov (Ui , Uj ) = σ ij

                                         i = 0; j = 1

                                         cov (U1 , V ) = σ 1V

                                         cov (U0 , V ) = σ 0V

Let φ (·) and Φ (·) be the pdf and the cdf of a standard normal random variable. Then, the propensity
score for this model is given by:

                      Pr (V > 0|µV (Z)) = P (µV (Z)) = Pr (UV > −µV (Z)) = p
                                                µ         ¶
                                                  −µV (Z)
                                        = 1−Φ               =p
                                                    σV

so
                                        −µV (Z)
                                                = Φ−1 (1 − p) .
                                           σV
                    ³                     ´
     Since the event V S 0, P (µV (Z)) = p can be written as

                                           UV            µV (Z)
                                                S −
                                           σV              σV
                                           UV
                                                S Φ−1 (1 − p)
                                           σV

we can write the conditional expectations required to get the biases deÞned in Section (4) as a function of



                                                    31
p. For U1 :
                                                       µ                                         ¶
                                               σ 1V      UV         UV   −µV (Z)
               E (U1 |V > 0, P (µV (Z)) = p) =      E             |    >         , P (µV (Z)) = p
                                                σV       σV         σV     σV
                                                       µ                            ¶
                                               σ 1V      UV         UV    −1
                                             =      E             |    > Φ (1 − p)
                                                σV       σV         σV
                                             = β 1 M1 (p)

                                                      µ                                   ¶
                                              σ 1V      UV UV   −µV (Z)
              E (U1 |V = 0, P (µV (Z)) = p) =      E      |   =          , P (µV (Z)) = p
                                               σV       σV σV     σV
                                                      µ                                     ¶
                                              σ 1V      UV UV
                                            =      E      |   = Φ−1 (1 − p) , P (µV (Z)) = p
                                               σV       σV σV
                                                   −1
                                            = β 1 Φ (1 − p)

where
                                                         σ 1V
                                                  β1 =
                                                         σV
Similarly for U0 :

                                 E (U0 |V > 0, P (µV ) = p) = β 0 M1 (p)

                                 E (U0 |V < 0, P (µV ) = p) = β 0 M0 (p)

                                 E (U0 |V = 0, P (µV ) = p) = β 0 Φ−1 (1 − p)

where
                                                         σ 0V
                                                 β0 =         .
                                                         σV
and
                                                   ¡           ¢
                                                 φ Φ−1 (1 − p)
                                        M1 (p) =
                                                        p
                                                     ¡ −1        ¢
                                                   φ Φ (1 − p)
                                        M0 (p) = −
                                                       (1 − p)

are inverse Mills ratio terms.
Substituting these into the expressions for the biases

                                    Bias T T (p) = β 0 M1 (p) − β 0 M0 (p)

                                                  = β 0 M (p)


                                 Bias AT E (p) = β 1 M1 (p) − β 0 M0 (p)

                                                 = M (p) (β 1 (1 − p) + β 0 p)

                                                      32
        Bias M T E = β 1 M1 (p) − β 0 M0 (p) − β 1 Φ−1 (1 − p) + β 0 Φ−1 (1 − p)

                     = M (p) (β 1 (1 − p) + β 0 p) − Φ−1 (1 − p) [β 1 − β 0 ] .

where
                                                    φ(Φ−1 (1 − p))
                      M (p) = M1 (p) − M0 (p) =
                                                      p(1 − p)




                                            33
Bibliography

  1. Aakvik, Arild, James J. Heckman, and Edward Vytlacil, “Estimating Treatment Eﬀects for Discrete
    Outcomes When Responses to Treatment Vary: An Application to Norwegian Vocational Rehabili-
    tation Programs,” The Journal of Econometrics (forthcoming, 2003).

  2. Abadie, Alberto, (2002) “Semiparametric Diﬀerence-in-diﬀerences Estimators,” Unpublished Manuscript,
    Harvard University.

  3. Ahn, Hyungtaik and James L. Powell, “Semiparametric Estimation of Censored Selection Models
    with a Nonparametric Selection Mechanism,” The Journal of Econometrics 58:1-2 (1993), 3-29.

  4. Andrews, Donald W.K. and Marcia M.A. Schafgans, “Semiparametric Estimation of the Intercept of
    a Sample Selection Model,” The Review of Economic Studies 65:3 (1998), 497-518.

  5. Björklund, Anders and Robert Moﬃtt, “The Estimation of Wage Gains and Welfare Gains in Self-
    selection,” The Review of Economics and Statistics 69:1 (1987), 42-49.

  6. Cameron, Stephen V. and James J. Heckman, “Life Cycle Schooling and Educational Selectivity:
    Models and Choice,” Journal of Political Economy 106:2, (1998), 262-333

  7. Carneiro, Pedro, “Heterogeneity in the Returns to Schooling: Implications for Policy Evaluation,”
    Unpublished Ph.D. Thesis University of Chicago (2002).

  8. Carneiro, Pedro, Karsten Hansen, and James J. Heckman, “Removing the Veil of Ignorance in As-
    sessing the Distributional Impacts of Social Policies,” Swedish Economic Policy Review 8, (2001),
    273-301

  9. _____, “Estimating Distributions of Treatment Eﬀects with an Application to the Returns to
    Schooling and Measurement of the Eﬀects of Uncertainty on College Choice,” (forthcoming 2003)
    International Economic Review, May.

 10. Carneiro, Pedro, James J. Heckman, and Edward Vytlacil, “Estimating the Return to Education
    When it Varies Among Individuals,” Working paper, University of Chicago (2001).

 11. Dehejia, Rajeev and Sadek Wahba, “Causal Eﬀects in Nonexperimental Studies: Reevaluating the
    Evaluation of Training Programs,” Journal of the American Statistical Association 94:448, (1999),
    1053-1062.


                                                 34
12. GerÞn, Michael and Lechner, Michael, “A Microeconometric Evaluation of the Active Labor Market
   Policy in Switzerland,” The Economic Journal 112 October, (2002), 854-893.

13. Gill, Richard D. and James M. Robins, “Causal inference for complex longitudinal data: the contin-
   uous case,” The Annals of Statistics 29:6, (2001), 1-27.

14. Hahn, Jinyong, “On the Role of the Propensity Score in Eﬃcient Semiparametric Estimation of
   Average Treatment Eﬀects,” Econometrica 66:2, (1998), 315-332.

15. Hansen, Karsten, James J. Heckman and Kathleen Mullen, “The Eﬀect of Schooling and Ability on
   Achievement Test Scores,” The Journal of Econometrics (forthcoming, 2003).

16. Heckman, James J., “Addendum to Sample Selection Bias as a SpeciÞcation Error,” in Ernst Stroms-
   dorfer and George Farkas (Eds.), Evaluation Studies Review Annual Vol. 5, (Beverly Hills, CA: Sage
   Publications, 1980)

17. _____, “Varieties of Selection Bias,” American Economic Review 80:2, (1990), 313-318.

18. _____, “Randomization and Social Policy Evaluation,” in Charles Manski and Irwin GarÞnkel
   (Eds.), Evaluating Welfare and Training Programs (Cambridge: Harvard University Press, 1992).

19. _____, “Instrumental Variables: A Study of Implicit Behavioral Assumptions Used in Making
   Program Evaluations,” The Journal of Human Resources 32:3, (1997), 441-462.

20. _____, “Detecting Discrimination,” Journal of Economic Perspectives 12:2, (1998), 101-116.

21. _____, “Micro Data, Heterogeneity, and the Evaluation of Public Policy: Nobel Lecture,” Journal
   of Political Economy 109:4, (2001), 673-748.

22. Heckman, James J. and V. Joseph Hotz, “Choosing Among Alternative Nonexperimental Methods
   for Estimating the Impact of Social Programs: The Case of Manpower Training (in Applications and
   Case Studies),” Journal of the American Statistical Association 84:408, (Deccember, 1989), 862-874.

23. Heckman, James J., Hidehiko Ichimura, Jeﬀrey Smith, and Petra Todd, “Characterizing Selection
   Bias Using Experimental Data,” Econometrica 66, (1998), 1017 -1098.

24. Heckman, James J., Hidehiko Ichimura, and Petra Todd, “Matching as an Econometric Evaluation
   Estimator: Evidence from Evaluating a Job Training Program,” The Review of Economic Studies
   64:4, (1997), 605-654.

                                                  35
25. _____, “Matching as an Econometric Evaluation Estimator,” The Review of Economic Studies
   65:2, (1998), 261-294.

26. Heckman, James J. and Richard Robb, “Alternative Methods for Estimating The Impact of Inter-
   ventions,” In James J. Heckman and Burton Singer (Eds.), Longitudinal Analysis of Labor Market
   Data (Cambridge: Cambridge University Press, 1985).

27. _____, “Alternative Methods for Solving the Problem of Selection Bias in Evaluating the Impact
   of Treatments on Outcomes,” in Howard Wainer (Ed.), Drawing Inferences from Self-selected Samples
   (New Jersey: Lawrence Erlbaum Associates, 1986. Reprinted 2000).

28. Heckman, James J., Jeﬀrey Smith and Nancy Clements, “Making the Most Out of Programme
   Evaluations and Social Experiments: Accounting for Heterogeneity in Programme Impacts,” Review
   of Economic Studies 64:4, (1997), 487-535.

29. Heckman, James J., Justin Tobias and Edward Vytlacil, “Four Parameters of Interest in the Evalu-
   ation of Social Programs,” Southern Economic Journal (2001), 68(2), 210-223..

30. Heckman, James J. and Edward Vytlacil, “Local Instrumental Variables and Latent Variable Models
   for Identifying and Bounding Treatment Eﬀects,” Proceedings of the National Academy of Sciences
   96, (1999), 4730-4734.

31. _____, “The Relationship Between Treatment Parameters within a Latent Variable Framework,”
   Economics Letters 66:1, (2000), 33-39.

32. _____, “Local Instrumental Variables,” in Cheng Hsiao, Kimio Morimune, and James Powell
   (Eds.), Nonlinear statistical modeling :proceedings of the thirteenth International Symposium in Eco-
   nomic Theory and Econometrics : essays in honor of Takeshi Amemiya (New York : Cambridge
   University Press, 2001).

33. _____, “Econometric Program Evaluation,” in James Heckman and Edward Leamer (Eds.), Hand-
   book of Econometrics, Volume 5, (Amsterdam: Elsevier, forthcoming 2003).

34. Imbens, Guido and Joshua Angrist “IdentiÞcation and Estimation of Local Average Treatment Ef-
   fects,” Econometrica 62:2., (1994), 467-475.

35. LaLonde, Robert, “Evaluating the Econometric Evaluations of Training Programs with Experimental
   Data,” American Economic Review 76:4, (1986), 604-20.

                                                  36
36. Navarro-Lozano, Salvador, (2002) “The Importance of Being Formal: Testing for Segmentation in
   the Mexican Labor Market,” Unpublished Manuscript, University of Chicago.

37. Olley, G. Steven and Ariel Pakes, “The Dynamics of Productivity in the Telecommunications Equip-
   ment Industry,” Econometrica 64:6 (1996), 1263-97.

38. Powell, James, “Estimation of Semiparametric Models,” In Robert F. Engle and Daniel L. McFadden
   (Eds.), Handbook of Econometrics Vol. 4 (Amsterdam, London and New York: Elsevier, North-
   Holland, 1994).

39. Robins, James M, “Causal Inference from Complex Longitudinal Data Latent Variable Modeling and
   Applications to Causality,” in M. Berkane, (Ed), Lecture Notes in Statistics (New York: Springer
   Verlag, 1997).

40. Rosenbaum, Paul, Observational Studies, New York: Springer-Verlag. First edition 1995, second
   edition 2002.

41. Rosenbaum, Paul and Donald Rubin, “The Central Role of the Propensity Score in Observational
   Studies for Causal Eﬀects,” Biometrika 70:1, (1983), 41-55.

42. Smith, Jeﬀrey and Petra Todd, “Reconciling Conßicting Evidence on the Performance of Propensity-
   Score Matching Methods,” American Economic Review 91:2, (2001), 112-18.

43. Smith, Jeﬀrey and Petra Todd, “Is Matching the Answer to LaLonde’s Critique of Nonexperimental
   Methods?,” Forthcoming Journal of Econometrics.

44. Vijverberg, Wim, “Measuring the UnidentiÞed Parameter of the Roy Model of Selectivity,” The
   Journal of Econometrics 57:1-3, (1993), 69-89.

45. Vytlacil, Edward, “Independence, Monotonicity, and Latent Index Models: An Equivalence Result,”
   Econometrica 70:1, (2002), 331-341.




                                                37
Notes
 1  See, e.g., Olley and Pakes (1996) who confuse the use of the propensity score in matching and in
control function methods.
   2 Heckman,     Ichimura and Todd (1997) introduced this distinction into matching models.
   3 Papers    that account for estimated P include Heckman, Ichimura and Todd (1997, 1998), and Hahn
(1998).
   4 See   Heckman, Ichimura and Todd (1997) and Abadie (2002).
   5 As    demonstrated in Carneiro (2002), one can still distinguish marginal and average eﬀects in terms of
observables.
   6 The    precise condition is that Support (X|Z) = Support (X) .
   7 As   noted by Heckman, Ichimura, Smith and Todd (1998), if one seeks to identify E (Y1 − Y0 |D = 1, W )
one only needs to impose a weaker condition (E (Y0 |D = 1, W )) = E (Y0 |D = 0, W ) or Y0 ⊥
                                                                                          ⊥ D|W rather
than (M-1). This imposes the assumption of no selection on levels of Y0 (given W ) and not the assumption
of no selection on levels of Y1 or change, as (M-1) does.
   8 Examples     of nonseparable models are found in Cameron and Heckman (1998).
   9 Or    mean independent in the case of mean parameters.
  10 Heckman     and Robb (1985, 1986) introduce this general formulation of control functions. The identiÞa-
bility requires that the members of the pairs (µ1 (X) , E (U1 |X, Z, D = 1)) and (µ0 (X) , E (U0 |X, Z, D = 0))
be “variation free” or “measurably separable” so that they can be independently varied against each other.
See Heckman and Vytlacil (2003) for a precise statement of these conditions.
  11 More    precisely, Support (Z|X) = Support (Z). This is also the support condition used in the general-
ization of matching by Heckman, Ichimura and Todd (1997).
  12 This    condition is sometimes called “identiÞcation at inÞnity.” See Heckman (1990) or Andrews and
Schafgans (1998).
  13 Since


                          E (U0 ) = 0

                                 = E (U0 |D = 1, Z) P (Z) + E (U0 |D = 0, Z) (1 − P (Z))
                                     (1 − P (Z))                      (1 − P (Z))
                E (U0 |D = 1, Z) = −             E (U0 |D = 0, Z) = −             K0 (P (Z))
                                        P (Z)                            P (Z)
    See Heckman and Robb (1986).
  14 For   many common functional forms for the distributions of unobservables, no exclusion is required.
  15 Relaxing    it, however, requires that the analyst model the dependence of the unobservables on the
observables and that certain variation-free conditions are satisÞed (See Heckman and Robb, 1985).

                                                       38
  16 See   also Viverberg (1993) who does such a sensitivity analysis in a parametric model with an uniden-
tiÞed parameter.
  17 Proof:



                          E (Y |X, P (Z)) = E (Y1 |D = 1, X, P (Z)) P (Z)

                                             +E (Y0 |D = 0, X, P (Z)) (1 − P (Z))
                                             Z ∞Z ∞
                                           =            y1 f (y1 , UV∗ |X) dUV∗ dy1
                                                −∞       −P (Z)
                                                    Z   ∞ Z −P (Z)
                                                +                    y0 f (y0 , UV∗ |X) dUV∗ dy0
                                                    −∞     −∞

where UV∗ = FV (UV ) . Thus

                             ∂E (Y |X, P (Z))
                                                = E (Y1 − Y0 |X, UV∗ = −P (Z))
                                 ∂P (Z)
                                                = M T E.

  18 See   also the discussion in GerÞn and Lechner (2002).
  19 We    start with a primitive probability space (Ω, σ, P ) with associated random variables I. We use
minimal sigma algebras and assume the I are measurable with respect to these random variables.
  20 This   formulation assumes that the agent makes the treatment decision. If not, then we mean by the
agent, the decision maker.
  21 Models   that take this form are known as factor models and have been applied in the context of selection
by Aakvik, Heckman and Vytacil (2003), Carneiro, Hansen and Heckman (2001, 2003) Hansen, Heckman
and Mullen (2003) and Navarro-Lozano (2002) among others.
  22 For   example, the returns to schooling literature often uses diﬀerent test scores, like AFQT or IQ, to
proxy for missing ability variables.
  23 See   Aakvik, Heckman and Vytlacil (2003) for an analysis of discrete treatment eﬀects in a latent
variables model. See also Heckman (1998) where this framework originates.
  24 A   widely cited paper by Dehejia and Wahba (1999) claims that matching overcomes the sensitivity to
estimators problem displayed by LaLonde (1986). Smith and Todd (2001, 2003) show that the Dehejia-
Wahba results were manufactured by selectively discarding data from LaLonde’s original sample and that
when the full sample is used matching produces substantial biases. Matching does not solve the LaLonde
sensitivity problem.




                                                          39
                                Table 1
            Mean Bias for Treatment on the Treated
      ρ0V            Average Bias (σ0=1)   Average Bias (σ0=2)
     -1.00                 -1.7920               -3.5839
     -0.75                 -1.3440               -2.6879
     -0.50                 -0.8960               -1.7920
     -0.25                 -0.4480               -0.8960
       0                       0                     0
      0.25                  0.4480                0.8960
      0.50                  0.8960                1.7920
      0.75                  1.3440                2.6879
      1.00                  1.7920                3.5839
BIASTT =   ρ0V*σ0*M(p)
M(p) = φ(Φ (p))   / [p*(1-p)]
            -1
                                              Table 2
                              Mean Bias for Average Treatment Effect*
                                               (σ0=1)
                                                            ρ1V (σ1=1)
     ρ0V           -1.00        -0.75     -0.50     -0.25        0         0.25      0.50      0.75     1.00
   -1.00          -1.7920     -1.5680   -1.3440   -1.1200    -0.8960     -0.6720   -0.4480   -0.2240      0
   -0.75          -1.5680     -1.3440   -1.1200   -0.8960    -0.6720     -0.4480   -0.2240       0     0.2240
   -0.50          -1.3440     -1.1200   -0.8960   -0.6720    -0.4480     -0.2240       0      0.2240   0.4480
   -0.25          -1.1200     -0.8960   -0.6720   -0.4480    -0.2240         0      0.2240    0.4480   0.6720
     0            -0.8960     -0.6720   -0.4480   -0.2240        0        0.2240    0.4480    0.6720   0.8960
    0.25          -0.6720     -0.4480   -0.2240       0       0.2240      0.4480    0.6720    0.8960   1.1200
    0.50          -0.4480     -0.2240       0      0.2240     0.4480      0.6720    0.8960    1.1200   1.3440
    0.75          -0.2240         0      0.2240    0.4480     0.6720      0.8960    1.1200    1.3440   1.5680
    1.00             0         0.2240    0.4480    0.6720     0.8960      1.1200    1.3440    1.5680   1.7920

                                                            ρ1V (σ1=2)
     ρ0V           -1.00       -0.75     -0.50      -0.25        0         0.25     0.50      0.75      1.00
   -1.00          -2.6879     -2.2399   -1.7920   -1.3440    -0.8960     -0.4480      0      0.4480    0.8960
   -0.75          -2.4639     -2.0159   -1.5680   -1.1200    -0.6720     -0.2240   0.2240    0.6720    1.1200
   -0.50          -2.2399     -1.7920   -1.3440   -0.8960    -0.4480         0     0.4480    0.8960    1.3440
   -0.25          -2.0159     -1.5680   -1.1200   -0.6720    -0.2240      0.2240   0.6720    1.1200    1.5680
     0            -1.7920     -1.3440   -0.8960   -0.4480        0        0.4480   0.8960    1.3440    1.7920
    0.25          -1.5680     -1.1200   -0.6720   -0.2240     0.2240      0.6720   1.1200    1.5680    2.0159
    0.50          -1.3440     -0.8960   -0.4480       0       0.4480      0.8960   1.3440    1.7920    2.2399
    0.75          -1.1200     -0.6720   -0.2240    0.2240     0.6720      1.1200   1.5680    2.0159    2.4639
    1.00          -0.8960     -0.4480      0       0.4480     0.8960      1.3440   1.7920    2.2399    2.6879
*Equal to the Mean Bias for the Marginal Treatment Effect
BIASATE = ρ1V*σ1*M1(p) - ρ0V*σ0*M0(p)
BIASMTE = BIASATE - Φ-1(1-p)*(ρ1V*σ1 - ρ0V*σ0)
M1(p) = φ(Φ (p))    /p
            -1

M0(p) = -φ(Φ (p))   / [1-p]
             -1
                                                                               Table 3
Method             Exclusion Required?   Separability of Observables   Functional Forms    Marginal =          Key IdentiÞcation
                                         and Unobservables             Required?          Average?             Condition for Means
                                         in Outcome Equations?                            (Given X, Z)         Assuming Separability (See text for full conditions)
Matching                   No                        No                      No                Yes         E (U1 |X, D = 1, Z) = E (U1 |X, Z)
                                                                                                           E (U0 |X, D = 0, Z) = E (U0 |X, Z)
Control Function         Yes (for              Conventional,            Conventional,          No          E (U0 |X, D = 0, Z) and
                     nonparametric            but not required         but not required                    E (U1 |X, D = 1, Z)
                      identiÞcation)                                                                       can be varied independently of
                                                                                                           µ0 (X) and µ1 (X) , respectively
                                                                                                           and intercepts can be identiÞed through limit arguments
IV                         Yes                      Yes                      No            No (Yes in      E (U0 + D (U1 − U0 ) |X, Z)
(conventional)                                                                            standard case)   = E (U0 + D (U1 − U0 ) |X) (AT E)
                                                                                                           E (U0 + D (U1 − U0 ) − E (U0 + D (U1 − U0 ) |X) |P (Z) , X)
                                                                                                           = E (U0 + D (U1 − U0 ) − E (U0 + D (U1 − U0 ) |X) |X) (T T )
LIV                        Yes                       No                      No                No          (U0 , U1 , UV) ⊥⊥ Z|X
                                        Table 4
   Variables in            Goodness of fit statistics                    Average Bias
                                                                  2
     Probit       Correct in-sample prediction rate     Pseudo R       TT      ATE      MTE
        Z                      66.88%                    0.1284       1.1380   1.6553   1.6553
       Z, f2                   75.02%                    0.2791       1.2671   1.9007   1.9007
      Z, f1, f2                83.45%                    0.4844       0.0000   0.0000   0.0000
       Z, S1                   77.59%                    0.3352       0.8603   1.2513   1.2513
       Z, S2                   92.45%                    0.7555       0.3156   0.4591   0.4591


Model:
V= Z+f1+f2+ev      ev~N(0,1)     S1 = V + u1   u1~N(0,4)
Y1=2f1+0.1f2+e1    e1~N(0,1)     S2 = V + u2   u2~N(0,0.25)
Y0=f1+0.1f2+e0     e0~N(0,1)
                   f1~N(0,1)
                   f2~N(0,1)
                                                   F igure 1                                                                                                          F igure 3                                                                                                      Figure 5
                                    B ias for T reatment on the T reated                                                                                                                                                                                               Bias for Average Treatment Effect
                                                                                                                                                          B ias for Marginal T reatment E ffect                                                                                                       ~
                        S pecial cas e: Adding relevant information f2 increas es the bias                                                  S pecial cas e: Adding relevant information f2 increas es the bias                                           Special case: Adding irrelevant information Z increases the bias
                                                                                                                                                                                                                                                                                             ~
         2                                                                                                               4                                                                                                                                                       correlation(Z,f2)=0.5
                                                                                     Matching on P (Z)                                                                                                                                   3.5
                                                                                                                                                                                                       Matching on P (Z)                                                                                            Matching on P(Z)
                                                                                     Matching on P (Z,f2)                                                                                              Matching on P (Z,f2)                                                                                                         ~
                                                                                                                                                                                                                                                                                                                    Matching on P(Z,Z)
        1.9

                                                                                                                        3.5
        1.8
                                                                                                                                                                                                                                           3

        1.7
                                                                                                                         3

        1.6
                                                                                                                                                                                                                                         2.5
B ias




                                                                                                                B ias
        1.5                                                                                                             2.5




                                                                                                                                                                                                                                  Bias
        1.4
                                                                                                                                                                                                                                           2
                                                                                                                         2
        1.3
                                                                                                                                                                    Average B ias = 1.9007

        1.2
                                                      Average B ias = 1.2671                                                                              Average B ias = 1.6553                                                         1.5                    Average Bias = 1.6553                       Average Bias = 1.7019
                                                                                                                        1.5

        1.1
                                                      Average B ias = 1.1380
         1                                                                                                               1
              0   0.1       0.2       0.3      0.4          0.5      0.6       0.7   0.8       0.9          1                 0       0.1        0.2      0.3      0.4       0.5       0.6     0.7      0.8       0.9         1            1
                                                                                                                                                                                                                                               0   0.1        0.2       0.3       0.4    0.5      0.6      0.7       0.8          0.9     1
                                                             P                                                                                                                P
                                                                                                                                                                                                                                                                                          P


                                                                                                                                                                        Figure 4                                                                                                    Figure 6
                                                 F igure 2
                                                                                                                                                          Bias for Treatment on the Treated                                                                             Bias for Marginal Treatment Effect
                                   B ias for Average T reatment E ffect                                                                                                                                                                                                                                  ~
                                                                                                                                                                                         ~                                                                  Special case: Adding irrelevant information Z increases the bias
                        S pecial cas e: Adding relevant information f2 increas es the bias                                                  Special case: Adding irrelevant information Z increases the bias                                                                                    ~
                                                                                                                                                                                ~                                                                                                   correlation(Z,f2)=0.5
          4                                                                                                                                                         correlation(Z,f2)=0.5
                                                                                     Matching on P (Z)                  1.8                                                                                                              3.2
                                                                                     Matching on P (Z,f2)                                                                                               Matching on P(Z)                                                                                             Matching on P(Z)
                                                                                                                                                                                                                        ~
                                                                                                                                                                                                        Matching on P(Z,Z)
                                                                                                                                                                                                                                                                                                                                     ~
                                                                                                                                                                                                                                                                                                                     Matching on P(Z,Z)
                                                                                                                                                                                                                                           3
                                                                                                                        1.7
        3.5

                                                                                                                                                                                                                                         2.8
                                                                                                                        1.6

          3
                                                                                                                                                                                                                                         2.6
                                                                                                                        1.5

                                                                                                                                                                                                                                         2.4
B ias




        2.5




                                                                                                                                                                                                                                  Bias
                                                                                                                Bias




                                                                                                                        1.4
                                                                                                                                                                                                                                         2.2

                                                                                                                        1.3
          2
                                                 Average B ias = 1.9007                                                                                                                                                                    2

                                                                                                                        1.2
                                                                                                                                                                                                                                         1.8
        1.5                 Average B ias = 1.6553

                                                                                                                        1.1                                                                                                              1.6
                                                                                                                                                                      Average Bias = 1.1616                                                                                                               Average Bias = 1.6553
                                                                                                                                  Average Bias = 1.1380                                                                                                        Average Bias = 1.7019
          1
              0   0.1        0.2      0.3       0.4         0.5      0.6       0.7   0.8       0.9          1             1                                                                                                              1.4
                                                             P                                                                0       0.1        0.2      0.3       0.4       0.5       0.6    0.7      0.8       0.9         1                0   0.1        0.2       0.3       0.4     0.5      0.6      0.7       0.8         0.9     1
                                                                                                                                                                               P                                                                                                           P

          Model:
          V= Z+f1+f2+ev                                      ev~N(0,1)                      f1~N(0,1)
          Y1=2f1+0.1f2+e1                                    e1~N(0,1)                      f 2~N(0,1)
          Y0=f1+0.1f2+e0                                     e0~N(0,1)
                                                 Figure 7
                      Estimated Effect of Treatment under Different Information Sets                                                                    Figure 8
                                                 No Effect of Treatment and αv2=1                                            Estimated Effect of Treatment under Different Information Sets
                                                    ^                                                                                                 No Effect of Treatment and αv2=-1
                                                    ∆1= Pr(Y  1=1,D=1|IE)
                                                         ________________
                                                         Pr(Y0=1,D=0|IE)                                                                                 ^    Pr(Y1=1,D=1|IE)
                                                                                                                                                          ∆1= ________________
      16
                                                                                        IE={f1,f2}
                                                                                                                                                              Pr(Y0=1,D=0|IE)
                                                                                                              6
                                                                                        IE={f2}                                                                                                     IE={f1,f2}
                                                                                                                                                                                                    IE={f2}
      14


                                                                                                              5
      12



      10                                                                                                      4


<
    ∆1




          8
                                                                                                          <




                                                                                                          ∆1
                                                                                                              3

          6


                                                                                                              2
          4



          2
                                                                                                              1
          1

          0
          -2        -1.5        -1      -0.5        0       0.5      1       1.5    2   2.5           3
              Model:                                        f2                                                0
                                                                                                              -1                -0.5             0                  0.5             1         1.5                2
              V= -1+f1+f2+εv         εv~N(0,1)
                                                                                                                  Model:                                            f2
              Y*1=-1+f1+f2+ε1        ε1~N(0,1)
              Y*0=-1+f1+f2+ε0        ε0~N(0,1)                                                                    V= -1+f1-f2+εv          εv~N(0,1)
              Y1=1(Y*1>0)            f1~N(0,1)                                                                    Y*1=-1+f1+f2+ε1         ε1~N(0,1)
                                                                                                                  Y*0=-1+f1+f2+ε0         ε0~N(0,1)
              Y0=1(Y*0>0)            f2~N(0,1)
                                                                                                                  Y1=1(Y*1>0)             f1~N(0,1)
              D=1(V>0)
                                                                                                                  Y0=1(Y*0>0)             f2~N(0,1)




                                               Figure 9
                    Estimated Effect of Treatment under Different Information Sets                                                                       Figure 10
                                               No Effect of Treatment and αv2=1                                                Estimated Effect of Treatment under Different Information Sets
                                                        Pr(Y  1=1,D=1|IE)
                                                        ________________                                                                                  No Effect of Treatment and αv2=-1
                                                   ^    Pr(Y1=0,D=1|IE)                                                                                             Pr(Y 1=1,D=1|IE)
                                                   ∆2= __________________                                                                                           ________________
                                                        Pr(Y  0=1,D=0|IE)                                                                                     ^     Pr(Y1=0,D=1|IE)
                                                        ________________                                                                                      ∆2= __________________
                                                        Pr(Y0=0,D=0|IE)                                                                                             Pr(Y 0=1,D=0|IE)
                                                                                                                                                                    ________________
     14
                                                                                                                                                                    Pr(Y0=0,D=0|IE)
                                                                                         IE={f1,f2}
                                                                                         IE={f2}                   25
                                                                                                                                                                                                    IE={f1,f2}
                                                                                                                                                                                                    IE={f2}
     12


                                                                                                                   20
     10




      8                                                                                                            15

<
∆2




                                                                                                              <
                                                                                                              ∆2




      6
                                                                                                                   10


      4


                                                                                                                    5
      2

      1
                                                                                                                    1
      0
      -2           -1.5     -1         -0.5         0       0.5      1       1.5    2   2.5           3             0
                                                                                                                    -1             -0.5               0               0.5               1     1.5                2
                                                            f2
          Model:                                                                                                        Model:                                        f2
          V= -1+f1+f2+εv             εv~N(0,1)                                                                          V= -1+f1-f2+εv      εv~N(0,1)
          Y*1=-1+f1+f2+ε1            ε1~N(0,1)                                                                          Y*1=-1+f1+f2+ε1     ε1~N(0,1)
          Y*0=-1+f1+f2+ε0            ε0~N(0,1)                                                                          Y*0=-1+f1+f2+ε0     ε0~N(0,1)
          Y1=1(Y*1>0)                f1~N(0,1)                                                                          Y1=1(Y*1>0)         f1~N(0,1)
          Y0=1(Y*0>0)                f2~N(0,1)                                                                          Y0=1(Y*0>0)         f2~N(0,1)
          D=1(V>0)                                                                                                      D=1(V>0)
                                                   Figure 11                                                                                                                             Figure 12
                         Estimated Effect of Treatment under Different Information Sets                                                                        Estimated Effect of Treatment under Different Information Sets
                                                         No Effect of Treatment and αv2=1                                                                                             No Effect of Treatment and αv2=-1


        3
                                                         ^
                                                                     (
                                                                    Pr(Y1=1,D=1|IE)
                                                         ∆3= Log ________________
                                                                    Pr(Y0=1,D=0|IE)          )                                                   2
                                                                                                                                                                                      ^
                                                                                                                                                                                              (
                                                                                                                                                                                      ∆3= Log ________________  )
                                                                                                                                                                                                  Pr(Y1=1,D=1|IE)
                                                                                                                                                                                                  Pr(Y0=1,D=0|IE)
                                                                                                                     IE={f1,f2}                                                                                                   IE={f1,f2}
                                                                                                                     IE={f2}                                                                                                      IE={f2}
     2.5

                                                                                                                                                 1
        2


     1.5
                                                                                                                                                 0

        1


<
∆3




                                                                                                                                            <




                                                                                                                                            ∆3
     0.5                                                                                                                                        -1


        0

                                                                                                                                                -2
    -0.5


        -1
                                                                                                                                                -3
    -1.5


        -2
         -2            -1.5          -1        -0.5          0           0.5         1           1.5         2       2.5          3             -4
                                                                                                                                                 -1                -0.5           0               0.5            1         1.5                  2
                                                                         f2
             Model:                                                                                                                                  Model:                                       f2
             V= -1+f1+f2+εv                εv~N(0,1)                                                                                                 V= -1+f1-f2+εv         εv~N(0,1)
             Y*1=-1+f1+f2+ε1               ε1~N(0,1)                                                                                                 Y*1=-1+f1+f2+ε1        ε1~N(0,1)
             Y*0=-1+f1+f2+ε0               ε0~N(0,1)                                                                                                 Y*0=-1+f1+f2+ε0        ε0~N(0,1)
             Y1=1(Y*1>0)                   f1~N(0,1)                                                                                                 Y1=1(Y*1>0)            f1~N(0,1)
             Y0=1(Y*0>0)                   f2~N(0,1)                                                                                                 Y0=1(Y*0>0)            f2~N(0,1)
             D=1(V>0)                                                                                                                                D=1(V>0)




                                                                                                                                                                                           Figure 14
                                                         Figure 13                                                                                               Estimated Effect of Treatment under Different Information Sets
                               Estimated Effect of Treatment under Different Information Sets                                                                                          No Effect of Treatment and αv2=-1



                                                                                                                                                                                           (                    )
                                                           No Effect of Treatment and αv2=1                                                                                                      Pr(Y 1=1,D=1|IE)
                                                                                                                                                                                                 ________________




                                                                 (                           )
                                                                    Pr(Y  1=1,D=1|IE)                                                                                                 ^          Pr(Y1=0,D=1|IE)
                                                                    ________________                                                                                                  ∆4=Log __________________
                                                         ^          Pr(Y1=0,D=1|IE)                                                                                                              Pr(Y 0=1,D=0|IE)
                                                         ∆4=Log __________________                                                                                                               ________________
                                                                    Pr(Y  0=1,D=0|IE)
                                                                    ________________                                                                                                             Pr(Y0=0,D=0|IE)
                                                                                                                                                 3.5
                                                                    Pr(Y0=0,D=0|IE)                                                                                                                                                IE={f1,f2}
               3                                                                                                                                                                                                                   IE={f2}
                                                                                                                           IE={f1,f2}
                                                                                                                           IE={f2}                   3

             2.5
                                                                                                                                                 2.5

               2
                                                                                                                                                     2

             1.5
                                                                                                                                                 1.5
                                                                                                                                            <
                                                                                                                                            ∆4




    <
    ∆4




               1
                                                                                                                                                     1


             0.5                                                                                                                                 0.5



               0                                                                                                                                     0


                                                                                                                                                -0.5
             -0.5


                                                                                                                                                     -1
              -1                                                                                                                                      -1             -0.5             0            0.5               1      1.5                     2
               -2             -1.5        -1          -0.5       0             0.5       1             1.5       2         2.5          3                                                          f2
                                                                               f2
                                                                                                                                                         Model:
                    Model:                                                                                                                               V= -1+f1-f2+εv      εv~N(0,1)
                    V= -1+f1+f2+εv             εv~N(0,1)                                                                                                 Y*1=-1+f1+f2+ε1     ε1~N(0,1)
                    Y*1=-1+f1+f2+ε1            ε1~N(0,1)                                                                                                 Y*0=-1+f1+f2+ε0     ε0~N(0,1)
                    Y*0=-1+f1+f2+ε0            ε0~N(0,1)
                                                                                                                                                         Y1=1(Y*1>0)         f1~N(0,1)
                    Y1=1(Y*1>0)                f1~N(0,1)
                                                                                                                                                         Y0=1(Y*0>0)         f2~N(0,1)
                    Y0=1(Y*0>0)                f2~N(0,1)
                                                                                                                                                         D=1(V>0)
                    D=1(V>0)
