                               NBER WORKING PAPER SERIES




  TESTS OF HYPOTHESES ARISING IN THE CORRELATED RANDOM COEFFICIENT
                                MODEL

                                        James J. Heckman
                                       Daniel A. Schmierer

                                       Working Paper 16421
                               http://www.nber.org/papers/w16421


                     NATIONAL BUREAU OF ECONOMIC RESEARCH
                              1050 Massachusetts Avenue
                                Cambridge, MA 02138
                                   September 2010




This research was supported by NIH R01-HD043411, NSF SES-024158, the American Bar Foundation
and the Geary Institute, University College Dublin, Ireland. The views expressed in this paper are
those of the authors and not necessarily those of the funders listed here, nor of the National Bureau
of Economic Research. We have received helpful comments from Pedro Carneiro, Jeremy Fox, Joel
Horowitz, Benjamin Moll, Azeem Shaikh, Christopher Taber, Edward Vytlacil, and Sergio Urzua.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2010 by James J. Heckman and Daniel A. Schmierer. All rights reserved. Short sections of text,
not to exceed two paragraphs, may be quoted without explicit permission provided that full credit,
including © notice, is given to the source.
Tests of Hypotheses Arising in the Correlated Random Coefficient Model
James J. Heckman and Daniel A. Schmierer
NBER Working Paper No. 16421
September 2010
JEL No. C31

                                               ABSTRACT

This paper examines the correlated random coefficient model. It extends the analysis of Swamy (1971,
1974), who pioneered the uncorrelated random coefficient model in economics. We develop the properties
of the correlated random coefficient model and derive a new representation of the variance of the instrumental
variable estimator for that model. We develop tests of the validity of the correlated random coefficient
model against the null hypothesis of the uncorrelated random coefficient model.


James J. Heckman
Department of Economics
The University of Chicago
1126 E. 59th Street
Chicago, IL 60637
and University College Dublin and IZA
and also NBER
jjh@uchicago.edu

Daniel A. Schmierer
Department of Economics
University of Chicago
1126 E. 59th Street
Chicago IL 60637
dschmier@uchicago.edu
1         Introduction

The correlated random coefficient model is the new centerpiece of a large literature in mi-
croeconometrics. It extends the classical uncorrelated random coefficient model of Swamy
(1971, 1974). For person i, outcome Yi in terms of choice indicator Di is written as


                                                Yi = αi + βi Di                                (1)


where Di = 1 if a choice is made; Di = 0 if not and both the intercept, αi , and the slope,
βi , vary among persons. In this expression both the αi and βi may depend on regressors Xi
which we keep implicit.
        βi is the causal effect of Di on Yi holding αi fixed. If agents make their choices to take
treatment based on components of βi that depend on variables not available to the observing
economist, Di is correlated with βi even after conditioning on Xi . Most recent studies focus
on estimating means or quantiles of the distribution of βi .1
        The model that motivated the research of a previous generation (see, e.g., Griliches,
1977) assumes no response heterogeneity (βi = β) or else an uncorrelated random coefficient
model as in Swamy (1971, 1974) or Mincer (1974), so βi is independent of Di . The correlated
random coefficient model assumes that βi varies in the population and in addition that


                                               Cov (Di , βi ) 6= 0.                          (C-1)


The model also accounts for selection on intercepts, i.e. selection on pretreatment unobserv-
ables:
                                              Cov (Di , αi ) 6= 0.                           (C-2)

When (C-1) holds, marginal returns to an activity in general differ from average returns.
When assumption (C-2) holds but Di is independent of βi , standard IV identifies the mean
    1
        Abbring and Heckman (2007) discuss methods for estimating the distribution of βi .



                                                        3
of βi , which we denote by β̄. This configuration of assumptions includes the case when βi is
random but independent of Di and the case when βi is the same for everyone.2,3
       As first noted by Heckman and Robb (1985, reprinted in 2008), instrumental variables
(IV) applied to (1) when (C-1) holds produces an instrument-dependent parameter that, in
general, is not β̄.4 In general, different instruments identify different parameters. Under
conditions specified in Yitzhaki (1989),5 Imbens and Angrist (1994), Heckman and Vyt-
lacil (1999), and Heckman, Urzua, and Vytlacil (2006), IV estimates weighted averages of
marginal effects. Heckman and Vytlacil (1999, 2001, 2005, 2007a) generalize the marginal
treatment effect (MTE) introduced by Björklund and Moffitt (1987) and show that the MTE
plays the role of a functional that is invariant to the choice of instrument. The MTE can be
used to unify the literature on treatment effects.6
       Heckman and Vytlacil (2001, 2005, 2007b) derive testable implications of the hypothesis
that βi is statistically independent of Di given Xi :


                                           H0 : βi ⊥
                                                   ⊥ Di | Xi ,


where A ⊥
        ⊥ B | C means A is independent of B given C. In this paper, we examine tests
of this hypothesis, drawing heavily on our previous work (Heckman, Schmierer, and Urzua,
2010). We also consider tests of the hypothesis that E(βi ) = β̄ = 0 and that the IV estimand
of (1) is zero.
       The paper proceeds as follows. Section 2 establishes the equivalence of the correlated
random coefficient model with the Generalized Roy model. We examine two testable im-
plications of it. One test exploits the insight that, in general, in the case when H0 is false,
   2
      See Heckman and Vytlacil (1998), Heckman and Vytlacil (2007a,b). The standard “ability bias” problem
(Griliches, 1977) assumes that βi = β, a constant for all i, and that Cov(Di , αi ) 6= 0.
    3
      Evidence from parametric models on the empirical relevance of (C-1) in a variety of areas of economics
is presented in Heckman (2001, Table 3).
    4
      See the discussion of the ensuing literature in Heckman, Urzua, and Vytlacil (2006) or Heckman and
Vytlacil (2007a,b).
    5
      Posted at website for Heckman, Urzua, and Vytlacil (2006), see http://jenni.uchicago.edu/
underiv/.
    6
      See Heckman and Vytlacil (2005, 2007a,b).


                                                     4
different instruments identify different parameters. Section 3 presents some new results on
the sampling distribution of the instrumental variable estimator and develops tests for β̄ = 0
and for the IV estimand to be zero. Section 4 develops tests of the correlated random co-
efficient model. Section 5 develops other tests of H0 and places all tests in the conditional
moment testing framework (Ai and Chen, 2003). Section 6 concludes.



2         Equivalence with the Generalized Roy Model and

          Two Testable Implications of H0

An alternative way to represent equation (1) makes the link to economic choice theory
more explicit. Individual i experiences outcome Y1,i if Di = 1 and outcome Y0,i if Di = 0,
i = 1, . . . , I. The observed outcome is Yi = Di Y1,i + (1 − Di )Y0,i .7 Let µj (Xi ) = E(Yj,i | Xi ),
j ∈ {0, 1}. One can write the model for potential outcomes conditional on Xi as Y1,i =
µ1 (Xi ) + U1,i and Y0,i = µ0 (Xi ) + U0,i where E(Uj,i | Xi ) = 0, j ∈ {0, 1}. In this notation,
the observed outcome is


                         Yi = µ0 (Xi ) + [µ1 (Xi ) − µ0 (Xi ) + U1,i − U0,i ] Di + U0,i .


This is the correlated random coefficient model of equation (1) where the baseline outcome
is αi = µ0 (Xi ) + U0,i and the gain is βi = µ1 (Xi ) − µ0 (Xi ) + U1,i − U0,i where, for notational
simplicity, we suppress the dependence of αi and βi on Xi . To simplify the expressions, we
drop the i subscripts throughout the rest of the paper unless their use clarifies the discussion.
We define α = α + Uα and β = β̄ + Uβ where E(Uα | X) = 0 and E(Uβ | X) = 0. Table 1
shows the equivalent parameters for the two models.
        Whether the null hypothesis H0 is true or not depends on the underlying choice model.
We postulate a threshold crossing model which assumes separability between observables
    7
        This is the Quandt (1958) switching regression model.



                                                        5
Table 1: Equivalence of Notation Between the Correlated Random Coefficient Model and the
Generalized Roy Model. All parameters are defined conditional on Xi which is left implicit.
                                                    Generalized Roy                          Correlated random
                                                        model                                coefficient model

 Baseline outcome                                   Y 0,i = μ 0 + U 0,i                              αi



 Outcome in treated state                           Y 1,i = μ 1 + U 1,i                           β i + αi



 Gain to treatment                       Y 1,i - Y 0,i = μ 1 - μ 0 + U 1,i - U 0,i                  βi

 (Individual causal effect)

 Outcome                           Y i = Y 0,i + D i (Y 1,i - Y 0,i )                        Y i = αi + β i D i
                                    = μ 0,i + (μ 1,i - μ 0,i + U 1,i - U 0,i ) D i + U 0,i



Z that affect choice and an unobservable V : D = 1(µD (Z) − V > 0), where 1(·) is an
indicator function that takes the value 1 if its argument is true and is 0 otherwise, and
µD is a deterministic function of Z.8 Z can include components of X. Letting FV be the
distribution of V conditional on X, and assuming that Z ⊥
                                                        ⊥ V | X, the choice probability
or “propensity score” is


                              P (z) = Pr(D = 1|Z = z) = FV (µD (z)),


where to simplify the notation, we keep the conditioning on X implicit. The choice equation
can be written in several alternative and equivalent ways:


               D = 1(µD (Z) − V > 0) = 1(FV (µD (Z)) > FV (V )) = 1(P (Z) > UD )


where UD = FV (V ) so UD ∼ Uniform[0, 1].
   8
    See, e.g., Thurstone (1927) and McFadden (1974, 1981). We do not strictly require separability, but we
do require that the choice equation has one representation in separable form. See Heckman and Vytlacil
(2007b).


                                                         6
       We invoke the assumptions of Heckman and Vytlacil (2005, 2007b).9 A fundamental
treatment parameter introduced by Björklund and Moffitt (1987) is the marginal treatment
effect (MTE). The MTE for a given value of X = x is


              M T E(x, uD ) = E(Y1 − Y0 | X = x, UD = uD ) = E(β | X = x, UD = uD ).


It is the mean effect of treatment when the observables X are fixed at a value x and the
unobservable in the choice equation UD is fixed at a value uD . Heckman and Vytlacil (1999,
2001, 2005, 2007b) use the MTE to develop implications of the model to test H0 .
       In the general case, the conditional expectation of Y given X and Z is


                E(Y |X = x, Z = z) = E(Y |X = x, P (Z) = p)

                                       = E(α|X = x) + E(βD|X = x, P (Z) = p)

                                       = E(α|X = x) + E(β|X = x, D = 1)p
                                                      Z p
                                       = E(α|X = x) +     E(β|X = x, UD = uD )duD ,                   (2)
                                                              0



where the integrand in the final expression is the M T E(x, uD ).10 Under H0 ,


                               E(β | X = x, UD = uD ) = E(β | X = x),
   9
       Their conditions are:

(A-1) (U0 , U1 , V ) ⊥
                     ⊥ Z | X. Alternatively, (α, β, V ) ⊥⊥ Z | X.

(A-2) The distribution of µD (Z) conditional on X is nondegenerate. Thus the distribution of P (Z) is
nondegenerate conditional on X.

(A-3) The distribution of V is continuous (i.e., absolutely continuous with respect to Lebesgue measure).
Thus UD = FV (V ) is uniform.

(A-4) E |Y1 | < ∞ and E |Y0 | < ∞, so defining E(β) = β̄, |β̄| < ∞.

(A-5) 1 > Pr (D = 1 | X) > 0.

Vytlacil (2002) shows that under mild regularity conditions, assumptions (A-1)-(A-5) are equivalent to the
IV conditions of Imbens and Angrist (1994) used to define the local average treatment effect (LATE).
  10
     The first line follows from (A-1). The rest of the derivation comes from (1) and the law of iterated
expectations.


                                                       7
so
                  E(Y | X = x, P (Z) = p) = E(α | X = x) + E(β | X = x)p.11                                 (3)

Thus the function E(Y |X = x, P (Z) = p) is linear in p, conditional on X = x, which is a
testable hypothesis.
       A second implication of H0 is that any standard instrument identifies β = E(β).12 Thus
under H0 all valid instruments have the same estimand. Under conditions presented in this
paper, comparing the estimates produced by different instruments tests the weaker hypoth-
esis H00 : Cov(β, D | X) = 0, which is an implication of the stronger hypothesis H0 . The
analysis in this paper thus provides an alternative interpretation of standard tests of overi-
dentification. A rejection of the null hypothesis that two instrumental variable estimands
are different is not necessarily a rejection of the validity of one instrument. It could be
interpreted as evidence in support of a correlated random coefficient model.



3        General Properties of the IV Estimator for the Cor-

         related Random Coefficient Model and Tests of the

         Hypotheses β̄ = 0 and that the IV Estimand Is Zero

We present a new representation of the sampling distribution of the IV estimator. We
consider the problem of constructing the power of tests of several hypotheses using the
sampling distribution of the IV estimator for the correlated random coefficient model.
  11
     To see this, notice that β ⊥⊥ D | X ⇐⇒ β ⊥⊥ 1(P (Z) > UD ) | X ⇐⇒ β ⊥⊥ UD | X given (A-1).
  12
     In the notation of equation (1), but dropping subscripts i, a standard instrument J has the two properties:
(i) Cov(J, D | X) 6= 0 and (ii) Cov((α, β), J | X) = 0. Note that J is shorthand for J(Z). Note further that
the condition Cov(β, J | X) = 0 only emerges as an interesting condition in a random coefficient model.




                                                       8
3.1     IV in the Correlated Random Coefficient Model

Consider an instrument J(Z). Denote J(Z) by J and define J˜ = J − J¯ where J¯ is the sample
mean of J(Z). E(J) is assumed to be finite. The IV estimator is

                                                               P ˜
                                                                 Yi Ji
                                                βbIV,J        =P        .
                                                                 Di J˜i

Define Cov(J, D) = ωJ and let I denote the sample size. Under a weak law of large numbers,
  P ˜ p                p
1
I
    Di Ji → ωJ and J¯ → E(J). As shown in Heckman and Vytlacil (2005, 2007b), under
the conditions (A-1)–(A-5) stated in Section 2,

                                                     Z    1
                              p
                       βbIV,J → βIV,J =                       E(β|UD = uD )hJ (uD )duD   (4)
                                                      0



where
                                   E[(J − E(J)) | P (Z) > uD ] Pr(P (Z) > uD )
                  hJ (uD ) =                                                   ,         (5)
                                                      ωJ

and we keep the conditioning on X implicit. Heckman and Vytlacil (2005) show that
R1
   h (t)dt = 1. Thus we can write
 0 J

                                            Z    1
                       βIV,J = β +                   E(Uβ | UD = uD )hJ (uD )duD .       (6)
                                             0



For later use we break out the component of βIV,J that depends on the instrument J:

                           Z       1
                                       E(Uβ | UD = uD )hJ (uD )duD = ΥJ ,
                               0



so βIV,J = β̄ + ΥJ . By definition, conditional on X, β̄ does not depend on J.
   Under independent sampling,

                                       √               
                                                          d
                                        I βbIV,J − βIV,J → N (0, ΩJ )




                                                                9
where

                              Z1
                2  Var(J)
                                  2E (αβ | UD = uD ) + E β 2 | UD = uD hΩJ (uD )duD
                                                                     
         ΩJ = E α       2
                            +                                                                       (7)
                       ωJ
                              0
                  1                            2
                    Z
               −  E(β | UD = uD )hJ (uD )duD 
                        0


and

                                        Z∞                     Z1
                                  1                        2
                      hΩJ (uD ) = 2          (j − E(J))             fP,J (P (z), j)dP (z)dj         (8)
                                 ωJ
                                       −∞                      uD
                                                       2
                                     E[(J − E(J)) | P (Z) > uD ] Pr(P (Z) > uD ) 13
                                 =                                              .
                                                        ωJ2

The weight hΩJ (uD ) does not necessarily integrate to 1:

                                             1
                                                                Cov(J˜2 , D)
                                        Z
                                                 hΩJ (t)dt =                 .
                                         0
                                                                     ˜ D)]2
                                                               [Cov(J,

Appendix A presents the full derivation. The weight hΩj (uD ) plays a role in determining
the variance of the IV estimator that is analogous to the role of hJ (uD ) in generating the
probability limit of the IV estimator. 2E[αβ | UD = uD ] + E[β 2 | UD = uD ] plays a role in
generating the variance of the IV estimator analogous to the role of the MTE in generating
the probability limit of the IV estimator. We use this representation to facilitate comparison
of the power of the tests under alternative data generating processes and to consider the
problem of the optimal choice of instruments.
       These formulae hold for general functions J(·) of instruments Z that satisfy assumptions
(A-1)–(A-5) given in Section 2. For example, suppose that J(Z) has discrete support on
points j1 , . . . , jK with corresponding values of the propensity score p1 , . . . , pL with L possibly
not equal to K. Let p0 = 0. In this case, for uD ∈ (pl , pl+1 ) both hJ and hΩJ are constant
  13
       fP,J (P (z), j) is the density of P (Z) and J(Z) evaluated at P (Z) = P (z) and J(Z) = j.



                                                           10
so we can write

                  L−1                         Z   pl+1
        Var(J) X                                                                                           1
ΩJ = E α2                                                    2E (αβ | UD = uD ) + E β 2 | UD = uD
                                                                                                   
                +     λΩl                                                                                          duD
            ωJ2   l=0                           pl                                                       pl+1 − pl
              L−1
                                                                           !2
                             Z   pl+1
              X                                             1
          −             λl              E(β | UD = uD )           duD           .
                  l=1        pl                         pl+1 − pl

The weights λΩl and λl are defined in the following way. Let ji be the ith smallest value in
the support of J(Z), then

                                          PK
                                                − E(J)]2 Lt>l fP,J (pt , ji )
                                                        P
                                             i=1 [ji
                                 λΩl =                                        (pl+1 − pl )
                                                     ˜
                                                 Cov(J(Z), D)2
                                       PK               PL
                                        i=1 [ji − E(J)]  t>l fP,J (pt , ji )
                                  λl =                                       (pl+1 − pl ).
                                                     ˜
                                                 Cov(J(Z), D)

For the special case of a binary instrument J(Z) has two points of support, j1 and j2 ,
corresponding to the points p1 and p2 in the propensity score distribution. Let Pr(J(Z) =
j1 ) = Pr(P (Z) = p1 ) = q and Pr(J(Z) = j2 ) = Pr(P (Z) = p2 ) = 1 − q. The λl are λ1 = 1
and λl = 0, l > 1.14 The weights for the variance simplify to

           [j1 − E(J)]2 q + [j2 − E(J)]2 (1 − q)                 [j2 − E(J)]2 (1 − q)
  λΩ0 =                                          (p1 ) and λΩ1 =                      (p2 − p1 ),
                           ˜
                     Cov(J(Z),   D)2                                    ˜
                                                                    Cov(J(Z),  D)2

and

                                        (j1 − E(J))2 qp1 + (j2 − E(J))2 (1 − q)p2   Cov(J˜2 , D)
              λΩ0 + λΩ1 =                                                         =              .
                                                            ˜ D)2
                                                       Cov(J,                            ˜ D)2
                                                                                    Cov(J,

      Formula (4) extends the representation of IV as weighted averages of slopes of the un-
derlying function, due to Yitzhaki (1989). It allows the instrument J(Z) be different from
 14


                  [j2 − E(J)](1 − q)              (j2 − j1 )(p2 − p1 )q(1 − q)       ˜
                                                                                 Cov(J(Z), P (Z))
           λ1 =                      (p2 − p1 ) =                              =                  = 1.
                         ˜
                     Cov(J(Z), D)                              ˜
                                                        Cov(J(Z), D)                   ˜
                                                                                  Cov(J(Z), D)




                                                                   11
the propensity score P (Z) or a monotonic function of it. It reveals that, in general, different
instruments identify different parameters. Thus, in general, βIV,J 6= βIV,J 0 if J and J 0 apply
different weights (5) to a common MTE.
      As noted by Heckman and Vytlacil (2005, 2007b), while the weight in (5) integrates to
1, it is not necessarily non-negative for all values of uD so the interpretation of the weighted
average produced by IV is obscure. Even though the MTE is positive everywhere, the IV
estimate may be negative.15
      Some applied economists report tests based on IV sampling distributions as if they are
testing the null hypothesis that β̄ = 0. Under H0 , i.e., the absence of a correlated random
coefficient model, the sampling distribution of the standard IV estimator, β̂IV,J , can be used
to consistently test the null hypothesis that β̄ = 0. However, when H0 is false, a test of
β̄ = 0 based on the sampling distribution of the IV estimator is, in general, inconsistent and
biased because by (6), IV does not, in general, converge to β̄.
      Consider the following example based on the normal generalized Roy Model,
                                                                       
                           U1      0         σ10 σ1Vσ12                
                                                                       
                           U  ∼ N  0  ,  σ     2                        ,           (9)
                            0        10 σ0 σ0V                         
                                                                       
                            V          0       σ1V σ0V σV2


and assume X = 1. Recalling that uD = FV (v), when V is a normal random variable, the
marginal treatment effect is

                                                                     
                                                          σ1V − σ0V
                           M T E(UD = uD ) = β̄ +                         Φ−1 (uD )        (10)
                                                              σV

where Φ−1 (·) is the inverse of a standard normal CDF (hence Φ−1 (uD ) = v). Alternatively,
in terms of v,
                                                            σ1V − σ0V
                                  M T E(V = v) = β̄ +                 v.
                                                                σV
 15
      See the examples in Heckman, Urzua, and Vytlacil (2006).



                                                    12
            σ1V −σ0V
Let τ =        σV
                       . A value of τ 6= 0 produces a correlated random coefficient model. For such
values plim β̂IV,J 6= β̄. The choice equation is assumed to be D = 1(Z > V ) where both
Z (a single instrument) and V are normally distributed and Z ⊥
                                                             ⊥ V . Additionally, assume
that σ12 = σ02 = σU2 , σ10 = 0.5 × σ1 × σ0 and σV2 = 1.
       Figure 1 plots the power of a Wald test of the hypothesis that β̄ = 0 based on β̂IV,J .
We compute the power function for different values of β̄. Recall from (6) that this is the
component of βIV,J that does not depend on J. In Panel A, β̂IV,J is a consistent estimator
for β̄. In the other two panels it is not. Thus in the top panel of the figures, when τ = 0,
and hence H0 is true, the test of the hypothesis β = 0 is unbiased and consistent and the
size of the test is controlled.16 As expected, smaller values of σU2 produce higher power,
and larger values of σZ2 produce higher power. The bottom two panels plot the power of
the test that β = 0 when τ = −1 and τ = 0.6, respectively. In these two latter cases,
plim βbIV,J = βIV,J 6= β̄. Hence the tests are biased and inconsistent. The power and size of
the test for the existence of an “effect” (i.e., whether β = 0) can be badly distorted. Thus
even if β = 0, an “effect” can be detected, and if β 6= 0, no “effect” can be detected.


3.2       Testing Hypotheses About Instrument-Dependent Parame-

          ters

More recently, many applied economists, following Imbens and Angrist (1994), interpret IV
as a weighted average of “LATEs,” or in our framework, a weighted average of MTEs, as in
equation (3). It is understood that β̂IV,J is not, in general, consistent for the true β̄. Within
this framework, economists often report tests of the hypothesis that βIV,J = 0.
       To calculate the power of such tests, consider alternative values of βIV,J (= β̄ + ΥJ from
equation (6)) obtained by varying β̄ holding ΥJ fixed. Notice that unlike the analysis in
the preceding section, in this section we are not testing the hypothesis that β̄ = 0. Instead
  16
    Although Figure 1 shows the power function only for one sample size, the consistency of the test is
readily verified.



                                                   13
Figure 1: Power function for a Wald test of β̄ = 0 based on the sampling distribution of
β̂IV,J .
                                                                                            A. = 0
                                                        Z2  1,  U2  1                                                         Z2  1,  U2  0.1



                                 1.0




                                                                                                            1.0
                                 0.8




                                                                                                            0.8
                                 0.6




                                                                                                            0.6
                         Power




                                                                                                    Power
                                 0.4




                                                                                                            0.4
                                 0.2




                                                                                                            0.2
                                 0.0




                                                                                                            0.0
                                       -1.5   -1.0   -0.5     0.0         0.5   1.0   1.5                         -1.5   -1.0    -0.5     0.0      0.5   1.0   1.5




                                                      Z2  0.1,  U2  1                                                        Z2  0.1,  U2  0.1
                                 1.0




                                                                                                            1.0
                                 0.8




                                                                                                            0.8
                                 0.6




                                                                                                            0.6
                         Power




                                                                                                    Power
                                 0.4




                                                                                                            0.4
                                 0.2




                                                                                                            0.2
                                 0.0




                                                                                                            0.0
                                       -1.5   -1.0   -0.5     0.0         0.5   1.0   1.5                         -1.5   -1.0    -0.5     0.0      0.5   1.0   1.5




                                                                                            B. = -1
                                                      Z2  1,  U2  1                                                          Z2  1,  U2  0.1
                                 1.0




                                                                                                            1.0
                                 0.8




                                                                                                            0.8
                                 0.6




                                                                                                            0.6
                         Power




                                                                                                    Power
                                 0.4




                                                                                                            0.4
                                 0.2




                                                                                                            0.2
                                 0.0




                                                                                                            0.0




                                       -1.5   -1.0   -0.5     0.0         0.5   1.0   1.5                         -1.5   -1.0    -0.5     0.0      0.5   1.0   1.5




                                                      Z2  0.1,  U2  1                                                        Z2  0.1,  U2  0.1
                                 1.0




                                                                                                            1.0
                                 0.8




                                                                                                            0.8
                                 0.6




                                                                                                            0.6
                         Power




                                                                                                    Power
                                 0.4




                                                                                                            0.4
                                 0.2




                                                                                                            0.2
                                 0.0




                                                                                                            0.0




                                       -1.5   -1.0   -0.5     0.0         0.5   1.0   1.5                         -1.5   -1.0    -0.5     0.0      0.5   1.0   1.5




                                                                                            C.  = 0.6
                                                      Z2  1,  U2  1                                                           Z2  1,  U2  0.1
                                 1.0




                                                                                                            1.0
                                 0.8




                                                                                                            0.8
                                 0.6




                                                                                                            0.6
                         Power




                                                                                                    Power
                                 0.4




                                                                                                            0.4
                                 0.2




                                                                                                            0.2
                                 0.0




                                                                                                            0.0




                                       -1.5   -1.0   -0.5     0.0         0.5   1.0   1.5                         -1.5   -1.0    -0.5     0.0      0.5   1.0   1.5




                                                      Z2  0.1,  U2  1                                                        Z2  0.1,  U2  0.1
                                 1.0




                                                                                                            1.0
                                 0.8




                                                                                                            0.8
                                 0.6




                                                                                                            0.6
                         Power




                                                                                                    Power
                                 0.4




                                                                                                            0.4
                                 0.2




                                                                                                            0.2
                                 0.0




                                                                                                            0.0




                                       -1.5   -1.0   -0.5     0.0         0.5   1.0   1.5                         -1.5   -1.0    -0.5     0.0      0.5   1.0   1.5




Note: Each plot shows the power with a hypothetical sample size of 500. The size of the test is 0.05. The model is the normal generalized Roy
                                                           2
model with the unobservables jointly normal with variance σU  and correlation 0.5. The choice equation is D = 1(Z > V ) where V ∼ N (0, 1) and
           2
Z ∼ N (1, σZ ). The power functions plot the power of the Wald test of βIV,J = 0 for alternative values of β̄. The vertical dashed lines denote the
null hypothesis β̄ = 0. Each panel fixes τ = Cov(β, V )/ Var(V ) at a different level. When τ = 0, plim β̂IV = β̄0 , which in these figures is zero,
and hence the test is consistent. For all nonzero values of τ , the test is inconsistent.




                                                                                              14
we are testing the hypothesis that βIV,J = 0 (or some other specified value). We vary β̄
to calculate the power of the test for alternative values of βIV,J . This is a sensible way to
proceed because β̄ is instrument invariant. Investigating the power of the test in this fashion
allows us to construct power functions for instrument-invariant alternatives.
      Figure 2 plots the power function for the Wald test of the hypothesis βIV,J = 0 as a
function of βIV,J holding ΥJ fixed at -0.5. Consequently, the β̄ compatible with the null
hypothesis, β̄0 , is 0.5. For the model of unobservables used in the previous subsection,
                                                                 σ1V −σ0V
keeping ΥJ fixed entails, among other things, holding τ =           σV
                                                                            fixed along with the
weighting function hJ (uD ). For a given τ and a fixed IV weighting function hJ (uD ), we vary
the parameters of covariance matrix (9). These parameters affect the sampling distribution
of β̂IV,J and hence the power of the test.
      Neither the IV estimand nor the variance of the IV estimator depends on σ10 . Therefore,
the power of the test of the null hypothesis βIV,J = 0 does not depend on σ10 . The only
remaining parameters that can be changed without changing ΥJ are σ02 , σ12 , σ1V and σ0V .
To keep τ fixed, we can only vary σ1V and σ0V subject to a constraint that σ1V − σ0V is
constant.17 For σV = 1, the four A panels of Figure 2 show the power of the test for different
values of β̄ when we vary σ1V and σ0V such that σ1V − σ0V = −1. The power of the test is
highest when σ1V and σ0V are both close to 0 (ie. straddling 0), and lowest when both are
far from zero (either positive or negative). The panels in B vary βIV,J by varying β̄ holding
ΥJ fixed and hold fixed all of the parameters of (9) except for σ12 , while the panels in C vary
β̄ hold fixed all of the parameters of (9) except σ02 . As expected, power decreases as both
variances increase, in general at different rates.
      There are other ways to calculate the power of the test that βIV,J = 0 for alternative
values that are obtained by varying β̄ keeping ΥJ fixed. If the choice equation is


                                              D = 1(Zγ > V )
 17
      Variations in σV2 affect the denominator of the weights.




                                                      15
Figure 2: Power function for the test of the hypothesis that plim β̂IV,J = 0 when β¯0 = 0.5.
Alternatives are different values of βIV,J obtained by fixing ΥJ and varying β̄.
                                                                                         A.
                                                  1V  0.5,  0V  0.5                                                   1V  0.2,  0V  0.8

                             1.0




                                                                                                      1.0
                             0.8




                                                                                                      0.8
                             0.6




                                                                                                      0.6
                     Power




                                                                                              Power
                             0.4




                                                                                                      0.4
                             0.2




                                                                                                      0.2
                             0.0




                                                                                                      0.0
                                   -1.5   -1.0      -0.5       0.0     0.5   1.0   1.5                      -1.5   -1.0      -0.5         0.0      0.5   1.0   1.5

                                                              IV , J                                                                    IV , J
                                                     1V    0,  0V  1                                                    1V      0.5,  0V  1.5
                             1.0




                                                                                                      1.0
                             0.8




                                                                                                      0.8
                             0.6




                                                                                                      0.6
                     Power




                                                                                              Power
                             0.4




                                                                                                      0.4
                             0.2




                                                                                                      0.2
                             0.0




                                                                                                      0.0
                                   -1.5   -1.0      -0.5       0.0     0.5   1.0   1.5                      -1.5   -1.0      -0.5         0.0      0.5   1.0   1.5

                                                             IV , J                                                                     IV , J
                                                                                         B.
                                                    02  1,  12  0.1                                                       02  1,  12  0.5
                             1.0




                                                                                                      1.0
                             0.8




                                                                                                      0.8
                             0.6




                                                                                                      0.6
                     Power




                                                                                              Power
                             0.4




                                                                                                      0.4
                             0.2




                                                                                                      0.2
                             0.0




                                                                                                      0.0




                                   -1.5   -1.0      -0.5       0.0     0.5   1.0   1.5                      -1.5   -1.0      -0.5         0.0      0.5   1.0   1.5

                                                             IV , J                                                                    IV , J
                                                      02  1,  12  1                                                        02  1,  12  2
                             1.0




                                                                                                      1.0
                             0.8




                                                                                                      0.8
                             0.6




                                                                                                      0.6
                     Power




                                                                                              Power
                             0.4




                                                                                                      0.4
                             0.2




                                                                                                      0.2
                             0.0




                                                                                                      0.0




                                   -1.5   -1.0      -0.5       0.0     0.5   1.0   1.5                      -1.5   -1.0      -0.5         0.0      0.5   1.0   1.5

                                                             IV , J                                                                    IV , J
                                                                                         C.
                                                    02  0.1,  12  1                                                       02  0.5,  12  1
                             1.0




                                                                                                      1.0
                             0.8




                                                                                                      0.8
                             0.6




                                                                                                      0.6
                     Power




                                                                                              Power
                             0.4




                                                                                                      0.4
                             0.2




                                                                                                      0.2
                             0.0




                                                                                                      0.0




                                   -1.5   -1.0      -0.5       0.0     0.5   1.0   1.5                      -1.5   -1.0      -0.5         0.0      0.5   1.0   1.5

                                                             IV , J                                                                 IV , J
                                                      02  1,  12  1                                                        02  1,  12  2
                             1.0




                                                                                                      1.0
                             0.8




                                                                                                      0.8
                             0.6




                                                                                                      0.6
                     Power




                                                                                              Power
                             0.4




                                                                                                      0.4
                             0.2




                                                                                                      0.2
                             0.0




                                                                                                      0.0




                                   -1.5   -1.0      -0.5       0.0     0.5   1.0   1.5                      -1.5   -1.0      -0.5         0.0      0.5   1.0   1.5

                                                             IV , J                                                                    IV , J


Note: Each plot shows the power with a hypothetical sample size of 500. The size of the test is 0.05. The instrument is normally distributed,
                                                                                                                   2                 2       2
Z ∼ N (1, 1); D = 1(Z > V ). In panel A, the unobservables are generated with covariances given in the figure and σV  = 1, σ10 = 0, σ1 = 1, σ0 = 1.
                                                                                             2
In panels B and C the unobservables are generated with variances given in the figure and σV    = 1, σ10 = 0, σ1V = −0.5, σ0V = 0.5. In all panels,
under the null hypothesis β̄0 = 0.5, and alternative hypotheses are generated by changing β̄. The vertical dashed line shows the value of
plim β̂IV,J = 0 under the null hypothesis and the vertical dotted line shows the value of β̄ under the null hypothesis.
                                                                                         16
and Z ∼ N (Z, ΣZ ) and V ∼ N (0, σV2 ), all instruments constructed from linear or affine
transformations of Z have the same weight function (5) and hence have the same instrument-
dependent value, βIV,J . For proof of this claim, see Appendix B.18
       This result implies that one can construct power functions for the hypothesis βIV,J = 0
for different values of βIV,J = β̄ + ΥJ for alternative choices of ΣZ , holding γ 0 ΣZ γ, the
variance of the choice index, constant. The derivation in Appendix B shows that the IV
estimand depends only on the distribution of the index Zγ − V . From assumption (A-1),
Zγ and V are statistically independent. σV2 has to be held constant to keep ΥJ fixed. We
keep this term fixed by varying components of Z while keeping γ 0 Zγ fixed. An instrument
with greater variance that obeys this constraint will produce greater power. Figure 3 plots
power functions of the test of the hypothesis that βIV,J = 0 using each component of a
two-dimensional instrument Z = (Z1 , Z2 ). These plots show that for a given IV estimand
βIV,J , the power of the test is higher when using the instrument that accounts for more of
the variance of the index Zγ. Going from top to bottom, the variance of Z1 is increasing
while the variance of Z2 is decreasing. Accordingly, from top to bottom the power of the
test βIV,J = 0 using Z1 as an instrument is increasing while the power of the test using Z2
as an instrument is decreasing. Each panel shows the fraction of γ 0 Zγ accounted for by the
variance of the instrument used to construct the power function (either Z1 or Z2 ).19 We now
use the tools developed for IV in a correlated random coefficient model to test H0 .




4        Testing H0 Using Instrumental Variables

Armed with the results of Section 3, we study how to use different IVs to test H0 . Under
H0 , the probability limits of any two IV estimators are identical, because for any choice of
  18
     This result is special to the case of J(Z) linear or affine in Z with Z normally distributed, so J(Z) is
normally distributed and the further assumption (A-1) that Z ⊥⊥ V , where V is normally distributed. We
have not analyzed more general conditions on Z and V under which the invariance holds.
  19
     Note that in a given row, the fractions do not sum to 1 because there is a covariance (of 0.1) between
Z1 and Z2 .

                                                     17
Figure 3: Power functions for the test of the hypothesis that βIV,J = plim β̂IV,J = 0 for
β̄0 = 0.5. Alternatives are different values of βIV,J obtained by fixing ΥJ and varying β̄.
                                                       A. σ Z2 =0.5, σ Z2 =2.3, σ Z ,Z =0.1
                                                                 1                        2                                  1   2

                                   Using Z 1 as instrument                                                                               Using Z 2 as instrument
                        ((Fraction of γ ' Σ Ζ γ accounted for byy Z 1 = 1/6))                                            (Fraction
                                                                                                                         (         of γ ' Σ Ζ γ accounted for byy Z 2 = 23/30))
           1.0




                                                                                                            1.0
           0.8




                                                                                                            0.8
           0.6




                                                                                                            0.6
  Power




                                                                                                   Power
           0.4




                                                                                                            0.4
           0.2




                                                                                                            0.2
           0.0




                                                                                                            0.0
                 -1.5       -1.0      -0.5       0.0       0.5           1.0    1.5                               -1.5            -1.0     -0.5       0.0      0.5      1.0        1.5

                                              β IV , J                                                                                             β IV , J

                                                         B. σ Z21 =1, σ Z22 =1.8, σ Z1,Z2 =0.1
                                   Using Z 1 as instrument                                                                               Using Z 2 as instrument
                        (Fraction of γ ' Σ Ζ γ accounted for by Z 1 = 1/3)                                                   (Fraction of γ ' Σ Ζ γ accounted for by Z 2 = 3/5)
           1.0




                                                                                                            1.0
           0.8




                                                                                                            0.8
             6
           0.6




                                                                                                              6
                                                                                                            0.6
  Powerr




                                                                                                   Powerr
           0.4




                                                                                                            0.4
           0.2




                                                                                                            0.2
           0.0




                                                                                                            0.0




                 -1.5       -1.0      -0.5       0.0       0.5           1.0    1.5                               -1.5            -1.0     -0.5       0.0      0.5      1.0        1.5

                                              β IV , J                                                                                             β IV , J

                                                         C. σ Z2 =2, σ Z2 =0.8, σ Z ,Z =0.1
                                                                     1                2                           1      2

                                   Using Z 1 as instrument                                                                               Using Z 2 as instrument
                        (Fraction of γ ' Σ Ζ γ accounted for by Z 1 = 2/3)                                                   (Fraction of γ ' Σ Ζ γ accounted for by Z 2 = 4/15)
           1.0




                                                                                                            1.0
           0.8




                                                                                                            0.8
           0.6




                                                                                                            0.6
  Power




                                                                                                   Power
           0.4




                                                                                                            0.4
           0.2




                                                                                                            0.2
           0.0




                                                                                                            0.0




                 -1.5       -1.0      -0.5       0.0       0.5           1.0    1.5                               -1.5            -1.0     -0.5       0.0      0.5      1.0        1.5

                                              β IV , J                                                                                             β IV , J

Note: Each plot shows the power with a hypothetical sample size of 500 varying β̄ keeping ΥJ fixed. The size of the test is 0.05 The instruments
                                      2                  2
are distributed normally, Z1 ∼ N (1, σZ   ), Z2 ∼ N (1, σZ   ) and Cov(Z1 , Z2 ) = σZ1 ,Z2 , Z2 = 0.1; D = 1(Z1 + Z2 > V ) so γ = (1, 1). The
                                        1                  2
                                                                                                                                             2
distribution of the index is held fixed and is distributed N (2, 3). The unobservables are jointly normally distributed with zero means and σV = 1,
             2       2
σ10 = 0.5, σ1  = 1, σ0 = 1, σ1V = −0.3, σ0V = 0.3. In all panels, under the null hypothesis β̄0 = 0.5, and alternative hypotheses are generated by
changing β̄. The vertical dashed line shows the value of βIV,J under the null hypothesis and the vertical dotted line shows the value of β̄ = β̄0
under the null hypothesis being considered, i.e. that βIV,J = β̄ + ΥJ .




                                                                                              18
J,
                                     Z   1                                   Z   1
             plim βbIV,J = βIV,J =           E(β|UD = uD )hJ (uD )duD = β̄           hJ (uD )duD = β̄.
                                     0                                       0

If H0 is false, in general any two IV estimators will differ. Excluding the case of equal IV
weights for the two instruments, our IV test forms two estimators βbIV,1 and βbIV,2 , based on
J1 (Z) and J2 (Z) respectively, and tests the null hypothesis


                                              H0IV : βIV,1 − βIV,2 = 0


against the alternative hypothesis


                                              HAIV : βIV,1 − βIV,2 6= 0.


This test is identical to a standard test for overidentification. However, within the context of
a correlated random coefficient model, we do not interpret rejections of the null hypothesis
as evidence of the violation of the assumptions required for the validity of an instrument.
Rather, rejections are interpreted as evidence of selection on heterogeneous gains to treat-
ment.
     Under the null hypothesis, the Wald test statistic is asymptotically distributed as a
χ21 . Under the alternative, in the general case, the Wald statistic converges to a noncen-
tral chi-square distribution. Let h1 (·) and h2 (·) denote the weights (akin to hJ (·) above)
corresponding to J1 (Z) and J2 (Z), respectively. To simplify the notation, we suppress
the Z argument. Define J˜1 = J1 − J 1 and J˜2 = J2 − J 2 as the demeaned values of
the instruments. Let J̃1 = (J˜11 , . . . , J˜1I )0 and J̃2 = (J˜21 , . . . , J˜2I )0 be the matrices of de-
meaned instruments stacked across individuals. Let D = (D1 , . . . , DI )0 be the stacked
values of the choice variable Di . Under random sampling, and the assumptions of Section 2,
J̃01 D   p          J̃02 D   p
   I
         → ω1 and      → ω2 for some finite constants ω1 and ω2 . Under HAIV : βIV,1 − βIV,2 =
                       I
hR                                     i √
    1
   0
      M T E(u D )(h (u
                   1 D  ) − h (u
                             2 D ))duD / I, the noncentrality parameter of the chi-square




                                                         19
distribution of the test statistic is

                                            Z    1                                          2
                                        1                                                          −1
                       λIV,1,2        =               M T E(uD )(h1 (uD ) − h2 (uD ))duD          Ψ1,2              (11)
                                        2     0



where

                                                                  
                              Var (J1 ) 2 Cov(J1 , J2 ) Var (J2 )
                                  2
               Ψ1,2   =E α              −               +                                                           (12)
                                 ω12          ω1 ω2         ω22
                         Z1
                             2E(αβ | UD = uD ) + E(β 2 | UD = uD ) hΩ,J1 ,J2 (uD )duD
                                                                  
                       +
                          0
                           1                                  2
                           Z
                        −  M T E(uD )(h1 (uD ) − h2 (uD ))duD  .
                              0


Defining J1∗ = J1 − E(J1 ) and J2∗ = J2 − E(J2 ), the weight hΩ,J1 ,J2 (·) is given by

                                  Z1 Z∞                   2
                                              J1∗ J2∗
            hΩJ1 ,J2 (uD ) =                     −              f(J1 −J2 ),P (j1 − j2 , P (z)) d(j1 − j2 ) dP (z)
                                              ω1 ω2
                               uD −∞
                                       "                2                 #
                                            J1∗ J2∗
                          =E                   −              | P (Z) > uD Pr(P (Z) > uD ).20
                                            ω1 ω 2


 The derivation follows a logic similar to that used to derive (7).21 Notice that not only
will the difference in the IV estimands depend on the alternative under consideration, but
the variance of the difference between the IV estimators will also depend on the alternative
under consideration.
       We present this characterization of the variance in order to understand the properties of
tests of H0 based on IV estimators. This expression for the variance is not meant as a guide
for how to implement such tests. In practice the analyst would form the test statistic using
  20
     f(J1 −J2 ),P (j1 − j2 , P (z)) is the joint density of J1 − J2 , and P (Z) evaluated at J1 − J2 = j1 − j2 and
P (Z) = P (z).
  21
     The logic is not, however, identical. Using (J1 − J2 ) as an instrument and testing if βIV,J1 −J2 = 0 is not
equivalent to the test presented in the main text of the paper. The denominators of the IVs differ in the two
approaches.



                                                                  20
a standard estimator of the variance of the vector of IV estimates.
       In general, the weights presented above do not have simple analytical expressions. They
do in the case of a model with normal error terms with normally distributed instruments
and a linear index structure for the choice equation. However, for this case, the proposed IV
test has no power, because, and as previously discussed and as established in Appendix B,
βIV,J1 ≡ βIV,J2 irrespective of the truth or falsity of H0 . For this case, the noncentrality
parameter of the asymptotic chi-square distribution of the test statistic will be zero so the
power of the test equals its size. To have a test with any power, we have to rule out
instruments with equal weights. Since the weights can be constructed from the data on Z,
it is possible to check this condition in any sample.22
       We do not formally analyze conditions that guarantee that the two instruments J1 and
J2 , constructed from Z, optimize the power function of the test. From the expression for
the noncentrality parameter, one can see the ingredients required to construct an asymp-
totically most powerful test. Let Z ∈ Rk be the vector of available instruments and let
     
J = J | J : Rk → R be the space of functions which map the vector of instruments to the
real line. Then for a given MTE, the optimal choice of J1 and J2 solves the problem

                                       Z   1                                          2
                                   1                                                         −1
                       max                      M T E(uD )(hJ1 (uD ) − hJ2 (uD ))duD        Ψ1,2 .
                    J1 ∈J ,J2 ∈J   2    0



The optimal choice of instruments will generally depend on the shape of the MTE(uD ).23
       We present an example with two non-normal instruments in Figure 4. Specifically, let
D = 1(γ1 Z1 + γ2 Z2 > V ) where the vector Z = (Z1 , Z2 ) is distributed as a multivariate
mixture of normals with the distribution given at the base of the figure. The unobservables
are assumed to be generated by a normal generalized Roy model. The test of equality of the
IV estimators constructed using these two instruments has power to detect deviations from
  22
     It would be desirable to develop a formal test for equality of the two IV weights. The required ingredients
are in the literature. We leave the formal derivation for another occasion.
  23
     More generally, one could use multiple instruments and base a test on multiple contrasts of the set of
instruments. We do not develop this test in this paper.



                                                            21
H0 . Figure 4A plots the weights h1 (·) and h2 (·) which the IV estimator places on the MTE,
using Z1 or Z2 respectively. The weights must differ for the test based on the difference in
IV estimators to have power to detect deviations from H0 . When the mixing proportion in
the mixture of normals is 0.45, the instruments are highly non-normal and the IV weights
differ substantially. However, when the mixing proportion is 0.75, the instruments become
closer to normal, the weights become very similar, and the test of H0 loses power. This case
is discussed further in Heckman, Schmierer, and Urzua (2010).
   Another example of a test that has power to detect deviations from H0 , even with normal
instruments, constructs IV estimators using nonlinear functions of Z. We consider a normal
generalized Roy model where there is one Z variable in the choice equation that is normally
distributed, D = 1(Z > V ). We plot the weights of the IV estimators based on Z and Z 2 .
Figure 4B plots the weights for these two choices of instruments. The weights differ, and in
addition the amount by which they differ generally depends on the distribution of Z. We
plot the weights for two choices of the mean of Z presented in the figure. These choices
clearly affect the weights and hence will generally affect the power of a test of H0 based on
these IV estimators.
   Another choice of instruments uses P (Z) on disjoint intervals of the support of P (Z) as
two instruments. Form two disjoint intervals [p1 , p1 ] and [p2 , p2 ], and construct IV estimators
over these intervals as sample analogs to
                                                                          
                                          Cov Y, P (Z) | P (Z) ∈ [p1 , p1 ]
                        βIV,[p1 ,p1 ]   =                               
                                           Var P (Z) | P (Z) ∈ [p1 , p1 ]


and
                                                                          
                                          Cov Y, P (Z) | P (Z) ∈ [p2 , p2 ]
                        βIV,[p2 ,p2 ]   =                               
                                           Var P (Z) | P (Z) ∈ [p2 , p2 ]




                                                   22
                                     Figure 4: IV weights for alternative choices of the instrument.
                                                                                  A. Z 1 vs. Z 2, mixtures of normals
                                                          p mix = 0.45                                                                           p mix = 0.75




                                                                                                                      1.5
                                                               Z2
                    1.5




                                     Z1
                                                                                                                                  Z1




                                                                                                                      1.0
           Weight




                                                                                                             Weight
                    1.0




                                                                                                                                                        Z2




                                                                                                                      0.5
                    0.5
                    0.0




                                                                                                                      0.0
                          0.0             0.2            0.4          0.6           0.8           1.0                       0.0         0.2      0.4             0.6            0.8         1.0
                                                               uuDD                                                                                     uuDD


                                                                                                 B. Z vs. Z 2
                                                           E (Z ) = 1                                                                            E (Z ) = -0.5




                                                                                                                      2.5
                    2.5




                                                                            Z2                                                                         Z2




                                                                                                                        0
                                                                                                                      2.0
                    2.0




                                                                                                                                                                                Z
           Weight




                                                                                                             Weight
                                                                                                                      1.5
                    1.5




                                                Z
                                                                                                                      1.0
                    1.0




                                                                                                                      0.5
                    0.5




                                                                                                                      0.0
                    0.0




                          0.0             0.2            0.4          0.6           0.8           1.0                       0.0         0.2      0.4             0.6            0.8         1.0
                                                               uuDD                                                                                     uuDD

                                                                             C. P (Z ) above and below the median
                                                           E (Z ) = 0                                                                                E (Z ) = 1
                    3.0
                    2.5




                                                                                                                      8
                    2.0




                                                                                                                                                                        P(Z)
           Weight




                                                                                                             Weight
                                                                                                                      6




                                                                                                                                  P(Z)                                  above
                    1.5




                                          P(Z)                                   P(Z)                                             below
                                                                                                                      4
                    1.0




                                          below                                  above
                                                                                                                      2
                    0.5
                    0.0
                      0




                                                                                                                      0




                          0.0             0.2            0.4          0.6           0.8           1.0                       0.0         0.2      0.4             0.6            0.8         1.0
                                                               uuDD                                                                                     uD

                                                                                   D. P (Z ) separated by quartiles
                                                           E (Z ) = 0                                                                                E (Z ) = 1
                                                                                                                      30
                    6




                                                                                                                                                                                P4
                                                                                                                      25
                    5




                                                                                                                      20
                    4
           Weight




                                                                                                             Weight




                                                                                                                                                                           P3
                                                                                                                      15
                    3




                                P1                  P2                                    P4                                                                     P2
                                                                      P3                                                           P1
                                                                                                                      10
                    2
                    1




                                                                                                                      5
                    0




                                                                                                                      0




                          0.0             0.2            0.4          0.6           0.8           1.0                       0.0         0.2      0.4             0.6            0.8         1.0
                                                               uuDD                                                                                     uuDD

         Note: Panel A plots the weights of IV estimates constructed using either Z 1 or Z 2 as an instrument where Z 1 (Z 1,1 Z 2) is distributed as a
Note: Panel A plots the weights of IV estimates constructed using either Z1 or Z2 as an instrument where (Z1 , Z2 ) is distributed as a multivariate
mixturemultivariate
        of normals,mixture
                    with D of=normals,
                               1(γ1 Z1with
                                       + γD2 Z= >γV1Z).1 +
                                              2 1(       Toγ construct
                                                             2Z 2 > V ). these
                                                                          To construct
                                                                               results,these results, we assume
                                                                                        we assume

                                                     ⎛ Z ⎞             ⎛ -0.8
                                                                             ⎛ 1.4 0.5 ⎞ ⎞                      ⎛ -0.8 ⎛ 0.6 -0.3 ⎞ ⎞ 
                                                     ⎜ 1 ⎟ ∼ −0.8
                                                             p mix × N ⎜⎜      , 1.4 ⎟0.5    + (1 − p mix ) × N ⎜⎜                  ⎟ ⎟⎟
                                                                                                 
                                                                                                                       ,⎜
                                                                                                                                                                                      
                                                                      , ⎝ 1 ⎜⎝ 0.5 1.4 ⎟⎠ ⎟⎠
                                Z1                                                                                                            −0.8               0.6      −0.3
                                                     ⎜Z ⎟
                                            ∼ pmix × N                                                    + (1 −        ⎝ -0.3) ×
                                Z2                   ⎝ 2⎠       1                0.5        1.4                  ⎝ 1pmix        0.6 N
                                                                                                                                    ⎠⎠         1
                                                                                                                                                   ,
                                                                                                                                                                 −0.3      0.6


and the and  the coefficients
         coefficients  in thein  the choice
                                choice      equation
                                       equation       γ1 γ=1=0.2,
                                                  are are     0.2, γγ2 2=1.
                                                                        = 1. In
                                                                              Inthe
                                                                                 theleft plot
                                                                                      left    of Panel
                                                                                           plot         A weAlet
                                                                                                 of Panel      wep mix
                                                                                                                   let = 0.45 =
                                                                                                                       pmix    and0.45
                                                                                                                                     in the
                                                                                                                                         andright plot right
                                                                                                                                               in the   p mix =plot
                                                                                                                                                                 0.75.pmix = 0.75.
Panel B plots the weights of IV estimates constructed using either Z or2 Z 2 as an instrument where Z ∼ N (µZ , 1), µZ = 1 or µZ = −0.5,
        Panel B plots the weights of IV estimates constructed using either Z or Z as an instrument where Z ~ N(μ Z ,1),
and D = 1(Z > V ). Panel C plots the weights of IV estimates constructed using either P (Z) below the
                                                                                                                                        μ Z =or
                                                                                                                                    median
                                                                                                                                                1 or μ Z =above
                                                                                                                                                   P (Z)
                                                                                                                                                             -0.5, and
                                                                                                                                                                    the median as
        D = 1(Z
instruments.      ≥ VD). plots
               Panel       Panelthe
                                  C plots the weights
                                      weights          of IV estimates
                                               of IV estimates           constructed
                                                                   constructed    usingusing  either
                                                                                          P (Z)   in P (Z ) below
                                                                                                     different     the median
                                                                                                                quartiles       or distribution
                                                                                                                            of its  P (Z ) above the as median   as
                                                                                                                                                        instruments.     In Panels
                                                                                                                 2
C and D,instruments.
           Z ∼ N (µZPanel
                        , 1), µDZplots
                                  = 0 the
                                       or µweights
                                            Z = 1,of  IV D
                                                    and   estimates
                                                             = 1(Z constructed
                                                                       > V ). In allusing P (Zplots,
                                                                                       of the  ) in different
                                                                                                       we setquartiles
                                                                                                               σV   = 1.of its distribution as instruments. In
                                                                                                         23                                      2
and test


                                  H0IV   : βIV,[p1 ,p1 ] = βIV,[p2 ,p2 ]

                                  HAIV   : βIV,[p1 ,p1 ] 6= βIV,[p2 ,p2 ] .


There is no a priori guidance on which intervals to use so we consider two ways to construct
intervals over which to form IV estimates: (1) use the intervals [0, pmed ) and [pmed, 1] where
pmed is the sample median of P (Z), and (2) use the intervals [0, pq1 ), [pq1 , pq2 ), [pq2 , pq3 )
and [pq3 , 1], where pqj is the jth sample quartile of the distribution of P (Z) and form all
pairwise contrasts between these estimates. Note that even though we split the propensity
score into four intervals, we are still conducting pairwise tests. However, because there is a
multiplicity of pairwise tests, we must control the size of the test. We do this by using the
stepdown procedure of Romano and Wolf (2005). Figures 4C and 4D plot the weights for the
instruments constructed in this manner. These weights are nonoverlapping by construction
and will also depend on the distribution of the instrument Z.
   The power of the test of H0 based on IV estimators also depends on the variance (12),
which determines the denominator of the noncentrality parameter. The important terms
which are affected by the choice of instruments are the variance of the difference in the
            h                                    i
              Var(J1 )  2 Cov(J1 ,J2 )  Var(J2 )
instruments     ω2
                       − ω1 ω2         + ω2        and the variance weight hΩ,J1 ,J2 (·). The variance
                  1                       2

of the difference in the instruments is identified from the distribution of Z given X. The
weights hΩ,J1 ,J2 (·), can also be estimated from the data but are less transparent. For each of
the examples presented in Figure 4, we plot the variance weights hΩ,J1 ,J2 (·). In the case of
the normal generalized Roy model, the weights are more intuitive and more easily calculated
when conditioning directly on V = v (rather than UD = uD ), so we plot them as a function
of v. Figure 5 plots the variance weights. Ceteris paribus, the larger the variance weights,
the larger is the variance of the difference in the IV estimators and hence the lower the
power of a test based on this difference. In Panel A of Figure 5 we see that when the



                                                   24
mixing proportion is 0.45 the variance of the difference in the estimators is higher than when
the mixing proportion is 0.75 due to the fact that the IV weights covary highly when the
instruments are closer to normal so the variance of their difference is smaller. In Panel B, the
variance weights are roughly similar for E(Z) = 1 and E(Z) = −0.5. Finally, in Panel C the
variance weights are much larger when E(Z) = 1 than when E(Z) = 0. This demonstrates
that even when the IV weights are nonoverlapping, as is the case in both examples in Panel
C, the variance of the difference in the IV estimators will generally depend on the distribution
of Z.
    We emphasize that the specific comparisons of IV estimators presented in this section are
illustrative examples. Our formal analysis is completely general and allows for any choice of
valid instruments which satisfy (A-1)–(A-5).



5       Testing H0 by Testing for Linearity

We next consider tests of H0 based on linearity in p. Keeping the conditioning on X implicit,
we can write (3) as
                                 E(Y | P (Z) = p) = µ + g(p)                               (13)

for some general nonlinear function g(·) where µ and g may depend on X. Our test for the
absence of selection on the gain to treatment is a test of whether the function g(·) belongs
to the linear parametric family F = {a + bp, (a, b) ∈ R2 }. Let P be the support of P (Z),
with typical element p ∈ P. The null hypothesis of linearity can be written as


           H0L : There exists some (a, b) ∈ R2 such that g(p) = a + bp for almost all p ∈ P,


while the alternative is


           HAL : There exists no (a, b) ∈ R2 such that g(p) = a + bp for almost all p ∈ P.




                                               25
Figure 5: IV variance weights (hΩJ1 ,J2 (·)) as a function of V = v for alternative choices of
instruments.
                                                                      A. Z 1 vs. Z 2, mixtures of normals
                                            p mix = 0.45                                                                                     p mix = 0.75
            120
            100




                                                                                                              15
            80
   Weight




                                                                                                     Weight
                                                                                                              10
            60
            40




                                                                                                              5
            20
            0




                                                                                                              0
                  -4            -2                0                   2                  4                          -4             -2              0               2     4
                                                  vv                                                                                               vv

                                                                                                               2
                                                                                        B. Z vs. Z
                                             E (Z ) = 1                                                                                      E (Z ) = -0.5
            6




                                                                                                              6
            5
   Weight




                                                                                                     Weight
            4




                                                                                                              4
            3
            2




                                                                                                              2
            1
            0




                                                                                                              0




                  -4            -2                0                   2                  4                          -4             -2              0               2     4
                                                  vv                                                                                               vv

                                                                  C. P (Z ) above and below the median
                                             E (Z
                                               ( )=0                                                                                          E ((Z ) = 1
            15




                                                                                                                0
                                                                                                              150
            10




                                                                                                              100
   Weight




                                                                                                     Weight
            5




                                                                                                              50
            0




                                                                                                              0




                  -4            -2                0                   2                  4                          -4             -2              0               2     4
                                                  v                                                                                                vv


 Note:Panel
Note:    PanelAAplots
                 plotsthe
                       thevariance
                           varianceweights
                                    weightsofofthe
                                                thedifference
                                                    differenceininthe
                                                                   theIV
                                                                       IVestimates
                                                                          estimates constructed
                                                                                    constructed using
                                                                                                using either
                                                                                                      either Z
                                                                                                             Z1 or
                                                                                                               1
                                                                                                                 or Z
                                                                                                                    Z2 as
                                                                                                                      2
                                                                                                                        as an
                                                                                                                            aninstrument
                                                                                                                               instrumentwhere
                                                                                                                                          where(Z
                                                                                                                                                (Z1 , Z2 )
                                                                                                                                                  1,
is distributed as a multivariate mixture of normals, with D = 1(γ1 Z1 + γ2 Z2 > V ). To construct these results, we assume
 Z 2) is distributed as a multivariate mixture of normals, with D = 1(γ 1Z 1 + γ 2Z 2 > V ). To construct these results, we assume
                                                                                                                                                            
                           Z1                             −0.8                   1.4  0.5                                           −0.8           0.6      −0.3
                                     ∼ pmix⎛⎜ ×
                                              Z1 ⎞N           ⎛ ,                ⎞ ⎞⎟            ⎛+               ⎞ ⎞⎟ ) × N
                                                            1 ⎜ -0.8 , ⎛⎜ 1.4 0.50.5                    ⎛ 0.6
                                                 ⎟                                                 -0.8(1 − p-0.3                        ,
                                                  ∼ p mix × N                                    ⎜            mix                                  −0.3
                                                              ⎜ 1 0.5 1.4 ⎟ ⎟ + (1 − p mix ) × N ⎜ 1 , ⎜ -0.3 0.6 ⎟ ⎟
                           Z2                                                         1.4                                            1                       0.6
                                             ⎜Z ⎟                       ⎝        ⎠⎠                     ⎝         ⎠⎠
                                             ⎝ 2⎠             ⎝                                  ⎝

 and the coefficients in the choice equation are γ 1=0.2, γ 2=1. In the left plot of Panel A we let p mix = 0.45 and in the right plot p mix = 0.75.
and the coefficients in the choice equation are γ1 = 0.2, γ2 = 1. In the left plot of Panel A we let pmix = 0.45 and in the right plot pmix = 0.75.
Panel B plots the variance weights of the difference in the IV estimates constructed using either Z or Z 2 as an instrument where Z ∼ N (µZ , 1),
                                                                                                               2
µPanel
 Z = 1Borplots
           µZ = the−0.5,
                    variance
                          and weights
                              D = 1(Zof>the difference
                                         V ). Panel C in  thethe
                                                       plots  IV estimates constructed
                                                                 variance weights        using
                                                                                    of the      either in
                                                                                           difference  Z the
                                                                                                          or ZIV asestimates
                                                                                                                     an instrument  where using
                                                                                                                              constructed Z ~ N(  μ Z ,1),
                                                                                                                                                either P (Z)
below the median or P (Z) above the median as instruments. In Panel C, Z ∼ N (µZ , 1), µZ = 0 or µZ = 1, and D = 1(Z > V ). In all of the
 μ Z =we
plots,    setμσZV
       1 or     2 = -0.5, and D = 1(Z > V ). Panel C plots the variance weights of the difference in the IV estimates constructed using either
                   = 1.
 P (Z ) below the median or P (Z ) above the median as instruments. In Panel C, Z ~ N(μ Z ,1), μ Z = 0 or μ Z = 1, and D = 1(Z ≥ V ). In all
 of the plots, we set σ V 2 = 1.



                                                                                                26
There is a large and still unsettled literature in econometrics and statistics dealing with
specification tests of this type.24 These tests proceed in one of two ways: (i) testing or-
thogonality restrictions implied by the parametric model, or (ii) comparing a nonparametric
estimate of g(p) with a parametric estimate, â + b̂p. We discuss both types of tests. Heck-
man, Schmierer, and Urzua (2010) present Monte Carlo analyses of the power of these tests.
We briefly discuss a third test due to Li and Nie (2007).


Linearity Test 1: Wald Test Based on Series

The first test of linearity of E(Y |P (Z) = p) in p determines whether terms in addition to
p are required to fit the data. It is instructive to consider the case of the normal selection
model as a baseline. When the data are generated from the normal generalized Roy model,
we can characterize E(Y |P (Z) = p) by

                                                               Z     p
                          E(Y |P (Z) = p) = ᾱ + β̄p + τ                 Φ−1 (uD )duD .
                                                                 0



Heckman, Schmierer, and Urzua (2010) examine this case in depth.
       In the general non-normal case, polynomials can approximate classes of smooth alterna-
tives for the function g(·). One can estimate E(Y |P (Z) = p) using polynomials of degree 2
or higher. Polynomials approximate well a broad class of functions. Exploring power in this
class gives us an indication of the power of our procedures against such alternatives.25 One
specification of a more general, but still parametric, alternative model is

                                                   L
                                                   X
                                      g(P (Z)) =          φl (P (Z))l ,
                                                    l=0

  24
     See, e.g., Horowitz and Spokoiny (2001) and the references therein. The properties of particular tests
depend on the specification of alternatives.
  25
     Ichimura and Todd (2007) discuss the properties of series estimators. Newey (1997) establishes conver-
gence rates and proves asymptotic normality of such estimators.




                                                    27
where L is assumed to be known.26 The proposed test for linearity is


                           H0 : φl = 0 for l = 2, . . . , L

                           HA : φl 6= 0 for some (or all) l = 2, . . . , L.


Heckman, Schmierer, and Urzua (2010) develop properties of this test for linearity.


Linearity Test 2: Bierens Conditional Moment Test

One can also test the validity of representation (3) using orthogonality restrictions implied
by the parametric model. One approach is the conditional moment (CM) test of Bierens
(1990).27 This test uses the fact that under the null hypothesis the following moment con-
dition must be satisfied
                                 E[Y − a0 − b0 P (Z) | P (Z)] = 0

for the true parameter vector (a0 , b0 ) ∈ R2 . This conditional moment restriction implies the
set of unconditional moment restrictions


                           E[(Y − a0 − b0 P (Z)) exp(t0 Λ(P (Z)))] = 0                           (14)


for all t ∈ R, for some bounded one-to-one, mapping Λ from R into R. A test can be
constructed using the sample analog of the left-hand side of (14). Bierens (1990) shows how
one can use sample analogs to construct a test statistic which, under the null hypothesis,
converges in distribution to a χ21 and under the alternative diverges to infinity. Heckman,
Schmierer, and Urzua (2010) discuss the properties of this test.




  26
    Below, we discuss a procedure when L is unknown.
  27
    See also Bierens (1982) and Bierens and Ploberger (1997) for related tests. Newey (1985) discusses
conditional moment tests more generally.


                                                 28
The Preceding Tests are Conditional Moment Tests28

All of the tests discussed so far test if


                                        E(Y | P (Z)) = a + bP (Z)


which is equivalent to
                                    E [h(P (Z)) [Y − a − bP (Z)]] = 0

for any function h. Conditional moment tests would typically use a vector of functions
h(P (z)) to construct tests.
       The tests previously discussed use different choices of h(P ). For the Bierens test, one
would use h(P (Z)) = exp(t0 Λ(P (Z))). The test of linearity based on polynomials takes
                                               
h(P (Z)) = 1, P (Z), (P (Z))2 , . . . , (P (Z))L . The IV test can also be cast in this framework,
as the following argument shows.
       The plim of the IV estimator obtained using Jk (Z), k = 1, . . . , K, as an instrument are
the values of (ak , bk ) that solve


                                      E [Jk (Z) [Y − ak − bk D]] = 0


and
                                E [Y − ak − bk D] = 0,        k = 1, . . . , K.

By the law of iterated expectations, this is equivalent to solving


                              E [Jk (Z) [E(Y | Z) − ak − bk P (Z)]] = 0

                                      E [E(Y | Z) − ak − bk P (Z)] = 0,
  28
       We thank Edward Vytlacil for suggesting this unifying approach.




                                                     29
which is equivalent to solving


                                E [Jk (Z) [Y − ak − bk P (Z)]] = 0

                                       E [Y − ak − bk P (Z)] = 0.


For one instrument there is no test, but for two or more (K ≥ 2), one can test if a common
pair of (a, b) satisfies all of the moment conditions produced from using different instrumental
variables. This is the classical test of overidentification. Thus, all of the tests previously
discussed can be viewed as conditional moment tests.


Linearity Test 3: A Semiparametric Test Based on Local Linear Regression29

A potential problem with the test based on series estimators (Linearity Test 1) is that it
assumes that the degree of the highest order polynomial in P (Z) is finite and known. A
semiparametric approach that did not rely on strong functional form assumptions about the
generator model is more desirable.
       Li and Nie (2007) use local linear regression methods to develop a test for linearity of an
unknown parametric function in a semiparametric model. They develop a test of linearity of
the unknown nonparametric component (linearity in P (Z) in our setup) that can be applied
to the problem analyzed in this paper if it is adapted to the case of an estimated P (Z).
                                               √
If P (Z) is parametric and its coefficients are N estimable, their analysis can be applied
directly. The case where P (Z) is estimated nonparametrically is left for another occasion.
       Li and Nie (2007) conduct a Monte Carlo study of their approach. They show good
size and power properties for their test statistic. Their test can be interpreted as a local
conditional moment test.
  29
   We thank Xiaohong Chen for directing us to this paper and clarifying our thinking about semiparametric
approaches to testing for linearity.




                                                   30
Conditioning on X

Throughout, we have conditioned on X. An important practical problem not addressed in
this paper but common to all empirical models is picking the appropriate conditioning set,
and determining how to explicitly model the dependence of Y on X. Heckman, Schmierer,
and Urzua (2010) discuss the power of these tests and conduct extensive Monte Carlo studies.



6     Summary and Conclusion

P.A.V.B Swamy’s classic work (1971, 1974) developed estimators for the uncorrelated ran-
dom coefficient model. This is the case when H0 is true. In this paper, building on the work
of Heckman and Vytlacil (1999, 2005, 2007a,b) and Heckman, Schmierer, and Urzua (2010),
we develop tests for the presence of a correlated random coefficient model and related tests
on parameters derived from the model. All of the tests we consider can be interpreted as
conditional moment tests. We develop instrumental variable tests for the null hypothesis of
the absence of a correlated random coefficient model. To implement it, we develop the sam-
pling distribution of the IV estimator using the marginal treatment effect and its extensions
to higher moments of the distribution of the heterogeneity on which agents select.
    Heckman, Schmierer, and Urzua (2010) conduct a Monte Carlo investigation of the power
of these tests. One disturbing finding from their work is that the power of all of the tests
we consider is low. They show that among all of the tests considered, the test based on
comparing alternative IV estimators above and below the median propensity score has the
highest power.
    This paper analyzes the case of a binary treatment. Heckman, Urzua, and Vytlacil (2006)
and Heckman and Vytlacil (2007b) analyze the cases of a multiple treatment model generated
by an ordered choice model with stochastic thresholds and a multiple treatment model
generated by an unordered choice model. In all of these cases, IV produces an instrument-
dependent parameter so the IV test for selection on unobserved gains based on comparing


                                             31
the estimands of two different IVs developed in this paper carries over in general to these
settings. A test of linearity of the conditional expectation of Y given P is developed for the
outcome model for multiple treatments generated by the ordered choice model in Heckman,
Urzua, and Vytlacil (2006). It also applies to the unordered multiple choice model that
identifies the treatment effect of a gain compared to the next best option which Heckman,
Urzua and Vytlacil show is a direct extension of the binary model.




                                             32
A     The Variance of Linear IV in the Correlated Ran-

      dom Coefficient Model

The IV estimator, using instrument J(Z), is

                                                        P ˜
                                                          Yi J i
                                              βbIV,J   =P
                                                          Di J˜i

and hence
                             √                 √1
                                                     P      ¯ i + βi Di )
                                                      (Ji − J)(α
                                                 I
                                 I βbIV,J =                               .
                                                         1
                                                             Di J˜i
                                                           P
                                                         I

Invoking standard central limit theorems,

                                 √               
                                                    d
                                  I βbIV,J − βIV,J → N (0, ΩJ ).


Defining J ∗ = J − E(J), where ΩJ is given by

                    (                2 )
                      Y J∗       Y J∗
                               
            ΩJ = E          −E
                        ωJ        ωJ
                     2 ∗ 2             2
                     Y (J )           Y J∗
               =E            − E
                        ωJ2            ωJ
                                1                          2
                     2 ∗ 2      Z
                     Y (J )
               =E            −  E(β | UD = uD )hJ (uD )duD 
                        ωJ2
                                  0
                                            1                        2
                                            Z
                  1
               = 2 E (α + βD)2 (J ∗ )2 −  E(β | UD = uD )hJ (uD )duD 
                                     
                 ωJ
                                                         0
                 1               2                  1 
               = 2 E α2 (J ∗ )2 + 2 E αβD(J ∗ )2 + 2 E β 2 D(J ∗ )2
                                                                  
                ωJ               ωJ                  ωJ
                     1                             2
                     Z
                  −     E(β | UD = uD )hJ (uD )duD  .
                         0




                                                        33
Using the law of iterated expectations as well as the assumption that α is independent of Z,
this expression can be written as

        Var(J)     1                                 1 
ΩJ = E α 2        + 2 2E αβ(J ∗ )2 | D = 1 Pr(D = 1) + 2 E β 2 (J ∗ )2 | D = 1 Pr(D = 1)
                                                                            
              2
             ωJ     ωJ                                ωJ
         1                             2
           Z
      −     E(β | UD = uD )hJ (uD )duD 
              0


where
                                    E[J ∗ | P (Z) > uD ] Pr(P (Z) > uD )
                       hJ (uD ) =                                        .
                                                     ωJ

Under the conditions of Fubini’s Theorem, we can exchange the order of integration and
write

         Var(J ∗ )
ΩJ = E α 2
               ωJ2
       Z1
                                  2
                                               E((J ∗ )2 | P (Z) > uD ) Pr(P (Z) > uD )
     +     2E (αβ | UD = uD ) + E β | UD = uD                                             duD
                                                                    ωJ2
       0
        1                           2
         Z
     −  E(β | UD = uD )hJ (uD )duD 
          0
                    Z1
      2  Var(J)                               2
                                                              
   =E α           +     2E (αβ | UD = uD ) + E β   | UD = u D    hΩJ (uD )duD
             ωJ2
                    0
        1                            2
          Z
     −  E(β | UD = uD )hJ (uD )duD 
              0


where

                                    Z∞                    Z1
                              1                       2
                  hΩJ (uD ) = 2          (j − E(J))            fP,J (P (z), j)dP (z)dj
                             ωJ
                                  −∞                      uD
                                               2
                              E[(J − E(J)) | P (Z) > uD )] Pr(P (Z) > uD )
                          =
                                                 ωJ2



                                                   34
which is the expression in the text.



B        Proof of Invariance of the IV Estimand to the Choice

         of a Linear Instrument under Normality with a Lin-

         ear Index Choice Equation

Suppose that the choice equation has a linear index structure, so that


                                         D = 1(Zγ > V )


where Z ∼ N (Z̄, ΣZ ), an L-dimensional multivariate normal random variable, γ an L × 1
vector and V ∼ N (0, σV2 ). Consider the instrument J(Z), which is a linear function of Z,
say Z 0 η. In this case, the IV estimand (written as a weighted average over the support of
V ) is

                                     Z   ∞                                   
                                                                          v
                           βIV,J =           M T E(v)hJ (v)φ                      dv
                                       −∞                                σV

where φ(·) is a standard normal pdf and the IV weight is

                                E[J(Z) − E(J(Z))|Zγ > v] Pr(Zγ > v)
                     hJ (v) =                                       .
                                           Cov(J(Z), D)

Under the assumption of multivariate normality for the instruments,
                                                                             
                                         Cov(J(Z),Zγ)
                                             √          φ       √v−Z̄γ
                                              Var(Zγ)               Var(Zγ)
                          hJ (v) =                                               .
                                     Cov(J(Z),Zγ−V )                  −Z̄γ
                                         √              φ       √
                                         Var(Zγ−V )                 Var(Zγ−V )




                                                  35
Under assumption (A-1) in Section 2, Cov(J(Z), Zγ) = Cov(J(Z), Zγ − V ), and we obtain

                                                                       
                                          √ 1
                                                  φ       √v−Z̄γ
                                          Var(Zγ)             Var(Zγ)
                           hJ (v) =                                        .
                                      √   1               √     −Z̄γ
                                                 φ
                                      Var(Zγ−V )              Var(Zγ−V )



That is, the IV weights, and hence the IV estimand, are the same for all J(Z) = Z 0 η for any
finite η.




                                              36
Acknowledgments

This research was supported by NIH R01-HD043411, NSF SES-024158, the American Bar
Foundation and the Geary Institute, University College Dublin, Ireland. The views expressed
in this paper are those of the authors and not necessarily those of the funders listed here. We
have received helpful comments from Pedro Carneiro, Jeremy Fox, Joel Horowitz, Benjamin
Moll, Azeem Shaikh, Christopher Taber, Edward Vytlacil, and Sergio Urzua.




                                              37
References

Abbring, J. H. and J. J. Heckman (2007). Econometric evaluation of social programs, part
  III: Distributional treatment effects, dynamic treatment effects, dynamic discrete choice,
  and general equilibrium policy evaluation. In J. Heckman and E. Leamer (Eds.), Handbook
  of Econometrics, Volume 6B, pp. 5145–5303. Amsterdam: Elsevier.

Ai, C. and X. Chen (2003). Efficient estimation of models with conditional moment restric-
  tions containing unknown functions. Econometrica 71 (6), 1795–1843.

Bierens, H. J. (1982). Consistent model specification tests. Journal of Econometrics 20 (1),
  105–134.

Bierens, H. J. (1990, November). A consistent conditional moment test of functional form.
  Econometrica 58 (6), 1443–1458.

Bierens, H. J. and W. Ploberger (1997, September). Asymptotic theory of integrated condi-
  tional moment tests. Econometrica 65 (5), 1129–1151.

Björklund, A. and R. Moffitt (1987, February). The estimation of wage gains and welfare
  gains in self-selection. Review of Economics and Statistics 69 (1), 42–49.

Griliches, Z. (1977, January). Estimating the returns to schooling: Some econometric prob-
  lems. Econometrica 45 (1), 1–22.

Heckman, J. J. (2001, August). Micro data, heterogeneity, and the evaluation of public
  policy: Nobel lecture. Journal of Political Economy 109 (4), 673–748.

Heckman, J. J. and R. Robb (1985). Alternative methods for evaluating the impact of
  interventions. In J. Heckman and B. Singer (Eds.), Longitudinal Analysis of Labor Market
  Data, Volume 10, pp. 156–245. New York: Cambridge University Press. Reprinted in 2008,
  Econometric Society Monograph 10.



                                             38
Heckman, J. J., D. Schmierer, and S. Urzua (2010). Testing the correlated random coefficient
  model. Journal of Econometrics 158 (2), 177–203.

Heckman, J. J., S. Urzua, and E. J. Vytlacil (2006). Understanding instrumental variables in
  models with essential heterogeneity. Review of Economics and Statistics 88 (3), 389–432.

Heckman, J. J. and E. J. Vytlacil (1998, Fall). Instrumental variables methods for the
  correlated random coefficient model: Estimating the average rate of return to schooling
  when the return is correlated with schooling. Journal of Human Resources 33 (4), 974–987.

Heckman, J. J. and E. J. Vytlacil (1999, April). Local instrumental variables and latent
  variable models for identifying and bounding treatment effects. Proceedings of the National
  Academy of Sciences 96 (8), 4730–4734.

Heckman, J. J. and E. J. Vytlacil (2001). Local instrumental variables. In C. Hsiao,
  K. Morimune, and J. L. Powell (Eds.), Nonlinear Statistical Modeling: Proceedings of
  the Thirteenth International Symposium in Economic Theory and Econometrics: Essays
  in Honor of Takeshi Amemiya, pp. 1–46. New York: Cambridge University Press.

Heckman, J. J. and E. J. Vytlacil (2005, May). Structural equations, treatment effects and
  econometric policy evaluation. Econometrica 73 (3), 669–738.

Heckman, J. J. and E. J. Vytlacil (2007a). Econometric evaluation of social programs, part I:
  Causal models, structural models and econometric policy evaluation. In J. Heckman and
  E. Leamer (Eds.), Handbook of Econometrics, Volume 6B, pp. 4779–4874. Amsterdam:
  Elsevier.

Heckman, J. J. and E. J. Vytlacil (2007b). Econometric evaluation of social programs, part
  II: Using the marginal treatment effect to organize alternative economic estimators to
  evaluate social programs and to forecast their effects in new environments. In J. Heckman
  and E. Leamer (Eds.), Handbook of Econometrics, Volume 6B, pp. 4875–5144. Amsterdam:
  Elsevier.

                                             39
Horowitz, J. L. and V. G. Spokoiny (2001, May). An adaptive, rate-optimal test of a para-
  metric mean-regression model against a nonparametric alternative. Econometrica 69 (3),
  599–631.

Ichimura, H. and P. E. Todd (2007). Implementing nonparametric and semiparametric
  estimators. In J. Heckman and E. Leamer (Eds.), Handbook of Econometrics, Volume 6B,
  pp. 5369–5468. Amsterdam: Elsevier.

Imbens, G. W. and J. D. Angrist (1994, March). Identification and estimation of local
  average treatment effects. Econometrica 62 (2), 467–475.

Li, R. and L. Nie (2007, November). Efficient statistical inference procedures for partially
  nonlinear models and their applications. Biometrics 64 (3), 904–911.

McFadden, D. (1974). Conditional logit analysis of qualitative choice behavior. In P. Zarem-
  bka (Ed.), Frontiers in Econometrics, pp. 105–142. New York: Academic Press.

McFadden, D. (1981). Econometric models of probabilistic choice. In C. Manski and D. Mc-
  Fadden (Eds.), Structural Analysis of Discrete Data with Econometric Applications, pp.
  198–272. Cambridge, MA: MIT Press.

Mincer, J. (1974). Schooling, Experience and Earnings. New York: Columbia University
  Press for National Bureau of Economic Research.

Newey, W. K. (1985, September). Maximum likelihood specification testing and conditional
  moment tests. Econometrica 53 (5), 1047–1070.

Newey, W. K. (1997, July). Convergence rates and asymptotic normality for series estimators.
  Journal of Econometrics 79 (1), 147–168.

Quandt, R. E. (1958, December). The estimation of the parameters of a linear regres-
  sion system obeying two separate regimes. Journal of the American Statistical Associa-
  tion 53 (284), 873–880.

                                             40
Romano, J. P. and M. Wolf (2005, March). Exact and approximate stepdown methods for
  multiple hypothesis testing. Journal of the American Statistical Association 100 (469),
  94–108.

Swamy, P. (1971). Statistical Inference in Random Coefficient Regression Models. New York:
  Springer-Verlag.

Swamy, P. (1974). Linear models with random coefficients. In P. Zarembka (Ed.), Frontiers
  in Econometrics, pp. 143–168. New York: Academic Press.

Thurstone, L. L. (1927, July). A law of comparative judgement. Psychological Review 34 (4),
  273–286.

Vytlacil, E. J. (2002, January). Independence, monotonicity, and latent index models: An
  equivalence result. Econometrica 70 (1), 331–341.

Yitzhaki, S. (1989). On using linear regression in welfare economics. Working Paper 217,
  Department of Economics, Hebrew University.




                                            41
