                          NBER WORICfl PAPER SERIES




                         ROBUST LINE ESTATION WTI'H
                           ERRORS IN BOTH VARIABLES


                               Michael L. Brown

                            Working Paper No. 83




     COMPUTER RESEAjCJ-j CENTER FOR ECONOMICS AND MANAGEMENT SCIENCE
                National Bureau of Econcmic Research, Inc.
                         575 Technology Square
                     Cambridge, Massachuset-ts 02139


                                  May 1975


                        Preliminary: not for quotation
    NBER working papers are distributed infonially and in limited numbers for
    coninents only. They should not be quoted without written permission.
    This report has not undergone the review accorded official NBER publications;
    in particular, it has not yet been sU]iTlitted for approval by the Board of
    Directors.
*   NBERCanputer Research Center. Research supported in part by National Science
    Fourxlation Grant GJ-U54X3 to the National Bureau of Econaiiic Research, Inc.
                               Abstract


The estimator holding the cen-tral place in the theory of the multivariate
"errors-in--the--variables" (EV) model results frcm performing orthogonal
re'ess ion on variables rescaled according to the covariance matrix of
the errors [7]. Our first principal finding, via Monte Carlo on the
univariate model, essentially relegates this estimator to use only in large
samples on very well-behaved data, i.e., with no trace of outlier contamina-
tion. A modification, requiring a robust preliminary slope, is proposed
that essentially sets out the generalization to EV of the w-estimator
in regression. It is d-nonstrated that the modification is robust to outlier
contamination even in small samples, given a sufficiently good preliminary
estimator. A candidate for a preliminary slope estimator based on the data
is proposed arid its performance under simulation examined. Least-absolute
residuals estimation in EV is cited as an alternative candidate.
                                       Contents


1.   Motivation from   the Robust Regression                  1

2. The Simplest EV Model                                 .2
     2.1 Classical assumptions for the univariate nde1   .2
     2.2   Contaminated errordistributions               .3
     2.3   Identifiability ob1em in classical EV         .3
3.   Form of ML Estinator for Classical EV

4. 3"IL as the "Best" ML/LS Estintor                          6

5. Monte Carlo Results for      L
     5.1   Specification
     5.2 CaiTnents   on   Table 1

6. "Robustizing" 3'1L : w—Estimation
     6.1 Introduction to w-Estimation in Regression           9a
     6.2 Proposed    w-Estinlator   in EV                    10
     6.3 ASimplification                                     12
     6.4 Details of the Intplanentation                  .   12
     6.5 CcmnentsonTabJ.e2
     6.6 Influence of sample size n

7. Data-based   Preliminary Estimators
     7.1 Apreliminaryslope
     7.2 CamnentsonTable3                                    15
     7.3 LAR estimation on EV                                15

BIBLIO                                                       17
1. Motivation from    the   Robust Regression

     Huber  [3] addressed the question of generelizing robust etima.tes 5f
a   location parameter to the problen of estimating robustly the coefficients
{ •J }"   in a multivaria-te regression model


                                 jl jij +       i                               (1)

with   contaminated errors v. He asserts     that "[in] the classical   least

                    the matrix
                                      (X.) is thought to derive from a fixed
squares theory...                X=

and rigorous mathematical model. In statistics, it is more customary to
treat the coefficients        as independent variables, possibly also subject
                                    —2—




to errors.    Next to nothing is known about how to robustize regression
procedures   with respect to errors in the        (underlines ours). The
underlined statement is the problem we consider.
2. The Simplest EV Model
       2.1 Classical assumptions for the univariate model

       We set p=l in (1) and, to quantify the phrase "errors in the

we introduce an error u into X, so that we no longer observe X directly

but rather x.,1 where

                           x.1 X.+u.,
                                 1  1                                       (2)



To fill out the EV model specification, we assume


                           y.
                            1
                              Y.+v.
                               1   :i                                       (3)


with   the "true" values exactly linearly related:

                           Y. =
                            1     x.1   .                                   ('iV)


(For   the moment we assume a constant term a equal to 0 for simplicity.)

Further,
                           E(v.) = 0                                        (5a)



                           E(u1) = 0        .                               (5b)


                           cov(u1,v1)
                                                0   .                       (6)

We suppose



                           X1 -   N(0,a)
(a "structural relation") and


                                                                                    .
                                        —3—




                                                0                                              (7a)
                               cov(X1,v1)


                                                0                                              (7b)
                               cov(X,u)

(Note: Our discussion applies equally well to the "functional relation",

where the X.'s arise in same         detenministic       fashion rather than   as   realizations

of   a randan variable X.)
       Our given data consist of the n observations



with   successive observations taken
                                    1     il         ,



                                           independently.
                                                                                               (8)



       Notice that xEX(uEO) is the case of regression with a line through                the
origin.

     22 Contaminated error distributions
       We define a contaminated EV model with u and v samples from the
contaminated noriial distribution. This means that


                              u     is drawn   fran N(0 ,a) with probability (l_y) (9a)
                                        and fran N( 0 ,h) with      probability



where h >>         and   similarly for v.

     2.3   Identifiability problem in classical EV

       Lindley   [5] and   others   have pointed out that, in the classical case
of gaussian errors,
                           yU        .00        h    0                                          (lOa)
                                                U

                                =    .00              0                                         (lOb)




the ML estniators of the parameters ,
                                                a,   c cannot all be consistent.

Kendall and   Stuart []   observe that       "we must make an assumption about            the
error   variances.... [Assuming]
                                           known


   is the classical means of resolving the unidentifiability problem."
Throughout,   we assume

                              X                                                                 (11)


is known.


3. Form of ML Esthnator for         Classical   EV

     It   is convenfLent to derive the ML estimator of ,             BML   say, for the
functional relation; its form remains unchanged for the structural relation.
The likelihood function is

                                                      -
                                                                    (x—X±)2 } , which            (12)

                      ex{_ 2a                             2(
allows   us to characterize the ML estimator of as



                      -                                        +   (x.-X.)2           ,         (13)
                      ;x1...,Xn1ilL v'X     [(Yç-X)                  1 1      }   }
                                          —5—




where the symbol to the left of the braced quantity means "that value
of for which the braced function of ( )<             is a minimum, for
- <   <-<X<                  , i1,.
                                 ,n." We see that transforming
                                      .



y   to y/v' and to /v'X makes the effective A equal to 1, so we assume

                               A=l                                                           (lu)

from now on without loss of generality.

        Setting         { }    0 where "       }" is   the braced quantity from the
                   xi
min condition yields
                               A    x.+(BML)y.
                                     1       1
                                1                                                            (15)
                                      l+(BNL)2

        Setting          }    0 fran the min condition implies that
                                          nA
                                          E   X.y.
                                           flA                                               (16)
                                           E X.
                                          il 1


where      of (16) is itself a function of BIlL. Thus BIlL is of precisely the

same form as BMLR, the ML estiirator in regression, except that in place

of the known       in regression are estimates X. (Note: Because the various

approaches to the identifiability problem in EV are distinguished essentially

by how   they   obtain the X, it is in our           view   unfortunate   that the   in EV

bear the   name incidental parameters.)
      "Unwrapping" the iiilicit characterization (16) yields
                                           —6—




                                                             12
                                               2                                         (17)
                                                   S12

where


                   inE 2                           inE                           in y.
                                                                                    2
          s11
                     il               s12
                                                       i=1
                                                             xy1         s22 E      E
                                                                                   i=1




                                      22
                                           -   S11



BML is formed by mininiizirig the sum of squared-"residuals" taken perpendicular
to the estimated line. I . e.,        BML is the orthogonal regression estintor

on rescaled variables; see e.g., Malinvaud [7]

L.•   ___________
       BIVIL   as the "Best" ML/LS Estimator
                                                                   chaps. 1, 10.
                                                                                                .
        The classical ML estima-tors that apply throughout the range of nde1

parameters are those which assume ]<riowledge of

                                                   '   say);
or

                                           '           say);

or

                                a2
                           x4   u
                                            BIlL



or    both or a and       CJç    ,'   say).         See Madansky      [6].




                                                                                                .
                                    —7--




       Kendall   and Stuart sunirnarize a result of Birch that justifies calling
BNL the "best" of , ' BML. Birch demonstrated that


                                    u,v
except under condit±ons (the violation of certain inequalities relating
sample product-moments to model parameters) which Kendall arid Stuart state
"seem unlikely [to hold] in practice". While we regard this last claim as
perhaps   a bit optimistic (because the violation occurred    about 5-10%
of   the time in our   sinuilations), this fact nevertheless allows us to understand
why BML is at least as good an estimate      of ,   uniformly over the range of model
parameters, as the better of the other two ML estimates which each require
knowledge of exactly one of        or      Our s:iinulations confirmed this
property when both u and v are normally distributed.
       Madansky gives a remarkable survey of the history of this estimator,
asserting that the form of BML "has appeared iridependen-tly innumerable times
with the earliest appearance in 1879.. .".
       It is also of interest to recall Malinvaud's laudatory remarks about BML.
He derives an approximation to L' s asymptotic variance which, he says,

"allows us to verify that, in the case [of one X-variate      --- M.L.B.]   and

probably generally, the weighted re'ession has good asymptotic       efficiency

at least when the dispersion of the errors is small relative to      that   of   the

true   variables, and when the errors are normally distributed". Comparing
this variance with the minimum variance lower bound, he concludes that
                                                         —8—


 "the   asymptotic efficiency is very               near 1 jf [11C                      01'S]      small".
 Later: "[these]   results.., apply only to the asymptotic distribution of the

 [estimator BML]. Unfortunately there seems to exist                          no   study of the properties
of this [estimator] for finite samples."

5. Monte Carlo Results for BML

    5,]. Speciflcatjon


        We consider first the performance of the ML/LS estimator BML under

contamination;    for comparison we include

                                               n

                              BMLR =
                                            i-i
                                             r
                                               x1y
                                                                                                       (18)
                                                     2
                                               E    x.
                                                     1
                                            1=1
BMLR is the    usual ML/LS estimator that                     would   be appropriate if   x were   err-
free (i.e. u      0, x       X).
        We choose model parameters which, except for the contamination in y
only, are smetric in x and y:
                         n = 20;l00
                         8    = 1.
                                   =   1.
                             2 =       0.502
                         U                               u      .00
                                                                          u =.0
                                                                        11.



                                       0.50                     .05 h varying
                                                         cS
Note that this implies A = 1.
     We are thus   considering both                the small-sample and the large-sample performance
of BML.    When n 20, e.g., an average of (.05) (20)                              1 out of 20 observations is

drawn from N (0 ,h) and the rest from N (0, .502). Notice that h 0.50 corresponds

to classical EV.

                                                                                                                .
                                                      —9—


                                     Table 1.         y, O.05;81.
                                           100 replications


                               n                       MSE(BNLR)        MSE(BML)

                              2O         0.50               .01.99             .0391
                              20         1.00               .0501              .0I1.7L1.
                              20         1.25               .0506             .0559
                              20         1.50               .0513             .06914
                              20         20                 .0533             1261
                              20         3.0                .0602           2.1.1.50
                              20      10.0                  .2120        21.1.6..


                           100           1.50               .0395           .0119
                           100           3.0                .01401          .105
                           100        10.0                  .0580         26.9



       5.2 Corrvnents on Table 1

        It   is in our view   to overstate the gravity
                              difficult                                             - and   ne irony!   -
of the results of Table 1, which tables the quantity
                              100
              MSE(.)
                              iJ..   (.        l.)2
At    n2O we see that when just one observation i,s drawn from N(0,1.252) instead

of from N( 0, . 502),    BML is a poorer estimate than BMLR, which i-res the error

in x! Contamination this light would almost always be indistinguishable f±anpure

gaussian     sampling,   By   the tine h becomes             "very noticeably" large, BML's distribution

has   acquired outrageously heavy          tails, while BMLR has kept               relatively stable.
(See the hlO entries.)             The  transition at n100 occurs between h1 .5 and 3., which
is still more than small           enough to go unnoticed in a real sampling situation.
BML' s performance is thus so shockingly poor as to make                    the      MIlLS estiiator
appropriate for the regression model, BNLR --                   whose   non-robustness properties
are   now notorious -- look almost well-behaved by comparison! For this reason,
      by

we come to the ironic conclusion that those investigators who have "looked the
other way" when the possibility arose of error in the independent variable in
                                          - 9a   -




their "regression" irdels, arid used BMLR "by default", instead of BML which
requires knowledge of X, probably made the better choice. But this is a
choice   between Scylla and   Charybdis; the ne'ct section proposes a way out
of this strait.


6. "Robustizing" BML w-Estation

   6.1     Introduction to w-Estirnation in Regression (R)
                                                                          V
     The w-estarnator in R, BWR, with preLuxanary slope ,                      robust   scale-

mease s of residuals, and       "psi-function"       ip,       is   defined as



                           BWR =   mm1
                                           i=l yi
                                                           .
                                                                      1   i
                                                                . (y.—X.)2]       ,                 (19)




where

                                   y1 —   X.                                                        (20)




is the residual fran the preliinary       slope      ',        and



                                    iP(1/Y)
                              yi      r .15
                                       yi y
                                                                                                    (21)




is a weight superposed onto the i' residual serving to "damp the influence of"
the th point on BWR -to the ectent that it is diagnosed as an outlier. Notice
that when i, (r) r, BWRBMLR. See     Beaton    and Thkey [ 2              ]   and Andrews et.    al. [ii.




                                                                                                            S
                                                             — 10   -

   6. 2   Prposed w-Estiinator                  in EV

        We   propose the following natural generalization BW                        of BWR   to   EV.

BW involves two weights .,                    . each             intended to "weight-down an

outlying      coordinate"    of the th point,                    superposed onto   the ML/LS   esthiiator
             EV
in EN, $          :   for p, ,   and      s with the meanings as in R, we set


                       BW        min                 •
                                                    il
                                                                                                            (22)



                                          V
                                     +   wXl.        (x.—X.Y]
                                                       11

where

                                 I
                                 V

                                                1         1                                                 (23a)



                                     Xl         1        1                                                  (23b)


are the      two EV residuals associated with v arid                     u respectively and
                                                    1,       V


                                  yl          rIyi 7                                                        (24




                                 V
                                          -
                                              )X(rXl./sX
                                              rXl./sX                                                       (2tb)
                                               — 11   -

are the two weights. Notice that we now require a preliminary estimate
X. for each X. as well aa
 1          1                       $ for 8.
     Th.irther   on we   discuss   the some data-based possibilities for choosing
I,      V
$andX1.
    Taking the (n+1) derivatives of the weighted min condition (22)
yields, after simplifying, this 6th degree equation for BW:




                  i=l  1 yi[2
                   E Ck.w   xi
                              x      ii   + (w
                                               Xl.wyi.y.
                                                      1
                                                          2
                                                              - v2  2
                                                                w .x.).BW
                                                                 Xl 1


                          —   i.
                              Xl yi.x.y.(BW)2)}
                                     1 1
                                                      0                             (25)




where
                                                                                           .
                               A
                               k                                                    (26)
                                     Ew
                                      V
                                        +




                                                                                           .
                                                 — 12 —




    6.3           A Simplification

                    Choosing X1 to have the ML/LS fonn (15) as a function of

8   we   may verify         easily   that

                                     r      —$r .                                            (27)

         I,                                               V

                            yi}, for this
Thus {r . }         and   {r .                  choice of X, yield the same measures of
              x   -,
scale              s, whence (assuming               we have



                                                                                             (28)
                                      Xl    yl

Substituting this coninon weight ( say) into the min condition (22)

shows that          BW   has the   form (17)     of BML except   that

                                     t12
                                            1

                                            n i::1
                                                   E w.x.y.                                  (29)




replaces s12 and corresponding weighted moments t11, t22 replace                      S22.
    6.            Details of the Implementation

     We have used



                                            (l_(.)2)2 if() c
                                                                    ff() >c                  (30)


                                 with c     5.0.
                                         — 13 —




  This   p-function,   due to   Tukey,   is known as     the "bisquare". Our robust

  scale is due    to Harnpel:




                               median    {(r. - median {r.})}                         (31)
               s({r1})                                   J
                                                     j



         In order to separate the issues of preliminary and final robust estimators,

  we set

                                                                                      (32)

  for the present w-simulations. Thus BW in our simulations takes "one

step away from the true s", and these results indicate the character of

performances to be expected with a good data-based preliminary estiimtot'
V
. See   later remarks.

     We also exhibit the performances of the regression w-estimator BWR,

which is (19) with x in place of X.


                          Table 2. Same 100 samples as for Table 1

              n                             NSE(BWR)              MSE(BW)

             20         0.50                .02714                 .0211
             20         1.0                 .0277                  .02140
             20         1.25                .0277                  .0257
             20         1.50                .0277                  .0263
             20         2.0                 .0273                  .0250
             20         3.0                 .0271                  .02'#8
             20        10.0                 .0317                  .0225

            100         1.50                .0211                  .003142
            100         2.0                 .0213                  .00363
            100         3.0                 .0215                  .00361
            100        10.0                 .0223,                 .00322
                                                  — 14       —




      6.5   Coimierits   on Table 2
        Table 2 exhibits the MSE 's for 8W arid for BWR corresponding to the
situations of Table 1. Besides the artifactitious "superefficiency"
induced by    assuming        8   (so that MSE ( BWR) <MSE ( BML) when li is         near 0.50),
we see    that MSE (BW) remains         below    MSE(BWR)        in all cases.   In other words,

superposing the w-weighting allows the                error-in-x       correction using     A to

operate     on an effectively uncontaminated sample.

      6.6   Influence    of sample      size n

        The ratio

                          MSE (BW)
                          MSE(BWR)
decreases as n increases for fixed y, h. This accords with our expectation,
for as n increases both variances decrease like 1/n but bias-squared approaches
a non-zero limit in the case of BWR and zero in the case of 8W because of BW' s
bias-correction     using A.
7. Data-based Preliminary Estimators
    7.1 A preliminary slope
                                               V
     One straightforward method of obtaining a 8 for                     EV might   be to   combine

good preliminary regression            estimators,               and     in the mariner that
BNL combines BMLF.. and

                                                  2
                                             E
                                            ii          ..
                                                                                                   (34)
                                             n
                                                 x.y.
                                            i1
                                             L




We   have simulated
                                  BL      t +(sgn(BYX)) •              +A                          (35a)

where

                                          [BRXY   —
                                                                 ]                                 (35b)
                                                          — 15   —




with BYX the "robust line" ("medians-of-thirds grouping" estimator) of
Thkey C      8    ],    and     BRXY the reciprocal of the same estimator corresponding
to regression of x on y.

                                                      Table 3.

                 r                        (Y,h)           MSE(BYX) MSE(BRXY)      MSE(BL)

                 20       (.0,c)           (.G,0)           .061499    .3777       .1359
                 50       (.0,0)           (.0,0)           .05143     .1327       .0260

                 20       (.0,0)           (.05,3)          .06514     .5822       .1921
                 50       (.0,0)           (.05,10)         .0599      .6985       .1637
                 20       (.05,10)         (.0,0)           .1037      .1421414    .16014
                 50       (.05,10)         (.0,0)           .1026      .1123       .01417

      7.2 Coinents on Table 3

       Either a small sample or contamination in y renders' BRXY suffiOiently
unstable as to make MSE (BL) >MSE (BYX).                   But for moderately large n arid
contamination in x only, the estimator EL with A-correction improves on
BYX, the "regression" slope. Notice that contamination in x alleviates
some of     BRXY's       overestimating       bias at


      7,3    LAR estimation in EV

       One   of       the best-}c-iown proposals for a preliminary           estimator inregress±on
is   the LAR (least-absolute--residuals) slope

                                               -
                 $min-1 i:l
                        .Z y.—BX.
                             1  1

                                               fv
The EV generalization requires B, X1,..., Xn jointly to minimize
                                                                 V.




                 il                       I + I x.1       X.}
                                                                                                      .
                  Z       y.1   —   )<•               —
                                     1                     1
                                     - 16   —




Thesolution (C. MUows, 1973 personal carrnunication) is 8 to minimize the
smaller of
                                     -
          iii I c8 x     ,      I         I



In classical EV, we may remark that this essentially c'nputes the LAR estimator
of y on x or of x on y according as which of x or y respectively has the smaller
"noise-to-signal ratio". We conj ecture that this estimator would be very
satisfactory in, arid only in, "large" samples in contaminated EV.
                                        - 17    -

                                    BIBLIOGRAPHY



[1]    Andrews, et. al., Robust Estimates of Location: Survey and Advances,
         Princeton University Press (Princeton, New Jersey), 1972.

[2] Beaton, A.E. arid J.W. Tu3cey, "The Fitting of Power Series, Meaning
       Polynomials, Illustrated on Bandspectroscopic Data," Technometrics,
         Vol. 16, No. 2, May 1971+, pp. 1l+7_192.

[3] Huber,    P.,   "The 1972 Wald Lecture. Robust Statistics: A Review,"
        Ann. Math. Stat., Vol. '43, No. 14,    1972,   pp. 10'-l.l—1067.

['4]   Kendall, M. G. and Stuart, A., The Advanced Theory of Statistics, Vol. 2,
         3rd ed., Griffin (London), 1973.


[5] Lindley, D. V., "Regression Lines and the Linear Functional Relationship,"
        Jour. Royal Stat. Soc., Supp., 9, 19147, 219-21+14.

[6] Madarisky, A., "The Fitting of Straight Lines When Both Variables are
       Subject    to
                  Error," Jour. Amer. Statist. Assn., 51+, March 1959,
       pp. 173—205.

[7] Malinvaud, E., Statistical Methods of Econometrics, 2nd ed., American
      Elsevier Pub. Co. (New York), 1970.

[8] Tukey, J., Exploratory Data Analysis, 1iited preLiminary edn., Vol. I,
      Addison-Wesley Pub. Co. (Reading, Mass.) 1972.
