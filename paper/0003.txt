                    NBER WORKING PAPER SERIES




              ERROR COMPONENTS REGRESSION MODELS

                     AND   THEIR APPLICATION S


                        Swarnjit S. Arora



                 Working     Paper No.3                1



COMPUTER RESEARCH CENTER FOR ECONOMICS AND MANAGEMENT SCIENCE
          National Bureau of Economic Research, Inc.
                    575 Technology Square
                Cambridge, Massachusetts 02139



                             June 1973



              Preliminary: not for quotation
     NBER working   papers are distributed informally and in
limited numbers for comments only. They  should not be quoted
without written permission.
      This report has not undergone the review accorded offi-
cial NBER publications; in particular, it has not yet been
submitted for approval by the Board of Directors.


      NBER Computer Research Center and University of
Wisconsin, Milwnukee. Research supported in part by National
ScIence Foundation Grant GJ-l15X2 to the National Bureau of
Eeonom c Rcs carob, Trio.
                         Abstract


     In this paper, we have developed an operational method
for estimating error components regression models when the
variance-covariance matrix of the disturbance terms is un-
known. Monte Carlo studies were conducted to compare the
relative efficiency of the pooled estimator obtained by this
procedure to (a) an ordinary least squares estimator based
on data aggregated over time, (b) the covariance estimator,
(c) the ordinary least squares estimator, and (d) a gener-
alized least squares estimator based on a known variance-
covariance matrix. For T small, and large p, this estimator
definitely performs better than the other estimators which
are also based on an estimated value of the variance—covariance
matrix of the disturbances. For p small and large T it com-
pares equally well with the other estimators.




                      Acknowledgments


     A major portion of this research was done during the
authorts stay, as a Research Fellow, at the NBER Computer
Research Center for Economics and Management Science. Special
thanks are due to Professors David Beisley, Murray Brown, Paul
Holland, and Edwin Kuh for their valuable comments and sugges-
tions. Thanks are also due to Mr. David Jones for programming
assistance. The simulation for this paper was done at the
NBER Computer Research Center using the TROLL system.
Section 1. Introductjon




Section 3.
                          .Contents




Section 2. Estimation of Error Components
            Regression Models .


             Properties of the Estimators


Section 4. Design of the Experiment arid
            the Comparative Properties of
            the Various Estimators
                                                1



                                                4


                                               ii




Section 5. Conclusion                          18



Appendix A-i. Tables of Mean and Mean Square
               Error of the Coefficients for
               Various Estimating Procedures   19


Appendix A-2. Random Number Generating
               Procedures                      21


References                                     23
               ERROR COMPONENTS REGRESSION MODELS

                     AND THEIR APPLICATIONS




                           Section 1

                          Introduction


        In several recent studies, attempts have been made to

analyze the problems involved in pooling cross section and

time series data by error components (or variance components)

regression models. These !nodels can be formulated as


                                   +
(1.1)                +
                         E6kZk.        uit
                                   (i1,2,...,n; tl,2,...,T),

where        is an observation on the dependent variable for

individual     in period t. Zkjt is an observation on the kth

independent variable, cD    is   an intercept term, 3k (k1,2,.. ,K-l)
are the fixed but unknown slope coefficients, and u± is an error
term. This disturbance term is supposed to represent the net

effect of numerous individually unimportant, but collectively
significant, variables whidh have been omitted from the analysis.

Some of these are specific to the individual and remain invariant

over time (say p1); some are specific to the time period but are
                                  —2—


invariant over all individuals (say Xi); and some are specific

to both individual and time (say            In this case we can

write       as

                     +        +
(1.2)                    X.

Mundlak (10) and Hoch (5) analyzed this model, treating p and
                                       n            T
   as unknown parameters and assu4ning E i.  0 and E A = 0.
 L                                         i=l     t:l L
Maddala (9) points out a principal weakness in this approach:
it eliminates a major portion of the variation among both the

explained and the explanatory variables when the between indi-
viduals and between time periods variation is large.      This ap-

proach can also cause a substantial loss in degrees of freedom.
An alternative approach is to treat all components as random.

This case was analyzed by Wallace and Hussain (1L), Maddala (9),

Nerlove (12), and Swamy and the present author (13).1
        Under the assumptions of weakly non—stochastic X's and

normally distributed disturbance terms, both approaches yield

asymptotically equivalent estimates with asymptotically equi-
valent variance—covariance matrices. In fact, it can be shown

 that there are an infinite number of estimators which have the

 same asymptotic variance-coVarianCe matrices.2


 1.     Whether or not the individual effects may be treated as
        parameters or random components for the purpose of statis-
        tical analysis depends upon the underlying data generating
        mechanism assumed. For an illuminating discussion of such
        data generating Tnechanisms, see Nerlove (11) p. 3614.

 2.     See Swamy and Arora (13) p. 267.
                             —.3—



    Asymptotic properties, however, are cold comfort to the
econometrician for whom the choice of a practical estimator

(and its related small sample properties) is a problem of
crucial importance. Unfortunately, because of mathematical
intractibility, small sample properties are often hard to
obtain theoretically. We therefore employ Monte Carlo experi-
ments to evaluate relative efficiency of the various estimators.

     The plan of this paper is as follows: In section 2, a

means of estimating error components regression models is
developed for a case when the variance-covariance matrix of
the disturbance term is unknown. We also show the equivalence
of this estimator with an ordinary least squares estimator

when inter-individual and inter-temporal variations are zero.

In section 3, the asymptotic properties of this estimator are
derived. Section 4 describes the design of the Monte Carlo

experiments and compares the relative efficiency of this esti-

mator with the ordinary least squares estimator, a covariance

 estimator, an ordinary least squares estimator based on data
 aggregated over time and ,a generalized least squares estimator

 based on a known variance—covariance matrix of the disturbance
 terms. Concluding remarks are presented in section 5. An
 efficient way of generating random numbers and independently
 distributed normal variates is described in an appendix to

 this paper.


                                                                   S
                                         —it—




                                      Section 2

       Estimation of Error Components Regression Models


       Let us assume that u1                    +   and the components

       and           are random such that

                      0



                                                     ifij
             F411




             Ep.p.
               i      10
                        ' ía2
                          4




             Ev1tO
             Ev. V.
                              ía2                    ifijandt=t
                                                            .     3
                itjt          I
                                  0                  otherwise.


Let us further assume that p1 and                   are independent of each

other. Furthermore T > K and n > K and the variances a2 and


3.     In (13) Swamy and I have analytically shown that the
       estimator based onthe assumption of both pj and A-
       being random is more efficient than either Ehe co-
       variance estimator or the ordinary least squares esti-
       mator only if (a) n and T are sufficiently larger than
       10, and (b) if the sum of squares due to variation over
       time exceeds the sum of squares due to remaining varia-
       tion. If these conditions are not satisfied, random
       error components model with both components random may
       give results inferior to other estimators. For a case
       where either n or T is less than 10, we conjecture that
       the error components models with a random component (the
       other component being a parameter or zero) perform better
       than the model which assumes both Pj and At as random.
       Here we consider a model with AtA for all t, but we can
       easily treat all At's as different.

Li..   A model in this form was also used by Kuh (7), except that
       he did not assume Pj and Vit are uncorrelated. Hussain (6)
                                       —5—

  2
                                                                                      .
       are unknown. For all the nT observations combined we can

write (1.1)


(2.1)         y X + u,
where y       (y11,       '            .                       is an nTxl

vector of observations on the dependent variable, X =                    'nT'   Z]

is an nTxK matrix of explanatory variables, tnT is a vector of
l's of order nTxl, and Z is an nTK matrix of independent

variables given by


                      z1 Z211 ... ZK_l,ll

              Z =
                      Z1       Z;lT             ZK_i,1T


                      Zini Z21 ...

                                           *
                      Z1flT    Z2flT            ZK_l,flT

      is a (K'xl) vector of slope coefficients,                  (, 5')',
K'      K —   1, and u        (u11, ...,       U1,   ... u1,   ...,   uflT)   is an

nTxl vector of disturbance terms. Under the above assumptions,

it is readily verified that


(2.2)          Euu'           2(10               'T
Since the variance—covariance matrix of u is not scalar, ap-



 ()      treats a model with nj's a parameters, and At and jt
         as random. His estimator is identical with the covariance
         estimator and does not utilize full data information.
                                             —6—


plication of the ordinary least squares procedure might lead
to an inefficient estima.tor of 3.

        Let us consider an orthogonal matrix, °T' of order T

such that its first row is equal to i/1T. Let                        =


           C] where C1 is a (T-1)xT matrix such that CuT = 0,
[tT//T,
                                         —
ll 'T-l' and C1C1                            1•TtT/T.
        Define the transformations Q1                   (ItT/vW) and Q2    (IC1).
By applying the transformation Q1 to all nT observations, we get


(2.3)          y1 X + u1,
where y1 Q1y is an nXl vector of transformed dependent var-
iables, and u1 is an nxl vector of transformed observations.

The variance-covariance matrix of u1 is


(2.4)          Eu1u1       Q1EuuQ1

Substituting for Euu' from (2.2) and simplifying we get

                                 2
(2.5)          Eu1u1 = a1

           2           2             2
wherea1 =Ta                +cy

Thus the variance-covariance matrix reduces to scalar form,
a best linear unbiased estimator of                      is an OLS estimator


(2.6)          (1) (X1X1YX1y1.

The subvector of (1) corresponding to the slope coefficient

only is given by

               6(1) =
(2.7)
                           (ZNZ1)ZNy1              ,
                                 -7-


where Z1   Q1Z is an nxK matrix, and N         I -   it/n;   a

subscript 1 is attached to       and to differentiate these
estimators from the other estimators of        and tS to be described

later.

The va'jano-covariance matrix of (l) is

           V[(l)] =
(2.8)                 a12(ZNZ1).
Applying the transformation Q2         (IC1)   to all nT observations

we have


(2.9)      y2   Z26 + u2


where y2    Q2y is an nTxl vector of tr.nsformed observations
on the dependent variable, Z2 is an nT#xK matrix of transformed

observations on K independent variables, u2 = Q2u is an nTxl
vector of transformed disturbances, and use is also made of the

result Q2inT = 0.     The variance-covariance matrix of u2 is


(2.10)     Eu2u; EQ2uuQ2 Q2EuuQ2 ;
                 = (IflCl)[Q2(IfltTtT)     +
                                               vIT     IC1)

which can easily be reduced to              . Thus the variance-
                                       VnT
covariace matrix of U2     is   of scalar form. A best linear un-

biased estimator of 6 is the OLS estjm.tor given by




                                                                        S
                                    —8--



(2.11)
            -'

            (2)              —1          5
                      (Z2Z2) Z2y2

The variance-covariance matrix of (2) is a2(ZZ2)1. Notice

that Q1Q2        0. The rank of Q1 is equal to the rank of

/v'T multiplied by the rank of I because if A and B are any
arbitrary matrices, the rank of (AB) is equal to the rank of
A multiplied by the rank of B. Therefore the rank of Q1 +
                                                          Q2
  n + nT - n = nT, which is thetotal number of observations.

This indicates that in estimating (2.3) and (2.9) we have used
up all the orthogonal linear combinations of the available
observations.       S(l) and S(2) are two uncorrelated estimators

of the same parametric vector and we can pool them in the
following manner.


            A         rzNz
                        1        ZZ
                                  2 11ZNy
                                      1   Zy
                                           2 2
(2.12)      6(0)          21 +                      21 +
                      L           aJ
                                    2j       I


                                             La 1          a


where 0 =   [a2     a2] The estimator S(0) is a generalized
least squares estimator of 6. For given values of a12 and a 2,
it is a best linear unbiased estimator. Any other estimator of
6 which is also linear in the vector y and is unbiased, has a

variance-covariance matrix which exceeds that of S(0) by a
positive semidefinite matrix.


5.   It can be easily recognized that the estimator 8(1) in (2.6)
     is an OLS estimator obtained by applying OLS to data agre-
     gated over time and multiplied by lIvW; the estimator 6(2)
     in (2.11) i obtained by applying OLS to nT observations,
     each observation expressed as a deviation from its time
     series mean and the overall mean. Please note that there
     are only n(T-1) independent observations.
              •             .
     An unbiased estimator of               2.is    .

                                                   given by

(2.13)
                        y1M1y1/(n-K)

where         I -
                        X1(XX1)X            Also an unbiased estimator
of       is


(2.1k) 2 y;M2Y2/(nT'- K')

where I -                z2(z;z2)-1z;,      T' = T -    1,    and K'     K - 1.

An Aiticen es1imtor of the slope coefficients based on the
estimated values of             and a2 is given by



                    =           +
(2.l)
                         [zz        z2z2]
                                             [Z1NY1 +
                                                        Z2Y2]

where Q [2
We can readily show that the estimator 2) as obtained in (2.11)
is equivalent to a covariance estimator (iS)                 obtained   by assuming
     as fixed parameters. We can also show equivalence of iS(O)

with an ordinary least squares, estimator (iS) when                        0 as
follows:

     An ordinary least squares estimator of the slope coeffi-
cients in (1.1) is



6.   A similar pooled estimator of (O) can be obtained if A
     and Vjt are assumed to beH random. The variance-covarjance
     matrix of u is giver by aX2(LninIT) + GV2InT .   To reduce
     this to scalar form we consider an orthogonal matrix
     O [t/v'i, C1]' of order n, and apply transformations
     Qi   (1.n/1/'®IT) and Q2 (C1øITto all the nT observations.
                                              —10—


(2.16)       6
                    (ZQZ)ZQy ,
where
                         1nT'nT
                 'nT —    nT
                                                                     2
The Aitken estimator 6(0) in (2.15) when a                               o is
                                                                 U

(2.17)       () ZNZ1 + Z2Z2][Z1Ny1                           +


                     = [z(QNQ1            +
                                              Q;Q2)Z]—1cZ-(QNQ1
                                                                              +
                                                                                  Q;Q2)y]
                                  a                                                                  ft

                    (I
                            1T'T )    nT'nT
                                      —______            and
                                                                     a
                                                                          =                 —   ____
                                                                                                'T1T)
Since Q1NQ1                                                                                      T
                                                                         nT nT
we can easily show that QNQ1 +                               'nT
                                                                     -
                                                                          nT          Q , thus
                                                         2
proving equality of            and () when a                       o.
                                                     U
                                                -11-
                                                                                                     .
                                         Section       3

                            Properties of the Estimators


        The estimator (O) of can be written in the generalized

least squares form as follows:
                                                                       -l
                                                                                      1
                              [Z1N                           NZ1            uz;N               Ny1
(3.1)                                           a1                          I        a1

                                                  0
                                                                  Z2


                                  A                                                       —

If W        a


          [Z1N, Z2] ,
                        a
                                  V
                                      diag[a1
                                                A. 2       A. 2
                                                                  mT...] and y     [y1N, y2]
the equation (3.1) can be written as

                A A            A_i —1
(3.2)           tS(O)        (WV W) W V y

Substituting y W + e, where e                              [uN, u;] in (3.2), we get

                A A
(3.3)           5(O)          5 + (W'V
                                         A —1
                                                W)—1W A....]
                                                      V e

A generalized least squares estimator for a given V, as obtained

in (2.12), can also be expressed as follows:

                A
                6(0) =        6   + (W
                                         ---1  —l-1
                                          V W) W V
(3.')


If we assume that U's are normally distributed, and since
                                                                   0, we an show that the
M1Z1N MX1 [In - X1(XX1)1X]X1
 linear form Z1N1 is distributed independently of the quadratic

 form a2 yM1y1/(n-K) .                          Similarly,        we can show that the

 linear form Z2u2 is independently distributed of the quadratic
                                             —12—


form                yM2y2/(nT-K). With these results, we can show
that


(3.5)               E[()IO] = 6

Since        the expectation of 6 over the distribution of                      is 6,

i.e., E6              6, this   proves that 6(0)          is   an unbiased estimator

of 6.

To establish the asymptotic properties of 6(0), we assume that
                                                          —l -
X's are weakly non-stochastic and, for a fixed n, urn (nT) Z1NZ1,
                                                                         T+oo

urn      (nT)-'z;z2 are all finite positive definite matrices.7

For a fixed n, under the above assumptions, we can show that

lim n—2—2                liin —2—2—
                 -                       -
      T (Z1Nu1u1NZ1)          n T Z,,uu,Z
                                    L L.
                                             0, thus insuring
T+co                                     T.+co

that plim (nT)ZNu1 0                             plim (nT)-iz;u2. Also, plim a12
             T÷                                  T÷oo                           T÷°°
     2
         ,
                .
             plim
                     "2
                     c1 =
                                2
                                    .   Under these conditions, we can easily
              T-
show that


(3.6)               plirn /if[() — 6]               0 ,
                     T-,co


i.e., 6(e) is a consistent estimator of 6.



7.           The assumption of non-stochastic implies that the time
             pattern of the variable is bounded by some finite limits,
             even though it is not necessary for the pattern of the
             variable to repeat itself. The meaning of non-stochastic
             X's is simply that the realization of the X's is in ac-
             cordance with some fixed (albeit unknown) process. Since
             economic data are stochastic, whichever assumption we
             adopt about the nature of the fixity of X's, we are sin—
             plifying and possibly mis-specifying the model. See also
             Wallace and Hussain (P4) pp. 55—72.
                                     -13-


Under the above assumptions, we can readily show that V is a

ccnsistent estimator for V, and that 6(0), as obtained in

(2.15), is asymptotically equivalent to 6(0), in the sense

that /i[6(0)        6(0)] converges in probability to zero as

T+oo, both coefficient estimators being asymptotically normally
distributed with mean vector 6 and covariance matrix         (WVW).
Since plim            =        2, we can further show that () is also
                          Ii
        T+oo

asymptotically     equivalent to the covariance estimator 6, i.e.,


(3.7)          plim /iTI6(G) — 61       0
                T+°

In fact we can show that there is an infinitely large number

of estimators which yield asymptotically equivalent estimates

with asymptotically equivalent variance-covariance matrices.
Thus asymptotic theory casts relatively little light on the
comparative small sample properties of the estimators. In the

next section, we evaluate relative efficiency of the various

estimation procedures by using a Monte Carlo study.




                                                                        .
                                     —l4—

                                Section

                Design of the Experiment and the
        Comparative Properties of the Various Estimators


     The design of the Monte Carlo experiments given here is

similar to that of Nerlove, except that our model contains an
inLercept and we generate random numbers by a slightly differ-

ent, but more efficient, method.8 Since Nerlove has already
done extensive Monte Carlo studies, we examine intensively

only those cases with large inter-individual heterogeneity and

varying T. The model is given by


(L.l)            =       +           + 'i   +
                                                'it
The explanatory variable, X1, held fixed throughout the
experiment, is generated as follows:


(.2)          X1 =   o.i(t-l)    +
                                     l.05X1±t_i
                                                      +
                                                          w,
where w is uniformly distributed in the range from 0 to 2.
Initial values of X10 are chosen at random from the uniformly
distributed numbers in the range 0 to 100. To generate nT
values of        independent normal variables with zero mean and

init variance, n p.'s are first selected with                     N(0,

nT          are then selected with                    N(0, cY2), and these are

summed   to   give the               Defining p, the intra-class correla-



8.      This method and a method to convert uniformly distributed
        random variables to normal, variate is described in Appendix
        A—2. See also Nerlove (11), pp. 366—371.
                                  —15—


tion coefficient, as p = a 21a2, where            a2 2 + a2     we can
                                                                          .
write           N(0, pa2) and   v N(0,          (l—p)a2)

         Twelve sets of y's were generated for, various combina-

tions of the parameter values               0 and 5;       0.5 and 0.8;

p        0, 0.4 and 0.8; and a2    10. Initially n is set at 25 and

T at 6 .        For each set of parameters, five estimating proce-
dures were examined:

          (a) OLS estimator based on data aggregated over time,



          (b)   covariance estimator, (2),

          (c)   ordinary least squares, (s),

          (d)   pooled estimator based on the estimated variance—

                covariance matrix, ô(O),
          (e) generalized least squares based on known variance-

                covariance matrix, (8).

          In each experiment, 20 repetitions are performed, from
which the mean and the mean square error of the estimated
coefficients are calculated. The entire set of experiments is

ibepeated with T set at 15, giving 480 runs and 24 tables of



    9.    These parameter's were selected from the initial set of
          parameters c'    0.0, 0.5,     1.0,
                                         5.0 and 10.0;
                                                    a2 =
                                                            0.1,
                                                         lOand2O.
          0.5, 08; p= 0.0, 0.2, 0.4, 0.6and0.8;
          For these 150 sets of parameters 2 repetitions were per-
          formed. On the basis of mean square error of the estima-
          tors in the various estimating procedures only 12 para-
          meter sets were selected for intensive study. The choice
          of these parameter values may itself cause bias in our
          results, but the very consistency of the trend strengthens
          our belief that this is a representative set.
                                                                          .
                                 -16—


mean and mean square error of the coefficients for different

etimating procedures. Table 1 presents the mean and the mean
square error for one such experiment. Results of various other
runs are presented in an appendix to this paper)°

                                Table 1

         Mean and Mean Square Error of the Coefficients
               for Various Estimating Methods for
                        10, N   25, T      6, and p =    .8




True Value        5                   0            0.5         0

OLS--Agg.         'I.   99184         0. 703589    0.499717    7. 725E—05
Covariance                                         0.499688    5. 221E—05
OLS               Li.   99208         0.594746     0.499714    5. 5LiOE—05

Pooled            5. 00708            0.400888     0.499682    2. 791E—05
GLS                     99329         0.386660     0.499695    2. 788E—05




      From Table 1, we find that the mean values of           and

for all estimating methods are finitely close to the true
values, thus demonstrating that all estimators under considera-

tion are   unbiased,     but that the mean square error for the dif-

ferent estimators varies considerably. The mean square error

of OLS--Agg. is about three times as large as that of the
generalized least squares estimator, while those of the covari-

  nce estimator and of the ordinary least squares estimator are



 10. Mean square error of an estimator            of 0 is given by
                          A       2
      m.s.e.        E (0. — 0)
                   i=l
                             —17—
                                                                  .
only   about twice as large. The mean square error of the
pooled estimator is nearly the same as that of the generalized

least squares.
       This is true for all values of p except pO. In this
case, all estimators have mean square error equal to that of

the generalized least squares estimator.11 As p increases,
sà does the ratio of the mean square error of the OLS estimator
tc that of the GLS estimator, but for all values of p the mean

square error of the pooled estimator nearly equals the mean

square error of the GLS. Further, for large values of p,
the OLS method gives a serious underestimate of a2, giving low
standard errors of the estimates. In contrast, the standard
errors for the pooled estimator and the GLS are nearly equal.

As T increases, the mean square error of the covariance esti-
mator declines, becoming almost equal to that of the pooled

estimator and the GLS estimator.
       Hence we see, on the basis of the criterion of minimum

mean square error, that the pooled estimator compares favor-
ably, for all T's and all p's, with all other estimators which

do not require a prior knowledge of the variance-covariance
matrix. Furthermore, this estimator shows definite superiority
to other estimators for small T's and large p. On the basis of

 the criterion of unbiasedness, this compares equally well with

 all other estimators.


 11. See tables 1 and 2 in the appendix to this paper.
                            —18—



                         Section 5

                         Conclusion


     In this paper, we have developed an operational method

for estimating error components regression models when the
variance-covarjance matrix of the disturbance terms is un-
known. Monte Carlo studies were conducted to compare the

relative efficiency of the pooled estimator obtained by this
procedure to (a) an ordinary least squares estimator based
on data aggregated over time, (b) the covariance estimator,

(c) the ordinary least squares estimator, and Cd) a gener-
alized least squares estimator based on a known variance-
covariance matrix. For T small and large p, this estimator

definitely performs better than the other estimators which
are also based on an estimated value of the variance-covarjance

matrix of the disturbances. For p small and large T it com-
pares equally well with the other estimators. In this instance,
therefore, we are able to give a definite unconditional answer

to the question posed to Nerlove's Dodo, "But who has won?"--
the pooled estimator, of course!
                                   —19—



                              Appendix A-i
                                                                                      .
                                  Table 1

      Mean and Mean Square Error of the Coefficients
          for Various Estimating Procedures for
                   T       6, N     25, and a2 = 10



      Method           Mean               in.s.e.       Mean         m.s.e.



p=0.O: True Value      5                  0             0. 80        0

                       5.20645            0.286728      0.798293     6. 5614E—05
       OLS——Agg.
       Covariance                                       0.806431     3. 370E014

       o LS            5. 14152           0.216076      0.799298     5. 342E—05

       Pooled          5.12310            0.200142      0.799582     5. 227E—05

       GLS             5. 114149          0.216061      0. 799 297   5. 3'42E—05



p0.4 True Value        5                  0             0.80         0

       OLS--Agg.       5.34402            0.839164      0.794023     1. 970E—04

       Covarianc e                                      0. 800825    2. 19 OE—0 '4

       OLS             5.28975            0.6732143     0. 794863    1. 900E—04
       Pooled          5.18582            0.544211      0.796471     9. 10 8E—05

       GLS             5.16231            0.538673      0.796834     8. 246E—05


p0.8 True Value        5                  0             0. 50        0

                       4. 99184           0.70300       0.499717     7. 725E—05
       OLS—-Agg.
       Coy ar iance                                     0.499688     5. 221E—05

       o L1S           4.99208              0. 594746   0.499714     5. 540E—05

       Pooled          5. 00708             0.400888    0.499682     2. 7 3 1E —05

       GLS             4.99329              0.386660    0. 499695        2. 788E—05



                                                                                      S
                                —20—



                            Table 2

      Mean and Mean Square Error of the Coefficients
             for Various Estimating Procedures for
                  T   15, N   25, and 2 = 10


      Method        Mean               m.s.e. ()    Mean   ()    m.s.e.



p0.0: True Value     5.0               0            0.80         0

      OLS——Agg.      5.04207           0.214164     0.800005     l.527E—05
      Covariance                                    0.800616     1.241E-05
      OLS            5.00136           0.0741410    0.800355     '4.829E—06
      Pooled         5.00143           0.074891     0.800353     14.72lE06
      GLS            5.00150           0.0714409    0.800352     4.827E—06


p=0.4: True Value    5.0               0            0.80         0

      OLS——Agg.      14.88203          1.03832      0.800393     6.816E—05
      Covariance     --                —-           0.799514     6.300E-06
      OLS            4.94057           0.346083     0.799890     l.593E—05
       Pooled        4.97141           0.217877     0.799625     6.210E—06
      GLS            4.97792           0.222983     0.799570     6.l35E—06


p=0.8: True Value    5.0               0            0.80         0
       OLS--Agg.     5.573014          2.61798      0.793599     2.O1OE—04
       Covariance                                   0. 799 879   1. 413E—06
       OLS           5.15513           0.639253     0.797190     3.645E—05
       Pooled        4.85477           0. 3414144   0.799771     1. 381E—06
       GLS           4.85116           0.344148     0.799802     1.368E—06
                                  —21—
                                                                     .
                              Appendix A-2
                Random Number Generating Procedures

          A desired sequence of random numbers X is obtained by

setting

               Xn+1     (aXn + c) mod m         — 0
                                              m >


where a is the multiplier, c is the increment and m is the
modulus, a>0, c>O, m>c, m>a, and m>X0, where X0 is the starting
value. This method is called linear congruential sequence.

When c0, the random generation process is slightly faster,
but the maximum period length (length after which sequence
starts repeating itself) can not be achieved. Nerlove (11)
in order to avoid this problem, suggests mixing two random

sequences into a third, so that the third one is extremely
random. We use a method suggested by Maclaren and Marsaglia
as described below.1

          A quite random sequence     Given methods for generating

two sequences Xn and Y, this method produces a "considerably
more random" sequence. We use an auxiliary table V(0), V(l),

     .,   V(k-l),   where k is some number chosen for convenience,

usually in the neighborhood of 100. Initially, the V-table
is filled with the first k values of the X-sequence.



1.        See also Knuth (8) pp. 25-31
                                  —22--


     Step 1:   [Generate X, Y]            Set X, Y equal to the next

          number of the sequence (X Y) respectively.
     Step 2:   [Extract j]        Set 5       [kY/rn], where m is the

          modulus used in the sequence Y; i.e., 5 is a random
          value, 0 < 5 <     k   determined by Y.

     Step 3:   [Exchange]        Output V(j) and then set V(j) ÷ X.


     This method gives an incredibly long period if the periods
of (X) and (Y) are relatively prime; and even if the period
is of no consequence, there is very little relation between
the nearby terms of the sequence. To generate independently
normal variates we follow the Polar Method, which consists of

generating two independent random variables (u1 and u2) uni-

formly distributed between zero and one.2 A set of independent
normal variates with mean zero and variance one is obtained by
the transformation

                                 1/2
          w1 = (—2 log u1)             cos (2iru2)

                                 1/2                 3
          w2 = (—2 log u2)             cos (2iru1)




2.   To generate variable uniformly distributed between zero
     and one, we first generate some random number X between
     zero and in as described above, and then the fraction
     u1 = X/m will lie between zero and one.
3.   For a comprehensive discussion of this method see
     Knuth (8) pp. 103-105; also see Nerlove (11), p. 368
     footnote 11.
                            —23—


                         Ref erences


1.   Anemiya, Takeshi (1971): "The Estimation of the
     Variances in a Variance-Components Model," International
     Economic Review, Vol. 12, No. 1, pp. 1—13.

2.   Arora, Swarnjit S. (1971): "Error Components and Multi-
     polar Human Flow Models," unpublished Ph.D. dissertation
     submitted to the State University of New York at Buffalo,
     September 1971.
3.   Balestra, P. and M. Nerlove (1966): "Pooling Cross
     Section and Time Series Data in the Estimation of a
     Dynamic Model: The Demand for Natural Gas," Econornetrica,
     Vol. 34, No. 4, pp. 585—612.

4.   Henderson, Charles R., Jr. (1971): "Comment on the Use
     of Error Components Models in Combining Cross Section
     with Time Series Data," Econometrica, Vol. 39, No. 2,
     pp. 397—401.
5.   Hoch, I. (1962): "Estimation of Production Function
     Parameters Combining Time-Series and Cross-Section Data,'t
     Econometrica, Vol. 30, No. 1, pp. 34-53.

6.   Hussain, A. (1969): "A Mixed Model for Regression,"
     Biometrika, Vol. 56, pp. 327—36.
7.   Kuh, E. (1959): "The Validity of Cross-Sectionally
     Estimated Behavior Equations in Time Series Application,"
     Econornetrica, Vol. 27, No. 2, pp. 197-214.

8.   Knuth, Donald E. (1968): Seminumerical Algorithms--The
     Art of Computer Progranuning, Vol. 2, Addison-Wesley
     Publishing Company, Reading, Massachusetts, pp. 25-31
     and pp. 103-105.

9.   Maddala, G.S. (1971): "The Use of Variance Components
     Models in Pooling Cross Section and Time Series Data,"
     Econometrica, Vol. 39, No. 2, pp. 341-58.
10. Mundlak, Y. (1963): "Estnation of Production and
     Behavioral Functions from a Combination of Cross-Section
     and Time Series Data," pp. 138-66 in Measurement in
     Economics, C.F. Christ, Ed., Stanford, California,
     Stanford University Press.
                            —24—


11. Nerlove, M. (1971a): "Further Evidence on the Estimation
     of Dynamic Economic Relations from a Time Series of Cross
     Section," Econometrica, Vol. 39, No. 2, PP. 359-81.

12.   ______ (1971b): "A Note on Error Components Models,"
      Econometrica, Vol. 39, No. 2, pp. 382-96.
13. Swamy, P.A.V.B. and S.S. Arora (1972): "The Exact Finite
     Sample Properties of the Estimators of Coefficients in
     the Error Components Regression Models," Econometrica,
     Vol. 4O, No. 2, pp. 261-75.
l'4. Wallace, T.D. and A. Hussain (1969): "The Use of Error
     Components Models in Combining Cross Section with Time
     Series Data," Econometrica, Vol. 37, No. 1, pp. 55-72.
