                                 NBER WORKING PAPER SERIES




                    FAST, "ROBUST", AND APPROXIMATELY CORRECT:
                         ESTIMATING MIXED DEMAND SYSTEMS

                                             Bernard Salanié
                                             Frank A. Wolak

                                          Working Paper 25726
                                  http://www.nber.org/papers/w25726


                       NATIONAL BUREAU OF ECONOMIC RESEARCH
                                1050 Massachusetts Avenue
                                  Cambridge, MA 02138
                                       April 2019




We are grateful to Dan Ackerberg, John Asker, Steve Berry, Xiaohong Chen, Chris Conlon, Pierre
Dubois, Jeremy Fox, Han Hong, Guy Laroque, Simon Lee, Arthur Lewbel, Thierry Magnac, Lars
Nesheim, Ariel Pakes, Mathias Reynaert, Tobias Salz, Richard Smith, Pedro Souza, Frank Verboven,
Martin Weidner, and Ken Wolpin for their useful comments, as well as to seminar audiences at NYU,
Rice, UCL, and the Stanford Institute for Theoretical Economics (SITE). We also thank Zeyu Wang
for excellent research assistance. The views expressed herein are those of the authors and do not necessarily
reflect the views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2019 by Bernard Salanié and Frank A. Wolak. All rights reserved. Short sections of text, not to
exceed two paragraphs, may be quoted without explicit permission provided that full credit, including
© notice, is given to the source.
Fast, "Robust", and Approximately Correct: Estimating Mixed Demand Systems
Bernard Salanié and Frank A. Wolak
NBER Working Paper No. 25726
April 2019
JEL No. L00,L13

                                          ABSTRACT

Many econometric models used in applied work integrate over unobserved heterogeneity. We
show that a class of these models that includes many random coefficients demand systems can be
approximated by a “small-σ” expansion that yields a linear two-stage least squares estimator. We
study in detail the models of product market shares and prices popular in empirical IO. Our
estimator is only approximately correct, but it performs very well in practice. It is extremely fast
and easy to implement, and it is “robust” to changes in the higher moments of the distribution of
the random coefficients. At the very least, it provides excellent starting values for more
commonly used estimators of these models.


Bernard Salanié
Department of Economics
Columbia University
1033 International Affairs Building, MC 3308
420 West 118th Street
New York, NY 10027
and CEPR
bs2237@columbia.edu

Frank A. Wolak
Department of Economics
Stanford University
Stanford, CA 94305-6072
and NBER
wolak@zia.stanford.edu
Introduction
Many econometric models are estimated from conditional moment conditions that
express the mean independence of random unobservable terms η and instruments Z:

                                      E pη|Zq “ 0.

In structural models, the unobservable term is usually obtained by solving a set of
equations—often a set of first-order conditions—that define the observed endogenous
variables as functions of the observed exogenous variables and unobservables. That
is, we start from
                                    Gpy, η, θ0 q “ 0                            (1)
where y is the vector of observed endogenous variables and θ0 is the true value of the
vector of unknown parameters. The parametric function G is assumed to be known
and can depend on a vector of observed exogenous variables. Then (assuming that
the solution exists and is unique) we invert this system into

                                      η “ F py, θ0 q

and we seek an estimator of θ0 by minimizing an empirical analog of a norm

                                  kE pF py, θqmpZqqk

where mpZq is a vector of measurable functions of Z.
    Unless F py, θ0 q exists in closed form, inversion often is a step fraught with diffi-
culties. Even when a simple inversion algorithm exists, it is still costly and must be
done with a high degree of numerical precision, as errors may jeopardize the “outer”
minimization problem. One alternative is to minimize an empirical analog of the
norm
                                       kE pηmpZqqk
subject to the structural constraints (1). This “MPEC approach” has met with
some success in dynamic programming and empirical industrial organization (Su and
Judd 2012, Dubé et al 2012). It still requires solving a nonlinearly constrained,
nonlinear objective function minimization problem; convergence to a solution can
be a challenging task in the absence of very good initial values. This is especially

                                            2
galling when the model has be estimated many times, for instance within models of
bargaining like those of Crawford and Yurokoglu (2012) or Ho and Lee (2017).
    We propose an alternative that derives a linear model from a very simple series
expansion. To fix ideas, suppose that θ0 can be decomposed into a pair pβ0 , σ0 q, where
σ0 is a scalar that we have reasons to think is not too far from zero. We rewrite (1)
as
                             Gpy, F py, β0 , σ0 q, β0 , σ0 q “ 0.
We expand σ Ñ F py, β0 , σq in a Taylor series around 0 and re-write F py, β0 , σ0 q as:

                                                                                      σ0L
   F py, β0 , σ0 q “ F py, β0 , 0q ` Fσ py, β0 , 0qσ0 ` . . . ` Fσσ...σ py, β0 , 0q       ` Opσ0L`1 q,
                                                                                      L!
where the subscript σ denotes a partial derivative with respect to the argument σ.
   This suggests a sequence of “approximate estimators” that minimize the empirical
analogs of the following norms

                    kE pF py, β, 0qmpZqqk
                    kE ppF py, β, 0q ` Fσ py, β, 0qσq mpZqqk
                    › ˆˆ                                              σ2
                                                                         ˙     ˙›
                    ›E    F py, β, 0q ` Fσ py, β, 0qσ ` Fσσ py, β, 0q      mpZq ›
                    ›                                                           ›
                                                                      2
              ...

If the true value σ0 is not too large, one may hope to obtain a satisfactory estimator
with the third of these “approximate estimators.” In general, this still requires solving
a nonlinear minimization problem. However, suppose that the function F satisfies
the following three conditions:

C1: Fσ py, β0 , 0q ” 0

C2: F py, β, 0q ” f0 pyq ´ f1 pyqβ is affine in β for known functions f0 p¨q and f1 p¨q.

C3: the second derivative Fσσ py, β, 0q does not depend on β.

Denote f2 pyq ” ´Fσσ py, β, 0q. Under C1–C3, we would minimize
                    › ˆˆ                              σ2
                                                         ˙     ˙›
                    ›E      f0 pyq ´ f1 pyqβ ´ f2 pyq      mpZq ›.
                    ›                                           ›
                                                      2

                                                   3
Taking the parameters of interest to be pβ0 , σ02 q, this is simply a two-stage least
squares regression of f0 pyq on f1 pyq and f2 pyq with instruments mpZq. As this is
a linear problem, the optimal1 instruments mpZq associated with the conditional
moment restrictions Epη|Zq “ 0 are simply

                          mpZq “ Z ˚ “ pE pf1 pyq|Zq , E pf2 pyq|Zqq .

These optimal instuments could be estimated directly from the data using nonpara-
metric regressions. Or more simply, we can include flexible functions of the columns
of Z in the instruments used to compute the 2SLS estimates.
   The resulting estimators of β0 and σ02 are only approximately correct, because
they consistently estimate an approximation of the original model. On the other
hand, they can be estimated in closed form using linear 2SLS. Moreover, because
they only rely on limited features of the data generating process, they are “robust”
in ways that we will explore later.
   Conditions C1–C3 extend directly to a multivariate parameter σ0 . They may
seem very demanding. Yet as we will show, under very weak conditions the Berry,
Levinsohn, and Pakes (1995) (macro-BLP) model that is the workhorse of empirical
IO satisfies all three. In this application, σ0 is taken to be the square root of the
variance–covariance matrix Σ of the random coefficients in the mixed demand model.
More generally, we will characterize in Section 6.1 a general class of models with
unobserved heterogeneity to which conditions C1–C3 apply.
    Our approach builds on “small-Σ” approximations to construct successive approx-
imations to the inverse mapping (from market shares to product effects). Kadane
(1971) pioneered the “small-σ” method. He applied it to a linear, normal simulta-
neous equation system and studied the properties of k-class estimators2 when the
number of observations n is fixed and σ goes to zero. He showed that when the num-
ber of observations is large, under these “small-σ asymptotics” the k-class estimators
have biases in σ 2 , and that their mean-squared errors differ by terms of order σ 4 .
Kadane argued that small σ, fixed n asymptotics are often a good approximation to
finite-sample distributions when the estimation sample is large enough.
      The small-σ approach was used by Chesher (1991) in models with measurement er-
  1
      In the sense of Amemiya (1975).
  2
      Which include OLS and 2SLS.

                                               4
ror. Most directly related to us, Chesher and Santos-Silva (2002) used a second-order
approximation argument to reduce a mixed multinomial logit model to a “heterogene-
ity adjusted” unmixed multinomial logit model in which mean utilities have additional
terms3 . They suggested estimating the unmixed logit and using a score statistic based
on these additional covariates to test for the null of no random variation in preferences.
Like them, we introduce additional covariates. Unlike them, we develop a method to
estimate jointly the mean preference coefficients and parameters characterizing their
random variation; and we only use linear instrumental variables estimators. To some
degree, our method is also related to that of Harding and Hausman 2007, who use a
Laplace approximation of the integral over the random coefficients in a mixed logit
model without choice-specific random effects. Unlike them, we allow for endogeneous
prices; our approach is also much simpler to implement.
    Section 1 presents the model popularized by Berry–Levinsohn–Pakes (1995) and
discusses some of the difficulties that practitioners have encountered when taking it
to data. We give a detailed description of our algorithm in section 2; readers not in-
terested in the derivation of our formulæ in fact can jump directly to our Monte Carlo
simulations in section 7. The rest of the paper justifies our algorithm (sections 3 and
4); studies its properties (section 5); and discusses a variety of extensions (section 6).



1       The macro-BLP model
Our leading example is taken from empirical IO. Much work in this area is based on
market share and price data. It has followed Berry et al (1995—hereafter BLP) in
specifying a mixed multinomial logit model with product-level random effects. To
deal with the endogeneity of prices implied by these product-level random effects,
BLP use a Generalized Method Moments (GMM) estimator that relies on the mean
independence of the product-level random effects and a set of instruments.
   To fix ideas, we define “the standard model” as follows4 . Let J products be
available on each of T markets. Each market contains an infinity of consumers who
    3
     Ketz (2018) builds on a quadratic expansion in σ0 “ 0 to derive asymptotic distributions when
the true σ0 is on the boundary.
   4
     While some of our exposition relies on it for simplicity, our methods apply to a more general
model— see section 6.1.


                                                5
choose one of J products. Consumer i in market t derives a conditional indirect utility
from consuming product j equal to

                                   Xjt pβ0 ` i q ` ξjt ` uijt .

There is also a good 0, the “outside good”, whose utility for consumer i is typically
normalized to equal ui0t . The random variables  represent individual variation in
tastes for observed product characteristics, while the u stand for unobserved hetero-
geneity observed by the individual, but not by the econometrician. The vector  and
u are independent of each other, and of the covariates X and product random effects
ξ. Berry et al. (1995) assume that the vector uit “ pui0t , ui1t , . . . , uiJt q is indepen-
dently and identically distributed (iid) as standard type-I Extreme Value (EV); the
product effects ξjt are unknown mean zero random variables conditional on a set of
instruments; and the random variation in preferences i has a mean-zero distribution
which is known up to its variance-covariance matrix Σ0 . The i are often taken to be
independent, identically distributed N p0, Σ0 q random vectors with a diagonal Σ0 .
    Some of the covariates in Xjt may be correlated with the product-specific random
effects. The usual example is a model of imperfect price competition where the prices
firms set in market t depend on the value of the vector of unobservable product
characteristics, ξt , some of which the firms observe.
    The parameters to be estimated are the mean coefficients β0 and the variance-
covariance matrix of the random coefficients Σ0 . We collect them in θ0 “ pβ0 , Σ0 q.
The data available consists of the market shares ps1t , . . . , sJt q and prices pp1t , . . . , pJt q1
of the J varieties of the good, of the covariates Xt , and of additional instruments
Zt , all for market t. Note that the market shares do not include information on
the proportion S0t of consumers who choose to buy good 0. Typically the analyst
estimates this from other sources. Let us assume that this is done, so that we can deal
with the augmented vector of market shares pS0t , S1t , . . . , SJt q, with Sjt “ p1 ´ S0t qsjt
for j P J “ t1, . . . , Ju.
   The market shares for market t are obtained by integration over the variation in
preferences : for good j P J ,
                               „                                    
                                     exp pXjt pβ ` q ` ξjt q
                     Sjt “ E                                                   (2)
                                 1 ` ΣJk“1 exp pXkt pβ ` q ` ξkt q
and S0t “ 1 ´ Jj“1 Sjt .
               ř


                                                  6
       Berry et al. (1995) assume that

                                         E pξjt |Zjt q “ 0

for all j P J and t. The instruments Zjt may for instance be the characteristics
of competing products, or cost-side variables. The procedure is operationalized by
showing that for given values of θ, the system (2) defines an invertible mapping5 in
IRJ . Call ΞpSt , Xt , θq its inverse; a GMM estimator obtains by choosing functions
  ˚
Zjt  of the instruments and minimizing a well-chosen quadratic norm of the sample
analogue of:
                                      `              ˚
                                                        ˘
                                    E ΞpSt , Xt , θqZjt
over θ.
    These models have proved very popular; but their implementation has faced a
number of problems. Much recent literature has focused on the sensitivity of the
estimates to the instruments used in GMM estimation of the mixed multinomial
logit model. Reynaert–Verboven (2014) showed that using linear combinations of
the instruments can lead to unreliable estimates of the parameters of interest. They
recommend using the optimal instruments given by the Amemiya (1975) formula:
                                   ˆ                       ˙
                            ˚        BΞ
                          Zjt “ E       pSt , Xt , θ0 q|Zjt .
                                     Bθ
As implementing the Amemiya formula relies on a consistent first-step estimate of θ0 ,
this is still problematic. Gandhi and Houde (2016) propose “differentiation IVs” to
approximate the optimal instruments for the parameters Σ of the distribution of the
random preferences . They also suggest a simple regression to detect weak instru-
ments. An alternative is to use the Continuously Updating Estimator to build up the
optimal instruments as minimization progresses. Armstrong (2016) points out that
instruments based on the characteristics of competing products achieve identification
through correlation with markups. But when the number of products is large, many
models of the cost-side of the market yield markups that just do not have enough
variation, relative to sampling error. This can give inconsistent or just uninformative
estimates6 .
   5
    See Berry (1994).
   6
    Instruments that shift marginal cost directly (if available) do not need variation in the markup
to shift prices, and therefore do not suffer from these issues. Variation in the number of products
per market may also be used to restore identification, data permitting.

                                                 7
    Computation has also been a serious issue. The original BLP approach used a
“nested fixed point” (NFP) approach: every time the objective function to be mini-
mized was evaluated for the current parameter values, a contraction mapping/fixed-
point algorithm must be employed to compute the implied product effects ξt from the
observed market shares St and for the current value of θ. This was both very costly
and prone to numerical errors that propagate from the nested fixed point algorithm
to the minimization algorithm. Dubé et al (2012) proposed a nonlinearly-constrained,
nonlinear optimization problem to estimate θ. Their simulations suggest that this
“MPEC” approach often outperforms the NFP method, sometimes by a large factor.
Lee and Seo (2015) proposed an “approximate BLP” method that inverts a linearized
approximation of the mapping from ξt to St . They argue that this can be even faster
than the MPEC approach to estimation. Petrin and Train (2010) have proposed a
maximum likelihood estimator that replaces endogeneous regressors with a control
function. This circumvents the need to compute the implied value of ξ for each
value of θ, but still requires solving a nonlinear optimization problem to compute an
estimate of θ0 .
    Solving a nonlinear optimization problem for a potentially large set of parame-
ters is time-consuming. It typically requires starting values in the neighborhood of
the optimal solution; closed-form gradients; and careful monitoring of the optimiza-
tion algorithm by the analyst, as the objective function is not globally concave. The
method we propose in this paper completely circumvents the need to solve a non-
linear optimization problem. Our estimator relies on an approximate model that is
exactly valid when there is no random variation in preferences, and becomes a coarser
approximation as the amplitude of the random variation in elements of β ` i grows.
As such, our estimator is not a consistent estimator of the parameters of the BLP
model. On the other hand, it has some very real advantages that may tip the scale
in its favor. First, it requires a single linear 2SLS regression that can be computed
in microseconds with off-the-shelf software7 . Second, our estimator needs to assume
very little about the form of the distribution of the random variation in preferences 
(beyond its small scale), justifying the “robust” in our title—where the quotes reflect
   7
    Fox et al. (2011) discretize the distribution of the random coefficients on a grid and estimate the
corresponding probability masses. This also results in a least-squares estimator—but it is constrained
by linear inequalities and may be sensitive to the choice of the grid points.



                                                  8
our awareness that we are taking some liberties with the definition of robustness. Fi-
nally, because our estimating equation is linear, computing the “optimal” instruments
for our estimator is also straightforward.
    Some readers may find the “approximate correctness” of our estimator unsatis-
fying. It at least yields “nearly consistent” starting values for the classical nested-
fixed point and MPEC nonlinear optimization procedures at a minimal cost. This
can address a major challenge associated with successfully implementing the MPEC
estimation procedure. It also provides useful diagnoses about how well different pa-
rameters can be identified with a particular model and dataset; and a simple way to
select between models, as we discuss below.



2         2SLS Estimation in the Standard BLP Model
For the reader primarily interested in applying our method, this section provides a
step-by-step guide to implementing the estimator in the standard macro-BLP model.
This requires some notation. The dimensions of the vectors and matrices are as
follows:

    • for each j P J and t, Xjt is a row vector with nX components

    • β is a column vector with nX components

    • for each i, i is a row vector with ne components; in the standard model,
      ne ď nX .

We denote I as the set of pairs of ordered indices pm, n ď mq such that the variance-
covariance element Σmn “ covpim , in q is not restricted to be zero8 . For notational
simplicity, we also assume that we use all J ˆ T conditional moment restrictions:

                                             E pξjt |Zjt q “ 0.

Adapting our procedure to subsets of moment restrictions is straightforward.

        Our procedure runs as follows:
    8
        E.g. if ne “ nX and Σ is assumed to be diagonal, I “ tp1, 1q, . . . , pnX , nX qu.


                                                       9
Algorithm 1. Fast Robust and Approximately Correct (FRAC) estima-
tion of the standard BLP model

  1. For every market t, augment the market shares from ps1t , . . . , sJt q to pS0t , S1t , . . . , SJt q

  2. For every product-market pair pj P J , tq :
                                                                                řJ
         (a) compute the market-share weighted covariate vector et “              k“1   Skt Xkt ;
                                                                        jt
         (b) for every pm, nq in I, compute the “artificial regressor” Kmn as
                                    ´           ¯
                              jt      Xjtm
               • if n “ m: Kmm    “    2
                                           ´ etm Xjtm ;
                             jt
                • if n ă m: Kmn “ Xjtm Xjtn ´ etm Xjtn ´ etn Xjtm .
         (c) for every j “ 1, . . . , J, define yjt “ logpSjt {S0t q

  3. Run a two-stage least squares regression of y on X and K, taking as instru-
     ments a flexible set of functions of the columns of Z. Define β̂ to be the es-
     timated coefficients associated with X and (the nonzero part of ) Σ̂ to be the
     estimated coefficients associated with K.

  4. (optional9 ) Run a three-stage least squares (3SLS) regression across the T mar-
     kets stacking the J equations for each product with a weighting matrix equal to
     the inverse of the sample variance of the residuals from step 3.

    Consistent estimates of the covariance matrix of the asymptotic distribution of
?
  T Jpθ̂ ´ plimpθ̂qq can be obtained from the expressions for the heteroskedasticity
consistent covariance matrix for the 2SLS estimator given in White (1982).
   Ideally, the “flexible set of functions of the columns of Z” in step 3 should be
able to span the space of the optimal instruments EpX|Zq and EpK|Zq for our
approximate model. Alternatively, these optimal instruments can be estimated by a
nonparametric regressions of each the column of X on the columns of Z.
    As is well-known, misspecification of one equation of the model can lead to incon-
sistency in 3SLS parameter estimates of all equations of the model. It is therefore
unclear whether Step 4 is worth the additional effort. We intend to explore this
question in future work.
  9
      This step should only be considered when T " J.


                                                10
    It is important to note here that e is not a simple weighted average, as the weights
do not sum to one, but only to p1´S0t q. To illustrate, if Xjtm ” 1 is the constant, then
etm is p1 ´ S0t q and the artificial regressor that identifies the corresponding variance
parameter is
                                        jt           1
                                      Kmm   “ S0t ´ .
                                                     2
More generally, if Xjtn “ 1 pj P J0 q is a dummy that reflects whether variety j belongs
to group J0 Ă J , then it is easy to see that the corresponding variance parameter is
the coefficient of the artificial regressor
                                                 ˆ           ˙
                                  jt               1
                               Knn “ 1 pj P J0 q     ´ SJ0 t
                                                   2

where SJ0 t is the market share of group J0 on market t.



3      Second-order Expansions
The rest of the paper justifies algorithm 1 and discusses extensions. We first derive
the small-σ expansions of the introduction.
   We start from a specification of the conditional indirect utility of variety j for
consumer i on market t as

                                Xjt β ` g pXjt , i q ` ξjt ` uijt                    (3)

for j P J ; and Ui0t “ ui0t . Define the vectors uit “ pui0t , ui1t , . . . , uiJt q; Xt “
pX1t , . . . , XJt q; and ξt “ pξ1t , . . . , ξJt q. We assume that

    1. the random terms i are i.i.d. across i with finite variance;

    2. they are distributed independently of pXt , ξt q;

    3. EgpXjt , i q “ 0 for all Xjt ;

    4. the random vectors uit are i.i.d. across i and t; and they are distributed inde-
       pendently of pi , Xt , ξt q.




                                               11
These assumptions are all standard, except for the third one which is only a mild
extension of the usual normalization Ei “ 0. They allow for any type of codepen-
dence between the product effects ξt and the covariates Xt . Note that the additive
separability between β and  is not as strict as it seems. If for instance we start from
a multiplicative model with utilities
                                   nX
                                   ÿ
                                         Xjtk βk ζki ` ξjt ` uijt
                                   k“1

we can always redefine ki “ βk pζki ´ 1q to recover (3).
    Our crucial assumption, which we maintain throughout, is that the utilities are
affine in β and additive in the product effects ξ and in the idiosyncratic terms u. On
the other hand, we allow for any kind of distribution for i and uit . This encompasses
most empirical specifications used, as well as many more. We will refer to three special
cases for illustrative purposes:

   1. The standard model, also known as the mixed multinomial logit model, has
      g pX, q “ X; and the vector uit is distributed as iid standard type-I Extreme
      Value random variables.

   2. The standard binary model (or mixed logit model) further imposes J “ 1.

   3. The standard symmetric model is a standard model with  distributed symmet-
      rically around 0;

   4. The standard Gaussian model is a standard model with  normal with a diag-
      noral covariance matrix. It is probably the most commonly used in applications
      of the macro-BLP method.

   5. Finally, the standard Gaussian binary model imposes both 2 and 4.

    In order to do small-σ expansions, we need to introduce a scale parameter σ.
We do this with Assumption 1, which fits the usual understanding of what a scale
parameter is10 and also imposes the restriction that all moments of  are finite-valued.
The most common specification of the “macro-BLP” model has a Gaussian  and
satisfies Assumption 1.
  10
    In principle it should be possible to use several scale parameters, say σ1 for one part of the
variance-covariance matrix and σ2 for another one.

                                                 12
Assumption 1 (Finite moments). For some integer L ě 2, all moments of order
1 ď l ď L ` 1 of the vector  are finite; they are of order l in some non-negative scalar
σ. The first moment is zero: E “ 0. We denote Σ “ E1 the variance-covariance
matrix of , and µl (for l ě 3) its (uncentered) higher order moments.

   It will be convenient to write  ” σBv with v a random vector of mean zero
and covariance matrix equal to the identity matrix, so that V arpq ” Σ “ σ 2 BB 1 .
We only use this decomposition for intermediate results. Note that B is an ne ˆ nv
matrix pne ě nx q, where v is a row vector with nv components. This allows for Σ
to have a factor structure. Our final expansions do not depend on how σ and B are
normalized, and we won’t need to specify it.
    We drop the index t from the notation in most of this section as we will only need
to deal with one market at a time.


3.1       Second-order Expansions in the Standard Model
Much of the rest of the remainder of the paper focuses on the standard model, where
the idiosyncratic error terms u have iid Type I extreme value distributions. We will
show in section 6.4 how to extend our results to more general distributions.
   Recall that in the standard model, market shares are given by (2). If the scale
parameter σ was zero, inverting (2) would simply give us
                                                Sj
                                   ξj “ log        ´ Xj β for j P J ,                (4)
                                                S0
which is standard multinomial logit model without random coefficients. This is the
starting point of the contraction algorithm described in Berry (1994).
   Now let σ be positive. With  “ σBv, a Taylor expansion of (4) at σ “ 0 would
give (assuming that the expansion is valid11 )
                                Sj                             σl
                     ξj “ log      ´ Xj β ` ΣLl“1 alj pS, X, βq ` Opσ L`1 q,         (5)
                                S0                             l!
where the alj pS, X, βq are defined below. In this equation, X collects the covariates
of all products and S is the vector of market shares. Market-share weighted sums
will play a crucial role in what follows:
 11
      We return to this point in section 5.1.

                                                    13
Definition 1 (Market-share weighting). For any J-dimensional vector T of J com-
ponents, we define the scalar
                                       ÿJ
                               eS T “     Sk Tk .
                                            k“1

By extension, if m is a pJ ˆ Jq matrix with J columns pm1 , . . . , mJ q, we define the
vector
                                        ÿJ
                                eS m “      Sk mk .
                                            k“1

Finally, we denote T̂j “ Tj ´ eS T and m̂j “ mj ´ eS m.

    Note that we are using the observed market shares of the J goods, so that these
weighted sums are very easy to compute from the data. It is important to emphasize
that the operator eS is not an average, as the augmented market shares Sk for k P J
do not sum to one but to p1 ´ S0 q. Similarly, the T̂j terms are not residuals, and
eS T̂ ‰ 0 in general.
   Our first goal is to find explicit formulæ for the coefficients alj in (5). While this
can be done at a high level of generality, let us start with a result that covers a large
majority of applications.
   In the standard model, g pXj , q is simply Xj . Using the identity  ” σBv,
denote xj “ pXj Bq1 , a vector of nv components; and x the matrix whose J columns
are px1 , . . . , xJ q. Then
                                  g pXj , q “ σx1j v.
We now use this notation to derive the second-order expansion in σ in the standard
model.

Theorem 1 (Intermediate expansion in the standard model). In the standard model,

  (i) the alj coefficients only depend on S and on x;

 (ii) the first-order coefficients are zero: a1j ” 0 for all j;

(iii) the second-order coefficients are given by
                                                        ˜                       ¸
                                                                  J
                                                                  ÿ
                    a2j “ 2xj ¨ eS x ´ kxj k2 “ ´xj ¨    xj ´ 2         Sk xk       ;   (6)
                                                                  k“1


                                            14
 (iv) in the standard symmetric model, alj “ 0 for all j and odd l ď L. Therefore if
      L ě 3,
                                   Sj           a2j 2
                          ξj “ log    ´ Xj β `     σ ` Opσ 4 q.                 (7)
                                   S0            2
Proof. See Appendix A.


3.2    The Artificial Regressors in the Standard Model
When truncated of its remainder term, equation (7) becomes linear in the parame-
ters pβ, σ 2 q. The coefficients a2j , however, are quadratic combinations of the vectors
xj , which are themselves linear in the unknown coefficients of the matrix B. Fortu-
nately, the formula that gives a2j can be transformed so that it becomes linear in the
coefficients of the variance-covariance matrix Σ of .
   To see this, note that since xk “ B 1 Xk1 ,

                                            x1k xl “ Xk BB 1 Xl1 .

But because Σ “ σ 2 BB 1 , we have
                                            nX
                                            ÿ
                         2
                     σ       x1k xl   “            Σmn Xkm Xln “ Tr pΣXl Xk1 q
                                           m,n“1

where Trp¨q is the trace operator.
   Plugging this into (6) gives
                                      ˆ ˆ          ˙ ˙
                                2 a2j           Xj
                              σ   “ Tr Σ eS X ´     Xj1 .
                                2               2
Define the nX ˆ nX matrices Kj by
                                                   ˆ            ˙
                                            j          Xj
                                          K “             ´ eS X Xj1
                                                       2
                                      a
so that we can also write σ 2 22j “ ´ TrrΣKj s. The matrices Kj can be constructed
straightforwardly from the covariates X and the market shares S. Given that Σ is
symmetric,
                                      nX
                                      ÿ                        ÿ         ` j        ˘
                TrrΣKj s “                      j
                                           Σmm Kmm `                  Σmn Kmn    j
                                                                              ` Knm   .   (8)
                                  m“1                         m,năm

Define the lower-triangular matrices K j by

                                                         15
                        j
    • on the diagonal: Kmm “ Kjmm
                                     j
    • off the diagonal (for n ă m): Kmn “ Kjmn ` Kjnm .

We call their elements the “artificial regressors”, for reasons that will soon become
clear.
     Additional a priori restrictions can be accommodated very easily. For instance,
it is common to restrict Σ to be diagonal, as is the case in Berry et al. (1995). Then
only nX terms enter in the sum (8); moreover,
                                   ˜                  ¸
                                             J
                            j        Xjm    ÿ
                          Kmm    “        ´     Sk Xkm Xjm .
                                       2    k“1

If Σ is not diagonal, then we need to also use additional terms
                                ˜           ¸        ˜           ¸
                                  J
                                  ÿ                    ÿJ
                j
              Kmn  “ Xjm Xjn ´       Sj Xkm Xjn ´          Sj Xkn Xjm .
                                  k“1                  k“1


    To summarize, we have:

Theorem 2 (Final expansion in the standard model). In the standard model,
                                 nX
                     Sj          ÿ
                                          j
                                                ÿ
                                                        j
          ξj “ log      ´ Xj β ´     Σmm Kmm ´     Σmn Kmn ` OpkΣkk{2 q,           (9)
                     S0          m“1           năm

where the integer k ě 3; and k “ 4 if the model is symmetric.
    The artificial regressors are given by
                       ˜                    ¸
                                  J
                j        X jm
                                 ÿ
              Kmm    “        ´      Sk Xkm Xjm
                           2     k“1
                                   ˜          ¸      ˜         ¸
                                     ÿJ                J
                                                       ÿ
                 j
               Kmn   “ Xjm Xjn ´        Sj Xkm Xjn ´     Sj Xkn Xjm .
                                  k“1                  k“1



4     2SLS Estimation
Equation (9) is linear in the parameters of interest θ “ pβ, Σq, up to the remainder
term. This immediately suggests neglecting the remainder term and estimating the
                             S
approximate model ξj “ log S0j ´ Xj β ´ TrrΣK j s.

                                         16
   More precisely, assume we are given a sample of T markets, and instruments
Zjt such that E pξjt |Zjt q for all j and t. Then our proposed estimator θ̂ fits the
approximate linear set of conditional moment restrictions:
                      ˆ „                                 ˙
                              Sjt     `             jt
                                                       ˘
                  E log            ´ Xjt β ` TrrΣK s |Zjt “ 0                   (10)
                              S0t

which only differs from the original model by a term of order σ 3 (or σ 4 if the distri-
bution of  is symmetric)12 . This can simply be done by choosing vector functions
  ˚
Zjt of the instruments and running two-stage least squares: for each j “ 1, . . . , J
and t “ 1, . . . , T , we run a 2SLS regression of logpSjt {S0t q on Xjt and the relevant13
variables K jt , using the instruments Zjt ˚
                                             .


5      Pros and Cons of the 2SLS Estimation Approach
The drawback of our method is obvious: because this is only an approximate model,
the resulting estimator θ̂ will not converge to θ0 as the number of markets T goes
to infinity. We discuss this in much more detail in section 5.1. For now, let us
note that this drawback is tempered by several considerations. First, the number of
markets available in empirical IO is typically small; finite-sample performance of the
estimator is what matters, and we will examine that in Section 7. More importantly,
our estimator has several useful features. Let us list six of them:

    1. Because the estimator employs linear 2SLS, computing it is extremely fast and
       can be done in microseconds with any of-the-shelf software.

    2. We do not have to assume any distributional form for the random variation
       in preferences . This is a notable advantage over other methods: while they
       yield inconsistent estimates if the distribution of  is misspecified, our estimator
       remains consistent for the parameters of the approximate model.
  12
      Note that because our model is only an approximation, there may not exist a value of θ that
satisfies the conditional moment restrictions (10) in the population. Nevertheless, our 2SLS estima-
tion procedure will still yield an estimate of the value of θ that minimizes the probability limit of
the 2SLS objective function. We explore this further in our Monte Carlo study in section 7.
   13                               jt
      E.g. only the nX variables Kmm    if Σ is restricted to be diagonal, or even a subset if some
coefficients are non-random.

                                                 17
  3. Computing the optimal instruments does not require any first-step estimate
     because the estimating equation is linear. We can just use a flexible set of
     functions of the columns of Z that span the space of the optimal instruments
     EpX|Zq and EpK|Zq.

  4. Even if the econometrician decides to go for a different estimation method,
     our proposed 2SLS estimates obtained should provide a set of very good initial
     parameter values for a nonlinear optimization algorithm.

  5. The confidence regions on the estimates will give useful diagnoses about the
     strength of identification of the parameters, both mean coefficients β and their
     random variation Σ. This would be very hard to obtain otherwise, except by
     trying different specifications.

  6. There has been much interest in systematic specification searches in recent
     years; see e.g. Horowitz–Nesheim 2018 for a Lasso-based selection approach in
     discrete choice models. With our method any number of variants can be tried
     in seconds, and model selection is drastically simplified.


5.1    The Quality of the Approximation
Ideally, we would be able to bound the approximation error in the expansion of ξj ,
and use this bound to majorize the error in our estimator in the manner described in
Kristensen and Salanié (2017). While we have not gone that far, we can justify the
local-to-zero validity of the expansion in the usual way. We are taking a mapping

                                    S “ G pξ, X, σq

that is differentiable in both ξ and σ; inverting it to ξ “ Ξ pS, X, σq; and taking
an expansion to the right of σ “ 0 for fixed market shares S and covariates X. The
validity of the expansion for small σ and fixed pX, Sq depends on the invertibility of
the Jacobian Gξ .
    First consider the standard model. It follows from Berry 1994 that Gξ is invertible
if no observed market share hits zero or one. Applying the Implicit Function Theorem
repeatedly shows that in fact the Taylor series of ξ converges over some interval r0, σ̄s


                                           18
if all moments of  are finite; and that the expansion is valid at order L if the moments
of  are bounded to order pL ` 1q.
    Characterizing this range of validity is trickier. Figure 1 uses formulæ derived
in Appendix B to plot the first four coefficients of the expansion in Σ11 X12 for the
standard Gaussian binary model (that is, the Gaussian mixed logit) with one covariate
X1 :
                                        4
                          S1           ÿ          `       ˘k
                 ξ1 “ log    ´ βX1 `      tk pS1 q Σ11 X12 ` Opσ 10 q.
                          S0          k“1

Each curve plots the function tk as market shares vary between zero and one. The
visual impression is clear: the coefficients damp quickly. Beyond the first term (which
corresponds to our 2SLS method), the coefficients are always smaller than 0.05 in
absolute value. Of course, the approximation error also depends on the values taken
by the covariates.


  0.15


  0.10

                                                                               t1(S1 )
  0.05
                                                                               t2(S1 )

                 0.2         0.4         0.6           0.8         1.0
                                                                         S1    t3(S1 )
                                                                               t4(S1 )
 -0.05


 -0.10


 -0.15


                           Figure 1: Coefficients t1,2,3,4 pS1 q

    While this simple example can only be illustrative, we find the figure encouraging
as to the practical range of validity of the approximation. We have not determined
the extent to which these results generalize to the multinomial logit model.




                                            19
5.2       “Robustness”
Our expansions only rely on the properties of the derivatives of the logistic cdf Lptq “
    1
1`expp´tq
          and on the first two moments of . This has a distinct advantage over
competing methods: the lower-order moments of  can be estimated by 2SLS, and
nothing more needs to be known about its distribution.
    Suppose for instance that the analyst does not want to assume that  has a
symmetric distribution. Then the coefficients a1j are still zero, and the coefficients
a2j are unchanged. In the absence of symmetry, the approximate model is only valid
up to Opσ 3 q; but running Algorithm 1 may still provide very useful estimators of the
elements of Σ0 .



6       Extensions
Our technique can be extended to other random coefficient models as long as the
conditional indirect utility remains additive in the product-specific random effects ξ.
This is shown in section 6.1. We follow with a modification of our multinomial logit
random coefficients modeling framework to account for the third and fourth moments
of . We then turn to methods for improving the quality of our 2SLS estimates14 .
Finally, section 6.4 presents a nonlinear 2SLS estimation procedure for the nested
logit model.


6.1       Quasi-linear Random Coefficients Models
Consider the following class of models, whose defining characteristic is that the error
term η and the mean coefficients β only enter via a linear combination η ´ f1 pyqβ:

                    Gpy, η, β, σq ” G˚ py, Ev A˚ py, η ´ f1 pyqβ, σBvqq .                        (11)

where v is unobserved heterogeneity distributed independently of y and η and nor-
malized by Ev “ 0 and V v “ I; and both functions G˚ and A˚ are assumed to be
known.
 14
      We explore the small sample properties of several of these corrections in our Monte Carlo study.



                                                  20
   Note that the macro–BLP model takes this form, with y “ pS, Xq; f1 pyq “ ´X;
η “ ξ; and
                    ˆ                                            ˙
              ˚
            Aj “ Pr j “ arg max pXk β ` ξk ` σXk Bvq |X, ξ, v
                                   J“0,1,...,J


so that, denoting aj ” Xj and bj “ Xj β ` ξj ,

                                               exp pbj ` aj cq
                          A˚ pa, b, cq ”      řJ                   ;
                                           1 ` k“1 exp pbk ` ak cq

and G˚j ” Sj ´ Ev A˚j .
    We continue to assume that E pη|Zq “ 0. The quasi-linear structure in (11)
allows this class of models to be approximately estimated by 2SLS. Denoting partial
derivatives with subscripts, we have:

Theorem 3 (Expansions for quasi-linear random coefficients models). Consider a
model of the class defined by (11) and assume that

   • G˚ is twice differentiable with respect to its second argument

   • A˚ is twice differentiable with respect to its last two arguments

   • the matrices G˚2 py, A˚ py, η´f1 pyqβ, 0qq and A˚2 py, η´f1 pyqβ, 0q are invertible
     for all py, η, βq.

Any such model satisfies the conditions C1–C3 in the introduction. Moreover,

   • f1 pyq appears directly in (11)

   • the variables f0 pyq are defined by the system of equations

                                    G˚ py, A˚ py, f0 pyq, 0qq “ 0

   • and the variables f2 pyq solve the linear system

                          A˚33 py, f0 pyq, 0qq f2 pyq “ ´A˚2 py, f0 pyq, 0qq.




                                                 21
   Proof: See Appendix E.

    As explained in the introduction, these models can be estimated by regressing
f0 pyq on f1 pyq and f2 pyq with a set of flexible functions of Z as instruments. As
the macro–BLP model belongs to this class, this confirms that conditions C1–C3
hold in the BLP model; we had shown it implicitly in section 3 by deriving the
expansions. Note also that we did not use any distributional assumption on the
random coefficients and the idiosyncratic shocks—although of course the terms in
the expansions do depend on these distributions. We give an illustration for a one-
covariate mixed binary model without any distributional assumption in Appendix B.3.


6.1.1   Examples

It is easy to generate models in the quasi-linear class (11). Starting from any Genes-
ralized Linear Model gpyq “ Xβ ` η, we can for instance transform the right-hand
side by adding additive unobserved heterogeneity and another link function:

                              gpyq “ Eε h pXβ ` η, σεq .

When the link functions g and h are both assumed to be known, all such models obey
conditions C1–C3 and can therefore be studied with our method. Note that in these
models f1 pyq ” ´X and f2 pyq “ ´ph1 {h22 qpf0 pyq, 0q where

                                f0 pyq “ hp¨, 0q´1 pgpyqq

(assuming the inverse is well-defined.)
   The nested logit of section 6.4 shows that our method remains useful beyond
the class of quasi-linear models, at the cost of breaking conditions C2 and C3 and
requiring (simple) numerical optimization.


6.2     Higher-order terms
In Appendix B, we study in more detail the standard binary model. For this simpler
case, calculations are easily done by hand for lower orders of approximation, or using
symbolic software for higher orders.

                                           22
    More generally, return to the standard model and assume (as is often done in
practice) that the m are independent across the covariates m “ 1, . . . , nX . We
denote as before Σmm “ Ep2m q, and µlm the expected value of lm for l ě 3. Tedious
calculations15 show that the second- to fourth-order terms of the expansion in σ are
                                                     4
                                        Sj          ÿ
                               ξj “ log    ´ Xj β `     Alj ` Opσ 5 q
                                        S0          l“2

with
                                      ÿ
                             A2j “        Xjm peS Xm ´ Xjm {2q Σmm ;
                                      m
                               ˆ                   2    2              ˙
                     ÿ                 eS Xm eS pXm  q Xjm           2
             A3j “       Xjm       Xjm      `         ´    ´ peS Xm q µ3m ;
                     m
                                          2      2      6
and
                             ˆ                                                  3
                ÿ
                                          3                 2      peS Xm q2 Xjm
        A4j “     µ4m Xjm peS Xm q ´          peS Xm qpeS pXm
                                                           ´ Xjm
                                                              qq            ´
              m
                                                                       2       24
                                3               2
                                                                 ˙
                          eS pXm  q       eS pXm  q    2 e S Xm
                        `           ` Xjm           ` Xjm
                              6               4              6
                2                   ˆ                     ˆ                  ˙˙
              A2j    ÿ                                       Xjm
            `      ` Σmm Xjm eS pA2 Xm q ` peS A2 q               ´ 2peS Xm q   .
               2      m
                                                               2

First consider the third-order term A3j . It is a linear function of the unknown skew-
nesses µ3m ; in fact it can be rewritten as
                                         ÿ
                                       ´ Tmj µ3m
                                                m

where we introduced new artificial regressors
                       ˆ 2                                  2
                                                                ˙
               j
                        Xjm               2     eS Xm eS pXm  q
             Tm ” Xjm          ` peS Xm q ´ Xjm      ´            .
                          6                        2      2

Algorithm 1 can be adapted in the obvious way to take possible skewness of  into
account. Note that the procedure remains linear in the parameters pβ, Σ, µ3 q, for
which it generates approximate estimates by 2SLS.
 15
      Available from the authors.



                                                    23
    The fourth-order term, on the other hand, contains terms that are linear in the
µ4m (the first two lines of the formula) as well as terms that are quadratic in Σ (the
last line). The first group suggests introducing more artificial regressors
                     ˆ
           j                         2                      peS Xm q2
         Qm ” Xjm peS Xm qpeS pXm      qq ´ peS Xm q3 ` Xjm               3
                                                                      ` Xjm {24
                                                                2
                     3               2
                                                      ˙
               eS pXm  q       eS pXm  q     2 e S Xm
             ´           ´ Xjm           ´ Xjm          ,
                   6               4               6
whose coefficients are the µ4m . The second group yields
                                    ÿ
                                                  j
                                 ´      Σmm Σnn Wmn
                                  m,n“1

where new artificial regressors W are assigned products of the elements of Σ. Esti-
mating the resulting regression requires nonlinear optimization (albeit a very simple
one).


6.3     Correcting the 2SLS estimates
If the analyst is willing to make more distributional assumptions, she can resort to
bootstrap or asymptotic corrections to improve the accuracy of our 2SLS estimators.


6.3.1   Bootstrapping

Once we have approximate estimators β̂ and Σ̂, we can use them to solve the market
shares equations for estimates of the product effects ξ and bootstrap them, provided
that we are willing to impose a distribution for v (beyond the normalization of its
first two moments.)
   We use Berry inversion to solve for ξ̂t in the system
                                   ´       ´           ¯   ¯
                               exp Xjt β̂ ` Σ̂1{2 v ` ξˆjt
                Sjt “ Ev                 ´     ´         ¯      ¯,
                          1 ` ΣJk“1 exp Xkt β̂ ` Σ̂1{2 v ` ξˆkt
where Ev p¨q denotes the expectation taken with respect to the assumed distribution
of v. For any resample ξ ˚ of the ξ̂, we simulate the market shares from
                                     ´     ´          ¯      ¯
                                exp Xjt β̂ ` Σ̂1{2 v ` ξjt ˚
                   ˚
                  Sjt “ Ev               ´     ´          ¯      ¯
                           1 ` ΣJk“1 exp Xkt β̂ ` Σ̂1{2 v ` ξkt˚



                                          24
and we use our 2SLS method to get new estimates β ˚ , Σ˚ . Finally, we compute
bias-corrected estimates by e.g.
                                                           B
                                                        1 ÿ ˚
                                        β C “ 2β̂ ´          β .
                                                        B b“1 b
More generally, the resampled estimates can be used to estimate the distribution of
β̂ and Σ̂ in the usual manner.


6.3.2      Asymptotic Correction

Another way to use the third- and fourth-order terms in our expansion is as a cor-
rective term: that is, we run 2SLS on the second-order expansion and we use the
formulæ for the higher-order terms to correct for the effects of the approximation.
    Denote θ “ pΣ, βq, and θ0 its true value. Let θ̂2 be our 2SLS estimator based on
a second-order expansion. That is, we estimate the approximate model Epξ2 Zq “ 0
with instruments Z and weighting matrix W , where
                                              Sj
                                  ξ2j “ log      ´ Xj β ´ Tr ΣK j .                                (12)
                                              S0
As the number of markets T gets large, θ̂2 converges to the solution θ2 of Ef2 pθ2 q “ 0,
with
                               Bξ2
                      f2 pθq ”     pθ, X, Sq1 ZW Z 1 ξ2 pθ, X, Sq.
                               Bθ
Alternatively, we could have estimated the model using inversion or MPEC, with an
“exact” ξ8 . Let λ0 denote additional parameters of the model (such as higher-order
moments of the distribution of ) that are identified using the exact ξ8 but not16
with our approximate ξ2 .
      Since by assumption E pξ8 pθ0 , λ0 , X, SqZq “ 0, a fortiori Ef8 pθ0 ; λ0 q “ 0 with
                             Bξ8
                    f8 pθ; λ0 q ” pθ, λ0 , X, Sq1 ZW Z 1 ξ8 pθ, λ0 , X, Sq.
                              Bθ
The dominant term in the asymptotic bias is given by expanding Ef8 pθ; λ0 q around
θ “ θ2 , keeping λ0 fixed. It is
                                 ˆ                   ˙´1
                                     Bf8
                      θ2 ´ θ0 » E          pθ2 ; λ0 q    Ef8 pθ2 ; λ0 q.
                                      Bθ
 16
      If the only free parameters of the distribution of  are the elements of Σ, then λ0 will be empty.

                                                   25
    Denote X the matrix with terms Xjm and K the matrix whose row j “ 1, . . . , J
                                    j
contains the artificial regressors Kmn . We define e2 pθ; λ0 q “ ξ8 pθ; λ0 q ´ ξ2 pθq, the
approximation error on ξ. Under any assumption about the parameters in λ0 , we
can compute the higher-order terms ξ3 , ξ4 , . . . to approximate e2 . If for instance we
maintain the assumption that the model is symmetric, we can approximate e2 »
ξ4 ´ ξ2 . Results in Robinson (1988) and in Kristensen and Salanié (2017) could be
adapted to show that the resulting corrected estimator not only has a smaller bias;
it is in fact asymptotically equivalent to the estimator θ̂4 based on a fourth-order
expansion.
   Let us suppose then that we have a reliable estimator ê2 pθ; λ0 q of e2 pθ; λ0 q. Define
V by the Cholesky decomposition ZW Z 1 “ V V 1 , so that V is a pJ ˆ Jq matrix. We
prove in Appendix D that this asymptotic correction yields the following formula:
           ˜                                 ¸´1 ˜                                        ¸
             E pX 1 V V 1 Xq E pX 1 V V 1 Kq                 E pX 1 V V 1 ê2 q
θ0 » θ2 `                                                                 `              ˘ .
            E pK 1 V V 1 Xq E pK 1 V V 1 Kq        E pK 1 V V 1 ê2 q ´ E Bê
                                                                            BΣ
                                                                              2
                                                                                V V 1 ξ2

To interpret this formula, note that if ê2 did not depend on Σ the corrective term
on the right-hand-side would simply be the 2SLS estimate of the regression of ê2
on pX, Kq with instruments V . In fact, if we are only interested in correcting the
estimators of the mean coefficients β2 , we can simply keep the corresponding part
of the 2SLS estimate. The correction on Σ2 has an additional term as higher order
terms in the expansion of ξ typically depend on Σ. (Recall from Theorem 1.(i) that
they do not depend on β.)


6.3.3   Two-Step Estimator Based on the Asymptotic Correction

In practice, we use the estimate of ξ8 pθ̂2 q computed using our 2SLS estimator and
the Berry inversion as well as the residual from our 2SLS estimate
                                             Sj
                          ξ2j pθ̂2 q “ log      ´ Xj β̂2 ´ Tr Σ̂2 K j
                                             S0
to correct our initial 2SLS estimate. The difference between ξ2j pθ0 q and ξ8 pθ0 q is
equal to the higher-order terms in equation (5). Computing a new dependent variable
                                  „ 
                                    Sj
                         yj “ log      ´ pξ2j pθ0 q ´ ξ8 pθ0 qq
                                    S0

                                                26
and applying 2SLS would yield a consistent estimate of θ0 . Using our 2SLS estimate
θ̂2 and computing               „ 
                        ˚        Sj
                      yj “ log        ´ pξ2j pθ̂2 q ´ ξ8 pθ̂2 qq
                                 S0
and applying 2SLS with this dependent variable is a feasible version of this correc-
tion17 . In our Monte Carlo study we will explore the small sample properties of this
procedure.


6.4       The Two-level Nested Logit
Campioni (2018) applies a nonparametric approach to the choice among a very large
set of products. He shows that the mixed logit specification forces the price elasticity
to become “too small” at high price levels. This raises the question of the appropriate
choice of a distribution for the idiosyncratic terms uijt .
    For the mixed logit (J “ 1), it is very easy to compute the artificial regressors
for any distribution of the idiosyncratic terms; we give the formulæ in Appendix B.3.
When J ą 1, the space of possible distributions increases dramatically. The compu-
tations also become more complicated. Finally, estimating the additional parameters
of the distribution of u requires (simple) nonlinear optimization.
    For illustrative purposes, we give the estimating equation for the two-level nested
logit model. Assume that there is a nest for good 0, and K nests N1 , . . . , NK for the
varieties of the good. For k “ 1, . . . , K, we denote λk the corresponding distribution
parameter—with the usual interpretation that p1 ´ λk q proxies for the correlation
between choices within nest k, and that the multinomial logit model obtains when all
λk “ 1.
                                                              ř
    We denote the market share of nest k by SNk “ jPNk Sj . Take any variable
T “ pT0 , T1 , . . . , TJ q. We define the within-nest-k share-weighted average as
                                                  ÿ Sj
                                         T̄k “            Tj .
                                                 jPN
                                                     S Nk
                                                     k

                                      řK
Note in particular that eS T “          k“1   SNk T̄k .
 17
      Note that this procedure could be iterated.



                                                    27
   Appendix C derives the equivalent of (6): for j P Nk ,
                        ˆ                          ˙
                                 xj      1 ´ λk         1 ´ λk
             a2j “ xj ¨ 2eS x ´      `2         x̄k ´          kx̄k k2 .
                                 λk        λk             λk
Reintroducing the market index t, the corresponding artificial regressors are
             ˆ                      ˙                       ˆ                 ˙
       jt      Xjt,m 1 ´ S0t λk       Xjt,m 1 ´ λk                     2Xjt,m
     Kmm “          ´           etm         `        X̄kt,m X̄kt,m ´
                 2      1 ´ S0t         λk      λk                       λk
and for any off-diagonal term n ă m,
                                                                           ˆ                                                  ˙
 jt                 1 ´ S0t λk etm Xjt,n ` etn Xjt,m 1 ´ λk                                     X̄kt,m Xjt,n ` X̄kt,n Xjt,m
Kmn “ Xjt,m Xjt,n ´                                 `2                          X̄kt,m X̄kt,n ´
                     1 ´ S0t             λk            λk                                                    λk

where as in section 3, etm “ Jj“1 Sjt Xjtm .
                              ř

   If the λk parameters are known, then our procedure becomes:

Algorithm 2. FRAC estimation of the two-level nested logit BLP model

  1. on every market t, augment the market shares from ps1t , . . . , sJt q to pS0t , S1t , . . . , SJt q

  2. for every product-market pair pj P J , tq :
                                                                                         řJ
      (a) compute the market-share weighted covariate vector et “                           l“1   Slt Xlt and
          the within-nest weighted average covariate vector
                                                      ÿ          Slt
                                        X̄kpjq,t “                        Xlt
                                                     lPNkpjq
                                                               SNkpjq,t

           where kpjq is the nest that variety j belongs to.
      (b) for every pm, nq in I, compute the “artificial regressor”
                  ˆ                         ˙                            ˆ                    ˙
            jt      Xjt,m 1 ´ S0t λkpjq       Xjt,m 1 ´ λkpjq                          2Xjt,m
          Kmm “           ´             etm         `          X̄kpjq,t,m X̄kpjq,t,m ´
                      2        1 ´ S0t        λkpjq     λkpjq                           λkpjq

           and for any off-diagonal term n ă m,

               jt                 1 ´ S0t λkpjq etm Xjt,n ` etn Xjt,m
              Kmn “ Xjt,m Xjt,n ´
                                     1 ´ S0t               λkpjq
                               ˆ                                                             ˙
                     1 ´ λkpjq                           X̄kpjq,t,m Xjt,n ` X̄kpjq,t,n Xjt,m
                  `2             X̄kpjq,t,m X̄kpjq,t,n ´                                       .
                       λkpjq                                            λkpjq

                                                28
         (c) define
                                                 SNkpjq ,t              Sjt
                                     yjt “ log             ` λkpjq log
                                                  S0t                  SNkpjq,t

    3. run a two-stage least squares regression of y on X and K, taking as instruments
       a flexible set of functions of Z

    4. (optional) run a three-stage least squares (3SLS) regression across the T markets
       stacking the J equations for each product with a weighting matrix equal to the
       inverse of the sample variance of the residuals from step 43.

   If the parameters λ are not known, then things are slightly more complicated:
the formulæ cannot be made linear in λ, and there are no corresponding artificial
regressors. Estimation of pβ, Σ, λq requires numerical minimization over the λ.
    More general distributions in the GEV family could also be accommodated. As
the nested logit example illustrates, there is a cost to it: the approximate model
becomes nonlinear in some parameters18 . Note however that if there is reason to
believe that the true distribution is close to the multinomial logit (say λ » 1 in the
example above), then one can take expansions in the same way we did for the random
coefficients and use a 2SLS estimate again.



7       Monte Carlo Analysis of the Small-Sample Per-
        formance of our Estimator
This section presents the results of a Monte Carlo study of an aggregate discrete choice
demand system with random coefficients. It compares the finite sample performance
of our estimator of the parameters to estimators computed using the mathematical
programming with equilibrium constraints (MPEC) approach recommended by Dubé,
Fox and Su (2012). We also show results demonstrating some of the “robustness”
of our estimation procedure to assumptions about the distribution of the random
coefficients. Specifically, we find that even if the distribution of random coefficients is
misspecified, our procedure still yields very good estimates of the means and variances
of the random coefficients.
 18
      Technically, condition C1 in the introduction still holds, but conditions C2 and C3 do not.

                                                  29
    The basic set-up of our Monte Carlo study follows that in Dubé, Fox and Su
(2012). It is a standard static aggregate discrete choice random coefficients demand
system with T “ 50 markets and J “ 25 products in each market, and K “ 3
observed product characteristics. Following Dubé, Fox, and Su (2012), let Mt denote
the mass of consumers in market t “ 1, 2, . . . , T . Each product is characterized by the
             1
vector pXjt    , ξjt , pjt q1 , where Xjt is a K ˆ 1 vector of observable attributes of product
j “ 1, 2, . . . , J in market t, ξjt is the vertical product characteristic of product j
in market t that is observed by producers and consumers, but unobserved by the
econometrician, and pjt is the price of product j in market t. Collect these variables
                                                                                   1            1 1
for each product into the following market-specific variables: Xt “ pX1t             , . . . , XJt q,
                               1                                     1
ξt “ pξ1t , ξ2t , . . . , ξJt q , and pt “ pp1t , p2t , . . . , pJt q .
       The conditional indirect utility of consumer i in market t from purchasing product
j is
                                            1
                               uijt “ β0 ` Xjt βix ´ βip pjt ` ξjt ` ijt
The utility of the j “ 0 good, the “outside” good, is equal to u0jt “ i0t . Each
element of βix “ pβi1 x             x 1
                         , . . . , βiK q is assumed to be drawn independently from N pβ¯kx , σk2 q
distributions, and each βip is assumed to be drawn independently from N pβ¯p , σp2 q. We
denote βi “ pβix 1 , βip q1 .
       We collect all parameters into

                             θ “ pβ0 , β¯1x , . . . , β¯K
                                                        x ¯
                                                          , βp , σ12 , . . . , σK
                                                                                2
                                                                                  , σp2 q1 .

Our simulations all have mean coefficients

                           pβ0 , β¯1x , β¯2x , β¯3x , β¯p q “ p´1, 1.5, 1.5, 0.5, ´1q.

and varying variances pσ12 , σ22 , σ32 , σp2 q. We also experiment with varying β̄p .
   To compute the market shares for the J products, we assume that the ijt are
independently and identically distributed Type I extreme value random variables, so
that the probability that consumer i with random preferences βi purchases good j in
market t is equal to

                                                exppβ 0 ` Xjt
                                                            1
                                                              βix ´ βip pjt ` ξjt q
               sijt pXt , pt , ξt |βi q “
                                            1 ` Jk“1 exppβ0 ` Xkt   βix ´ βip pkt ` ξkt q
                                               ř                  1




                                                          30
We compute the observed market shares for all goods in market t by drawing ns “
1, 000 draws pζikt q from four N p0, 1q random variables and constructing 1, 000 draws
from βi |θ as follows:
                           x
                          βikt “ β¯kx ` σk ζikt and βitp “ β¯p ` σp ζipt .

We then use these draws to compute the observed market share of good j in market
t as:                                           ns
                                            1 ÿ
                    sjt pXt , pt , ξt |θq “        sijt pXt , pt , ξt |βi q
                                            ns i“1
given the vectors Xt , pt , and ξt for each market t.
   Consistent with the experimental design in Dubé, Fox and Su (2012), we generate
the values of Xt , pt , ξt and a vector of 6 instruments Zjt as follows. First we draw
Xt for all markets t “ 1, 2, . . . , T from a multivariate normal distribution:
                        » fi           ¨» fi »                  fi˛
                          x1j             0      1     ´0.8 0.3
                        –x2j fl „ N ˝–0fl , –´0.8        1   0.3fl‚
                        — ffi          ˚— ffi —                 ffi‹

                          x3j             0     0.3     0.3   1
The price of good j in market t is equal to

                          pjt “ |0.5ξjt ` ejt ` 1.1px1j ` x2j ` x3j q|,

where ejt „ N p0, 1q, distributed independently across products and markets. The
ξjt are N p0, σξ2 q random variables drawn independently across products and markets
for different values of σξ2 described below. The data generating process for the vector
of instruments is:

                      zjtd „ U p0, 1q ` 0.25pejt ` 1.1px1j ` x2j ` x3j qq

where d “ 1, . . . , 6.
  For a specified value of the parameter vector θ, following this process for T “ 50
markets yields the dataset for one Monte Carlo draw.


7.1     MPEC Approach
The MPEC approach solves a nonlinear minimization problem subject to nonlin-
ear equilibrium constraints. The first step of the estimation process constructs the

                                                31
following instrumental variables for all the products in all the markets. There are
42 instruments in total; they are constructed from product characteristics xj and
excluded instruments zjt :
                                                                                               6
                                                                                               ź
             1, xkj , x2kj , x3kj , x1j x2j x3j , zjtd , zjtd
                                                          2      3
                                                              , zjtd , zjtd x1j , zjtd x2j ,         zjtd
                                                                                               d“1

Let W denote this pJ ˆ T q ˆ 42 matrix of instruments. In our case J ˆ T “ 1, 250
since J “ 25 and T “ 50.
   The MPEC approach solves for θ by minimizing

                                           η 1 W pW 1 W q´1 W 1 η

subject to the “equilibrium constraints”

                                                 spη, θq “ S

where S is the vector of observed market shares computed as described above given
the values of xt , pt and ξt and η is a pJ ˆ T q ˆ 1 vector defined by the following
equation:
                       N
                   1 ÿs                     x
                              exppθ1 ` x1j β1i        x
                                               ` x2j β2i        x
                                                         ` x3j β3i ` pjt βip ` ηjt q
    sjt pη, θq “
                   Ns i“1 1 ` Jk“1 exppθ1 ` x1k β1i                     ` pkt βip ` ηkt q
                             ř                   x         x          x
                                                    ` x2k β2i ` x3k β3i

where each pβix , βip q is a random draw from the following normal distribution:
                                    ¨» fi »             fi˛
                                      θ2       θ6 0 0 0
                                    ˚—θ ffi — 0 θ 0 0 ffi‹
                                    ˚— 3 ffi —     7
                                  N ˚— ffi , —
                                                        ffi‹
                                                        ffi‹
                                    ˝–θ4 fl – 0 0 θ8 0 fl‚
                                      θ5       0 0 0 θ9

Note that θ1 (like β0 ) is not allowed to be random. For purposes of estimation, we
set Ns “ 1, 000. For each Monte Carlo simulation, we start the optimization with
the following initial point: true values for θ, and a vector of zeros for the η vector.
Clearly, these starting values are not feasible for empirical researchers; we use them
to maximize the chances that the MPEC estimation will converge to a solution.



                                                        32
7.2     Our 2SLS Approach
Our 2SLS approach resorts to a slight modification of the standard linear 2SLS esti-
mator to account for the fact that the estimates of the σk2 and σp2 cannot be negative.
First, we construct the instrumental variables as the MPEC approach. We then con-
struct the artificial regressors K1 , K2 , K3 , Kp of Theorem 2 for each product in each
market by applying
                                            J
                                            ÿ
                                   X̄it “         xik Skt
                                            k“1

                                  Kijt   “ xij pxij {2 ´ X̄it q

for i “ 1, 2, 3, p.
                                                                                      Sjt
    The next step performs an instrumental variable regression of yjt “ logp S0t          q on
1, x1 , x2 , x3 , x4 , K1 , K2 , K3 , Kp using all 42 instruments. If any coefficient for the
last four variables is negative, we set that coefficient to 0 and rerun the regression
without that variable. We iterate this process until all the coefficients are positive,
or all four variables are excluded from the instrumental variables regression.
   In addition to this standard 2SLS estimator, we compute a corrected estimator as
explained in section 6.3.3. To evaluate it, we replace yjt , the dependent variables for
2SLS estimates, with yjt ´ pξ2,jt ´ ξ8,jt q, where

    • ξ2,jt is the residual from our initial 2SLS estimation procedure

    • ξ8,jt is the value of ξjt that results from solving the equation st pξt , θ̂q “ St ,
      where θ̂ is the initial 2SLS estimate of θ.

We found that it worked as well as and often better than the bootstrap, at a con-
siderably lower computational cost. We also experimented with using the opti-
mal instruments, obtained by a kernel regression of X and of K on the variables
x1 , x2 , x3 , z1 , . . . , z6 .


7.3     Pseudo True Values for the 2SLS Approach
As explained earlier, the 2SLS estimator is not consistent for the true parameter
values, as it estimates an approximate model. We constructed estimates of the pseudo

                                                  33
true values to which our 2SLS estimators converge by simulating their probability
limit. A first approach increases the number of markets and computes our 2SLS
estimates for this large number of markets. The second approach computes estimates
of the population values of the moments of our 2SLS estimator.


7.3.1   Increasing-number-of-markets Approach

For each simulation, we keep the size and distribution of product characteristics for
each market fixed, but increase the number of markets. For each scenario, we cal-
culate the pseudo true value (and its standard error) by 20 simulations of 100,000
markets. Note that across different simulations, we generate different product char-
acteristics. Also, when calculating market shares, we use different random draws of
βi across different simulations, but the same random draws of βi within a simulation.
Estimates are calculated by the sample mean of the 20 simulations. Standard errors
are calculated by the sample standard errors of the 20 simulations.


7.3.2   Moment-based Approach

We can also calculate the pseudo true values in a different way. We first run the first
stage projection: Π̂ “ pW 1 W q´1 W 1 X for each simulation, where W is our matrix of
instruments and X is our matrix of regressors. We then take the average across all
the simulations to get our estimate of the population value of Π. Then in the second
stage, we calculate pW Πq1 X and pW Πq1 Y for each simulation, and then take averages
across all the simulations to get two matrices A and B. The final estimate is then
A´1 B. In short, we have

                          Π “ Eall simulations rpW 1 W q´1 W 1 Xs
                             A “ Eall simulations rpW Πq1 Xs
                             B “ Eall simulations rpW Πq1 Y s
                                   Estimate “ A´1 B

With this method, we only have the estimates but cannot get the standard errors.
We used 1000 simulations of 10,000 markets.



                                            34
7.4     Monte Carlo Simulation Results
We used the SNOPT optimization package available from the Stanford Systems Op-
timization Laboratory to solve the nonlinear optimization problems for the MPEC
estimator. The software employs a sparse sequential quadratic programming (SQP)
algorithm with limited-memory quasi-Newton approximations to the Hessian of the
Lagrangian.
    We run simulations for 9 scenarios obtained by setting three values for the variance
of the product random effects: σξ2 “ Varpξq “ 0.1, 0.5, 1 and three values for the vector
                                             x
of variances of the coefficients βi “ pβ0 , β1i    x
                                                , β2i    x
                                                      , β3i , βip q1 :

        Varpβi q “ p0, 0.1, 0.1, 0.1, 0.05q, p0, 0.2, 0.2, 0.2, 0.1q, p0, 0.5, 0.5, 0.5, 0.2q.

Note that the square roots of the elements of Varpβj q represent the relative values of
the scale parameter σ of models 1, 2, and 5.
    It is worth noting here that we explored other scenarii in which MPEC often failed
to converge, even though we are starting it from the true values of the parameters. In
particular, larger variances of ξ are problematic. It is also the reason why we reduced
the highest value of σp2 from 0.25 to 0.2.
   All the other parameter specifications are as described above.


7.4.1    Distribution of the Estimates

We summarize the estimation results in Tables 1 to 9, where density plots are grouped
by parameter for all scenarii. These plots suggest that if the researcher is interested
in a precise estimate of the mean of the random coefficients, then using our 2SLS
approach does not imply any significant bias or loss in efficiency relative to the MPEC
approach.
   The MPEC approach appears to dominate the 2SLS approach for the variance of
the random coefficients. The 2SLS estimators of the variances have a downward bias
that increases with the variance of the random coefficients. However, larger values of
the variance of ξjt do seem to improve the performance of the 2SLS estimator of the
variance of the random coefficients.
   We also implemented the Petrin and Train (2010) control function approach. To

                                                 35
do this, we first run a linear regression of the price pjt on all 42 instruments. We
denote the residuals from this regression by ε̂jt and we include them as an additional
covariate in the mean utility of product j. The middle panel of Tables 1 to 9 presents
the distribution of estimated parameters for the case that Varpξq “ 0.5. The middle
panels of Tables 8 and 9 shows the results of this experiment for the mean and variance
of the price coefficients in the central case Varpξq “ 0.5. The control function approach
exhibits substantial bias in the estimates of both means and variances of the random
price coefficients19 .


7.4.2    Starting Values

We are giving a big advantage to MPEC in our comparisons, since we allow the
algorithm to start from the true values of the parameters. This is of course infeasible
in practice. With this initial boost, MPEC converges 100% of the time, after 1,030
iterations on average; the minimization takes 110 seconds on average. Our 2SLS
approach provides a more realistic alternative, in which we start MPEC from the
results of our 2SLS regression. This appears to work very well: MPEC converges
after an average 125 seconds and 1,280 iterations, again with a 100% success rate.
The resulting estimates are very close to those obtained when staring from the true
values: the difference is between 10´6 and 10´7 .
    These results are very encouraging for the use of our approach to find very good
starting values for the MPEC and nested-fixed point estimation procedures. Given
that 2SLS takes no time at all, we would strongly recommend running it before a
more sophisticated algorithm.


7.4.3    Pseudo-true Values

Tables 10 and 11 demonstrate that for most scenarii and coefficients, the pseudo true
values implied by our 2SLS procedure are not substantially different from the true
values. Based on these results, it is difficult to argue that a researcher would draw
conclusions from 2SLS estimates that differ in an economically or even statistically
meaningful way from those obtained with MPEC estimates.
  19
    Other experiments (not reported here) show that this bias increases with the variance of ξjt , as
one would expect since the control function is inexact.


                                                 36
7.4.4    Price Elasticities

Based on the parameter estimates, we can estimate the own price elasticity of the
demand for each product. The graphs in table 12 plot the distribution of the difference
between the true price elasticity and the estimated price elasticity for the MPEC
approach, our standard 2SLS approach, and our corrected 2SLS estimator. For space
reasons, we only presents the results for five products: numbers 5, 10, 15, 20, 25.
Our simulations have variances pσ12 , σ22 , σ32 , σp2 q “ p0, 0.4, 0.4, 0.4, 0.2q with Varpξq “ 1.
Table 12 demonstrates that our procedure recovers nearly identical mean own-price
elasticities for products as the MPEC approach, although the spread for our estimates
is slightly larger than in the MPEC approach.
    We also performed a set of simulations (with the same variances) to determine
if changing the true value of the price coefficient β¯p changes the performance of the
estimators. The results in tables 13 and 14 reinforce our previous conclusions about
the 2SLS approach. For a range of values of the mean value of the price coefficient,
our approach introduces minimal bias in the estimates of the means of the random
coefficients. The estimates of the variances of the random coefficients for our 2SLS
estimate continue to be downwards biased in general, but the bias is smaller for larger
price coefficients.


7.4.5    Variable Selection Tests

Researchers in empirical IO have little guidance on the list of characteristics X they
should include, or how to specify the matrix Σ. Experimenting with different speci-
fications is costly with the usual estimators. Our 2SLS approach, on the other hand,
makes variable selection very easy. We can decide whether a characteristic simply
by testing whether the corresponding covariate can be dropped from the estimat-
ing equation; and to decide whether we should allow for a random coefficient, we
only need to test whether the associated artificial regressor can be dropped from the
equation. We experimented with this approach to detecting random coefficients by
setting β̄1x “ σ1x “ 0 in the data generating process and applying standard tests that
that the covariate x1 and/or the artificial regressor K1 has a zero coefficient in the
2SLS regression. We also performed this test using our corrected 2SLS estimates.
Tables 15 to 20 give the probability that the null hypothesis is not rejected, where

                                               37
the null hypothesis is

   • β̄1x “ 0 (Tables 15 and 16)

   • σ1x “ 0 (Tables 17 and 18)

   • β̄1x “ σ1x “ 0 (Tables 19 and 20).

The row labelled “2SLS with heteroskedasticity-robust standard error” is our 2SLS
estimate, using a standard heteroskedasticity-robust covariance matrix to compute
standard errors. The row labelled “GLS estimator and standard errors” uses Cragg’s
(1983) generalized least squares estimator and his recommended standard error es-
timates. The row labelled “2SLS with clustered standard errors” uses our 2SLS
estimates with standard errors clustered at market level.
    Since the null hypothesis is true, each row in Tables 15-20 would ideally contain
0.99, 0.95, and 0.90. Clearly, our test rejects the null too often. In this particu-
lar application, this is probably better than the alternative: better to include more
variables and lose some efficiency than to incur bias by leaving them out. The size
distortion is smaller for tests on the means (Tables 15 and 16); it is also smaller when
we use corrected estimates. The clustered standard error estimates appear to have
the largest size distortions. On the whole, we take this to suggest that demonstrate
that our estimator can be used to good effect in order to decide which coefficients
should be modelled as random.


7.5    Lognormal Distribution for β
As explained in section 5.2, our estimating equation is the same whether the dis-
tribution of the random coefficients is normal or not. To illustrate this, we modify
the data-generating process so that the consumer preference parameters βi have a
lognormal distribution:

                                   βi “ β̄i i
                                   β̄i “ p1, 1.5, 1.5, 0.5, 1q
                              lnpi q „ N p´0.5σ 2 , σ 2 q.


                                             38
We study several cases, with σ “ 0.3, 0.4, 0.5 and ξjt „ N p0, 0.1q. The rest of the
specification is as before. Lognormality induces significant skewness and kurtosis into
the distribution of the random coefficients. The standard 2SLS approach gives us
estimates of the first and second moments. We can also introduce the additional
artificial regressors T of section 6.2, either to control for skewness or to estimate it.
We experimented with both possibilities. Each plot in Tables 21, 22, and 23 shows
the distributions when we use only X and K (“only include 2nd moment”) and when
we add T (“include third moment”). These three tables report the distributions of
the estimates of the first, second, and third moments of β1 , β2 , β3 , and βp . Table 24
provides the corresponding summary statistics.
    For a variety of values for the parameter σ of our lognormal distribution, the
2SLS estimates are just as good as they were in the normal setup. The additional
information in the third moment of the random coefficients does not appreciably
increase the precision in our estimates of the means and variances of the random
coefficients. In fact, for some of the coefficients, including the third-order artificial
regressors T leads to significantly less efficient estimates. This is likely due to the fact
that our procedure has a difficult time estimating the third moment of the random
coefficients, as Table 23 shows.


7.5.1   Correction and Kernel

Our paper suggested two potential improvements to the standard 2SLS regression:
our two-step asymptotic correction described in section 6.3.3, and using a kernel
regression to estmate the optimal instruments EpX|Zq and EpK|Zq. We compare
both methods, when coefficients are normal with variances p0, 0.5, 0.5, 0.5, 0.2q and
when they are lognormally distributed with σ “ 0.4 for lnpi q „ N p´0.5σ 2 , σ 2 q. In
both cases we took V arpξq “ 0.1.
   Tables 25 and 26 plot the distributions of the estimators. They suggest that our
correction helps reduce the bias, both for the means and the variances. This holds
whether the random coefficients are normally or log-normally distributed. Using
kernel regressions to approximate the optimal instruments appears to slightly reduce
both the bias and the variance of some of the estimates.



                                            39
Concluding Comments
Our FRAC estimation procedure applies directly to the random coefficients demand
models commonly used in empirical IO. For the most part, our Monte Carlo results
confirm the findings from the expansions. The 2SLS approach yields reliable estimates
of the parameters of the model and of economically meaningful quantities such as price
elasticities; and it does so at a very minimal cost. It is “robust” to variations on the
distribution of the random coefficients. In addition, it provides straightforward tests
that help in variable selection, especially as a guide to determine which coefficients
in the demand system should be modeled as random.
   Some of our simulation results point to directions for future research. We hope to
report more general analytical results that illuminate these findings.



References



    Amemiya, T. (1975), “The nonlinear limited-information maximum-likelihood
estimator and the modified nonlinear two-stage least-squares estimator,” Journal of
Econometrics, 3, 375–386.
  Armstrong, T. (2016), “Large Market Asymptotics for Differentiated Product
Demand Estimators With Economic Models of Supply”, Econometrica, 84, 1961-1980.
    Berry, S. (1994), “Estimating Discrete Choice Models of Product Differentia-
tion”, Rand Journal of Economics, 23, 242-262.
   Berry, S., Levinsohn, J., and A. Pakes (1995), “Automobile Prices in Market
Equilibrium”, Econometrica, 60, 889-917.
   Campioni, G. (2018), “Nonparametric Demand Estimation in Differentiated
Products Markets”, mimeo Yale.
   Chesher, A. (1991), “The Effect of Measurement Error”, Biometrika, 78, 451–
462.




                                          40
  Chesher, A. and J. Santos-Silva (2002), “Taste Variation in Discrete Choice
Models”, Review of Economic Studies, 69, 147–168.
    Cragg, J.G. (1983), “More Efficient Estimation in the Presence of Heteroscedas-
ticity of Unknown Form”, Econometrica, 51, 3, 751–763.
   Crawford, G., and A. Yurukoglu (2012), “The Welfare Effects of Bundling
in Multichannel Television Markets,” American Economic Review, 102, 643-685.
   Dubé, J.-P., Fox, J., and C.-L. Su (2012), “Improving the Numerical Per-
formance of BLP Static and Dynamic Discrete Choice Random Coefficients Demand
Estimation”, Econometrica, 80, 2231-2267.
   Fox, J., Kim, K., Ryan, S.. and P. Bajari (2011), “A simple estimator for
the distribution of random coefficients”, Quantitative Economics, 2, 381–418.
    Gandhi, A. and J.-F. Houde (2016), “Measuring Substitution Patterns in
Differentiated Products Industries”, mimeo.
   Harding, M. and J. Hausman (2007), “Using a Laplace Approximation to
Estimate the Random Coefficients Logit Model by Nonlinear Least Squares”, Inter-
national Economic Review, 48, 1311–1328.
   Ho, K. and R. Lee (2017), “Insurer Competition in Health Care Markets”,
Econometrica, 85, 379-417.
   Horowitz, J. and L. Nesheim (2018), “Using Penalized Likelihood to Select
Parameters in a Random Coefficients Multinomial Logit Model”, mimeo UCL.
  Kadane, J. (1971), “Comparison of k-class Estimators when the Variance is
Small”, Econometrica, 39, 723–737.
   Ketz, P. (2018), “On Asymptotic Size Distortions in the Random Coefficients
Logit Model”, mimeo Paris School of Economics.
  Kristensen, D. and B. Salanié (2017), “Higher-order Properties of Approxi-
mate Estimators”, Journal of Econometrics, 189–208.
    Lee, J. and K. Seo (2015), “A computationally fast estimator for random
coefficients logit demand models using aggregate data”, Rand Journal of Economics,
46, 86-102.
   Reynaert, M. and F. Verboven (2014), “Improving the Performance of Ran-
dom Coefficients Demand Models: The Role of Optimal Instruments”, Journal of

                                        41
Econometrics, 179, 83-98.
    Robinson, P. (1988), “The Stochastic Difference between Econometric Statis-
tics”, Econometrica, 56, 531–548.
   Su, C.-L. and K. Judd (2012), “Constrained Optimization Approaches to
Estimation of Structural Models”, Econometrica, 80, 2213-2230.
    White, H. (1982), “Instrumental Variables Regression with Independent Ob-
servations”, Econometrica, 50(2), 483-499.




                                      42
A        Proof of Theorem 1
Proof. We start from (2); we drop the market index t and the bold letters. Since we
now denote X “ σx ¨ v, we can rewrite (2) in the standard model as
                                      exppXj β ` σxj ¨ v ` ξj q
                       S j “ Ev      řJ                                .
                                  1 ` k“1 Sk exppXk β ` σxk ¨ v ` ξk q
Given that
                                   Sj                     σ2
                        ξj “ log      ´ Xj β ` a1j σ ` a2j ` Opσ 3 q,
                                   S0                     2
we get                          ´                                   ¯
                                                          2
                         Sj exp σ pxj ¨ v ` a1j q ` a2j σ2 ` Opσ 3 q
            Sj “ Ev
                     S0 ` Jk“1 Sk exp σ pxk ¨ v ` a1k q ` a2k σ2 ` Opσ 3 q
                         ř           `                          2         ˘

Eliminating Sj gives
                              ´ `            ˘     2
                                                                  ¯
                          exp σ x1j v ` a1j ` σ2 a2j ` Opσ 3 q
             1 “ Ev                                                      ˘.                 (13)
                     S0 ` Jk“1 Sk exp σ px1k v ` a1k q ` σ2 a2k ` Opσ 3 q
                         ř           `                      2



In this form, Theorem 1.(i) is obvious since only the vectors xk and market shares Sk
enter the system of equations.
    Now use the notation eS Z ” Jk“1 Sk Zk and Ẑj “ Zj ´ eS Z to rewrite (13) as
                                  ř
                                         ˜          ¸
                                              V̂j
                                 0 “ Ev                                          (14)
                                           1 ` eS V
where Vj ” fj ` αj ` fj αj , with
                                                         σ2
                  fj ” exppσxj ¨ vq ´ 1 “ σxj ¨ v `         pxj ¨ vq2 ` OP pσ 3 q
                                                         2
and
                                                                  σ2
         αj “ exppa1j σ ` a2j σ 2 {2 ` Opσ 3 qq ´ 1 “ a1j σ ` pa2j ` a21j q
                                                                     ` Opσ 3 q.
                                                                   2
We note that fj is OP pσq and αj is Opσq, so that Vj is also OP pσq.
    Now expanding (14) gives
                              ´           ¯
         Opσ 3 q “ Ev V̂j ´ Ev V̂j peS V q                                                  (15)
                  “ Ev fˆj ` α̂j                                                            (16)
                  ` α{                  ˆ                                     ˆ
                     j Ev fj ´ peS αqEv fj ´ α̂j Ev peS f q ´ α̂j peS αq ´ Ev fj peS f q.   (17)

                                                43
Only the terms on line (16) can be of order 1 in σ. But using Ev v “ 0 and Ev pxj ¨
                                        2
vqpxk ¨ vq “ xj ¨ xk gives us Ev fj “ σ2 kxj k2 ` Opσ 3 q. Therefore the only term of order
1 is in α̂j “ ap1j σ ` Opσ 2 q, and we must have ap1j “ 0. We note that the “hat” operator
is linear and invertible:
Lemma 1. If Ẑj “ Ŵj for all j and S0 ă 1, then Zj “ Wj .

Proof. Zj ´ eS Z “ Wj ´ eS W implies Zj “ Wj ` λ, where λ “ eS Z ´ eS W . But then
eS Z “ eS W ` eS λ “ p1 ´ S0 qλ, so that λ “ p1 ´ S0 qλ “ 0.

    Applying the lemma gives a1j “ 0. As a consequence, αj “ a2j σ 2 {2 ` Opσ 3 q; and
all terms on line (17) except the last one are of order at least 3 in σ. Since

                                        σ 2 z2 σ 2
                          Ev fˆj ` α̂j “ kx jk `   ap2j ` Opσ 3 q
                                        2        2
and

        Ev fˆj peS f q “ σ 2 Ev px̂j ¨ vqppeS xq ¨ vq ` Opσ 3 q “ σ 2 px̂j ¨ peS xqq ` Opσ 3 q

applying the lemma again gives us pkxj k2 ` a2j q{2 ´ xj ¨ peS xq “ 0.
   Finally, if the distribution of v is symmetric around 0 changing σ to ´σ in (2)
must leave all market shares unchanged; therefore all expansions can only contain
even-degree terms in σ.



B     Detailed Examination of the Mixed Logit
The standard binary model is simply a mixed logit. Applying Theorem 1 with J “ 1
and using S0 ` S1 “ 1, we obtain

                                     a21 “ p2S1 ´ 1qkx1 k2

and K 1 “ p1{2 ´ S1 qX1 X11 . Therefore
                                      ˆ       ˙
                        S1              1
               ξ1 “ log      ´ X1 β ´     ´ S1 Tr ΣX1 X11 ` Opσ k q
                        S0              2

where k “ 3 in general, and k “ 4 if the distribution of  is symmetric around zero.

                                                 44
    The presence of the term p1{2 ´ S1 q in this formula is a consequence of the sym-
metry of the distribution of v around 0 and of the logistic distribution around 0.
Taken together, this implies that market shares around one half vary very little with
σ. The random variation in tastes can only be identified from nonlinearities in the
market shares; but since the cdf of the logistic has an inflexion point when its value
is one half, market shares are essentially linear around that point. It is easy to check
that this is specific to the one-product case; when J ą 1, the mixed multinomial logit
does not face any such difficulty.
   Let us focus for simplicity on the case when random variation in preferences is
uncorrelated across covariates: Σ is the nX ˆ nX diagonal matrix with elements Σmm .
Then given instruments such that E pξ1 |Zq “ 0, the approximate model is
                 ˜                  ˆ          nX
                                              ˙ÿ               ¸
                       S1              1                 2
                                                           ˇ
               E log      ´ X1 β ´       ´ S1      Σmm X1m ˇ Z “ 0.             (18)
                                                           ˇ
                       S0              2       m“1



B.1     Identification
The form of the estimating equation holds interesting insights about identification.
First note that the optimal instruments are
                                            ˆˆ       ˙      ˙
                                               1        2
                      f pZq “ E pX1 |Zq , E      ´ S1 X1 |Z
                                               2
where X12 is the vector with components X1m  2
                                               . The asymptotic variance-covariance
matrix of our estimator θ̂ is given by the usual formula:

                            T Vas θ̂ » J ´1 V pξ1 f pZqqJ ´1 ,

where                          ˆˆ      ˆ       ˙      ˙     ˙
                                        1           2
                        J “E      X1 ,    ´ S1 X1 f pZq .
                                        2
The identifying power of the (approximate) model relies on the full-rank of the ma-
trix J . Suppose for instance that after projecting (via nonparametric regression)
the regressors on the instruments, the residual variation in the artificial regressor
             2
p1{2 ´ S1 qX1m  is very well explained in a linear regression on the other covariates.
Then the estimate of Σmm will be very imprecise, and random taste variation on the
characteristic X1m is probably best left out of the model. Of course, this can be
diagnosed immediately by looking at the precision of the 2SLS estimates.

                                           45
B.2     Higher-order terms
It is easy to program a symbolic algebra system to compute higher-order terms alj for
l ą 2. We show here how to compute the fourth-order term in the mixed logit model
by hand. This will also illustrate the “robustness” of our method to distributional
assumptions.
    Assume that  has a distribution that is symmetric around zero, and that its com-
ponents are independent of each other with variances Σmm and fourth-order moments
km . As before, we assume that Σmm is of order σ 2 and km is of order σ 4 . We also
assume that we can take expansions to order L ě 5.
   Since the distribution is symmetric, we already know that
                                S1          a21 2 a41 4
                     ξ1 “ log      ´ X1 β `    σ `    σ ` Opσ 6 q.
                                S0           2     24
Define Lptq “ 1{p1 ` expp´tqq the cdf of the logistic distribution. Note that L1 “
Lp1 ´ Lq, and that higher-order derivatives follow easily:

                        L2 “ Lp1 ´ Lqp1 ´ 2Lq
                      Lp3q “ Lp1 ´ Lqp1 ´ 6L ` 6L2 q
                      Lp4q “ Lp1 ´ Lqp1 ´ 2Lqp1 ´ 12L ` 12L2 q.

   Since the market share of good 1 is

                                S1 “ E L pX1 pβ ` q ` ξ1 q

we obtain, much as in Appendix A,
                         ˆ                                   ˙
                              S1              2      4     6
                S1 “ E L log    ` X1  ` α2 σ ` α4 σ ` Opσ q
                              S0

where we defined αl “ al1 {l! for l “ 2, 4.
    Let a 0 subscript indicate that we take the value and derivatives of Lptq at t “
logpS1 {S0 q. Defining upq “ X1  ` α2 σ 2 ` α4 σ 4 ` Opσ 6 q and expanding gives
             ˆ            ˙                           p3q      p4q
                 S1                            L20 2 L0 3 L0 4
           L log    `u        “ L0 ` L10 u `      u `   u `    u ` Opu5 q.
                 S0                            2      6     24


                                               46
                                                           p4q
Incorporating L0 “ S1 , L10 “ S1 p1 ´ S1 q, up to L0 gives
         ˆ
                                                       upq2
S1 “ E S1 ` S1 p1 ´ S1 qupq ` S1 p1 ´ S1 qp1 ´ 2S1 q
                                                         2
                                     3                                               4
                                                                                                 ˙
                              2 upq                                          2 upq           5
   ` S1 p1 ´ S1 qp1 ´ 6S1 ` 6S1 q      ` S1 p1 ´ S1 qp1 ´ 2S1 qp1 ´ 12S1 ` 12S1 q      ` Opupq q ;
                                   6                                              24
dividing by S1 p1 ´ S1 q yields

E u`p1´2S1 qE u2 {2`p1´6S1 `6S12 qE u3 {6`p1´2S1 qp1´12S1 `12S12 qE u4 {24 “ E Opu5 q.
                                                                                 (19)
   Finally, up to order 6 in σ:

                   E u “ α 2 σ 2 ` α 4 σ 4
                                                          nX
                                                          ÿ
                  E u2 “ σ 2 E pX1 q2 ` α22 σ 4 “             Σmm x21m ` α22 σ 4
                                                        m“1
                                                        nX
                                                        ÿ
                  E u3 “ 3α2 σ 4 E pX1 q2 “ 3α2               Σmm x21m
                                                        m“1
                                              nX
                                              ÿ
                  E u4 “ σ 4 E pX1 q4 “          km x41m .
                                              m“1
                         2
Regrouping terms in σ in (19) confirms that
                                                     nX
                                                     ÿ
                             α2 σ 2 “ pS1 ´ 1{2q           Σmm x21m ,
                                                     m“1

which we knew from Theorem 1. The terms in σ 4 give us
                                                                         nX
                                                                         ÿ
            α4 σ 4 “ α22 σ 4 pS1 ´ 1{2q ´ α2 σ 2 p1 ´ 6S1 ` 6S12 q              Σmm x21m {2
                                                                         m“1
                                                        nX
                                                        ÿ
                  ´ p1 ´ 2S1 qp1 ´ 12S1 ` 12S12 q                km x41m {24.
                                                       m“1

This simplifies to
          ˆ        ˙
      4      1
  α4 σ “       ´ S1 ˆ
             2
        ¨                                ¸2 ˆ                                ˛
         ˆ                 ˙˜ÿ
                             nX                                  nX
                                                                ˙ÿ
        ˝ 1 ´ 2S1 p1 ´ S1 q      Σmm x21m ´
                                              1
                                                 ´ S1 p1 ´ S1 q      km x41m ‚.
            4                m“1
                                              12                 m“1



                                              47
   This formula may not seem especially enlightening, but it shows several impor-
tant points. First, terms of higher orders can be computed without much difficulty.
                                                                            2
Second, each additional term adds information on lower-order moments (here σm ), as
well as on the moments of higher order (here km ). The model remains linear in the
highest order moments; here for km we have new artificial regressors
                         ˆ        ˙ˆ                   ˙
                           1           1
                             ´ S1        ´ S1 p1 ´ S1 q x41m .
                           2          12
On the other hand, the higher-order expansions introduce nonlinear functions of the
lower-order moments, here represented by
                ˆ         ˙ˆ                  ˙˜ÿ nX
                                                             ¸2
                   1         1
                     ´ S1      ´ 2S1 p1 ´ S1 q       Σmm x21m ,
                   2         4                   m“1

and the model is not linear in these parameters any more. This could be dealt with
in several ways: by nonlinear optimization (of a very simple kind), or by iterative
methods. In any case, we will see in our simulations that stopping with the second-
order expansion often gives results that are already very reliable.
    Finally, while the estimator based on the second-order expansion is “robust” to
any (well-behaved) distribution, the estimator based on this fourth-order expansion
also assumes symmetry: a skewed distribution would generate terms in σ 3 . Making
more assumptions changes the form of the artificial regressors. To illustrate this,
consider a mixed logit with one covariate only (nX “ 1). The expansion to order 2L
can be written
                                      L
                          S1         ÿ           `       ˘k
                 ξ1 “ log    ´ βX1 `     tk pS1 q Σ11 X12 ` Opσ 2L`2 q.
                          S0         k“1

Assume that  has normal kurtosis. Then k1 “ 3Σ211 and we find the simpler formula
                                   ˆ       ˙
                                     1
                        t2 “ α4 “      ´ S1 S1 p1 ´ S1 q.
                                     2
    Specializing further, Figure 1 in the main text plots the terms tk pS1 q for k “
1, 2, 3, 4 as the market share goes from zero to one when  is Gaussian.
     If the components of  are independently distributed and have third moments
ps1 , . . . , snX q, then it is easy to see that an additional term
                                    ˆ                 ˙ nX
                                                     1 ÿ        3
                                      S1 p1 ´ S1 q ´       sm X1m
                                                     6 m“1

                                        48
enters the expansion. To test for skewness on covariate m, one could simply test that
              `                ˘ 3
the regressor S1 p1 ´ S1 q ´ 61 X1m  can be omitted.


B.3     Beyond Logit and Gaussian
The properties of the logistic function may seem to have been more central to our
calculations; but in fact they are quite ancillary. Suppose that ui1t ´ ui0t has some
distribution with cdf Q instead of L. While the derivatives of Q may not obey the
nice polynomial formulæ we used for L, it is still true that if Q is invertible and
smooth then we can define functions Fk by

                                    Qpkq ptq “ Fk pQptqq.

This is all we need to carry out the expansions. One can show for instance that the
factor pS1 ´ 12 q that appears in (18) just needs to be replaced with

                                             F2 pS1 q
                                         ´             .
                                             2F1 pS1 q

Take for instance a mixed binary model with such a general distribution for u1 ´
u0 , and a distribution of the random coefficient on the single covariate X1 that has
successive moments 0, Σ, µ3 , µ4 . Then it is easy to derive the following fourth-order
expansion, which could perhaps serve as the basis for a semiparametric estimator:

                S1              F2 pS1 q 2
        ξ2 “ log     ´ βX1 ´            X Σ
                S0              F1 pS1 q 1
            F3 pS1 q 3
          ´          X µ3
            F1 pS1 q 1
                     ˜              ˆ           ˙2 ¸
            F2 pS1 q     F3 pS1 q      F2 pS1 q               F4 pS1 q
          `            3          ´                  X14 Σ2 ´          µ4 X14 ` Opσ 5 q.
            F1 pS1 q     F1 pS1 q      F1 pS1 q               F1 pS1 q

This can be extended in the obvious way to make v heteroskedastic (just replace Σ
with Ep2 |X1 q and µm with Epεm |X1 q in the above formula.)




                                               49
C        The Two-level Nested Logit
In the unmixed model (σ “ 0) the mean utility of alternative j is Uj “ Ik `λk log Sj|Nk
if j P Nk , with Ik ” logpSNk {S0 q and Sj|Nk ” Sj {SNk .This gives

                          ξj0 “ ´Xj β ` logpSNk {S0 q ` λk log Sj|Nk .

We write (imposing a1j “ 0 from the start as this is a general property of models
with Ev “ 0)

                                                                              σ2
                   Uj pvq “ logpSNk {S0 q ` λk log Sj|Nk ` σxj ¨ v `             a2j
                                                                              2
and
                                        ÿ
               exppIk pvq{λk q “               exppUj pvq{λk q “ pSNk {S0 q1{λk f¯k pvq
                                        jPNk
                            ř
where we denote X̄k “           jPNk   Sj|Nk Xj and
                   ˆ                      ˙
                       σ ´           a2j ¯       σ           σ2 `                 ˘
    fj pvq “ exp          xj ¨ v ` σ        » 1 ` pxj ¨ vq ` 2 λk a2j ` pxj ¨ vq2
                       λk             2          λk         2λk

so that
                                    σ           σ2 `
                       f¯k pvq » 1 ` x
                                                                      ˘
                                       sk ¨ v ` 2 λk ā2k ` px
                                                             Ğ ¨ vq2 k .
                                    λk         2λk
Now using
                                                                   exppIk pvqq
                   Sj “ Ev expppUj pvq ´ Ik pvqq{λk q              řK
                                                                1 ` l“1 exppIl pvqq
we get                             ˜                                   ¸
                                                            ˘λk
                                                      ¯
                                                          `
                                        fj pvq       f k pvq
                          1 “ Ev        ¯                           ˘λ .
                                                         SNl f¯l pvq l
                                                    řK
                                        fk pvq S0 `
                                                             `
                                                          l“1

We note that
            1 ` aσ ` bσ 2
                          “ 1 ` pa ´ cqσ ` pb ´ d ´ cpa ´ cqqσ 2 ` Opσ 3 q.               (20)
            1 ` cσ ` dσ 2

Denote Âj|k “ Aj ´ Āk . Applying (20) gives

                              fj pvq        σ             σ2
                                      » 1 `    C j pvq `      Dj pvq.
                              f¯k pvq       λk           2λ2k


                                                     50
with
                                          Cj pvq “ x̂j|k ¨ v
and
                      Dj pvq “ λk ap2j|k ` px
                                            { ¨ vq2 j|k ´ 2px̄k ¨ vqpx̂j|k ¨ vq.
Moreover,
                                                 ˜                                           ¸
                       ˘λ                   σ2       λl ´ 1                     px
                                                                                 Ğ ¨ vq2 l
                f¯l pvq l » 1 ` σ x̄l ¨ v `
            `
                                                            px̄l ¨ vq2 ` ā2l `
                                            2          λl                          λl

and
                                                   ´                             Ğ2
                                                                                        ¯
             ˘λk                                σ2       λk ´1            2     px¨vq
      f¯k pvq                   1 ` σ x̄k ¨ v ` 2 ā2k ` λk px̄k ¨ vq ` λk            k
       `
                    ˘λl »                     ´                 ´                             ¯¯
S0 ` l“1 SNl f¯l pvq
    řK        `                                                                         Ğ2
                          1 ` σeS x ¨ v ` σ2 eS a2 ` K
                                            2                     λl ´1           2 ` px¨vq l
                                                     ř
                                                       l“1 SN l    λl
                                                                        px̄ l ¨ vq        λl
                            řJ                řK
where as usual eS T “         j“1   Sj Tj “    k“1   SNk T̄k .
   Then, using (20) again,
                                 ˘λ
                          f¯k pvq k
                        `
                                                         σ2
                                      ˘λl » 1 ` σEk pvq ` Fk pvq
                 S0 ` l“1 SNl f¯l pvq
                      řK          `
                                                         2

with
                                      Ek pvq “ px̄k ´ eS xq ¨ v
and

                       Fk pvq “ ā2k ´ eS a2
                                                    K
                                λk ´ 1          2
                                                   ÿ       λl ´ 1
                              `        px̄k ¨ vq ´     SNl        px̄l ¨ vq2
                                  λk               l“1
                                                             λl
                                             K
                                px
                                 Ğ ¨ vq2 k ÿ        px
                                                     Ğ ¨ vq2 l
                              `           ´     SNl
                                   λk       l“1
                                                       λl
                              ´ 2peS x ¨ vqppx̄k ´ eS xq ¨ vq.

This allows us to write
                   ˆ                 ˙ˆ                ˙
                        σ       σ2                  σ2
            1 » Ev 1 ` C j ` 2 D j       1 ` σEk ` Fk
                        λk     2λk                  2
                   ˆ     ˆ         ˙     2 `
                                                               ˙
                           Cj          σ          2
                                                             ˘
              » Ev 1 ` σ      ` Ek ` 2 Dj ` λk Fk ` 2λk Cj Ek .
                           λk         2λk

                                                     51
We have Ev Cj “ Ev Ek “ 0; also,

                         EDj “ λk ap2j|k ` kxj k2 ´ kxk
                                                    Ę2 ´ 2x̄k ¨ x̂j|k
                                                        k

                         EFk “ ā2k ´ eS a2
                                                 K
                                λk ´ 1       2
                                                ÿ       λl ´ 1
                              `        kx̄k k ´     SNl        kx̄l k2
                                  λk            l“1
                                                          λl

                                Ę2       K      Ę2
                                kxk k
                                        ÿ       kxk  l
                              `       ´     SNl
                                 λk     l“1
                                                 λ l

                              ´ 2peS xq ¨ px̄k ´ eS xq
                    EpCj Ek q “ x̂j|k ¨ px̄k ´ eS xq.

Writing EpDj ` λ2k Fk ` 2λk Cj Ek q “ 0 gives us an equation of the form

                   λk pa2j ´ ā2k q ` λ2k pā2k ´ eS a2 q “ λ2k M ` νk ` µj

where
     K                      K      Ę2
     ÿ     λl ´ 1       2
                           ÿ       kxk
M“     SNl        kx̄l k `     SNl     l
                                         ´ 2keS xk2
   l“1
             λl            l“1
                                    λl

     Ę2 ´ 2kx̄k k2 ´ λk pλk ´ 1qkx̄k k2 ´ λk kxk
νk “ kxk                                     Ę2 ` 2λ2 eS x ¨ x̄k ` 2λk kx̄k k2 ´ 2λk x̄k ¨ eS x
          k                                        k   k
              `                     2
                                                     ˘
                Ę2 ´ p2 ´ λk qkx̄k k ´ 2λk x̄k ¨ eS x
   “ p1 ´ λk q kxk                                                               (21)
                     k
              2
µj “ ´kxj k ` 2xj ¨ x̄k ´ 2λk xj ¨ px̄k ´ eS xq
   “ xj ¨ p2λk eS x ´ xj ` 2p1 ´ λk qx̄k q .                                        (22)

It is easy to aggregate from a2j “ p1 ´ λk qā2k ` λk eS a2 ` λk M ` pνk ` µj q{λk to
                                                        νk ` µ̄k
                                ā2k “ eS a2 ` M `
                                                           λ2k

and then to
                                                    K
                                                    ÿ           νk ` µ̄k
                         S0 eS a2 “ p1 ´ S0 qM `          SNk            ,
                                                    k“1
                                                                   λ2k




                                               52
which gives
                                            νk ` µ̄k νk ` µj
              a2j “ eS a2 ` M ` p1 ´ λk q           `
                                               λ2k      λk
                            K
                     M   1 ÿ       νl ` µ̄l             νk ` µ̄k νk ` µj
                 “     `       SNl     2
                                            ` p1 ´ λk q         `
                     S0 S0 l“1        λl                   λ2k      λk
                            K
                     M   1 ÿ       νl ` µ̄l νk ` p1 ´ λk qµ̄k µj
                 “     `       SNl         `                 ` .
                     S0 S0 l“1        λ2l          λ2k        λk

Finally, using equations (21) and (22) we aggregate

                     µ̄k “ 2λk x̄k ¨ eS x ` 2p1 ´ λk qkx̄k k2 ´ kxk
                                                                Ę2 ,
                                                                    k


which gives
                νk ` µ̄k “ 2λ2k x̄k ¨ eS x ` λk p1 ´ λk qkx̄k k2 ´ λk kxk
                                                                      Ę2
                                                                          k

and
                        νk ` p1 ´ λk qµ̄k “ ´λk p1 ´ λk qkx̄k k2 .
Putting everything together, we get
                              K
                    M    1 ÿ        νl ` µ̄l νk ` p1 ´ λk qµ̄k µj
              a2j “   `        SNl            `                   `
                    S0 S0 l“1          λ2l             λ2k           λk
                      ˜                                                    ¸
                        K                         K           2
                    1   ÿ      λl ´ 1            ÿ       kxk l
                                                         Ę
                  “        SNl         kx̄l k2 `     SNl        ´ 2keS xk2
                    S0 l“1       λl              l“1
                                                           λl
                                   K       Ę2 ` p1 ´ λl qkx̄l k2
                   2       2    1 ÿ       ´kxk  l
                 ` keS xk `           SNl
                   S0          S0 l“1               λl
                       ˆ                          ˙
                                xj     1 ´ λk         1 ´ λk
                 “ xj ¨ 2eS x ´     `2        x̄k ´          kx̄k k2 .
                                λk        λk            λk


D     Our Correction Formula
Remember from section 6.3.2 that
                              ˆ                ˙´1
                                 Bf8
                   θ0 » θ2 ´ E       pθ2 ; λ0 q    f8 pθ2 ; λ0 q.              (23)
                                  Bθ


                                            53
The term in the inverse is easily proxied:
                                                    ˆ            ˙1
                 Bf8                Bf2                 Bξ2             Bξ2
               E     pθ2 ; λ0 q » E     pθ2 q “ E           pθ2 q V V 1     pθ2 q,
                  Bθ                Bθ                  Bθ              Bθ
since ξ2 is linear in θ. Note that this is EX 1 X , where
                                      Bξ2
                             X ”V1        pθ2 q “ ´V 1 pX, Kq
                                      Bθ
and row j “ 1, . . . , J of pX, Kq lists the covariates and artificial regressors for this
product. It follows that
                                      ˜                                ¸
                                             1     1         1     1
                    Bf8                 E pX V V Xq E pX V V Kq
                 E       pθ2 ; λ0 q »                                    .
                     Bθ                 E pK 1 V V 1 Xq E pK 1 V V 1 Kq

To the second-order in e2 , Ef8 pθ2 ; λ0 q equals
             ˆ                             ˙    ˆ                             ˙
               Bξ2      1    1                    Bê2           1    1
           E       pθ2 q V V ê2 pθ2 ; λ0 q ` E        pθ2 ; λ0 q V V ξ2 pθ2 q .        (24)
               Bθ                                 Bθ
The first term in (24) is simply E pX 1 V 1 ê2 q. Going back to (23), we get
            ˜                                   ¸´1 ˆ
                                                                         B ê2 1
                                                                       ˆ                ˙˙
              E pX 1 V V 1 Xq E pX 1 V V 1 Kq              1 1                       1
θ0 » θ2 ´                                             E pX V ê2 q ` E           V V ξ2    .
             E pK 1 V V 1 Xq E pK 1 V V 1 Kq                             Bθ

Finally, using Theorem 1(i), we know that Bê
                                          Bβ
                                             2
                                               “ 0. Therefore
                                    ˙ ˜                                          ¸
                      B ê2 1                    ´E pX 1 V V´1 ê2 q
                    ˆ
         1 1                      1
   E pX V ê2 q ` E           V V ξ2 “                               1
                                                                                ¯  .
                      Bθ               ´E pK 1 V V 1 ê2 q ` E Bê
                                                                BΣ
                                                                   2
                                                                       V V 1 ξ2


E     Proof of Theorem 3
We drop the bold letters in this proof to alleviate the notation, and without loss of
generality we normalize B “ 1.
    Remember that Gpy, F py, β, σq, β, σq “ 0, so that Gpy, F py, β, 0q, β, 0q “ 0. Given (11),
this gives G˚ py, A˚ py, F py, β, 0q ´ f1 pyqβ, 0q “ 0 for all β. This can only hold if
F py, β, 0q ´ f1 pyqβ does not depend on β, which implies condition C2. Denoting
f0 pyq “ F py, β, 0q ´ f1 pyqβ, we obtain

                                G˚ py, A˚ py, f0 pyq, 0qq “ 0.

                                             54
Now writing G˚ py, Ev A˚ py, F py, β, σq´f1 pyqβ, σvqq “ 0 as an identity in σ and taking
derivatives with respect to σ, we get

                                                       G˚2 Ev pA˚2 Fσ ` A˚3 vq “ 0
                              G˚22 rEv pA˚2 Fσ ` A˚3 vqsrEv pA˚2 Fσ ` A˚3 vqs
             `G˚2 Ev pA˚2 Fσσ ` A˚22 rFσ , Fσ s ` 2A˚23 rFσ , vs ` A˚33 rv, vsq “ 0.

Fortunately, this simplifies greatly at σ “ 0. The first equation gives

                             G˚2 Ev pA˚2 Fσ py, β, 0q ` A˚3 vq “ 0,

where the derivatives A˚2 and A˚3 do not depend on v since σ “ 0. It follows that
G˚2 A˚2 Fσ py, β0 , 0q “ 0 since Ev “ 0. Given our invertibility assumption, condition C1
also holds. Using the second equation at σ “ 0, and given that Fσ py, β0 , 0q “ 0, we
get
                              G˚2 Ev pA˚22 rFσ , Fσ s ` 2A˚23 rFσ , vsq “ 0
so that
                                   G˚2 pA˚2 Fσσ ` A˚33 q “ 0.
Given that G˚2 is invertible, this gives (reintroducing the arguments)

                    A˚2 py, f0 pyq, 0qFσσ py, β, 0q ` A˚33 py, f0 pyq, 0q “ 0.

Therefore Fσσ py, β, 0q is independent of β and condition C3 holds. Noting that
f2 pyq “ ´Fσσ py, β, 0q completes the proof.




                                               55
List of Tables
  1    Distribution of the Estimates of β0 . . . . . . . . . . . . . . . . . . .         57
  2    Distribution of the Estimates of β¯1x . . . . . . . . . . . . . . . . . . .       58
  3    Distribution of the Estimates of   Varpβ1x q    . . . . . . . . . . . . . . . .   59
  4    Distribution of the Estimates of   β¯2x . . .   . . . . . . . . . . . . . . . .   60
  5    Distribution of the Estimates of   Varpβ2x q    . . . . . . . . . . . . . . . .   61
  6    Distribution of the Estimates of   β¯3x . . .   . . . . . . . . . . . . . . . .   62
  7    Distribution of the Estimates of   Varpβ3x q    . . . . . . . . . . . . . . . .   63
  8    Distribution of the Estimates of   β¯p . . .    . . . . . . . . . . . . . . . .   64
                                                 p
  9    Distribution of the Estimates of Varpβ q . . . . . . . . . . . . . . . .          65
  10   Pseudo True Value: Increasing-number-of-markets Approach . . . . .                66
  11   Pseudo True Value: Moment-based Approach . . . . . . . . . . . . .                67
  12   Distribution of the Difference between True and Estimated Elasticity              68
  13   Distribution of the Estimates of the Means — Different β¯p . . . . . .            69
  14   Distribution of the Estimates of the Variances — Different β¯p . . . .            70
  15   Testing for Zero Means — Standard 2SLS . . . . . . . . . . . . . . .              71
  16   Testing for Zero Means — Corrected 2SLS . . . . . . . . . . . . . . .             72
  17   Testing for Zero Variances — Standard 2SLS . . . . . . . . . . . . . .            73
  18   Testing for Zero Variances — Corrected 2SLS . . . . . . . . . . . . .             74
  19   Joint Test of Zero Means and Variances — Standard 2SLS . . . . . .                75
  20   Joint Test of Zero Means and Variances — Corrected 2SLS . . . . . .               76
  21   Distribution of the Estimates of the Means (Lognormal Case) . . . .               77
  22   Distribution of the Estimates of the Variances (Lognormal Case) . . .             78
  23   Distribution of the Estimates of the Third-order Moments (Lognormal
       Case) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .     79
  24   Summary Statistics for the Lognormal Case . . . . . . . . . . . . . .             80
  25   Distribution of Three Estimates of the Means — Normal and Lognormal 81
  26   Distribution of Three Estimates of the Variances — Normal and Log-
       normal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      82




                                          56
                  Table 1: Distribution of the Estimates of β0

       Var(ξ)=0.1                   Var(ξ)=0.1                    Var(ξ)=0.1
Var(β)=(0,0.1,0.1,0.1,0.05)   Var(β)=(0,0.2,0.2,0.2,0.1)    Var(β)=(0,0.5,0.5,0.5,0.2)




       Var(ξ)=0.5                   Var(ξ)=0.5                    Var(ξ)=0.5
Var(β)=(0,0.1,0.1,0.1,0.05)   Var(β)=(0,0.2,0.2,0.2,0.1)    Var(β)=(0,0.5,0.5,0.5,0.2)




       Var(ξ)=1.0                   Var(ξ)=1.0                    Var(ξ)=1.0
Var(β)=(0,0.1,0.1,0.1,0.05)   Var(β)=(0,0.2,0.2,0.2,0.1)    Var(β)=(0,0.5,0.5,0.5,0.2)




                                      57
                  Table 2: Distribution of the Estimates of β¯1x

       Var(ξ)=0.1                   Var(ξ)=0.1                      Var(ξ)=0.1
Var(β)=(0,0.1,0.1,0.1,0.05)   Var(β)=(0,0.2,0.2,0.2,0.1)      Var(β)=(0,0.5,0.5,0.5,0.2)




       Var(ξ)=0.5                   Var(ξ)=0.5                      Var(ξ)=0.5
Var(β)=(0,0.1,0.1,0.1,0.05)   Var(β)=(0,0.2,0.2,0.2,0.1)      Var(β)=(0,0.5,0.5,0.5,0.2)




       Var(ξ)=1.0                   Var(ξ)=1.0                      Var(ξ)=1.0
Var(β)=(0,0.1,0.1,0.1,0.05)   Var(β)=(0,0.2,0.2,0.2,0.1)      Var(β)=(0,0.5,0.5,0.5,0.2)




                                       58
               Table 3: Distribution of the Estimates of Varpβ1x q

       Var(ξ)=0.1                   Var(ξ)=0.1                     Var(ξ)=0.1
Var(β)=(0,0.1,0.1,0.1,0.05)   Var(β)=(0,0.2,0.2,0.2,0.1)     Var(β)=(0,0.5,0.5,0.5,0.2)




       Var(ξ)=0.5                   Var(ξ)=0.5                     Var(ξ)=0.5
Var(β)=(0,0.1,0.1,0.1,0.05)   Var(β)=(0,0.2,0.2,0.2,0.1)     Var(β)=(0,0.5,0.5,0.5,0.2)




       Var(ξ)=1.0                   Var(ξ)=1.0                     Var(ξ)=1.0
Var(β)=(0,0.1,0.1,0.1,0.05)   Var(β)=(0,0.2,0.2,0.2,0.1)     Var(β)=(0,0.5,0.5,0.5,0.2)




                                       59
                  Table 4: Distribution of the Estimates of β¯2x

       Var(ξ)=0.1                   Var(ξ)=0.1                      Var(ξ)=0.1
Var(β)=(0,0.1,0.1,0.1,0.05)   Var(β)=(0,0.2,0.2,0.2,0.1)      Var(β)=(0,0.5,0.5,0.5,0.2)




       Var(ξ)=0.5                   Var(ξ)=0.5                      Var(ξ)=0.5
Var(β)=(0,0.1,0.1,0.1,0.05)   Var(β)=(0,0.2,0.2,0.2,0.1)      Var(β)=(0,0.5,0.5,0.5,0.2)




       Var(ξ)=1.0                   Var(ξ)=1.0                      Var(ξ)=1.0
Var(β)=(0,0.1,0.1,0.1,0.05)   Var(β)=(0,0.2,0.2,0.2,0.1)      Var(β)=(0,0.5,0.5,0.5,0.2)




                                       60
               Table 5: Distribution of the Estimates of Varpβ2x q

       Var(ξ)=0.1                   Var(ξ)=0.1                     Var(ξ)=0.1
Var(β)=(0,0.1,0.1,0.1,0.05)   Var(β)=(0,0.2,0.2,0.2,0.1)     Var(β)=(0,0.5,0.5,0.5,0.2)




       Var(ξ)=0.5                   Var(ξ)=0.5                     Var(ξ)=0.5
Var(β)=(0,0.1,0.1,0.1,0.05)   Var(β)=(0,0.2,0.2,0.2,0.1)     Var(β)=(0,0.5,0.5,0.5,0.2)




       Var(ξ)=1.0                   Var(ξ)=1.0                     Var(ξ)=1.0
Var(β)=(0,0.1,0.1,0.1,0.05)   Var(β)=(0,0.2,0.2,0.2,0.1)     Var(β)=(0,0.5,0.5,0.5,0.2)




                                       61
                  Table 6: Distribution of the Estimates of β¯3x

       Var(ξ)=0.1                   Var(ξ)=0.1                      Var(ξ)=0.1
Var(β)=(0,0.1,0.1,0.1,0.05)   Var(β)=(0,0.2,0.2,0.2,0.1)      Var(β)=(0,0.5,0.5,0.5,0.2)




       Var(ξ)=0.5                   Var(ξ)=0.5                      Var(ξ)=0.5
Var(β)=(0,0.1,0.1,0.1,0.05)   Var(β)=(0,0.2,0.2,0.2,0.1)      Var(β)=(0,0.5,0.5,0.5,0.2)




       Var(ξ)=1.0                   Var(ξ)=1.0                      Var(ξ)=1.0
Var(β)=(0,0.1,0.1,0.1,0.05)   Var(β)=(0,0.2,0.2,0.2,0.1)      Var(β)=(0,0.5,0.5,0.5,0.2)




                                       62
               Table 7: Distribution of the Estimates of Varpβ3x q

       Var(ξ)=0.1                   Var(ξ)=0.1                     Var(ξ)=0.1
Var(β)=(0,0.1,0.1,0.1,0.05)   Var(β)=(0,0.2,0.2,0.2,0.1)     Var(β)=(0,0.5,0.5,0.5,0.2)




       Var(ξ)=0.5                   Var(ξ)=0.5                     Var(ξ)=0.5
Var(β)=(0,0.1,0.1,0.1,0.05)   Var(β)=(0,0.2,0.2,0.2,0.1)     Var(β)=(0,0.5,0.5,0.5,0.2)




       Var(ξ)=1.0                   Var(ξ)=1.0                     Var(ξ)=1.0
Var(β)=(0,0.1,0.1,0.1,0.05)   Var(β)=(0,0.2,0.2,0.2,0.1)     Var(β)=(0,0.5,0.5,0.5,0.2)




                                       63
                  Table 8: Distribution of the Estimates of β¯p

       Var(ξ)=0.1                   Var(ξ)=0.1                     Var(ξ)=0.1
Var(β)=(0,0.1,0.1,0.1,0.05)   Var(β)=(0,0.2,0.2,0.2,0.1)     Var(β)=(0,0.5,0.5,0.5,0.2)




       Var(ξ)=0.5                   Var(ξ)=0.5                     Var(ξ)=0.5
Var(β)=(0,0.1,0.1,0.1,0.05)   Var(β)=(0,0.2,0.2,0.2,0.1)     Var(β)=(0,0.5,0.5,0.5,0.2)




       Var(ξ)=1.0                   Var(ξ)=1.0                     Var(ξ)=1.0
Var(β)=(0,0.1,0.1,0.1,0.05)   Var(β)=(0,0.2,0.2,0.2,0.1)     Var(β)=(0,0.5,0.5,0.5,0.2)




                                       64
               Table 9: Distribution of the Estimates of Varpβ p q

       Var(ξ)=0.1                   Var(ξ)=0.1                     Var(ξ)=0.1
Var(β)=(0,0.1,0.1,0.1,0.05)   Var(β)=(0,0.2,0.2,0.2,0.1)     Var(β)=(0,0.5,0.5,0.5,0.2)




       Var(ξ)=0.5                   Var(ξ)=0.5                     Var(ξ)=0.5
Var(β)=(0,0.1,0.1,0.1,0.05)   Var(β)=(0,0.2,0.2,0.2,0.1)     Var(β)=(0,0.5,0.5,0.5,0.2)




       Var(ξ)=1.0                   Var(ξ)=1.0                     Var(ξ)=1.0
Var(β)=(0,0.1,0.1,0.1,0.05)   Var(β)=(0,0.2,0.2,0.2,0.1)     Var(β)=(0,0.5,0.5,0.5,0.2)




                                       65
     Table 10: Pseudo True Value: Increasing-number-of-markets Approach

 Parameter                                            Scenarios
True Varpβq :       p0, 0.1, 0.1, 0.1, 0.05q      p0, 0.2, 0.2, 0.2, 0.1q        p0, 0.5, 0.5, 0.5, 0.2q
True Varpξq :      0.1        0.5           1    0.1        0.5          1      0.1        0.5         1
                  -1.00      -1.00       -1.00 -1.00       -1.00      -1.00    -1.02 -1.03 -1.03
  β0 “ ´1
                (.0043) (.0050) (.0058) (.011) (.012) (.013)                  (.032) (.035) (.038)
                  1.51        1.51        1.51  1.53       1.53        1.53     1.56      1.57       1.57
  β¯1x “ 1.5
                 (.022) (.023) (.024) (.050) (.050) (.050)                     (.13)      (.13)     (.13)
                  1.51        1.51        1.51  1.52       1.52        1.52     1.55      1.56       1.56
  β¯2x “ 1.5
                 (.023) (.024) (.025) (.048) (.049) (.049)                     (.12)      (.12)     (.12)
                 0.487       0.487       0.487 0.465      0.465       0.464   0.403 0.400 0.398
 β¯3x q “ 0.5
                 (.022) (.022) (.022) (.048) (.047) (.047)                     (.12)      (.12)     (.11)
                 -0.999 -0.999 -0.999 -0.990 -0.990 -0.990                    -0.954 -0.955 -0.956
  β¯p “ ´1
                (.0086) (.0088) (.0090) (.0184) (.0186) (.0188)               (.043) (.044) (.045)
                0.0857 0.0856 0.0856           0.152      0.152       0.152   0.288 0.290 0.291
  Varpβ1x q
                 (.011) (.011) (.011) (.028) (.027) (.027)                    (.078) (.076) (.075)
                0.0863 0.0865 0.0866           0.152      0.152       0.153   0.284 0.286 0.288
  Varpβ2x q
                (.0086) (.0086) (.0087) (.0205) (.020) (.020)                 (.059) (.057) (.056)
                0.0952 0.0949 0.0946           0.182      0.181       0.181   0.400 0.399 0.397
  Varpβ3x q
                (.0097) (.010) (.010) (.024) (.023) (.023)                    (.063) (.063) (.062)
                0.0480 0.0479 0.0478 0.0888               0.088       0.088    0.148 0.147 0.147
  Varpβ p q
                (.0056) (.0057) (.0059) (.013) (.013) (.014)                  (.031) (.032) (.033)




                                          66
            Table 11: Pseudo True Value: Moment-based Approach

 Parameter                                          Scenarios
True Varpβq :     p0, 0.1, 0.1, 0.1, 0.05q    p0, 0.2, 0.2, 0.2, 0.1q        p0, 0.5, 0.5, 0.5, 0.2q
True Varpξq :     0.1        0.5         1   0.1        0.5         1       0.1        0.5         1
  β0 “ ´1        -1.01     -1.01      -1.01 -1.04      -1.04      -1.04    -1.11      -1.11      -1.12
  β¯1x “ 1.5     1.49       1.49       1.49 1.48       1.48        1.48     1.43       1.43       1.43
  β¯2x “ 1.5     1.49       1.49       1.49 1.48       1.48        1.48     1.43       1.43       1.43
  β¯3x “ 0.5    0.496 0.496 0.496 0.486 0.486 0.486                       0.455 0.455 0.455
  β¯p “ ´1      -0.989 -0.988 -0.988 -0.958 -0.957 -0.955                 -0.873 -0.869 -0.864
   Varpβ1x q    0.0854 0.0855 0.0855 0.149 0.149 0.149                     0.275 0.275 0.276
   Varpβ2x q    0.0855 0.0855 0.0856 0.149 0.149 0.149                     0.273 0.274 0.274
   Varpβ3x q    0.0938 0.0938 0.0937 0.176 0.175 0.175                     0.369 0.368 0.366
   Varpβ p q    0.0421 0.0421 0.0419 0.0685 0.0681 0.0676                 0.0920 0.0906 0.0888




                                          67
Table 12: Distribution of the Difference between True and Estimated Elasticity

       Product 5                    Product 10                    Product 15




      Product 20                    Product 25                     Legend




                                     68
 Table 13: Distribution of the Estimates of the Means — Different β¯p

     β¯p “ ´1                     β¯p “ ´2                     β¯p “ ´3
Distribution of β0           Distribution of β0           Distribution of β0




Distribution of β¯1x         Distribution of β¯1x         Distribution of β¯1x




Distribution of β¯2x         Distribution of β¯2x         Distribution of β¯2x




Distribution of β¯3x         Distribution of β¯3x         Distribution of β¯3x




Distribution of β¯p          Distribution of β¯p          Distribution of β¯p




                                  69
  Table 14: Distribution of the Estimates of the Variances — Different β¯p

       β¯p “ ´1                     β¯p “ ´2                      β¯p “ ´3
Distribution of Varpβ1x q    Distribution of Varpβ1x q     Distribution of Varpβ1x q




Distribution of Varpβ2x q    Distribution of Varpβ2x q     Distribution of Varpβ2x q




Distribution of Varpβ3x q    Distribution of Varpβ3x q     Distribution of Varpβ3x q




Distribution of Varpβ p q    Distribution of Varpβ p q     Distribution of Varpβ p q




                                     70
         Table 15: Testing for Zero Means — Standard 2SLS

Significance level                                  1%    5%   10%
2SLS with heteroskedasticity-robust standard error 0.904 0.793 0.711
GLS estimator and standard errors                  0.889 0.765 0.678
2SLS with clustered standard error                 0.904 0.780 0.702




                                71
         Table 16: Testing for Zero Means — Corrected 2SLS

Significance level                                  1%    5%   10%
2SLS with heteroskedasticity-robust standard error 0.915 0.819 0.725
GLS estimator and standard errors                  0.882 0.767 0.669
2SLS with clustered standard error                 0.906 0.809 0.731




                                72
        Table 17: Testing for Zero Variances — Standard 2SLS

Significance level                                  1%    5%   10%
2SLS with heteroskedasticity-robust standard error 0.746 0.625 0.556
GLS estimator and standard errors                  0.738 0.618 0.542
2SLS with clustered standard error                 0.740 0.617 0.547




                                73
        Table 18: Testing for Zero Variances — Corrected 2SLS

Significance level                                  1%    5%   10%
2SLS with heteroskedasticity-robust standard error 0.792 0.688 0.626
GLS estimator and standard errors                  0.773 0.680 0.613
2SLS with clustered standard error                 0.775 0.679 0.620




                                 74
  Table 19: Joint Test of Zero Means and Variances — Standard 2SLS
Significance level                                  1%    5%   10%
2SLS with heteroskedasticity-robust standard error 0.731 0.578 0.507
GLS estimator and standard errors                  0.699 0.545 0.483
2SLS with clustered standard error                 0.702 0.565 0.498




                                75
 Table 20: Joint Test of Zero Means and Variances — Corrected 2SLS
Significant level                                   1%    5%   10%
2SLS with heteroskedasticity-robust standard error 0.756 0.642 0.568
GLS estimator and standard errors                  0.738 0.631 0.548
2SLS with clustered standard error                 0.749 0.617 0.546




                                76
Table 21: Distribution of the Estimates of the Means (Lognormal Case)

     σ “ 0.3                     σ “ 0.4                      σ “ 0.5
Distribution of β0          Distribution of β0           Distribution of β0




Distribution of β¯1x        Distribution of β¯1x         Distribution of β¯1x




Distribution of β¯2x        Distribution of β¯2x         Distribution of β¯2x




Distribution of β¯3x        Distribution of β¯3x         Distribution of β¯3x




Distribution of β¯p         Distribution of β¯p          Distribution of β¯p




                                 77
 Table 22: Distribution of the Estimates of the Variances (Lognormal Case)

        σ “ 0.3                      σ “ 0.4                      σ “ 0.5
Distribution of Varpβ1x q    Distribution of Varpβ1 q     Distribution of Varpβ1 q




Distribution of Varpβ2x q    Distribution of Varpβ2 q     Distribution of Varpβ2 q




Distribution of Varpβ3x q    Distribution of Varpβ3 q     Distribution of Varpβ3 q




Distribution of Varpβ p q    Distribution of Varpβ p q    Distribution of Varpβ p q




                                     78
Table 23: Distribution of the Estimates of the Third-order Moments (Lognormal
Case)

          σ “ 0.3                     σ “ 0.4                    σ “ 0.5
  Distribution of β1x 3rdM    Distribution of β1x 3rdM   Distribution of β1x 3rdM




  Distribution of β2x 3rdM    Distribution of β2x 3rdM   Distribution of β2x 3rdM




  Distribution of β3x 3rdM    Distribution of β3x 3rdM   Distribution of β3x 3rdM




  Distribution of β p 3rdM    Distribution of β p 3rdM   Distribution of β p 3rdM




                                     79
             Table 24: Summary Statistics for the Lognormal Case

            Parameter                                           Scenarios
                σ                             σ “ 0.3            σ “ 0.4            σ “ 0.5
         Moments included:                   2          3      2          3         2         3
                                           -1.03      -1.01  -1.06      -1.03     -1.10     -1.06
              β0 “ ´1
                                         (0.013)    (0.080) (0.021) (0.092)     (0.031)   (0.107)
                                           1.49       1.49    1.47       1.48      1.44      1.45
              β¯1x “ 1.5
                                         (0.032)    (0.057) (0.056) (0.072)     (0.086)   (0.096)
                                           1.49       1.49    1.47       1.47      1.44      1.45
              β¯2x “ 1.5
                                         (0.033)    (0.054) (0.057) (0.070)     (0.088)   (0.093)
                                          0.499       0.502  0.497      0.502    0.496      0.501
              β¯3x “ 0.5
                                         (0.020)    (0.039) (0.036) (0.047)     (0.056)   (0.060)
                                          -0.976     -0.991 -0.946 -0.972        -0.906    -0.940
              β¯p “ ´1
                                         (0.014)    (0.076) (0.020) (0.084)     (0.027)   (0.095)
                                          0.153       0.159  0.229      0.241    0.295      0.316
    Varpβ1x q “ 0.212{0.390{0.639
                                         (0.020)    (0.030) (0.039) (0.048)     (0.062)   (0.072)
                                          0.153       0.160  0.228      0.244    0.294      0.319
    Varpβ2x q “ 0.212{0.390{0.639
                                         (0.020)    (0.028) (0.038) (0.045)     (0.060)   (0.067)
                                          0.025       0.030  0.045      0.036    0.068      0.056
     Varpβ3x q “ 0.04{0.043{0.071
                                         (0.014)    (0.033) (0.025) (0.041)     (0.038)   (0.059)
                                          0.0579      0.077  0.081      0.115     0.099     0.145
    Varpβ p q “ 0.094{0.174{0.284
                                         (0.007 )   (0.095) (0.011) (0.11)      (0.016)    (0.12)
                                                      0.044            0.096               0.160
   3rdM pβ1x q “ 0.093{0.322{0.894
                                                    (0.067)           (0.091)             (0.135)
                                                      0.043            0.097               0.161
   3rdM pβ2x q “ 0.093{0.322{0.894
                                                    (0.082)           (0.101)             (0.130)
                                                    -0.0073            -0.012              -0.015
   3rdM pβ3x q “ 0.003{0.012{0.033
                                                    (0.070)           (0.084)             (0.120)
                                                    -0.0095            -0.018              -0.024
3rdM pβ p q “ ´0.027{ ´ 0.096{ ´ 0.265
                                                    (0.050)           (0.058)             (0.066)




                                         80
Table 25: Distribution of Three Estimates of the Means — Normal and Lognormal

          Normal                    Lognormal
     Distribution of β0          Distribution of β0




     Distribution of β¯1x       Distribution of β¯1x




     Distribution of β¯2x       Distribution of β¯2x




     Distribution of β¯3x       Distribution of β¯3x




     Distribution of β¯p        Distribution of β¯p




                                     81
Table 26: Distribution of Three Estimates of the Variances — Normal and Lognormal

           Normal                     Lognormal
   Distribution of Varpβ1x q    Distribution of Varpβ1x q




   Distribution of Varpβ2x q    Distribution of Varpβ2x q




   Distribution of Varpβ3x q    Distribution of Varpβ3x q




   Distribution of Varpβ p q    Distribution of Varpβ p q




                                        82
