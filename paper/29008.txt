                              NBER WORKING PAPER SERIES




                                COLLECTIVE PROGRESS:
                               DYNAMICS OF EXIT WAVES

                                        Doruk Cetemen
                                         Can Urgun
                                         Leeat Yariv

                                      Working Paper 29008
                              http://www.nber.org/papers/w29008


                    NATIONAL BUREAU OF ECONOMIC RESEARCH
                             1050 Massachusetts Avenue
                               Cambridge, MA 02138
                                    July 2021




We thank Arjada Bardhi, Alessandro Bonatti, Francesc Dilme, Hari Govindan, Faruk Gul,
Toomas Hinnosaar, Alessandro Lizzeri, Chiara Margaria, Pietro Ortoleva, Wolfgang Pesendorfer,
and Heikki Rantakari for helpful comments and suggestions. We also thank seminar audiences at
Central European University, Collegio Carlo Alberto, Princeton, Seoul National University, Tel-
Aviv University, UCLA, University of Rochester, and VSET. Yariv gratefully acknowledges
financial support from the National Science Foundation through grants SES-1629613 and
SES-1949381. The views expressed herein are those of the authors and do not necessarily reflect
the views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

© 2021 by Doruk Cetemen, Can Urgun, and Leeat Yariv. All rights reserved. Short sections of
text, not to exceed two paragraphs, may be quoted without explicit permission provided that full
credit, including © notice, is given to the source.
Collective Progress: Dynamics of Exit Waves
Doruk Cetemen, Can Urgun, and Leeat Yariv
NBER Working Paper No. 29008
July 2021
JEL No. C73,D81,D83,O35

                                          ABSTRACT

We study a model of collective search by teams. Discoveries beget discoveries and correlated
search results are governed by a Brownian path. Search results' variation at any point---the search
scope---is jointly controlled. Agents individually choose when to cease search and implement
their best discovery. We characterize equilibrium and optimal policies. Search scope is constant
and independent of search outcomes as long as no member leaves. It declines after departures. A
simple drawdown stopping boundary governs each agent's search termination. We show the
emergence of endogenous exit waves, whereby possibly heterogeneous agents cease search all at
once.

Doruk Cetemen                                    Leeat Yariv
Collegio Carlo Alberto                           Department of Economics
Piazza Vincenzo Arbarello, 8                     Princeton University
Torino, TO 10122                                 Julis Romo Rabinowitz
Italy                                            Princeton, NJ 08544
doruk.cetemen@carloalberto.org                   and NBER
                                                 lyariv@princeton.edu
Can Urgun
Department of Economics
Princeton University
Julis Romo Rabinowitz
Princeton, NJ 08544
curgun@princeton.edu




A data appendix is available at http://www.nber.org/data-appendix/w29008
1     Introduction
Discoveries are often made by teams. Advances in motor vehicles, communication
devices, and pharmaceuticals frequently take place as joint ventures. Understand-
ing collective progress is therefore vital for the analysis of innovation. Much of the
literature on teamwork has focused on experimentation models, starting from the
canonical work of Bolton and Harris (1999) and Keller, Rady, and Cripps (2005).
Those models center on teams' efforts to ascertain whether one direction or project
is superior to another. Nonetheless, many discovery processes follow a path of
search. Building on past discoveries, teams come up with new ones. Furthermore,
there is a richness of dynamics in collective efforts not captured in prior models--
alliances tend to dissolve over time, with exiting members exploiting knowledge
accrued during their collaborations.
    This paper offers a new framework for studying collective progress based on
a process of search. We identify how the breadth of search and decisions to ter-
minate search vary with members' characteristics, the synergies in place. We also
show that exit waves, where multiple members halt search simultaneously, are an
inherent feature of such processes.
    Technological developments rarely occur in a vacuum and discoveries build
on one another. We therefore consider environments in which search results are
correlated over time and follow a Brownian path, as first modeled by Callander
(2011). The scope of search, captured by the Brownian path's instantaneous vari-
ance, is chosen at each moment by the searching alliance. Specifically, each mem-
ber of a searching alliance incurs a strictly positive cost that depends on that mem-
ber's own search scope. While jointly searching, the Brownian path's instantaneous
variance corresponds to the sum of members' search scopes. Any member can ter-
minate her search at any point. A member ceasing her search receives a lump sum
payoff corresponding to the maximal value the search has produced till her depar-
ture. Certainly, some alliance members may choose to continue their search even
after other members have exited. These remaining members experience prolonged
search costs, but benefit from any further breakthroughs, as reflected by search re-
sults that exceed the previously-observed maxima. As search progresses, members
gradually terminate their search until it halts altogether.1
    1 Most   of our qualitative results carry over when introducing penalties for later exits, though


                                                   1
    We characterize equilibrium search in Markov strategies, where state variables
correspond to the current search results, the attained maximum, and the active
alliance. We show that, in any active alliance, search scope is constant and inde-
pendent of search results as long as no member leaves. Individual search scopes
increase when members depart, reflecting the more limited free-riding opportu-
nities present. The optimal time at which members depart and alliances shrink is
governed by a simple stopping boundary, often referred to as a drawdown stop-
ping boundary. Such boundaries are defined by one number, the drawdown size.
Whenever search results fall by more than the drawdown size relative to the max-
imal observation achieved, a subset of members ceases search.
    The ratio of marginal to fixed costs governs both equilibrium search scopes
and drawdown sizes and serves as a proxy for the synergies present in an alliance.
In particular, agents may prefer to team up with others exhibiting both higher
marginal and fixed costs, provided the ratio guarantees they are more willing to
contribute to the collective search.
    Relative to an individual searching on her own, standard free-riding motives
drive search scopes down in an alliance. This is a form of a discouragement effect,
whereby members do not search as intensely when they expect others to bear some
of the search costs. Nonetheless, externalities make search more valuable in a
team: a member can reap the benefits of her peers' efforts. There is therefore also
an encouragement effect, reminiscent of that present in experimentation settings,
that leads team members to search for longer than they would have on their own.
    Our equilibrium characterization allows us to identify members' patterns of
exits. In general, those exhibiting high ratios of marginal to fixed costs leave ear-
lier than those exhibiting low such ratios. We show that, even when individual
costs are fully heterogenous, clustered exits, or exit waves, may occur in equi-
librium. Importantly, while the precise timing of exit waves may depend on the
realized path of discoveries, their sequencing--who leaves first, second, etc., and
with whom--does not.
    Beyond its substantive implications, our equilibrium characterization offers a
technical contribution. As we detail in our literature review below, extant analyses
of single-agent search processes often resort to modeling short-lived agents, absent
naturally such penalties alter exit patterns. In particular, penalties for later exits can introduce
exit waves mechanically--once one agent departs, others may follow suit to avoid penalties.



                                                 2
any controls. In contrast, we analyze the evolution of collective search by forward-
looking and sophisticated agents who can utilize a costly control.
    In the last part of the paper, we characterize the socially optimal search scope
and stopping policies. The socially optimal search scope is also constant and in-
dependent of search results within any active alliance. Naturally, the positive ex-
ternalities induced by each member's investment in search scope imply that the
socially optimal level is higher than that chosen in equilibrium. Furthermore, in
contrast to equilibrium search scopes, as alliance members terminate their search,
the optimal scope of those remaining declines. Optimal exits are governed by
drawdown stopping boundaries, although the drawdown sizes corresponding to
each active alliance differ from those determined in equilibrium--optimal draw-
down sizes are larger, corresponding to longer search durations.2 In terms of exit
waves, clustered exits may be optimal even when individuals incur fully heteroge-
neous costs. As in equilibrium, the sequence of optimal exit waves is deterministic
and independent of the realized search path. However, optimal exit waves may
differ substantially from those induced in equilibrium.
    Finding the optimal sequence of exit waves is a challenging combinatorial prob-
lem. A social planner needs to consider all possible ordered partitions of the orig-
inal searching team and assess search outcomes from the corresponding exit wave
sequences. We show a simple method for identifying the optimal sequencing for
one class of settings, when individual search costs are proportional to one another.
Similar to equilibrium, the social planner terminates the search of those with the
highest search costs first. This limits the exit wave sequences to consider. We il-
lustrate a simple procedure, akin to a greedy algorithm (see, e.g., Papadimitriou
and Steiglitz, 1998) that yields the optimal exit wave sequence. In rough terms,
the social planner can use a recursive procedure, first identifying the optimal last
alliance to search--the alliance that would generate the highest welfare when all
members are constrained to stop jointly. Once that alliance is identified, the social
planner can find the optimal penultimate alliance. And so on. The procedure al-
lows us to highlight settings in which equilibrium exit waves differ substantially
from those set optimally.
   2 As we show, allowing for non-Markovian equilibria does not eliminate some of the inefficien-

cies we highlight.




                                               3
2     Literature Review
Since Weitzman (1979), much of the search with recall literature has focused on
individual agents' discovery process, where the set of options is independent of
one another. Our consideration of a Brownian path of discoveries, capturing in-
tertemporal correlations, is inspired by the setting studied by Callander (2011).
He studies short-lived agents who decide whether to choose an optimal, previ-
ously explored, result or experiment on their own. Most of the work that ensued
considers behavior of short-lived agents as well. Urgun and Yariv (2021) analyze
an individual-search setting similar to the one analyzed here.
    In recent years, substantial attention has been dedicated to the study of col-
lective experimentation. Much of this literature focuses on learning spillovers be-
tween team members. For instance, the classic papers of Bolton and Harris (1999),
Keller et al. (2005) extend the two-armed bandit problem to a team setting, where
agents learn from others. Information is a public good. Thus, there is a free-rider
problem that discourages experimentation. Nonetheless, there may also be an en-
couragement effect through the prospect of others' future experimentation. See
Hörner and Skrzypacz (2016) for a survey.
    Another strand of literature inspects settings in which stopping is determined
collectively. Albrecht, Anderson, and Vroman (2010) and Strulovici (2010) con-
sider sequential search and experimentation, respectively, where a committee votes
on when to stop. They illustrate when collective dynamics may impede search or
experimentation. Bonatti and Rantakari (2016) offer a model in which agents exert
effort on different projects but stop experimentation jointly. Optimally, one agent
advances her preferred project quickly. Her opponent agrees to early advanced
projects in order to limit effort. Deb, Kuvalekar, and Lipnowski (2020) take a de-
sign perspective--for a given deadline at which a project has to be chosen, the
principal commits to a selection rule. Titova (2019) studies a public-good setting
in which a team decides whether to implement a public good. Payoffs are revealed
through a Pandora's box problem à la Weitzman (1979). Optimal information and
projects are selected, but free-riding may generate inefficient delays.3
    There are also several papers illustrating patterns reminiscent of the clustered
    3 Dynamic contribution games without experimentation or uncertainty have also been heavily
studied, see for instance Admati and Perry (1991), Marx and Matthews (2000), Yildirim (2006),
and Cetemen, Hwang, and Kaya (2020).


                                              4
exits we characterize, mostly in settings in which agents have private information.
Bulow and Klemperer (1994) consider a seller who dynamically reduces the price
of identical goods until demand meets supply. Agents have independent valu-
ations and decide if and when to buy. In equilibrium, frenzies, where multiple
agents buy at the same price, may occur. Caplin and Leahy (1994) study a three-
period irreversible-investment game in which each firm receives private informa-
tion on the aggregate state of the economy as well as observes others' prior deci-
sions. Firms' actions reveal information and can generate a wave. Gul and Lund-
holm (1995) analyze a two-agent model in which both try to predict the value of a
project using their private information. Each decides when to issue a prediction,
where delay entails a flow cost. The timing of decisions is then informative and
clustered predictions occur in equilibrium. Rosenberg, Solan, and Vieille (2007)
study a multi-agent version of the standard real-options problem (see Dixit and
Pindyck, 1994). Agents observe private signals about common returns to a risky
project, as well as the actions of others. If one agent switches to a safe project--
namely, exercises an option--this can lead the other agent to immediately switch to
the safe project as well. See also Murto and Välimäki (2011) and Anderson, Smith,
and Park (2017). In a static information-collection setting, Bardhi and Bobkova
(2021) characterize optimal subsets, or mini-publics, to be activated.4
    The techniques we develop relate to the applied mathematics literature on op-
timal stopping, see Peskir and Shiryaev (2006) and Azéma and Yor (1979) for par-
ticularly relevant sources.


3      A Model of Collective Search
Consider a team of N agents--product developers, academic researchers, etc.--
searching through a terrain of ideas in continuous time. Time is indexed by t and
runs through [0, ). Each seeks good outcomes and ultimately benefits from the
maximal value they have found when they stop their search. Formally, we assume
all agents are risk neutral. At each time t , agent i = 1, 2, ..., N decides on the scope
    4 Thereis also a literature that tries to explain industry "shakeouts," corresponding to times at
which firm numbers plummet, absent a decline in output. For example, Jovanovic and MacDonald
(1994) suggest shakeouts result from exogenous technological shocks. Initially, firms enter new
profitable markets. Profits decrease as more firms enter. When there is a technological shock, some
firms become more productive than others, potentially leading to clustered exits.


                                                 5
                A
of her search i,t  [ ,  ], where    > 0 and A  {1, ..., N } is the alliance of agents
still searching at time t . Agents' scope choices are observed within their alliance.
As we soon describe, the search scope naturally feeds into the breadth of search
conducted by the alliance of active agents. For i = 1, 2, . . . N , any search scope 
comes at a cost of ci ( ), where ci is twice continuously differentiable, increasing,
and convex, with a second derivative bounded above zero over [ ,  ]. The special
case of  =  corresponds to settings in which search scope is not controlled and
agents' only choose when to stop search.
    We model the progress of discoveries using a Weiner process, which allows us
to capture the correlation of new developments over time, and the impact of search
scope of those who engage in search.5 Formally, for any time t , denote by Bt the
standard Brownian motion with B0 = 0, and let tA denote the controlled breadth
of search, which will depend on the search scopes of all members of the active
alliance A as we soon describe. The observed value at t --which can be thought of
as the expected value of the discovery--is denoted by Xt , where X0 = 0 and the law
of motion is given by:

      dXt = tA dBt .

Whenever the alliance A of agents is searching, we assume tA = i A i,t        A 6
                                                                                .
    The search scope can be interpreted in two ways. First, it can capture search
breadth. Investment in development, through acquisition of instruments or expert
time, often entails an increase in risk: it either leads to substantial leaps, or to more
pronounced losses. Second, given our modeling of search values, the search scope
can also be thought of as capturing search speed. Changing the search scope from
1 to  at any small interval of time is tantamount to "speeding up" the process by
a factor of  2 . As we soon show, search returns depend linearly on search scope.
    We assume the discovery process exhibits no drift: in applications, the mere
passage of time rarely improves or worsens search outcomes over standard hori-
zons of research and development. Naturally, one could consider a team that con-
   5 We view correlation as an important feature of discovery processes.     Nonetheless, from a purely
theoretical perspective, one could analyze an analogous model with independent samples. As it
turns out, such a model is far less tractable. Details appear in the Online Appendix.
   6 In the Online Appendix, we show that our analysis can be directly extended to the case in

which, for any alliance A, we have tA = f A ({i,t
                                                A
                                                  }i A ), with f A a differentiable function. Compara-
tive statics would naturally depend on alliances' technologies captured by {f A }A .



                                                  6
trols drift rather than search scopes, which would also translate to the returns of
search with recall. The analysis would follow similar lines to those we describe, al-
though with an important loss in tractability.7 We view endogenous search scopes
as natural for most applications, where investments in innovation either affect the
speed at which progress is made, or entail non-trivial risks.


3.1     Payoffs
Each agent is rewarded according to the maximal project value observed up to her
stopping time. Let Mt denote the maximum value observed by time t :

      Mt = max Xr  M0 ,
              0r t

where we assume that M0 = 0.
                                                                  
   For any aggregate fixed search scope  , at time t , E(Mt ) =  2t/. Thus, the
choice of search scope translates directly to the expected returns from search.
   When any agent i stops at time  , her resulting payoff is given by
                
      M -          ci (i,t )dt ,
               0

where i,t is the timed search scope of individual i , which may depend on the
alliances she is active in.8 Any progress made after an agent stops searching does
not impact her payoffs.
    Agents observe one another's search. In particular, whenever agents stop search-
ing, other agents realize their search will continue within a smaller alliance.


3.2     Strategies and Equilibrium
At any time t , the state of the environment is summarized by Xt , Mt , and At , where
At is the active alliance of agents still searching.
    A strategy for agent i dictates her chosen search scope over time and her stop-
ping policy. Formally, it is a pair of functions (iA , iA ), where A  {1, ..., N } and
i  A. In principle, (iA , iA ) may depend on time, as well as the entire path of
   7 Taylor et al. (1975) characterize the maximal value of search with constant drift. The resulting
value is far less amenable to further analysis than ours.
   8 In Section 6.1, we discuss an extension in which agents who stop later are penalized.




                                                 7
observed search values and corresponding maxima. Let {Ft } denote the natural fil-
tration induced by the governing Brownian motion. Agents' strategies are adapted
to this filtration.
     We restrict attention to Markov strategies. That is, we assume agents use strate-
gies of the form (iA , iA ) that depend only on the state variables Xt , Mt , and At .
Formally, iA : R2  [ ,          ¯ ], and iA is a random variable over R+ such that Pr(iA =
t |Ft ) = Pr(iA = t |Xt , Mt ) for all i .9
     We further assume that a continuous stopping boundary determines when each
agent halts her search. Formally, for all i and all alliances A such that i  A, the
stopping policy takes the following form:

       iA = inf{t  0 : Xt = giA (Mt )},

where giA (·) is a continuous function. This formulation implicitly implies that,
upon indifference, agents exit the search. Our assumption that stopping bound-
aries are continuous is without loss of generality as long as any agent is willing to
search on her own, which we show in the Online Appendix. As we soon show, in
our setting, departing agents would never benefit from continuing the search in a
smaller alliance: the externalities offered by a larger alliance are always beneficial.
    Given (jA , jA ) , agent i 's best-response strategy simply maximizes her ex-
                     j i
pected payoff given this profile. Formally, it is determined by solving the following
problem for each alliance A such that i  A:
                                       A
                                                     
                                              A      
        sup E ( A , A )      M A -        ci i,t  dt .
                                                     
                                                     
                   j  j
             
                            
            A
        A , i,t     i     j A\{i }        0
                  t =0

An equilibrium is a profile of Markov strategies satisfying the assumptions above
and constituting best responses for all agents.


4      Equilibrium Team Search
In this section, we characterize the outcomes of team search. We describe the equi-
librium search scopes and stopping boundaries. We also identify the sequencing
of agents' search termination, and the patterns of equilibrium exit waves.
    9 Theinefficiencies we highlight do not vanish when considering equilibria in non-Markovian
strategies, see our discussion in Section 6.2.


                                              8
4.1   Equilibrium Characterization
Given our restriction on agents' strategies, it follows that any alliance A gets smaller
at the minimal stopping time of its members. That is, the time  A at which the first
members of A stop search is given by  A = mini A iA . Equivalently,

       A = inf{t  0 : Xt = max giA (Mt )}.
                                 i A

Since agents use continuous stopping boundaries, we can write

       A = inf{t  0 : Xt = g A (Mt )},

where g A (Mt )  maxi A giA (Mt ) is continuous.
   We start by identifying equilibrium search scopes. Individual search scopes
depend only on the active alliance and are constant as long as no member departs.

Proposition 1 (Team Search Scope). For any agent i in an active alliance A, equi-
librium search scopes are constant, iA (Mt , Xt ) = iA . Whenever interior, search scopes
satisfy the system:

      2ci (iA )
                  = A =         iA     i  A.
      ci (iA )            i A

   Why are search scopes constant as long as a certain alliance of agents is active?
The rough intuition is the following. Consider an agent i in an active alliance
A. Suppose i believes that all other agents j in the alliance search with scope jA .
When away from agent i 's stopping boundary, agent i can contemplate a small
interval of time in which she is unlikely to hit her stopping boundary. For that
                                                                            A 2
small interval, agent i considers the induced speed of the process:    k A k    and
                          A
the cost she incurs, ci (i ). Ultimately, the agent aims at minimizing the cost per
                                                                              ci (iA )           ci (iA )
speed, or the overall cost to traverse any distance on the path,                         2   =   ( A )2
                                                                                                          .
                                                                     ( kA kA )
The identity in the proposition reflects the corresponding first-order condition.
    In general, there might be multiple solutions to the system in Proposition 1,
some possibly corresponding to less efficient equilibria. Nonetheless, Fleming and
Rishel (2012) (Theorem 6.4) guarantees that any equilibrium features continuous
search scopes within any alliance. Hence, within an active alliance, agents can
utilize only one of the solutions.10 Importantly, it is the ratio of costs to marginal
 10 The   conditions of Theorem 6.4 in Fleming and Rishel (2012) follow from our assumption that

                                                9
costs that govern equilibrium search scopes. In particular, in our setting, teaming
up with agents who have both higher costs and marginal costs can be beneficial in
terms of externalities.
   Whenever interior solutions to the system in Proposition 1 are unique, com-
parisons of search scopes within various alliances are well defined. Uniqueness of
interior solutions is guaranteed when, e.g., all scope costs are log-convex. A direct
corollary of Proposition 1 is then the following.

Corollary 1 (Search Scope and Alliance Size). Suppose costs are log-convex and an
interior solution exists for the systems specified in Proposition 1. As an alliance shrinks,
individual members' search scopes increase, while total search scope decreases. That is,
                            A\{j }
for any i, j  A, we have i          iA while  A >  A\{j } .

    The corollary highlights a form of free-riding. Search scope is substitutable
across individuals. The more agents searching, the less each one searches. Since
individual search scopes decrease within an alliance, the total search scope in any
active alliance is smaller than that which would be generated by the alliance's
members searching independently.11
    The corollary indicates that agents departing would never benefit from contin-
uing search on their own, nor from switching to search in a smaller alliance than
the one they have left. In particular, our assumption that agents who cease search
in an alliance reap the benefits from past discoveries rather than pursue further
discoveries with other newly-departed agents is without loss of generality.
    We now turn to the characterization of equilibrium stopping boundaries. Agents
cease their search whenever search results fall by more than a set amount relative
to the observed maximum. Consequently, the order in which agents terminate
their search is fixed and does not depend on the realized path of search values.

Proposition 2 (Alliance Stopping Boundary). There exists an equilibrium such that,
for any agent i in any active alliance A,

                          ( A )2
      giA (M ) = M   -               .
                         2ci (iA )
the cost function's second derivative is bounded above zero. For an alliance composed of one
individual, there is a unique optimal solution due to concavity of the objective function.
  11 Any agent receives a higher payoff within an alliance than she would on her own. Indeed, any

agent can emulate her solo-search policy in an alliance and guarantee at least as high a payoff.


                                               10
                                       ( A )2
In particular, agent i  arg minj                  is the first to stop in any alliance A. Further-
                                      2cj (jA )
more, given equilibrium search scopes, there is a unique equilibrium in which stopping
boundaries are weakly undominated.

    Stopping boundaries of the form g (M ) = M - d are often termed drawdown
stopping boundaries with drawdown size of d . In equilibrium, agents stop whenever
the gap between the observed maximum and the current observation exceeds their
drawdown size, as identified in the proposition.
    To glean some intuition for the structure of the equilibrium stopping bound-
ary, consider some alliance A and suppose all agents believe that other members
of the alliance will continue searching indefinitely with search scopes given by
Proposition 1. Each individual agent i 's optimization problem then boils down
to a solo searcher's optimization, with others' search simply affecting the experi-
enced search costs. Namely, the induced cost of implementing search scope  is
ci ( - j A,j i jA ). Since agent i 's optimization problem is identical when observ-
ing X and M , or X + k and M + k for any arbitrary constant k , her stopping boundary
must coincide as well and hence takes the form of a drawdown stopping boundary,
see Urgun and Yariv (2021) for further details. Denote the corresponding draw-
down size by diA . Suppose diA = minj A djA . Consider then another iteration of best
responses, where all agents use the drawdown stopping boundary calculated as
above. Agent i would still be best responding since, from her perspective, others
in the alliance would continue searching for as long as she does. Furthermore,
while other agents may want to alter their stopping boundary, intuitively, none
would want to cease search before agent i since that would contradict their desire
to continue searching for at least as long as agent i in the first place.
    This line of argument suggests that, given equilibrium search scopes, the stop-
ping boundary of the first agent i to terminate search in any alliance A is deter-
mined uniquely when focusing on equilibria in which stopping boundaries are
weakly undominated.12 Multiplicity of equilibria arises from the stopping bound-
aries of other agents j  A. Indeed, any agent j who stops strictly after agent i is
indifferent across all stopping boundaries gjA (·) that satisfy gjA (M ) > giA (M ) for all
M . Naturally, all such choices of stopping boundaries by agents other than i do
  12 Thefocus on weakly undominated stopping boundaries--given the equilibrium search
scopes--allows us to rule out inefficient equilibria that are an artifact of coordination failures,
with multiple agents stopping at an earlier time than desired since other alliance members do so.


                                                   11
not impact when the alliance first loses some of its members, nor the search scope
while it is fully active. Consequently, equilibrium outcomes are unique.13


4.2     Equilibrium Exit Waves
When all agents have the same costs and solutions are interior, equilibrium takes
a simple form. Team members choose identical search scopes, as determined by
Proposition 1. They also leave in unison--there is only one exit wave. Proposition
2 suggests that joint departures may occur even when individual costs differ.
    To see how those happen, consider any active alliance A. Suppose agent i is first
to exit: diA = minj A djA . Let Z 1 = {i }. Now consider the alliance A Z 1 resulting
from i 's departure. For all remaining agents, there is then a new drawdown that
                                                                             1
governs their decision to stop search. These new drawdowns are {djA Z }j A Z 1 .
The discrete drop in overall search scope induced by i 's departure may imply that
     1
djA Z  diA for some j  A Z 1 . Let Z 2 correspond to all these agents together
with agent i . It follows that, as soon as agent i terminates her search, so will
all other agents in Z 2 . We can continue this process recursively to identify the
clustered exits that occur in equilibrium. Their characterization depends only on
the magnitudes of the drawdown sizes identified in Proposition 2. In particular,
they are identified deterministically. Thus,

Corollary 2 (Equilibrium Exit Waves). The order of exists is deterministic, while exit
times are stochastic.

   Our description above suggests that one agent leaving may trigger the depar-
ture of multiple agents--a form of snow-balling effect. This implies that targeted
interventions, subsidizing the search of only particular agents, may impact the
entire path of exit waves.


4.3     Well-ordered Costs
We now consider a particular setting, where the identification of exit waves and
their comparative statics is particularly simple.
  13 Our analysis indicates a link to other cooperative solution concepts in the spirit of the core. At

any point in time, were active agents free to form any coalition to pursue search, or cease search,
the externalities present in our environment would imply a unique outcome corresponding to the
equilibrium outcome we identify.

                                                 12
    Suppose agents' cost functions are proportional to one another: c = c1 1 =
c2 2 · · · = cN N , where 1 = 1 < 2 < ... < N . That is, agent 1 has the highest
search costs, while agent N has the lowest search costs.
    Proposition 1 implies that all agents in an active alliance choose the same
search scope, assuming an interior solution exists. Suppose  denotes the search
                                                                   2c( )
scope all agents utilize in the full alliance. It follows that c ( ) = N  .
    As N increases, individual search scopes decrease. Let N         ^ be the maximal inte-
                2c( )  ^  . For any N > N    ^ , there is no interior equilibrium. Further-
ger such that c ( ) < N
more, when there are N   ^ agents in the team, individual search scopes are initially
roughly at their minimum  , while overall search scope is N          ^ .
    Agents' search scope changes only when their alliance shrinks. In this special
case, we can pin down the weak order by which agents stop their search without
calculating their corresponding drawdown sizes, which greatly simplifies the anal-
ysis. Specifically, Proposition 2 implies that agent N exits no sooner than agent
N - 1, who exits no sooner than agent N - 2, and so on. In equilibrium, agents
with higher costs terminate search earlier. Can non-trivial exit waves occur when
agents' costs are strictly ordered?
    Consider any active alliance {j, . . . , N }. If
          {j,...,N }         {j +1,...,N }      {j +2,...,N }        {j +k,...,N }
        dj              dj +1                , dj +2            , dj +k               ,

then agents j, j + 1, j + 2, ..., j + k will all terminate their search at the same time.
Figure 1 depicts an example for N = 10 individuals. In the figure, once agent 1
leaves, agents 2 and 3 leave as well. Similarly, once agent 4 leaves, agent 5 leaves.
And so on. Ultimately, the drawdowns that govern agents' departures correspond
                                                        {j,...,N }
to the "upper envelope" of the graph depicting dj                  as a function of j .
    Despite agents' costs being strictly ordered, clustered exits are possible. In
fact, when costs are close to one another, all agents might exit at once. Indeed,
                                       {j...N }        ( {j,...,N } )2 j
from Proposition 2, dj                            =             {j,...,N }       . From Corollary 1,  {j,...,N } decreases in j ,
                                                       2c(j                  )
              {j,...,N }
while j                    increases in j . Therefore, for {j } sufficiently close to one another,
 {1,...,N }       {2,...,N } N
d1       >       d2 > ... > dN and all agents exit at once. Naturally, when costs are
sufficiently far from one another, agents exit at different points.
     A decrease in 1 , keeping c1 1 and all other parameters fixed, increases the
agent 1's search costs and leads to her earlier search termination, potentially too


                                                                                 13
            Figure 1: Equilibrium exit waves with well-ordered costs



soon for other agents to exit. Consequently, the number of exit waves weakly in-
creases. In contrast, a decrease in N , keeping cN N and all other parameters fixed,
increase agent N 's search costs, making her more inclined to exit when agent N - 1
does. Consequently, the number of exit waves weakly decreases.


5     The Social Planner's Problem
We now consider a social planner who dictates agents' search scopes and exit poli-
cies to maximize overall utilitarian efficiency of the team. This analysis highlights
the type of inefficiencies that strategic forces in our joint search process imply.


5.1   The Social Objective
The social planner aims to maximize the agents' expected utilitarian welfare. The
instruments at her disposal are the times at which various agents exit--the se-
quence of active alliances--and the search scopes within each active alliance.


                                         14
    Standard arguments allow us to restrict attention to Markovian policies for the
social planner, see Puterman (2014). Formally, we consider a Markov decision
problem in which the state at each date t is three-dimensional and comprising
(i) the set of active agents At , (ii) the current maximum Mt , and (iii) the current
observed project value Xt . The social planner chooses a continuation alliance of
agents--a subset of the current alliance At --and the search scope of each member
in that alliance.
    The social planner has two Markovian controls. The first pertains to the selec-
tion of a continuation alliance, and denoted by G(M, X, A) : R2 × 2N  2A . The
mapping G determines the subset of agents continuing the search as a function of
the current state. In particular, if G(M, X, A) = A, the current alliance continues the
search. If  G(M, X, A) A, the alliance reduces in size. Whenever G(M, X, A) = ,
no agent is left searching and the search terminates.
    The social planner's second control is the profile of search scopes within any
alliance A, which can be written as iA (M, X ) : R2  [ ,   ¯ ] for each i  A. As before,
agents that already exited cannot be induced to choose positive search scope and
do not participate in any future search: exit is irreversible. We therefore write
iA (M, X ) = 0 for each i A. For any active alliance A, we write:

      A (M, X ) =         iA (M, X )
                    i A

and, as a shorthand, we drop the arguments when there is no risk of confusion.
    Given these controls, we can now associate a stopping time for each active al-
liance A. This is the first time at which the alliance shrinks in size. That is:

      A = inf{t  0 : G(Mt , Xt , A)         A}.                                      (1)

If an alliance A is never reached, we set  A = 0.
    Let A~ t denote the induced process of active alliances. For any active agent i ,
the time at which her search stops is given by

     i = inf{t  0 : i                   ~ t )}.
                            G(Mt , Xt , A

This is the first time at which agent i is not included in an active alliance.




                                                  15
                                                ~ t , given the controls {G, i }, is
   At any time t , the welfare of individual i  A
                                                   i
                                                            ~
                                                            A
                    ~ t |i , G) = E M -
      Wi (Mt , Xt , A                                  ci (i,ss )ds .
                                     i
                                               t

For any i    ~ t , we set Wi (Mt , Xt , A
             A                          ~ t |i , G) = 0. The social planner's problem is then:
                                                                                                i
                                                                                                         ~
                                                                                                         A
                   ~ t ) = sup
      W (Mt , Xt , A                                ~ t |i , G) = sup
                                      Wi (Mt , Xt , A                              E Mi -           ci (i,ss )ds .
                          {G,i } i                                      {G,i } i            t

    We assume that whenever the social planner is indifferent between maintaining
a certain set of agents searching or having them exit, she chooses the latter.
    Given a pair of controls (G,  ), with slight abuse of notation, let A1 = {1, ..., N }
denote the first active alliance, containing all agents.14 Using (1), let A2 = A            ~ A1
                                                                                             
be the alliance that succeeds the initial alliance, the alliance resulting from the
first agents halting their search. In principle, A2 could entail some randomness--
depending on the path observed, different agents may be induced to stop their
search. We then use (1) to define  A2 , the (random) time at which the second set of
agents stops search and define A3 = A         ~ A2 as the (potentially random) resulting al-
                                               
liance. We continue recursively to establish the (random) time  Ak at which the k 'th
set of agents stops search and define Ak +1 = A            ~ Ak as the (potentially random) re-
                                                             
sulting alliance. Let K denote the (potentially random) number of different active
alliances the social planner utilizes till search terminates for all. For any controls
{G, i }, we then have a sequence of active alliances A1 , A2 , ..., AK with associated
stopping times  A1 ,  A2 , ...,  AK .
    Suppose our team-search problem starts at the state (M, X, A). We set  A0 = 0
and AK +1 =  so that the social planner's problem can now be written as:
                                                                  Ak
                                                                                         
                                  K                                                      
                                                                                Ak       
       W (M, X, A1 ) = sup E          | A   \ A      | M     -           c   (      ) dt  .
                                                                                         
                                          k     k +1      A                i
                                                                                         
                                                           k
                                                                 Ak -1
                                                                                i,t      
                       {G, }i
                                      
                                     k =1                                     i Ak
                                                                                         

Equivalently, we can write the problem recursively starting from any state (M, X, Ak ):
                                                                                                 
                                                  Ak                                             
                                                          Ak                                     
 W (M, X, Ak ) = sup E  | A   \ A     | M     -      c (     ) dt + W ( M     , X      , A     ) .
                                                                                                 
                            k    k +1      A          i i,t               A        A       k +1 
                                            k                               k        k           
                 {G, }i
                                                0               i Ak
                                                                                                 

   Suppose the social planner finds it optimal to halt the search of agent i in an
  14 We abuse notation by using subscripts to denote the alliance's order in the sequence, rather
than time, in order to maintain clarity and simplified notation throughout our analysis.


                                                       16
active alliance A when observing X and M . It would then also be optimal to halt
the search of this agent when observing X and M with any X < X . Intuitively, the
social planner's solution would be the same were the process shifted by a constant.
Therefore, her choice when observing value X and a maximum M is the same as
when observing X and maximum value M  M + X - X > M . As we soon show,
search scopes do not explicitly depend on the achieved maximum. Hence, when
observing X and M , were the social planner to continue agent i 's search for a small
time interval, the optimal search scopes in the active alliance would coincide with
those she would pick for the same alliance were search continued when observing
X and M . However, the likelihood of surpassing M at this small time interval is
lower than the likelihood of surpassing M . Furthermore, the social planner could
gain M from releasing agent i with the current observed maximum relative to the
lower M she would get from releasing that agent when observing X and M . Thus,
if it is optimal to halt agent i 's search when observing X and M , it is also optimal
to halt that agent's search when observing X and M .
     We can therefore write

       A = inf{t  0 : Xt = g A (Mt )},

where g A (M ) = sup{X : G(M, X, A) A}.15 This kind of stopping time  A is com-
monly known as an Azéma-Yor stopping time (Azéma and Yor, 1979), with the
function g A defining the corresponding stopping boundary.
    For any active alliance A, we note that g A (M ) < M for all M . In other words,
it is never optimal to stop that alliance at any t such that Mt = Xt . If an alliance
searches for a non-trivial amount of time at its inception, say at time t0 , it must be
that Mt0 > Xt0 . The alliance would then continue searching jointly even were the
planner to observe, at some time t , the value Xt and recorded maximum of Mt with
Xt = Mt = Mt0 . But then the same should hold when Mt = Xt = y, with arbitrary y ;
this corresponds to a shifted problem and does not alter welfare considerations.16
  15 We implicitly assume, without loss of generality, that whenever the social planner is indifferent

between halting the search of a subset of agents or continuing their search, she chooses the former.
  16 This would not hold were the social planner's objective concave in the maximum observed.

Concavity introduces new challenges, see Urgun and Yariv (2021) for a discussion of its impact on
single-agent decisions. Its investigation would be an interesting direction for the future.




                                                 17
5.2   Optimal Team Search
Our first result illustrates that the social planner chooses constant search scopes
for each active alliance. However, the specification of these search scopes differs
from that dictated by equilibrium.

Proposition 3 (Optimal Search Scope). Search scopes within an alliance are constant
and depend only on the alliance's composition. Furthermore, whenever interior, search
scopes satisfy the system:
                    A
      2    i A ci (i )
                         =         iA i  A.
           ci (iA )          i A

   The intuition for this result resembles that provided for equilibrium choices.
For any active alliance A, the social planner considers the induced speed of the
                          A 2                                  A
process, given by    k A k    and the cost she incurs, k A ck (k ). The social plan-
ner then aims at minimizing the cost per speed, or the overall cost to traverse any
distance on the path,
                   A                   A
          k A ck (k )         k A ck (k )
                        =                 .
                  A 2           ( A )2
            k A k

The identity in the proposition reflects the corresponding first-order condition.
    When costs are log-convex, the proposition implies that socially optimal search
scopes are higher than those prescribed in equilibrium. Furthermore, when al-
liance A is active, each alliance as a whole searches weakly more under the social
planner's solution. Intuitively, the social planner internalizes the positive exter-
nalities entailed by agents' contributions to the scope of search and thus specifies
greater overall search investments. Comparative statics of the socially optimal
search scopes resemble those described for equilibrium choices in Section 4.
    In equilibrium, Corollary 1 indicated that, as alliances shrink, remaining agents
increase their search scope. The impacts of agents departing are quite different in
the social planner's solution. As members depart, the externalities of each remain-
ing agent decline: there are fewer others their search scope helps. Consequently,
the socially optimal search scope of each individual agent declines. That is,

Corollary 3 (Optimal Scope and Alliance Size). Suppose costs are log-convex and
the equilibrium and social planner's search scopes are interior. Then, in any alliance,

                                              18
an agent's equilibrium search scope is lower than that agent's search scope in the social
planner's solution. Furthermore, in the social planner's solution, each agent's search
scope decreases as her alliance shrinks in size.

   The sequencing of alliances and their search duration also differ between the
social planner's solution and the corresponding equilibrium:

Proposition 4 (Optimal Alliance Sequencing). The socially optimal sequence of al-
liances is deterministic. For any deterministic sequence of alliances A1 , ..., Ak exerting
optimal search scopes, the socially optimal stopping boundaries are drawdown stopping
boundaries. That is, for each alliance Ak , g Ak (M ) = M - dAk with dAk  R+ . Furthermore,
the drawdown sizes {dAk } exhibit a recursive structure: for any k ,

                            |Ak \ Ak +1 |
      dAk =                Ak                     Ak +1
                                                            .
                  i Ak ci (i )        i Ak +1 ci (i     )
              2       Ak 2       -          Ak +1 2
                   (    )                (       )

    Why does the social planner use drawdown stopping boundaries for various
alliances? Intuitively, for any active alliance Ak , the social planner considers the
marginal group of agents Ak \ Ak +1 whose search will be terminated next. The
relevant marginal added cost per speed for that group is then
                  Ak                       Ak +1
        i Ak ci (i )          i Ak +1 ci (i      )
                       -                           .
          ( Ak )2               ( Ak+1 )2
    Each of these agents would receive the established maximum once they depart,
thereby generating a multiplier of |Ak \Ak +1 | of the maximum in the social planner's
objective. The resulting stopping boundary then emulates that of a single decision
maker, a special case of Proposition 2, with scaled up returns to each maximum
established when the alliance shrinks, and adjusted costs as above.
    To glean some intuition into the deterministic nature of the sequence of al-
liances, suppose that the social planner, starting with some active alliance A, pro-
ceeds to either alliance A or alliance A , depending on the realized path, with
A , A  A. Following our discussion above, both transitions--from A to A and
from A to A --are associated with a drawdown stopping boundary, with draw-
down sizes of d and d , respectively. If d < d , starting from alliance A, the social
planner would always shrink the alliance to A as the relevant stopping bound-
ary would always be reached first. Similarly, if d < d , the social planner would

                                                        19
always reduce the alliance to A . In other words, different drawdown stopping
boundaries never cross one another, and so the path of alliances is deterministic.
   Propositions 3 and 4 suggest that the general structure of efficient search is
similar to that conducted in equilibrium. Agents depart the search process in a
pre-specified order and do so using drawdown stopping boundaries. Furthermore,
within each active alliance, search scopes are constant over time. Nonetheless, the
optimal sequence of active alliances, their corresponding drawdown sizes, and the
search scopes do not generally coincide with those prescribed by equilibrium.
   Certainly, agents who search exert positive externalities on others searching.
Naturally, then, the social planner exploits these externalities by extending the
time individuals spend searching. In fact, the expressions derived for the optimal
and equilibrium alliance drawdown sizes imply directly the following.

Corollary 4 (Longer Optimal Search). Suppose costs are log-convex and the equilib-
rium and social planner's search scopes are interior. Consider any alliance that is active
on path in both the social planner's solution and in equilibrium. Then, the drawdown
chosen by the social planner for that alliance is weakly larger than the equilibrium draw-
down of the same alliance.

    The results of this section provide some features of the optimal solution.17
However, they do not offer a general characterization of the optimal sequence of al-
liances, which is the result of a challenging combinatorial optimization problem--
in principle, the planner needs to consider all possible exit patterns, corresponding
to ordered partitions of the team. A sharper characterization requires more struc-
ture on the environment's fundamentals. In the next subsection, we impose such
a structure and solve the social planner's problem completely, illustrating the op-
timal sequence of alliances and contrasting it with that emerging in equilibrium.


5.3    Optimal Team Search with Well-ordered Costs
Suppose, as in Section 4.3, that agents' cost functions are proportional to one an-
other and point-wise ordered: c1 1 = c2 2 = · · · = cN N , where 1 = 1 < 2 < ... < N .
    We start by showing that the social planner uses a similar sequencing of active
alliances to that used in equilibrium.
  17 In addition, in the Online Appendix, we show a recursive formulation of the social planner's

objective--the resulting welfare--in terms of the optimal drawdowns and search scopes.

                                               20
Lemma 1 (Optimal and Equilibrium Alliance Sequence). In the social planner's so-
lution, agent i never terminates search before agent j if i > j . In particular, whenever
agent i terminates search before agent j in equilibrium, the social planner terminates
agent i 's search either with, or before, agent j 's.

     Intuitively, the social planner optimally terminates the search of agents with
the highest search costs first, so agent 1's search is terminated no later than agent
2's search, which is terminated no later than agent 3's, etc. This mimics, "weakly,"
the order governed by equilibrium. Nonetheless, the social planner's sequencing
need not echo that prescribed by equilibrium since clustered exits can differ dra-
matically, as we soon show.
     It will be useful to introduce the following notation for our characterization
of the socially optimal sequence of alliances. Let Bk = {k, k + 1, ..., N } for all k =
1, ..., N . Lemma 1 and our equilibrium characterization imply that the optimal se-
quence of active alliances has to correspond to a subset of {Bk }N k =1 . This already
suggests the computational simplicity well-ordered costs allow. For instance, in-
stead of considering 2N - 1 alliances that could conceivably be the last ones active,
we need to consider only N .
     For B B, we denote by dBB the socially optimal drawdown size associated
with alliance B, when it is followed by alliance B , as described in Proposition 4.
In particular, dB denotes the optimal drawdown of an alliance B when it is the
last active alliance. We now characterize the optimal sequence of alliances.

Proposition 5 (Optimal Alliance Sequence with Well-Ordered Costs). The optimal
sequence of alliances is identified as follows:

   · There is a unique maximizer of {dBk  }N
                                           k =1 . Let L1 = arg maxdBk  . The last
                                                                 k =1,...,N
      active alliance is BL1 , with L1  N . If L1 = 1, all agents optimally terminate their
      search at the same time. Otherwise,
                                               1     L -1
   · There is a unique maximizer of {dBk BL }k =1 . Let L2 = arg max dBk BL . The
                                                 1                                 1
                                                                  k =1,...,L1 -1
      penultimate active alliance is BL2 , with L2 < L1 . If L2 = 1, there are optimally
      only two active alliances: B1 followed by BL1 . Otherwise,

   · Proceed iteratively until reach Ln , where Ln = 1. The socially optimal order of
     alliances is given by B1 , BL1 , . . . , BLn-1 .

                                            21
                       Figure 2: Socially optimal exit waves


    The optimal sequence of alliances is constructed recursively. Consider first the
case in which an alliance's search is terminated jointly. That is, once search termi-
nates for one of the alliance's members, it is terminated for all others. Our analy-
sis in the previous section suggests that, restricted in this way, the social planner
would optimally determine the stopping time using a drawdown stopping bound-
ary. Naturally, any possible alliance would be associated with a different optimal
drawdown size. Higher drawdown sizes correspond to alliances the planner would
prefer to have searching for longer periods. It is therefore natural to suspect that
the alliance corresponding to the highest such drawdown size is the last active
alliance. Since we already determined that optimal search exits occur in "weak"
order, with agent i never exiting after agent i + 1, it suffices to consider drawdown


                                         22
sizes corresponding to each alliance Bk .18 This allows us to determine the last
active alliance chosen by the social planner, BL1 , as in panel (a) of Figure 2.
    Once BL1 is identified, we proceed to the penultimate active alliance. Namely,
we consider all plausible super-sets of BL1 and assess drawdown sizes when the
social planner is constrained to transition directly to BL1 , see panel (b) of Figure
2. The alliance generating the maximal such drawdown size is the one the planner
would want to keep searching the longest, foreseeing her optimal utilization of the
next alliance BL1 . That is the penultimate alliance. We continue recursively until
reaching the maximal active alliance B1 , see panel (c) of Figure 2.


5.4    Comparing Exit Waves in an Exponential World
In order to contrast the structure of equilibrium and socially optimal exit waves,
we now consider a particular example. Suppose the team comprises three agents,
N = 3, and assume cost functions are exponential and well ordered: c( ) = c1 ( ) =
eb = 2 c2 ( ) = 3 c3 ( ), where 1 < 2 < 3 . There are four possible exit wave
structures: all agents can leave at once; agent 1 might leave first, followed by the
clustered exit of the lower-cost agents 2 and 3; agents 1 and 2 might leave together,
followed by agent 3; or agents may exit at different points.
    Figure 3 focuses on the case in which the social planner would cluster all
agents' exits (each tick on the axes corresponds to one unit of the corresponding
multiplier, so that both 2 and 3 range from 0 to 24). The figure depicts the differ-
ent regions of 2 and 3 combinations that generate the four possible structures of
equilibrium exit waves. Since 3 > 2 , all regions are above the gray 45-degree di-
agonal line. We use {1, 2, 3} to denote one clustered exit wave including all agents;
{1, 2}, {3} to denote an exit wave consisting of agents 1 and 2, followed by the exit
of agent 3; and so on.
    When the cost multipliers are sufficiently close to one another, agents exit in
unison even in equilibrium. When 2 is sufficiently close to 1, but 3 is sufficiently
higher, agent 3 has substantially lower search costs. Since agents 1 and 2 do not
internalize their externalities on agent 3, they prefer to leave early on, generating
two exit waves. Similarly, when 2 and 3 are sufficiently high but close to one an-
other, two exit waves occur in equilibrium. Last, when agents' costs are sufficiently
  18 As mentioned, this simplifies the computation problem substantially. Instead of considering

2N - 1 alliances, we need to consider only N .

                                              23
Figure 3: Equilibrium exit wave patterns when the optimal policy entails one exit
                        wave including all three agents


different, equilibrium dictates agents exiting at different points, resulting in three
exit waves, even when externalities are sufficiently strong so that the social plan-
ner would prefer to have the agents search together till they all exit. Naturally, for
sufficiently high 2 and 3 , the wedge in costs is big and even the social planner
would prefer to split agents' exits. The Online Appendix contains detailed char-
acterization of the equilibrium and social planner's solutions and displays similar
figures for other exit-wave structures chosen by the social planner.


6    Conclusions and Discussion
This paper analyzes team search patterns. We show that the equilibrium and so-
cially optimal search scopes are constant within an alliance. However, as alliance
members depart, individual search scopes increase in equilibrium and decrease
under the optimal policy. We also characterize the deterministic path of exit waves
generated in equilibrium. In particular, even when team members are fully het-
erogeneous, clustered exits may occur. The optimal path of exit waves shares fea-
tures with the equilibrium path in terms of the structure of stopping boundaries
that govern departures. However, search externalities naturally prolong optimal
search in teams and alter resulting exit waves.
    In what follows, we consider two extensions of our model, explicit rewards for

                                         24
innovating early and the utilization of non-Markovian strategies in equilibrium.
In the Online Appendix, we also analyze the limitations introduced by a fixed
search scope that cannot be altered, and our model's implications for settings with
independent search observations.


6.1    Equilibrium with Penalties for Later Innovations
Suppose stopping earlier grants one an advantage. For example, a firm that pro-
duces the first product of its type might capture a market segment that is later
more challenging to capture. Similarly, researchers arguably get additional credit
for being the first to suggest a modeling framework or a measurement technique.
    For simplicity, consider a team of two agents. Assume that the first agent to
stop, say at time t , receives Mt . The second agent to stop, say at time s > t , receives
Ms , with   1. If both agents stop at the same time t , they both receive Mt .19
As we show in the Online Appendix, the order of exits remains deterministic. Fur-
thermore, as long as both agents are searching, the search scope and the initial
stopping boundary are identical to those in our benchmark setting, where  = 1.
Thus, if there is a unique exit wave when  = 1, that is still the case when  < 1.
    Suppose there are two distinct exit waves with  = 1. Then, there is a leader--
the agent who exits early--and a follower--the agent who exits later. The leader's
stopping boundary gL (·) remains her equilibrium stopping boundary regardless of
 and is governed by the drawdown identified in Proposition 2. The follower's
stopping boundary, however, may change.
    To characterize the follower's stopping boundary, denote the costs of the leader
by cL (·) and those of the follower by cF (·). Let L denote the leader's search scope
when searching within the full team, T denote the total search scope in the full
team, and F denote the follower's optimal solo search scope. Similar calculations
to those underlying Proposition 2 yield the follower's stopping boundary gF (·):
                            2                     2          2
                 
                         F                    F           T
                  M  -          if M < M and          >          ,
                 
                       2cF (F )              2cF (F )   2cL (L )
                 
       gF (M ) = 
                 
                 gL (M ) otherwise,
                 

  19 The analysis naturally extends to N agents via a decreasing sequence of discounts: 0 = 1 
1  2  ...  N . In addition, one could consider a continuous version of this setup, where the
second agent who stops at time s > t receives Mt +  (Ms - Mt ). That model generates qualitatively
similar results, but is more cumbersome to analyze.


                                               25
where
                           2        2                 2
            1 cF (F ) F          T
       M=         2
                              -                           .
          1 -  F      2cF (F ) 2cL (L )
   To glean some intuition, consider the follower's problem after the leader's de-
parture. The follower faces a similar problem to the individual agent's problem,
with identical search costs and rewards scaled down by  . This case falls within
the analysis of Urgun and Yariv (2021). The search scope is unaffected by the at-
tenuated rewards, but the drawdown size is scaled linearly by  --as  declines,
the rewards from search become less meaningful, and the follower ceases search
more willingly. Naturally, for sufficiently low  , search continuation would not be
worthwhile altogether, regardless of the maximal observation achieved when the
                                                                                                    2
                                                                                                F
leader exits. That corresponds to the drawdown used by the follower alone,                     2cF (F )
                                                                                                        ,
                                                                 2
being smaller than the full alliance's drawdown, 2c (T ) . In that case, the stopping
                                                     L L
boundary of the leader governs the exit of both. In addition, when the maximal
observation M achieved when the leader exits is high enough, the loss from leav-
ing at a later point, (1 -  )M is substantial for any  < 1.20 For sufficiently high
M , search continuation would again not be profitable. As  increases, the thresh-
old level M increases. To summarize, for the follower to continue search after the
leader,  needs to be sufficiently high and the current maximum sufficiently small.
    Importantly, when later innovations are penalized, there are no preemption
motives. The main impact is on later innovators, who face weakened incentives
to search. Mechanically, larger exit waves occur for a larger set of parameters.
Nonetheless, the main messages of the paper extend directly to such settings.


6.2     Non-Markovian Strategies
Our equilibrium analysis restricts attention to Markovian strategies. In our set-
ting, the use of non-Markovian strategies cannot yield the socially optimal solution
in general.21 To see why, consider a team of two agents and suppose the optimal
search scope can be implemented in equilibrium--say, when there is only one vi-
able scope,  =  . Our results show that the social planner would like agents to
  20 Specifically,   the gain from continuation for the follower is given by (1 -  )M + (dL - dF )2 cF
                                                                                                     2,
                                                                                                     F
where dL and dF are the drawdown sizes for the leader and the follower, respectively.
 21 This contrasts insights on collective experimentation, see Hörner, Klein, and Rady (2020).



                                                    26
search for a longer time than the (Markovian) equilibrium we identify would pre-
scribe. Suppose agent 1 is the first to exit in such an equilibrium, where stopping
strategies are not weakly dominated given the search scopes. As long as agent 2 is
searching, agent 1 has a unique best response. She would like to use a drawdown
size d1 , while the social planner would like her to use a drawdown size d1 > d1 .
However, regardless of the space of strategies, there is no way to punish agent 1
for leaving early, and no way to foretell that she will do so. A full analysis of
equilibria in non-Markovian strategies is left for the future.


A      Appendix
Corollary proofs are immediate and, for completeness, available in the Online Ap-
pendix. In what follows, we provide proofs for the paper's main results.


A.1     Proofs for Equilibrium Team Search
First, we note a useful lemma, commonly known as "reflection on the diagonal".
This lemma allows us to omit the partial derivatives pertaining to M in the con-
trol problem in the various Hamilton-Jacobi-Bellman (HJB) equations that we will
derive. Proofs of this result can be found in various sources, including Dubins,
Shepp, and Shiryaev (1994), Urgun and Yariv (2021) and Peskir (1998) among oth-
ers and hence omitted.

Lemma A.1. The infinitesimal generator of the two dimensional process Z = (M, X )
satisfies the following:
                                              2
    1. If Mt > Xt , then AZt = AXt = 1     2 
                                     2 (t ) X 2 .

                           V
    2. If Mt = Xt , then   M
                                  = 0.

Proof of Proposition 1. For any agent i in an alliance A, the value function takes the
following form
                      A                                            
        A                                         A        A       
      Vi (M, X ) = E      ci (i,t |M, X )dt + E Vi (M A , g (M A ) ,                (2)
                                                                   
                                                                   
                           0




                                             27
where
                             
                                        A            A
                             M A if gi (M A ) = g (M A )
                             
          A        A
                             
      E Vi (M A , g (M A ) =                                                                          .
                             ViA\{j } if gjA (M A ) = g A (M A ) > giA (M A ), j  A
                             

In words, with Markov strategies, agent i 's expected value is derived from two
components: the cost accrued until her alliance shrinks, and the continuation
value once that happens. If the alliance shrinks with agent i 's departure, her con-
tinuation value is simply the maximum value when she exits.
   For a given observed maximum M , there are two cases to consider for an active
agent i in A: either her stopping boundary is the highest within the active alliance,
or not. We discuss these in sequence.
   Suppose first that giA (M ) = maxj A gjA (M ). Consider any observed value X such
that giA (M )  X  M . The Green function on the interval [a, b] is defined as follows:
                     
                      (b-x)(y -a)
                            b-a          if a < y < x < b
      Ga,b (x, y ) =                                      .
                     
                      ( b - y )( x - a )
                     
                            b-a          if a < x < y < b
Following standard techniques,we can write the equilibrium value function of
agent i in the following recursive fashion:

                              M -X                              X - giA (M )
      ViA (M, X ) = M                           + ViA (M, M )
                         M - giA (M )                           M - gi (M )
                        M
                                                                             2
                   -              Gg A (M ),M (x, y )ci (iA (M, y ))                  dy.
                       giA (M )        i                                ( A (M, y ))2
Rearranging terms, we get:

                              M - giA (M )
      ViA (M, M ) - M =                              ViA (M, X ) - M
                              X - giA (M )
                                   M
                                                                                                  
                                                                                     2
                                             Gg A (M ),M (x, y )ci (iA (M, y ))
                                                                                                  
                          +                                                                    dy .
                                                                                                  
                                                                                   A         2
                                                                                                  
                                  giA (M )     i                                  ( (M, y ))

   Since agent i optimally terminates her search at giA (M ), smooth pasting must
hold at giA (M ). The derivative of the continuation value as X  giA (M ) can be writ-
                       ViA (M,X )-M
ten as limX g A (M )                 . By smooth            pasting, this must be equal the derivative
               i         X -giA (M )
                                      
of the value from      stopping, x      M = 0.



                                                           28
   Consider then the above equality for ViA (M, M ). Taking the limit as X  giA (M ),
                                 M
                                                                     2
     ViA (M, M ) = M +                    (M - y )ci (iA (y ))                dy.
                                gi (M )                          ( A (y ))2
This, in turn, implies that
                                X
                                                                    2
     ViA (M, X ) = M +                    (X - y )ci (iA (y ))              dy.
                             giA (M )                            ( A (y ))2
Now, taking the second derivative with respect to X and simplifying yields:

      2 ViA (M, X ) 2ci (iA (M, X ))
                   =                 .
         X 2         (iA (M, X ))2
Plugging this back into the HJB for agent i and simplifying further yields:

      2ci (iA (M, X ))
                         = iA (M, X ) +                    jA (M, X ).
      ci (iA (M, X ))                           j A\{i }

                                                                              A
    Suppose now that i does not have the highest stopping boundary: maxk A gk   (M ) >
  A                                    A        A
gi (M ). Let F (M ) = {j  N : maxk A gk (M ) = gj (M )}. Choose an arbitrary agent
j  F (M ). As above, we can write the continuation payoff of i as follows:

                                                 M -X                           X - gjA (M )
     ViA (M, X ) = ViA (M, giA (M ))                            + ViA (M, M )
                                              M - gjA (M )                    M     - gj (M )
                         M
                                                                               2
                 -              Gg A (M ),M (X, y )ci (iA (M, y ))                       dy.
                     gjA (M )       j                                    ( A (M, y ))2

Rearranging terms, we get:

                                                  M - gjA (M )
     ViA (M, M ) - ViA (M, gjA (M )) =                              ViA (M, X ) - ViA (M, gjA (M ))
                                                  X   - gjA (M )
                                                                                                            
                                                      M
                                                                                               2            
                                              +         Gg A (M ),M (X, y )ci (iA (M, y )) A             dy .
                                                                                                            
                                                 A        j                               (  ( M, y )) 2    
                                                gj (M )

                                                                                          D ViA (M,g (M ))
Again, taking the limit as X  g (M ) from above, and letting                                    X
                                                                                                             denote




                                                           29
the upper Dini derivative of ViA (M, g (M )) at g (M ), we have:22

                       D ViA (M, g (M ))
      ViA (M, M ) =                        (M - g (M )) + ViA (M, gjA (M ))
                                X
                        M
                                                          2
                     +         (M - y )ci (iA (M, y )) A          dy.
                       gj (M )                        ( (M, y ))2

Plugging this identity in ViA (M, X )'s expression and taking the second derivative:

      2 ViA (M, X ) 2ci (iA (M, X ))
                   =                 .
         X 2         (iA (M, X ))2
Plugging this back into the HJB for agent i and simplifying further generates:

      2ci (iA (M, X ))
                          = iA (M, X ) +                   A
                                                           k (M, X ).
       ci (iA (M, X ))                          k A\{i }

Our assumption that the cost function's second derivative is bounded above zero
implies that the conditions of Theorem 6.4 of Fleming and Rishel (2012) are sat-
isfied. Hence, any equilibrium features continuous search scopes within any al-
liance. In particular, within an active alliance, agents can utilize only one of the
solutions for the system above.

Proof of Proposition 2. The statement of Proposition 2 is a combination of the fol-
lowing claims.
Claim A.1. For any given alliance A with i  A, if giA (M  ) = maxj A gjA (M  ) for some
M  , then giA (M ) = maxj A gjA (M ) for all M .

Proof of Claim. The proof of the claim relies on the following lemma.
Lemma A.2. Suppose agent i  A has the highest stopping boundary at a given observed
M, X . Then giA (M ) is a drawdown stopping boundary.

Proof of Lemma A.2. Suppose maxj A gjA (M ) = giA (M ). As shown in the proof of
Proposition 1, we have
                                X
                                                                         2
      ViA (M, X ) = M     +               (X - y )ci (iA (M, y ))                   dy.
                               giA (M )                             ( A (M, y ))2
 22 Since   i 's are bounded, V is Lipschitz, hence the Dini derivative is finite.




                                                           30
Furthermore using Proposition 1 we know that  A (M, X ) =  A for all M, X and
iA (M, X ) = iA for all M, X .
    Now, differentiating ViA (M, X ) with respect to M and evaluating the derivative
at X = M yields the following ordinary differential equation (ODE) for giA (M ):

                               ( A )2
          giA (M ) =                                   ,
                       2ci (iA ) M - giA (M )

which leads to the following solution:

                            ( A )2
          giA (M ) = M -               .
                           2ci (iA )
                                                                                                              ( A )2
This is a drawdown stopping boundary with drawdown size diA :=                                                         .
                                                                                                             2ci (iA )

    We can now proceed with the claim's proof. Towards a contradiction, suppose
that maxj A gjA (M ) = giA (M ) for some M . Suppose M = infM>M
                                                             ^                          ^ )}
                                                                  {M |i arg maxj A gjA (M
and that for some  > 0, for any M    ^  (M , M +), for some k i , we have maxj A g A (M ) =
                                                                                  j
 A
gk (M ) > giA (M ). From continuity of the stopping boundary and Lemma A.2,

                            ( A )2                                             ( A )2
          giA (M ) = M -                   and          A
                                                       gk (M ) = M -                         .
                           2ci (iA )                                              A
                                                                             2ck (k )
                                            ( A )2          ( A )2            ( A )2          ( A )2
Our choice of i and k yields                                           and               >         A ,   in contradiction.
                                           2ci (iA )       2ck (kA
                                                                   )         2ci (iA )       2ck (k  )


                                                                                                  ( A )2           ( A )2
Claim A.2. Suppose that for some i in an active alliance of A,                                                                for all
                                                                                                 2ci (iA )        2cj (jA )
j  A. Then i is the first to exit alliance A.23
                                   ( A )2          ( A )2
Proof of Claim. Suppose                                       for all j  A but that agent i is not one of
                                  2ci (iA )       2cj (jA )
the first agents to exit from alliance A for some path of observed values. For that
path, agent i ceases her search when active at a smaller alliance A \ K . Without
loss of generality, suppose agent j exits alliance A first (if there are multiple such
agents, pick any) when observing M and X . From Lemma A.2, agent j 's stopping
  23 If
      there are multiple agents who satisfy the condition, all exhibiting the same drawdown size,
they all exit jointly, weakly before others.




                                                              31
boundary is characterized by a drawdown. However, from the Claim's restriction,

                 ( A )2                ( A )2
      M-                    M-                    .
                2cj (jA )             2ci (iA )

                                          A                         ( A )2
    For each k  A, the stopping boundary gk (M ) = M -                   A is identified by
                                                                   2ck (k  )
                                                                      A       A
value matching and smooth pasting. In particular, we have Vk (M, gk             (M )) = M . If
                                                                              (  A )2     ( A )2
giA (M ) > gjA (M ), this implies that ViA (M, gjA (M ) + ) < M for 0 < <              -           .
                                                                             2cj (jA )   2ci (iA )
Therefore, agent i would prefer to stop strictly before agent j .

    The two claims and Lemma A.2's characterization yield the proposition's proof.



A.2     Proofs for the Social Planner's Solution
Proof of Proposition 3. Let iA (M, X, A) and G(M, X, A) correspond to a solution
to the social planner's problem. Consider any alliance Ak at some observed values
and let Ak +1 denote the potentially empty random alliance dictated by this optimal
solution. Optimality implies that the induced search scopes with Ak should solve:
                                                         
                                      Ak                 
                                                 Ak      
        sup E  |Ak \ Ak +1 |M Ak -         c  (     ) dt .
                                                         
                                             i i,t       
      { }
               
           i,t i Ak                 0                    
                                                         i Ak

Following similar steps to the proof of proposition 1 in the equilibrium analysis,
the continuation HJB for the social planner can be written as:


                                            Ak                  Ak
                                     2 E              i Ak ci (i (X ))dt |M, X
       2 W (M, X, A         k)             0
                                 =                                               .
                 X 2                                     X 2
It follows that

                         Ak
       2      i Ak ci (i (X ))              A
                       Ak
                                     = cj (i k (X ))     j  Ak .
                i Ak i (X )

   Since there is no direct dependence on X on either side, optimal search scopes
are independent of observed values and constant over time for each active alliance.



                                                           32
Proof of Proposition 4. The proof follows from two lemmas:
Lemma A.3. If the set of agents exitting an alliance is independent of the observed path,
each alliance has a stopping boundary identified by a drawdown size dAk .

Proof of Lemma A.3. Let AK be the final alliance in the social planner's problem,
with cardinality |AK |. The social planner's problem when left with alliance AK ,
and observing maximum M and current value X , takes the following form:
                                                                      
                                              K                       
                                                       AK             
      WK (M, X ) = sup E      | A   | M  K -     c  (     ) dt | M, X  .
                                                                      
                                  K               i
                                                                      
                                                      i               
                   K
                   ,{ }
                                             0                        
                              i iAK                            i AK

This is tantamount to a single-searcher problem, where search rewards are scaled
by |AK |. From Urgun and Yariv (2021), the stopping boundary is given by:

      g AK (M ) = M - dAk ,
                   |AK | AK
where dAk =                AK       .
              2   i AK ci (i )
    Consider the social planner's problem when the penultimate alliance AK -1 is
active and the observed maximum and value are M and X , respectively:

      WK -1 (M, X ) =           sup         E |AK -1 \ AK |M K -1 + WK (M K -1 , g AK -1 (M K -1 )) | M, X
                         K -1 ,{i }iAK -1
                                                                         
                                  K -1                                   
                                                        A                
                     - E                           ci (i K -1 )dt | M, X  .
                                                                         
                                                                         
                                0        i AK -1
                                                                         

By optimality of the stopping time  K -1 , we have smooth pasting of WK -1 (M, X )
and WK (M, X ). Therefore,

                  WK -1 (M, g AK -1 (M )) = |AK -1 \ AK |M + WK (M, g AK -1 (M )),
      WK -1 (M, g AK -1 (M ))              (|AK -1 \ AK |M + WK (M, g AK -1 (M )))
                              |X =g Ak-1 =                                         |X =g AK -1 (M ) .
             X                                             X
Similar to our equilibrium analysis, and using the notation for the Green function




                                                         33
introduced there, we can write the welfare maximization problem as
                                                                                     M -X
      WK -1 (M, X ) = |AK -1 \ AK |M + WK (M, g AK -1 (M ))
                                                                                  M - g AK -1 (M )
                                                            M                                                         AK -1
                       X - gAK -1 (M )                                                           2       i AK -1 ci (i      )
       + WK -1 (M, M )                  -                          Gg AK -1 (M ),M (X, y )                                    dy.
                       M - g AK -1 (M )                  g AK -1                                           ( AK -1 )2

Letting X approach g AK -1 (M ), smooth pasting and rearranging yields:24
WK -1 (M, X ) = |AK -1 \ AK |M + WK (M, g AK -1 (M ))
                                                      AK                                                             AK -1
                        g AK -1 (M ) 2      i AK ci (i )             X                       2       i AK -1 ci (i         )
+ X - g AK -1 (M )     g AK (M )              ( AK )2
                                                            dx +    g AK -1 (M )
                                                                                 (X   - y)             (  A K - 1 ) 2          dy,

   Using the closed-form representation of WK leads to:
                                                                                                   AK
                                 1                            2                          i AK ci (i )
      WK -1 (M, X ) = |AK -1 |M + (g AK -1 (M ) - g AK (M ))2
                                 2                                                         ( AK )2
                                                                                      AK
                  AK -1             AK -1              AK
                                                                    2       i AK ci (i )
       + X -g             (M ) (g           (M ) - g        (M ))
                                                                              ( AK )2
                                                         AK -1
        1                    2              i AK -1 ci (i      )
       + (X - g AK -1 (M ))2                                     .
        2                                     ( AK -1 )2
To generate an ODE that identifies g AK -1 (M ), we take the derivative with respect to
M that, evaluated at X = M , should equal 0. After some algebraic manipulations,
this ODE take the form
      dg AK -1 (M )                                     |AK -1 \ AK |
                    =                                                   AK -1                    AK
                                                                                                              .
          dM                                                i AK -1 ci (i     )        i AK ci (i )
                            2 M - g AK -1 (M )                    A
                                                               ( K -1 ) 2         -      ( AK )2

   It is straightforward to verify that the unique solution for this ODE satisfying
the value-matching condition takes the form g AK -1 (M ) = M - dAK -1 , where

                                   |AK -1 \ AK |
      dAK -1 =                        AK -1                   AK
                                                                        .
                          i AK -1 ci (i     )       i AK ci (i )
                  2          ( AK -1 )2
                                                -     ( AK )2

  24 The   equality follows from
                                                                                                  g AK -1 (M )                  AK
     (|AK -1 \ AK |M + WK (M, g AK -1 (M )))                                                                      2   i AK ci (i )
                                             |X =g AK -1 (M ) = X - g AK -1 (M )                                                   dx.
                     X                                                                           g AK (M )              ( AK )2




                                                             34
In particular, the optimal stopping boundary is a drawdown stopping boundary.
   Proceeding inductively, for any alliance m  K , the continuation value when M
and X are observed can be written as:

      Wm (M, X ) =|Am \ Am+1 |M + Wm+1 (M, g Am (M ))
                                        
                                   K -1  g k (M )                    Ak +1      
                                                  2 i Ak +1  ci (   i      )    
                   + X - g Am (M )                                           dx
                                                                                
                                                                                
                                         k+1
                                        
                                                      (  A k +1 ) 2             
                                         g (M )
                                             k =m
                       X                                   Am
                                             2   i Am ci (i )
                  -               (X - y )                    dy.
                      g Am (M )                    ( Am )2

We can then repeat the steps above to generate an analogous ODE for g Am (M ) and
verify that it is uniquely identified as a drawdown stopping boundary. Namely,
g Am (M ) = M - dAm , where

                           |Am \ Am+1 |
      dAm =                 Am                    Am+1
                                                           .
                  i Am ci (i   )       i Am+1 ci (i    )
              2     ( Am )2
                                   -      ( Am+1 )2




Lemma A.4. The set of agents dropping from an alliance is deterministic. That is
for any alliance A, for all pairs (M, X ), (M , X ) such that G(M, X, A) A, we have
G(M, X, A) = G(M , X , A).

Proof of Lemma A.4. We prove this result by induction on the size of the initial
team N , regardless of the starting values of the maximum and the current value.
The claim follows immediately for N = 1. In that case, the agent uses a drawdown
stopping boundary and the only way for the singleton alliance to change is for the
agent to terminate her search.
    For the inductive step, assume that for any initial team of size N - 1 or less the
optimal alliance sequence is deterministic. By Lemma A.3, each of these alliances
is associated with a drawdown stopping boundary. Let A1 be an alliance of size N .
The continuation value when M and X are observed is:
                                                                                        
                                                                     1                  
               M 1 + max {|A1 \ A2 |M 1 + WA (M 1 , g A1 (M 1 ))} -
                                                                                A1      
W1 (M, X ) = E                                                             c (     ) dt .
                                                                                        
                                              2                             i i         
                      A2 A1
                                                                                        
                                                                    0              i Ak
                                                                                        

Suppose that, for some path, the social planner optimally transitions from alliance


                                                     35
A1 to a strictly smaller alliance A2 . In particular, alliance A2 contains fewer
than N agents. By the inductive hypothesis, the sequence that ensues is path inde-
pendent. We can therefore write the continuation value as:

        W1 (M, X ) =|A1 \ A2 |M + W2 (M, g A1 (M ))
                                          
                                     K -1  g Am (M )                                                Am+1         
                                                     2                            i Am+1 ci (i             )     
                     + X - g A1 (M )                                                                           dx
                                                                                                                 
                                                                                                                 
                                                                                      ( Am+1 )2
                                           Am+1                                                                  
                                                    m=2     g        (M )

                              X                                   A1
                                                    2   i A1 ci (i )
                     +                   (X - y )                    dy.
                             g A1 (M )                   ( A1 )2

As before, this yields an ODE characterizing g A1 (M ) and a unique solution of the
                                               |A1 \A2 |
form g A1 (M ) = M - dA1 , where dA1 =          A1         A2 . Towards a contra-
                                                              
                                                                 i A1 ci (i   )        i A2 ci (i   )
                                                            2
                                                                                                     
                                                                                  -                 
                                                                   ( A1 )2               ( A2 )2
                                                                                                    
                                                                                                    
diction, suppose that at some other path a different alliance is optimally chosen
                                                    ^ 2 A2 . Similar argument would
to follow the full alliance A1 . Call that alliance A
then imply that the stopping boundary for A1 is given by g A1 (M ) = M - d^A , where
                                                                            1
                    ^2|
               |A1 \A
 ^
dA =                            . Suppose dA = d ^ . Without loss of generality, as-
  1
        
                      A1    ^ 
                                         A2                          1      A1
            i A1 ci (i )          ^ ci (i ) 
                               i A 2
        2
                                            
                         -           ^ 2
                                            
              ( A1 )2
                                            
         
                                   A
                                 ( 2 )
                                            
                                            
                   ^                          ^ 2  A2 . Indeed, if that were the case, the social
sume  A2   A2 . We cannot have A
planner would be indifferent between keeping the agents in A2 \ A                ^ 2 at points at
which either A    ^ 2 or A2 are chosen to continue. However, our tie-breaking rule im-
plies that such indifferences are broken in favor of stopping; that is, in such cases,
the smaller set A    ^ 2 would be chosen. Therefore, A       ^ 2 A2 and there exists an agent
iA    ^ 2 \ A2 . Suppose that whenever the social planner transitions from A1 to A2 ,
she instead transitions to A2  {i }, maintaining the search scopes of members of A2
as before and having agent i search with the lowest scope  for a sufficiently small
interval of time. In that interval of time, everyone in A2 benefits. When alliance
A^ 2 is picked, agent i uses at least as high a search scope, while benefitting from
lower overall search scope. In particular, agent i benefits as well from this change.
     Suppose dA1 dA         ^ 1 . In this case, the two stopping boundaries identified above,
M - dA1 and M - dA       ^ 1 never intersect, in contradiction.

      Combining the two lemmas leads to the conclusion of the proposition.




                                                                36
A.3    Proofs for Optimal Sequencing with Well-ordered Costs
Proof of Lemma 1. As introduced in the proof of Corollary 3, we use here the su-
perscripts eq and sp to denote the equilibrium and social planner's solution, re-
spectively. When costs are well-ordered, in equilibrium, in any alliance, all agents
utilize the same search scope. In particular, for any active alliance A and any
                  A,eq    A,eq
i, j  A, we have i     = j . This implies that, in equilibrium, each agent k exits
no later than agent k - 1, for all k = 2, ..., N . Indeed, in any active alliance A, the
equilibrium stopping boundary is governed by drawdown size

       eq          ( A,eq )2                ( A,eq )2
      dA = min          A,eq
                                   = max         A,eq
                                                            .
            i A   2ci (i       )     i A   2ci (i       )
Suppose, towards a contradiction, that there exists a pair i, j such that i > j , so that
i > j , and the social planner has agent i terminate her search strictly before agent
j . There are then two distinct alliances in the social planner's solution, Ak and Am ,
with k < m, where i, j  Ak but i Ak +1 and j  Am but j Am+1 .
      As we showed, the social planner's solution associates a drawdown stopping
                                                                                    sp
boundary with each alliance. Denote the corresponding drawdown sizes dk and
   sp
dm for Ak and Am , respectively. Suppose that, instead, the social planner swaps
the exits of agents i and j , exiting agent j from Ak whenever agent i was to cease
her search and exit from Ak and exiting agent i from Am whenever agent j was to
cease her search and exit from Am . Furthermore, the social planner can have agent
i use the same search scope as agent j had originally in the alliances that follow Ak .
The overall search scope in any alliance does not change after this modification.
Consequently, expected search outcomes are unaltered. However, the overall cost
decreases weakly in every alliance and strictly in all alliances Ak +1 , ..., Am , contra-
dicting the optimality of the proposed solution.

Proof of Proposition 5. Recall that our results so far imply that the social planner
can restrict attention to the choice between deterministic alliance sequences. Fur-
thermore, given a deterministic sequence of alliances, Lemma A.3 identifies the
optimal drawdown stopping boundaries associated with that alliance sequence. If
the chosen sequence is suboptimal, some of its associated drawdown sizes might
be negative or zero, implying the corresponding alliance is utilized for no length
of time. This observation helps us to identify the optimal sequence. The proof


                                                    37
of Proposition 5 follows from several lemmas. For any alliance Bk , regardless of
whether it is on the social planner's optimal alliance sequence, we denote the op-
timal overall search scope within the alliance by    ~ k and the consequent overall
                                    ~k .
search cost within that alliance by c

Lemma A.5. For any m, j, k such that m < j < k , if the welfare-maximizing sequence is
such that Bk is preceded by Bm , then for any sequence where Bk is preceded by Bj , we
have dBm Bk > dBj Bk .

Proof of Lemma A.5. from the characterization of drawdowns in the well-ordered
settings, dBm Bk dBj Bk . Suppose that dBm Bk < dBj Bk . Since Bk is preceded by
Bm in the optimal sequence, dBm Bk > 0. It then follows that dBj Bk > 0. This im-
plies that it would be beneficial for the planner to have alliance Bm first transition
to alliance Bj , and only then transition to alliance Bk .

Lemma A.6. If m < k , dBm  > dBk  implies dBm Bk > dBk  .

Proof of Lemma A.6. dBm  > dBk  implies

         1 c  ~m     1 c~k    1  ~m c
                                 c    ~k   1 c ~k
                 <         =    ( m - k)<         ,
       N -m   ~m N - k  ~k   k-m ~    ~   N -k ~k
illustrating the claim.

Lemma A.7. For any k such that dBk  > dBN  > dBk+1  , we have dBk Bk+1 > dBN  .

Proof of Lemma A.7. Observe that dBk  > dBN  > dBk+1  implies

                   ~N
                   c    ~k +1
                        c             ~k
                                      c              ~N
                                                     c
     (N - k + 1)      >         and      > ( N - k )    .
                   ~N  k
                          ~+1         ~k
                                                     ~N
                                                     
Simply summing the inequalities and reorganizing yields the implied statement.


Lemma A.8. If dBk  > dBk-1 Bk , then dBk  > dBk-1  > dBk-1 Bk .

Proof of Lemma A.8. From the first inequality, dBk  > dBk-1 Bk , we have,

        1 c ~k    ~k -1 c
                  c       ~k                ~k
                                N - (k - 1) c    ~k -1
                                                 c
               <        -    =                 <        = dBk  > dBk-1  .
      N -k  ~k    ~ k -1  ~k      N -k      ~k   ~ k -1
But this inequality implies that


                                           38
 1 c ~k          1      ~k -1
                        c             ~k -1
                                      c         ~
                                                ck         1       ~k -1
                                                                   c
N -k ~k
          <   N -(k -1) ~ k -1
                                 =    ~ k -1
                                               -~k
                                                   >   (N -(k -1)) ~ k -1
                                                                            = dBk-1  > dBk-1 Bk .


Lemma A.9. If k satisfies maxj dBj  = dBk  , then any alliance Bl with l < k cannot
be the welfare maximizing last alliance.

Proof of Lemma A.9. Suppose not, so that, form some l < k , alliance Bl is the last.
Since Bk is strictly contained in Bl , from the characterization of drawdowns in
the well-ordered settings, dBk  dBl  . The social planner would, then, benefit
from transitioning from Bl to Bk instead of exiting all members of Bl since dBk  >
dBl  > 0, in contradiction.

Lemma A.10. If k satisfies maxj dBj  = dBk  , then any alliance Bl with l > k cannot
be the last.

Proof of Lemma A.10. We use induction on the cardinality of the set Bk . The claim
certainly holds when |Bk | = 1, so that Bk = BN = {N }.
    For the proof, it is useful to notice that our entire analysis does not hinge on the
range of viable search scopes coinciding across agents. In fact, the analysis would
go through in its entirety if each agent i had an individual range of scope [ i ,  i ],
as long as all solutions remain interior.
    Assume the statement is true for sets up to cardinality n. We show the state-
ment holds for |Bk | = n + 1 (so that k = N - n + 1). By Lemma A.9, the last alliance
cannot be Bj with j < k . Towards a contradiction, suppose that a smaller set Bm ,
with m > k , is the last alliance. From the inductive hypothesis, we must have
dBm  > dBl  for all l > m, as otherwise the social planner would benefit by in-
ducing Bl to continue search instead of terminating it for all agents in Bm .
   Suppose that m < N . Consider an equivalent problem, where alliance Bm is re-
                                                         ^(·) defined so that c
placed with a single individual M that has cost function c                    ^( ) is
the minimal overall cost in Bm required for implementing an overall search scope
 . That is, if
                                                                      N
          B           B                                                         B
      mm , ..., Nm  arg                         min                         c (^j m ),
                                  B                    N     B
                                 ^j m [ , ]
                                                 j,    j =m ^j m =   j =m

                     N       B
     ^( ) =
then c               j =m c(^j m ).    Under this definition,   [(N - m + 1) , (N - m + 1) ].


                                                              39
    In the equivalent problem, we have m agents 1, 2, ..., m - 1, M .25 From our con-
struction, in the optimal solution, for any j = 1, ..., m - 1, the corresponding draw-
down size d{j,...,M } coincides with the optimally-set drawdown size dBj  in our
original problem. Furthermore, d{M } coincides with dBm in our original prob-
lem. Therefore, maxj {1,...,m-1,M } d{j,...,M } = d{k,...,M } . By our induction hypothe-
sis, {j, ..., M } with j > k cannot optimally be the last alliance, in contradiction.
    Suppose now that m = N and, towards a contradiction, assume BN is the wel-
fare maximizing last alliance. Now consider the sequence of welfare maximizing
alliances Bp such that Bp  Bk . There are three cases to consider.
    Case 1: For all p  {k, ..., N - 1}, the alliance Bp is part of the welfare-maximizing
sequence. That is, agents terminate their search one by one starting from Bk
onwards. Since BN is the last alliance, we must have that dBN  > dBN -1 BN >
dBN -2 BN -1 > . . . > dBk Bk+1 . Applying Lemma A.8 repeatedly implies that dBN  >
dBN -1  > dBN -2  . . . > dBk+1  . The assumed maximality of dBk  implies, in
particular, that dBk  > dBN  that, combined with the above, yields dBk  >
dBN  > dBk+1  . By Lemma A.7, we then have that dBk Bk+1 > dBN  . It follows
that whenever agents in the active alliance Bk optimally stop searching, the social
planner would benefit from halting all agents' search instead of proceeding with
Bk +1 , Bk +2 , ..., BN , in contradiction.
    Case 2: There does not exist any p  {k, ..., N - 1} such that Bp is part of the
optimal sequence. Thus, the penultimate alliance in the optimal sequence is Bl
with l < k . Maximality of dBk  implies that dBk  > dBN  . By Lemma A.6,
dBk BN > dBN  and by Lemma A.5, dBl BN > dBk BN > dBN  . Thus, whenever
agents in active alliance Bl optimally stop searching, the social planner would ben-
efit from halting all agents' search instead of proceeding with BN , in contradiction.
   Case 3: There exist p, q  {k, ..., N -1} such that Bp is part of the optimal sequence
but Bq is not. Here we have two subcases:
   Subcase 1: BN -1 is the penultimate alliance. We must have dBN -1  < dBN  ;
otherwise, by Lemma A.6, we would have dBN -1 BN > dBN  and it would be sub-
optimal to utilize alliance BN as the last alliance. From the maximality of dBk 
and Lemma A.5, for any l < k such that Bl precedes BN -1 on the optimal path,
  25 Our assumption that all alliance optimally have members using an interior search scope guar-

antees that this fictitious agent would choose an interior search scope as well.


                                               40
dBl BN -1 > dBk BN -1 > dBN -1  . Finally, dBN  > dBN -1  implies that

       ~N -1
      1c       ~N
               c     ~N -1
                    1c       ~N -1 c
                             c       ~N
             >    =        <       -    = dBN -1  > dBN -1 BN .
      2~ N -1  ~N   2~ N -1  ~ N -1  ~N
Thus, dBl BN -1 > dBk BN -1 > dBN -1  > dBN -1 BN . Therefore, whenever agents in
the active alliance Bl optimally stop searching, the social planner would benefit
from transitioning to BN directly, thereby terminating the search of agent N - 1 as
well, instead of transitioning to BN -1 first, in contradiction.
   Subcase 2: The penultimate alliance is Bp with p  {k, ..., N - 2}. We can now em-
ulate the argument above pertaining to the construction of an equivalent problem
in which agents {p, ..., N - 1} are viewed as one agent with appropriately induced
search costs. We can then consider an equivalent problem with fewer agents to
achieve a contradiction through our induction hypothesis.

    It follows that the last alliance is given by Bk with maxj dBj  = dBk  .
    The proofs of the following Lemmas are a consequence of identical arguments
to those in of Lemmas A.9 and A.10 and are therefore ommitted.
Lemma A.11. Consider Bk where k is such that dBk BL > dBj BL for all j < L1 and
                                                          1         1
BL1 as the last alliance as identified above. Then any alliance with l < k cannot be the
welfare maximizing second to last alliance.
Lemma A.12. Consider Bk where k is such that dBk BL > dBj BL for all j < L1 and
                                                           1          1
BL1 as the last alliance as identified above. Then any alliance Bl with L1 > l > k cannot
be the welfare maximizing second to last alliance.
    The proof of Proposition 5 then follows. Using the proposition's notation, BL1
is the last alliance on the social planner's optimal path. Similarly, the penultimate
alliance is given by Bk where k is such that dBk BL > dBj BL for all j < L1 . We can
                                                     1        1
continue recursively to establish the proposition's claim.


References
Admati, A. R. and M. Perry (1991). Joint projects without commitment. The Review
  of Economic Studies 58(2), 259­276.
Albrecht, J., A. Anderson, and S. Vroman (2010). Search by committee. Journal of
  Economic Theory 145(4), 1386­1407.


                                           41
Anderson, A., L. Smith, and A. Park (2017). Rushes in large timing games. Econo-
  metrica 85(3), 871­913.
Azéma, J. and M. Yor (1979). Une solution simple au problème de skorokhod.
  Séminaire de probabilités de Strasbourg 13, 90­115.
Bardhi, A. and N. Bobkova (2021). Local evidence and diversity in minipublics.
  mimeo.
Bolton, P. and C. Harris (1999). Strategic experimentation. Econometrica 67(2),
  349­374.
Bonatti, A. and H. Rantakari (2016). The politics of compromise. American Eco-
  nomic Review 106(2), 229­59.
Bulow, J. and P. Klemperer (1994). Rational frenzies and crashes. Journal of political
  Economy 102(1), 1­23.
Callander, S. (2011). Searching and learning by trial and error. American Economic
  Review 101(6), 2277­2308.
Caplin, A. and J. Leahy (1994). Business as usual, market crashes, and wisdom
  after the fact. American Economic Review, 548­565.
Cetemen, D., I. Hwang, and A. Kaya (2020). Uncertainty-driven cooperation. The-
  oretical Economics 15, 1023­1058.
Deb, J., A. Kuvalekar, and E. Lipnowski (2020). Fostering collaboration. mimeo.
Dixit, A. K. and R. S. Pindyck (1994). Investment under uncertainty. Princeton
  university press.
Dubins, L. E., L. A. Shepp, and A. N. Shiryaev (1994). Optimal stopping rules and
  maximal inequalities for bessel processes. Theory of Probability & Its Applica-
  tions 38(2), 226­261.
Fleming, W. H. and R. W. Rishel (2012). Deterministic and stochastic optimal control,
  Volume 1. Springer Science & Business Media.
Gul, F. and R. Lundholm (1995). Endogenous timing and the clustering of agents'
  decisions. Journal of Political Economy 103(5), 1039­1066.
Hörner, J., N. Klein, and S. Rady (2020). Overcoming free-riding in bandit games.
  mimeo.
Hörner, J. and A. Skrzypacz (2016). Learning, experimentation and information
  design. mimeo.
Jovanovic, B. and G. M. MacDonald (1994). The life cycle of a competitive industry.
  Journal of Political Economy 102(2), 322­347.


                                         42
Keller, G., S. Rady, and M. Cripps (2005). Strategic experimentation with expo-
  nential bandits. Econometrica 73(1), 39­68.
Marx, L. M. and S. A. Matthews (2000). Dynamic voluntary contribution to a
  public project. The Review of Economic Studies 67(2), 327­358.
Murto, P. and J. Välimäki (2011). Learning and information aggregation in an exit
  game. The Review of Economic Studies 78(4), 1426­1461.
Papadimitriou, C. H. and K. Steiglitz (1998). Combinatorial optimization: algorithms
  and complexity. Courier Corporation.
Peskir, G. (1998). Optimal stopping of the maximum process: The maximality
  principle. Annals of Probability, 1614­1640.
Peskir, G. and A. Shiryaev (2006). Optimal stopping and free-boundary problems.
  Springer.
Puterman, M. L. (2014). Markov decision processes: discrete stochastic dynamic pro-
  gramming. John Wiley & Sons.
Rosenberg, D., E. Solan, and N. Vieille (2007). Social learning in one-arm bandit
  problems. Econometrica 75(6), 1591­1611.
Strulovici, B. (2010). Learning while voting: Determinants of collective experi-
  mentation. Econometrica 78(3), 933­971.
Taylor, H. M. et al. (1975). A stopped brownian motion formula. The Annals of
  Probability 3(2), 234­246.
Titova, M. (2019). Collaborative search for a public good.
Urgun, C. and L. Yariv (2021). Retrospective search: Exploration and ambition on
  uncharted terrain. mimeo.
Weitzman, M. L. (1979). Optimal search for the best alternative. Econometrica:
  Journal of the Econometric Society, 641­654.
Yildirim, H. (2006). Getting the ball rolling: Voluntary contributions to a large-
  scale public project. Journal of Public Economic Theory 8(4), 503­528.




                                        43
