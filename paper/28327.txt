                              NBER WORKING PAPER SERIES




     INFORMATIONAL CONTENT OF FACTOR STRUCTURES IN SIMULTANEOUS
                      BINARY RESPONSE MODELS

                                         Shakeeb Khan
                                         Arnaud Maurel
                                         Yichong Zhang

                                       Working Paper 28327
                               http://www.nber.org/papers/w28327


                     NATIONAL BUREAU OF ECONOMIC RESEARCH
                              1050 Massachusetts Avenue
                                Cambridge, MA 02138
                                    January 2021




We thank Serena Ng, Arthur Lewbel, two anonymous referees, and seminar participants at
Arizona State University, Emory, Michigan State, Shanghai University of Finance and
Economics, University of Arizona, as well as conference participants at the 2015 SEA meetings
for helpful comments. We also thank Zhangchi Ma and Qingsong Yao for excellent research
assistance. Zhang acknowledges the financial support from Singapore Ministry of Education
Tier 2 grant under grant MOE2018-T2-2-169 and the Lee Kong Chian fellowship The views
expressed herein are those of the authors and do not necessarily reflect the views of the National
Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

© 2021 by Shakeeb Khan, Arnaud Maurel, and Yichong Zhang. All rights reserved. Short
sections of text, not to exceed two paragraphs, may be quoted without explicit permission
provided that full credit, including © notice, is given to the source.
Informational Content of Factor Structures in Simultaneous Binary Response Models
Shakeeb Khan, Arnaud Maurel, and Yichong Zhang
NBER Working Paper No. 28327
January 2021
JEL No. C14,C21,C25,C38

                                          ABSTRACT

We study the informational content of factor structures in discrete triangular systems. Factor
structures have been employed in a variety of settings in cross sectional and panel data models,
and in this paper we formally quantify their identifying power in a bivariate system often
employed in the treatment effects literature. Our main findings are that imposing a factor
structure yields point identification of parameters of interest, such as the coefficient associated
with the endogenous regressor in the outcome equation, under weaker assumptions than usually
required in these models. In particular, we show that a non-standard exclusion restriction that
requires an explanatory variable in the outcome equation to be excluded from the treatment
equation is no longer necessary for identification, even in cases where all of the regressors from
the outcome equation are discrete. We also establish identification of the coefficient of the
endogenous regressor in models with more general factor structures, in situations where one has
access to at least two continuous measurements of the common factor.

Shakeeb Khan                                     Yichong Zhang
Department of Economics                          School of Economics
Boston College                                   Singapore Management University
Chestnut Hill, MA 02467                          90 Stamford Rd.
shakeeb.khan@bc.edu                              Singapore 178903
                                                 Singapore
Arnaud Maurel                                    yczhang@smu.edu.sg
Department of Economics
Duke University
213 Social Sciences Building
Box 90097
Durham, NC 27708
and IZA
and also NBER
apm16@duke.edu
1       Introduction

Factor models see widespread and increasing use in various areas of econometrics. This type of
structure has been employed in a variety of settings in cross sectional, panel and time series models,
and have proven to be a flexible way to model the behavior of and relationship between unobserved
components of econometric models. The basic idea behind factor models is to assume that the
dependence across the unobservables is generated by a low-dimensional set of mutually independent
random factors. The applied and theoretical research employing factor structures in econometrics
is extensive. In particular, these models are often used in the treatment effect literature as a
way to identify the joint distribution of potential outcomes from the marginal distributions, and
then recover the distribution of treatment effects from this joint distribution.1 Factor models have
been used in a number of different contexts in applied microeconomics. These include, among
others, earnings dynamics (Abowd and Card, 1989; Bonhomme and Robin, 2010), estimation of
returns to schooling and work experiences (Ashworth, Hotz, Maurel, and Ransom, 2020), as well as
cognitive and non-cognitive skill production technology (Cunha, Heckman, and Schennach, 2010).
Heckman and Vytlacil (2007a,b) provide various additional references. All of these papers, with
the notable exception of Cunha, Heckman, and Schennach (2010), rely on linear factor models
where the unobservables are assumed to be written as the sum of a linear combination of mutually
independent factors and an idiosyncratic shock.

     In this paper we bring together the literature on factor models with the literature on the
identification and estimation of triangular binary choice models (Chesher (2005); Vytlacil and
Yildiz (2007); Shaikh and Vytlacil (2011); Han and Vytlacil (2017)) by exploring the informational
content of factor structures in this class of models.2 Focusing on this class can be well motivated
from both an empirical and theoretical perspective. From the former, many treatment effect models
fit into this framework as treatment is typically a binary and endogenous variable in the system,
whose effect on outcomes is often a parameter the econometrician wishes to conduct inference
on. From a theoretical perspective, inference on this type of system can be complicated, if not
impossible without strong parametric assumptions, which may not be reflected in the observed
data. Imposing no restriction on the structure of endogeneity often fails to achieve identification
of parameter, or at best only do so in sparse regions of the data, thus making inference impractical
in practice. In this context, modeling the endogeneity between the selection and the outcome by
a factor structure may be a useful "in-between" setting, which, at the very least, can be used to
gauge the sensitivity of the parametric approach to their stringent assumptions.

    We start our analysis by imposing a particular factor structure to the two unobservables in this
system and explore the informational content of this assumption. We assume that the unobservables
    1
     See also Abbring and Heckman (2007) for an extensive discussion of factor structures and prior studies using
these models in the context of treatment effect estimation.
   2
     See also recent work by Lewbel, Schennach, and Zhang (2020), who study the identification of a triangular linear
model assuming that the disturbances are related through a factor model.



                                                         2
from the treatment equation (V ) and the outcome equation (U ) are related through the following
factor model:

      U = 0 V +                                                                                    (1.1)

where  is an unobserved random variable assumed to be distributed independently of V and 0 is a
scalar parameter. This structure generalizes the canonical case where the unobservables (U, V ) are
jointly normally distributed, for which this relationship always holds. Our main finding is that there
is indeed informational content of factor structures in the sense that, in contrast to prior literature -
notably Vytlacil and Yildiz (2007) - one no longer requires an additional "non-standard" exclusion
restriction, nor the strong support conditions on the covariates entering the outcome equation that
are generally needed for identification in these models. Our identification results are constructive
and translate directly into a rank based estimator of the coefficient associated with the binary
endogenous variable, which we provide and study in a supplement to this paper.

    While an appealing feature of the structure considered in Equation (1.1) is that it is a natural
extension of the bivariate Probit specification that has often been considered in the literature, this
model does impose significant restrictions on the nature of the dependence between the unobserv-
ables U and V . In the paper we extend this baseline specification by considering a linear factor
structure of the form:


      U   = 0 W + 1                                                                                (1.2)
      V   = W + 2                                                                                  (1.3)


where (W, 1 , 2 ) are mutually independent unobserved random variables. We study the informa-
tional content of this extended factor structure in the context of triangular binary choice models
and establish identification, assuming access to at least two continuous noisy measurements of the
unobserved factor W . This setup has been used in a number of applications, in particular in labor
economics. In these applications, the unobserved factor is typically interpreted as latent individual
ability, about which several continuous noisy measurements are available from the data. This is
the case of, for instance, Carneiro, Hansen, and Heckman (2003), Cunha, Heckman, and Schennach
(2010), Heckman, Humphries, and Veramendi (2018) and Ashworth, Hotz, Maurel, and Ransom
(2020), who use components of the Armed Services Vocational Aptitude Battery test as measure-
ments of cognitive ability. At a high level, it is interesting to note that these results complement
Bai and Ng (2010), who show that, in the context of a linear regression model with endogenous
regressors, factor models have identifying power, in that they can be used to create instrumental
variables even when none of the observed variables are valid instruments.

    The rest of the paper is organized as follows. In Section 2 we formally describe the triangular
system with our factor structure, and discuss our main identification results for the parameters of



                                                   3
interest in this model. Section 3 explores identification in more general factor structure models
which involve multiple idiosyncratic errors, in a context where one has access to two continuous
noisy measurements of the common unobserved factor. Finally, Section 4 concludes. The Supple-
mentary Material collects the proofs of our results, derives and studies the asymptotic properties
of a rank-based estimator for 0 , explores its finite sample properties through some Monte Carlo
simulation exercises, and shows the non-identification of the two-factor model with a compact-
supported common factor and no continuous repeated measurements. Finally, we also establish the
sharp identified set of 0 when the support condition for point-identification is violated.

    Notation: throughout the paper we write 1{A} to denote the usual indicator function that takes
value 1 if event A happens, and 0 otherwise. We also denote by d(U ) and d(U |V ) the lengths of
the support of random variable U , and the conditional support of U given V , respectively.


2     Triangular Binary Model with Factor Structure

2.1    Set-up and Main Identification Result

In this section we consider the identification of the following triangular binary model:


      Y1 = 1{Z1 0 + Z3 0 + 0 Y2 - U > 0}                                                                         (2.4)
      Y2 = 1{Z 0 - V > 0}                                                                                        (2.5)


where Z  (Z1 , Z2 ) and (U, V ) is a pair of random shocks. Z2 and Z3 provide the exclusion
restrictions in the model, and the distribution of (Z2 , Z3 ) is required to be nondegenerate conditional
on Z1 0 + Z3 0 . We further assume that the error terms U and V are jointly independent of
(Z1 , Z2 , Z3 ). The endogeneity of Y2 in (2.4) arises when U and V are not independent.

    The above model, or minor variations of it, have often been considered in the recent literature.
See for example, Vytlacil and Yildiz (2007), Abrevaya, Hausman, and Khan (2010), Klein, Shan,
and Vella (2015), Vuong and Xu (2017), Khan and Nekipelov (2018) and references therein. A
key parameter of interest in our paper and in the rest of the literature is 0 . In this paper we
provide conditions under which the parameters of interest are point-identified. As such, our analysis
complements alternative partial-identification approaches that have been proposed in the context
of triangular binary models. See, in particular, Chiburis (2010), Shaikh and Vytlacil (2011), and
Mourifi´e (2015).3 As discussed in the aforementioned papers, the parameter 0 is difficult, if not
impossible to identify and estimate without imposing parametric restrictions on the unobserved
    3
      In Section E in the supplement, we establish the sharp identified set of 0 when the support condition for
point-identification is violated. This result highlights that, except for the fact that the sign of 0 is identified, we
generally cannot say much about the value of |0 |. Related work by Shaikh and Vytlacil (2011) also provides partial
identification results for a triangular binary model. That the bounds for 0 are generally tighter in their analysis
reflects the identifying power of the additional support restrictions that they impose.



                                                          4
variables in the model, (U, V ).

    The difficulty of identifying 0 in semi-parametric "distribution-free" models, and the sensitivity
of its identification to misspecification in parametric models is what motivates the factor structure
we add in this paper to the above model. Specifically, to allow for endogeneity in the form of possible
non-zero correlation between U and V , we augment the model with the following equation:

      U = 0 V +                                                                                   (2.6)

where  is an unobserved random variable, assumed to be distributed independently of (V, Z1 , Z2 , Z3 ),
and 0 is an additional unknown scalar parameter. Importantly, this type of factor structure al-
ways holds when the residuals of both equations are jointly normally distributed. Furthermore, this
specification corresponds to the type of structure used in Independent Component Analysis (ICA),
where V and  are two mutually independent factors. This method has found many applications in
various fields, including signal processing and image extraction; applications in economics include
e.g., Hyv¨arinen and Oja (2000), Moneta, Hoyer, and Coad (2013) and Gourieroux, Monfort, and
Renne (2017). While, in contrast to the ICA literature, the factors and the factor loadings are not
the main objects of interest in our analysis, this dimension-reducing structure plays a key role in
our identification results.

   Our aim is to first explore identification of the parameters (0 , 0 , 0 , 0 , 0 ) under standard
nonparametric regularity conditions on (V, ). Note that the parameter 0 in the selection equation
can be identified up to scale in various ways. See, for example, Klein and Spady (1993) and Han
(1987), among others. We then impose the usual condition that one of 0 's coordinates is equal to
one to fix the scale. For simplicity, for the rest of the paper, we denote X  Z 0 and assume X is
observed. We further define X1  Z1 0 + Z3 0 . However, we cannot identify 0 and 0 beforehand.
We propose instead to identify them along with 0 .

   Our main identification result is based on the Assumptions A1-A4 we state below:

A1 The first coefficient of 0 is normalized to one so that 0 = (1, T         T
                                                                       0,-1 ) . The parameter
    0  (0 , 0 , 0,-1 , 0 ) is an element of a compact subset of d1 +d3 +1 , where d1 and d3 are
    the dimensions of Z1 and Z3 , respectively.

A2 The vector of unobserved variables, (U, V, ) is continuously distributed with support on a
    subset of 3 and independently distributed of the vector (Z1 , Z2 , Z3 ). Furthermore, we assume
    that the unobserved random variables , V are distributed independently of each other.

A3 X is continuously distributed with absolute continuous density w.r.t. Lebesgue measure. Its
    density is bounded and bounded away from zero on any compact subset of its support.

A4 Let Z1,-1 be all the coordinates of Z1 except the first one, and d = d1 + d3 + 1. There exist 2d




                                                  5
                 (l)    (l)                   (l)    (l)
      vectors {z1 , z3 , x(l) }d
                               l=1 and {z
                                        ~1 , z    ~(l) }d
                                             ~3 , x     l=1 in the joint support of (Z1 , Z3 , X ) such that

                       (l)        (l)               (l)        (l)               (l)    (l)
            0 + (z1,-1 - z                    ~3 ) 0 - 0 (x(l) - x
                         ~1,-1 ) 0,-1 + (z3 - z                  ~(l) ) = z
                                                                          ~1,1 - z1,1 , l = 1, · · · , d

      and rank(M) = d, where
                                                                     
                              1         ···                1
                 (1)      (1)        (d)      (d) 
                z1,-1 - z
                        ~1,-1 · · · z1,-1 - z~1,-1 
            M =  (1)
                
                          (1)          (d)    (d)
                                                     .
                 z3 - z ~3    · · · z3 - z   ~3      
                   (1)
                 x -x   ~ (1) ··· x - x( d ) ~ ( d )



   Before turning to our main identification result, a couple of remarks are in order.

Remark 2.1. The first part of Assumption A1 is a standard scale normalization. Assumption
A2 is also standard in this literature. The assumption that the instruments are independent of the
unobservables can also be found in, among others, Abrevaya, Hausman, and Khan (2010), Vytlacil
and Yildiz (2007), Klein, Shan, and Vella (2015), and Khan and Nekipelov (2018). The assumption
of independence between  and V is also made in Bai and Ng (2002) and Carneiro, Hansen, and
Heckman (2003).

Remark 2.2. Assumptions A3 and A4 impose some restrictions on the distributions of the co-
variates entering the selection and outcome equations, respectively. Specifically, Assumption A3
requires one component of the covariates Z entering the selection equation to be continuously dis-
tributed, which is often required in models with discrete outcomes. In contrast, Assumption A4 only
requires some variation of (Z1 , Z3 ). In particular, the distribution of (Z1 , Z3 ) cannot be degenerate
but is allowed to be discrete. This assumption can be interpreted as a full rank condition, which
ensures that the system of linear equations that delivers point identification has a unique solution.

    We now turn to our main identification result, Theorem 2.1, which concludes that under our
stated conditions and our factor structure we can attain point identification of the vector of pa-
rameters 0 .

Theorem 2.1. Under Assumptions A1-A4, 0 is point identified.

    An important takeaway from this result, which we discuss further in Subsection 2.2 below, is
that imposing the factor structure (2.6) yields point-identification under weaker support conditions
when compared to the existing literature, and does not require a second exclusion restriction either.
In particular, our model delivers point-identification of the parameters of interest even in situations
where all of the regressors from the outcome equation are discrete. This indicates that, from the
selection equation combined with the factor structure that we impose here, we can overturn the
non-identification result of Bierens and Hartog (1988) which would apply to the outcome equation
alone.


                                                                6
    The proof of Theorem 2.1, which is reported in Section H in the Supplementary Appendix,
                                                                  ~1 , Z
relies on the fact that, for two observations (Z1 , Z3 , X ) and (Z    ~3 , X
                                                                            ~ ),

                                                    ~1 , Z
            x P 11 (Z1 , Z3 , X )/fV (X ) + x P 10 (Z    ~3 , X
                                                              ~ )/fV (X
                                                                      ~) = 0
                  ~1 ) 0 + (Z3 - Z
        0 + (Z1 - Z              ~3 ) 0 - 0 (X - X
                                                 ~ ) = 0,                                                   (2.7)

where fV (·) is the pdf. of V , which is identified over the support of X , and P ij (z1 , z3 , x) 
P rob(Y1 = i, Y2 = j |Z1 = z1 , Z3 = z3 , X = x) (x P ij (z1 , z3 , x)) denote the choice probability
(partial derivative of the ij -choice probability with respect to the third argument), which are both
identified from the data.
Remark 1. This identification result can be extended to the case of a separable nonparametric
factor model. Namely, consider the following relationship between unobserved components:

      U = g0 (V ) + ~                                                                                       (2.8)

where   ~ is an unobserved random variable assumed to be distributed independently of V and all
instruments. g0 (·) is an unknown function assumed to satisfy standard smoothness conditions. The
parameter of interest is (0 , 0 , 0 ), but now the unknown nuisance parameter in the factor equation
is infinite dimensional. By replacing 0 X by g0 (X ) in (2.7), we have

                                                    ~1 , Z
            x P 11 (Z1 , Z3 , X )/fV (X ) + x P 10 (Z    ~3 , X
                                                              ~ )/fV (X
                                                                      ~) = 0
                  ~1 ) 0 + (Z3 - Z
        0 + (Z1 - Z              ~3 ) 0 - (g0 (X ) - g0 (X
                                                         ~ )) = 0.                                          (2.9)

One can then establish identification after modifying the rank condition A4 by replacing 0 (x(l) -x
                                                                                                  ~(l) )
by g0 (x(l) ) - g0 (~
                    x(l) ).


2.2    Connection with Prior Literature

We now discuss in detail how our setup and main identification result relates to the existing liter-
ature.

   In a related work, Han and Vytlacil (2017) consider the identification of a generalized bivariate
Probit model.4 Our linear factor structure and the one-parameter copula model considered in Han
and Vytlacil (2017) are not nested by each other. First, note that based on the factor structure,
we can recover F , the distribution of , as a function of (FU , FV , 0 ) by deconvolution. We can
then write the copula of (U, V ) as
                                  -1
                                 FV  (v )
         -1       -1                            -1
  FU,V (FU  (u), FV  (v ))   =              F (FU  (u) - 0 w; FU , FV , 0 )fV (w)dw = C (u, v ; FU , FV , 0 ).
                                 -
   4
     See also recent work by Han and Lee (2019) who study semiparametric estimation and inference in the framework
considered by Han and Vytlacil (2017).



                                                          7
The copula depends not only on 0 but also on two infinite dimensional parameters (FU , FV ). Thus,
unlike Han and Vytlacil (2017), our factor structure cannot be characterized by a one-parameter
copula. In addition, in order to achieve identification, Han and Vytlacil (2017) first nonparametri-
cally identify the two marginals by assuming the existence of a full support regressor that is common
to both equations.5 In contrast, our approach does not rely on the existence of such a regressor.
Under the factor structure assumed in our analysis, we bypass the nonparametric identification of
the marginals as a whole and directly consider the identification of the structural parameters. It
follows that our model cannot be nested by the one-parameter copula model considered by Han
and Vytlacil (2017). On the other hand, there exist one-parameter copula models that cannot be
decomposed into linear factor structures.6 This implies that our model does not nest Han and
Vytlacil (2017) either.

   Our analysis also relates to Vytlacil and Yildiz (2007) and Vuong and Xu (2017), who consider
the identification of 0 in a triangular binary model. Our identification result, however, differs
from theirs in important ways. Namely, denote X = Z 0 = Z1 1,0 + Z2 2,0 . Then, Assumption A4
implies that we can find a pair of observations (z1 , z2 , z3 ) and (~
                                                                     z1 , z
                                                                          ~2 , z
                                                                               ~3 ) such that

       z1 0 + z3 0 + 0 - 0 (z1 1,0 + z2 2,0 ) = z      ~3 0 - 0 (~
                                                ~1 0 + z         z1 1,0 + z
                                                                          ~2 2,0 ).                           (2.10)

In contrast, using our notation, Vytlacil and Yildiz (2007) require that one can find a pair of
observations (z1 , z2 , z3 ) and (~
                                  z1 , z
                                       ~2 , z
                                            ~3 ) such that z 0 = z
                                                                 ~ 0 and

       z1 0 + z3 0 + 0 = z
                         ~1 0 + z
                                ~3 0 .                                                                        (2.11)

Vuong and Xu (2017) do not assume the existence of Z3 . In our binary outcome setup, the
                                                                                         -1
functions h(0, x,  ) and h(1, x,  ) defined in Vuong and Xu (2017) are equal to 1{x + F-  U ( )  0}
                  -1
and 1{x +  + F-U ( )  0}, respectively, where x = z1 0 and F-U is the CDF of -U . Then, Vuong
and Xu (2017, Assumption C'(ii)) requires that we can find z1 and z  ~1 in the support of Z1 so that
                             -1                         -1                                 -1
                     ~1 0 + F-U (1 )  0} = 1{z
for any 1 , 2 , if 1{z                          ~1 0 + F-U (2 )  0}, then 1{z1 0 + 0 + F-    U (1 ) 
                        -1
0} = 1{z1 0 + 0 + F-U (2 )  0}. Provided that the support of U nests the supports of Z1 0 and
   5
      Han and Vytlacil (2017) establish their identification of the coefficient on the endogeneous regressor (Theorems
4.2 and 5.1) under the assumption that the marginal distributions F and F are known. Then, they verify this
condition by showing the identification of these two marginal distributions using large support common regressors.
    6
      For instance, suppose that (U, V ) has a Gaussian copula with correlation , and that the marginal distributions
of U and V are uniform [0, 1]. It then follows that, denoting by (.) the standard normal cdf., -1 (U ), -1 (V )
is bivariate normal with correlation , which in turn yields the following non-linear relationship between U and V :
U =  -1 (V ) + W , where W is normally distributed and independent from V .




                                                          8
Z1 0 + 0 , Vuong and Xu (2017, Assumption C'(ii)) is then equivalent to:7

       z1 0 + 0 = z
                  ~1 0 .                                                                                          (2.12)


    Several remarks are in order. First, note that sufficient support conditions for the restrictions
(2.10)­(2.12) are d(Z1 0 + Z3 0 - Z 0 0 )  |0 |, d(Z1 0 + Z3 0 |Z 0 )  |0 |, and d(Z1 0 |Z 0 ) 
|0 | with a positive probability, respectively. These three support conditions are such that

                    d(Z1 0 + Z3 0 - Z 0 0 )  d(Z1 0 + Z3 0 |Z 0 )  d(Z1 0 |Z 0 ),

where the first and second inequalities are strict if Z2 and Z3 have at least one continuous com-
ponent, respectively. Importantly, we show in Section E of the Supplement that for a version of
the triangular binary model with univariate Z2 and Z3 and no common regressor Z1 , the support
condition d(Z1 0 + Z3 0 |Z 0 )  |0 | is actually also necessary to the identification of the model
without factor structure. This implies that by imposing our factor structure, one can identify values
of 0 in a region that cannot be identified in the model considered by Vytlacil and Yildiz (2007).
Such region is characterized in Section E of the Supplement.

    Second, it directly follows from these support conditions that, in the presence of a factor model
and in contrast to both Vytlacil and Yildiz (2007) and Vuong and Xu (2017), variation in Z2
helps in the identification of 0 . In that sense, the factor model allows to restore the intuition
from standard IV approaches in linear models that variation in the instrument Z2 is critical to the
identification of the parameters of the outcome equation. Related to this, the support of Z2 plays
an important role in our identification analysis. In particular, if Z2 is discrete, our identification
strategy requires sufficient variation in the variables in the outcome equation, namely Z1 and Z3 .
In this case, our support requirement is equivalent to that assumed by Vytlacil and Yildiz (2007).

    Third, another important aspect of Assumption A4 is that it does not impose any constraint on
the variables from the outcome equation. Specifically, consider a case where the outcome equation
does not contain a variable that is excluded from the selection equation (i.e., 0 = 0), the regressor
that is common to both equations, Z1 , is scalar and binary, and where 0 = 1. In this case, one can
show that the identifying support conditions associated with Vytlacil and Yildiz (2007) (2.11) and
Vuong and Xu (2017) (2.12) generally fail to hold, except for a finite set of values 0  {-1, 0, 1}. In
contrast, our support restriction (2.10) holds under more general conditions: without any restriction
on 0 if one element of Z2 is continuous with large support, and on a continuum of possible values
for 0 if one element of Z2 is continuous with bounded support. In that sense, the factor structure
replaces the need for a continuous component in (Z1 , Z3 ) in the outcome equation.
   7                                                                                                     -1
    To see this, note that if, say, z1 0 + 0 > z  ~1 0 , then we can find 1 , 2 such that -z1 0 - 0  F-    U (1 ) < -z~1 0
       -1
and F-    (
        U 2  ) < - z1   -  0 < - z
                                 ~  
                                   1 0 . This violates the above requirement,  and  thus, shows that Vuong and  Xu  (2017,
Assumption C'(ii)) implies (2.12). On the other hand, if z1 0 + 0 = z       ~1 0 , then Vuong and Xu (2017, Assumption
C'(ii)) holds trivially.




                                                            9
    Finally, at a high level, our identification strategy shares similarities with the Local Instrumen-
tal Variable (LIV) approach that has been proposed by Heckman and Vytlacil (2005) and further
discussed by Carneiro and Lee (2009). In particular, our identifying restriction (2.7) can be al-
ternatively derived from a local IV strategy applied to a potential outcomes model characterized
by Y1 (y2 ) = 1{Z1 0 + Z3 0 + 0 y2 - U > 0}, with treatment given by Y2 = 1{Z 0 - V > 0}.
In contrast to the LIV literature though, we focus in our analysis on the structural parameter 0
rather than on the marginal treatment effects. Our identification result shows that, by leveraging
the identifying power of the factor structure, one can identify 0 under weaker support restric-
tions than in the prior literature. In particular, our strategy makes it possible to use variation in
X = Z 0 to identify 0 , even when all the components of Z1 and Z3 are discrete.8



3        Extended Factor Structure in the presence of Continuous Mea-
         surements

Up until now we have proposed identification and estimation results for a triangular system with a
particular factor structure. A disadvantage of this structure is that it only includes one idiosyncratic
shock (). We consider below an extension that addresses this limitation.

        Namely, we consider the following model:

          Y1 = 1{X1 + 0 Y2 - U  0}
                                                                                                             (3.1)
          Y2 = 1{X - V  0},

where X1 = Z1 0 + Z3 0 , X = Z 0 , U = 0 W + 1 , V = W + 2 , and (W, 1 , 2 ) are mutually
independent. Recall that, following the arguments in Section 2.1 above, we assume that X is
observed. In addition, we assume two auxiliary continuous measurements

          Y3 = 0 W + 3                                                                                       (3.2)
          Y4 = 0 W + 4 ,                                                                                     (3.3)

where (W, 1 , 2 , 3 , 4 ) are mutually independent, and 0 = 0.

        Our identification result is based on the following assumptions:


B0 The first coefficient of 0 is normalized to one so that 0 = (1, T           T
                                                                         0,-1 ) . The parameter
    0  (0 , 0 , 0,-1 , 0 , 0 , 0 ) is an element of a compact subset of d1 +d3 +3 , where d1 and d3
    are the dimensions of Z1 and Z3 , respectively. The vector of unobservables in the outcome
    8
    An alternative approach to identifying this parameter can be found in Lewbel (2000). In his approach a second
equation to model the endogenous variable is not needed, nor is the factor structure we impose. However, he imposes
a strong support condition on a variable like Z3 requiring that it exceeds the length of the unobservable U .




                                                        10
      and selection equations (W, 1 , 2 , 3 ) are independently distributed of the vector (Z1 , Z2 , Z3 ).
      Both 1 and 2 are continuously distributed.

B1 0 = 0. X is continuously distributed with absolute continuous density w.r.t. Lebesgue
   measure over the whole real line, conditionally on Z1 and Z3 . The unconditional density of
   X is bounded and bounded away from zero on any compact subset of its support.

B2 W is not normally distributed or both 3 and 4 do not have a Gaussian component.

B3 E (3 ) = E (4 ) = 0, E (|3 |) < , and E (|4 |) < .
                                                                                                     
B4 E (exp(i2 )), E (exp(i3 )), and E (exp(i4 )) do not vanish for any                  , where i =      -1.

B5 E (exp(iW )) = 0 for all  in a dense subset of               .

B6 The distributions of W , 2 , and 3 admit uniformly bounded densities fW (·), f2 (·), and f3 (·)
    with respect to the Lebesgue measure that are supported on an interval (which may be
    infinite), respectively.

B7 Let Z1,-1 be all the coordinates of Z1 except the first one, and d = d1 + d3 + 1. There exist 2d
              (l) (l)            (l) (l) d
    vectors {z1 , z3 }d
                      l=1 and {z
                               ~1 , z~3 }l=1 in the joint support of (Z1 , Z3 ) and {w(l) }d      ~ (l) }d
                                                                                           l=1 , {w      l=1
    such that

                    (l)        (l)           (l)       (l)                       (l)    (l)
            0 + (z1,-1 - z                    ~3 ) 0 - 0 (w(l) - w
                         ~1,-1 ) 0,-1 + (z3 - z                  ~ (l) ) = z
                                                                           ~1,1 - z1,1 , l = 1, · · · , d

      and rank(M) = d, where
                                                             
                           1         ···           1
               (1)       (1)             (d)          (d) 
              z1,-1 - z~ 1 , - 1 · · · z 1 , - 1  - ~
                                                    z 1 ,-1 
                                                            
            M= z (1) - z (1)                (d)       (d)  .
               3       ~3        · · · z3 - z       ~3 
                 (1)
               w -w    ~  (1)    ··· w - w  ( d )   ~ (d)

We now discuss these assumptions, before turning to the identification result. First, Assumption
B0 is similar to Assumptions A1 and A2. We only need one of the idiosyncratic errors in the
continuous measurements to be independent of the covariates because the other one is used to
identify the distribution of the common factor W only. Second, as we assume in Assumption B1
that 0 = 0 and X has full support, the support condition

                                     d(Z1 0 + Z3 0 - 0 X )  |0 |.

holds automatically. The full support condition of X is necessary to identify the density of V ,
which is further used to identify the distribution of 2 . Assumption B1 reinforces this condition
by supposing that X has full support conditional on Z1 and Z3 , which is needed to identify the
parameters from the outcome equation in a second step. Since X = Z 0 with Z = (Z1 , Z2 ), this is


                                                       11
in turn equivalent to Z2 having full support conditional on Z1 and Z3 . Third, Assumptions B2­B6
imply Assumptions 1 to 4 in Hu and Schennach (2013). In practice we add the condition that the
characteristic function of 2 does not vanish, which is used for the deconvolution arguments in the
proof of Theorem 3.1. We refer the reader to Hu and Schennach (2013) for more discussions of
these assumptions.9

Theorem 3.1. If (K.31)­(3.3) and Assumptions B0­B7 hold, then 0 are identified.

    The proof of Theorem 3.1 can be found in Section I of the Supplement. Several remarks
are in order. First, while we allow for a more general factor structure on the unobservables U
and V , we also depart from our baseline specification by supposing that we have access to two
continuous noisy measurements of the common factor W . This is a standard requirement in the
nonparametric measurement error literature (Hu and Schennach, 2008). Besides, assuming access
to a set of (selection-free) noisy measurements of the unobserved factors is also very standard in the
evaluation literature. See, among many others, Carneiro, Hansen, and Heckman (2003), Heckman
and Navarro (2007), Heckman and Vytlacil (2007a), and Cunha, Heckman, and Schennach (2010).
For instance, in applications in labor economics, the unobserved factor W often captures individual
ability. This would apply, for example, to the evaluation of the effect of college employment (Y2 )
on college graduation (Y1 ). In these cases, cognitive skill measurements, such as the ASVAB test
components that are available in the NLSY79 and NLSY97 surveys, are natural and often used
candidates for these types of continuous measurements (Ashworth, Hotz, Maurel, and Ransom,
2020).

    Second, as is clear from the proof of Theorem 3.1, the key purpose of the continuous measure-
ments is to identify the distribution of the common factor W . While we assume in this section
that the measurement equations are linear, it is possible to identify 0 with a more general nonlin-
ear system of continuous measurements, provided that the researcher has access to at least three
such measurements. One can then combine Theorem 2 in Cunha, Heckman, and Schennach (2010)
(Section 3.3, pp. 894-895), that yields identification of the distribution of W , with the proof of
Theorem 3.1 in order to show identification of 0 for the case of nonlinear auxiliary measurements.
Assuming access to a set of at least three measurements also makes it possible to relax the non-
normality requirement imposed in Assumption B2.

     Third, similar to the earlier discussions in Remark 2.2 and Section 2.2, Assumption B7 may
still hold even when Z3 is an empty set and Z1 is discrete, since W is assumed to have full
support. In such a case, identification primarily relies on the factor structure and the variation
of the covariates in the selection equation, rather than that in the outcome equation. In this
respect, this identification result is similar in spirit to Theorem 2.1 and different from the existing
identification results in the literature for triangular binary models, e.g., Vytlacil and Yildiz (2007)
and Vuong and Xu (2017). More generally, in Section E.2 in the supplement we establish that the
  9
      Note that Hu and Schennach (2013, Assumptions 5 and 6) hold automatically in our model with 0 = 0.



                                                       12
factor model provides identification restrictions that are not otherwise available.10



4        Conclusion

In this paper, we explore the identifying power of linear factor structures in the context of si-
multaneous binary response models. We impose two alternative types of factor structures on the
unobservables of the model. The first setup is a natural distribution-free extension of the bivariate
Probit model, while the second model corresponds to a standard linear factor model with one com-
mon factor and two equation-specific idiosyncratic shocks. We establish that both factor models
have identifying power in that they make it possible to relax some of the exclusion and support
conditions typically required for identification in this class of models (Vytlacil and Yildiz, 2007).
Overall, our analysis complements results obtained by Bai and Ng (2010) in the context of a linear
regression model with endogenous regressors, and, more generally, adds to our understanding of
the identifying power of factor models, beyond their well known usefulness to recover the joint
distribution of potential outcomes from the marginal distributions.

    The work here opens areas for future research. The factor structure we assume could prove
useful in more general nonlinear models. For instance, non-triangular discrete systems have shown
to be an effective way to model entry games in the empirical industrial organization literature- see,
for example, Tamer (2003). However, as shown in Khan and Nekipelov (2018), identification of
structural parameters in these models can be even more challenging than for the triangular model
considered in this paper. It would be useful to determine if factor structures on the unobservables
could alleviate this problem. We leave this open question to future work.



References
Abbring, J., and J. Heckman (2007): "Econometrics Evaluation of Social Programs, Part III:
 Distributional Treatment Effects, Dynamic Treatment Effects, Dynamic Discrete Choice, and
 General Equilibrium Policy Evaluation," in Handbook of Econometrics, Vol. 6B, ed. by J. J.
 Heckman, and E. E. Leamer. North Holland.

Abowd, J. M., and D. Card (1989): "On the Covariance Structure of Earnings and Hours
 Changes," Econometrica, 57(2), 411­445.
    10
     Specifically, we consider a version of the model (K.31), where we do not impose the factor structure and allow
for an arbitrary (unknown to econometricians) dependence structure across the unobservables of the model. In this
case, we show non-identification of 0 as long as |0 | > b - a, where [a, b] denotes the conditional support of X1 given
X and, consistent with our Assumption B1, X has full support on the real line. However, by imposing the factor
structure (and other conditions implied by B0­B7), Theorem 3.1 shows that 0 is identified for this model even when
|0 | > b - a.




                                                          13
Abrevaya, J., J. Hausman, and S. Khan (2010): "Testing for Causal Effects in a Generalized
 Regression Model with Endogenous Regressors," Econometrica, 78(6), 2043­2061.

Ashworth, J., V. J. Hotz, A. Maurel, and T. Ransom (2020): "Changes Across Cohorts
 in Wage Returns to Schooling and Early Work Experiences," forthcoming, Journal of Labor
 Economics.

Bai, J., and S. Ng (2002): "Determining the Number of Factors in Approximate Factor Models,"
 Econometrica, 70(1), 191­221.

Bai, J., and S. Ng (2010): "Instrumental Variable Estimation in a Data Rich Environment,"
 Econometric Theory, 26(6), 1577­1606.

Bierens, H., and J. Hartog (1988): "Non-Linear Eegression with Discrete Explanatory Vari-
  ables, with an Application to the Earnings Function," Journal of Econometrics, 38(3), 269­299.

Bonhomme, S., and J.-M. Robin (2010): "Generalized Non-Parametric Deconvolution with an
 Application to Earnings Dynamics," Review of Economic Studies, 77(2), 491­533.

Butucea, C., and C. Matias (2005): "Minimax estimation of the noise level and of the decon-
 volution density in a semiparametric convolution model," Bernoulli, 11(2), 309­340.

Carneiro, P., K. Hansen, and J. J. Heckman (2003): "Estimating Distributions of Treatment
 Effects with an Application to the Returns to Schooling and Measurement of the Effects of
 Uncertainty on College Choice," International Economic Review, 44(2), 361­422.

Carneiro, P., and S. Lee (2009): "Estimating distributions of potential outcomes using local
 instrumental variables with an application to changes in college enrollment and wage inequality,"
 Journal of Econometrics, 149(2), 191­208.

Chen, S., S. Khan, and X. Tang (2016): "On the Informational Content of Special Regressors
 in Heteroskedastic Binary Response Models," Journal of Econometrics, 193, 162­182.

Chesher, A. (2005): "Nonparametric identification under discrete variation," Econometrica,
 73(5), 1525­1550.

Chiburis, R. (2010): "Semiparametric Bounds on Treatment Effects," Journal of Econometrics,
 159(2), 267­275.

Cunha, F., J. J. Heckman, and S. M. Schennach (2010): "Estimating the Technology of
 Cognitive and Noncognitive Skill Formation," Econometrica, 78(3), 883­931.

Geary, R. (1942): "Inherent relations between random variables," Proceedings of the Royal Irish
 Academy, 47, 63­76.




                                               14
Gourieroux, C., A. Monfort, and J.-P. Renne (2017): "Statistical inference for independent
 component analysis: Application to structural VAR models," Journal of Econometrics, 196(1),
 111­126.

Han, A. (1987): "Non-parametric analysis of a generalized regression model: The maximum rank
 correlation estimator," Journal of Econometrics, 35(2­3), 303­316.

Han, S., and S. Lee (2019): "Estimation in a Generalization of a Bivariate Probit Models with
 Dummy Endogenous Regressors," Journal of Applied Econometrics, 34(6), 994­1015.

Han, S., and E. J. Vytlacil (2017): "Identification in a generalization of bivariate probit models
 with endogenous regressors," Journal of Econometrics, 199(1), 63­73.

Heckman, J., and S. Navarro (2007): "Dynamic Discrete Choice and Dynamic Treatment
 Effects," Journal of Econometrics, 136(2), 341­396.

Heckman, J. J., J. Humphries, and G. Veramendi (2018): "Returns to Education: The
 Causal Effects of Education on Earnings, Health, and Smoking," Journal of Political Economy,
 126, S197­S246.

Heckman, J. J., and E. Vytlacil (2005): "Structural equations, treatment effects, and econo-
 metric policy evaluation 1," Econometrica, 73(3), 669­738.

Heckman, J. J., and E. J. Vytlacil (2007a): "Econometric evaluation of social programs, part I:
 Causal models, structural models and econometric policy evaluation," Handbook of econometrics,
 6, 4779­4874.

         (2007b): "Econometric evaluation of social programs, part II: Using the marginal treat-
  ment effect to organize alternative econometric estimators to evaluate social programs, and to
  forecast their effects in new environments," Handbook of econometrics, 6, 4875­5143.

Henderson, D., Q. Li, C. Parmeter, and S. Yao (2015): "Gradient-based Smoothing Parame-
 ter Selection for Nonparametric Regression Estimation," Journal of Econometrics, 184, 233­241.

Horowitz, J. (1992): "A Smoothed Maximum Score Estimator for the Binary Response Model,"
 Econometrica, 60(3).

Hu, Y., and S. M. Schennach (2008): "Instrumental variable treatment of nonclassical mea-
 surement error models," Econometrica, 76(1), 195­216.

Hu, Y., and S. M. Schennach (2013): "Nonparametric identification and semiparametric estima-
 tion of classical measurement error models without side information," Journal of the American
 Statistical Association, 108(501), 177­186.

Hyva¨ rinen, A., and E. Oja (2000): Independent component analysis: algorithms and applica-
 tions, vol. 13. Elsevier.


                                               15
Jochmans, K. (2013): "Pairwise-comparison estimation with nonparametric controls," Economet-
  rics Journal, 16, 340­372.

Khan, S. (2001): "Two Stage Rank Estimation of Quantile Index Models," Journal of Economet-
 rics, 100, 319­355.

Khan, S., and D. Nekipelov (2018): "Information structure and statistical information in dis-
 crete response models," Quantitative Economics, 9(2), 995­1017.

Khan, S., and E. Tamer (2018): "Discussion of "Simple Estimators for Invertible Index Models"
 by Ahn et al.," Journal of Business & Economic Statistics, 36, 11­15.

Klein, R., C. Shan, and F. Vella (2015): "Estimation of marginal effects in semiparametric
 selection models with binary outcomes," Journal of Econometrics, 185(1), 82­94.

Klein, R., and R. Spady (1993): "An Efficient Semiparametric Estimator for Binary Response
 Model," Econometrica, 61(2), 387­421.

Lewbel, A. (2000): "Semiparametric Qualitative Response Model Estimation with Unknown
  Heteroscedasticity or Instrumental Variables," Journal of Econometrics, 97(1), 145­177.

Lewbel, A., S. M. Schennach, and L. Zhang (2020): "Identification of a Triangular Two
  Equation System Without Instruments," Working Paper.

Moneta, A., D. E. P. O. Hoyer, and A. Coad (2013): "Causal inference by independent
 component analysis: Theory and applications," Oxford Bulletin of Economics and Statistics,
 75(5), 705­730.

       ´, I. (2015): "Sharp Bounds on Treatment Effects in a Binary Triangular System," Journal
Mourifie
 of Econometrics, 187(1), 74­81.

Newey, W., and D. McFadden (1994): "Large Sample Estimation and Hypothesis Testing," in
 Handbook of Econometrics, Vol. 4, ed. by R. Engle, and D. McFadden. North Holland.

Powell, J., J. Stock, and T. Stoker (1989): "Semiparametric Estimation of Index Coeffi-
 cients," Econometrica, pp. 1403­1430.

Reiersol, O. (1950): "Identifiability of a Linear Relation Between Variables Which are Subject
 to Error," Econometrica, 18(4), 375­389.

Shaikh, A. M., and E. Vytlacil (2011): "Partial Identification in Triangular Systems of Equa-
  tions with Binary Dependent Variables," Econometrica, 79(3), 949­955.

Sherman, R. (1993): "The Limiting Distribution of the Maximum Rank Correlation Estimator,"
  Econometrica, 61, 123­137.



                                              16
         (1994a): "Maximal Inequalities for Degenerate U-Processes with Applications to Opti-
  mization Estimators," Annals of Statistics, 22, 439­459.

        (1994b): "U-Processes in the Analysis of a Generalized Semiparametric Regression Esti-
  mator," Econometric Theory, 10, 372­395.

Tamer, E. (2003): "Incomplete Bivariate Discrete Response Model with Multiple Equilibria,"
 Review of Economic Studies, 70(1), 147­167.

Vuong, Q., and H. Xu (2017): "Counterfactual mapping and individual treatment effects in
 nonseparable models with binary endogeneity," Quantitative Economics, 8(2), 589­610.

Vytlacil, E. J., and N. Yildiz (2007): "Dummy Endogenous Variables in Weakly Separable
 Models," Econometrica, 75(3), 757­779.




                                             17
                Supplement to "Informational Content of Factor Structures in
                         Simultaneous Binary Response Models"


      Shakeeb Khan                  Arnaud Maurel                           Yichong Zhang
      Boston College       Duke University, NBER and IZA           Singapore Management University


                                              December 2020



                                                Abstract

         This paper gathers the supplementary material to the original paper. In Section E, we discuss
      the identification power of the factor structure. In Section F, we propose an estimator based
      on our constructive identification strategy and establish its asymptotic properties. Section G
      contains a simulation study. In Sections H and I, we prove Theorems 2.1 and 3.1, respectively.
      In Section J, we establish the asymptotic distribution for the rank estimator. In Section K,
      we consider the identification of the model with two idiosyncratic shocks but no continuous
      repeated measurements of the common factor. In Sections L, M, N and O, we prove Theorems
      E.1, E.2, K.1 and K.2, respectively.


    Keywords: Factor Structures, Discrete Choice, Causal Effects.




E     Identification with and without Factor Structure

E.1    Identification Without Auxiliary Measurements

In this section, we discuss the information content of factor structure. For illustration purpose, we
focus on the "condensed" model:

      Y1 = 1{X1 + 0 Y2 - U  0}
                                                                                                         (E.4)
      Y2 = 1{X - V  0}.




                                                    18
Assumption 1.
1. (X1 , X )  (U, V ).
2. (X1 , X ) are continuously distributed with absolute continuous joint density w.r.t. Lebesgue
measure. The conditional support of X1 given X is [a, b].
3. V is continuously distributed over and its density w.r.t. Lebesgue measure exist.

Theorem E.1. If Assumption 1 holds, then |0 |  b - a is necessary and sufficient for 0 to be
identified.

   We note that under Assumption 1, |0 |  b - a is equivalent to the fact that we can find x1
    ~1 in the support of X1 such that 0 = x1 - x
and x                                          ~1 .

   Next, we assume, in addition to Assumption 1, the factor structure, i.e., (2.6) in Section 2. Our
rank estimator can be written as an M-estimator

       ^ = arg max Qn () 
                                   g
                                   ^i,j ()
                
                             i=j


in which

                                 ^
               ^ 11 (X1,i , Xi )/f
^i,j () = [1{2 P
g                                             ^ 10           ^
                                  V (Xi ) + 2 P (X1,j , Xj )/fV (Xj )  0}1{(X1,i , Xi , X1,j , Xj ;  )  0}
                                 ^
               ^ 11 (X1,i , Xi )/f
         + 1{2 P                              ^ 10           ^
                                  V (Xi ) + 2 P (X1,j , Xj )/fV (Xj ) < 0}1{(X1,i , Xi , X1,j , Xj ;  ) < 0}],



with

       (x1 , x, x    ~; ) = x1 +  - x - (~
                ~1 , x                   x1 -  x
                                               ~).

We will study the asymptotic properties of this estimator in Section F.

   The information content explored by the M-estimator can be summarized as follows:

                      ~ 1 , X, X
       A2 () = {(X1 , X        ~ ),(X1 , X, X
                                            ~1, X
                                                ~ ; 0 )  0 > (X1 , X, X
                                                                      ~1, X
                                                                          ~ ; )
                                                ~1, X
                                    or (X1 , X, X   ~ ; 0 ) < 0  (X1 , X, X
                                                                          ~1, X
                                                                              ~ ; )}.

Then we cannot distinguish, from the true parameter 0 , all impostors in

                                         A2 = { : P (A2 ()) = 0}.

In the condensed model, if Supp(X1 , X ) = [a, b]×[c, d], then 0 is identified if |0 | < b-a+|0 |(d-c).
Recall Theorem E.1, without imposing factor structure, the necessary and sufficient condition for
achieving identification is |0 |  b - a. Therefore, the blue area in the Figure below is the additional
parts of parameter space that are identified with factor structure but not otherwise.



                                                    19
                                            |0 | = b - a + |0 |(d - c)
                                               0




                                 |0 | = b - a
                                                                    0




                           Figure 1: Identifying Power of Factor Structure

Theorem E.2. Assumption 1 holds. When |0 | > b - a, the sharp identified set for 0 is

                       A = { :  > b - a if 0 > 0 and  < a - b if 0 < 0}.




    Theorem E.2 highlights that, in the case without the factor structure and 0 does not satisfy
the parameter restriction, except for the fact that the sign of 0 is identified, we actually cannot
say much about the value of |0 |. When we assume the factor structure, the parameter is still not
identified if |0 | > b - a + |0 |(d - c). In addition, suppose 0 > 0. In this case, if we do not impose
factor structure, by Theorem E.2, the sharp identified set is { :  > b - a} while with the factor
structure, the identified set (not necessarily sharp) is  > b - a + | |(d - c). This implies, when
identification fails in both cases, the blue area is also the extra identifying power on the identified
set given by the factor structure.


E.2       Identification with two auxiliary measurements

Next, we expand our condensed model to include two continuous measurements. We show in this
case, without the factor structure, 0 is not identified. This is in contrast with the identification
result established in Theorem 3.1.

       Suppose in addition to (E.4), we also observe two continuous measurements denoted as Y3 and
Y4 .

Assumption 2.
1. (X1 , X )  (U, V, Y3 , Y4 ).
2. (X1 , X ) are continuously distributed with absolute continuous joint density w.r.t. Lebesgue


                                                  20
measure. The conditional support of X1 given X is [a, b].
3. V is continuously distributed over and its density w.r.t. Lebesgue measure exist.

Theorem E.3. If Assumption 2 holds, then |0 |  b - a is necessary and sufficient for 0 to be
identified.

    The proof of Theorem E.3 is similar to that of Theorem E.1, and thus, is omitted. In the proof
of Theorem E.1, we show that when |0 | > b - a, we can find an impostor  = 0 and U     ~ such that
for any x1  [a, b] and any v  Supp(V ), we have

         ~  x1 + |V = v ) = P (U  x1 + 0 |V = v )
      P (U
         ~  x1 |V = v ) = P (U  x1 |V = v ).
      P (U

                                                                                              ~ , V, )
This implies the conditional CDF of (Y1 , Y2 ) given (X1 , X ) under the DGPs (U, V, 0 ) and (U
are the same, and thus, 0 is observationally equivalent to the impostor . Similarly, with the two
continuous measurements, we can use the exact same construction of U       ~ and  to show that, for
any x1  [a, b] and (v, y3 , y4 )  Supp(V, Y3 , Y4 ), we have

         ~  x1 + |V = v, Y3 = y3 , Y4 = y4 ) = P (U  x1 + 0 |V = v, Y3 = y3 , Y4 = y4 )
      P (U
         ~  x1 |V = v, Y3 = y3 , Y4 = y4 ) = P (U  x1 |V = v, Y3 = y3 , Y4 = y4 ).
      P (U

This implies the conditional CDF of (Y1 , Y2 , Y3 , Y4 ) given (X1 , X ) under the DGPs (U, V, Y3 , Y4 , 0 )
and (U~ , V, Y3 , Y4 , ) are the same too. Such non-identification result holds even when X has full
support.


F     Estimation and Asymptotic Properties

Our identification result is constructive in the sense that it motivates an estimator for the param-
eters of interest which we describe in detail here.

    As we did in Section E, to simplify exposition, in the following we focus exclusively on the
parameters 0 , 0 . Recall the choice probabilities P ij (x1 , x) = P rob(Y1 = i, Y2 = j |X1 = x1 , X = x)
and its second derivative 2 P ij (x1 , x), which can be estimated as we describe below. Another
function needed for our identification result is the density function of the unobserved term V ,
denoted by fV (·). This is also unknown, but from the structure of our model can be recovered
from the derivative with respect to the instrument X of E [Y2 |X ], and hence is estimable from
the data. Note that the proof of Theorem 2.1 shows that the sign of the index evaluated at two
different regressor values, which we denote here by (X1 , X ) and (X  ~1, X
                                                                          ~ ) is determined by the choice
probabilities via

                                         ~1, X
      2 P 11 (X1 , X )/fV (X ) + 2 P 10 (X   ~ )/fV (X
                                                     ~)  0                    ~1 -  X
                                                                 X1 +  - X - (X     ~ )  0.


                                                    21
This motivates us to use the maximum rank correlation estimator proposed by Han (1987).

    Implementation requires further details to pay attention to. The unknown choice probabilities,
their derivatives, and the density of V will be estimated using nonparametric methods, and for this
we adopt locally linear methods as they are particularly well suited for estimating derivatives of
functions.

    With functions and their derivatives estimated in the first stage of our procedure, the second
stage plugs in these estimated values into an objective function to be optimized. Specifically, letting
^ denote (^
          ,  ^ ), our estimator is of the form:

       ^ = arg max Qn (),
                                Qn ()            g
                                                 ^i,j ()                                                      (F.5)
                  
                                           i=j


in which

                                 ^
               ^ 11 (X1,i , Xi )/f
^i,j () = [1{2 P
g                                             ^ 10           ^
                                  V (Xi ) + 2 P (X1,j , Xj )/fV (Xj )  0}1{(X1,i , Xi , X1,j , Xj ;  )  0}
                                 ^
               ^ 11 (X1,i , Xi )/f
         + 1{2 P                              ^ 10           ^
                                  V (Xi ) + 2 P (X1,j , Xj )/fV (Xj ) < 0}1{(X1,i , Xi , X1,j , Xj ;  ) < 0}],



with

       (x1 , x, x    ~; ) = x1 +  - x - (~
                ~1 , x                   x1 -  x
                                               ~).

We note that this estimator falls into the class of those which optimize a nonsmooth U-process
involving components estimated nonparametrically in a preliminary stage.1 Examples of other
estimators in this class can be found in Khan (2001), Abrevaya, Hausman, and Khan (2010),
Jochmans (2013), Chen, Khan, and Tang (2016), and our approach to deriving the limiting dis-
tribution theory of our estimator will follow along the steps used in those papers. Our limiting
distribution theory for this estimator is based on the following regularity conditions:

RK1 0 lies in the interior of , a compact subset of R2 .

RK2 The index X is continuously distributed with support on the real line, and has a density
   function which is twice continuously differentiable.

RK3 (Order of smoothness of probability functions and regressor density functions) The functions
   P i,j (·) and fX1 ,X (·.·) (the density function of the random vector (X1 , X )) are continuously
    1
      An alternative estimation procedure could be based on the exact relationship in (2.7). Note the equality on the
left-hand side of (2.7) is a function of the data alone and not the unknown parameters. The right-hand side equality
can then be regarded as a moment condition to estimate the unknown parameters. We describe this estimator and
derive its asymptotic properties in the Online Supplement to the paper. While the two estimation approaches will
have similar asymptotic properties (root-n consistent, asymptotically normal), we prefer the rank estimator in (F.5)
which involves fewer tuning parameters. Furthermore rank type estimators in general are more robust to certain
types of misspecification, as pointed out in Khan and Tamer (2018).



                                                           22
      differentiable of order p2 .

RK4 (First stage kernel function conditions) K (·), used to estimate the choice probabilities and
   their derivatives is an even function, integrating to 1 and is of order p2 .

RK5 (Rate condition on first stage bandwidth sequence) The first stage bandwidth sequence Hn
   used in the nonparametric estimator of the choice probability functions and their derivatives
                 p2 -1
   satisfies nHn        0 and n-1/4 Hn-1  0.



   The smoothness condition in Assumption RK4 and Assumption RK5 is due to the fact that we
need to nonparametrically estimate 2 P ij (X1 , X ) with sufficiently faster convergence rate. This will
require a stronger smoothness condition than that required for standard nonparametric estimation.
Assumption RK5 ensures that the bias of the first stage estimator of the derivative function con-
verges at the parametric rate and the RMSE of this estimator (with two regressors) is fourth-root
consistent, so results for two step estimation in Newey and McFadden (1994) can be applied.

    Based on these conditions, we have the following theorem, whose proof is in Section J of the
Supplementary Appendix which characterizes the rate of convergence and asymptotic distribution
of the proposed estimator:

Theorem F.1. Under Assumptions RK1-RK5,
      
         n(^ - 0 )  N (0, V -1 V -1 )                                                             (F.6)

where the forms of the Hessian term V and outer score term  are described in detail in Section J
of the Supplementary Appendix.



G     Finite Sample Properties

In this section we explore the finite sample properties of the proposed estimation procedure via a
simulation study. We will also see how sensitive the performance of the proposed estimator is to the
factor structure assumption. As a base comparison, we also report results for the estimator proposed
in Vytlacil and Yildiz (2007) to see how sensitive it is to their second instrument restriction.

    Our data are simulated from base models of the form

      Y1 = 1{X1 + 0 Y2 - U  0}                                                                    (G.1)
      Y2 = 1{X - V > 0},                                                                          (G.2)


   where X1 is binary with success probability 0.6, X has marginal distribution N (0, 1), X1 and
X are mutually independent, (X1 , X )  (V, ), U = 0 V + . (V, ) are distributed independently



                                                  23
of each other, where  is distributed following a standard normal distribution, and V is distributed
either standard normal, Laplace, or T (3). The parameters (0 , 0 ) = (-0.25, 1.2) or (0.5, 1.2).

   Since X1 is discrete, Vytlacil and Yildiz's (2007) identification condition does not hold. However,
the identification condition in this paper becomes

                                                     ||  length of the support of X,

which holds.

    For each choice of sample size n = 100, 200, 400, 800, 1, 600, we simulate 280 samples and report
the bias, standard deviation (std), root mean squared error (RMSE), and median absolute deviation
(MAD) for both Vytlacil and Yildiz's (2007) estimator (VY) and ours (KMZ). For implementation,
we use the second order local polynomial along with Gaussian kernels to nonparametrically estimate
the 2 P 11 (x1 , x) and 2 P 10 (x1 , x). The bandwidth we use is h1 = x N -1/7 where x is the standard
deviation of X . fV (x) is nonparametrically estimated using a local linear estimator with the tuning
parameter h2 = x N -1/6 .

   As results from the table indicate, the finite sample performance of our estimator generally
agrees with the asymptotic theory. The RMSE for the estimator proposed here is decreasing as the
sample size increases, as one could expect given the consistency property of our estimator. Besides,
                                                  
the decay rate of the RMSE and MAD is about 2 when n  400 as sample sizes doubles, in line
with the parametric rate of convergence of our estimator.

    Vytlacil and Yildiz's (2007) estimator, which does not exploit the factor structure, demonstrates
inconsistency for certain parameter values, as indicated by the bias and median bias not shrinking
with the sample size. In addition, the RMSE and MAD do not appear to decline at all, which also
suggests that Vytlacil and Yildiz's (2007) estimator is inconsistent in these designs.2

                                                            Table 1: Normal V ,  = 0.5
                            Normal                                               Laplace                                               T(3)
                  kmz                       vy                         kmz                       vy                        kmz                       vy
  N     Bias     RMSE     MAD     Bias     RMSE     MAD      Bias     RMSE     MAD     Bias     RMSE     MAD     Bias     RMSE     MAD     Bias     RMSE     MAD
  100   -0.026    0.665   0.660   -0.246    0.658   0.500     0.032    0.634   0.560   -0.293    0.658   0.500    0.010    0.676   0.665   -0.225    0.662   0.500
  200    0.004    0.591   0.475   -0.329    0.633   0.500    -0.015    0.568   0.400   -0.336    0.612   0.500   -0.003    0.616   0.495   -0.279    0.629   0.500
  400    0.005    0.483   0.365   -0.341    0.573   0.500     0.030    0.459   0.310   -0.323    0.559   0.500    0.018    0.542   0.405   -0.314    0.589   0.500
  800    0.065    0.456   0.300   -0.348    0.544   0.500     0.096    0.391   0.250   -0.357    0.511   0.500    0.046    0.462   0.295   -0.346    0.552   0.500
1,600    0.040    0.321   0.195   -0.413    0.503   0.500     0.017    0.294   0.190   -0.450    0.506   0.500    0.034    0.371   0.240   -0.368    0.506   0.500

   2
    Because X1 is binary, Vytlacil and Yildiz's (2007) estimator can only take 3 possible values: 0, -1 or 1. In
particular, when  = 0.5, in most of the replications, the estimator takes values 0 or 1. When  = -0.25, in most of
the replications, the estimator takes value -1. In both of these cases, the MAD remains constant over the different
sample sizes.




                                                                               24
                                                         Table 2: Normal V ,  = -0.25
                               Normal                                              Laplace                                               T(3)
                     kmz                       vy                        kmz                       vy                        kmz                       vy
     N     Bias     RMSE     MAD     Bias     RMSE     MAD     Bias     RMSE     MAD     Bias     RMSE     MAD     Bias     RMSE     MAD     Bias     RMSE     MAD
     100   -0.088    0.650   0.555   -0.466    0.710   0.750    0.092    0.614   0.530   -0.358    0.650   0.750    0.004    0.619   0.505   -0.430    0.681   0.750
     200   -0.035    0.599   0.420   -0.446    0.681   0.750    0.012    0.552   0.385   -0.485    0.689   0.750   -0.008    0.583   0.425   -0.463    0.687   0.750
     400   -0.016    0.467   0.325   -0.487    0.668   0.750   -0.010    0.388   0.200   -0.552    0.686   0.750   -0.003    0.496   0.340   -0.489    0.675   0.750
     800   -0.028    0.324   0.165   -0.591    0.697   0.750    0.006    0.279   0.180   -0.599    0.701   0.750    0.032    0.399   0.230   -0.533    0.682   0.750
   1,600   -0.006    0.244   0.150   -0.654    0.718   0.750   -0.028    0.204   0.130   -0.714    0.738   0.750   -0.021    0.279   0.190   -0.629    0.710   0.750




      In the following, we also consider three DGPs (DGPs 1­3) such that the one-factor model does
  not hold but the identification assumption in Vytlacil and Yildiz (2007) does. In this case, our
  simulation results show that while, as expected, the estimator VY is still valid, our estimator still
  performs reasonably well. Interestingly, this offers suggestive evidence that our estimator is robust
  to some degree of misspecification. As such, these results complement previous work highlighting
  the robustness of rank type estimators to misspecification Khan and Tamer (2018). In DGP 4, the
  identification assumptions in both Vytlacil and Yildiz (2007) and our paper hold. In this case, we
  found that our estimator has similar performance as that proposed by Vytlacil and Yildiz (2007).

       The outcome and selection equations are the same as (G.1) and (G.2), respectively. Then,

DGP 1 : (X1 , X ) is jointly standard normally distributed. Let (e1 , e2 ) jointly Laplace distributed
                                                                1    -0.5
      with mean zero and variance-covariance matrix  =                       , e3 and e4 are uniformly
                                                              -0.5     1
      distributed on (0, 1), independent of each other, and independent of (e1 , e2 ), V = e1 + e3 - 0.5,
      U = e2 + e4 - 0.5, and  = -0.25.

DGP 2 : (X1 , X ) are the same as above, U = e1 + e2 - 0.5, and V = e1 + e3 - 0.5, where e1
      is standard normally distributed, (e2 , e3 ) are uniformly distributed on (0, 1), (e1 , e2 , e3 ) are
      mutually independent, and  = -0.25.
                                                                             exp(e1 +e2 -0.5)-1                    exp(e1 +e3 -0.5)-1
DGP 3 : (X1 , X ) are the same as above, V =                                          4         ,          U =              4         ,         (e1 , e2 , e3 ) are
      defined as above, and  = -0.5.

DGP 4 : (X1 , X ) are the same as above, V is Laplace distributed with mean zero and standard
      derivation 0.5, U = V + V - 0.5, where V is uniform distributed on (0, 1) and is independent
      of V , and  = -0.25.

  For DGPs 1, 2, and 4, when computing 2 P 11 (x1 , x) and 2 P 10 (x1 , x), we use bandwidths h1 =
  x1 N -1/7 and h = x N -1/7 for variables X1 and X , respectively, where x1 and x are the standard
  errors of X1 and X , respectively. To estimate the density fV (x), we use bandwidth h2 = x N -1/6 .
  For DGP 3, we use h1 = h2 = h = x1 N -1/5 . In all simulations, we use 280 replications.




                                                                                 25
                                               Table 3: Alternative DGPs

                                      DGP 1                                            DGP 2
                    kmz                            vy                        kmz                     vy
 N        Bias     RMSE      MAD Bias             RMSE     MAD     Bias     RMSE     MAD Bias       RMSE     MAD
   100    -0.065    0.678    0.600 -0.055          0.666   0.535   -0.058    0.621   0.505  -0.05    0.621   0.470
   200    -0.118    0.543    0.370 -0.080          0.497   0.320   -0.122    0.523   0.350 -0.097    0.495   0.350
   400    -0.117    0.413    0.280 -0.071          0.378   0.245   -0.062    0.335   0.215 -0.033    0.316   0.220
   800    -0.102    0.287    0.170 -0.062          0.243   0.160   -0.031    0.242   0.150 -0.008    0.215   0.150
 1,600    -0.071    0.193    0.140 -0.035          0.155   0.100   -0.038    0.167   0.100 -0.031    0.158   0.100
                                DGP 3                                                   DGP 4
   100    -0.012    0.583    0.480 -0.015          0.565   0.430   -0.057    0.401   0.240 -0.066    0.422   0.240
   200    -0.061    0.425    0.275 -0.068          0.399   0.270   -0.041    0.282   0.180 -0.049    0.263   0.145
   400    -0.041    0.259    0.170 -0.042          0.237   0.155   -0.062    0.184   0.135 -0.047    0.186   0.120
   800    -0.061    0.219    0.140 -0.047          0.182   0.120   -0.029    0.119   0.080 -0.034    0.115   0.070
 1,600    -0.038    0.130    0.080 -0.035          0.119   0.080   -0.024    0.090   0.060 -0.022    0.086   0.070



     In the first three DGPs, we see that VY's estimator has better performance in terms of both bias
and MSE. On the other hand, although the models do not have a factor structure, our estimator
still performs reasonably well. In the last DGP, support conditions in both Vytlacil and Yildiz
(2007) and our paper hold. Table 3 shows that our and Vytlacil and Yildiz's (2007) estimators
have similar performance in terms of bias and MSE. Although our estimator is expected to be more
efficient as we use the factor structure in estimation, it is not. We conjecture that it is because our
estimator does not necessarily use all the information, or in other words, achieve the semiparametric
efficiency bound. To establish the semiparametric efficient estimator in the model with and without
the factor structure is an interesting yet challenging task. We leave it as a topic for future research.



H        Proof of Theorem 2.1

Proof: Note that
                                 x
      P 11 (z1 , z3 , x) =            F (z1 0 + z3 0 + 0 - 0 v )fV (v )dv
                             -
                              +
      P 10 (~
            z1 , z
                 ~3 , x
                      ~) =             F (~      ~3 0 - 0 v )fV (v )dv.
                                          z1 0 + z
                             x
                             ~

Taking derivatives w.r.t. the third argument of the LHS function, we obtain

         x P 11 (z1 , z3 , x)/fV (x) = F (z1 0 + z3 0 + 0 - 0 x)
      -x P 10 (~
               z1 , z    ~)/fV (~
                    ~3 , x      x) = F (~      ~3 0 - 0 x
                                        z1 0 + z        ~).

By Assumption A4, we know that there exists pairs such that

                                                             ~ 0 + Z
                                     Z1 0 + Z3 0 + 0 - 0 X = Z     ~ 0 - 0 X.
                                                                           ~
                                                               1     3



                                                            26
Because F (·) is monotone increasing, we have

                                                   ~1 , Z
           x P 11 (Z1 , Z3 , X )/fV (X ) + x P 10 (Z    ~3 , X
                                                             ~ )/fV (X
                                                                     ~) = 0
                 ~1 ) 0 + (Z3 - Z
       0 + (Z1 - Z              ~3 ) 0 - 0 (X - X
                                                ~) = 0

Note the LHS of the above display is identified from data. Denote Z1,1 as the first element of Z1 ,
whose coefficient is set to one. The rest of Z1 is denoted as Z1,-1 , whose coefficient is denoted as
0,-1 . Then, we have

                  ~1,-1 ) 0,-1 + (Z3 - Z
     0 + (Z1,-1 - Z                    ~3 ) 0 - 0 (X - X
                                                       ~) = Z
                                                            ~1,1 - Z1,1 .

                                             (l)    (l)              (l)   (l)
Then, by Assumption A4, we can find (z1 , z3 , x(l) )d
                                                     l=1 and (~
                                                              z1 , z    ~(l) )d
                                                                   ~3 , x     l=1 such that
                                                   
                    1         ···        1
           (1)      (1)        (d)      (d) 
          z1,-1 - z
                  ~1,-1 · · · z1,-1 - z~1,-1 
     rank  (1)
          
                    (1)          (d)    (d)
                                                = d.
           z3 - z ~3    · · · z3 - z   ~3      
             (1)
           x -x   ~ (1) ··· x - x( d ) ~ ( d )


Then, we can identify (0 , 0 , 0 , 0 ) by solving the linear system that

              (1)       (1)            (1)    (1)                             (1)   (1)
      0 + (z1,-1 - z                    ~3 ) 0 - 0 (x(1) - x
                   ~1,-1 ) 0,-1 + (z3 - z                  ~(1) ) =~z1,1 - z1,1 ,
                                                                   .
                                                                   .
                                                                   .
              (d)       (d)           (d)     (d)                             (d)   (d)
     0 + (z1,-1 - z                    ~3 ) 0 - 0 (x(d) - x
                  ~1,-1 ) 0,-1 + (z3 - z                  ~(d) ) =~
                                                                  z1,1 - z1,1 .



This concludes the proof.



I   Proof of Theorem 3.1

                                  ~ = 0 W , 
For notation simplicity, we write W         ~0 = 0 /0 , ~0 = 1/0 , and

     Y2 = 1{X     ~ + 2 }
               ~0 W
          ~ + 3
     Y3 = W
     Y4 =    ~ + 4 .
          ~0 W

Because Assumptions B2­B6 hold, by applying Hu and Schennach (2013, Theorem 1) to Y3 and
                                             ~ , 3 , and 4 as well as 0 /0 = 
Y4 , we can identify the densities for 0 W = W                               ~0 .




                                                          27
     Then, we have

      y3 P(Y2 = 1, Y3  y3 |X = x) =y3               F2 (x - ~0 w)F3 (y3 - w)fW
                                                                             ~ (w )dw


                                         =     F2 (x - ~0 w)f3 (y3 - w)fW
                                                                        ~ (w )dw.


Applying Fourier transform w.r.t. y3 on both sides, we have

      F (y3 P(Y2 = 1, Y3  ·|X = x))(t) = F (F2 (x -       ~ (·))(t)F (f3 (·))(t),
                                                    ~0 ·)fW

where for a generic function g (w),

                     1
      F (g (·))(t) =             exp(-2itw)g (w)dw.
                      2

Therefore,

              F (y3 P(Y2 =1,Y3 ·|X =x))(·)
       F -1           F (f3 (·))(·)          (w)
                                                   = F2 (x - ~0 w),                                      (I.1)
                       fW
                        ~ (w )


where for a generic function g (w),

                        1
      F -1 (g (·))(t) =            exp(2itw)g (w)dw.
                         2

Note the LHS of (I.1) can be identified from data. We choose two pairs (x, w) and (x , w ) such
that w = w and

              F (y3 P(Y2 =1,Y3 ·|X =x))(·)                    F (y3 P(Y2 =1,Y3 ·|X =x ))(·)
       F -1           F (f3 (·))(·)          (w)       F -1           F (f3 (·))(·)           (w )
                                                   =                                                 .
                       fW
                        ~ (w )                                         fW
                                                                        ~ (w )

Then, given the monotonicity of F2 , we have

      x-~0 w = x - ~0 w ,

or

      ~0 = (x - x )/(w - w ),
      

which is identified. Given the identification of                            ~ , we can identify the
                                                 ~0 and the distribution of W
distribution of W =     ~ . Recall F (·) and f (·) are the CDF and PDF of 1 and 2 , respectively.
                     ~0 W           1          2

Then, we have

      P (Y2 = 1|X = x) = P (W + 2  x).


                                                          28
Because X has full support, we can identify the distribution of W + 2 . Then, it follows from
standard deconvolution argument and the fact that the distribution of W is identified that we can
identify the distribution of 2 . In addition, note that

      P 11 (z1 , z3 , x) =P (Y1 = 1, Y2 = 1|Z1 = z1 , Z3 = z3 , X = x)

                      =        F1 (z1 0 + z3 0 + 0 - 0 w)F2 (x - w)fW (w)dw

and

      P 10 (z1 , z3 , x) =P (Y1 = 1, Y2 = 0|Z1 = z1 , Z3 = z3 , X = x)

                      =        F1 (z1 0 + z3 0 - 0 w)(1 - F2 (x - w))fW (w)dw.

Taking derivatives of P 11 (z1 , z3 , x) and P 10 (z1 , z3 , x) w.r.t. x, we have

      x P 11 (z1 , z3 , x) =     F1 (z1 0 + z3 0 + 0 - w)f2 (x - w)fW (w)dw                 (I.2)

and

      -x P 10 (z1 , z3 , x) =      F1 (z1 0 + z3 0 - 0 w)f2 (x - w)fW (w)dw.                (I.3)


    Applying Fourier transform on both sides of (I.2) and (I.3), we have

      F (x P 11 (z1 , z3 , ·)) = F (F1 (z1 0 + z3 0 + 0 - ·)fW (·))F (f2 (·))               (I.4)

and

      F (-x P 10 (z1 , z3 , ·)) = F (F1 (z1 0 + z3 0 - ·)fW (·))F (f2 (·)).


    Then, by (I.4), we can identify F1 (z1 0 + z3 0 + 0 - ·) by

                                                 F (x P 11 (z1 , z3 , ·))
      F1 (z1 0 + z3 0 + 0 - 0 ·) = F -1                                     (·)/fW (·).
                                                     F (f2 (·))

Similarly, we can identify

                                           F (-x P 10 (z1 , z3 , ·))
      F1 (z1 0 + z3 0 - 0 ·) = F -1                                    (·)/fW (·).
                                               F (f2 (·))




                                                       29
    Because F1 (·) is monotone increasing, we have

                     F (x P 11 (z1 , z3 , ·))                         F (-x P 10 (~    ~3 , ·))
                                                                                  z1 , z
            F -1                                  (w)/fW (w) = F -1                               (w
                                                                                                   ~ )/fW (w
                                                                                                           ~)
                         F (f2 (·))                                       F (f2 (·))
        0 + (z1 - z
                  ~1 ) 0 + (z3 - z
                                 ~3 ) 0 - 0 (w - w
                                                 ~) = 0

                                                       (l)   (l)               (l)    (l)
    Then, by Assumption B7, we can find (z1 , z3 , w(l) )d
                                                         l=1 and (~
                                                                  z1 , z    ~ (l) )d
                                                                       ~3 , w      l=1 such that
                                                       
                     1           ···             1
             (1)      (1)        (d)       (d) 
            z1,-1 - z
                    ~1,-1 · · · z1,-1 - z~1,-1 
       rank  (1)
            
                      (1)          (d)     (d)  = d.
                                               
             z
             3    - z
                    ~ 3   · · ·  z 3   - ~
                                         z 3   
             w(1) - w
                    ~ (1) · · · w(d) - w ~ (d)

Then, we can identify (0 , 0 , 0 , 0 ) by solving the linear system that

               (1)       (1)                (1)      (1)                             (1)    (1)
       0 + (z1,-1 - z                    ~3 ) 0 - 0 (w(1) - w
                    ~1,-1 ) 0,-1 + (z3 - z                  ~ (1) ) =~z1,1 - z1,1 ,
                                                                     .
                                                                     .
                                                                     .
               (d)       (d)                (d)      (d)                             (d)    (d)
       0 + (z1,-1 - z                    ~3 ) 0 - 0 (w(d) - w
                    ~1,-1 ) 0,-1 + (z3 - z                  ~ (d) ) =~
                                                                     z1,1 - z1,1 .



This concludes the proof.



J      Proof of Theorem F.1

Recall we defined our two step rank estimator as follows: Letting ^ denote (^
                                                                            , ^ ), our estimator is
of the form:



       ^ = arg max Q
                   ^ n ()              g
                                       ^i,j ()
                 
                                 i=j


in which

                                 ^
               ^ 11 (X1,i , Xi )/f
^i,j () = [1{2 P
g                                             ^ 10           ^
                                  V (Xi ) + 2 P (X1,j , Xj )/fV (Xj )  0}1{(X1,i , Xi , X1,j , Xj ;  )  0}
                                 ^
               ^ 11 (X1,i , Xi )/f
         + 1{2 P                              ^ 10           ^
                                  V (Xi ) + 2 P (X1,j , Xj )/fV (Xj ) < 0}1{(X1,i , Xi , X1,j , Xj ;  ) < 0}],



with

       (x1 , x, x    ~; ) = x1 +  - x - (~
                ~1 , x                   x1 -  x
                                               ~)


                                                             30
    We first show consistency of the rank estimator. To do so we first define the objective function
Qif
 n,2 ( ),defined as

      Qif
       n,2 ( )              gi,j ()
                      i=j


where


gi,j () = [1{2 P 11 (X1,i , Xi )/fV (Xi ) + 2 P 10 (X1,j , Xj )/fV (Xj )  0}1{(X1,i , Xi , X1,j , Xj ; )  0}
            + 1{2 P 11 (X1,i , Xi )/fV (Xi ) + 2 P 10 (X1,j , Xj )/fV (Xj ) < 0}1{(X1,i , Xi , X1,j , Xj ; ) < 0}],


Since gi,j is bounded by 1 i, j , and our random sampling assumption, we have for each ,

                  p
      Qif
       n,2 ( )  E [gi,j ( )]  0 ( )


Furthermore, by Assumptions RK2, RK3 we can extend this result to converging uniformly over
   (see, e.g. Sherman (1994a), Sherman (1993).) 0 () is continuous in  by Assumptions
RK2,RK3, and uniquely maximized at  = 0 by our identification result in Theorem 2.1. Along
with Assumption RK1, the infeasible estimator, defined as the maximizer of Qif   n,2 ( ) converges in
probability to 0 by, for example Theorem 2.1 in Newey and McFadden (1994). To show consis-
tency of the feasible estimator, where we first estimate the choice probability functions and their
derivatives nonparametrically, we only now need to show the two objective functions converged to
each other uniformly in   . Consistency of the first stage estimators follows from Assumptions
RK3-RK5, see for example Henderson, Li, Parmeter, and Yao (2015). However, this does not
immediately imply convergence of the difference in feasible and infeasible objective functions since
the nonparametric estimators are inside indicator functions so the continuous mapping theorem
does immediately not apply. Nonetheless the desired result can still be attained in one of two ways.
One would be to replace indicator functions with smooth distribution functions in a fashion analo-
gous to Horowitz (1992). This would have the disadvantage of introducing tuning parameters, but
another approach would be to replace the indicator functions with their conditional expectations,
and note that the conditional expectations are smooth functions using Assumption RK2, RK3.
To see why, let m^ (xi ) be a nonparametric estimator of a function m(xi ), which is assumed to be
smooth. We evaluate the plim of

         ^ (xi ) > 0] - I [m(xi ) > 0] = I [m
      I [m                                  ^ (xi ) > 0, m(xi ) < 0] - I [m
                                                                          ^ (xi ) < 0, m(xi ) > 0]

we show that the first term converges in probability to 0 as identical arguments can be used for the
second term. Let  > 0 be given; P (I [m ^ (xi ) > 0, m(xi ) < 0] > )  E [I [m
                                                                            ^ (xi ) > 0, m(xi ) < 0]/




                                                       31
by Markov's inequality. But the expectation in the numerator on the right hand side is

     P (m                             ^ (xi ) > 0, m(xi )  -n ) + P (m
        ^ (xi ) > 0, m(xi ) < 0) = P (m                              ^ (xi ) > 0, m(xi )  (-n , 0))

where n is a sequence of positive numbers converging to 0, at a slow rate, e.g.(log n-1 ). The first
term on the right hand side is bounded above by

     P (|m
         ^ (xi ) - m(xi )| > n )  P ( m
                                      ^ (·) - m(·) > n )

where the notation m  ^ (·) - m(·) above denotes the sup norm over xi . The right hand side
probability above will be sufficiently small for n large enough by the rate of convergence of the
nonparametric estimator. The second term, P (m   ^ (xi ) > 0, m(xi )  (-n , 0)), is bounded above by
P (m(xi )  (-n , 0)) which by the smoothness of m(xi ) converges to 0, and hence can be made
arbitrarily small.

    To derive the rate of convergence and limiting distribution theory for the feasible estimator
where we first estimate choice probability functions and their derivatives nonparametrically, we
expand the nonparametric estimators around true functions that are inside the indicator function in
Qn2 . Then we can follow the approach in Sherman (1994b). Having already established consistency
of the estimator, we will first establish root-n consistency and then asymptotic normality. For
root-n consistency we will apply Theorem 1 of Sherman (1994b) and so here we change notation
to deliberately stay as close as possible to his. We will actually apply this theorem twice, first
establishing a slower than root-n consistency result and then root-n consistency. Keeping our
notation deliberately as close as possible to Sherman(1994b), here replacing our second stage rank
objective function Q^ 2,n () with G^n (), our infeasible objective function Qif () with Gn (), and
                                                                             n,2
denoting our limiting objective function, previously denoted by 0 (), by G (). We have the
following theorem:

Theorem J.1. (From Theorem 1 in Sherman (1994b)).

If n and n are sequences of positive numbers converging to 0, and

  1. ^ - 0 = op (n )

  2. There exists a neighborhood of 0 and a constant  > 0 such that G () - G (0 )    - 0              2

     for all  in this neighborhood.

  3. Uniformly over Op (n ) neighborhoods of 0
                                     
           ^n () = G () + Op (  - 0 / n) + op (  - 0 2 ) + Op (n )
           G


then ^ - 0 = Op (max(1/2 , n-1/2 )).




                                                 32
    Once we use this theorem to establish the rate of convergence of our rank estimator, we can
attain limiting distribution theory, which will follow from the following theorem:
Theorem J.2. (From Theorem 2 in Sherman (1994b)). Suppose              ^ is n-consistent for 0 , an
interior point of . Suppose also that uniformly over Op (n-1/2 ) neighborhoods of 0 ,

     ^n () = 1 ( - 0 ) V ( - 0 ) + 
     G
                                   1
                                      ( - 0 ) Wn + op (1/n)                                     (J.1)
             2                      n

where V is a negative definite matrix, and Wn converges in distribution to a N (0, ) random vector.
Then
     
        n(^ - 0 )  N (0, V -1 V -1 )                                                            (J.2)


    We first turn attention to applying Theorem J.1 to derive the rate of convergence of our estima-
tor. Having already established consistency of our rank estimator, we turn attention to the second
condition in Theorem J.1. To show the second condition, we will first derive an expansion for
G () around G (0 ). We denote that even though Gn () is not differentiable in , G () is sufficiently
smooth for Taylor expansions to apply as the expectation operator is a smoothing operator and
the smoothness conditions in Assumptions RK2, RK3. Taking a second order expansion of G ()
around G (0 ), we obtain

                                      1
     G () = G (0 ) +  G (0 ) ( - 0 ) + ( - 0 )  G ( )( - 0 )                                    (J.3)
                                      2

where  and  denote first and second derivative operators and  denotes an intermediate
value. We note that the first two terms of the right hand side of the above equation are 0, the first
by how we defined the objective function, and the second by our identification result in Theorem
2.1. Define

     V   G (0 )                                                                                 (J.4)

and V is positive definite by Assumption A3, so we have

     ( - 0 )  G (0 )( - 0 ) > 0                                                                 (J.5)

 G () is also continuous at  = 0 by Assumptions RK2 and RK3, so there exists a neighborhood
of 0 such that for all  in this neighborhood, we have

     ( - 0 )  G ()( - 0 ) > 0                                                                   (J.6)

which suffices for the second condition to hold.
   To show the third condition in Theorem J.1, we next establish the form of the remainder term
when we replace nonparametric estimators with the true functions they are estimating. Specifically


                                                   33
we wish to evaluate the difference between

            [1{2 P                 ^
                 ^ 11 (X1,i , Xi )/f                               ^
                                                 ^ 10 (X1,j , Xj )/f
                                     V (Xi ) + 2 P                   V (Xj )  0}1{(X1,i , Xi , X1,j , Xj ;  )  0}           (J.7)
        +   1{2 P                 ^
                ^ 11 (X1,i , Xi )/f                               ^
                                                ^ 10 (X1,j , Xj )/f
                                    V (Xi ) + 2 P                   V (Xj ) < 0}1{(X1,i , Xi , X1,j , Xj ;  ) < 0}          (J.8)


and

            [1{2 P 11 (X1,i , Xi )/fV (Xi ) + 2 P 10 (X1,j , Xj )/fV (Xj )  0}1{(X1,i , Xi , X1,j , Xj ; )  0}              (J.9)
                    11                                 10
        +   1{2 P        (X1,i , Xi )/fV (Xi ) + 2 P        (X1,j , Xj )/fV (Xj ) < 0}1{(X1,i , Xi , X1,j , Xj ; ) < 0}    (J.10)



 To establish a representation for this difference, we first simplify notation we write the expressions
as:


            I [m           ^ 2 (xj )  0]I [xij   0]
               ^ 1 (xi ) + m                                                                                              (J.11)
        + I [m
             ^ 1 (xi ) + m
                         ^ 2 (xj ) < 0]I [xij  < 0]                                                                       (J.12)


and


            I [m1 (xi ) + m2 (xj )  0]I [xij   0]                                                                         (J.13)
        + I [m1 (xi ) + m2 (xj ) < 0]I [xij  < 0]                                                                         (J.14)


respectively, where here xi denotes the separate components of x1i , xi , and analogous for xj . We
first explore

      (I [m           ^ 2 (xj )  0] - I [m1 (xi ) + m2 (xj )  0])I [xij   0]
          ^ 1 (xi ) + m

for each i, j inside the double summation:

         1
                         (I [m           ^ 2 (xj )  0] - I [m1 (xi ) + m2 (xj )  0])I [xij   0]
                             ^ 1 (xi ) + m                                                                                (J.15)
      n(n - 1)
                 i=j


    An immediate technical difficulty that arises with the above term is the presence of a nonpara-
metric estimator inside the indicator function above. A simple approach to deal with this would
be to replace the indicator function with a smoothed indicator function in a fashion analogous to
Horowitz (1992), under appropriate conditions on the kernel function and smoothing parameter.
Such an approach is not necessary as long as the nonparametric estimator m  ^ 1 (xi ) is asymptotically
normal, and asymptotically centered at m1 (xi ), which will be the case with our proposed kernel
estimator of the probability function and its derivative. In either approach (smoothed indicator or




                                                                       34
not) we can show that (J.15) can be represented as:

     1
                                                             ^ 2 (xj ) - m2 (xj ))) I [xij   0]+ op (n-1 ) (J.16)
                                    ^ 1 (xi ) - m1 (xi )) + (m
                      (0)fmij (0) ((m
  n(n - 1)
                i=j


where (0) denotes the standard normal pdf evaluated at 0, fmij (0) denotes the density function of
m1 (xi ) + m2 (xj ) evaluated at 0, and the op (n-1 ) term is uniform in  lying in op (1) neighborhoods
of 0 . Therefore, uniformly for  in an op (1) neighborhood of 0 , this remainder term converges to
0 at the rate of convergence of the first stage nonparametric estimator, which under Assumptions
RK3, RK4, RK5, is op (n-1/4 ). Thus by repeated application of Theorem J.1, we can conclude that
the estimator is root-n consistent. To show that the estimator is also asymptotically normal, we
will first derive a linear representation for the term:

         1
                                        ^ 1 (xi ) - m1 (xi ))I [xij   0]
                            (0)fmij (0)(m                                                                  (J.17)
      n(n - 1)
                      i=j


As this term is linear in the nonparametric estimator m  ^ 1 (xi ), the desired linear representation
follows from arguments used in Khan (2001). One slight difference here compared to Khan (2001)
is that here our nonparametric estimators and estimands are each ratios of derivatives. Nonetheless,
after linearizing these ratios as done in, e.g. Newey and McFadden (1994). Specifically, we have
that J.17 can be expressed as:

         1                                    1
                            (0)fmij (0)                  ^ 1num (xi ) - m1num (xi ))I [xij   0]
                                                        (m                                                 (J.18)
      n(n - 1)                            m1den (xi )
                      i=j



             1                              m1num (xi )
      -                       (0)fmij (0)                 ^ 1den (xi ) - m1den (xi ))I [xij   0]
                                                         (m                                                (J.19)
          n(n - 1)                          m1den (xi )2
                        i=j


where m^ 1num (xi ) denotes the numerator {2 P      ^ 11 (X1,i , Xi )}, the estimator of m1num (xi ) which de-
notes {2 P 11 (X1,i , Xi )}, and m
                                 ^ 1den (xi ) denotes the denominator f     ^V (Xi ), the estimator of m1den (xi )
which denotes fV (Xi ).

    Plugging in the definitions of the kernel estimators of m
                                                            ^ 1num (xi ), and m
                                                                              ^ 1den (xi ), results in a
third order process. Using arguments in Khan (2001) and Powell, Stock, and Stoker (1989) we can
express the third order U process as a second order U process plus an asymptotically negligible
remainder term. This is of the form:
           n
      1                   (xi )
                (0)                (y1i - m1num (xi ))E I [fmij (0)xij   0]|xi                             (J.20)
      n                m1den (xi )
          i=1

                      -f (x )
                       i
where (xi )  fX   X
                   (xi ) . We note that the function E fmij (0)I [xij   0]|xi , which we denote
here by H(xi , ) is a smooth function in . We will use this feature to expand H(xi , ) around


                                                               35
H(xi , 0 ). Analogous arguments can be used to attain a linear representation of (J.19), which is of
the form:
              n
        1                  2 (x1i )m1num (xi )
                  (0)                          (y2i   - m1den (xi ))E I [fmij (0)xij   0]|xi                         (J.21)
        n                     m1den (xi )2
            i=1

                          -fX (x1i )
where       2 (x1i )       fX (x1i ) .
                              1
                                         Grouping (J.20) and (J.21) we have

             n
      1                        1                                         m1num (xi )
                  (0)                      (xi )(y1i - m1num (xi )) -                2 (x1i )(y2i - m1den (xi )) H(xi ,  )
      n                   m1den (xi )                                    m1den (xi )
            i=1
                                                                                                                     (J.22)

Note that by Assumptions RK2, RK3, H(xi , ) is smooth in  implying the expansion

      H(xi , ) = H(xi , 0 ) +  H(xi , 0 ) ( - 0 )

Thus we can express (J.22) as the which we note is a mean 0 sum
              n
        1
                  1rnki ( - 0 )                                                                                      (J.23)
        n
            i=1

where

                                 1                                       m1num (xi )
      1rnki = (0)                           (xi )(y1i - m1num (xi )) -               2 (x1i )(y2i - m1den (xi ))  H(xi , 0 )
                             m1den (xi )                                 m1den (xi )
                                                                                                                    (J.24)

We can use identical arguments to attain a linear representation for the U - process:

           1
                                          ^ 2 (xj ) - m2 (xj )) I [xij   0]
                             (0)fmij (0) (m                                                                          (J.25)
        n(n - 1)
                       i=j


where m^ 2 (xj ) is also a ratio of nonparametric estimators where here the numerator is m   ^ 2n (xj ) de-
noting {2 P ^ (X1,j , Xj )}, the estimator of m2n (x2 ) which denotes {2 P (X1,j , Xj )}, and m
             10                                                               10                  ^ 2d (xj )
denotes the denominator f     ^V (Xj ), the estimator of m1den (xj ) which denotes fV (Xj ).

   and by using identical arguments it too can be represented as a mean 0 sum denoted here by
              n
        1
                  2rnki                                                                                              (J.26)
        n
            i=1

where 2rnki is defined as:

   Finally after grouping the two terms and expanding H(xi , ) around H(xi , 0 ) we get that (J.16)



                                                                36
can be represented as:
             n
        1
                  (1rnki + 2rnki ) ( - 0 ) + op (n-1 )                                       (J.27)
        n
            i=1


    Combining our results, from Theorem J.2, we have that
      
            n(^ - 0 )  N (0, V -1 V -1 )                                                     (J.28)

where

      V =  G (0 )                                                                            (J.29)

and

       = E (1rnki + 2rnki )(1rnki + 2rnki )                                                  (J.30)



K       Model with Two Idiosyncratic Shocks

In this section, we focus on the identification of (0 , 0 ) in the "condensed" model that X1 =
Z1 0 + Z3 0 is observed and

      Y1 = 1{X1 + 0 Y2 - U  0}
                                                                                            (K.31)
      Y2 = 1{X - V  0}.

with the understanding that (0 , 0 ) can be identified jointly with 0 and 0 , as shown in Theorems
2.1 and 3.1. We further impose U = 0 W + 1 , V = W + 2 , and (W, 1 , 2 ) are mutually
independent. First we consider the case 0 = 1 and X1 is binary, because even in this context,
for the baseline case with one idiosyncratic shock, we can identify 0 . But identification of 0
becomes more difficult in this model without the help of repeated measurements, as established in
the following theorem.

Theorem K.1. Suppose (K.31) holds, 0 is known to be one, X1 is binary, and W has a bounded
support [-b, -a] such that 0.5 > b - a and 1 - (b - a) > 0 > b - a, then 0 is not point identified.

   This nonidentification result motivates imposing additional structure on W , and we consider
the following model

C1 U = 0 W + 1             and V = 0 W + 2 .

C2 W is standard normally distributed.



                                                         37
C3 W , 1 and 2 are mutually independent.

C4 X has full support.

C5 Denote the density of 2 as f2 , then f2 does not have a Gaussian component in the sense
    that

            f2  G = {g is a density on         s.t. : g = g   for some density g implies that  = 0},

      where  is the density for a normal distribution with zero mean and  2 variance.

     Assumption C5 effectively assumes that the distribution of 2 has tail properties different from
those of a normal distribution. This type of assumption is made in the deconvolution literature as
it is necessary for identification of the target density when the error distribution is not completely
known- see, e.g., Butucea and Matias (2005).3 The importance of non-normality in factor models
goes back to Geary (1942) and Reiersol (1950), who have shown that factor loadings are identified
in a linear measurement error model if the factor is not Gaussian. In our case, note V = 0 W + 2
where W is standard normal and the density of V is identified from data. Here we want to identify
0 and the density of 2 . If 2 has a Gaussian component, then

      2 = 2 +  ~,
              ~W

where W ~ is a standard normal random variable that is independent of  and W and ~ > 0. It
                                                                      2
implies

      V = (0 W +  ~ )+ ,
                 ~W   2


where 2 does not have a Gaussian component. In addition, note that (0 W +  ~ ) = 2 + 
                                                                          ~W         ~ 2 G,
                                                                                 0
for some standard normal random variable G. Therefore, without Assumption B5, 0 is not
identified.

Theorem K.2. If Assumptions C1­C5 hold, then 0 , 0 and 0 are identified.

    Note that this identification result does not require any variation from X1 , which is in spirit
close to the one-factor model in our paper and is different from the identification result in Vytlacil
and Yildiz (2007). We also note that this result does not contradict the counterexample in the
paper. In the counterexample, we only assume that we know the support of W is bounded. Here
we assume that the full density of W , and thus, the support of W is known.
  3
    In fact, based on the results in Butucea and Matias (2005), W can belong to a more general class of known
distributions. Furthermore, we note that if 0 is known, then Assumption C5 is not necessary.




                                                     38
L     Proof of Theorem E.1

Denote P ij (x1 , x) = P rob(Y1 = i, Y2 = j |X1 = x1 , X = x). Then
                            x
      P 11 (x1 , x) =           FU (x1 + 0 |V = v )f (v )dv
                        -
                         +                                                                         (L.32)
        10
      P (~
         x1 , x) =                   x1 |V = v )f (v )dv.
                                 FU (~
                        x

Taking derivatives w.r.t. the second argument of the the LHS function, we have

      2 P 11 (x1 , x) = FU (x1 + 0 |V = x)f (x)
      2 P 10 (~
              x1 , x) = -FU (~
                             x1 |V = x)f (x).

If |0 |  b - a, then there exists a pair (x1 , x
                                               ~1 ) such that x1 + 0 = x
                                                                       ~1 . This pair can be identified
by checking the equation below:

                                   2 P 11 (x1 , x)/f (x) + 2 P 10 (~
                                                                   x1 , x)/f (x) = 0.

This concludes the sufficient part.

    When 0 < a - b, for any  < 0 , we can define

      ~ = U +  - 0
      U                                               if                       U  b + 0
      ~ =U
      U                                               if                       U > b + 0



Then for any x1  [a, b],

       ~  x1 + |V = v ) = P (U
    P (U                     ~  x1 + , U  b + 0 |V = v ) + P (U
                                                              ~  x1 + , U > b + 0 |V = v )

                                 = P (U  x1 + 0 |V = v )
          ~  x1 |V = v ) = P (U
       P (U                   ~  x1 , U  b + 0 |V = v ) + P (U
                                                             ~  x1 , U > b + 0 |V = v )

                                 = P (U  b + 0 , U  x1 + 0 - |V = v ) + P (b + 0 < U  x1 |V = v )
                                 = P (U  b + 0 |V = v ) + P (b + 0 < U  x1 |V = v )
                                 = P (U  x1 |V = v ),

where the third equality holds because, since 0 < a - b and  < 0 , b + 0  x1 + 0 -  for
x1  [a, b]. Let GU,V and GU                                                      ~
                              ~ ,V be the joint distribution of (U, V ) and (U , V ) respectively. Then
the above calculation with (L.32) imply that (0 , GU,V ) and (, GU       ~ ,V ) produce the identical pair
    11          10
(P (x1 , x), P (x1 , x)). In addition, the distribution of V is unchanged so that P (Y2 = 1|X = x)
is identified from data. Therefore, (0 , GU,V ) and (, GU  ~ ,V ) are observationally equivalent.




                                                            39
    Similarly, when 0 > b - a, for any  > 0 , we can define

      ~ = U +  - 0
      U                                      if                 U > a + 0
      ~ =U
      U                                      if                 U  a + 0



Then for any x1  [a, b],

     ~  x1 + |V = v ) = P (U
  P (U                     ~  x1 + , U  a + 0 |V = v ) + P (U
                                                            ~  x1 + , U > a + 0 |V = v )

                           = P (U  a + 0 |V = v ) + P (a + 0 < U  x1 + 0 |V = v )
                           = P (U  x1 + 0 |V = v ).
          ~  x1 |V = v ) = P (U
       P (U                   ~  x1 , U  a + 0 |V = v ) + P (U
                                                             ~  x1 , U > a + 0 |V = v )

                           = P (U  x1 |V = v ),

where we use the facts that x1  a + 0 and x1 - a <  for x1  [a, b]. So again, (0 , GU,V ) and
(, GU~ ,V ) are observationally equivalent.




M     Proof of Theorem E.2

The sign of 0 is identified by the data. In the following, we focus on deriving the results when
0 > b - a. By the proof of Theorem E.1, we have already shown that all  > 0 is in the identified
set. Now we consider b-a2+0
                               < 0 .

      ~ = U +  - 0
      U                                      if                  U >a+
      ~ =U
      U                                      if                  U a+



Then for any x1  [a, b],

       ~  x1 + |V = v ) = P (U
    P (U                     ~  x1 + , U  a + |V = v ) + P (U
                                                            ~  x1 + , U > a + |V = v )

                            = P (U  a + |V = v ) + P (a +  < U  x1 + 0 |V = v )
                            = P (U  x1 + 0 |V = v ).
           ~  x1 |V = v ) = P (U
        P (U                   ~  x1 , U  a + |V = v ) + P (U
                                                            ~  x1 , U > a + |V = v )

                            = P (U  x1 |V = v ) + P (U  x1 + 0 - , U > a + |V = v ).
                            = P (U  x1 |V = v ).

Here note that the last equality is because x1 + 0 -   b + 0 -   a +  if   b-a2
                                                                              +0
                                                                                 . Denote
 (1)   b-a+0                                          (1)
 =        2  . Then we have shown that there exists U () which only depends on  such that


                                                   40
for any x1  [a, b], any v and any 0 >   (1)

      P (U (1) ()  x1 + |V = v ) = P (U  x1 + 0 |V = v )
      P (U (1) ()  x1 |V = v ) = P (U  x1 |V = v ).

In particular, there exists U (1) ((1) ) such that

      P (U (1) ((1) )  x1 + (1) |V = v ) = P (U  x1 + 0 |V = v )
      P (U (1) ((1) )  x1 |V = v ) = P (U  x1 |V = v ).

Now repeating the above construction but replacing U with U (1) and 0 with (1) , we have for
                           (1)
any (1) >   (2)  b-a+    2     , there exists U (2) () such that for any x1  [a, b], any v and any
(1) >   (2) ,

      P (U (2) ()  x1 + (2) |V = v ) = P (U (1) ((1) )  x1 + (1) |V = v ) = P (U  x1 + 0 |V = v )
      P (U (2) ()  x1 |V = v ) = P (U (1) ((1) )  x1 |V = v ) = P (U  x1 |V = v ).

This concludes that any  such that 0 >   (2) is in the identified set. In general, by repeating
the procedure k times, we have that any  such that

                                                            1            0
                                  0 >   (k) = (1 -           k
                                                               )(b - a) + k
                                                            2            2
                                                                                                      0
is in the identified set. For any  > b - a, there exists some finite k such that  > (1 - 21
                                                                                          k )(b - a)+ 2k .

This concludes the result that  > b - a is in the identified set.

    Finally, since if  > b - a, 2 P 11 (x1 , x)+ 2 P 10 (~
                                                         x1 , x) > 0 for all pairs of (x1 , x) and (~
                                                                                                    x1 , x) while,
if   b-a, at least there exists one pair (x1 , x) and (~                          11
                                                           x1 , x) such that 2 P (x1 , x)+2 P (~  10  x1 , x)  0.
This implies   b - a is not in the identified set. Therefore, the sharp identified set when 0 > b - a
is (b - a, ).

    When 0 < a - b, a symmetric argument implies that the identified set is (-, a - b).



N      Proof of Theorem K.1

Our first result for this model illustrates how identification can become more difficult. In our first
result for this model, we show when -W has a bounded support, say [a, b], then 0 is not identified
if 0 > b - a. To establish this, consider an impostor  such that  < 0 . In addition, we consider
the case where 0 -  + b < 0 + a and  + b < a + 1. Such  exists because of the fact that
1 - (b - a) > 0 > b - a. Let  = 0 -  and (W       ~ ,~1 ,                                        ~ is
                                                          ~2 ) be mutually independent such that W




                                                       41
distributed as W - ,  ~2 is distributed as 2 - , and
           
            F1 (e)                                                   on e  a,
           
                                                                     on 1  (a, a + ],
            F1 (a)
           
            F1 (e - )                                                on e  (a + , b + ],
           
             0 +a-e              -b-
                    F (b) + 0e                                       on e  (b + , 0 + a],
           
                                 +a-b- F1 (0 + a)
           
            0 +a-b- 1
           
F ~1 (e) =  F1 (e)                                                   on e  (0 + a, 0 + b),
           
                            e-0 -b
                                -0 -b (F1 (a + 1) - F1 (0 + b))      on e  (0 + b, a + 1 + ],
           F1 (0 + b) + a+1+
           
           
            F1 (e - )                                                on e  (a +  + 1, b +  + 1],
           
           F (b + 1) + e-(b++1) (F (a + 0 + 1) - F (b + 1))
           
                                                                     on e  (b +  + 1, a + 0 + 1],
           
              1        a+0 -b-        1                 1
           
           F (e)
           
                                                                     on e > a + 0 + 1.
              1



Then, because -w
               ~ =  - w  [a + , b + ] and x1 = 0, 1,

     P (Y1 = 1, Y2 = 0|X = x, X1 = x1 ) =    F1 (x1 - w)(1 - F2 (x - w))fW (w)dw

                                        =    F~1 (x1 - w
                                                       ~ )(1 - F~2 (x - w
                                                                        ~ ))fw
                                                                             ~ (w
                                                                                ~ )dw.
                                                                                    ~

Similarly, because  - w ~ = 0 - w  [0 + a, 0 + b] and for e  (0 + a, 0 + b]  (1+ 0 + a, 1+ 0 + b],
F ~1 (e) = F1 (e), we have


     P (Y1 = 1, Y2 = 1|X = x, X1 = x1 ) =    F1 (x1 + 0 - w)F2 (x - w)fW (w)dw

                                        =    F1 (x1 +  - (w +  - 0 ))F2 (x - w)fW (w)dw

                                        =    F~1 (x1 +  - w
                                                          ~ )F~2 (x - w
                                                                      ~ )fw
                                                                          ~ (w
                                                                             ~ )dw.
                                                                                 ~

This implies 0 is not identified from the impostor .



O    Proof of Theorem K.2

We first show that both 0 and the density of 2 are identified. Note X has full support. This
implies the density of V denoted as fV (·) is identified via

     fV (v ) = v E (Y2 |X = v ).




                                                42
   In addition, we have

      fV (·) = f2  0 (·),

where  denotes the convolution operator. Suppose f2 (·) and 0 are not identified so that there
exist f2 (·) and  such that

      fV (·) = f2   (·).

Without loss of generality, we assume   0 , otherwise, we can just relabel f2 (·) and f2 (·).
Then we have

      f2 (·) = f2  ( 2 -2 ) .
                             0


By Assumption B5, we have  = 0 , which implies f2 (·) = f2 (·).

    In the following, we proceed given that f2 (·) and 0 are known. Recall F1 (·) as the CDF of
1 . Then,

      P 11 (x1 , x) =P (Y1 = 1, Y2 = 1|X1 = x1 , X = x) =      F1 (x1 + 0 - 0 w)F2 (x - 0 w)fW (w)dw

and

      P 10 (x1 , x) =P (Y1 = 1, Y2 = 0|X1 = x1 , X = x) =      F1 (x1 - 0 w)(1 - F2 (x - 0 w))fW (w)dw.

Taking derivatives of P 11 (x1 , x) and P 10 (x1 , x) w.r.t. x, we have

      x P 11 (x1 , x) =    F1 (x1 + 0 - 0 w)f2 (x - 0 w)fW (w)dw                              (O.33)

and

      -x P 10 (x1 , x) =    F1 (x1 - 0 w)f2 (x - 0 w)fW (w)dw.                                (O.34)


   Applying Fourier transform on both sides of (O.33) and (O.34), we have

      F (x P 11 (x1 , ·)) = F0 (F1 (x1 + 0 - 0 ·)fW (·))F (f2 (·))                            (O.35)

and

      F (-x P 10 (x1 , ·)) = F0 (F1 (x1 - 0 ·)fW (·))F (f2 (·)),                              (O.36)




                                                    43
where for a generic function g (w),

                     1
     F0 (g (·))(t) =         exp(-2it0 w)g (w)dw.
                      2

   Then, by (O.35), we can identify F1 (x1 + 0 - ·) by

                          -1       F (x P 11 (x1 , ·))
     F1 (x1 + 0 - 0 ·) = F                                (·)/fW (·).
                           0
                                      F (f2 (·))

Similarly, we can identify

                      -1     F (-x P 10 (x1 , ·))
     F1 (x1 - 0 ·) = F                               (·)/fW (·),
                       0
                                 F (f2 (·))

where for a generic function g (w),

      -1              0
     F 0
         (g (·))(t) =         exp(2it0 w)g (w)dw.
                       2

By finding the two pairs ((x1 , w), (x1 , w )) and ((~
                                                     x1 , w
                                                          ~ ), (~    ~ )) such that w - w = w
                                                                x1 , w                      ~-w
                                                                                              ~,

     F1 (x1 + 0 - 0 w) = F1 (x1 - 0 w ),                    x1 + 0 - 0 w
                                                    and F1 (~                    x1 - 0 w
                                                                       ~ ) = F1 (~      ~)

we can identify both 0 and 0 as the solution of the following linear system:

     0 + 0 (w - w) = x1 - x1                                     ~ -w
                                                          0 + 0 (w       ~1 - x
                                                                    ~) = x    ~1 .



References
Abbring, J., and J. Heckman (2007): "Econometrics Evaluation of Social Programs, Part III:
 Distributional Treatment Effects, Dynamic Treatment Effects, Dynamic Discrete Choice, and
 General Equilibrium Policy Evaluation," in Handbook of Econometrics, Vol. 6B, ed. by J. J.
 Heckman, and E. E. Leamer. North Holland.

Abowd, J. M., and D. Card (1989): "On the Covariance Structure of Earnings and Hours
 Changes," Econometrica, 57(2), 411­445.

Abrevaya, J., J. Hausman, and S. Khan (2010): "Testing for Causal Effects in a Generalized
 Regression Model with Endogenous Regressors," Econometrica, 78(6), 2043­2061.

Ashworth, J., V. J. Hotz, A. Maurel, and T. Ransom (2020): "Changes Across Cohorts
 in Wage Returns to Schooling and Early Work Experiences," forthcoming, Journal of Labor
 Economics.



                                                     44
Bai, J., and S. Ng (2002): "Determining the Number of Factors in Approximate Factor Models,"
 Econometrica, 70(1), 191­221.

Bai, J., and S. Ng (2010): "Instrumental Variable Estimation in a Data Rich Environment,"
 Econometric Theory, 26(6), 1577­1606.

Bierens, H., and J. Hartog (1988): "Non-Linear Eegression with Discrete Explanatory Vari-
  ables, with an Application to the Earnings Function," Journal of Econometrics, 38(3), 269­299.

Bonhomme, S., and J.-M. Robin (2010): "Generalized Non-Parametric Deconvolution with an
 Application to Earnings Dynamics," Review of Economic Studies, 77(2), 491­533.

Butucea, C., and C. Matias (2005): "Minimax estimation of the noise level and of the decon-
 volution density in a semiparametric convolution model," Bernoulli, 11(2), 309­340.

Carneiro, P., K. Hansen, and J. J. Heckman (2003): "Estimating Distributions of Treatment
 Effects with an Application to the Returns to Schooling and Measurement of the Effects of
 Uncertainty on College Choice," International Economic Review, 44(2), 361­422.

Carneiro, P., and S. Lee (2009): "Estimating distributions of potential outcomes using local
 instrumental variables with an application to changes in college enrollment and wage inequality,"
 Journal of Econometrics, 149(2), 191­208.

Chen, S., S. Khan, and X. Tang (2016): "On the Informational Content of Special Regressors
 in Heteroskedastic Binary Response Models," Journal of Econometrics, 193, 162­182.

Chesher, A. (2005): "Nonparametric identification under discrete variation," Econometrica,
 73(5), 1525­1550.

Chiburis, R. (2010): "Semiparametric Bounds on Treatment Effects," Journal of Econometrics,
 159(2), 267­275.

Cunha, F., J. J. Heckman, and S. M. Schennach (2010): "Estimating the Technology of
 Cognitive and Noncognitive Skill Formation," Econometrica, 78(3), 883­931.

Geary, R. (1942): "Inherent relations between random variables," Proceedings of the Royal Irish
 Academy, 47, 63­76.

Gourieroux, C., A. Monfort, and J.-P. Renne (2017): "Statistical inference for independent
 component analysis: Application to structural VAR models," Journal of Econometrics, 196(1),
 111­126.

Han, A. (1987): "Non-parametric analysis of a generalized regression model: The maximum rank
 correlation estimator," Journal of Econometrics, 35(2­3), 303­316.




                                               45
Han, S., and S. Lee (2019): "Estimation in a Generalization of a Bivariate Probit Models with
 Dummy Endogenous Regressors," Journal of Applied Econometrics, 34(6), 994­1015.

Han, S., and E. J. Vytlacil (2017): "Identification in a generalization of bivariate probit models
 with endogenous regressors," Journal of Econometrics, 199(1), 63­73.

Heckman, J., and S. Navarro (2007): "Dynamic Discrete Choice and Dynamic Treatment
 Effects," Journal of Econometrics, 136(2), 341­396.

Heckman, J. J., J. Humphries, and G. Veramendi (2018): "Returns to Education: The
 Causal Effects of Education on Earnings, Health, and Smoking," Journal of Political Economy,
 126, S197­S246.

Heckman, J. J., and E. Vytlacil (2005): "Structural equations, treatment effects, and econo-
 metric policy evaluation 1," Econometrica, 73(3), 669­738.

Heckman, J. J., and E. J. Vytlacil (2007a): "Econometric evaluation of social programs, part I:
 Causal models, structural models and econometric policy evaluation," Handbook of econometrics,
 6, 4779­4874.

         (2007b): "Econometric evaluation of social programs, part II: Using the marginal treat-
  ment effect to organize alternative econometric estimators to evaluate social programs, and to
  forecast their effects in new environments," Handbook of econometrics, 6, 4875­5143.

Henderson, D., Q. Li, C. Parmeter, and S. Yao (2015): "Gradient-based Smoothing Parame-
 ter Selection for Nonparametric Regression Estimation," Journal of Econometrics, 184, 233­241.

Horowitz, J. (1992): "A Smoothed Maximum Score Estimator for the Binary Response Model,"
 Econometrica, 60(3).

Hu, Y., and S. M. Schennach (2008): "Instrumental variable treatment of nonclassical mea-
 surement error models," Econometrica, 76(1), 195­216.

Hu, Y., and S. M. Schennach (2013): "Nonparametric identification and semiparametric estima-
 tion of classical measurement error models without side information," Journal of the American
 Statistical Association, 108(501), 177­186.

Hyva¨ rinen, A., and E. Oja (2000): Independent component analysis: algorithms and applica-
 tions, vol. 13. Elsevier.

Jochmans, K. (2013): "Pairwise-comparison estimation with nonparametric controls," Economet-
  rics Journal, 16, 340­372.

Khan, S. (2001): "Two Stage Rank Estimation of Quantile Index Models," Journal of Economet-
 rics, 100, 319­355.


                                               46
Khan, S., and D. Nekipelov (2018): "Information structure and statistical information in dis-
 crete response models," Quantitative Economics, 9(2), 995­1017.

Khan, S., and E. Tamer (2018): "Discussion of "Simple Estimators for Invertible Index Models"
 by Ahn et al.," Journal of Business & Economic Statistics, 36, 11­15.

Klein, R., C. Shan, and F. Vella (2015): "Estimation of marginal effects in semiparametric
 selection models with binary outcomes," Journal of Econometrics, 185(1), 82­94.

Klein, R., and R. Spady (1993): "An Efficient Semiparametric Estimator for Binary Response
 Model," Econometrica, 61(2), 387­421.

Lewbel, A. (2000): "Semiparametric Qualitative Response Model Estimation with Unknown
  Heteroscedasticity or Instrumental Variables," Journal of Econometrics, 97(1), 145­177.

Lewbel, A., S. M. Schennach, and L. Zhang (2020): "Identification of a Triangular Two
  Equation System Without Instruments," Working Paper.

Moneta, A., D. E. P. O. Hoyer, and A. Coad (2013): "Causal inference by independent
 component analysis: Theory and applications," Oxford Bulletin of Economics and Statistics,
 75(5), 705­730.

       ´, I. (2015): "Sharp Bounds on Treatment Effects in a Binary Triangular System," Journal
Mourifie
 of Econometrics, 187(1), 74­81.

Newey, W., and D. McFadden (1994): "Large Sample Estimation and Hypothesis Testing," in
 Handbook of Econometrics, Vol. 4, ed. by R. Engle, and D. McFadden. North Holland.

Powell, J., J. Stock, and T. Stoker (1989): "Semiparametric Estimation of Index Coeffi-
 cients," Econometrica, pp. 1403­1430.

Reiersol, O. (1950): "Identifiability of a Linear Relation Between Variables Which are Subject
 to Error," Econometrica, 18(4), 375­389.

Shaikh, A. M., and E. Vytlacil (2011): "Partial Identification in Triangular Systems of Equa-
  tions with Binary Dependent Variables," Econometrica, 79(3), 949­955.

Sherman, R. (1993): "The Limiting Distribution of the Maximum Rank Correlation Estimator,"
  Econometrica, 61, 123­137.

         (1994a): "Maximal Inequalities for Degenerate U-Processes with Applications to Opti-
  mization Estimators," Annals of Statistics, 22, 439­459.

        (1994b): "U-Processes in the Analysis of a Generalized Semiparametric Regression Esti-
  mator," Econometric Theory, 10, 372­395.



                                              47
Tamer, E. (2003): "Incomplete Bivariate Discrete Response Model with Multiple Equilibria,"
 Review of Economic Studies, 70(1), 147­167.

Vuong, Q., and H. Xu (2017): "Counterfactual mapping and individual treatment effects in
 nonseparable models with binary endogeneity," Quantitative Economics, 8(2), 589­610.

Vytlacil, E. J., and N. Yildiz (2007): "Dummy Endogenous Variables in Weakly Separable
 Models," Econometrica, 75(3), 757­779.




                                           48
