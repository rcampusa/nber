                      NBER WORK[NG PAPER SERIES




                     THE DECISION-STATE METHOD:
             CONVERGENCE PROOF, SPECIAL APPLICATIONS,
                  AND COMPUTATIONAL D<PERIENCE


                      V.K. Dharmdhikari*

                       Working Paper No. 9L



 COMPUTER RESEARCH CENTER FOR ECONOMICS AND MANAGEMENT SCIENCE
           National Bureau of Economic Research, Inc.
                     575 Technology Square
                 Cambridge, Massachusetts 02139


                           July 1975

                Preliminary: not for quotation

NBER working papers are distributed infonma.iiy and in limited
numbers for camrxents only. They should not be quoted without
written permission.
This report has not undergone the review accorded official NBER
publications; in particular, it has not yet been submitted for
approval by the Board of Directors.

*NBER Computer Research Center. Research supported in part by
 National Science Foundation Grant DCR 70U 356_A0L. to the National
 Bureau of Economic Research, Inc.
                             Abstract

This paper presents a new method for obtaining exact optimal solutions
for a class of discrete-variable non-linear resource-allocation problems.
The new method is called the decision-state method because, unlike the
conventional   dynamic programming method which works only in the state
space, the   new method works in the state space and   the decision space.
It generates and retains only a fraction of the points in     the state space
at which the state functions are discontinuous; and thus overcomes to
some extent the curse of dimensionality. It carries the cumilative
decision-strongs associated with these points, and thus avoids the back-
tracking entailed by the conventional dynamic programming method for
recovering the optimal decisions.
     A concise and complete statement of the method is given in Algorithm
2 and it is proved that the algorithm finds all exact optimal solutions.
In addition the method is adapted for solving sar problems with special
structures such as block-angular or split-block--angular constraints arid
the resultant substantial advantages are dencnstrated. The performance
of Algorithm 2 on many resource-allocations problems is reported, along
with investigations on many tactical decisions which have substantial
impact on the perfonnance. The performance of the computer implementa-
tion of Algorithm 2 is compared with that of the P algorithm and it
showed that for the class of problems at which the two are aimed, the
decision-state Algorithm 2 performed better than MIF algorithm both in
terms of storage requirement and solution tine. In fact, it achieved
an order of magnitude saving in storage requirement.
                              Contents

 1.    Representation of the Problem .
 2.    Literature Survey                                        2
 3.    The Decision-State Method                                14


 14.   Proof of Convergence for Algorithm 2                   16

 5.    Computational and Storage Efficiency                   214

 6.    Comparison of Algorithm 2 with the MMDP Algorithm . . 30
 7.    Non-integrality                                        30
 8.    Block-angular Constraint Set                           31
 9.    Mixed-Integer Problems                                  314


10.    Bounding Elimination                                    36

11.    Conclusions                                             38

 References                                                   '40



                              Tables

Table 1. Input Data for Non-linear Problems 1 to 9 .   . .   . 26
Table 2. General Non-linear Problems                          27
A large number of planning situations involve optimization of a sum of non-

linear return functions of discrete—variables, such that the sums of non-

linear, resource—consumption functions do not exceed the given resource—

availabilities. Consider, for example, the following situations:



     1.     Design of multiple—reservoir, water—supply systems.

     2.     Deployment of self—contained manpower units for servicing a

           missile system subjected to constraints on personnel of dif-

            ferent kinds, on equipment of different kinds, and constraints

            of logistics.

    3.      Selection of investment projects where, instead of mere acceptance!

            rejection, it is required to decide among various specified levels

            of projects in a given set.



1. pesentation of the Problem.



    Let 1+ denote the set of non—negative integers, and let R+ denote the

    non—negative real line. Let functions f.(.), g() be defined as
    f.    : 1 ÷ R, g.. : 1 -÷     for   all and j.   Let the M-cornponent vector
    B(bl,b2___bM) represent the given resource availabilities, and M and N,
    the given positive integers. Then the problem can be represented as,
                                   —2—

     I.     Maximize
                         1Jf(x)
                             J
            Subject to    g..(X.) <   b.,   1 <   i<N
                              X.            1<j <N

      In this paper, we deal with the abov problem where f(.)
                                                                 g• (.)
      are assumed to be non—decreasing functions such that some real num-
      ber a 0 3 g.. (a) > b for all j. Very often f (.) and      g  (.) are
      polynomials or linear functions, and M is much sn.ller than N. Problem I
      is thus a special type of an integer proaniming problem.


2.   Literature Survey


     Two main categories of techniques that may be considered for solving

     the above problem are: 1) dynamic programming and 2) implicit enumera—

     tion. In this paper, we present a method that was derived from dynam-

     ic programming, but which also shares some characteristics of the

     implicit enumeration or directed tree search category. The standard

     dynamic programming procedure suffers from what Bellman [2] called,

     "The Curse of Diinensionality", arising from multiple constraints. For

     continuous variable problems, Larson's [31 State Increment Dynamic

     Programming method was able to handle at most 4 to 5 constraints by

     carrying out computations in what are called "blocks". In this method,

     as in other recent approaches such as those of Wong & Luenberger [i.   I,
     Wong [5 ], and Yormark & Baker [6 1, the exhorbitant memory require-

     ment is reduced at the cost of increased computation.




                                                                                 .
                                 —3—




        For the multiple-constraint, knapsack problems with zero-one vari-

ables, Weingartner and Ness [7], and Nemhauser and Ullman [   8]   developed

specialized dynamic programming algorithms that perform much better than

the conventional dynamic programming algorithm. In these algorithms, the

dimensionality problem of conventional dynamic programming is mitigated by

noting that the optimal return functions are monotone step functions and

thus it is sufficient to record the optimal returns and decisions only at

the points at which the step functions change value. Weingartner and Ness have

also reported on various heuristics that can be used with their algorithm to

achieve computational savings. They also consider elimination of solutions

through bounding in addition toel.imination through dominance. In a way,

the theoretical work of Haymond [9] in the one-dimensional case, and some

portions of the extensive paper on knapsack functions by Gilmore and Gomory

[10] can be said to contain the germs of the ideas that are developed in

[7], [8] cited above.

        Although the algorithms in [7], [8] performed much better than

conventional dynamic programming, they are quite inferior to other specialized

algorithms such as Geoffrion's RIP-30 [ill designed for solving zero-one

problems. Thus the main value of the ideas contained in these works is in

how they can be applied to or extended for solving problems which are in-

tractable to other known techniques.

        Motivated by the above papers, Morin and Marsten [12] recently de-

veloped their Imbedded State Space Approach which can be considered as an

extension of the ideas contained in those papers. It is aimed at solving
                                      -4-



     general, non-linear, multi—dimensional knapsack problems Later, an

     improved and somewhat more complete version of their algorithm was given

     in [13]. A different paper by Morin and Esogbue [14] gives a theoretical

     exposition of the method and its extension to a broader class of sequential

     decision problems with additive and multiplicative returns. Some compu—

     .tational results are given in [13], [14]. The Imbedded State Space Approach

     developed by Morin et al., it is seen, is conceptually similar to the

     decision-state approach developed in this dissertation. The implementations

     of the two basically similar approaches in the form of specific algorithms

     are, however, quite dissimilar in many important aspects;


3.   The Decision-State Method


     We now proceed to qive an alqorithmic statement.of the decision—state method.

     By an algorithm we mean, in accordance with Knuth1s [15] definition, a proce-

     dure consisting of a sequence of instructions which are unambiguous and         I
     executable such that theprocedure terminates in finite time.

             In this section we define the symbols and operations used in the

     statement of the algorithm and its proof. The algorithm is stated in an

     easy to read FORTRAN-like language whose instructions are numbered state-

     ments in upper case letters. The instructions are preceded by explanatory

     comments distinguished by asterisks.




                                                                                     .
                                            —5—


3.1         Definitions

            Let   the k-tuple 0 =(x1, x2, ..., Xk) for 1 <              k < N,   where all xeI÷,
denote     a particular set of values of the first k variables. For each decision
vector     D, the M-component vector-valued function ks(D) is defined as kS(D) =


(s1,      S2,..., SM)
                        = S where each component is given by s1 =
                                                                                     (xj) for
1<i<M         and the scalar-valued function kv(D) is defined as kv(D) =


E! f(x) =
          v.            For a given 0, the triplet (D,S,v) is said to form a list

entry or 'entry, where D is the decision vector, S is the state vector, and

v    is   the objective value.

             If there exist two      list   entries E' =       (D',   S', v') and E° =     (0",   S",   v")
such that v' <       v°   and S'   >S'   then (..D, S',      v')    is said to be dominated by

 (D", S"I     v") and     this is expressed       as E" > E'    or E' <   E".    It is established

 in Lemma 4 that a dominated entry (D', S', v') cannot be a part of an optimal

solution      because it    can    be replaced by      (0", S1, v")     to increase the objective

val ue.      Hence, while searching for optimal solutions, only the undominated

entries need be examined.

             For   an integer k > 0,     given D =
                                                        (x1,   2'         Xk) and Xk+lEI÷ the
notation (0, x÷.) denotes the (k + 1)-tuple (x1, x2,                             Xkl Xk+l). More
generally, for n > 0, the notation (D, Xk÷1,.i..Xk+n)                      denotes   the (k +
 tuple (x1, x2, •..xk+n). In the special case of k = o when                          D is vacuous the
notation (D, xk+l) denotes the one-tuple (x1). Given an                         integer Xk÷lCI+ and

 a   list L with k—component decision vectors, the notation (L, Xk+l)                      denotes
a list which is identical with the list L except that in each entry (D, 5, v)

•D is replaced by (D,xk÷l) and S and                v are   adjusted accordingly. Given a list L

and an entry (D,S, v), the notation L.plus.(D, S, v) denotes the list con-

taining all entries from L and the entry (D, 5, v). Similarly, for a list

L and an entry CD, S, v) which may or may not be in it, the notation L.minus.

 (D, S, v) denotes the list of all                entries from L except the entry (0, S, v).
Given ak-stage feasible entry (0, S1 v), for k < N,                       we   define   its various

order descendents as follows:
                                              -6-


               O-th order descendents

                           S, v)]     {(D, 5, v)}

               n-th order descendents for n             1, 2, ...,   N-k,



                                                           (Dxk+1,...,xk+fl),   Xk+jCI÷ 1 < j <
                           c
 DescE(D, 5, v)]                      S', v')
                                                    1      k+ns(DI) BV -        k+nv(DI)

                                                                                                  J
In    addition, the set of n-th order descendent entries of the entries in

the k-stage list L is defined as


             tDesc [L] =              Desc [E1].
                             E1c L




 3.2         Decision-State Algorithm 2
                                                                                                      .
              In this section we develop and present Algorithm 2. It is divided

     in subsections, a subsection beginning with some motivation underlying the

     development. The steps of the procedure are preceded by asterisked cOrn—

     ment statements.                                                  -




         * We    begin with k =      0,   and the list L containing one entry (D,S,v)

         * where D is vacuous, and S and v are zero. L occupies one top
         *    location   of the storage area, the rest of the locations are identi-
         *     fied   as being empty.




                                                                                                      .
                                            —7—




Step 0: k          0, L =   (-,O,0),   KOUNTL =   1,   KOUNTC = 1


3.2.1       Systematic Entry Generation

            In Algorithm 1 we construct many entries (D,S,v) with S >B

which, upon testing for feasibility of state are then discarded. This

is so because each value of Xk+l is combined with each entry in the k-th

stage list of undominated entries, in order to insure that all feasible

entries are constructed. By noting the non-decreasing property of the

gjj(•) functions, a new procedure is developed which tends to generate

fewer infeasible entries. To achieve this, we first assume that the en-

tries In L are in the non—increasing order of objective value. Then we

combine each entry E11cL in a non-empty location [Ni] starting with

Ni = KOUNTC, with successively increasing values of Xk+lcI+. As soon as

an infeasible entry is generated, the cycle is started all over for the

next entry EnhiCL with Ni =       Ni-i;   and so on. We describe below how the

(k+1)- stage entry list C is generated from the k— stage list L. It is

assumed that enough storage locations are available between the top and

bottom of the storage area to carry out the procedure. Later in Chapter 4

we will deal with the question of precisely how many storage locations

are needed in our specific computer implementation. For Ni an integer,

[Ni] denotes the Ni-th location of the storage area and if [Ni] is not

empty then EN1 denotes the entry (DN1,S1U,v) in location [Ni].



        *    Steps 1 to 6 constitute the Systematic Entry Generation phase.
        *    We   begin with list L occupying the upper KOUNTC locations of the
        *
             storage   area. It contains KOIJNTL non-empty locations, containing
                                              —8--




     * the      k- stage entries interspersed with KOUNTC-KOUNTL empty

     *    locations.    The rest of the locations below [KOUNTC] are empty.

     *    The entries are      in    monotonic order of the objective value so

     *    that   the top entry has the largest objective value. At the
     *    end of the phase we will have generated list C of the (k-fl)—
     *
          stage entries. C will be occupying the lower contiguous loca-
     *    tions    of the storage area.

     *    We    start with location KOUNTC at the bottom of the list L, with

     *    the list C empty, the counter KOUNTC reset to zero and defining
     *         to equal KOUNTL.

Step 1: Ni = KOUNTC, C =            ,   KOUNTC =     0,   2   = KOUNIL

     *    When the location [Ni] is empty we go to the next location.

Step 2: IF [Ni] IS EMPTY THEN GO TO 6

     *    When     Ni contains an entry, we refer                   to   its three parts as

     *    EN1 =    (DNiSNivNi)            We start the cycle by setting the new

     *
          stage-variable    to zero.

Step 3: Xk+i = 0
     *    Construct    a (k+1)- stage triplet by combining Xk+l with EN1.

Step 4: D =                    s = k+1 s(D), v =              k+i
                                                                    v(D)
                 (DN1,xk÷i),
     * The new triplet (D,S,v) is added at the lowermost empty location

      *   in    the storage area if and only if S is feasible. It is placed

      *   above the previous entry, if               there was one. The count KOUNTC

      *   of the entries in         C is   now incremented and so             is   the value of
      *xk+i
Step 5: IF S <B THEN PLACE (D,S,v) IN LOWERMOST EMPTY LOCATION,

          KOUNTC = KOUNTC +         i,   Xk+i =    Xk+i
                                                          + 1, GO TO 4
                                            —9—



        *   When S Is infeasible we go to the next location from list L.

 Step6: Nl=Ni-l,
            IF Ni > 0 THEN GO TO 2

        *   When Ni equals zero, the Systematic Entry Generation phase is

        *
            complete. The entries in the lower contiguous locations con—
        *   stitute    list C. All the feasible entries in C which are de—
        *   scendents of an entry in L are said to constitute a sublist.
        *            entries
            The £              from L will give rise to L         sublists.   Let the
        *   sublists    be numbered in the order in which these were generated.
        *
            SL1 will be the sublist occupying the lowermost locations and
        *   will    consist of the feasible descendents of the lowermost entry
        *   from L with the smallest objective value. The top sublist will
        * be SLL. We define C(k+1) =              C   for the current value of k and
        *   the    current list C.


3.2.2       Merging

            In this phase we merge the sublists comprising C into one merged

 list A. The task of this phase is similar to that in a situation where

 a given set of numbers is to be arranged in descending order. There are

 a number of ways ( See Knuth [ 23 ]) this can be done, some being more

 suited for some conditions than others. We develop here a new merging

 procedure that exploits the particular situation at hand. Let the number

 of entries in the sublists SL1,SL2,...ISL& be denoted, respectively, by

 I'(i), P(2),...,P(2). Let P = MAX {P(i), 1 <i <z}. In Lema 1 we prove

 that only P additional locations are sufficient for merging these sub-
                                       —10—




lists   by the procedure given below. We assume that P empty locations

are   available    above the uppermost sublist SL. Again, how this affects

the precise size required of the storage area in our computer implemen-

tation is dealt with in Chapter 4.

        *
            Steps 7 to 13 constitute the Merging phase. From the non-de—
        *
            creasing property of k•' we know that the entries within each
        *   sublist   are in monotonic order of the objective value, with the

        *   smallest   entry at the bottom. Thus, in order to obtain one or-

        *   dered list we need to merely merge these sublists. This we do
        *
            by first copying SL in the upper locations of the storage area
        *   and calling it list A. Then we merge A with SL,1 if it exists,
        *   and again call the result, list A. Then we merge A with SLL_2,
        *   and so on. We begin by copying sublist SL.

Step 7: COPY SUBLIST SL IN LOCATIONS 11] TO [PCi)] PRESERVING THE               S
            ORDER OF THE ENTRIES, KOUNTA =
                                              SL
        *   When there is only one sublist we iniiediately go to the next

        *
            stage.
Step 8: IF         = 1 THEN GO TO 31

        *   Initialize   the counters for the first merge.

Step 9: Mi = —1, Ni = P(2.), N2 = P(M1), N3 =       0,   N4 = 0

        *   The list occupying the upper contiguous locations will be called

        *   A.    At this point it consists of only the entries from SL. At

        *   the end of the first merge     it will be the resultant ordered
        *   list   of the merge of SL and SL , and so on.
        *   In   this step we identify the current lowest entries from the


                                                                                .
                                        -11-


     * current      list A called the initial list A and the portion re—
     *
          maining currently of sublist SLM1 and also identify their corn-
     *
          ponents with specific labels.
Step 10: EA = (DA,SA,vA) IS THE LOWEST REMAINING ENTRY IN THE INITIAL

          LIST A, EM1 = (0M1 sMl vM1) IS THE LOWEST REMAINING ENTRY IN


          SLM1
     * Now we compare A with M1 and remove the entry with the smaller

     *    value and transfer it to a new location in the merged list A.
     *    Increment the counters and repeat until the sublist is exhausted.

Step 11: IF A< M1 THEN REMOVE EA FROM THE INITIAL LIST A,

          TRANSFER EA TO [N1+N2-N3] IN THE MERGED LIST A, N3 = N3 + 1,

          IF bo>M1 THEN REMOVE EM1 FROM SLM1,

          TRANSFER EM1 TO [Ni+N2—N3] IN THE MERGED LIST A, N3 = N3 + 1,

          N4=N4+i
      *   As long as the counter N4 of the number of entries removed from
      *          is smaller than P(M1), we repeat with the new lowest entries,
          SLM1
      *   EA in the initial list A and EM1 in the remaining sublist SLM1.

Step 12: IF N4< P(Mi) THEN GO TO 10

      *   When N4 = P(M1) the merged list A contains the result of the

      *
          merge of initial list A and SLM1. If any more sublists remain
      *   to   be merged, we reinitialize the counters and start aqain with
      *   the    next sublist and current A as the initial list A.

Step 13: Mi =    Mi-i,   Ni =   Ni +P(Mi÷i), N2 = P(Mi),   N3 =   0,   N4 =   0,
          IF Ml> 0 THEN GO TO 10

      *   When Ml = 0 all the sublists will have been merged into list A
                                         -12—




        * which       will now contain all the KOUNTC entries arranged in

        *   monotonic order with the smallest objective value at the bot-
        *   torn in [KOUNTC]. We set counter KOUNTA to current value of
        *   KOUNTC.      In order to avoid the substantial computations in

        * the Identification and Elimination phases, when no greater

        * than 10 new entries are generated in C as compared to those

        *   in   L, we bypass these phases.

Step 14: KOUNTA = KOUNTC, IF (KOUNTC—KOUNTL)< 10 THEN GO TO 31



3.2.3       Identification

            In this phase we identify certain entries as distinguished

entries, which are potentially likely to be found dominated, i.e., the

non-distinguished entries cannot be dominated. These propositions are
                                                                               •
proved later in Section 3.3. As the distinguished entries are identi-
fled, certain M-component vectors called T vectors are stored as mar-

ker vectors in association with some entries. These are used in the

Elimination phase. We assume that enough storage locations are avail-

able to store the marker vectors.



        *
            Steps 15 to 21 constitute the Identification phase. For
        *   Ni   =   1,2,...,   KOUNTC we compute recursively an M-component

        *   I    vector for each entry ENEA. The i-th componentT of the

        *   I vector is defined as the smallest of the i-th components of
        *   the state vectors sMl of the entries in A where M1< Ni. If
        *   the      N1-th I vector does not exceed       in any component,

        *   then ENl is         identified as a dfstinguished entry. For a
                                                -13-



        * prespecified positive            integer t, every t-th vector is stored
        *   as a marker vector and the entry is marked to show this.
        *   We   begin by setting the initial T vector equal to the right
        *   hand side vector B.

Step 15: T = B, Ni =         0,   Ml = 0

        *   Increment the counters. If all entries have been examined then
        *
            go to the Elimination procedure, otherwise proceed to the next
        *
            step.
Step 16: Ni =       Nil-i,   Ml = Ml +
                                           1,
            IF Ni> KOUNTC THEN GO TO 22
        *   Test                                       is smaller than the corresponding
                    if any component of
        *
            component of the current I vector. When it is, then EN1 cannot
        *   be dominated by entries above it.

Step 17: IF 3t,        1<i<M       SUCH THAT S!<T THEN GO TO 19
        * When S>T then it is possible for EN1 to be dominated.

Step 18: IDENTIFY EN1 AS A DISTINGUISHED ENTRY, GO TO 16
        *
            Update the T vector.
Step 19: T =                         WHERE I = MIN
                   (Tl,T2,...,TM)                           (s!', T)
        * Store every t-th vector as a marker vector.

Step 20: IF Ml =t THEN STORE T AS A MARKER VECTOR IN ASSOCIATION WITH

            ENTRY EN1, Mi = 0

Step 21: GO TO 16


3.2.4       Elimination

            In this phase we detect which of the distinguished entries are
                                          _1'-I_




actually dominated, and then eliminate these by identifying- the entry

location as being empty. The marker vectors stored in association with

some of the         entries in the Identification   phase are used to reduce the

number    of      entry comparisons whenever possible, making use of Lemma 3 in

Section      4.


      * Steps          22 to 30 constitute the Elimination phase. When there

      *       are no distinguished entries we bypass the Elimination phase.
Step 22: IF THERE ARE NO DISTINGUISHED ENTRIES IN A THEN GO TO 31

      *        If there exist any distinguished entries, we start with the
      *       lowest    one, with the smallest objective value.

Step 23: Ni = THE LOCATION OF THE LOWEST DISTINGUISHED ENTRY, Ml =               Ni-i
              We go the next Ml if location [Mi] is empty or if Ml =        0.
Step 24: IF (Ml = 0 OR [Ml] IS EMPTY) THEN GO TO 29

         *    When there is a marker vector in association with EM1, we

         *
               identify   it with a label.

Step 25: IF THERE IS NO MARKER VECTOR IN ASSOCIATION WITH EM1 THEN

               G0T027,
               OTHERWISE LET TM BE THE MARKER VECTOR

         *     If any component of the state of EN1      is smaller than the re-

         *
               spective component of     TM, then EN1   cannot be dominated by   any

         *     further    entries above EM1;   therefore, we proceed with   the next

         *
               distinguished    entry.

Step 26: IF3i, 1<i<M SUCH THAT S< TM THEN GO TO 30
         *     In   order to ensure that all alternative optimal solutions are
                                     —15—
                          -



     * obtained,      entries with equal objective value are not compared

     *    for   dominance.

Step 27: IF v11 =    Mi   THEN GO TO 29

     *    When 5N1 >sM] then EN1 is dominated by EM1 and we eliminate

     *    EN1 by identifying [Ni] as being empty and decrease the counter

     *    KOUNTA by 1.

Step 28: IF 31, i<i<M, SUCH THAT S!'< S11 THEN GO TO 29,

          OTHERWISE ELIMINATE EN1 AND IDENTIFY [Ni] AS BEING EMPTY,

          KOUNTA = KOUNTA-i

      *   We prepare to examine the next upper location.

Step 29: Ml =    Mi-i,
          IF Mi >0 THEN GO TO 24

      *   At    this point we are finished with the distinguished entry in

      *   (Ni].    The elimination phase is complete ifthere.is no distinguished

      *
          entry    above (Ni], Otherwise we reinitialize Ni, Mi for the next

      *
          cycle.
Step 30: IF EN1 WAS THE UPPERMOST DISTINGUISHED ENTRY ThEN GO TO 3i

          OTHERWISE Ni = THE LOCATION OF THE NEXT DISTINGUISHED ENTRY

          ABOVE CURRENT [Ni], Ml =        Ni-i,   GO TO 24

      *   At    this point A occupies the upper KOUNTC locations and contains

      *   KOUNTA non-empty locations, each •in an undominated (k+i)- stage

      *   entry.     Now we rename this list as L, reinitialize KOUNIL with

      *   the    current value of KOUNTA, increment the stage number and

      *
          identify    all locations below [KOUNTC] as being empty, and go back

                                                                      is less
      *   to    Systematic Entry Generation for the next stage if k
                                                                                           .
                               -         —16—




       * than         N, the number of stage-variables in the given problem.
       *   Define     L (k+1) = A for the current value of k and the current

       *   list A.
 Step 31: L =    A,   KOUNTL = KOUNTA, k =      k+i,   IF k< N THEN GO TO 1
       *   When k equals N, the final list L of undominated entries con-.

       *   tains   all the optimal entries. Since the entries are             in   mono—
      *    tonic   order of the objective value, with the top entry having
      *    the   largest value, the optimal entries can be easily picked
      *
           up from the top.
      *    We    start with the top entry E1= (D1,S,v)

Step 32: Ni = 1, OPTLIST = {E'11}, OPTVAL =            N1,   N2 = 1

      *    When the next entry below has equal objective value, we add

      *    it to OPTLIST. Otherwise we terminate the algorithm.
Step 33: N2 = N2 + 1

           IF N2> KOUNTL THEN GO TO 35

Step 34: IF t2 =       N1     THEN OPTLIST = OPTLIST.PLUS.EN2, GO TO 33
      *    When I2 Ni then OPTLIST will contain all the entries yield-

      *
           ing   the optimal value OPTVAL. The decision vectors D of the
      *    entries    (D,S,V)EOPTLIST are the optimal solutions to the given
      *
           problem    1.

Step 35: END


4.         Proof of Convergence for Algorithm 2

           In this section we will establish that the 35—step procedure

presented in Section 3.1 achieves what it sets out to do. First we will
                                      —17—




prove    in Theorem 1 that the procedure can properly be called an algor-.
ithm    in the sense that each step of the procedure is unam&*guous and

executable, and that the procedure terminates finitely. Then we will

prove    four   lermias that will help in proving Theorem 2 which states

that   Algorithm 2 finds all optimal solutions to the given Problem 1.


Theorem 1:        The procedure of section 3 is an algorithm.
Proof:    The    procedure involves operations such as comparisons and ad-
ditions   of values, additions or deletions of list entries, storing

entries in storage locations, identifying or marking storage locations,

and so on, which are clearly executable. Thus we need only to establish

finiteness. The Systematic Entry Generation phase starts with stage-

number k = 0 and the list L containing only one entry. For each entry

in the list L, we start with Xk+l =      0 and at each pass through the loop
formed   by Step 4 and Step 5 we    increment          by 1.   Since the functions
                                                Xk+l
         are such that there exists a non-negative real number a for which

(a)> b          for.all I and j, the termination from this loop occurs in a

finite number of repetitions. Since we start with a finite number of

entries in the list L, termination from the Systematic Entry Generation

loop formed by Step 2 and Step 6 occurs finitely, and the list C con-

tains a finite number of entries.

          The number of subliststo be merged is finite, hence termination

from the merging procedure occurs finitely. In the Identification pro-

cedure each entry is compared once and only once with the recursively
                                     -18-




computed I vector, hence termination from this procedure       occurs   finitely

and the number of entries distinguished remains finite. In the Elimina-

tion procedure, each distinguished entry is compared with only a finite

number   of entries above it,   hence termination   from the Elimination pro-
cedure occurs finitely with the new list L containing only a finite
number of entries.

         In Step 31 the stage-number k is incremented by 1, and the next

cycle through the procedure begins all over. Since we started with

k   0, and since k is incremented at each cycle, the termination through

the largest loop, formed by Step 1 and Step 31, occurs finitely, after

the N-th pass. Exit from the loop formed by Step 33 and Step 34 occurs

finitely because the loop starts with N2 =     1,   increments N2 by 1 at each

pass, and since there are a finite number, KOUNTL entries in the list L.

Thus termination from the entire procedure occurs finitely. VV



         Lena 1 establishes that for the Merging procedure of Section

3.2.2 only P additional storage locations are sufficient to achieve a

complete merge without losing any of the entries. This is done by show-

ing that every entry is transferred to a location that has been made

empty before the transfer. Leniiia 2 helps us in determining some entries

that cannot be dominated by any entries in the list. By the use of

Lemma 2, in the Identification procedure we identify some entries from

the list A as those that can possibly be dominated by some other entries.

Lemma 3 helps us in reducing the number of entry comparisons in the

process of determining if a distinguished entry is actually dominated.

                                                                                   .
                                    —19—




Whenever we find a marker vector, at least one component of-which

exceeds the   corresponding component   of the   state   vector of   the   distin-
guished entry, we can stop the entry comparisons because Lemma 3 proves

that no further comparisons can show that the distinguished entry is

dominated. Lema 4 establishes that any descendent of a dominated

entry cannot be optimal.

Leimia 1: Suppose there are £   sublists   SL1,SL2,...,SLL. Each sublist

is in monotonic order of the objective value with the smallest entry

at the bottom. Suppose P(1), P(2),..., P() are the counts of the

number of entries in the sublists, respectively; and that P is the

largest of these numbers. Then the P additional storage locations

attached above the topmost list SL are sufficient to completely merge

the sublists.


Proof: The procedure starts by copying P(L) entries from SL into the

upper locations of the P additional locations attached and P(L)< P.

This leaves P - P(&) empty locations. The first merge of SL1 with

SLL entries will transfer entries to the uppermost P(&) +            P(L-1) loca-
tions, which number is no larger than the numberP +          P(,)    of locations

available for the resultant list A since P (2.—i) P. Thus:.P(2j+P(z-1)<P+P(2.).

Moreover, no entry from SL2._1 is transferred to a location that was occu-

pied by an entry from SL2., until the latter was removed from it. The same

holds for the next merge because P(2.) + P(&-1) +        P(&-2)<P + P(2.)     +   P(2.-i).
This continues to hold for all merges because P(L)+P(2.-1)+...+P(L—n)<P

+P(&)+...+P(&-n+i) for 1<n<L-1. vv
                                   —20—




Lena   2 In the Identification procedure, if there is an entry EN1 =
                                                                               .
(D,S,y)eA such that a component of sN] is smaller than the corres-
ponding component of the (Ni—1)-th I vector, then the entry EN1 cannot

be dominated by any entry in the list A.

Proof: From the procedure, we see that the recursion for the i-th com-

ponent, 1<i<M, of the (N1—l)-th T vector Is given by T. =   MIN(S'T).
Thus I1 = MIN                       Now for M1< Ni, M1> Ni from the

Merging procedure, and there exists an i such that s!"<           for all

Mi< Ni. Therefore EN1 cannot be dominated by EM1 for Mi< Ni. But for

M1>N1, M1<N1, therefore EN1 cannot be dominated by EM1 for M1>N1.v



Lemma 3:   In the Identification procedure, suppose TM is a marker vector

stored in association with the M1—th entry EM1 and that EN1 is a distin-

guished entry where Mi< Ni. Suppose also that a component of S is

smaller than the corresponding component of TM. Then EN1 cannot be dom-

inated by any entries above EM1.

Proof: By its construction, we know that TM1 = MIN                    We

are given that there exists an i ,i< i< M such that s!< TM. Therefore,

s!1< TM1< s!12 for all integers M2< Ml. From the Merging procedure we

know that v11< M2 for M2< Mi. Hence EN1 cannot be dominated by any entry

above EM1. vv



                                                            N—k
Lemma 4: Let E1 c C(N) and E2 c C(k) for k<N. Suppose E1          Desc [E2].

If there exists an entry E3 C(k) such that E3> E2, then     cannot be

optimal.
                                                     —21—




Proof: First, suppose that k = N.                           Then     c C(N), E3       C(N), and

E1 c 0Desc [E2]. Since E3> E2, v3> V2                          = v1 and hence E1 cannot be opti-

nial. Now suppose k< N. Since E1 £                          N_k5     (E2], we have       X =   X   for
j =   1,2,...k.                                                                          1,2,...,M.
                           Since E2< E3, S<S =E1
                                                                   g(X)        for I =

                                                =
Now construct a new entry                            (D4,S4,v4),   D4 =
                                                                           (X14,X24,...,XN4) where
      =
          X3   for j =       1,2,...,k      and X,j4 =             for   j =   k+1k+2,....,N.      For

this entry, for i =             1,2,...,M           we have

                       =
               s14              (X4)
                       =
                           z: g1(X4) + E:1 g1(X4)
                       —
                       —     s3
                              I         +3=N
                                             j=k+1 g1


Therefore, S4                     g1(X1) +                      g(X1) = S1<B             and entry E4

is feasible. Now

               4
               V
                   —
                   —    ,.j=N     ,.,   4


                   =
                                f(X4) + E:1 f(X4)
                   =
                                f(X) + z:÷1 f(X')

Since E2< E3, z f(X3) = v3>                             v2 =
                                                               E    f(X2) =       E       f(X')

Therefore, v4>
                                fx'         +               f(X,1) = v1.

Thus (D4,S4,v4) is a feasible N-stage entry with greater objective value

than that of E1. Therefore, E1 cannot be optimal. vv
                                           —22—




Theorem 2: Algorithm 2 finds all optimal solutions to the given
Problem 1.
Proof: In Theorem 1 we proved that the procedure in Section 3.3 termed
Algorithm 2 can properly be called an algorithm. 'In Lemma 1 we proved
that the P additional storage locations attached above the top sublist
SL&, where P is the largest of the sublist counts P(1),P(2),...,P(2),

are sufficient for the Merging procedure in that all the entries in the

i.   sublists     are preserved and arranged in one contiguous list A in mono-

tonic order at the end of the merge. Lemma 2 and Lemma 3 prove that the

Elimination procedure removes all the dominated entries from the list A

and only the dominated entries from the list A. Lemma 4 proves that no

descendent of a dominated entry can be optimal, hence no such descend-

ents need be generated nor kept in the search for the optimal solutions.

Using all the above, we provebelow that the final list L(N) produced

by the procedure of Section 3.2.4 contains all the optimal list entries

having     the optimal objective value. Let F and F0            denote, respectively,

the sets of feasible and optimal entries for the N-stage Problem 1.

Symbolically,

                                       (X1,X2,... ,XN), X j, i<j<N,
                                 D =
            F =    1 (D,S,v)
                                 s   = NS(D), v = "v(D), S<B


                   (             (D,S,v)    F, v>v
            F =        (D,S,v)
                                 forany(D,S,V)cF

Clearly F0CF. From the definition of the descendents we know that for
                                                  k                    k—i
                                                                        2
 k1
      =
          1,2,...,N—1,    k2 =   1,2,...,N—k1,        Desc [C(k1)} =         Desc [1Desc]
                                     —23—




            k                  k                k
             2Desc (C(k,)] =    2Desc [L(k1)] U 2Desc (C(k1).rninus.L(k1)]
(C(k1)]],
        k                 k
and • =   Desc [L(k1)]fl Desc C(k1).minus.L(k1) .          At the end of the

first   pass through the Systematic Entry Generation phase, C(1) contains
all feasible entries for the 1—stage problem, i.e., C(1) = UD,S,v) /
D = (X1)        1÷, S =        B, v = 1v(D)}. Therefore F = N_lDesc [C(1)].
(Recall that L(1)= C(1)). At the end of the second pass through the
Systematic Entry Generation phase, C(2) = 1Desc [L.(1)]. Therefore, F =
N_lDesc    (L(1)] = N_2sc[lDesc [L(1)]]= N_2Desc (C(2)]. Now C(2) =
L(2) U (C(2).minus,L(2)). Thus, F = N_2Desc         (L(2)] u N_2oesc [C(2).minus..

L(2)]. From Lemma 4 we have, since F0CF, F0C12Desc [L(2)].

           Proceeding in a similar manner, at the next pass we will obtain

F0C3Desc [1Desc [L(2)]] = N_3Desc [C(3)] =          N_35    [L(3)] U N_3Desc

(C(3).minus.L(3)]. This process can be continued till the (N-1)th pass

to obtain F0CN_(1)Desc L(N-1)]. From this we get F0C1Desc [L(N—1)] =

C(N).     Thus all optimal entries are members of the final list C(N) and

hence also of the final list L(N) of undominated entries. Since the

optimal entries have the maximum objective value, they will be at the

top of the list and thus OPTLIST will contain all of them. vv
5.   Conutationa1 and    Storage   Efficiency

                                                                                   .
     As with any solution technique, the ultimate test of an algorithm is

     in its efficiency. It is not difficult to see how our algorithm is a

     substantial improvement over a standard dynamic programming algorithm

     (SDPA). SDPA requires as many locations for storing the state function

     table for each stage, as the total number of state values. For an H—
                                                               i=M
     dimensional resource allocation problem, this number is 'rr1_i(b),      and

     since it grows exponentially, it becomes too large even for a modern day

     computer when N > 3 for any realistic problem. Our algorithm avoids

     this excessive storage requirement by considering only those points in

     the state space at which the function changes value. Thus, whereas

     the standard dynamic programming algorithm examines all the lattice

     points of the state space, our method examines only a fraction of

     these points imbedded in this complete state space. Noting this, Morin

     and   Marsten [13] have named the method, "The   Imbedded State Space

     Approach".

     More   siiificant and, in practice, more important evaluation of
     our algorithm can be obtained by comparing it to the MMDP algorithm.
     Since both algorithms are aimed at solving the non-linear resource-
     allocation problems, and the perfonnance of I1DP algorithm is
     reported on nine such problems, the identical nine problems were
     solved by using a computer implementation of our algorithm. These
     problems were constructed from Peterson' s problems [16] and the data
     for these    is given in Table 1. At the outset, it was evident that

     the   algorithms are   extremely sensitive to the sequence in which

                                                                                   .
                              —25—




the stage variables of the problem are considered. For example, one
10-constraint, 28-variable problem took 39.9 seconds with one sequence
and 8.5 seconds with another. The empirical performance reported
below was obtained on the problems with the variables arranged in
non-decreasing order of the peak resource consumption ratio defined
below; the variable with the smallest consumption ratio was the first
stage variable. The CPU time taken by the sequencing program is
included in the solution time. The peak resource consumption ratio
for the j-th variable of problem I is given by

                  max {g1(l)/b1}.
The   data for the nine non-linear problems solved by I'fi'U)P algorithm
is given in Table 1.
                                                                                          .
                                                —26—


                         TABLE 1: INPUT DATA FOR NON-LINEAR PROBLEMS 1 TO 9



                                                                                    ci
          1      2        3      4        5     6       7       8       9     10

     60       110        20     40       60    70      10      40      50     50    882
     40        40         5     25       25    25      10      20      20     20    650
     30        60         0     10       15    20       0       0      15     20    320
     22        22         6      6        6     6       8       0       8      8    500
     20        20        60     60       60    60       5       4      55     65    600
     24        44         6      9       11    11       0       0       6      8    220
     90       120        14     24       29    29       6       1      30     30    580
     13        13         4      6        7     7               0      9
                                                        1                      9     90
     18        28        10     20       20   20       10       2      28     28    520
     28        28        12     18       18    18       0       1      10     10    160
     80       100         6     16       20   22        0       2      30     30    403
     50
     40
     80
     80
               70
               40
              100
               90
                          0
                         12
                         20
                         30
                                30
                               20
                               40
                               40    .
                                         50
                                         24
                                         50
                                         40
                                              50
                                              28
                                              55
                                              40
                                                       10
                                                        0
                                                       10

                                                       10
                                                               0


                                                               2
                                                                2



                                                                2
                                                                      25
                                                                      40
                                                                      30
                                                                      20
                                                                              30
                                                                              50
                                                                              35
                                                                              20
                                                                                    450
                                                                                    327
                                                                                    400
                                                                                    400
                                                                                          .
     32       •32         6     16       21   21        3      0       18     20    140
     70       100    .20       30    .   40   40        4       1     29      29    300
     45        75         8     16       19   21        0      0      12      16    205
     12        27         5    10        15   20       10      2      25      25    300
     20        40         3    11        17   17        0      1      18      18    100
21   15        25.        3      5   .7        9        6       1     12      15    120
22   13       .13        4      8         8    8        0      0       4       4   1200
23   12        12        6     10        13   13        0      0       2       2    600
24   64        75    .18       32        42   48        0      0       0       8   2400
25   30        90        10    20        20   20        0      0      25      25    950
26   41        41    •   4     12        20   20        0      0       4       4   2000
27   50        80        40    50        55   55       10   .3        50      55   1100
28   20        55        5                    2i

                                                                                          .
                               13        25            0       0      18      22   480

B    90       120        60    60        60   70       10     45      55      65    RHS
                                        Table 2: General Non-linear Problems
       Prob.    No.               Storage in 60-bit words              Solution time in seconds of Cyber-72 Cpu
           .             MMDP     Algorithm 2      Difference            MMDP       Algorithm 2        Difference
                      Estimated                                        Equivalent
                                                    (1)                                                            (4)
                          (1)            (2)                              (3)
                                                          (2                             (4)
                                                                                                         (3—
           1.            3,600        320            1,025%               3.8             2.6             46%
           2             5,140         385           1,185     %          5.2             3.6                  %
                                                                                                          45
           3                          960              970 %
                        10,280                                           11.3             6.2             82 %
c'.J       4             6,180        320            1,831     %          3.8             2.5             52 %
           5             5,620        400                      %          5.2             3.3
                                                     J,305                                                57 %
           6             8,940        770            1,061     %         10.9             6.3             73 %
           7             3,830        320            1,096 %              4.1             2.6             57 %
                                                                   •
           8             4,130        360            1,047 %              5.2             3.4             53 %
           9             6,220        640              871 %              9,5             5.3             79 %
       .                                                                                                                 .
                                         —28—




                                                                                                   .
Briefly, these problems are of the following form,
                              Maximize
                                                      c f (x,)
                                           j=l           g (xi) <       b,       I =   l,2,...lO

                                                                x   =
                                                                        0, 1, 2,3, 4, 5
 where the functions f and g are chosen as x2, x or 1.                      Problems 1 -   3

 have                   4 -   6 have f (xi) =
        f (xi) =v',j;                            x;   and 7 —   9 have f (xi) = x.
                           g (xi) =V; 2, 5,
 Problems 3, 6, 9 have                                                  =
                                                  8 have
                                                           g (xi)           x;   and 1, 4, 7
             =
 have
        g        x. Problems 1    to 9 correspond to MMDP problems 15 to 23, respectively,

in   [iè].
            The storage requirements and solution times (inclusive of the time
taken by the sequencing program) for these problems are given in Table 7.1.

The storage requirement for the MMDP algorithm depends on the maximum length

UL of the list of undominated entries at any stage, and the total number LL of

feasible list entries generated throughout. Since their paper [12] does not

report these values, we estimate these by solving the problems by Algorithm 2

using the variables in the sequence in whi.ch they appear in the formulations

provided by Morin and Marsten [12]. The solution times thus obtained are

higher than those reported by Morin and Marsten which indicates that a sequence

favorable to one algorithm is not necessarily favorable to the other. Our

interest in this exercise was, however, to estimate the list lengths LL and UL

that were obtained by the MMDP algorithm. When both algorithms use the same

sequence of variables, the list lengths UL and LL yielded by Algorithm 2 provide

good   estimates for    those yielded by the MMDP algorithm. Using, these list

length statistics, we estimate the storage requirement for the MMDP algorithm

as follows. If M is the number of constraints, then each undominated list
                                       —29—

entry takes (2M +   4) computer words, in addition to the 2LL words needed to
store the TRACE entries to enable retracing of the optimal solutions. Thus

the MMDP storage requirement is given by (UL)(2M +   4) + 2LL.   In Algorithm 2

however, the storage requirement depends on L, the maximum size of the list

of feasible entries at any stage, and on fM/Si where IA1 represents the smallest

integer greater than or equal to A. In addition, we need memory space for

storing the state of every tenth entry in the identification phase as mentioned

in chapter 4. Thus the storage requirement for Algorithm 2 is given by

(L)(fM/si + 1) +   (IL/ioi)(rM/s1.
        The computer programs for both algorithms were written in CDC's Extended

FORTRAN and were compiled at optimization level 2, so that some code optimization

was obtained. The solution times of the MMDP algorithm reported in P2] are

based on a DCD-6400 computer., whereas those for Algorithm 2 are based on a

Cyber—72. The Cyber-72 is similar to the CDC-6400 except that the CPU is slightly

slower, being comparable to the CPU of a CDC-6200. According to a CDC manual

      for same (Boolean) instructions the CDC—6200 takes 1.6 times the time

taken. by the CDC-6400, while for some others (Shift, Memory Access) the factor

is 1.5, and finally for some arithmetic operations (Floating Multiply), the

factor is 1.05. As a reasonable average factor we consider that 1 second of

a CDC-6400 CPU is equivalent to 1.3 seconds of a CDC-6200 CPU. The running

times reported in column (3) of Table 7.1 are then the equivalent Cyber-72 times

for the times reported by Morin and Marsten.
                                          —30—


     6.     Comparison of Algprithm 2 with the MMDP Algorithm
            Both Algorithm 2 and the MMDP algorithm are aimed at solving general

     non—linear, resource—allocation problems involving integer-valued variables.

     Nine such problems were solved by Morin and Marsten [12]. The solution times

     reported in [12] are the smallest of the times obtained by using two variable

     sequencing heuristics and the time taken by the sequencing program is not in-

     cluded in reporting the solution times. For Algorithm 2 we solved these same

     problems using only the variable-sequencing heuristic given in Section 7.1,

     and the time taken by the sequencing program is included in the solution times

     reported. For these nine problems, Algorithm 2 performed better both in terms

     of high-speed storage requirement and in terms of solution time. Algorithm 2

     saved an order of magnitude in storage and so appears significantly better in

     this respect. In terms of solution time, the MMDP algorithm took 45 to 82%

     more time than that taken by Algorithm2, but this time difference could be

     attributed to programming techniques, and is thus not as conclusive as the

     storage improvement.




7.   Non—integrality.



     The standard dynamic programming algorithm, as well as most other inte-

     ger programming techniques demand that the resource availabilities b.,
                                                                                1

     and the values taken by the constraint functions g..(.) be integers.

     If, for example, b. equals 203.443, or if one of the   values   taken by

     g..(.) is 19.4321, then the corresponding constraints will have to

     be multiplied with sufficiently large powers of 10 to make these values

     integers, accurate to desired number of significant digits. This would

     substantially increase the size of the right hand side values and thus
                                              —31—



     the storage and computational requirements. In contrast to this, it

     is interesting to note that our algorithm is entirely insensitive to

     the non—integrality of these values. Instead of examining all lattice

     points of the state space as in SDPA, our algorithm examines only

     those points itnbedded in the state space at which the state functions

     change value. Since the algorithm generates these points as it pro-

     ceeds, instead of requiring these to be known a priori, the algorithm

     can   handle the non-integral values without any difficulty.


8.   Block—angular Constraint Set.

     Suppose we have an M by N problem of a diagonal matrix structure
     depicted in Figure 1. This problem has a set of M0 coupling constraints,
     and several blocks of constraints                  each of   size M1, il,2.... Let M                  M0 +
           + ...;    N N1 +       N2 + ...;   arid assume that M0 +          M.    .M* for            1,2
     Now consider applying our algorithm to this M by N problem. Each list
     entry, as usual, will have an M-component state vector. For the first
     block of N1 stage variables x, 1                     j . N, the state components S,
     1+          +    <   i   <   M,   will all be identically equal to zero. Thus, the
     siguificant information in              the state vectors can           be stored by storing
     only    the first (M0 + N1) non—zero state components.


                 Now consider the second block of stage variables x, 1 +
                                                                                                      N1
                                                                                                           <    j       <
                                                                                                                            N.
                                                                                                                                  +

     N2.     For these stages, M1 state components Si, 1 +                              <i   <       + M1, will
     remain unchanged for all subsequent stages j >                      1   +           Thus, for stages
                                                                                  N1.
     1 +
            N.
                 to N1 +
                              N2, we do not need the state components Si, 1 +                              <i       <
                                                                                                                        M0
                                                                                                                              +

     M1.     We need only theM2 state components Si, 1 +
                                                                              M0
                                                                                   +
                                                                                        M1
                                                                                             <   i         +
                                                                                                                M1
                                                                                                                        +
                                                                                                                            M2,
     in addition to the first M0 state components of the coupling constraints,

     since the components Si, 1 +
                                               M0
                                                    +      + M2 <   i   < M are identically equal to

     zero.
                                             —32—



            Continuing in this manner, we see that during the stages in the k-th

block, we need store and update only the Mk state components Si, 1 + EM1

<   i   <
              M,       in addition to the first M0 state components corresponding to

the coupling constraints. At the beginning of the next block,j =                   1   + N1 +

        + Nk we can clear out the storage locations containing the Mk components

Si, 1 +                 <    i   <   M
active Mk+l components of S1, 1 + E M1 <
                                         and use the same locations to store the newly

                                                      i<        M1. Hence, if M +M1<M*
for i =      1,2,...,        then M* storage locations per list entry to store the

active state components will be sufficient. Thus we will have effectively

reduced the original M by N problem to an M* by N problem.



            Resource-allocation problems of this special structure can occur,

for example, in situations involving multiple time-period planning, or in

situations involving variables that represent activities that use mutually

exclusive classes of resources in addition to a few common resources that

couple them together. In most such situations                   =   M   is likely to be

much larger than M*. Thus we can achieve                   substantial storage savings

by recognizing and exploiting the block-angular constraint structure.

Note also that in order to achieve this storage reduction, we did not have

to incur any additional computation. In fact, by noting that many state

components are inactive and hence need not be computed, we have actually

reduced the amount of computation by a factor comparable to that for the

reduction in storage. The above scheme of dealing with block-angular

constraint structure was empirically tested on a few problems, and the

substantial storage savings and some computational savings obtained are

reported in        1     .
                              —33—




This can be easily generalized further to include the cases where the

variables that use the same resources or have non-zero functions in

the same constraints do not belong to adjacent blocks. Such a situa-

tion can occur in resource allocation and production scheduling problems

where some resources are used by variables in more than one but not in

all blocks. Usually such constraints will be lumped with the coupling

constraints. We call such a constraint structure a split-block-angular

structure and it is depicted in Figure 2. Our algorithm handles such a

structure with only a slight additional bookkeeping work. For further

details regarding the split-block-angular structure, see [1].




                              N




                                                    M51




 1

                             Figure 2
                                      — 314_




9. Mixed-Integer   Problems

           The decision-state method can very easily handle problems where

    some variables are discrete and some are continuous, as long as the van-

    ables are sum-separable. Consider the following mixed problem;

                        Maximize               f(x) +            f (y)
                                                                              <               i<M
                         Subject to            g(x) +             g(y',)          b.,   1 <



                                                  x e I,
                                                                                        1< j<N

                                                                                        1 <j<N'.
                                                  Y ER,

 We will divide the mixed problem into two component problems..

 Continuous—Component Problem:        Max E f't')
                       Subject   to                                       I       M
                                                 g'(y') b., 1

                                                       y' R...,l       <j <N'
 DiscreteComponent Problem: Max E! f(x)

                       Subject toEg(x)<
                                                         <   I   < M


                                               x3cI,   1 <j      <N

  The continuous-component problem can be solved parametrically on b =

                       right-hand-side, i.e., the optimal solution can be ob-
   (bl,b2...,bM),the
   tained for the continuous—component problem for all feasible right-hand-

   side values ranging between zero and the given right-hand-side. This can

   be done by any conventional, non-linear programming techniques such as

   separable programming [18], [19]; a most advantageous technique which is

   inherently parametric on the right-hand-side is GLM o]. In the simplest

   situation, which is perhaps the most prevalent in practice, the continuous—
                                     —35—



component problem is a linear programming problem. In this case, most

production LP codes would be capable of providing the optimal solutions w4th

some computational effort, for all feasible right-hand-sides.

       The discrete-component problem, of course, can be solved by the

decision-state method, which, being a dynamic programming method, auto-

matically gives the optimal solution for all feasible right-hand side

values. Let us denote the list of optimal solutions to the discrete-compo-

nent problem as LD. Each entry in the list LD contains an optimal solution,

the corresponding state vector and, the optimal value.          Let LD be

arranged in non-decreasing order of the optimal values. Then given any

right-hand-side vector bs. <   b,   the optimal solution to the discrete problem

with right hand side b' is given by the entry with the largest objective

value whose state vector does not exceed b' in any component. Similarly,

let LC denote the list of optimal solutions to the continuous component-

problem, parametrically on b, arranged in a manner similar to ID. Then

we can obtain the optimal solution to the mixed problem by determining a

combination of one entry from LD, (D',S',v') c LD, and one entry from LC,

          c LC, such that v+v11 >v1+v2 for all entries (&,S1,v1)            LD

and (D2,S21v2) c IC, and such that combination is feasible.
                                         —36—



10.        Bounding Elimination

           Considering the given N-stage problem as a decision tree, each

feasible decision vector D =
                                  (x1,   x2, ...,    Xk) can be regarded as a node at
the k-th echelon. The main thrust of the decision-state method is to gener-

ate only the feasible nodes on the undominated branches of the decision tree.

When a list entry (D, s, v) corresponding to a node is found dominated, we

know that the branches emanating from this node cannot be optimal for any

member of the family of problems with identical f(.) g1(.) functions and

with different right—hand sides. Typically, however, our lists will contain

many entries that are undominated but are non-optimal for the particular

problem being solved. It would be useful, therefore, if we find a way to

detect and eliminate some of these 'dead-weight' entries in addition to

eliminating those that are clearly dominated.

           The directed-tree-search methods eliminate such nodes through the

technique of bounding. That is, if the upper bound on the objective value

for all nodes on the branches emanating from a given node is smaller than

the value of the current best feasible solution, then the given node is said

to be fathomed and can be eliminated. A similar approach can be used with

our method. There are a number of ways in which an upper bound can be

obtained. One simple way is to first identify one constraint, say the 9-th,

as the tightest or the most binding constraint. Then the upperbound for a

k-th stage node (D, S, v) is given by v +                                     is the
                                                             f () where
greatest integer for which g() <
                                            (b       s) for each j from k+l to N. A

better upper bound can be obtained through a little more computational effort

as v + Z                           for each .3 from k+l to N is computed as the
           +l f() where
                                    3 — (b.1
greatest integer for which g. .(x.) <            -
                                                     s.)   for all i from 1 to M. The
                             1,)

                                                                                        .
                                   —37—


upper bounds obtained in these ways are likely to be loose.        Thus, they will

tend to be useful in eliminating a significant number of list entries when

only the unproductive stage variables remain to be considered and most of

the productive stage variables have already been considered. A workable

strategy, then, may be to call upon bounding elimination only towards the

tail end of the algorithm, the exact stage number dependent on the particular

problem.

           Of course, better and tighter bounds can be obtained by other so-

phisticated technqiues from the branch and bound category. A crucial question,

however, is whether the computational effort expended in doing this would

pay off in terms of reducing the entry lists and their computation. This

area needs to be investigated empirically by solving problems of realistic

sizes and structures. In the absence of such hard evidence, it seems that

the option of bounding elimination should first be tried with the crude,

easily obtainable upper bounds.

           The above bounding elimination procedure assumes knowledge of a

lower bound, that is, the value of the current best feasible solution. Of

course, the entry in the current list with the largest objective value identi-

fied during the merging phase obviously gives such a lower bound. It may be

possible, however, to improve this lower bound by using up the unused re-

source amounts. If the entry with the largest value is (D,S,v), then an

improved lower bound can be obtained as v + Z              f(i) where xk+1 is the

greatest integer such that                  (b1 —          for all i and
                                                    s1)
                                                     —         -            for   all
the greatest integer such that                            s.
1, etc. The larger the lower bound, and the smaller the upper bound associ-

ated with the nodes, the more list entries can be eliminated by this bounding

elimination procedure. A good discussion of different bounding strategies, and

 their empirical evaluation can be found in [al].
                                                                                    .
                                    —38—




11.   Conclusions


          In this paper we presented a new method for obtaining exact

optimal solutions to certain types of discrete-variable, non-linear, resource

allocation problems. The new method is called the decision-state method be-

cause, unlike the conventional dynamic programming method which works only in

the state space, the new method works in the decision space as well as the

state space. It generates and retains only a fraction of the points in the

state space, thus overcoming much of the curse of dimensionality. It carries

the cumulative decision strings associated with these points, and thus avoids

the backtracking entailed bythe conventional dynamic programming method for

recovery of the optimal decisions at all the stages.

         A concise yet complete and self-contained statement of the method was

given in Chapter 3 in the form of an algorithm (Algorithm 2), and it was proven

there that the algorithm indeed finds all exact optimal solutions to the given

general, non-linear, resource allocation problem with discrete—value variables.

In Chapter 5 we considered problems with special conditions such as block-

angular or split—block-angular constraints, non-integrality and core limitations.

We showed how the method could be specialized or adapted to accommodate effec-

tively the above conditions. In Chapter 6 we considered such tactical options

as sequencing the decision-variables, sequencing and inclusion of constraints,

and bounding elimination.

         Although an algorithmic statement of a problem-solving method may be

precise and complete enough mathematically or theoretically, the algorithm can
                                           —39—




be. implemented on a computer in different ways, some being more efficient than

others. We gave, therefore, an advantageous computer implementation of the

decision-state Algorithm 2 which combines the flexibility and adaptability of

the basic algorithm with the characteristics of a particular digital computer

and some good programing practices.

           As with most large-scale, general-purpose, mathematical programming

methods, the decision-state method offers certain options or opportunities for

making the application of the method to a particular problem more advantageous.

We discussed the availability and use of such options with regard to the method

as well as in the formulation of the problem. We also gave some empirical

evaluation of some of the different options that are intuitively appealing.

           The computer implementation of Algorithm 2 developed in this dissertation

was empirically tested and evaluated by using a number of resource allocation

problems from the open literature and a number of problems that were artificially

constructed to have certain desired structural characteristics simulating ex-

pected real conditions. The performance of Algorithm 2 was also compared with

that of the MMDP algorithm of Morin and Marsten on identical problems. For all

9 problems run with both algorithms, Algorithm 2 performed consistently better

than the MMDP algorithm, both in terms of high-speed memory requirement and in

terms of solution time. In fact Algorithm 2 achieved an order of magnitude

saving in memory requirement, and the MMDP algorithm took 45 to 82% more time

than that taken by Algorithm 2. Although the storage saving is substantial,

the time saving is not, since the latter might be attributed to the programming

techniques used.
                                     _L.O_




                                REFERENCES

 1. Dharmadhikari, V.K., "Solving Discrete-Variable Multiple-Constraint
     Non-linear Programs: The Decision-State Method; doctoral dissertation,
     Department of Computer Science and Operations Research, Southern
     Methodist University, Dallas, Texas, (1975).

2.   Bellman, R., Dynamic Programming, Princeton University Press, Princeton,
     N.J., (1957).

3.   Larson, R.E., State Increment Dynamic Programming, American Elsevier
     Publishing Company, New York, (1968).

4.   Wong, P.3., and Luenberger, D.G., "Reducing Memory Requirements of
     Dynamic Programming" Oper. Res., 16, 1115 - 1125, (1968)

5.   Wong, P.3., "A New Decomposition Procedure for Dynamic Programming,"
     Oper. Res., 18, 119—131, (1970).

6.   Yormark, J.S., and Baker, N.R., "On a Two—Dimensional Resource Allo-
     cation Problem," presented at 39th ORSA Meeting, Dallas, Texas, (1971).

7.   Weingartner, H.M., and Ness, D.N., "Methods for the Solution of the Multi-
     Dimensional 0-1 Knapsack Problem," Oper. Res., 15, 83—103, (1967).

8.   Nemhauser, G.L., and Ullman, Z., "Discrete Dynamic Programming and
     Capital Allocation," Management Sci., 15, 494-505, (1969).

9.   Haymond, R.E., "Discontinuities in the Optimal Return in Dynamic Pro-
     gramming," Jour. Math. Anal. Appl., 30, 159-169, (1970).

10. Gilmore, P.C., and Gomory, R.E., "The Theory and Computation of Knapsack
     Functions," Oper. Res., 14, 1045-1074, (1966).

11. Geoffrion, A.M., and Marsten, RE., "Integer Programming Algorithms:
     A Framework and State—of—the-Art Survey," Management Sci., 18, (1972).

12. Morin, T.L., and Marsten, R.E., "An Algorithm for Non-linear Knapsack
     Problems," presented at ORSA/TIMS Meeting, Boston, Mass., (1974).

13. Morin, T.L., and Marsten, R.E., "An Efficient Dynamic Programming Algo-
     rithm for the General Non-linear Multidimensional Knapsack Problem,"
     Discussion Paper (Rough Draft) I.E. .and M.S. Dept., Northwestern University,
     Evanston, Ill., (1972).

14. Morin, T.L., and Esogbue, A.0., "Reduction of Dimensionality in Dynamic
     Programming of Higher Dimensions with the Imbedded State Space Approach,"
     Discussion Paper, IE and MS Dept., Northwestern University, Evanston, Ill.,
     (1973).

15. Knuth, D.E., The Art of Computer Programmin9, Vol. 1, 2, 3, Addison—
     Wesley Publishing Company, Reading, Mass., (1968).
                                      —'41—




16.   Peterson, C.C., "Computational Experience with Variants of the Balas
      Algorithm Applied to the Selection of R and D Projects," Management
      Sci., 13, 736-750, (1967).

17. Grishman, Ralph, Assembly Language Programing for the Control Data
     6000 Series and the Cyber 70 Series, Algorithmic Press, New York, (1974).

18. Hadley, G., Nonlinear and Dynamic Programming, Addison-Wesley Publishing
     Company, Reading, Mass., (1964).

19. Nemhauser, G.L., Introduction to Dynamic Programming, John Wiley, New
     York, (1966).

20. Greenberg, H.J., and Robbins, T.C., "The Theory and Computation of
     Everett's Lagrange Multipliers by Generalized Linear Programing,"
     Technical Report CP-70008, Computer Science/Operations Research Center,
     Southern Methodist University, (1971).

21. Morin, T.L., and Marsten,R.E., "Branch and Bound Strategies for
     Dynamic Programming," Working Paper WP 750-754, Sloan School of Management,
     Massachusetts Institute of Technology, Cambridge, Mass., (1974).
