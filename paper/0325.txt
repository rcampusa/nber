                 NBER WORKING PAPER SERIES




      Analysis   of   Covariance with Qualitative Data


                       Gary Chamberlain




                  Working Paper No. 325




           NATIONAL BUREAU OF ECONOMIC RESEARCH
                 1050 Massachusetts Avenue
                    Cambridge MA 02138

                           March 1979




The research reported here is part of the NBER's research
program in Labor Economics. Any opinions expressed are
those of the author and not those of the National Bureau
of Economic Research. Financial Support was provided by
the National Science Foundation (Grant No. SOC77'—l562').
                                                            NBER Working Paper 325
                                                                        March 1979




           Analysis of Covariance with Qualitative Data

                            ABSTRACT




      In data with a group structure, incidental parameters are
included to control for missing variables. Applications include
longitudinal data and sibling data. In general, the joint max-

imurn likelihood estimator of the structural parameters is not
consistent as the number of groups     increases,   with   a fixed number

of observations per group. Instead      a conditional likelihood
function is maximized, conditional on sufficient statistics for

the incidental parameters. In   the logit case, a standard condi-
tional   logit program can be used. Another solution is a random

effects rwdel, in which the distribution of the incidental par-

ameters may depend upon the exogenous variables.




                                           Gary Chamberlain
                                           Department of Economics
                                           Harvard University
                                           Littauer Center
                                            Cambridge, MA 02138
                                            6l7/L95_3203
              ANALYSIS OF COVARIANCE WITH QUALITATIVE DATA

                                    by

                          Gary Chamberlain
                          Harvard University



1. Introduction

     This paper deals with data that has a group structure. A simple

example in the context of a linear regression model is

     E(yjtlx, , ct) =   'x   + c (i=l..    .
                                               ., N;   t=l, .   . ., T),

where there are T observations within each of N groups. The           are group

specific parameters. Our primary concern is with the estimation of ,

a parameter vector conunon to all groups. The role of the          is to control

for group specific effects; i.e., for omitted variables that are constant

within a group. The regression function that does not condition on the

group will not in general flentlfy :

     E(y1Jx

In this case there is an omitted variable bias.

     An important application is generated by longitudinal or panel data,

in which there are two or more observations on each individual. Then the

group is the individual, and the c capture iiicH.vidual differences. If

these person effects are correlated with x, then a regression function that

fails to control for them will not identify 3. In another important application

the group is a family, with observations on two or more siblings within the

family. Then the ct. capture omitted variables that are family specific,

and t1iy give a concrete representation 10 family background.

    We shall assume that observations from different groups are independent.
                                                                      2




Then the c. are incidental parameters (Neyman and Scott 13111, and ,

which is common to the independent sampling units, is a vector of structural

parameters. In the application to sibling data, T is small,       typically
T=2, whereas there may be a large number of families. Small T and large N

are also characteristic of many of the currently available longitudinal

data sets. So a basic statistical issue is to develop an estimator for 3

that   has   good properties in this case. In particular, the estimator ought

to be consistent as N -        for   fixed T.

       It is well—known that analysis of covariance in the linear regression

model does have this consistency property. The problem of finding consistent

estimators in other models is non—trivial, however, since the number of

incidental parameters is increasing with sample size. We shall work with

the following probability model: y. is a binary variable with

       Prob(y =       lix, '     =          +   ci.),


where F( )     is   a cumulative distribution function such as a unit normal or

a logistic. For example, y may indicate labor force participation,

unemployment, job change, marital status, health status, or a college

degree. Section 2 considers maximum likelihood (ML) estimation of the fixed

effects version of this model. A simple algorithm is available which

involves a weighted analysis of covariance at each iteration. The ML

estimator of         is not consistent (for fixed T), however, and we present a

simple example with T=2 in which the I'IL estimator of       converges to 2.

       Section 3 presents one solution to this problem by working with a

conditional likelihood function that conditios on sufficient statistics

for the incidental parameters. This likelihood function does not depend

upon the incidental paraneters, and hence standard asymptotic theory for

maximum likelihood estimation applies. This approach is applied to a
                                                                         3


multinomial logit model for grouped data and to the inultivariate log—linear

probability model. Section 4 develops an alternative approach, based on

a random effects model in which the incidental parameters are assumed to

follow a distribution. The important point here is that the distribution

of the c is not assumed to be independent of x; otherwise the problem of

omitted variable bias would be assumed away from the beginning. Throughout

the paper we shall use the familiar linear regression case to guide the

exposition.



2. Fixed Effects: Maximization of the Joint Likelihood Function

      We shall begin with a brief review of the linear regression case.

Let

            =           + a. +

where           is i.i.d. N(O,       So in addition to assuming independence across

the groups, we are assuming that observations within a group are independent

as well, conditional on the group effects. The dependence of different

observations within a group is assumed to be due to their common dependence

on the group specific aj. More general forms of dependence are, of course,

possible; for example, there could be serial correlation in addition to

the c in the longitudinal case.

      Maximum likelihood for this model is simply a multiple regression of

y on x and a set of group indicator dummy variables. A useful computational

simplification is that the ML estimator of           can be obtained from a

regression of y—y. on iti' where y. and . are group means
                                                                              Iy1/T).
In the case of T=2, this is equivalent to a regression of y.2—y11 on

x —x. .     Since we have

                        i2 il
..i2 -.il

      i2 — y11      =            —   +    —
                                              £11,
                                                                                                             4


with the 's independent of x, it is clear this provides a consistent

estimator of                    as N -        (provided that there is sufficient variation in

       -

           There is a comparable computational simplification for the probability

models. We shall discuss ML estimation using either a Newton—Raphson or

 a scoring algorithm, and shall show that each iteration reduces to a weighted

 analysis of covariance. The binary y are assumed to be independent

 (conditional on x, , and a) both between and within groups, with

 Prob(yi =                lix, ,    a) =      F('x.it + a.).
                                                         1
                                                                   Let O'z.
                                                                        -'it
                                                                             =             'x.- it ÷ a..1
                                                                                           -
                                                                                                             Then

 the log—likelihood function is

           L =        E
                     i,t
                                 hF(6'z. )   it     + (1   -
                                                                   lnIl   —
                                                                              F(Ezj)]}.
Note that if y
                            it
                                 = 1 for all t         then the ML estimate ofia is °,                      and if

y it        0 for all t           then the         ML estimate of a is —no.
                                                                      i
                                                                                            Hence the observations

on such groups do not affect the ML estimate of , and we can simplify by

only including in L the groups within which y varies.

           We have the following score vector and Hessian:
                            y          l—y                     2
                        (it
                     i,t F             1—F
                                             it)
                                                   F'it OOT        = ih1
where F and its derivatives are evaluated at Oz
                                              -                                   it   ,   and
                   l—y
     h   =   it
            it
                      —i + - it]
                   _____
                           (F')2 +
                                   y at — l—y it
                                                                    1—F
                                                                              )    F".
                                   (1—F)

It is well—known that L is concave for probit [F(u)                                            Pu
                                                                                               J    e       dr/]
or for          logit IF(u)             eU/(l + eU)]. Hence a Newton—Raphson algorithm is

expected to be effective:



                     —(,) L-
                                  —1
           AU    =


Also of interest is a scoring algQrithm which replaces
                                                                                      5




 by its expectation:1


                     =
                                 E(h1)

 where

        E(hit)   —   —        (F')2
                         F(l—F)

        In either case the computational burden at each iteration comes from

 inverting                                     where s1 is either h.t or E(h.). Simplifying

 the partitioned inverse gives the following formulas for up-dating

—13 and a.:
         1


          =
                         s                     - E
                                                     Si        '*)
                                                                       it         it - S

      Aa =           —(L13)' . (i=1,.                     .   ., N),
where

      it =                — F)/F'


         i = .it,
                          —
                          -'-1
                                 =—..,
                                  S.
                                      1

                                      1   '-
                                                 it -it
                                                                —*
                                                                p.
                                                                 1 =S.—
                                                                    1
                                                                       t .itp.
                                                                       1
                                                                             it

At each iteration, F and its derivatives are evaluated at the current values

    —    it
for 13'x. + a..
             1

      This iterated, weighted analysis of covariance algorithm is computationally

effective. 2 Unfortunately, the consistency property (for fixed T) of the NL

estimator of 13          in   the linear regression model does not carry over to this

case. That maximum likelihood need not be consistent in the presence of

 incidental parameters can be illustrated in the linear regression model.

The ML estimator of a2                doe; not adjust for degrees of freedom, and hence

      plim a2 =          T-l

For T=2, the ML estimator is inconsistent by a factor of two.3
                                                                                         6

Ano               thexampl e is a. tour egJi on
             it =           i,t_i +        + Ei.

We    shall condition on y10. In that case the likelihood                    function with
6it     i.i.d. N(O, a2) is formally identical to the previous case. The log—

likelihood function is quadratic in 13 and a (given a2), and the ML estimator

of 13        is       analysis of covariance. With T=2, it can be obtained from a least

squares regression of y12y1 On Y1i—Yo. Given that the log—likelihood

function is quadratic, it is rather surprising that the ML est:finator

for 13 is not consistent. The inconsistency follows immediately since

             i2 —                = 13(y.1 — y.0) +    i2
and               is correlated with y1. If the joint distribution of
                                                                                  (y0. y1, y2)
is stationary, then the estimator converges to (13—1)12 as N-°.

             As an example of the inconsistency of maximum likelihood in the probability

models, consider the following logit model: F(u) =                      e'I(l   + eU), T=2,

x.1=O, x.2=l, i1, .                     . ., N.    So the "treatment" is administered only to

the second observation in the group. Assume that the sequence of a1ts is

such that the following limits exist:

             llm*
             N-°°
                  E[y.1(l—y2)Ja.]
                   1                                  m1


                                                  =
             1im E[(l—y.1)y.2a.]                      m2


where
          E1y11(1—y.2) a] = F(ai)F(—ct1 —13) and E[(l_y11)y12Ia] F(_a)F(a +                      ).
Then     Andersen 11973, P. 66] shows that the ML estimator of 13 almost surely                  satisfies

             13
                  =   213


as N-'°°. A simple extension of his argument shows that if F is a distribution

function corresponding to a symmetric, continuous, non—zero probability

density, then
                            1    m.)
        13        = 2F'(          L

                                m1+m2
                                                                       7



almost surely as N-. The logit case is special in that m2/m1 = e1,

independently of the sequence of ctj's.      In general the limiting

depends on this sequence; but if all of the      c = 0,   then once again we
obtain     =   2 almost surely as N-oo.
       We conclude that the linear regression model is very special. The

consistency of the ML est:imator of       does not carry over to other models.

The next section interprets this result by introducing a conditional

likelihood function that conditions on sufficient statistics for the

incidental parameters. In the linear regression case, the conditional ML

estimator of        is identical to the ML estimator based on the original joint

likelihood function. Then we show that the idea of using such a conditional

likelihood function can be applied to other models.



3. Fixed Effects: Lhe Conditional Likelihood Function

       We have seen that maximization of the fixed effects likelihood

function can give seriously inconsistent estimators if there are only a

small number of observations per group. This section will develop an

alternative approach using a conditional likelihood function. The key idea

is to base the likelihood function on the conditional distribution of the

data, conditioning on a set of sufficient statistics for the incidental
            4
parameters.

       We shall begin by applying this idea to the familiar linear regression

case. Let

       yit =         +    +

with       i.i.d. N(0, 02). Then a sufficient statistic for     a is
It is straightforward to check that the conditional density for
                                                                     y1, .     . .,
conditional    on         is
                                                                                                       8



                           •
                               '              =


       (2S-(T-l)/2(T-J) exp{-
                                                      22        (Yft
                                                                        -       - '(X       -




Note   that   this conditional density does not depend upon ..                                  Hence      the   conditional

lo;—likclihood function depends only upon                              and a:




       L      -   N(T-l)           Thu -          i
                                                      [(y.it - y.)
                                                                1
                                                                   -         '(x.it   -.fl2;
                                                                                        L


there is no incidental parameter problem, and so maximum likelihood will

give consistent estimates provided that the usual regularity conditions are

satisfied. The conditional ML estimator of                                  is the analysis of covariance

estimator that results from maximization of the joint likelihood function.

Hence the consistency of that estimat:or, which was surprising given the

incidental parameter problem, follows immediately from the coincidence of

the joint and the conditional ML estimators.

       The advantage of the conditional likelihood approach can be seen in

the conditional ML estimator for a

        2 =       1                      —                  —      2
              N(T—1)

Unlike the     joint ML estimator, here there is a correction for degrees of

freedom which ensures that â2 isa consistent estimator of cY2.

       The conditional likelihood approach can be applied directly to the

fixed effects logit probability model, since                                     is again a sufficient
                               5
statistic for ct..
                       1
                                   Consider first the case of T=2. If                 il +             = ( or 2,

then y and y.2 are both determined given their sum.                                   So   the only case of
interest is y11 + y2 = 1.                    Then the two possibilities               are w1 =     1       if
il' y12)              (0,1) and w. = 0            if (y.1, 'i2 =            (1,0). The conditional
                                                                                        9


density is

        Prob(w =           1         ÷     = 1)     Prob(w1 =      l)/[Prob(w1
                                                                                 = 0) + Prob(w   1)1




        = e
              ' i2il   ,                 = F['(x     —x.
                                                  -.12 -.il
          l+e

which does not depend upon c. The conditional log—likelihood function is

        L =
              J1 {w1           lnF[8'(x12—x1)] + (l_w) lnF[—8'(x.2—x.1)]},
                   1

 where I =
               {ijy11           + y12 =   l}.
        This conditional likelihood function does not depend upon the incidental

parameters. In fact, it is in the form of a binary logit likelihood function

 in which the two outcomes are (0,1) and (1,0) with explanatory variables

x,,—x1.
           This        is the analog of differencing in the two period regression

model. The conditional ML estimate of 8 can be obtained simply from a standard

ML binary logit program.

        The conditional ML estimator of B                  Is   consistent provided that the conditional

likelihood function satisfies regularity conditions, which impose mild

restrictions on the a. These restrictions, which constrain the rate at

which the sequence of a1's is allowed to become unbounded, are discussed in

Andersen [1], [2]. Furthermore, the inverse of the information matrix based

on the conditional likelihood function provides an a9ymptotIc (as N-*)

covariance matrix              for   the conditional ML estimator of 8 6 In deriving this

information matrix, one must be careful to note that I is a random set

of indices. This can be made more explicit by defining d1 = 1 if

    +         = 1 and d1 = 0
                             otherwise. Then we have

               =
                   _dF(l_F)              izn      i2il
                                                                                               10

where F and its derivatives are evaluated at (x.2.-xii). The

information matrix is

 J =
         LP1F(l-F)(x.2-x11)(x12-x11,

where

                             =        +
                                          il(1                         + F(—ci.
                                                                                          i?(c   +

       This        information matrix fs difficult to evaluate since we do not have



a consistent estimator for e., which appears in                                   Moreover, a standard

ML binary logit program will be evaluating
                                          2
             -
                   E
                        2
                       ___               ___
(since the Hessian of the logit log—likelihood function is non—stochastic),

which depends only upon                       (given d). In fact,             is an appropriate asymptotic

covariance matrix for the conditional ML estimator of ,                               since   we can apply

the strong law of large numbers to establish that
                                 a. s.

        NdJ N
            -                J    -       0    asN-
        if         m.m/i2 <
                 i -.J--,1


wherein., replaces each element of (x-x) by its square. This follows

since the d are independent with Ed.
                       1           1
                                     =                          P.,
                                                                 1
                                                                      and both F and the variance of

d1 are uniformly bounded. The condition for convergence clearly holds if the

       are uniformly bounded.7

        For general T, conditioning on                                11, .   . ., i,   gives the following

 conditional log—likelihood function:



         L =           ln
                       I
                          [exp('x             i t)/          exp('x. d )],
                                                                  t' t
                                                      dEB1
                                                                                     11.




where B1 = {d =
                     (d1,      .   .
                                       ., d)1d      0 or 1 and td =
                                                                        yft}.    L

is in conditional logit form with the alternative set
                                                                       (B1) varying
across the observations.8 There are T+l distinct alternative sets corresponding

to yft = 0,        1, .   . ., T.           Groups for which it: = 0 or T contribute zero

to L, however, and so only T—l alternative sets are relevant. The alternative

set for groups with it = s has () elements, corresponding to the distinct

sequences of T trials with s successes. For example, with T=3 and s=l

there are three alternatives with the following conditional probabilities:

        Prob(l,0,0jy. = 1)
                  tit
                                        =   - —il —xi3)
                                            D


        Prob(O,1,0Jy. = 1) =
                                            e' i2j3
                          it
        Prob(0,0,lJy. = 1) =
                          it                , D = e3'(x11—x.3) +   e'(x.2—x.3)+ 1.

        Since   L is in the form of a conditional logit log—likelihood function,

it can be maximized by standard programs. The information matrix evaluated

by such a program will Implicitly condition on the alternative sets, which

are random in our problem. So the program will evaluate                  B =   —E(2L/'IB).
Since the Hessian of the log—likelihood function in conditional logit is

non—stochastic, we have B =                 —2L/'.         Hence    is an appropriate asymptotic

covariance matrix for the conditional ML estimator of                   provided that

JB/N   converges to its expectation. This ili follow from the strong
law    of large numbers if, for             example, the       are uniformly bounded.
                                                                               12



     In the remainder of this section we :ha1l first extend our conditional

likelihood approach from the binary to the multinomial case; then we shall

apply our approach to the multivariate lcg-lincar probability model, thereby

relaxing the assumption that the observations within a group are independent.



Multinomial Logit for Grouped Data. Say that it can take on three values:

a, b, c. Then we have

                           a..   +   -3'x- itj
                            1J
    Prob(y. =   j) =   e
                            aii +                    (j = a, b, .
                                         -itj




We assume that the y's are independent both within and between groups. We

shall condition on the number of occurrences within the ith group of each of

the three events.

     If T=2, then the only cases of interest are those in which two of the

three events each occurs once, for otherwise there is no stochastic variation.

Conditioning on a and b each occurring once gives (suppressing the i subscript):


                                                       -
    PfObka,b)I (a,b)       or (b,a)] =
                                                 1   + e'
where z =   (x  —x ) — (x
             .2b .2a
                           —x ).
                        -lb .la
                                                 Hence we have a binary logit problem

with (a,b) and (b,a) as the two alternatives and with z as the explanatory

variables. The incidental parameters do not appear in this conditional probability.

There is a similar result when we condition on a and c each occurring once,

and also when b arid c each occur once.
                                                                                           13


        In the general case of T independent observations on each group with

                                                         = 1 if y. = j and                =0
y.
 it taking on J values, we define w. .           it]             it              w.
                                                                                   it].         otherwise.

We condition on 5.. =                     j=l,   .   .    ., J.    This gives the following

conditional log—likelihood function:




          L =




where

     Bi =   {d   =
                     (d11,
                             .   .
                                     ., d,.)1d1      = 0 or 1, dt.         1,             s r1
This is in the form of a conditional logit log—likelihood function and can

be maximized by standard programs.



The Log—Linear Probability Model. We shall relax the assumption that the

    are independent within a group by extending the conditional likelihood
                                                              9
approach to the general log—linear model. We begin by illustrating the log—

linear model for the binary case                         =0   or 1) with T=3 (the i subscripts

are suppressed):

        ln Prob(y1, y2, y3) = p          + y1y + y2y +

                                     +
                     + 112y1                     +
                                                         23yy + 123yyy,
where y* = 1 if y = 1 and y* =            —1   if y =      0.     This is a saturated model

since there are 2—l = 7 independent probabilities, and there are seven

free parameters with p determined by the constraint that the probabilities

sum to one.

     A common way to impose structure on this model is to specify the main

effects in terms of a set of explanatory variables: 1jt =                                 and to assume

that the interaction terms are constant:                           =     for    s, t=1, 2, 3, and

il23 l23 Additional structure can be imposed by specifying that the
                                                                                             14


lnteraction      terms beyond some order arc zero; for example, that                              =   0.
       We shall introduce group specific effects by letting
                                                                                     it = çi +        -Sit
It   is straightforward to check that
              Prob(y.1 =       ly.2,   y13) =                                *           *                   * *
         in
                                                   2a1 + 2'x + 2'y12y12 + 2l3i3 + 2il23Yf2vfl.
              l—Prob(y11=1jy12, •3)

So If the interaction terms                    =        =           0, then y1 is independent of
     and y3, and the probability of y1=l takes the logistic form that we have

been using (except for a scale factor of 2).

         For the general case of T binary variables we have (suppressing the i

subscripts):
                                                    T
         in Prob(y1,                        +
                                                   k=l tEM
                                                                        •
                                                                             •Y
where Mk =
               {(t1,   .   .
                                ., t)}   is the set consisting of the () groups of

k integers that can be formed from the integers 1,                      .   . ., T.    We shall

specify the first order terms asy =
                                               it
                                                        aI + -'x
                                                               =it.    Th        interaction terms

may depend upon x but with coefficients that do not vary in i, so that the

incidental parameters are confined to the first order terms.

         Since 1.yft is a sufficient statistic for a., we form the following

conditional density:
                                                exp[(a.1 + f'x.
         Prob(y.1, .
                               'IT't       =                 —it )yit + g(y.)]

                                                        exp[(a. +   it )d*
                                                                        t + g(d)]
                                               dEB1



                                                   exp[B'Ex.y+ g(y.)]

                                                                       +    g(d)I


where B. =     {d = (d1,       .
                                   . ., dT)Idt      = 0 or 1 and tdt =

     =             .               and g( ) does not depént upon a.. We see that

the conditional density does not depend upon a.. The corresponding

log—likelihood function differs from the one for independent y's
                                                                                  15

only in the g( )     terms.     For example, with T=3 and            = 1 we have

g(l,   0, 0) =    l2 l3 + 23 + l23
                      -
                                                g(O, 1, 0) =   l2   +
                                                                        l3    -



+    123 g(0, 0,          =
                              l2 l3
                                —       —
                                            23 + l23 Rescaling all the coefficients
by one—half, we can write the conditional probabilities as




       Prob(l, 0, 0Iy = 1) =            expt'(x.1   —
                                                        i3 +   23 12
       Prob(O, 1, OIy = 1) =            exp['(x12
                                                    —
                                                        j3) + 113   — 1121


       Prob(0, 0, ly = 1)           =




with   D determined so that the probabilities sum to one. So this differs

from the independence case by introducing alternative specific constants

into the conditional probabilities.

       We have seen that it is fruitful to base the likelihood function on a

conditional distribution that conditions on sufficient statistics for the

incidental parameters. It is not always possible, however, to find a sufficient

statistic for         such that the conditional distribution is sufficiently informative

about            The next section examines a random effects model in which a consistent

estimator for        can be obtained without relying upon sufficient statistics for

the cz1.




4. Random Effects: the Marginal Likelihood Function

       An alternative approach is to assume thai: the incidental parameters

follow a distribution. Then the likelihood function can be based on the

density for y, given x, , and G, the distribution function for ct.                     If

we specify a parametric family for C, indexed by a fthite parameter vector

T,    then   we have the following log—likelihood function for ,             i:
                                                                           16

     L   inff(yj, , )dG(cx,       i).


So the density function for y   conditional on   has been replaced by a
density function that is marginal on c.      The maximization of this i.ikeiThood
function will, under weak regularity conditions, give consistent. (as N -
                           2
estimators   for   and •
     This approach introduces additional information and is most naturally

formulated in Bayesian terms. A potentially appealing prior distribution

specifies that the ci's are independent and identically distributed.

This can often be justified bydeFinetti's 116]    exchangeability         criterion.

If (for arbitrary N) the distribution of the a.'s is not affected by

permuting them, so that the subscript is purely a labeling device with no

substantive content, then the joint distribution of the &s must be ex—

pressable as random sampling from a univariate distribution. This criterion

will often be satisfied when i indexes individuals (longitudinal data) or

families (sibling data).

     The main point I want to make here is that the random sampling

             on is appropriate only as a marginal distribution for a. We

must, however, specify a distribution for a conditional on x. The convent1uiii

random effects model assumes that a is independent of x. But our interest

in introducing the incidental parameters was motivated by missing variables

that are correlated with x. If one mistakenly models a as independent of x,

then the omitted   variable bias is not eliminated. So we want          to specify a
                                                                  13
conditional distribution for a given x that allows for dependence.              A

convenient possibility is to assume that the dependence is only via a linear

regression function: c. = Tr'x.
                      1     .i
                                  + v.,
                                     1
                                        withx =   (x
                                                   -.il
                                                          ,   .   . ., x-iT), and where v
is independent of x. We appeal to exchangeability to argue that the v are

independent and identically distributed. A restriction on the regression

function that may be appropriate is n'x. =
                                                                          17


     We shall illustrate this approach with a production function example

that leads to a linear regression model.14 Say that a farmer is producing

a product under the following Cobb—Douglas technology: Y =           LQ'e6,
where Y is output, L is a variable factor (labor), Q is a fixed factor (soil

quality),    is stochastic (rainfall), and 0 <         < 1.   Assume that c is

distributed independently of Q; persistent differences in average rainfall

can be incorporated into Q. We assume that the farmer knows the product

price (P) and the factor price (W), which do not depend on his decisions,

and that he knows Q. The factor input decision, however, is made before

knowing E, and we assume that L is chosen to maximize expected profit:

E(PY — WLP, W, Q).

     There are observations on il, .       . ., N   farms in each of t=l, .     .   ., T
periods. Assume that Q is constant over the period of the sample and tiat

the distribution of c conditional on Q, W, and P is             i.i.d. N(0, a2).

Then we have the following production and factor demand functions:

         =
             x1 +        + 61t

                   1
     x1 = p +              + c) +

where y =   mY,   x =   lnL, c=ylnQ, p =   (ln ÷ 42)/(l_),     z =   ln(P/W),
and u is a random term, reflecting optimization and other errors, which is

independent of c and c. Although Q is krown to the farmer and affects his

factor demand decisions, we assume that it is not observed by the econometrician;

   is included in order to capture this omitted variable. The example is

useful in showing explicitly how a correlation between x and a might arise.
    We shall focus on tiiing th roduction function without using

whatever price data is available. A pooled least squares regression of y

on x, which does not allow for farm effects, is inconsistent. If a is

independent of z, then as N-° this estimator converges to
                                                                                                               18

                               2
                           a
                               a
                (l—) (Vw + VB)
where
                 1                                                                                2
       V = plim-—-                           (x.     —
                                                         5L)          plim —               —
                  N-*° NTj,t it-                               VB                N   j I

and a Is the marginal variance of a. Now consider a random effects

approach, a1 i.i.d. N(i, az), that incorrectly assumes that a is independent

of x. Then the ML estimator of , conditional on A = a2/a2, is generalized

least squares. This is equivalent to ordinary least squares using deviations
                                      —            —
from fractional means: regress     —
                                     yy. on     —      where y=l — (1 + AT) —1/2           ix.,
This estimator converges,as N-*o, to

                          (l—y) 22
                                            a
                (1)                +        (1—y) VB1


Hence It        is consistent only as T-°.

       So    it    is     essential to allow for a dependence between a and x.
Let w1 =
                  z./(l)                + u1 and assume that w. is i.i.d. N(m, ).                              Then the

distribution of a                  conditional             on x is given by a1 = K + 'rr'x. +                      where
                                                                                                             v1,
                      2            2
                  aa.          a
            -
                  —
                                   a
                                        2
                                                    +] -l2
                           (l-.)
i is a Txl         vector              of ones, and v is independent of x with v1 i.i.d. N(O, a2).

Note that assuming a stationary                                 does no imply that IT'x1                  Sx. If T > 2.

A sufficient condition is that is                                    equic)rrelated:              =
                                                                                                      p11 + p29Q'.
       The ML estimator of (, Tr),                             allowing     or several variables in

x1 and given A = a2/a2,                              can   be obtained From the regression of                       —

on          —   ix.       and (l—y)x..                   The resulting estimator for                  can be obtained from

the   regression           of                   —
                                                    yy1 on the residual from the regression of x1 —
on   x1. This residual is                                  —
                                                                    hut the regression of                —                 —

is    equivalent to the regression of                                   —
                                                                            y1
                                                                                 on it —
       We have obtained the interesting result that a random effects
                                                                                                         19

 specification can give a ML estimator of 8                             that     is identical to the fixed

 effects estimator, if we allow the distribution of the incidental parameters

 to depend upon x)-50f course the linear regression case is special, since

 the fixed effects estimator is consistent. This is not true for the (joint)

 ML estimator of 8       in   the linear autoregressive model or in the probability

 models. So the random effects specification leads to new estimators In those

 cases.

         In the autoregressive case, let

                  io +a I +c Ii
         i2 =   il   + a1 +


 where, conditional on y
                                   10
                                        and a , we have
                                                   I                    (c.1,    c.i2 )   i.1.d.       a normal

distribution with mean 0 and diagonal covariance matrix: d1ag{o,

Let       =
              Try10 + v1, where, conditional on y10,we have v1 i.i.d. N0, 02). Then




where
                y12) =

              = + 2 =
                         i'        62'io + (u11, u12),

                                         +   IF,   and u1 is i.i.d. N(0, E). This is a inultivar late

regression model in which the ML estimator
                                                                        of 6 is obtained from the least
squares regressions of
                                   y1 and y2 on y0. Then we can solve for the ML
                                    A         A         A
estimator of $ from          8 =         —                   —
                                                                  1).
                                   (62        61)1(61                     This estimator is consistent
if the y10 have sufficient variation and if 8                              +    IF   1. It is equivalent
to taking first differences,
                                             2                =
                                                                   8(y11
                                                                            —
                                                                                  y10)
                                                                                          + £12 —
                                                                                                    c,   and
using y0 as an instrumental variable for y11 —                                            If   we add the
assumption that          =
                             02     then an additional consistent estimator of 8
can    be obtained from a consistent
                                                       estimator of E. Now the ML estimator
of 8   will   combine the estimator obtained from
                                                                           the regression coefficients
with the estimator obtained from the
                                                            residual covariance matrix.

       The likelihood function for the joint distribution of
                                                                                                (v0, y1, y2)
                                                                                20



is obtained by multiplying the likelihood conditional on y0 by the

marginal density of y0. If the parameters of this marginal density are

left unconstrained, then the ML estimator of             is unaffected. Imposing

stationarity on the joint distribution will, however, imply constraints.

If           is i.i.d. normal with variance p, then stationarity implies

that P =
                  /I(1rj.
        In    the binary data case, let Prob(y4 =      lix, ,    a) =   F('j     + a.). Then

the log—likelihood function under our random effects specification is

                                           y                              l-y
        L =   ElnfflF('x
                 t — _it
                               ÷ ir'x1 + v) it[1 —
                                                     F('x1   +irx
                                                              ——— + v)]          dH(vIp),
              i                                                                       —




wherekl( i)         is    a family of univariate distribution functions indexed by

the parameter vector P.            For example, if F is a unit normal distribution

function and we choose H to be the distribution function of a N(0, 02)

random variable, then our specification gives a multivariate probit model:

              = 1   1f         +       +       >



             i.i.d.      N(O, 0vTT +

where         is a T x 1 vector of ones. The novel feature of this model is

the inclusion of the term 7r'x. to capture the dependence between the

incidental parameters and x.

        For example,.consider estimating the effect of ability on the

probability of attending college, controlling for family background. There

is    a sample of N families with test scores (x) for         each of T2    brothers
per    famli, y. The family effect:        Is Intended to capture omitted variables
such as family wealth and parents' s.hooling. Under this interpretation,
a is likely to be correlated with x Our              procedure   n the probit ce
is to fit a (constrained) bivariate robit model.for y11 and y12 on                   and
                                                                  21



x12. This provides estimates of

     +ir1 r2        1



from which we obtain an estimate of    by taking the coefficient of sib l's

test score in sib l's equation minus the coefficient of sib l's test score

in sib 2's equation. We can do the same with sib 2's test score and hence

the constraint on the matrix of probit coefficieits.

     From the symmetry of this example (ignoring birth order effects),

it is appropriate to set    =
                                2   Then   can be consistently estimated

by taking the coefficient of sib l's test score in sib l's equation minus

the coefficient of sib 2's test score in sib l's equation. Hence we only

require y for one of the sibs provided that we have x for both. For

example, the Michigan Panel Study of Income Dynamics 1261 has extensive

information on the respondent and much less complete information on his

siblings. There is schooling data for the respondent and his oldest

brother, but earnings and occupation data only for the respondent.

Nevertheless, we can control for family backgiound In assessing the

relationship between schooling and earnings by including the schooling of

sib 2 in a regression of sib l's earnings on his schooling. Then

is estimated by the excess of sib l's schooling coefficient over that of

his brother. A probit example could arise in studying the relationship

between schooling and occupation, where occupations are classified into

two groups corresponding to production and non—production workers.



5. Conclusion

    The paper has discussed three approaches t   the analysis of grouped data:

the joint likelihood function, the conditional likelihood function, and the

marginal likelihood function. Throughout the paper, our concern has been with
                                                                        22

 the parameters ()    that   are common to all of the
                                                        groups; the incidental parameters
 (a1) are intended to capture
                                 group effects whose omission would
                                                                      result in biased
 estimates of .     The objective has been to obtain
                                                     estimators that converge
 to    as the number of groups (N)
                                 increases, even if the number of observations
 per group (T) is small. Important
                                   applications include longitudinal
                                                                              data,
 in which there are two or more observations on each individual,
  and the a. capture person effects; and sibling dnia, in which the
                                                                         a1
 capture family effects, such as omitted family background variables.

      Wehave illustrated the inconsistency or the olnt ML estimator iii
 the fixed effects probability model s. One solution, within the fixed
 effects model,   is to maximize a conditional
                                                 likelihood function that
 conditions on sufficient statistics for the incidental parameters.
                                                                             This
 conditional likelihood function does not depend upon the incidental
                                                                             parameters,
 and so standard asymptotic
                               theory can be applied. In the
                                                                 (normal—theory)
 linear regression model, the
                                 consistency of the joint ML estimator of
 corresponds to the coincidence of the
                                          joint and the conditional ML estimators.

 In the log:Lt case, however, the conditional ML estimator of
                                                                    is consistent
 whereas the joint ML estimator is not (for fixed T). The conditional
                                                                      ML
 estimator for the logit case can be implemented with a standard conditional

logit program, which allows the alternative
                                                 set to vary across the observations.
      Finally, we discussed random effects
                                             models which impose a (prior)
distribution on the incidental
                                   parameters. Then the likelihood function
is based on the distribution for
                                     y that is marginal on the incidental

parameters. The important point here is that the specification of the

conditional distribution fora. given x should allow for dependence;
                                                                    the
common assumption that a. is independent of x assumes away omitted
                                                                   varIable
bias. In the linear
                      regression model, the ML estimator for       under our
random effects specification is
                                   once again analysis of covariance.    So
in this special case, all three
                                   of our approaches give identical estimators
                                                                      23

for .      In   the probability models, however, the marginal likelihood

specification leads to new estimators.

    The marginal likelihood approach has the advantage of not requiring

simple sUfficient statistics for the incidental parameters. Furthermore,

it imposes (stochastic) restrictions on the fixed effects model, which will
lead to more precise estimators if the restrictions are valid. The dis—

advantage is that in order to specify that the c are independent of each

other (conditional on x), our approach requires a particular parametric
class of conditional distributions for . given x. Hence some
                                       1       --
                                                                   sensitivity
analysis   is called for. The fixed effects model allows for a very general

relationship between the incidental parameters and the explanatory variables.
                                                                           24


                                     Footno tes



11n the logit case the Hessian does not depend upon y, and so scoring is

     identical to the Newton—Raphson algorithm.


       program to implement this algorithm is described in Hall [21],

     along with an example of the computational efficiency of the program.

     A labor force participation application of a fixed effects probit model

     is presented in Heckman [221.

3mis example is discussed in Neyrnan and Scott [31].


4The use of conditional likelihood functions for incidental parameter

     problems is discussed in Bartlett [8], [9], [10], Andersen [1], [5],

     Kalbfleisch and Sprott [23], and Barndorff—Nielsen [7].

5The conditional likelihood approach in the logit case is closely related

     to R. A. Fisher's [17] exact test for independence in a 2x2 table. This

     exact significance test has been extended by Cox [15] and others to the

     case of several contingency tables. Additional references are in Cox [15]

     and in Bishop et al. [111. A conditional likelihood approach was used

     by Rasch [321, f 331 in his model for intelligence tests. The

     probability that person i gives a correct answer to item number t is

     exp(czi + )/[l + exp(ai +           this is a special case in which        is a

     set of dummy indicator variables. An algorithm for conditional maximum

     likelihood estimation in this model is described in Andersen [4].

(3

     The efficiency of the conditional ML estimator is maximized by conditioning

     on minimal sufficient statistics for the incidental parameters. Zy

     is a minimal sufficient statistic for        both in the linear regression

     model and in the logit model. Even so the conditional ML estimator need

     not attain the asymptotic Cramer—Rao bound as N-*oo for fixed T. It does

     in the linear regression case but not in the logit model. However, I
                                                                     25


  doubt whether there is another consistent estimator that has smaller

  asymptotic variance in the fixed effects logit model. The random effec:s

  model of section 4, which introduces additional (stochastic) restrictions,

  can lead to a more efficient estimator of .

 7An alternative justification for the use of —E(2L/B'Id) can be based on

  stating the limiting distribution properties in terms of the conditionaL

  distribution, in which the observed values of the sufficient statistics

  are treated as parameters. This approach is pursued in Andersen [3].

 8The conditional logit model is developed in McFadden 1 25].


 9The log—linear model is developed in Goodman 118], [19], Haberman [20],

  and Nerlove and Press 130]. Additional references are in Bishop et al. [11].

101n the probit model, for example, there does not appear to be such a

  sufficient statistic.

11Kalbfleisch and Sprott [23] call this an integrated likelihood function.

  A marginal likelihood function can also be useful in a fixed effects

  approach, in which we consider the distribution of some function of

  conditional on a.. For example, in the linear regression case with T=2,

  the distribution of y12—y11 does not depend upon ct1. Hence maximizing

  the associated likelihood function gives consistent (as N-oo) estimators

  of   and .   Once again the ML estimator of     is the standard analysis of

  covar lance estimator.


Note that   the   original Kiefer and Wolfowitz [24] results were not limited

  to the parametric case.


13Note that the empirical work by Chamberlain and Griliches [13], 114]

  and Chamberlain [121 does allow the random effects to be correlated with

  the explanatory variables. Also in the original Balestra and Nerlove

  [6] model, the autoregressive component is correlated with the random effects.
                                                                    26



14Thjs example is discussed in Mundlak [271, [281.


15This result is discussed in Mundlak [29] for the case ir'x.   =




                                                                         S
                                                                     27

                                  References


 [1] Andersen, E. B.      "Asymptotic Properties of Conditional Maximum

       Likelihood Estimators", Journal of the Royal Statistical Society,

       Series   B, 32 (1970), 283—301.
 [2] Andersen, E. B. "Asymptotic Properties of Conditional Likelihood
       Ratio Tests", Journal of the American Statistical Association,     66

       (1971), 630—633.

 13]   Andersen, E. B.    "A Strictly Conditional Approach in Estimation

       Theory", Skandinavisk Aktuarietidskrjft, (1971), 39—49.

 14] Andersen, E. B. "The Numerical Solution of a Set of Conditional

       Estimation Equations", Journal of the Royal Statistical Society,
       Series B, 34 (1972), 42—54.
 [5] Andersen, E. B. Conditional Inference and Models for Measuring

       (Copenhagen: Mentalhygiejnisk Forlag, 1973).

 [6] Balestra, P, and Nerlove, M. "Pooling Cross Section and Time Series

       Data in the Estimation of a Dynamic Model: The Demand for Natural

       Gas", Econometrica, 34 (1966), 585—612.

 17] Barndorff—Nielsen, 0. Information and Exponential Families in

       Statistical Theory (New York: Wiley, 1978).

 [8] Bartlett, M. S. "The Information Available in Small Samples",

       Proceedings of the Cambridge Philosophical Society, 32 (1936), 560—566.

 [9] Bartlett, M. S. "Statistical Information and Properties of Sufficiency",

       Proceedings of the Royal Society, Series A, 154 (1936), 124—137.

[10] Bartlett, M. S. "Properties of Sufficiency and Statistical Tests",

       Proceedings -f the Royal Society, Series A, 160 (1937), 268—282.

[11] Bishop, Y. M. N., Fienberg, S. E. and Holland, P. W. Discrete

       Multivariate Analysis: Theory and Practice (Cambrige, Mass.:N.I.T. Press,

       1975).
                                                                   28




[12] Chamberlain, G. "Omitted Variable Bias in Panel Data: Estimating

        the Returns to Schooling", Annales de l'INSEE, 30-31 (1978), 49—82

[13] Chamberlain, G. and Griliches, Z. "Unobservables with a Variance—

        Components Structure: Ability, Schooling, and the Economic Success

        of Brothers", International Economic Review, 16 (1975), 422—449.

[14] "Chamberlain, G. and Griliches, Z. "More on Brothers" in Taubman, P.

        (ed.), Kinometrics: The Determinants of Socio—economic Success Within

        and Between Families (Amsterdam: North Holland Publishing Company, 1977).

[15] Cox, D. R. Analysis of Binary Data (Jce, 4ylethuen, l?7O).

[16]    deFinetti, B. "La Prvision: Les Lois Logiques, ses Sources Subjectives",

        Annales de l'Institut Henri Poincar, 7 (1937). English translation in

 Kyburg, H. E. and Smokier, H. E. (eds.), Studies in Subjective Probability

 (New York: Wiley, 1964).,

[17] Fisher, R. A. "The Logic of Inductive Inference",Journal of the

        Royal Statistical Society, Series B, 98 (1935), 39—54.

[18] Goodman, L. "The Multivariate Analysis of Qualitative Data:

        Interactions Among Multiple Classifications", Journal of the American

        Statistical Association, 65 (1970), 226—256.

[19] Goodman, L "A Modified Multiple Regression Approach to the Analysis

        of Dichotomous Variables", American Sociological Review, 37 (1972),

        28—4 6.


[20] Haberman, S. J.
                                                        (Chicago: University
       of Chicago Press, 1974).

[21] Hall,' B. H. "A General Framework for Tine Series—Cross Section
                                                                        Estimation",

       Annales de l'INSEE, 303i (978), i77—202
[22] Heckman, J. J. "Statistical Models for Discrete Longitudinal Data",

       (tniversity of Chicago, 1978).
                                                                    29




[23] Kalbfleisch, J. D. and Sprott, D. A. "Application of Likelihood

     Methods to Models Involving Large Numbers of Parameters", Journal of

     the Royal Statistical Society, Series B, 32 (1970), 175—208.

[24] Kiefer, J. and Wolfowitz, J. "Consistency of the Maximum Likelihood

     Estimator in the Presence of Infinitely Many Incidental Parameters",

     Annals of Mathematical Statistics, 27 (1956), 887—906.

[25] McFadden, D. "Conditional Logit Analysis of Qualitative Choice

     Behavior" in Zarembka, P (ed.), Frontiers in Econometrics (New York:

     Academic Press, 1974).

[261 Morgan, J. N. et al. A Panel Study of Income Dynamics (Ann Arbor:

     Institute for Social Pesearch, 1972).

[27] Mundlak, Y. "Empirical Production Function Free of Management Bias",

     Journal of Farm Economics, 43 (1961), 44—56.

[28] Mundlak, Y. "Estimation of Production and Behavioral Functions from

     a Combination of Cross—Section and Time—Series Data" in Christ, C. et

     al., Measurement in Economics (Stanford University Press, 1963).

[29] Mundlak, Y. "On the Pooling of Time Series and Cross Section Data",

     Econometrica, 46 (1978), 69—85.

[30] Nerlove, M. and Press, S. J. "Multivariate Log—Linear Probability

     Models for the Analysis of Qualitative Data", Center for Statistics

     and Probability Discussion Paper,No. 1 (Northwestern University, 1976).

[31] Neyman,   J. and   Scott, E. L. "Consistent Estimates Based on Partially

     Consistent Observations", Econoinetrica, 16 (1948), 1—32.

[32] Rasch, G. Probabilistic Models for SOineIntelligence and Attaient-Tests

     (Copenhagen: Danmarks Paedagogiske Institut, 1960).

[33] Rasch, G. "On General Laws and the Meaning of Measurement in Psychology",

     Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics

     and Probability, 4 (1961), 321—333.
