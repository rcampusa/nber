                         NBER WORKING PAPER SERIES

              FINDING A DUAL—FEASIBLE SOLUTION TO AN LP

             WITH m EQUALITIES IN (1 & in)   DUAL ITERATIONS



                          by Vinay Dharmadhikari



                           Working Paper No. 100
              National Bureau of Economic Research, Inc.
                        575 Technology Square
                    Cambridge, Massachusetts 02139


                               August 1975




                     Preliminary: Not for Quotation

NBER working papers are distributed informally and in limited number for
comments only. They should not be quoted without written permission.

This report has not undergone the review accorded official NBER publications
in particular, it has not yet been submitted for approval by the Board of
Directors.
                                        Abstract
                                                                                   .
        Lemke's dual—simplex method of linear programming is usually considered

inferior to the primal simplex method for any general linear programming

problems. One reason given is the difficulty of finding a starting dual—

feasible basis. In    this   paper, a new starting technique is presented, which

finds   a dual—feasible basis in a single dual—simplex pivot for LP's with no

equality constraints, and in (l+m3) pivots for LP's with m3 equality constraints

irrespective of the number of inequality constraints. The technique is illus-

trated on a small example problem. The performance, in terms of the number

of pivots to optimality, of the dual—simplex with the new starting technique

on 100 medium sized problems is reported and compared with that of the primal

simplex. Finally, how the dual—simplex with the new starting technique can

be efficiently implemented is briefly discussed.
                           Con t('11   ts

Section 1:    Introduction                                 i

Section 2:    The New Starting Technique                   2

Section 3:    Modifications of the Dual Simplex   .   .    3

Section 4:    Elimination of Equalities                    4

Section 5:    Comparison with Other Starting
              Techniques                                   5

Section 6:    Conclusion

Appendix A:    Illustrative Example                        9

References:                                               11
                                      Section 1: introduction


        Since 1954 when Lemke presented the dual—simplex method for solving linear

programs, the method has been considered no better, and perhaps worse, than the

primal simplex method. One of the reasons stated in the literature and held in

the folklore is the difficulty of finding an initial dual—feasible basis so that

the dual—simplex can be started. In this report we will present a new technique

to obtain a dual—feasible basis. We will also discuss the suitability of the

dual—simplex method combined with the new starting technique as a general purpose

linear programming method.

        Consider the general linear programming problem:

Problem I:           Maximize         cx
                                       —t   —t
                     Subject     to   Ax p b                                         Problem I
                                        t
                                            o
where     =
              (c1,   c2, .   .   . c) is the vector of objective coefficients, A =               I a..
is an m by n constraint matrix, x =                                 . x) is the vector of n variables,
                                                 (x1,   x2, .   .



p is a vector composed of relations <, > and =, and b =                                     b) is the
                                                                          (b1,   b2, .
vector of right—hand—sides. Let the first m1 constraints be <                      type,   the next
be >   type and the last m3 be = type, where m1 + m2 + m3 = m. The j—th column of
A is denoted by A,, the basis matrix by B and the r—th row of B' by '.

       When        < 0 for all j, and each constraint has a surplus variable, the all
              Cj

logical—basis is automatically dual—feasible. When only c. < 0 for all j, Dantzig

[2] has given a technique that involves the direct use of the dual and a basis of

size n. Lemke [6] suggested a technique that assumes that a set of m linearly

independent columns of A is known. The Lemke technique first solves an identical

problem with a different right—hand—side vector by revised simplex and uses the

optimal basis as the dual—feasible starting basis for the original problem. The

artificial constraint technique [1] [7] assumes knowledge of a basis XB possibly
                                                  —2—




in teas   lb Ic .   it.Involves pricing the onbastc variab 1 es to               (IC   Le cm tue the var [ab    cs
x,   jEJ with negative revised prices and the           variable x with                    the smallest revised

price. An artificial constraint                x + x0 =   M is added to the problem where M is
                                        jEJ
a sufficiently large number. Substituting x = M                —
                                                                   x0
                                                                        —
                                                                                      x.    in all other con—
                                          p                                     jEJ
                                                                                j p

straints and the objective function yields a problem for which the basis XB                                     (Bxp)
is dual—feasible. Wagner [8] has given a modified dual—simplex algorithm for
upper bounded variables which, he states, can also be used as a starting technique

by providing artificially large, non—restrictive upper bounds.


                             Section 2: The New Starting Technique
          The new technique does not require any special conditions nor the knowledge of

a set of m linearly independent columns of A. It starts by multiplying the m2 con-

straints of > type by —1. A logical variable is added to each of the m constraints

with a zero objective coefficient. The last m3 logical variables in the = type

constraints are regarded as artificial variables. Let c = MAX                                {c j = 1,        2, .   .   . n}.
An artificial, non—restrictive constraint x0 +                     x.   =
                                                                                b0
                                                                                      is added to the problem

where b0 is sufficiently large. The objective coefficient c0 of x0 is set equal to c

initially; it will be reset to zero after the first dual pivot. With these modifi-

cations, our problem becomes as follows where A. now denotes the j—th column of the

constraint matrix of the modified problem.


Problem II:           Maximize          rjn     c.x. + C X
                                         J=l JJ pO
                                        L.


                      Subject to
                                             j=ljx•     + x0            b
                                                                            0

                                                 x. + xn+i = b.,
                                                              i.
                                                                 i=                        1, 2
                                        :J=1    ij j

                                                        + xn+i. = —b.,                 =   m +1
                                       j=n a. x.
                                        j=l ijj                    i
                                                                                  i
                                                                                            1


                                               a.x. + x. = b,,                    i =
                                                                                           m1+m2+l,
                                                                                                      .   .   . m1+m+m3

                                                                   x   > 0, j =             0,   1             n+m
                                                                           —3—




We start with a basis consisting of all slack, surplus and artificial variables

so   that the basis matrix B equals identity matrix of size (rn-I-i) by (rn+i). The
row vector   of objective coefficients of the basic variables is                                              (c, 0, 0,   .   ., 0).
                                                                                                         CE
Since B =   I,   B1 =            I. The revised prices of the nonbasic columns are given by,
                     —
                             —l
            ii. =    c B A. — c•
             j        B        j      j
                 = c     A — C.
                      Bj                  •j
                 =   (c,          0, 0, .           . ., O)A.          —
                                                                           c.
                 =   c       —    c.
                      p               j
Therefore if. =      c       —    c     > 0 for j =            1,      2, .      . . n-I-rn.   Thus we have a dual feasible
             J        p               :1—
basis for problem II. We then apply the dual—simplex method to problem II, with

following important modifications.


                     Section 3: Modifications of the Dual—Simplex

      1. At the very first iteration, choose x0 as the leaving variable and x as

the entering variable. Since c0 =                              C,      C remains unchanged after the pivot and

the new basis remains dual—feasible. Now that the variable x0 is out of basis,

we change its objective coefficient to zero so that the objective function becomes

identical with that of the original problem I.

      2. At all subsequent iterations, choose as the leaving variable one of the

artificial variables               x+1, .                      . ., x,              if any artificial variable remains

in the basis. Once the leaving variable is selected, determine the variable to

enter the basis by the usual dual—simplex rules, except in the case where

Y = 8rA 0 for all j.                            Thus the rule can be stated as: choose Xk as the variable
to enter the basis a) if                              <   0    then k is given by
            Zk_Ck =          max           ZjCj                <
                                                         'rj
             'rk                               rj
                         =   mm            Zj C1
                                               rj
                                                     '    rj       <   o
                                                               _/4.—




b)           > 0
     if y rj —     for all j,     then k is given by1

            Zk_Ck =
                        mm
                                ZjCj           y rj.   >
             rk                  rj        ,




                    =   mm      _____ ' rj             >

                                 rj
      We can see that the above modifications to the usual dual—simplex procedure

preserve the dual—feasibility of the basis, and allow the m3 artificial variables

to   be driven out of the basis in no more than m3                          pivot operations. Once an artificial

variable   is out of the basis, it can be eliminated from the problem and never con-
sidered as a candidate for an entering variable. If there is a negative
variable Xr in the basis with rj =      > 0 for all j, then the original problem

is infeasible. If when all artificial variables are removed, x0 is not in the basis,

then the original problem is unbounded.


                             Section 4: Elimination of Equalities

       Since the number of pivots to be performed in this technique in obtaining a

dual feasible basis depends on the number m3 of equality constraints in the original

problem I, it is worthwhile to see if the number of equalities can be reduced.

Given an equality                      =
                                               bi      express one variable, say Xk, ak 0 in terms

of the other variables as x =
                          k
                               aix
                                               —
                                               —l
                                               aik
                                                  (
                                                       j k
                                                           .
                                                           j=n
                                                           J=l j
                                                                   x. +    b.).
                                                                            1
                                                                                  This expression can be sub—


stituted wherever            occurs in the other constraints and the                        objective function,
and the non—negativity of Xk can be preserved by replacing the i—th equality in
the original problem by the inequality —                               (          x + bi)   >   0. This can be
repeated
                                                                   ik4
           for all other equalities, thus eliminating all equality constraints one
                                                                                            —




by one. The above procedure can always be used to obtain an exactly equivalent
 problem with no equality constraints, and the new starting technique applied to this

1This was suggested by Professor Kennington
                            Table 1. Descrintjon of Test Problems


          Set No.         N           Ml    M2      M3           Range of ajj         Range   of
                                                                                                     Cj

                 1     340             1     3       3           [511, 735]           [42, 88]
                 2     243             7     5       7           [342, 636]           [29, 48]
                 3     368             2     7       8           [—553, —429)         [65, 119]
                 4      85             8     8       2           (110, 164]           (—39, 34]
                 5     366             1     2       5           (213, 763]           (—67, 55)
                 6     157             7     7       4           (72, 725]            [—64, 62]
                 7     115             1     5       1           [961, 969]           [29, 36]
                 8     382             2     6       8           [234, 363]           (01, 62]
                                                             *
              9        295             2     2       3           (711, 884)           [—95, 48)
             10        349             4     4       5           [104, 277]           [14, 15]




                              Table 2       Total NmIer of Primal Pivots                 and Dual
                                            Pivots Raquired to Obtain an Optimal
                                            Solution
Prob.                                                        Prob1mNumbe
Set          1        2           3           4              5         6          7              8          9       10
-.----   —                             —                 —             —      —                       —              ,i
 1       28,17       26,9       23,17       33,18        35,8        39,17      24,15         26,23       32,15    26,20
 2       92,52       83,43      80,50       76,34        99,58       83,67      97,38     100,32          81,40     73,49
 3       53,46       68,42      59,41       59,52        79,43       66,40      63,26         68,59       55,42    60,52
 4       46,21       61,15      63,17       50,17        53,21       44,13      68,2.7        55,15       75,20     70,17
 5       38,20       37,16      21,15       29,12        37,16       29,15      29,20         26,18       37,21     36,14
 6       82,25       81,14      89,23       70,26        62,29       85,26      75,36         54,26       76,25     82,27
 7       20,4        21,5       24,6        23,4         25,5        27,4       22,5          22,11       23,5      265
 8       .03,31      87,26      72,46       80,29        70,40        70,27   100,37          77,35       77,46     71,52
 9       21,15       18,9       28,7        18,9         21,9        20,7       25,9          27,9        25,10     25,9
10       94,46       84,54      79,61       74,51        89,68       69,56      63,44         86,53       69,51     79,53




The f:st         entry in     each   pair   is the total number            of primal     pivots and       the secoad entry

is the total number of dual pivots.
                                           —5—



equivalent problem will yield a dual feasible basis in exactly one pivot operation.

A significant advantage of this equality—elimination procedure is that, unlike the         S
variable substitution in the artificial—constraint starting technique [7], our

procedure can be carried out a priori ——    in   the problem generation phase and hence

it need entail little additional effort.


              Section 5: Comparison With Other Starting Techniues

1. Whereas the other starting techniques require presence of special conditions

    (e.g. Dantzig's) or advance knowledge of a set of in linearly independent

    columns of A (e.g. Lemke's), the new technique is completely general.

2. Unlike any other technique, the new technique guarantees a dual—feasible

    starting basis in (l+m3) dual—simplex pivots. And, if the equality—elimination

   procedure is used, a starting basis can be obtained in just one pivot operation.

3. The tethniques such as Charnes' Big N, or Phase I — Phase II, for obtaining a

   primal—feasible starting basis involve introduction of artificial variables

   not only in the m3 equality constraints but also in the m2 >      type   constraints.

   Moreover, since in these primal methods, there is no freedom of choosing the

    leaving variable, the number of pivot operations required to obtain a starting

   basis can be, and usually is, quite large. The new technique for obtaining a

    a dual—feasible basis, on the other hand, does not need any artificial vari-

    ables in the >   type   constraints.

4. Compared with other starting techniques, the new technique is probably the

    simplest to code and involves very few changes from the usual dual—simplex

    method.


                               Section 6 Conclusion

    We presented a new technique for obtaining a dual—feasible basis for a general

linear programming problem. We have shown how the number of pivots required to
                                                   —6—




     obtain the dual—feasible basis can be bounded from above by (l+m3) where m3 is

     the number of strict equality constraints in the original linear program. We

     have given an equality—elimination procedure that can very easily be used to

     further reduce the number of pivots. Finally we have shown that the new technique

     compares favorably with other starting techniques. It is felt, in the light of

     the above that the dual—simplex method of Leinke needs to be reexamined for its

     suitability as a general purpose optimization method for solving linear programming

    problems. In addition to the very efficient starting technique for the dual—simplex,

    there is some evidence of the superiority of the dual—simplex over primal—simplex

    in two empirical investigations. The first by Hasegawa [4] was done in 1965 as

    a part of a master's thesis. Twenty—five problems with thirty constraints and

    seventy variables were randomly generated with five percent dense constraint ma-

    trices. A modified Lemke technique was used to obtain a starting dual—feasible

    basis      for the dual—simplex, and the two—phase technique to obtain a starting dual—
'   feasible basis for the primal method, revised—simplex. For each problem solved,

    the numbers of iterations of the dual—simplex were uniformly smaller than those

    for the revised—simplex. In fact, in many cases, the former required less than

    half as many iterations as the latter.

           The second computational study by Kennington [5] was carried out primarily

    to confirm or contradict Hasegawa's results. Instead of the modified Lemke

    starting technique as used by Hasegawa, the new starting technique presented in

    this paper was used to get a starting basis for the dual—simplex, and the Charnes'

    big—N method was used to get a starting basis for the primal—simplex. One hundred

    problems with upto twenty nine constraints and 420 variables were randomly generated

    with completely dense. constraint matrices. The sizes of the problems, the ranges

    of         and       and other statistics for the problems are given in Table 1. The
         ajj         c
    number of Iterations taken by both methods to reach
p                                                       optimal solution are given in

    Table 2. Here again we see that the dual—simplex combined with the new starting
                                                 —7—




technique took uniformly less iterations than did the primal simplex.

     Thus we now have evidence from two independent investigators that for medium—

scale linear programming problems of either low or high sparsity, the dual—simplex

method worked uniformly better in terms of the number of iterations. Incidentally,

the starting technique used by Hasegawa seems to take more iterations than those

for the technique presented here.

     It is felt, however, that the evidence presented above warrants a reevaluation

of the dual—simplex with the new starting technique, as a linear programming method

which may compete with other methods such as the revised—simplex, the primal—

dual, the composite—simplex [1]. Of course, the iteration count by itself is not

the most meaningful measure of a method's performance; the computational effort

per Iteration also needs to be taken into consideration. At the first glance a

dual—simplex iteration seems to involve more effort because in addition to evalua-

tion of all revised prices 7T
                                      CBBAj
                                                 —             for   all nonbasic ,       it requires evalua—
tion of yj = A for all nonbasic L where                              is the r—th row of B        .   This

situation, however, can be improved. As Kennington [5] has pointed out, the

need be computed only for those j for which rj < 0.                       Moreover, the r—th row of

B    can be conveniently obtained by using the column—access, packed—matrix and the

use of the product—form—of—inverse comprising of k elementary matrices at the k—th

Iteration since the last reinversion. If B =
                                                                  EkEki    .   .   . E,   then

      erEkEkl .   .   . E1,   where er =   (0,   0,    .   .    . 1, 0 .   .   . 0)   with a 1 in the

r—th position. The multiplication of packed eta—vectors representing the elementary

matrices     with a sparse row vector can be performed quite economically using

clever schemes such as that of Winograd'S [9]. When a nonbasic column k has

            < 0                       -          0, we have dual—degeneracy. In this case,
rk                and k       BB'Ak        Ck

one need not compute           for all i because the variable Xk can be chosen as the

entering variable while preserving dual—feasibility since the                             will remain un—
                                        —8--




 altered after the iteration. Thus the presence of dual—degeneracy actually can

 reduce the computation per iteration in determining the entering variable corres-

 ponding to a given leaving variable. It may also be pointed out that the dual—

 simplex method can use most of the efficient and storage saving techniques of pro—

 gramming used in the revised primal—simplex; in fact, many subroutines in a produc-

 tion LP code are directly transferrable, such as matrix packing, the product—form—

of—inverse, reinversLon, BTRAN and YRAN operations, and so on. Thus we see that

the dual—simplex, with the new starting technique not only shows great promise

in terms of efficiency, but the change over from a primal LP code can be quite

simple and inexpensive.

     Many questions concerning the uniformly better iteration count performance

of the dual—simplex as evidenced in the computational studies to date, yet remain

to be theoretically as well as empirically investigated for root causes and ex-

planations. But then there are many questions in the field of linear programming

that are not fully answered yet; namely, for example, why most linear programs

are solved in an extremely small fraction of the total number of iterations it

could theoretically take for any Simplex procedure?   Specifically, however, the

questions of better iteration count performance, of estimating the actual computa-

tional effort per iteration, and of whether there are any special structures such

as network problems, covering problems, decomposition problems that are especially

amenable to the dual—simplex will be investigated and the findings will be reported

in the future.
                                                            —9—




Problem I:

Maximize
                                    Appendix A Illustrative Example



                              3x1 + 6x2 + 8x3
                                                                                                                 .
Subject to

                              4x1 + 8x2 — 2x3 > 8

                              2x1 + 4x2 + 4x3 < 12

                               x1 — 2x2 + 4x3 = 0

                                               x. > 0 for all j

We add the new constraint x0 +                        + x2 + x3 = 100.       We evaluate the largest objective

coefficient c
                  p
             c max             {3, 6, 8)
                          J

                      c         8, p =   3.
             c0
Thus the modified problem is,

Problem III:

Maximize                      8x0 + 3x1 + 6x2 + 8x3

                                                                           = 100
                               xo+ x1+ x2 + x3
                                           -
                                               8x2 + 2x3    +x4
                                    2x1
                                           + 4x2 +
                                                   4x3
                                                                           =12
                                           - 2x2 +                ÷        = 0
                                                   4x3                x6

                                                         0 for all J
                                                 Xj




                                                                                                                 .
                                                                    —10—




 1'abjeau 1                              8       3        6        8
                                   CB

                                b
              CB        XB              A0       A1       A2    A3     A4       A5   A6
               8     x0 100 1                    1        1

               0 x4—8                          —4        —8     2      1

               0 x512                            2        4     4               1

               0
                    x6         0                1        —2     4                    1
                               ir       0       5         2


                                                                +
Tableau2                       CB
                                        0        3        6     8

               8             100 1              1         1     1

              0     x4 —208 —2                 —6 —10                  1

              0 x5—388—4                       —2                           1
        ÷     0     x6 —400 —4                 --3       —6                          1
                               71       0       5        2




Tableau 3

              8 x3 100/3 1/3 1/2                               1

                   x4 1376/3 14/3 —1                                   1

                             —388 —4             —2                         1
                   x5
              6 x2 400/6 2/3 1/2                          1                          —1/6

                              ir        20/3         4                                2/6

Tableau 3 gives the dual—feasible starting solution to problem III, since the

basis contains no artificial variables
                                                               corresponding to the original equality
constraints.
                                    REFERENCES



1.   Coopcr, L. and Steinberg, D. I. Methods and Applications of Linear Programming,
                                                                                       S
       W. B. Saunders and Co.., Philadelphia, Pa., 1974.

2. Dantzig, G. B., Ford, L. R., and Fulkerson, D. R. "A primal—dual algorithm for
      linear programming," Linear Inequalities and Related Systems, Kuhn and Tucker,
      eds., Princeton Univ. Press, Princeton, N. J., 1956.

3. Hadley, G. Linear Programming, Addison—Wesley Publishing Co., Reading, Mass.,
       1962.

4. Hasegawa, H. "A study of the dual simplex algorithm," M.S. Thesis, Washington
      Univ., St. Louis, Mo., 1965.

5. Kennington, Jeff L. "Computational Experience Using the Dual Simplex with
      Dharmadhikari's Starting Technique," Technical Report CP 74014, Southern
      Methodist University, Dallas, Texas, 1974.

6. Lemke, C. E. "The dual method of solving the linear programming problems,"
      Naval Research Logistics Quarterly, 1, 48—54, 1954.

7. Simmonard, H. Linear Programming, prentice—Hall, Englewood Cliffs, N. J., 1966.

8. Wagner, H. M. "The dual simplex algorithm for bounded variables," Naval
      Research Logistics Quarterly, 5, 257—261, 1958.

9. Winograd, S., "A new algorithm for inner—product," IEEE Transactions,
      Computers, 17, 693—694, 1968.




                                                                                       .
