                              NBER WORKING PAPER SERIES




                   THE CAUSES OF PEER EFFECTS IN PRODUCTION:
                  EVIDENCE FROM A SERIES OF FIELD EXPERIMENTS

                                        John J. Horton
                                     Richard J. Zeckhauser

                                      Working Paper 22386
                              http://www.nber.org/papers/w22386


                     NATIONAL BUREAU OF ECONOMIC RESEARCH
                              1050 Massachusetts Avenue
                                Cambridge, MA 02138
                                     July 2016




We thank Iwan Barankay, Raj Chetty, Lydia Chilton, Nicholas Christakis, Malcolm Wiley-Floyd,
Larry Katz, Renata Lemos, and Rob Miller for helpful comments and suggestions. We thank
Robin Horton, David Yerkes, and Heidi Yerkes for their help in preparing the manuscript. Horton
would like to thank the NSF-IGERT Multidisciplinary Program in Inequality and Social Policy
(GrantNo. 0333403), the University of Notre Dame, and the John Templeton Foundation’s
Science of Generosity Initiative for their generous financial support. We were greatly aided by
the excellent research assistance of Alex Breinen, John Comeau, Talia Goldman, Michelle
Lindner, and Justin Keenan. Author contact information, datasets, and code are currently, or will
be, available at http://www.john-joseph-horton.com/. The views expressed herein are those of the
authors and do not necessarily reflect the views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

© 2016 by John J. Horton and Richard J. Zeckhauser. All rights reserved. Short sections of text,
not to exceed two paragraphs, may be quoted without explicit permission provided that full
credit, including © notice, is given to the source.
The Causes of Peer Effects in Production: Evidence from a Series of Field Experiments
John J. Horton and Richard J. Zeckhauser
NBER Working Paper No. 22386
July 2016
JEL No. J01,J24,J3

                                           ABSTRACT

Workers respond to the output choices of their peers. What explains this well documented
phenomenon of peer effects? Do workers value equity, fear punishment from equity-minded
peers, or does output from peers teach them about employers’ expectations? We test these
alternative explanations in a series of field experiments. We find clear evidence of peer effects, as
have others. Workers raise their own output when exposed to high-output peers. They also punish
low-output peers, even when that low output has no effect on them. They may be embracing and
enforcing the employer’s expectations. (Exposure to employer-provided work samples
influences output much the same as exposure to peer-provided work.) However, even
when employer expectations are clearly stated, workers increase output beyond those
expectations when exposed to workers producing above expectations. Overall, the evidence is
strongly consistent with the notion that peer effects are mediated by workers’ sense of fairness
related to relative effort.


John J. Horton
Leonard N. Stern School of Business
Kaufman Management Center
44 West Fourth Street, 8-81
New York, NY 10012
john.horton@stern.nyu.edu

Richard J. Zeckhauser
John F. Kennedy School of Government
Harvard University
79 John F. Kennedy Street
Cambridge, MA 02138
and NBER
richard_zeckhauser@harvard.edu
                     The Causes of Peer Effects in Production:
                    Evidence from a Series of Field Experiments
                                 John J. Horton∗                  Richard J. Zeckhauser
                                   NYU Stern                       Harvard University

                                                      June 24, 2016



                                                          Abstract

       Workers respond to the output choices of their peers. What explains this well documented phe-
       nomenon of peer effects? Do workers value equity, fear punishment from equity-minded peers, or
       does output from peers teach them about employers’ expectations? We test these alternative expla-
       nations in a series of field experiments. We find clear evidence of peer effects, as have others. Workers
       raise their own output when exposed to high-output peers. They also punish low-output peers, even
       when that low output has no effect on them. They may be embracing and enforcing the employer’s
       expectations. (Exposure to employer-provided work samples influences output much the same as
       exposure to peer-provided work.) However, even when employer expectations are clearly stated,
       workers increase output beyond those expectations when exposed to workers producing above ex-
       pectations. Overall, the evidence is strongly consistent with the notion that peer effects are mediated
       by workers’ sense of fairness related to relative effort.
       JEL J01, J24, J3
       Keywords: peer effects, productivity, effort, field experiments



1 Introduction

A growing empirical literature documents that workers respond to the output choices of their fellow

workers. Chan et al. (2014), Bandiera et al. (2010), Mas and Moretti (2009), and Falk and Ichino (2006) all

demonstrate that co-workers can exert economically significant effects on their peers, via channels not

explicitly created by their firms. Even a firm that does not explicitly create the channels may welcome

their existence. Kandel and Lazear (1992) argue that peer pressure—or more precisely, peer punishment
  ∗ We thank Iwan Barankay, Raj Chetty, Lydia Chilton, Nicholas Christakis, Malcolm Wiley-Floyd, Larry Katz, Renata Lemos,

and Rob Miller for helpful comments and suggestions. We thank Robin Horton, David Yerkes, and Heidi Yerkes for their help
in preparing the manuscript. Horton would like to thank the NSF-IGERT Multidisciplinary Program in Inequality and Social
Policy (Grant No. 0333403), the University of Notre Dame, and the John Templeton Foundation’s Science of Generosity Initiative
for their generous financial support. We were greatly aided by the excellent research assistance of Alex Breinen, John Comeau,
Talia Goldman, Michelle Lindner, and Justin Keenan. Author contact information, datasets, and code are currently, or will be,
available at http://www.john-joseph-horton.com/.




                                                              1
for low productivity or low effort—ameliorates a firm’s agency problem that workers may slack. An ex-

ample of this potentially useful form of peer effects (from the firm’s perspective) comes from Mas and

Moretti (2009), who found that grocery checkout clerks were more productive when observed by highly

productive fellow clerks. Mas and Moretti suggest that this higher productivity might arise from the im-

plied threat of punishment. This seems plausible, particularly since, in their setting, there was a direct

free-riding externality: slacking off by one clerk would increase the workload imposed on other clerks.

It is unclear whether this result would extend to settings with no direct free-riding externality present.

Absent such an externality, workers might altruistically punish low-output peers if the actions of those

peers are perceived as unfair (Gachter and Fehr, 2000; Fehr and Gächter, 2000). In light of this possibility,

workers might try to match the output of their peers—or they may simply have an aversion to producing

low output relative to others.

    Social learning about the employer’s expectations for employees is an alternative explanation to the

equity-based viewed of peer effects. New employees could use experienced employees as a readily ac-

cessible and cheaply available source of information. These veteran peers, given their tenure on the job,

have usually learned how to satisfy their employer’s expectations. Hence, their current behavior presum-

ably represents a “revealed preference” example of effective worker behavior, unlike the statements made

by employers, which might be strategically motivated. This kind of social learning has proven important

in other production settings. For example, Pierce and Snyder (2008) document how vehicle emission in-

spectors quickly adopt the prevailing ethical norms of those with whom they work. In other production

settings, social learning has explained why different workers eventually make similar choices (Conley

and Udry, 2010), as they observe what “works” and then do the same.1 Nanda and Sørensen (2010) show

that having a colleague with entrepreneurial experiences increases the probability that that worker will
   1 There is a literature on another kind of social learning in production settings that focuses on knowledge spill-overs—see

Azoulay et al. (2010) and Waldinger (2012). This would be unlikely to be important in the kinds of low-skilled tasks with ob-
servable output that have been the focus of much of the empirical peer effects literature. Output observability seems to be a
necessary but not sufficient condition for peer effects in production; Guryan et al. (2009) exploited the random assignment of
professional golfers to tournament foursomes, finding that peers did not matter in a golf tournament. However, this was also a
setting where extra effort was unlikely to be costly and free-riding externalities were absent. This is quite consistent with Cor-
nelissen et al. (2013) and their finding compensating wages for peer effects only appear in settings where higher productivity is
costly. Further distinguishing the Guryan et al. (2009) setting from the rest of the literature, social learning about expectations
would be unlikely to be important, as professional golfers know precisely what constitutes good performance (though there
could have been learning about how to play a particular hole or situation).




                                                                2
also turn to entrepreneurship.2

    This paper reports the results of a series of field experiments conducted in an online labor mar-

ket whose purpose was to test the equity and the social learning explanations for peer effects in pro-

duction settings. In each experiment, we operated as employers looking to have a series of images

labeled—a common task in the marketplace. This experimenter-as-employer paradigm is a commonly

used approach for creating contextualized field experiments in real world settings—for other examples

see Gilchrist et al. (2016) and Pallais (2014). Each experiment required workers to decide how much costly

output to produce. In some experiments, workers were asked to evaluate the output of their peers. This

setup allowed us to both (a) measure peer-punishment and (b) expose workers to the output choices of

their peers. To detect peer effects, in some experiments we asked workers post-evaluation to complete

another task, for which we could also measure the output.

    Our first experiment established three important findings: workers were uncertain about the “firm’s”

expectations, they found output costly, and they responded to signals about the firm’s expectations (as

conveyed by employer-provided work samples). The experiment established that there was a need for

learning about employer expectations. The second experiment introduced a channel for peer effects

by having workers perform a task and then evaluate the output of a peer. The experiment showed that

workers readily punished low-output/low-effort peers, despite the absence of any direct free-riding ex-

ternalities.

    The third experiment added another component to the second experiment: evaluators had to per-

form a follow-on labeling task. Exposure to high-output peers increased workers’ output on the follow-

on task. Although this experiment demonstrated that peer effects can be easily generated, it could not

distinguish between social learning about employer expectations and equity-based explanations. That

is, workers exposed to high output peers could have revised their beliefs about what the employer ex-

pected or revised their beliefs about what is fair (or what their peers would likely regard as fair), or both.

    Our fourth experiment was designed to distinguish between social learning about employer expec-

tations and equity-based peer effects. In this experiment, the firm’s exact expectations were clearly

communicated, therefore removing uncertainty about what the firm expected. This communication
  2 The notion that peers could have useful information seems well supported empirically. For example, Iyer et al. (2015) shows

that peer evaluations of would-be lenders performs better than screening with purely a credit score.



                                                              3
of standards was effective, in that on the first task, nearly all workers complied with the output level

we requested. After this initial task, some evaluators evaluated work that complied with the firm’s ex-

pectations, while others evaluated work that exceeded the firm’s specified output expectations. We find

that not only did evaluators not punish high-effort workers (but technically speaking, non-complying

workers), but also they imitated them, increasing output in the subsequent task beyond what the firm

requested.

   An implication of the fourth experiment—and the punishment results more generally—is that work-

ers do not simply assess output by whether it perfectly matches with the employer’s stated expectations.

It is likely that the standard applied by the evaluating workers was not the employer’s standard per se, but

rather a judgment of the effort made by each evaluated worker relative to his/her own work. Consistent

with this interpretation, across experiments, punishment of peers increased strongly with the evaluator’s

own output.

   This paper’s primary contribution is to establish that equity-considerations are essential to explain-

ing peer effects in production, at least in the context considered. It is the first paper we are aware of that

can distinguish between social learning and equity-based explanations for peer effects. Further, it also

demonstrates that peers will punish even when there are no direct free-riding externalities. These find-

ings speak directly to the micro-foundations of peer effects in production, which in turn matter for our

conceptions of team production, and ultimately the role of the firm. This paper’s secondary contribution

is to add to the large literature documenting the importance and strength of peer effects. We found peer

effects strong enough to “override” the firm’s clearly communicated preferences.

   A limitation of our study is that it cannot distinguish between equity-based explanations that de-

pend on a worker’s preference against relatively low output and a worker’s preference against not being

perceived as having relatively low output. That we explicitly had workers evaluate each other may have

planted the seed that being perceived as having low output by a fellow worker could be harmful.

   As with any experimental study, our results have strong internal validity. They also have some of the

inherent external validity conferred by being conducted in the field, in a real production setting where

subjects were unaware of the experimental nature of their work. Although the production setting is atyp-

ical, it offers some advantages with respect to generalizability. Given the short duration of the “relation-



                                                      4
ships” created online and that fact that “peers” never interacted in person and are completely anony-

mous with respect to both the employer and each other, strong peer effects seemed a priori unlikely, yet

this proved not the case. What seems to matter for peer effects in productivity is the observability of out-

put rather than the social interaction. Also, as each worker was working for us as an employer for the first

time, we expected that social learning about our expectations would be particularly important, yet this

too was not the case. In settings where interactions are longer lasting, any social learning would likely

diminish in importance over time, leaving only equity considerations.

    Aside from our scientific interest in the workings of peer effects, our results speak to the efforts by

organizations to harness peer effects.3 While tantalizing, effectively exploiting peer effects has often

proven challenging in practice.4 The greatest successes of exploiting peer effects have not come from

production settings, but rather on product uptake—see Aral and Walker (2014), Aral and Walker (2011)

and Bapna and Umyarov (2015). We find that peer effects are strong and arise readily, but we also show

that they could in some cases be too strong; our final experiment showed that the effects of peers was

strong enough to counteract our clearly communicated output requests. Although this may seem like a

free lunch—higher output at the same pay—we also observed in the first experiment that workers sorted

themselves away from our tasks when they perceived our output standards as being excessive, making

it ambiguous whether actual firms would welcome the peer effects we created in the last experiment.

Consistent with the view that firms would have to compensate workers for higher and thus more costly

productivity, Cornelissen et al. (2013) find peer effects in wages, but only for relatively low-skilled occu-

pations. One interpretation of this finding is that those low-skilled occupations have both observable

output (a prerequisite for peer effects in output) and a more direct relationship between productivity

and the disutility of effort.

    The paper is organized as follows: Section 2 describes the empirical setting for our experiments. Sec-

tion 3 presents the experimental designs and describes the experimental tasks and the features common
   3 Harnessing such effects can be of significant value when contracts between a firm and its workers are incomplete, most

importantly when workers do not know the employer’s expectations. Basically, the firm is capitalizing on a behavioral phe-
nomenon. See Koszegi (2014) for an overview of the literature on incorporating insights from behavioral economics into con-
tract theory models.
   4 For example, Carrell et al. (2013) offers a cautionary tale about trying to engineer Air Force Academy peer groupings to

improve academic performance. The effort strongly backfired for unforeseen reasons relating to the micro-foundations for
peer effects in education.




                                                             5
across experiments. Section 4 presents the results, and Section 5 concludes.



2 Empirical setting

The experiments were conducted on Amazon’s Mechanical Turk (MTurk), an online labor market. MTurk

is one of several online labor markets that have emerged in recent years (Frei, 2009). Researchers in a

number of disciplines have begun using these markets for experimentation.5 Some examples in eco-

nomics include Mason and Watts (2009), Barankay (2010), Chandler and Kapelner (2013), and Horton

and Chilton (2010). Online experiments offer significant advantages relative to most laboratory exper-

iments in cost, speed of accruing subjects, and representativeness of the subject pool. However, they

tend to be harder to control than conventional laboratory experiments. Despite this difficulty, Horton et

al. (2011) explain how valid causal inferences can be drawn in online labor markets such as MTurk and

oDesk.

    Tasks posted on MTurk are called Human Intelligence Tasks (HITs). HITs vary, but most are small,

simple tasks that are difficult or impossible for machines but easy for people, such as transcribing au-

dio clips, classifying and tagging images, reviewing documents, and checking websites for pornographic

content. The originators of HITs are called “requesters.” Requesters and workers are anonymous to each

other. Rarely do they interact on a repeat basis. The requester constructs the user interface for the HIT

and sets the conditions for payment, worker qualifications, and timing (for example, how long a worker

can work on a task).

    To become eligible, a would-be worker must have a bank account and must have created a profile

with Amazon.6 Workers can only have one account, and Amazon uses several technical and legal means

to enforce this restriction. Workers can readily see the collection of HITs available to them and, in most

cases, view a sample of the required work. They can work on any task for which they are qualified. Once

they accept a HIT, they can begin work immediately.

    Once a worker completes a HIT, the work product is submitted to the requester for review. Requesters
  5 For an overview of online labor markets, see Horton (2010).
  6 MTurk workers are often called “Turkers.” Their ranks appear to be split approximately evenly between the US and India.

Horton (2011) finds workers generally view online employers as having the same level of trustworthiness as offline, conven-
tional employers. For the demographics of the MTurk population, see Ipeirotis (2010).




                                                              6
may “reject” work, in which case the worker is not paid. This ability of requesters to reject work creates

adverse consequences for providing work that falls short of employer expectations. Requesters may also

pay bonuses, allowing tailored payments based on individual performance within a nominally piece-rate

HIT.



3 Experimental design

Our first experiment, E XP-B ASELINE, tested two questions: 1) Do workers view the task as costly?, and

2) Does providing output samples effectively convey employer expectations, as revealed by changes in

output following observation of those samples? The experiment E XP-P UNISH tested a third question: 3)

Will workers punish low effort/low productivity relative to some standard? This test was conducted by

exposing workers to either a high- or low-output peer, and then asking the evaluating worker (a) whether

we, as the employer, should “approve,” that is, accept and pay for the work, and (b) how the evaluator

wanted to split a bonus with the evaluated worker. The third experiment, E XP-P EERS, tested: 4) Does

a worker’s exposure to the output choice of a peer—shown via an evaluation—influence that worker’s

subsequent output? In the fourth experiment, E XP-E XPLICIT, we attempted to remove any uncertainty

about the firm’s expectations in order to test: 5) Will workers thus informed punish work that exceeds

the employer’s stated expectations, and therefore deviates from them? In other words, will evaluators

punish workers for not conforming to what was expected? A worker might be understandably reluctant

to punish high-effort work even if it failed to comply on a technicality, especially if they think the em-

ployer has free disposal on “excess” output. The worker might believe as well that they are making the

same decision the employer would have made in the same context. However, the interesting result of

this experiment is whether exposure to this high-effort but non-complying work affects the evaluator’s

subsequent output. If it does, then it suggests a channel by which peer pressure can sustain behaviors or

levels of output that deviate from what the firm stated that it wants.


3.1 Preliminaries and common elements across experiments

In every experiment, before agreeing to participate, would-be workers were told about the task and the

payment for it, and were shown a completed work sample. The work sample was a “screenshot” of the


                                                     7
                      Figure 1: Pre-made Amazon image labeling or “tagging” interface




         Notes: Screenshot of the image-tagging interface developed by Amazon.


image-labeling interface as it would look after a worker completed the task.7


3.1.1 Task and interface

Computers have a difficult time recognizing objects in images, yet this task is often valuable for firms.

Thus, image labeling is a common “human computation” task found on MTurk (von Ahn and Dabbish,

2004; Huang et al., 2010). It is one of a handful of canonical MTurk tasks for which Amazon has created

pre-made templates. Figure 1 depicts one of Amazon’s pre-made interfaces.

    To label images, workers in our experiment used an interface we developed, shown in Figure 2. To

add a label, workers clicked a button labeled “Add a label” positioned below the image.8 Clicking the

button brought forth a new blank text field to be added to the survey. Workers could add as many labels

as they wanted. When they were finished, they clicked a button labeled “Submit labels.”
   7 Because subjects were not informed that they were participating in experiments, the experiments were “natural” field ex-

periments in the Harrison and List (2004) taxonomy.
   8 The images themselves were selected from the photo-sharing site Flickr. The images each had a Creative Commons license

and were chosen because they were conducive to labeling (for example, photos depicting elaborate meals with many easily
recognizable different foods).




                                                             8
3.1.2 Peer evaluations

In all but E XP-B ASELINE, a worker played two roles: the producer of labels of images and the evaluator of

the work of a peer who had also engaged in the image-labeling task. To evaluate the peer, the evaluating

worker viewed that labeling worker’s assigned image and that worker’s generated labels. The evaluating

worker then made recommendations on two matters: whether to approve (pay for) or not approve (not

pay for) the task, and how to split a 9-cent bonus between him- or herself and the evaluated worker.

   All evaluating workers within an experimental group assessed the same worker’s output. This eval-

uated worker was chosen at random from subjects who had participated in previous experiments and

whose work had exhibited the desired property for that experimental group (either high or low effort).

The workers performing either task – labeling images or evaluating performance – likely regarded the

evaluation and bonus schemes as unremarkable. It is a very common quality-control technique to have

workers evaluate the work of other Turkers. Furthermore, bonuses are frequently used to incentivize

good performance.

   For the accept/reject question, the evaluating workers were asked:


      Should we approve this work?


They had to answer “yes” or “no.” For the peer evaluation, the evaluators were told:


      We want to determine how good this work is. We would like you to decide, based on your

      work and the quality of the other work, how to split a 9-cent bonus.


The evaluating workers selected an answer from a list of 9 options of the form “X cents for the other, 9− X

cents for me,” with X ranging from 0 to 9 (9 cents was chosen as the endowment to reduce the salience

of the focal point 50-50 split). Both questions were asked on the same survey page, and the evaluators

could answer them in either order, though the approval question appeared first on the page. At the end

of the experiment, we implemented all choices, with the recommended bonuses paid to the evaluated

workers and their evaluators, in accordance with the evaluator’s preferences.




                                                     9
3.1.3 Demographic survey and allocation

In each experiment, subjects answered a short demographic survey before beginning work. Subjects

were asked their sex, nationality, and whether they were doing this work primarily to earn money, learn

new skills, or have fun. Demographics differed slightly across experiments, probably due to differences

in the times the experiments were launched. Although one might conjecture that the survey would raise

suspicions that the task was an experiment, we view this as unlikely. Asking workers for basic demo-

graphic information is fairly common in the market.9 In each experiment, subjects were assigned alter-

nately to groups in order of arrival time (for instance, Subject 1 was assigned to treatment, Subject 2 to

control, Subject 3 to treatment, and so on) to give better balance.


3.2 E XP-B ASELINE: Baseline experiment

Participants in this experiment were assigned to either H IGH or L OW. In H IGH , the employer-provided

work sample showed 9 labels, compared to only 2 labels in L OW. Figure 2 shows the two work samples.

After viewing their particular work sample, workers chose to participate and label an image or to exit.

Those who chose to participate then performed a labeling task.


    Table 1 reports the means of various demographic measures collected for the E XP-B ASELINE partici-

pants. While the set of covariates is limited, there is no indication that the randomization was ineffective.

The same analysis was conducted for the other experiments reported in the paper, but the results mir-

ror those from the first experiment. Although not reported, the full dataset and this auxiliary analysis is

available online.

    Table 1: Comparison of covariate pre-treatment means for exerimental groups in E XP-B ASELINE

                                 Variable                            HIGH group          LOW group          t-stat
                            From India                                    0.54                0.43          1.13
                               Male                                       0.61                0.55          0.54
            Reports that primary motivation is money                      0.78                0.74          0.43
                           From the US                                    0.35                0.40          -0.56
 Notes: This table reports the means for the H IGH and L OW experimental groups in E XP-B ASELINE. The t-statistic for a differ-
 ence in means is reported.


  9 The IRB approval for these experiments did not require notification that the work was part of a research project.



                                                               10
   Figure 2: Work samples shown to workers prior to task acceptance in E XP-B ASELINE




              (a) H IGH work sample                              (b) L OW work sample

Notes: The panels show the work sample given to would-be workers considering accepting the image-
labeling task. Subjects assigned to H IGH were shown the left image with its 9 labels; subjects assigned to
L OW were shown the right image with its 2 labels.




                                                    11
                             Figure 3: Work evaluated by subjects in E XP-P UNISH




                   (a) G OOD work sample                        (b) B AD work sample

        Notes: The image in the left panel is the work output evaluated by workers assigned to G OOD. The image in
        the right panel is the work output evaluated by workers assigned to B AD.


3.3 E XP-P UNISH: Punishment experiment

The job posting for this experiment was the same as in E XP-B ASELINE with one addition: potential sub-

jects were told that they would be evaluating the work of another worker. Before accepting the task, all

subjects were shown the H IGH work sample displayed in Figure 2. To reward the additional evaluation

work, the participation payment was raised from 30 cents to 40 cents. The requested sample size was

also increased from 100 to 200.10

    In this experiment, all subjects first completed the same image-labeling task before they were ran-

domized into the two groups. Subjects assigned to G OOD inspected the output of a worker (selected

from E XP-B ASELINE) who had produced 12 unique labels. Subjects assigned to B AD inspected the out-

put of a worker who had produced only one unique label. The output samples of the evaluated workers

are shown in Figure 3.
  10 We determined our sample size by examining what others had done—e.g., Falk and Ichino (2006) had 24 subjects—and

increasing it substantially (by a factor of 4), as is possible in online settings.




                                                           12
3.4 E XP-P EERS: Peer experiment

E XP-P EERS added one element to E XP-P UNISH : we asked workers to do a second labeling task after

completing their evaluation task. The two experimental groups are G OOD and B AD. In G OOD, subjects

evaluated a worker who produced 11 labels; in B AD, subjects evaluated a worker who produced only 2

labels. The requested sample size was 300, and each subject’s payment was 40 cents.


3.5 E XP-E XPLICIT: Explicit experiment

E XP-E XPLICIT re-ran E XP-P EERS with one exception: we provided explicit employer instructions to sub-

jects before exposing those workers to the output of peers. The workers were told that they should pro-

duce 2 labels per image. The requirement of 2 labels was stated before workers began the task, and was

presented again with each of the two image-labeling tasks, directly above the image and in the instruc-

tions. The left panel of Figure 4 shows initial instructions, while the right panel shows the instructions

placed above both the first and the subsequent labeling tasks. In the left panel, instructions explain that

the worker is to provide two labels for an image, rate the work of another worker and then provide two

labels for an additional image. The right panel shows the interface for the subsequent labeling task. Note

that it reiterates the requirement that the worker provide two labels.

   After performing the initial task, workers were randomized into one of two groups: M EET, in which

subjects evaluated a work sample showing x = 2, and O VER, in which subjects evaluated a work sample

showing x = 11. After evaluating the work, subjects performed an additional image-labeling task. The

requested sample size was 300, and the payment was 40 cents.



4 Results

4.1 E XP-B ASELINE: Is the labeling task perceived by the workers as costly, and are the con-

     veyed employer expectations salient?

The first experiment shows that having a worker observe employer-provided work samples affects la-

bor supply on the extensive margin. Showing a high-work sample in H IGH without requiring any peer

evaluations effectively conveys the firm’s expectations.


                                                    13
                                       Table 2: Overview of the experiments


                                                      E XP-B ASELINE
                                    (Groups = H IGH & L OW, Payment = 30¢, N = 93)
                       Description                                                            Results
Subjects, according to experimental group, viewed one of two     H IGH increased labor supply on intensive margin, but de-
employer-provided work samples, then produced whatever           creased it on the extensive margin.
number of labels they chose (if any). Work samples differed
by experimental group. In H IGH, subjects first viewed a high-
output work sample (many labels). In L OW, subjects first
viewed a low-output work sample (few labels).
                                                       E XP-P UNISH
                                   (Groups = G OOD & B AD, Payment = 40¢, N = 167)
                       Description                                                            Results
Subjects viewed an employer-provided high-output work            G OOD subjects decreased punishment.               More productive
sample, then produced whatever number labeles they chose.        workers punished more.
Subjects then evaluated another worker’s work product. Sub-
jects in G OOD evaluated a high-effort work sample. Subjects
in B AD evaluated a low-effort work sample.
                                                        E XP-P EERS
                                   (Groups = G OOD & B AD, Payment = 40¢, N = 275)
                       Description                                                            Results
Subjects viewed an employer-provided work sample, then           G OOD raised output on the second task. Effects were stronger
chose how many labels to produce. Subjects then evaluated        for subjects with high effort in the first task.
another worker’s work product, then labeled a second image.
Subjects in G OOD evaluated a high-effort work sample. Sub-
jects in B AD evaluated a low-effort work sample.
                                                       E XP-E XPLICIT
                                   (Groups = M EET & O VER, Payment = 40¢, N = 272)
                       Description                                                            Results
Subjects viewed an employer-provided work sample with 2 la-      O VER caused many workers to not comply, even when they
bels. Subjects were told that 2 and only 2 labels should be      complied on the first task. Workers did not punish high-effort
produced. Subjects then labeled an image, evaluated another      but non-complying work.
worker’s work product, then labeled a second image, with the
same instruction to create only 2 labels. In O VER, subjects
evaluated a worker producing too many labels. In M EET, sub-
jects evaluated a worker producing the required number of
labels.




                                                            14
                                       Figure 4: E XP-E XPLICIT communication of employer standards




                                       (a) Initial E XP-E XPLICIT instructions             (b) E XP-E XPLICIT instructions repeated

       Notes: This screenshot shows the labeling interface in E XP-E XPLICIT. The left panel shows the initial in-
       structions which explain that the worker is to provide 2 labels for an image, rate the work of another worker
       and then provide 2 labels for an additional image. The right panel shows the interface for the subsequent
       labeling task. Note that it reiterates the requirement that the worker provide 2 labels.



                        Figure 5: Distribution of labels produced by experimental group in E XP-B ASELINE
                                                                                    HIGH
                              16
                                                          Mean group output = 4.6
                              12
         Number of subjects




                              8
                              4


                                                                                    LOW
                              16
                                                 Mean group output = 2.6
                              12
                              8
                              4

                                   0                       5                         10             15                    20
                                                                   Number of labels produced
       Notes: This figure plots the count of experimental subjects producing the number of labels listed on the x-
       axis. For example, 14 subjects in the H IGH experimental group produced 0 labels. Subjects in H IGH were
       shown a work sample consisting of 9 labels prior to performing, while subjects in L OW were shown a work
       sample with only 2 labels. Group mean output is shown, with zeros included.



   The main results from E XP-B ASELINE are displayed in Figure 5. The number of labels produced is on

the x-axis, while the number of subjects producing that many labels is on the y-axis. The top facet shows

the output for subjects in the H IGH group, while the bottom facet shows output for subjects in L OW. It

shows that 5 subjects in H IGH produced more than 12 labels, but only 1 subject in L OW produced more


                                                                              15
than 12 labels.

   It also reveals that many more subjects in H IGH produced no output at all, namely 14 verus 5. Pre-

sumably, they were discouraged by the expectations implied in the image they were shown. In Table 3,

we confirm what Figure 5 suggests: In Column (1), we regress whether the worker produced any labels at

all on the treatment indicator, while in Column (2), we regress the count of labels on that same indicator.

   H IGH employer expectations reduced labor supply on the extensive margin, but increased it on the

intensive margin. However, the latter effect outweighed the former: even with the non-participants in-

cluded as x = 0 observations, subjects in H IGH produced roughly 2 more labels per person, on average

(4.6 versus 2.6). Because this unconditional output rose significantly in H IGH, we know that selection

does not explain the observed increase in output.


Table 3: Effects of perceptions of employer’s output expectations on extensive and intensive labor supply
in E XP-B ASELINE

                                                                        Dependent variable:
                                                                  Any output?          Output
                                                                       (1)                (2)
                      Assigned to H IGH                               −0.177∗             1.970∗
                                                                       (0.084)           (0.939)

                      Intercept                                         0.872∗∗∗          2.638∗∗∗
                                                                       (0.059)           (0.660)


                      Observations                                       93               93
                      R2                                               0.046             0.046
                      Adjusted R2                                      0.036             0.036
                      Residual Std. Error (df = 91)                    0.406             4.528
                      F Statistic (df = 1; 91)                        4.411∗            4.402∗
                  Notes: This table reports the results of two robust OLS regressions where the depen-
                  dent variables are measures of worker labor supply. In this experiment, all subjects were
                  invited to participate in a paid image-labeling task. Those subjects assigned to H IGH
                  viewed an employer-provided work sample with 9 labels, while subjects in L OW viewed
                  a sample with only 2 labels. In Column (1), the outcome variable is labor supply on the
                  extensive margin, that is, whether the worker accepted the task and generated any labels
                  at all. In Column (2), the outcome variable is labor supply on the intensive margin, that
                  is, the number of labels the worker produced, with 0s included. Significance indicators:
                  p ≤ 0.05 : ∗, p ≤ 0.01 : ∗∗ and p ≤ .001 : ∗ ∗ ∗.




                                                                 16
    This experiment has important implications for our study. These findings indicate that we chose a

task: (a) that workers found personally costly to perform, in a setting where (b) the employer’s expecta-

tions were easily conveyable, and (c) that workers factored those expectations into their decision making.


4.2 E XP-P UNISH: Do workers punish low effort/productivity?

In E XP-P UNISH, workers were randomly assigned to evaluate either good work (in G OOD) or bad work (in

B AD). We wanted to test whether evaluators would still punish even in the absence of direct free-riding

externalities.

    The main results from E XP-P UNISH can be seen in Figure 6. The figure contains four histograms;

each shows the count of evaluators choosing different allocations of the 9-cent bonus between them-

selves and their evaluated worker. The plots are defined by experimental group (column) and evaluator

recommendation regarding approval (row). Evaluators in G OOD were very unlikely to recommend re-

jection, whereas evaluators assigned to B AD were fairly likely to do so. For the B AD/reject evaluators,

the modal transfer was 0 cents. Apart from the evaluators in B AD who recommended rejection, few

evaluators in either group transferred 0 cents.

    Most G OOD evaluators, as well as a large number of evaluators who recommended approval despite

being in B AD, chose a more or less even split that gave 4 or 5 cents to the worker.


Figure 6: Distribution of reward and punishment by workers of their peers based upon perceived ef-
fort/quality in E XP-P UNISH
                                               Recomend Approval                               Recomend Approval
                                                 Evaluated BAD                                  Evaluated GOOD
                             30
                             20                     Mean bonus = 4.6                                     Mean bonus = 5.1
                             10
         Count of subjects




                             0
                                               Recomend Rejection                              Recomend Rejection               Treatment
                                                                                                                                 Evaluated BAD
                                                 Evaluated BAD                                  Evaluated GOOD                   Evaluated GOOD

                             30
                             20           Mean bonus = 2.4                                    Mean bonus = 3.0
                             10
                             0
                                  0   1    2    3    4   5    6     7   8   9    0    1   2      3   4     5   6    7   8   9
                                                                  Bonus Given to Other Player

        Notes: This figure shows the bonus split and the accept/reject recommendation by treatment group. Sub-
        jects in B AD evaluated work with 1 generic label, while subjects in G OOD evaluated work with 12 specific
        and appropriate labels. Group mean bonus size is shown with a vertical line.


                                                                                     17
   Table 4 confirms the results the graphical analysis portrays. The dependent variable in Columns (1)

and (2) indicates whether the evaluating worker recommended that we pay the evaluated worker. The

independent variable in Column (1) is the treatment indicator. Evaluators assigned to G OOD were far

more likely to recommend acceptance than those assigned to B AD. The baseline was 50% in B AD and

increased to above 90% in G OOD.

   The independent variable in Column (2) is the number of labels the evaluating workers produced

themselves, prior to the evaluation. As is evident, workers who produced more labels themselves were

more likely to recommend rejection. The dependent variable in Columns (3) and (4) is the bonus size.

As with the accept/reject measure, we can see that G OOD increased transfers in the contextualized dic-

tator game. Moreover, these transfers were decreasing in the evaluator’s own output. For both measures

of punishment, higher-output workers were harsher critics; they were both more likely to recommend

rejection and to award smaller bonuses.


4.3 E XP-P EERS: Do peers influence output?

As in E XP-P UNISH, subjects in G OOD in E XP-P EERS were more likely to recommend acceptance and

transfer larger bonuses on average than were subjects in B AD. This was expected, and we do not present

the analysis. Rather, we focus on the evaluating worker’s subsequent output. Recall that, in the E XP-

P EERS experiment, after the initial output task and evaluation task, workers performed a second labeling

task. As we show, workers assigned to G OOD produced more output than those in B AD in their second

production task.

   Figure 7 plots the workers’ subsequent output, x 2 , versus their initial output, x 1 . We fit separate lines

for the two treatment groups, with 95% confidence intervals for the conditional mean illustrated with

shaded regions. The regression line for G OOD lies everywhere above the line for B AD, and it is steeper.

The regression analysis in Table 5 confirms this graphical analysis. The outcome measure in each col-

umn is the number of labels each evaluating worker produced in the second labeling task. Column (1)

shows that assignment to G OOD increased the mean number of labels by more than 2, from a baseline

of just 5. Column (2) adds the worker’s pre-treatment output as a regressor. As would be expected, past

performance predicts future performance. Column (3) adds an interaction term between the number of



                                                      18
Table 4: Reward and punishment by workers of their peers based upon perceived effort/output in E XP-
P UNISH

                                                                         Dependent variable:
                                               Recommend Approval?                              Bonus Size Awarded
                                                (1)                     (2)                    (3)                     (4)
                                                       ∗∗∗                                            ∗∗∗
Assigned to GOOD                                0.418                                           1.442
                                               (0.063)                                         (0.390)

Initial Output, x 1                                                   −0.055∗∗∗                                      −0.259∗∗∗
                                                                       (0.013)                                        (0.074)

Intercept                                       0.500∗∗∗                0.960∗∗∗                3.488∗∗∗               5.376∗∗∗
                                               (0.045)                 (0.065)                 (0.278)                (0.383)


Observations                                   167                      167                   167                    167
R2                                            0.213                    0.105                 0.077                  0.069
Adjusted R2                                   0.208                    0.100                 0.071                  0.064
Residual Std. Error (df = 165)                0.404                    0.431                 2.518                  2.528
F Statistic (df = 1; 165)                   44.616∗∗∗                19.361∗∗∗             13.682∗∗∗              12.269∗∗∗
Notes: This table reports the results of robust OLS regressions of two measures of peer punishment in E XP-P UNISH. In G OOD,
workers evaluated the work output of a worker generating 12 specific and highly appropriate labels; the other workers in B AD
evaluated a worker producing only 1 generic label. In Columns (1) and (2), the outcome measure is whether the worker recom-
mended that the employer “approve” the work of the evaluated worker and thus pay them. In Columns (3) and (4), the outcome
measure is the amount of bonus transferred to that evaluated worker, out of an endowment of 9 cents. The key independent
variable in Columns (1) and (3) is the treatment indicator, G OOD, whereas in Columns (2) and (4), the key independent variable
is the evaluating worker’s output. The regressions in (2) and (4) are not causal, but they illustrate the strong negative relation-
ship between own-output and the tendency to punish. Significance indicators: p ≤ 0.05 : ∗, p ≤ 0.01 : ∗∗ and p ≤ .001 : ∗ ∗ ∗.




                                                                19
Figure 7: Subsequent number of labels, x 2 , versus initial number of labels, x 1 , by whether subject evalu-
ated G OOD or B AD in E XP-P EERS
          x2 : Labels produced after evaluation                                          GROUP
                                                                                             ●
                                                                                                 BAD
                                                  20                                             GOOD


                                                                                                                                                                                                           ●


                                                  15
                                                                                                                                                                                                   ●                   ●    ●


                                                                                                                                                                                                           ●

                                                                                                                                               ●       ●
                                                                                                                                                                                                       ●

                                                  10           ● ●
                                                                   ●                               ●                                           ●                    ●●                                         ●

                                                                                                                                                           ●
                                                                                                                         ●                             ●                              ●                            ●
                                                                                         ●                       ●            ●        ●                                         ●●
                                                                                                                                                   ●   ●                     ●                 ●
                                                                                                                 ●●                        ●           ●
                                                                        ●   ●                                ●   ● ● ●        ● ●                                   ●
                                                                                                                                                                    ●
                                                                                     ●             ●●
                                                                                                   ●         ●                ●●                   ● ●●
                                                                                                                                                      ●                  ●
                                                                                                                                                                         ●
                                                           ●                                         ●       ●                 ●                   ●●               ●            ●


                                                  5            ●
                                                                                     ●
                                                                                         ●               ●       ●●●●●
                                                                                                                          ●
                                                                                                                              ● ●●
                                                                                                                                   ●                                    ●

                                                                                ●                        ●       ●
                                                                                                                 ●        ●   ●                                     ●
                                                                                                   ●●●
                                                                                                   ●                                               ●
                                                                                                   ● ●
                                                                                                     ●●              ●                                                       ●
                                                                                     ●

                                                               ●                     ●                                   ●●

                                                           ●●
                                                           ●●●● ●
                                                                ●●
                                                                 ● ●                ●●                           ●
                                                            ●●●
                                                              ●●
                                                              ● ●
                                                                ●
                                                                ●●●         ●        ●                           ●                     ●


                                                  0        ●           ●
                                                                        ●                           ●




                                                       0                                                             5                                                                    10                           15
                                                                                                                         x1 : Labels produced before evaluation

        Notes: In this plot the y-axis is subsequent output, x 2 , and the x-axis is initial output, x 1 , in E XP-P EERS, by
        experimental group. All subjects performed an identical initial task and chose some number of labels to
        provide (shown on the x-axis). Each subject then evaluated another subject’s work that demonstrated either
        (B AD) low productivity or (G OOD) high productivity. All output levels are randomly perturbed by a small
        amount to prevent over-plotting.


number of labels, x 1 , and the treatment indicator. The positive and significant effect of the interaction

term implies that the initially high-output workers had the greatest subsequent output response to their

exposure.


4.4 E XP-E XPLICIT: How do peer effects change when employer requirements are explicitly

      identified? Do evaluators punish high effort if it deviates from employer expectations?

The results from E XP-P EERS show that workers are influenced by peers and not simply by employer-

provided sources of information. However, these results do not establish the source of these peer effects.

Are they driven by social learning about the firm’s standards or by social learning about peer standards?11

E XP-E XPLICIT was designed to distinguish between these two potential sources of social learning. In

E XP-E XPLICIT, workers first did a task in which they were given explicit instructions to produce only two

labels. They then evaluated a peer who either precisely met the employer’s expectation (M EET) or who

produced more than two labels (O VER). After this evaluation, the evaluating worker performed another

labeling task. For this subsequent labeling task, our key question was whether having explicit employer

instructions “stamped out” peer effects. Note that this experiment offers a strong test, since the employer
 11 Note that in defining what the firm expects for output, we use the terms “expectation,” “requirement” and “standard” inter-

changeably.


                                                                                                                                                               20
             Table 5: Effects of evaluating a co-worker on subsequent output in E XP-P EERS

                                                            Dependent variable:
                                             Number of labels produced after evaluation, x 2
                                       (1)                             (2)                             (3)
Assigned to GOOD                       2.169∗∗∗                        2.290∗∗∗                        0.971
                                      (0.488)                         (0.370)                         (0.699)

Initial Output, x 1                                                    0.884∗∗∗                        0.764∗∗∗
                                                                      (0.063)                         (0.083)

GOOD ×x 1                                                                                              0.280∗
                                                                                                      (0.126)

Intercept                              5.043∗∗∗                        0.815∗                          1.388∗∗
                                      (0.338)                         (0.395)                         (0.470)


Observations                          265                           265                             265
R2                                   0.070                         0.469                           0.479
Adjusted R2                          0.066                         0.465                           0.473
Residual Std. Error             3.970 (df = 263)              3.006 (df = 262)                2.984 (df = 261)
F Statistic                  19.744∗∗∗ (df = 1; 263)      115.600∗∗∗ (df = 2; 262)         79.858∗∗∗ (df = 3; 261)
Notes: This table reports robust OLS regressions from E XP-P EERS. Workers were randomly assigned to evaluate either a
worker exhibiting (G OOD) high productivity/effort or (B AD) low productivity/effort. In each column, the output measure
is the number of labels a worker produced after evaluating another worker, x 2 . Significance indicators: p ≤ 0.05 : ∗,
p ≤ 0.01 : ∗∗ and p ≤ .001 : ∗ ∗ ∗.




                                                          21
imposes a ceiling, as well as a floor, on production. The explicit employer instructions are designed to let

the worker know the firm’s precise expectation, potentially sealing the social learning channel for peer

effects.

Figure 8: Numbers of workers in different output bands for the subsequent task in E XP-E XPLICIT, faceted
by initial output, with experimental groups indicated by line type




           Notes: For initially complying subjects (middle panel), assignment to O VER increased the relative number
           of subjects producing additional (and hence non-complying) output in the second task.



    Figure 8 illustrates the main results of this experiment but needs some explanation. The plots show

subjects moving from different pre-treatment output “bins” to post-treatment output bins. The three

side-by-side panels correspond to the three output levels that workers chose for the initial task: x 1 = 1,

x 1 = 2, and x 1 ≥ 3 (labeled 3+ in the figure). Thus, the left panel presents results only for those subjects

that generated x 1 = 1 label; the center panel presents results only for those subjects that initially com-

plied by producing x 1 = 2 labels; and so on. Within each panel, these same bands are used again for

the x-axis, for the count of labels produced in the subsequent task; that is, x 2 = 1, x 2 = 2, and x 2 ≥ 3.

The y-axis indicates the count of subjects in that (x 1 , x 2 ) bin. The two experimental groups are shown

separately by differences in line type: M EET is shown with a dashed line; O VER is shown with a solid

line.

    Several interesting results emerge from Figure 8. First, the center panel received most of the ob-

servations, indicating that the majority of workers initially complied with employer requirements and

produced exactly 2 labels. In the left and right panels, there are no differences between the experimental

groups; the lines nearly overlap. The center panel shows results that are quite different. For these sub-

jects who complied initially, exposure to O VER reduced compliance in the subsequent task. Specifically,


                                                              22
the O VER treatment increased output among those initially complying. (The dotted line lies below the

solid line at x 2 ≥ 3.)

    Table 6 confirms our graphical analysis. However, unlike in Figure 8, it restricts attention solely to

those subjects who originally complied, x 1 = 2. In this restricted sample, the chance that worker error or

misunderstanding drove the results is reduced. Subjects in M EET had a 90% compliance rate, whereas

subjects in O VER had only a 70% compliance rate. Note that we can restrict our sample in this way

because the x 1 choice was made pre-treatment. In Columns (1) and (2), the outcome variable is an

indicator for compliance with the employer instructions, 1{x 2 = 2}, in the subsequent task. In Column

(3), the outcome variable is an indicator for choosing 1{x 2 ≥ 3}.

    We can see in Column (1) that assignment to M EET increased compliance in the subsequent task.

In Column (3), we can see where the noncompliers went; they almost universally increased their output

to x 2 ≥ 3. Might these results be driven by a misunderstanding of the instructions? The regression in

Column (2) helps us answer that question. In Column (2), we interact the treatment indicator with an

indicator for whether the subject was from India, where English is often not the primary language. Al-

though it is not necessarily the case that subjects from India have worse English skills than those from

the US, in other online labor markets, employers at least act is if they expect workers from less developed

countries to be more likely to have communication difficulties (Agrawal et al., 2013). In our regressions,

we see no country-specific effect, which suggests that exposure to the high-output peer did not cause

the subject to simply second-guess the employer’s written instructions.



5 Conclusion

The findings from these experiments contribute to the literature on peer effects in several ways. Their

primary contribution is to highlight the importance of equity considerations in explaining peer effects.

This finding of importance was not ex ante obvious, as our first experiment showed that workers are

sensitive to the perceived standards of employers. Thus, any peer effects could actually be social learning

about the employer’s expectations. However, subsequent experiments showed that workers evaluate and

altruistically punish other workers on the basis of perceived effort rather than on the basis of simple

adherence to the employer’s expectations. Furthermore, the results showed that workers were strongly


                                                     23
Table 6: Effects of exposure to high-effort, non-complying peer work after explicit employer instructions
about compliance

                                                                      Dependent variable:
                                                              Comply                                     Over-Produce
                                              1{x 2 = 2}                     1{x 2 = 2}                    1{x 2 > 2}
                                                 (1)                             (2)                            (3)
                                                        ∗∗∗                             ∗∗
Assigned to M EET                                 0.230                          0.247                        −0.218∗∗∗
                                                 (0.059)                        (0.077)                        (0.050)

India                                                                            0.046
                                                                                (0.086)

Complied (x 1 = 2) × India                                                     −0.045
                                                                                (0.122)

Constant                                          0.705∗∗∗                       0.688∗∗∗                       0.231∗∗∗
                                                 (0.042)                        (0.054)                        (0.035)


Observations                                  155                             155                            155
R2                                           0.089                           0.091                          0.110
Adjusted R2                                  0.083                           0.073                          0.104
Residual Std. Error                     0.370 (df = 153)                0.372 (df = 151)               0.311 (df = 153)
F Statistic                          15.002∗∗∗ (df = 1; 153)          5.038∗∗ (df = 3; 151)         18.956∗∗∗ (df = 1; 153)
  Notes: This table reports robust OLS regressions from E XP-E XPLICIT. In this experiment, subjects were assigned to eval-
  uate either (M EET) a work sample exactly meeting the employer’s expectations or (O VER) a non-complying (but high-
  effort) sample. In Columns (1) and (2), the outcome variable indicates whether the worker complied with the employer’s
  stated output requirement of exactly two labels, 1{x 2 = 2}. In Column (3), the outcome is whether the worker exceeded
  the employer’s stated output requirement of 2 labels. Significance indicators: p ≤ 0.05 : ∗, p ≤ 0.01 : ∗∗ and p ≤ .001 : ∗∗∗.




                                                               24
influenced by high-effort peers, even when they deviated from the employer’s instructions.

   A secondary contribution of our findings is to offer yet another example of peer effects in a real pro-

duction setting. Our results do not merely show that peer effects exist. They show that these effects

are strong and arise in a setting when there is no personal interaction between peers. Across the ex-

periments, variation in the exposure to different work samples explained a substantial fraction of the

variation in observed output. Moreover, this finding arose despite an experimental setting that offered

very short “interactions” that were ostensibly one-shot. In more traditional settings with longer and

more consequential channels for peer interactions, it seems likely that the incentives landscape created

by peers would substantially affect individual performance.

   A natural question for managers is whether they should encourage peers to influence each other,

such as by optimally arranging teams to maximize productivity. Context surely matters greatly. In set-

tings where effort and productivity are tightly coupled and workers can easily monitor each other, peer

pressure would seem to provide a kind of free lunch for the firm. However, recall from our first experi-

ment that workers who infer that the firm had high standards were more likely to exit and complete no

labels, so perhaps “cheap lunch” is a more appropriate term, in that our per-output costs went down

substantially because of the productivity boost from peer effects.

   In some contexts, giving workers the ability to punish or reward their peers may hurt, not help. Work-

ers might enforce inefficient standards. For example, in noise-filled environments, where the connection

between effort and productivity is tenuous and where explicit firm-provided incentives are highly muted,

the firm might be worried that otherwise-good workers would feel compelled to feign industry to placate

effort-monitoring peers who punish and/or reward. When such dangers loom, the firm might even want

to go so far as to prevent workers from monitoring each other. In short, enabling peer pressures when

peers may be punishing or rewarding undesired outcomes is risky.

   One unexplored organizational implication of these findings is the possibility of a feedback loop or

cascade. The potentially causal dependence between one’s own productivity and the willingness to pun-

ish, combined with susceptibility to peer effects, provides one such mechanism. Cascades can be harm-

ful, for example, if after workers observe idiosyncratically bad work, they lower their own output and

punish less, which in turn reduces other workers’ incentives to be highly productive. On the beneficial



                                                   25
side, employers will seek to harness peer effects when there is strong potential for a constructive cascade,

when idiosyncratically productive work spurs superior output from those monitoring, who then induce

superior output from others. The potential for hard effort to spread from one worker to another helps to

explain why organizational leaders often use the language of contagion to describe morale, and why so

much of management theory focuses on understanding and influencing organizational culture.



References

Agrawal, Ajay K, Nicola Lacetera, and Elizabeth Lyons, “Does information help or hinder job applicants

  from less developed countries in online markets?,” 2013.


Aral, Sinan and Dylan Walker, “Creating Social Contagion Through Viral Product Design: A Randomized

  Trial of Peer Influence in Networks,” Management Science, 2011, 57 (9), 1623–1639.


   and    , “Tie Strength, Embeddedness, and Social Influence: A Large-Scale Networked Experiment,”

  Management Science, 2014, 60 (6), 1352–1370.


Azoulay, Pierre, Joshua Graff Zivin, and Jialan Wang, “Superstar Extinction,” The Quarterly Journal of

  Economics, 2010, 125 (2), 549–589.


Bandiera, Oriana, Iwan Barankay, and Imran Rasul, “Social incentives in the workplace,” Review of

  Economic Studies, 2010, 77 (2), 417–458.


Bapna, Ravi and Akhmed Umyarov, “Do Your Online Friends Make You Pay? A Randomized Field Ex-

  periment on Peer Influence in Online Social Networks,” Management Science, 2015, 61 (8), 1902–1920.


Barankay, Iwan, “Rankings and Social Tournaments: Evidence from a Field Experiment,” Working Paper,

  2010.


Carrell, Scott E, Bruce I Sacerdote, and James E West, “From natural variation to optimal policy? The

  importance of endogenous peer group formation,” Econometrica, 2013, 81 (3), 855–882.


Chan, Tat Y., Jia Li, and Lamar Pierce, “Compensation and Peer Effects in Competing Sales Teams,”

  Management Science, 2014, 60 (8), 1965–1984.

                                                    26
Chandler, Dana and Adam Kapelner, “Breaking Monotony with Meaning: Motivation in Crowdsourcing

  Markets,” Journal of Economic Behavior & Organization, 2013, 90, 123–133.


Conley, Timothy G and Christopher R Udry, “Learning about a new technology: Pineapple in Ghana,”

  The American Economic Review, 2010, pp. 35–69.


Cornelissen, Thomas, Christian Dustmann, and Uta Schönberg, “Peer Effects in the Workplace,” 2013.


Falk, Armin and Andrea Ichino, “Clean evidence on peer effects,” Journal of Labor Economics, 2006, 24

  (1).


Fehr, Ernst and Simon Gächter, “Fairness and retaliation: The economics of reciprocity,” The journal of

  economic perspectives, 2000, pp. 159–181.


Frei, Brent, “Paid Crowdsourcing: Current State & Progress toward Mainstream Business Use,” Produced

  by Smartsheet.com, 2009.


Gachter, Simon and Ernst Fehr, “Cooperation and Punishment in Public Goods Experiments,” American

  Economic Review, 2000, 90 (4), 980–994.


Gilchrist, Duncan S, Michael Luca, and Deepak Malhotra, “When 3+ 1> 4: Gift structure and reciprocity

  in the field,” Management Science, 2016.


Guryan, Jonathan, Kory Kroft, and Matthew J. Notowidigdo, “Peer effects in the workplace: evidence

  from random groupings in professional golf tournaments,” American Economic Journal: Applied Eco-

  nomics, 2009, 1 (4), 34–68.


Harrison, G.W. and John A. List, “Field experiments,” Journal of Economic Literature, 2004, 42 (4), 1009–

  1055.


Horton, John, “Online Labor Markets,” Internet and Network Economics, 2010, pp. 515–522.


Horton, John J., “The condition of the Turking class: Are online employers fair and honest?,” Economics

  Letters, 2011, 111 (1), 10–12.




                                                   27
   and Lydia B. Chilton, “The labor economics of paid crowdsourcing,” Proceedings of the 11th ACM

  Conference on Electronic Commerce, 2010.


  , David G. Rand, and Richard J. Zeckhauser, “The online laboratory: Conducting experiments in a

  real labor market,” Experimental Economics, 2011, 14 (3), 399–425.


Huang, E., H. Zhang, D.C. Parkes, K.Z. Gajos, and Y. Chen, “Toward automatic task design: A progress

  report,” in “Proceedings of the ACM SIGKDD Workshop on Human Computation (HCOMP)” 2010.


Ipeirotis, Panagiotis G., “Demographics of Mechanical Turk,” Working Paper, 2010.


Iyer, Rajkamal, Asim Ijaz Khwaja, Erzo F. P. Luttmer, and Kelly Shue, “Screening Peers Softly: Inferring

  the Quality of Small Borrowers,” Management Science, 2015, 0 (0), null.


Kandel, Eugene and Edward P. Lazear, “Peer pressure and partnerships,” Journal of Political Economy,

  1992, pp. 801–817.


Koszegi, Botond, “Behavioral contract theory,” Journal of Economic Literature, December 2014, 52 (4),

  1075–1118.


Mas, Alexander and Enrico Moretti, “Peers at work,” American Economic Review, 2009, 99 (1), 112–145.


Mason, Winter and Duncan J. Watts, “Financial incentives and the ‘performance of crowds’,” in “Proc.

  ACM SIGKDD Workshop on Human Computation (HCOMP)” 2009.


Nanda, Ramana and Jesper B. Sørensen, “Workplace Peers and Entrepreneurship,” Management Sci-

  ence, 2010, 56 (7), 1116–1126.


Pallais, Amanda, “Inefficient hiring in entry-level labor markets,” The American Economic Review, 2014,

  104 (11), 3565–3599.


Pierce, Lamar and Jason Snyder, “Ethical Spillovers in Firms: Evidence from Vehicle Emissions Testing,”

  Management Science, 2008, 54 (11), 1891–1903.


von Ahn, L. and L. Dabbish, “Labeling images with a computer game,” in “Proceedings of the ACM

  SIGCHI conference on Human factors in computing systems” 2004, pp. 319–326.

                                                  28
Waldinger, Fabian, “Peer effects in science: Evidence from the dismissal of scientists in Nazi Germany,”

  The Review of Economic Studies, 2012, 79 (2), 838–861.




                                                  29
