                              NBER WORKING PAPER SERIES




          WHY YOU SHOULD NEVER USE THE HODRICK-PRESCOTT FILTER

                                       James D. Hamilton

                                      Working Paper 23429
                              http://www.nber.org/papers/w23429


                     NATIONAL BUREAU OF ECONOMIC RESEARCH
                              1050 Massachusetts Avenue
                                Cambridge, MA 02138
                                     May 2017




I thank Daniel Leff for outstanding research assistance on this project and Frank Diebold, Robert
King, James Morley, and anonymous referees for helpful comments on an earlier draft of this
paper. The views expressed herein are those of the author and do not necessarily reflect the views
of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

© 2017 by James D. Hamilton. All rights reserved. Short sections of text, not to exceed two
paragraphs, may be quoted without explicit permission provided that full credit, including ©
notice, is given to the source.
Why You Should Never Use the Hodrick-Prescott Filter
James D. Hamilton
NBER Working Paper No. 23429
May 2017
JEL No. C22,E32,E47

                                           ABSTRACT

Here's why. (1) The HP filter produces series with spurious dynamic relations that have no basis
in the underlying data-generating process. (2) Filtered values at the end of the sample are very
different from those in the middle, and are also characterized by spurious dynamics. (3) A
statistical formalization of the problem typically produces values for the smoothing parameter
vastly at odds with common practice, e.g., a value for λ far below 1600 for quarterly data. (4)
There's a better alternative. A regression of the variable at date t+h on the four most recent values
as of date t offers a robust approach to detrending that achieves all the objectives sought by users
of the HP filter with none of its drawbacks.


James D. Hamilton
Department of Economics, 0508
University of California, San Diego
9500 Gilman Drive
La Jolla, CA 92093-0508
and NBER
jhamilton@ucsd.edu




A data appendix is available at http://www.nber.org/data-appendix/w23429
1         Introduction.

Often economic researchers have a theory that is specified in terms of a stationary environment,

and wish to relate the theory to observed nonstationary data without modeling the nonstationar-

ity. Hodrick and Prescott (1981, 1997) proposed a very popular method for doing this, commonly

interpreted as decomposing an observed variable into trend and cycle. Although drawbacks to

their approach have been known for some time, the method continues today to be very widely

adopted in academic research, policy studies, and analysis by private-sector economists.                For

this reason it seems useful to collect and expand on those earlier concerns here and note that

there is a better way to solve this problem.




2         Characterizations of the Hodrick-Prescott filter.

Given T observations on a variable yt , Hodrick and Prescott (1981, 1997) proposed interpreting

the trend component gt as a very smooth series that does not differ too much from the observed

y t .1     It is calculated as

                                   nP                                                            o
                                      T             2
                                                           PT                                  2
                         min          t=1 (yt − gt ) + λ    t=1 [(gt − gt−1 ) − (gt−1 − gt−2 )]    .    (1)
                       {gt }T
                            t=−1



When the smoothness penalty λ → 0, gt would just be the series yt itself, whereas when λ → ∞

the procedure amounts to a regression on a linear time trend (that is, produces a series whose

second difference is exactly 0). The common practice is to use a value of λ = 1600 for quarterly

time series.
    1
         Phillips and Jin (2015) reviewed the rich prior history of generalizations of this approach.



                                                                1
       A closed-form expression for the resulting series can be written in vector notation by defining

T̃ = T + 2,      y      = (yT , yT −1 , ..., y1 )0 , g              = (gT , gT −1 , ..., g−1 )0 and
               (T ×1)                                 (T̃ ×1)

                                                                      "                          #
                                                      H =                     IT         0
                                                 (T ×T̃ )                   (T ×T )    (T ×2)

                                                                                                           
                                                1 −2                 1          0 ···       0     0 0
                                                                                                     
                                                                                                     
                                           
                                                0        1          −2 1             ··· 0     0 0  
                                                                                                     
                                    Q =
                                                ..       ..           ..        ..          .. .. .. 
                                                 .        .            .         .   ··· .      . . .
                                  (T ×T̃ )                                                           
                                                                                                     
                                                 0        0           0          0    · · · −2 1 0 
                                                                                                     
                                           
                                                                                                     
                                                                                                     
                                                 0        0           0          0    · · · 1 −2 1

The solution to (1) is then given by2


                                          g ∗ = (H 0 H + λQ0 Q)−1 H 0 y = A∗ y.                                                       (2)


The inferred trend gt∗ for any date t is thus a linear function of the full set of observations on y

for all dates.

       As noted by Hodrick and Prescott (1981) and King and Rebelo (1993), the identical inference

can alternatively be motivated from particular assumptions about the time-series behavior of the

trend and cycle components. Suppose our goal was to choose a value for a (T × 1) vector at

such that the estimate g̃t = a0t y has minimum expected squared difference from the true trend:


                                                         minE(gt − a0t y)2 .                                                          (3)
                                                               at


   2
     The appendix provides a derivation of equations (2) and (4).                                    Cornea-Madeira (forthcoming) provided
further details on A∗ and a convenient algorithm for calculating it.




                                                                             2
The solution to this problem is the population analog to a sample regression coefficient, and is

a function of the variance of y and its covariance with g:

                                                          −1
                                   g̃ = E(gy 0 ) [E(yy 0 )]    y = Ãy.                           (4)


As an example of a particular set of assumptions we might make about these covariances, let ct

denote the cyclical component and vt the second difference of the trend component:


                                              yt = gt + ct                                        (5)


                                       gt = 2gt−1 − gt−2 + vt .                                   (6)

Suppose that we believed that vt and ct are uncorrelated white noise processes that are also

uncorrelated with (g0 , g−1 ), and let C0 denote the (2×2) variance of (g0 , g−1 ). These assumptions

imply a particular value for Ã in (4). As we let the variance of (g0 , g−1 ) become arbitrarily large

(represented as C0−1 → 0), then in every sample the inference (4) would be numerically identical

to expression (2).

   Proposition 1. For λ = σ 2c /σ 2v and any fixed T, under conditions (5)-(6) with ct and vt

white noise uncorrelated with each other and uncorrelated with (g0 , g−1 ), the matrix Ã in (4)

converges to the matrix A∗ in (2) as C0−1 → 0.

   The proposition establishes that if researcher 1 sought to identify a trend by solving the

minimization problem (1) while researcher 2 found the optimal linear estimate of a trend process

that was assumed to be characterized by the particular assumption that vt and ct were both

white noise, the two researchers would arrive at the numerically identical series for trend and

cycle provided the ratio of σ 2c to σ 2v assumed by researcher 2 was identical to the value of λ used

by researcher 1.

                                                    3
      The Kalman smoother is an iterative algorithm for calculating the population linear projec-

tion (4) for models where the variance and covariance can be characterized by some recursive

structure.3      In this case, (5) is the observation equation and (6) is the state equation. Thus

as noted by Hodrick and Prescott, applying the Kalman smoother to the above state-space

model starting from a very large initial variance for (g0 , g−1 )0 offers a convenient algorithm for

calculating the HP filter, and is in fact a way that the HP filter is often calculated in practice.

Nevertheless, this observation should also be a bit troubling for users of the HP filter, in that

they never defend the claim that the particular structure assumed in Proposition 1 is an accurate

representation of the true data-generating process. Indeed, if a researcher did know for certain

that these equations were the true data-generating process, and further knew for certain the

value of the population parameter λ = σ 2c /σ 2v , he would probably be unhappy with using (2) to

separate cycle from trend! The reason is that if this state-space structure was the true DGP,

the resulting estimate of the cyclical component ct = yt − g̃t would be white noise– it would

be random and exhibit no discernible patterns.                 By contrast, users of the HP filter hope to

see suggestive patterns in plots of the series that is supposed to be interpreted as the cyclical

component of yt .

      Premultiplying (2) by H 0 H + λQ0 Q gives a system of equations whose tth element is


                       [1 + λ(1 − L−1 )2 (1 − L)2 ]gt∗ = yt        for t = 1, 2, ..., T − 2            (7)


for L the lag operator (Lk xt = xt−k , L−k xt = xt+k ). In other words, F (L)gt∗ = yt for


                                     F (L) = 1 + λ(1 − L−1 )2 (1 − L)2 .                               (8)

  3
      See for example Hamilton, 1994, equation [13.6.3].


                                                           4
The following proposition establishes some properties of this filter.4

       Proposition 2. For any λ : 0 < λ < ∞, the inverse of the operator (8) can be written

                                           1 − (φ21 /4)L    1 − (φ21 /4)L−1
                                                                               
                                −1
                       [F (L)]        =C                 +                    −1                               (9)
                                         1 − φ1 L − φ2 L2 1 − φ1 L−1 − φ2 L−2

where
                               1
                                          = ∞    j                            j
                                           P
                                        2   j=0 R [cos(mj) + cot(m) sin(mj)]z                                 (10)
                        1 − φ1 z − φ2 z
                                 1                      P∞
                                                    =     j=0   Rj [cos(mj) + cot(m) sin(mj)]z −j
                      1 − φ1   z −1   − φ2   z −2

                                                    φ1 (1 − φ2 ) = −4φ2                                       (11)

                                                (1 − φ1 − φ2 )2 = −φ2 /λ                                      (12)

                                                               −φ2
                                              C=                                                              (13)
                                                        λ(1 − − φ22 + φ31 /2)
                                                                φ21
                                                              p
                                                           R = −φ2

                                                    cos(m) = φ1 /(2R).                                        (14)

Roots of (1−φ1 z −φ2 z 2 ) = 0 are complex and outside the unit circle, φ1 is a real number between

0 and 2, φ2 a real number between −1 and 0, and R a real number between 0 and 1.

       Figure 1 plots the values of φ1 and φ2 generated by different values of λ. For λ = 1600,

φ1 = 1.777 and φ2 = −0.7994. These imply R = 0.8941, so that the absolute value of the

weights decay with a half-life of about 6 quarters while R60 = 0.0012.5

   4
     Related results have been developed by Singleton (1988), King and Rebelo (1989, 1993), Cogley and Nason
(1995), and McElroy (2008). Unlike these papers, here I provide simple direct expressions for the values of φ1
and φ2 , and my analytical expressions of the HP filter entirely in terms of real parameters in (9) and (10) appear
to be new.
   5
       The other parameters for this case are C = 0.056075, m = 0.111687 and cot(m) = 8.9164.




                                                                  5
    Expression (7) means that for t more than 15 years from the start or end of a sample of

quarterly data, the cyclical component ct = yt − gt∗ is well approximated by

                                                λ(1 − L−1 )2 (1 − L)2      λ(1 − L)4
             ct = λ(1 − L−1 )2 (1 − L)2 gt∗ =                         yt =           yt+2 .   (15)
                                                       F (L)                 F (L)

As noted by King and Rebelo, obtaining the cyclical component for these observations thus

amounts to taking fourth differences of the original yt+2 and applying the operator [F (L)]−1 to

the result, so that the HP cycle might be expected to produce a stationary series as long as

fourth-differences of the original series are stationary. However, De Jong and Sakarya (2016)

noted there could still be significant nonstationarity coming from observations near the start or

end of the sample, and Phillips and Jin (2015) concluded that for commonly encountered sample

sizes, the HP filter may not successfully remove the trend even if the true series is only I(1).


3     Drawbacks to the HP filter.
3.1    Appropriateness for typical economic time series.

The presumption by users of the HP filter is that it offers a reasonable approach to detrending

for a range of commonly encountered economic time series. The leading example of a time-series

process for which we would want to be particularly convinced of the procedure’s appropriateness

would be a random walk. Simple economic theory suggests that variables such as stock prices

(Fama, 1965), futures prices (Samuelson, 1965), long-term interest rates (Sargent, 1976; Pesando,

1979), oil prices (Hamilton, 2009), consumption spending (Hall, 1978), inflation, tax rates, and

money supply growth rates (Mankiw, 1987) should all follow martingales or near martingales. To

be sure, hundreds of studies have claimed to find evidence of statistically detectable departures

from pure martingale behavior in all these series. Even so, there is indisputable evidence that

                                                    6
a random walk is often extremely hard to beat in out-of-sample forecasting comparisons, as has

been found for example by Meese and Rogoff (1983) and Cheung, Chinn, and Pascual (2005)

for exchange rates, Flood and Rose (2010) for stock prices, Atkeson and Ohanian (2001) for

inflation, or Balcilar, et al. (2015) for GDP, among many others.            Certainly if we are not

comfortable with the consequences of applying the HP filter to a random walk, then we should

not be using it as an all-purpose approach to economic time series.

         For yt = yt−1 +εt , where εt is white noise and (1−L)yt = εt , Cogley and Nason (1995)6 noted

that expression (15) means that when the HP filter is applied to a random walk, the cyclical

component for observations near the middle of the sample will approximately be characterized

by
                                                          λ(1 − L)3
                                                   ct =             εt+2 .
                                                            F (L)

For λ = 1600 this is

               n                                                                                   o
     ct = 89.72 −q0,t+2 + ∞           j
                         P
                          j=0 (0.8941)  [cos(0.1117j) + 8.916 sin(0.1117j)](q 1,t+2−j + q 2,t+2+j )


with q0t = εt − 3εt−1 + 3εt−2 − εt−3 , q1t = εt − 3.79εt−1 + 5.37εt−2 − 3.37εt−3 + 0.79εt−4 ), 7 and

q2t = −0.79εt+1 +3.37εt −5.37εt−1 +3.79εt−2 −εt−3 . The underlying innovations εt are completely

random and exhibit no patterns, whereas the series ct is both highly predictable (as a result of

the dependence on lags of εt−j ) and will in turn predict the future (as a result of dependence

on future values of εt+j ). Since the coefficients that make up [F (L)]−1 are determined solely by

the value of λ, these patterns in the cyclical component are entirely a feature of having applied

     6
         Harvey and Jaeger (1993) also have a related discussion.
     7
         The term q1t is the expansion of (1 − L)3 [1 − (φ21 /4)L]εt .


                                                              7
the HP filter to the data rather than reflecting any true dynamics of the data-generating process

itself.

       For example, consider the behavior of stock prices and real consumption spending.8              The

top panels of Figure 2 show the autocorrelation functions for first-differences of these series,

confirming that there is little ability to predict either from its own past values, as we might

have expected from the literature cited at the start of this section. The lower panels show cross

correlations. Consumption has no predictive power for stocks, though stock prices may have a

modest ability to anticipate changes in aggregate consumption.

       Figure 3 shows the analogous results if we tried to remove the trend by HP filtering rather

than first-differencing. The HP cyclical components of stock prices and consumption are both

extremely predictable from their own lagged values as well as each other. The rich dynamics in

these series are purely an artifact of the filter itself and tell us nothing about the underlying data-

generating process. Filtering takes us from the very clean understanding of the true properties

of these series that we can easily see in Figure 2 to the artificial set of relations that appear in

Figure 3. The values plotted in Figure 3 summarize the filter, not the data.

3.2       Properties of the one-sided HP filter.

The HP trend and cycle have an artificial ability to “predict” the future because they are by

construction a function of future realizations. One way we might try to get around this would

be to restrict the minimization problem in (3), forcing at to load only on values (yt , yt−1 , ..., y1 )0

   8
     Stock prices were measured as 100 times the natural log of the end-of-quarter value for the S&P 500 and
consumption from 100 times the natural log of real personal consumption expenditures from the U.S. NIPA
accounts. All data for this figure are quarterly for the period 1950:1 to 2016:1.




                                                     8
that have been observed as of date t, rather than also using future values as was done in the HP

filter (4). The value of this one-sided projection for date t could be calculated by taking the

end-of-sample HP-filtered series for a sample ending at t, repeated for each t.9

       The top panel of Figure 4 shows the result of applying the usual two-sided HP filter to stock

prices. The trend is identified to have been essentially flat throughout the 2000s, with the pre-

recession booms and post-recession busts in stock prices viewed entirely as cyclical phenomena.

The bottom panel shows the results of applying a one-sided HP filter to the same data. This

would instead identify the trend component as rising during economic expansions and falling

during recessions.     The reason is that a real-time observer would not know in early 2009, for

example, that stock prices were about to appreciate remarkably, and accordingly would have

judged much of the drop observed up to that date to be permanent. It is only with hindsight

that we are tempted to interpret the 2008 stock-market crash as a temporary phenomenon.

Making use of unknowable future values in this way is in fact a fundamental reason that HP-

filtered series exhibit the visual properties that they do, precisely because they impose patterns

that are not a feature of the data-generating process and could not be recognized in real time.

Some researchers might be attracted by the simple picture of the “long-run” component of stock

prices summarized by the top panel of Figure 4. But that picture is just something that their

imagination has imposed on the data. And the end-of-sample value obtained from the procedure

is actually quite different from what the researcher is seeing in the middle of the sample.

       Moreover, although a one-sided filter would eliminate the problem of generating a series that

   9
       An easier way to calculate the one-sided projection is with a single pass of the Kalman filter through the
entire sample for the state-space model assumed in Proposition 1 with C0 large and σ 2c /σ 2v = 1600. The Kalman
filter gives the one-sided projection while the Kalman smoother gives the usual two-sided HP filter.


                                                       9
is artificially able to predict the future, changes in both the one-sided trend and its implied

cycle are readily forecastable from their own lagged values, and likewise by values of any other

variables. Again this is not a feature of the stock prices themselves, but instead is an artifact

of choosing to characterize the cycle and trend in this particular way.

3.3       Data-coherent values for λ.

A separate question is what value we should use for the smoothing parameter λ. Hodrick and

Prescott motivated their choice of λ = 1600 based on the prior belief that a large change in

the cyclical component within a quarter would be around 5%, whereas a large change in the

trend component would be around (1/8)%, suggesting a choice of λ = σ 2c /σ 2v = (5/(1/8))2 =

1600. Ravn and Uhlig (2002) showed how to choose the smoothing parameter for data at other

frequencies if indeed it would be correct to use 1600 on quarterly data. These rules of thumb

are almost universally followed.

       It’s worth noting that if the state-space representation in Proposition 1 were indeed an

accurate characterization of the trend that we were trying to infer, we would not need to make

up a value for λ but could in fact estimate it from the data. If for example we assumed a Normal

distribution for the innovations (vt , ct )0 we could use the Kalman filter to evaluate the likelihood

function for the observed sample (y1 , ...., yT )0 and find the values for σ 2v and σ 2c that maximize

the likelihood function.10             This could alternatively be given a quasi-maximum likelihood

interpretation as a GLS minimization of the squared forecast errors weighted by reciprocals of

  10
     See for example Hamilton (1994, equations [13.4.1]-[13.4.2]). Note that although the inferred value for the
trend gt depends only on the ratio σ 2c /σ 2v , the parameters σ 2c and σ 2v are separately identifiable because σ 2c can
be inferred from the average observed size of (yt − gt )2 .




                                                           10
their model-implied variance.

       Table 1 reports MLEs of σ 2v , σ 2c , and λ for a number of commonly studied macroeconomic

series. For every one of these we would estimate a value for σ 2c whose magnitude is similar to,

and in fact often smaller than, σ 2v , and certainly not 1600 times as large.11                    If we used a value

of λ = 1 instead of λ = 1600, the resulting series for gt would differ little from the original data

yt itself; λ = 1 implies a value for R in expression (10) of 0.48, which decays with a half-life of

less than one quarter.

       Thus not only is the HP filter very inappropriate if the true process is a random walk. As

commonly applied with λ = 1600, the HP filter is not even optimal for the only example (namely

(5)-(6)) for which anyone has claimed that it might provide the ideal inference!


4        A better alternative.

Here I suggest an alternative concept of what we might mean by the cyclical component of a

possibly nonstationary series: how different is the value at date t + h from the value that we

would have expected to see based on its behavior through date t?12                     This concept of the cyclical

component has several attractive features.                First, as noted by den Haan (2000), the forecast

error is stationary for a wide class of nonstationary processes. Second, the primary reason that

we would be wrong in predicting the value of most macro and financial variables at a horizon

  11
       Nelson and Plosser (1982, pages 157-158) have also made this observation.
  12
       This idea is related to Beveridge and Nelson’s (1981) definition of the trend component of yt as gt = lim
                                                                                                                    h→∞
lim E(yt+h |yt , yt−1 , .., yt−p+1 ) which limit exists and can be calculated provided that (1 − L)yt is a mean-zero
p→∞
stationary process. Beveridge and Nelson then (somewhat curiously) interpreted the cyclical component as ct =
gt −yt . By contrast, here we keep h and p fixed and take advantage of the fact that gt = E(yt+h |yt , yt−1 , .., yt−p+1 )
exists for a broad range of nonstationary processes. We interpret the cyclical component at date t + h as ct+h =
yt+h − gt .


                                                           11
of h = 8 quarters ahead is cyclical factors such as whether a recession occurs over the next two

years and the timing of recovery from any downturn.13

      While it might seem that calculating this concept of the cyclical component requires us

already to know the nature of the nonstationarity and to have the correct model for forecasting

the series, neither of these is the case.        We can instead always rely on very simple forecasts

within a restricted class, namely, the population linear projection of yt+h on a constant and the

4 most recent values of y as of date t. This object exists and can be consistently estimated for

a wide range of nonstationary processes, as I now show.

4.1      Forecasting when the true process is unknown.

Suppose that the dth difference of yt is stationary for some d. For example, d = 2 would mean

that the growth rate is nonstationary but the change in the growth rate is stationary. Note the

dth difference is also stationary for any series with a deterministic time trend characterized by

a dth-order polynomial in time. For any such process we can write the value of yt+h as a linear

function of initial conditions at time t plus a stationary process.            For example, when d = 1,

letting ut = ∆yt we can write
                                                               (h)
                                              yt+h = yt + wt                                            (16)

                                                     (h)
where the stationary component is given by wt              = ut+1 + · · · + ut+h . For d = 2 and ∆2 yt = ut ,

                                                                     (h)
                                          yt+h = yt + h∆yt + wt                                         (17)

                (h)
where now wt          = ut+h + 2ut+h−1 + · · · + hut+1 . This result holds for general d, as demonstrated

in the following proposition.

 13
      This same consideration suggests using h = 24 for monthly data and h = 2 for annual data.


                                                      12
   Proposition 3. If (1 − L)d yt is stationary for some d ≥ 1, then for all finite h ≥ 1,

                                      (1)     (2)                 (d)               (h)
                              yt+h = κh yt + κh ∆yt + · · · + κh ∆d−1 yt + wt

                         (1)                                (s)   Pj      (s−1)                             (h)
with ∆s = (1 − L)s , κ` = 1 for ` = 1, 2, .. and κj =              `=1   κ`       for s = 2, 3, ..., d and wt     is

a stationary process.

   It further turns out that if ∆d yt ∼ I(0) and we regress yt+h on a constant and the d most

recent values of y as of date t, the coefficients will be forced to be close to the values implied
                        (j)
by the coefficients κh in Proposition 3.            For example, if ∆2 yt is I(0) then in a regression of
                                                                                                      (h)
yt+h on (yt , yt−1 , 1)0 , the fitted values will tend to yt + h(yt − yt−1 ) + µh for µh = E(wt ) as the

sample size gets large; that is, the coefficient on yt will go to 1 + h and the coefficient on yt−1

will go to −h. The implication is that the residuals from a regression of yt+h on (yt , yt−1 , 1)0 will

be stationary whenever y itself is I(2). The reason is that any other values for these coefficients

would imply a nonstationary series for the residuals, whose sum of squares become arbitrarily

large relative to those implied by the coefficients 1 + h and −h as the sample size grows large.

   If ∆d yt is stationary and we regress yt+h on a constant and the p most recent values of

y as of date t for any p > d, the regression will use d of the coefficients to make sure the

residuals are stationary and the remaining p + 1 − d coefficients will be determined by the
                                                                                                            (h)
parameters that characterize the population linear projection of the stationary variable wt                       on

the stationary regressors (∆d yt , ∆d yt−1 , ..., ∆d yt−p+d+1 , 1)0 . The following proposition provides a

formal statement of these claims. In the proof of this proposition I have followed Stock (1994,

p. 2756) in defining a series ut to be I(0) if it has fixed mean µ and satisfies a Functional Central




                                                       13
Limit Theorem.14          This requires that the sample mean of ut has a Normal distribution as the

sample size T gets large, as does a sample mean that used only T r observations for 0 < r ≤ 1.

Formally,
                                                  P[T r]
                                         T −1/2     s=1 (ut       − µ) ⇒ ωW (r),                                 (18)

where [T r] denotes the largest integer less than or equal to T r, W (r) denotes Standard Brownian

Motion, and “⇒” denotes weak convergence in probability measure. I will show that if either

the dth difference (ut = ∆d yt ) satisfies (18) or if the deviation from a dth-order deterministic

polynomial in time (ut = yt − δ 0 − δ 1 t − δ 2 t2 − · · · − δ d td ) satisfies (18), then we can remove the

nonstationary component with the same simple regression.15
                                                                                                   Pd
       Proposition 4. Suppose that either ut = ∆d yt satisfies (18) or that ut = yt −                 j=0   δ j tj with

δ d 6= 0 satisfies (18) for some unknown d. Let xt = (yt , yt−1 , ..., yt−p+1 , 1)0 for some p ≥ d and

consider OLS estimation of yt+h = x0t β + vt+h for t = 1, ..., T with estimated coefficient

                                             P                  −1 P             
                                                  T                    T
                                      β̂ =        j=1   xt x0t         j=1   xt yt+h .                           (19)

                                                                                     (h)   (h)
If p = d, the OLS residuals yt+h − x0t β̂ converge to the variable wt − E(wt ) in Proposition 3.
                                                                                                                   (h)
If p > d, the OLS residuals converge to the residuals from a population linear projection of wt

on (∆d yt , ∆d yt−1 , ..., ∆d yt−p+d+1 , 1)0 .

  14
     Stock (1994, p. 2749) demonstrated that an example of sufficient conditions that imply (18) is that ut =
      ∞
µ + j=0 ψ j η t where η t is a martingale difference sequence with variance σ 2 and finite fourth moment, ψ(1) 6= 0,
    P
     P∞
and j=0 j|ψ j | < ∞, in which case ω 2 in (18) is given by σ 2 [ψ(1)]2 . Alternatively, Phillips (1987, Lemma 2.2)
derived (18) from primitive moment and mixing conditions on ut .
  15
      The reason to state these as two separate possibilities is that if the nonstationarity is purely deterministic,
then the dth differences will not satisfy the Functional Central Limit Theorem. For example, if yt = γ 0 + γ 1 t + εt
with εt white noise, then ∆yt = γ 1 + ψ(L)εt for ψ(L) = 1 − L and ψ(1) = 0. Of course when ut = ∆d yt
satisfies (18) with µ 6= 0, the series yt has both dth-order stochastic as well as dth-order deterministic polynomial
trends, so that case, along with pure stochastic trends (µ = 0) and pure deterministic trends are all allowed by
Proposition 4.


                                                                 14
       Proposition 4 establishes that if we estimate an OLS regression of yt+h on a constant and the

p = 4 most recent values of y as of date t,


                            yt+h = β 0 + β 1 yt + β 2 yt−1 + β 3 yt−2 + β 4 yt−3 + vt+h ,                            (20)


the residuals

                            v̂t+h = yt+h − β̂ 0 − β̂ 1 yt − β̂ 2 yt−1 − β̂ 3 yt−2 − β̂ 4 yt−3                        (21)

offer a reasonable way to construct the transient component for a broad class of underlying

processes. The series is stationary provided that fourth differences of yt are stationary, a goal

that HP intends but does not necessarily achieve. But whereas the approximation to the HP

filter in equation (15) imposes all 4 unit roots, the sample regression would only use 4 differences

if it is warranted by observed features of the data. The proposed procedure has a number of other

advantages over HP. First, any finding that v̂t+h predicts some other variable xt+h+j represents

a true ability of y to predict x rather than an artifact of the way we chose to detrend y, by virtue

of the fact that v̂t+h is a one-sided filter16 . Second, unlike the HP cyclical series ct+h , the value

of v̂t+h will by construction be difficult to predict from variables dated t and earlier.17 If we find

such predictability, it tells us something about the true data-generating process, for example,

that x Granger-causes y. Third, the value of v̂t+h is a model-free and essentially assumption-

free summary of the data.            Regardless of how the data may have been generated, as long as

(1 − L)d yt is covariance stationary for some d ≤ 4, there exists a population linear projection of

  16
     While the OLS coefficients (19) make use of future observations, this influence vanishes asymptotically, in
contrast to HP’s first-order dependence on future observations. Expression (22) below offers another alternative
that allows zero inference of future observations for any sample size T.
  17
     Note however that ct+h by construction can be predicted by variables known at date t + h − 1. The value of
ct+h will be correlated with its own lagged values ct+h−1 , ct+h−2 , ..., ct+1 but likely uncorrelated with ct , ct−1 , ...


                                                            15
yt+h on (yt , yt−1 , yt−2 , yt−3 , 1)0 . That projection is a characteristic of the data-generating process

that can be used to define what we mean by the cyclical component of the process and can be

consistently estimated from the data. Given a dynamic stochastic general equilibrium or any

other theoretical model that would imply an I(d) process, we could calculate this population

characteristic of the model and estimate it consistently from the data.

4.2      Properties in some common settings.

Random walk. Given the literature cited in Section 3.1 it is instructive to examine the conse-

quences if this procedure were applied to a random walk: yt = yt−1 + εt . In this case, d = 1
       (h)
and wt       = εt+h + εt+h−1 + · · · + εt+1 . For large samples, the OLS estimates of (20) converge to

β 1 = 1 and all other β j = 0, and the resulting filtered series would simply be the difference

                                              ṽt+h = yt+h − yt ,                                           (22)

that is, how much the series changes over an h = 8-quarter horizon, or equivalently the sum

of the observed changes over h periods. Note that for h = 8 the filter 1 − Lh wipes out any

cycles with frequency of exactly one year, and thus is taking out both the long-run trend as

well as any strictly seasonal components.18           This also fits with the common understanding of

what we would mean by the cyclical component. Because the simple filter (22) does not require

estimation of any parameters, it can also be used as a quick robustness check for concerns about

the small-sample applicability of the asymptotic claims in Proposition 4, as will be illustrated

in the applications below.

  18
      As in Hamilton (1994, pp. 171-172), the filter 1 − L8 has power transfer function (1 − e−8iω )(1 − e8iω ) =
2 − 2 cos(8ω) which is zero at ω = 0, π/4, π/2, 3π/4, π and thus eliminates not only cycles at the zero frequency
but also cycles that repeat themselves every 8,4,8/3, or 2 quarters. See also Hamilton (1994, Figures 6.5 and
6.6).


                                                       16
   Deterministic time trend. Another instructive example is a pure deterministic time trend of

order d = 1: yt = δ 0 + δ 1 t + εt for εt white noise. In this case ∆yt = δ 1 + εt − εt−1 is stationary
      (h)
and wt      = ∆yt+1 + · · · + ∆yt+h = δ 1 h + εt+h − εt is also stationary for any h. I show in the

appendix that for this case the limiting coefficients on yt , .., yt−p+1 described by Proposition 4

are each given by 1/p and the implied trend for yt+h is


                           δ 0 + δ 1 (t + h) + p−1 (εt + εt−1 + · · · + εt−p+1 ).                 (23)


Even for p = 1 this is not a bad estimate and for p = 4 should not differ much from the true

trend δ 0 + δ 1 (t + h). Again regardless of the choice of p, the difference between yt+h and (23)

will be stationary.

   Interpreting DSGE’s. A third instructive example is when yt is an element of a theoretical

dynamic stochastic general equilibrium model that is stationary around some steady-state value

µ. If the effects of shocks in the theoretical model die out after h periods, then the linear

projection (20) in the theoretical model is characterized by β 0 = µ and β 1 = β 2 = β 3 = β 4 = 0.

In other words, the component vt+h is exactly the deviation from the steady state. If shocks have

not completely died out after h periods, then part of what is being labeled trend by this method

would include the components of shocks that persist longer than h periods. But for any value

of h, the linear projection is a well-defined population characteristic of the theoretical stationary

model, and there is an exactly analogous object one can calculate in the possibly nonstationary

observed data. The method thus offers a way to make an apples-to-apples comparison of theory

with data of the sort that users of the HP filter often desire, but which the HP filter itself will

always fail to deliver.


                                                    17
4.3    Specification of p and h.

One might be tempted to use a richer model than (20) to forecast yt+h , such as using a vector of

variables, more than 4 lags, or even a nonlinear relation. However, such refinements are com-

pletely unnecessary for the goal of extracting a stationary component, and have the significant

drawback that the more parameters we try to estimate by regression, the more the small-sample

results are likely to differ from the asymptotic predictions.   The simple univariate regression

(20) is estimating a population object that is well defined regardless of whether the variable is

part of a large vector system with nonlinear dynamics. For this reason, just as the HP filter

is always implemented as a univariate procedure, my recommendation is to follow that same

strategy for the approach here.

   A related issue is the choice of h. For any fixed h, there exists a sample size T for which

the results of Proposition 4 hold. However, a bigger sample size T will be needed the bigger is

h. The information in a finite data set about very long-horizon forecasts is quite limited. If

we are interested in business cycles, a 2-year horizon should be the standard benchmark. It is

also desirable with seasonal data to have both p and h be integer multiples of the number of

observations in a year. Hence for quarterly data my recommendation is p = 4 and h = 8.

   In other settings the fundamental interest could be in shocks whose effects last substantially

longer than two years but are nevertheless still transient. A leading example would be the recent

interest in debt cycles prompted by datasets such as developed by Jordà, Schularick, and Taylor

(2016). For such an application I would use h = 5 years, with the regression-free implementation

(yt+5 − yt ) having particular appeal given the length of datasets available.



                                                18
4.4       Empirical illustrations.

Figure 5 shows the results when this approach is applied to data on U.S. total employment.

The raw seasonally adjusted data (yt ) are plotted in the upper left panel. The residuals from

regression (20) estimated for these data are plotted in black in the lower-left panel, while the

8-lag difference (22) is in red. The latter two series behave very similarly in this case, as indeed

I have found for most other applications. The primary difference is that the regression residual

has sample mean zero by construction (by virtue of the inclusion of a constant term in the

regression) whereas the average value of (22) will be the average growth rate over a two-year

period.

   One interesting observation is that the cyclical component of employment starts to decline

significantly before the NBER business cycle peak for essentially every recession. Note that this

inference from Figure 5 is summarizing a true feature of the data and is not an artifact of any

forward-looking aspect of the filter.

   The right panels of Figure 5 show what happens when the same procedure is applied to

seasonally unadjusted data. The raw data themselves exhibit a very striking seasonal pattern,

as seen in the top right panel. Notwithstanding, the cyclical factor inferred from seasonally un-

adjusted data (bottom right panel) is almost indistinguishable from that derived from seasonally

adjusted data, confirming that this approach is robust to methods of seasonal adjustment.

   Figure 6 applies the method to the major components of the U.S. national income and

product accounts. Investment spending is more cyclically volatile than GDP, while consumption

spending is less so.   Imports fall significantly during recessions, reflecting lower spending by



                                                19
U.S. residents on imported goods, and exports substantially less so, reflecting the fact that

international downturns are often decoupled from those in the U.S. Detrended government

spending is dominated by war-related expenditures– the Korean War in the early 1950s, the

Vietnam War in the 1970s, and the Reagan military build-up in the 1980s.

       Table 2 reports the standard deviation of the cyclical component of each of these and a

number of other series, along with their correlation with the cyclical component of GDP. We

find very little cyclical correlation between output and prices.19              Both the nominal fed funds

rate and the ex ante real fed funds rate (the latter based on the measure in Hamilton, et al.,

2016) are modestly procyclical, whereas the 10-year nominal interest rate is not.


5        Conclusion.

The HP filter is intended to produce a stationary component from an I(4) series, but in practice

it can fail to do so, and invariably imposes a great cost. It introduces spurious dynamic relations

that are purely an artifact of the filter and have no basis in the true data-generating process,

and there exists no plausible data-generating process for which common popular practice would

provide an optimal decomposition into trend and cycle.                   There is an alternative approach

that can isolate a stationary component from any I(4) series, preserves the underlying dynamic

relations and consistently estimates well-defined population characteristics for a broad class of

possible data-generating processes.

  19
     Identifying the sign of this correlation was one of the primary interests of den Haan’s (2000) application of
a related methodology. In contrast to the results in Table 2, he found a positive correlation between the cyclical
components of these series. I attribute the difference to differences in sample period.




                                                       20
   References
   Atkeson, Andrew and Lee E. Ohanian. 2001. “Are Phillips Curves Useful for Forecasting

Inflation?,” Quarterly Review, Federal Reserve Bank of Minneapolis, 25(1): 2-11.

   Balcilar, Mehmet, Rangan Gupta, Anandamayee Majumdar and Stephen M. Miller. 2015.

“Was the Recent Downturn in US Real GDP Predictable?,” Applied Economics, 47(28): 2985-

3007.

   Beveridge, Stephen, and Charles R. Nelson. 1981. “A New Approach to Decomposition of

Economic Time Series into Permanent and Transitory Components with Particular Attention to

Measurement of the ‘Business Cycle’,” Journal of Monetary Economics, 7:151-174.

   Cheung, Yin-Wong, Menzie D. Chinn & Antonio Garcia Pascual. 2005. “Empirical exchange

rate models of the nineties: Are any fit to survive?,” Journal of International Money and Finance,

24(7): 1150-1175.

   Choi, In. 1993. “Asymptotic Normality of the Least-Squares Estimates for Higher Order

Autoregressive Integrated Processes with Some Applications,” Econometric Theory, 9:263-282.

   Cogley, Timothy, and James M. Nason. 1995. “Effects of the Hodrick-Prescott Filter on

Trend and Difference Stationary Time Series: Implications for Business Cycle Research,” Journal

of Economic Dynamics and Control, 19(1-2): 253-278.

   Cornea-Madeira, Adriana. Forthcoming. “The Explicit Formula for the Hodrick-Prescott

Filter in Finite Sample,” Review of Economics and Statistics.

   De Jong, Robert M., and Neslihan Sakarya. 2016. “The Econometrics of the Hodrick-Prescott

Filter,” Review of Economics and Statistics, 98: 310–317.

   Den Haan, Wouter J. 2000. “The Comovement between Output and Prices,” Journal of

                                               21
Monetary Economics 46: 3-30.

   Fama, Eugene F. 1965. “The Behavior of Stock-Market Prices,” The Journal of Business,

38(1): 34-105.

   Flood, Robert P. and Andrew K. Rose. 2010. “Forecasting International Financial Prices

with Fundamentals: How do Stocks and Exchange Rates Compare?,” Globalization and Eco-

nomic Integration, Chapter 6, Edward Elgar Publishing.

   Hall, Robert E. 1978. “Stochastic Implications of the Life Cycle-Permanent Income Hypoth-

esis: Theory and Evidence,” Journal of Political Economy, 86(6): 971-987.

   Hamilton, James D. 1994. Time Series Analysis, Princeton: Princeton University Press.

   Hamilton, James D. 2009. “Understanding Crude Oil Prices,” Energy Journal, 30(2): 179-

206.

   Hamilton, James D., Ethan S. Harris, Jan Hatzius, and Kenneth D. West. 2016. “The

Equilibrium Real Funds Rate: Past, Present and Future,” IMF Economic Review 64: 660-707.

   Harvey, Andrew C., and Albert Jaeger. 1993. “Detrending, Stylized Facts and the Business

Cycle,” Journal of Applied Econometrics, 8(3): 231-247.

   Hodrick, Robert J. and Edward C. Prescott. 1981. “Postwar U.S. Business Cycles: An

Empirical Investigation,” working paper, Northwestern University.

   Hodrick, Robert J. and Edward C. Prescott. 1997. “Postwar U.S. Business Cycles: An

Empirical Investigation,” Journal of Money, Credit and Banking, 29(1): 1-16.

   Jordà, Òscar, Moritz Schularick, and Alan M. Taylor. 2016. “Macrofinancial History and

the New Business Cycle Facts,” in NBER Macroeconomics Annual 2016, volume 31.

   King, Robert and Sergio T. Rebelo. 1989. “Low Frequency Filtering and Real Business

                                             22
Cycles,” working paper, University of Rochester.

   King, Robert and Sergio T. Rebelo. 1993. “Low Frequency Filtering and Real Business

Cycles,” Journal of Economic Dynamics and Control, 17: 207-231.

   Mankiw, N. Gregory. 1987. “The Optimal Collection of Seigniorage: Theory and Evidence,”

Journal of Monetary Economics, 20(2): 327-341.

   McElroy, Tucker. 2008. “Exact Formulas for the Hodrick-Prescott Filter,” Econometrics

Journal, 11(1): 209-217.

   Meese, Richard A. and Kenneth Rogoff. 1983. “Empirical Exchange Rate Models of the

Seventies : Do they Fit Out of Sample?,” Journal of International Economics, 14(1-2): 3-24.

   Nelson, Charles R., and Charles R. Plosser. 1982. “Trends and Random Walks in Macroe-

conomic Time Series: Some Evidence and Implications,” Journal of Monetary Economics 10:

139-162.

   Park, Joon Y., and Peter C.B. Phillips. 1989. “Statistical Inference in Regressions with

Integrated Processes: Part 2,” Econometric Theory, 5:95-131.

   Phillips, Peter C.B. 1987. “Time Series Regression with a Unit Root,” Econometrica, 55:

277-301.

   Phillips, Peter C.B., and Sainan Jin. 2015. “Business Cycles, Trend Elimination, and the

HP Filter,” working paper, Yale University.

   Pesando, James E. 1979. “On the Random Walk Characteristics of Short- and Long-Term

Interest Rates in an Efficient Market,” Journal of Money, Credit and Banking, Blackwell Pub-

lishing, 11(4): 457-466.

   Ravn, Morten O., and Harald Uhlig. 2002. “On Adjusting the Hodrick-Prescott Filter for

                                              23
the Frequency of Observations,” Review of Economics and Statistics, 84(2): 371-380.

   Samuelson, P. 1965. “Proof that Properly Anticipated Prices Fluctuate Randomly,” Indus-

trial Management Review, 6(2): 41-49.

   Sargent, Thomas J. 1976. “A Classical Macroeconometric Model for the United States,”

Journal of Political Economy, 84(2): 207-237.

   Sims, Christopher A., James H. Stock, and Mark W. Watson. 1990. “Inference in Linear

Time Series Models with Some Unit Roots,” Econometrica 58: 113-144.

   Singleton, Kenneth J. 1988. “Econometric Issues in the Analysis of Equilibrium Business

Cycle Models,” Journal of Monetary Economics, 21: 361-386.

   Stock, James H. 1994. “Unit Roots, Structural Breaks, and Trends,” in Handbook of Econo-

metrics, Volume 4, pp. 2739-2841, edited by Robert F. Engle and Daniel L. McFadden. Ams-

terdam: Elsevier.




                                                24
   Appendix.
   Derivation of equation (2). The minimization problem (1) can be written


                                min {(y − Hg)0 (y − Hg) + λ(Qg)0 (Qg)} .
                                  g



The derivative with respect to g is −2H 0 (y − Hg) + 2λQ0 Qg and setting this to 0 gives (2).

   Derivation of equation (4). Define ãt = [E(yy 0 )]−1 E(ygt ). For any at we have


     E(gt − a0t y)2 = E(gt − ã0t y + ã0t y − a0t y)2

                      = E(gt − ã0t y)2 + 2E[(gt − ã0t y)y 0 ](ãt − at ) + (ãt − at )0 E(yy 0 )(ãt − at ).


The middle term equals 0 by the definition of ãt :


             E[(gt − ã0t y)y 0 ](ãt − at ) = {E(gt y 0 ) − E(gt y 0 )[E(yy 0 )]−1 E(yy 0 )}(ãt − at ).


Hence E(gt − a0t y)2 is minimized when at = ãt . Stacking ã0t y into a (T̃ × 1) vector gives (4).

   Proof of Proposition 1. The assumptions can be written formally as


                                                 E(vt ) = E(ct ) = 0                                             (24)
                                                                         
                                                            σ 2v 0
                                                        
                                                          
                                                                          
                          vt 
                                                        
                                                          
                                                            
                                                                           
                                                                                if j = 0
                        E
                         
                              
                                         vt−j   ct−j =         0 σ 2c                                          (25)
                           ct                             
                                                          
                                                          
                                                          
                                                          
                                                                  0            otherwise

                                              E(g0 ) = E(g−1 ) = 0                                               (26)
                                                  
                                             g0 
                                                               
                                           E
                                            
                                                    g g
                                                       0  −1
                                                                  = C0                                           (27)
                                              g−1



                                                         25
                                        
                               vt 
                                          
                             E
                              
                                    g0 g−1 = 0 for t = 1, ...T.
                                                                                                  (28)
                                ct

     I first establish that under (5)-(6) and (24)-(28),


                                             (Q0 Q)E(gg 0 )H 0 → σ 2v H 0                          (29)


as C0−1 → 0. To do so write (6) as Qg = v for v = (vT , vT −1 , ..., v1 ) and Q0 g = v0 for v0 a (2 × 1)

vector with mean 0 and variance σ 2v I2 . Also from (28), v0 is uncorrelated with v and
                                                              "                     #
                                               Q0 =                 0       P0−1
                                             (2×T̃ )              (2×T )    (2×2)


where P0 is the Cholesky factor of C0 (P0 P00 = C0 ). Stacking these,
                                                      
                                                 Q       v 
                                                    g =     
                                                            
                                                  Q0        v0

so                                                                 −1
                                                                                        −1
                                                           Q 
                                                                           
                                         0
                                 E(gg ) =          σ 2v   
                                                          
                                                               
                                                                              Q0 Q00
                                                            Q0
                                             
                                      Q 
                                
                        Q0 Q00            E(gg 0 ) = (Q0 Q + Q00 Q0 )E(gg 0 ) = σ 2v I
                                                                                      T̃
                                       Q0

                             (Q0 Q)E(gg 0 )H 0 = σ 2v H 0 − (Q00 Q0 )E(gg 0 )H 0

which goes to σ 2v H 0 as P0−1 → 0, as claimed in (29).

     Notice next from

                                               y      = H               g + c
                                             (T ×1)       (T ×T̃ )(T̃ ×1)      (T ×1)




                                                                  26
that E(yy 0 ) = HE(gg 0 )H 0 + σ 2c IT and E(gy 0 ) = E(gg 0 )H 0 + E(gc0 ) = E(gg 0 )H 0 . Hence


                               Ã = E(gy 0 )[E(yy 0 )]−1                                            (30)

                                    = E(gg 0 )H 0 [HE(gg 0 )H 0 + σ 2c IT ]−1 .


Combining (2) and (30),


                            (H 0 H + λQ0 Q)(A∗ − Ã)[HE(gg 0 )H 0 + σ 2c IT ]

                        = H 0 [HE(gg 0 )H 0 + σ 2c IT ] − (H 0 H + λQ0 Q)E(gg 0 )H 0                (31)

                        = H 0 σ 2c − (σ 2c /σ 2v )(Q0 Q)E(gg 0 )H 0


which from (29) goes to 0 as C0−1 → 0. Since the matrices premultiplying and postmultiplying

the left side of (31) are of full rank, this establishes that A∗ = Ã as claimed.

   Proof of Proposition 2. Let θ1 , θ2 , θ3 , θ4 be the roots satisfying F (θi ) = 0. As noted

by King and Rebelo (1989), since λ > 0, F (z) in (8) is positive for all real z meaning that θi

comprise two pairs of complex conjugates. Since F (z) = F (z −1 ), if θi is a root, then so is θ−1
                                                                                                i .


Thus the values of θi are given by Reim , Re−im , R−1 eim , and R−1 e−im for some fixed R and m;

one pair is inside the unit circle and the other is outside. Noting that the coefficients on z 2 and

z −2 in F (z) are both λ, it follows that F (z) can be written


                         F (z) = λ(1 − θ1 z)(1 − θ2 z)(θ−1   −1   −1   −1
                                                        1 − z )(θ 2 − z ).



From the symmetry of F (z) in z and z −1 we can without loss of generality normalize θ1 and θ2

to be inside the unit circle and write

                                  λ
                       F (z) =         (1 − θ1 z)(1 − θ2 z)(1 − θ1 z −1 )(1 − θ2 z −1 ).
                                 θ1 θ2

                                                      27
Define (1 − φ1 z − φ2 z 2 ) = (1 − θ1 z)(1 − θ2 z), namely φ1 is the real number θ1 + θ2 and φ2 is the

negative real number −θ1 θ2 . Note also that the roots of (1 − φ1 z − φ2 z 2 ) = 0 are the complex

conjugates θ−1      −1
            1 and θ 2 , which are both outside the unit circle. This gives the bounds on φ1


and φ2 stated in Proposition 2 as in Hamilton (1994, Figure 1.5). Then

                                   λ
                        F (z) =       (1 − φ1 z − φ2 z 2 )(1 − φ1 z −1 − φ2 z −2 ).                 (32)
                                  −φ2

   Evaluating (8) and (32) at z = 1 gives

                                  F (1) = 1 = (1 − φ1 − φ2 )2 λ/(−φ2 )                              (33)

as claimed in (12), Likewise evaluating (8) and (32) at z = −1 gives

                            F (−1) = 1 + 16λ = (1 + φ1 − φ2 )2 λ/(−φ2 ).                            (34)

Taking the difference between these last two equations establishes (4φ1 − 4φ1 φ2 )λ/(−φ2 ) = 16λ

or φ1 (1 − φ2 ) = −4φ2 as claimed in (11). Note that since φ2 < 0 (required by complex roots),

from (11) φ1 > 0.

   I next establish that

                         1                            C0 + C1 z         C0 + C1 z −1
                                                 =                  +                      + B0 .   (35)
    (1 − φ1 z − φ2 z 2 )(1 − φ1 z −1 − φ2 z −2 )   1 − φ1 z − φ2 z 2 1 − φ1 z −1 − φ2 z −2

Combining terms over a common denominator shows that (35) will hold provided

             1 = (C0 + C1 z)(1 − φ1 z −1 − φ2 z −2 ) + (C0 + C1 z −1 )(1 − φ1 z 1 − φ2 z 2 )

                    +B0 (1 − φ1 z − φ2 z 2 )(1 − φ1 z −1 − φ2 z −2 )

                = [2C0 − 2C1 φ1 + B0 (1 + φ21 + φ22 )]

                    +[C1 − C0 φ1 − C1 φ2 − B0 φ1 + B0 φ1 φ2 ](z + z −1 )

                    −[C0 φ2 + B0 φ2 ](z 2 + z −2 ).

                                                      28
The coefficient on (z 2 + z −2 ) will be zero provided B0 = −C0 , or provided


               1 = [C0 − 2C1 φ1 − C0 φ21 − C0 φ22 ] + [C1 − C1 φ2 − C0 φ1 φ2 ](z + z −1 ).             (36)


The coefficient on (z + z −1 ) will be zero provided

                                             C0 φ1 φ2
                                      C1 =            = −C0 φ21 /4                                     (37)
                                             1 − φ2

where the last equation made use of (11). Substituting (37) into (36), we see that (35) will be

true provided we set

                                    1 = C0 (1 − φ21 − φ22 + φ31 /2).

Combining these results we conclude that

                                                          1 − (φ21 /4)z     1 − (φ21 /4)z −1
                                                                                                 
                         1
                                                 = C0                    +                      −1 .
    (1 − φ1 z − φ2 z 2 )(1 − φ1 z −1 − φ2 z −2 )        1 − φ1 z − φ2 z 2 1 − φ1 z −1 − φ2 z −2

From (32) we then obtain (9) with C = −C0 φ2 /λ as claimed in (13).

   To derive (10), recall from Hamilton (1994, pp. 16 and 33) that

                              1
                                         = ∞     j                          j
                                          P
                                       2    j=0 R [2α cos(mj) + 2β sin(mj)]z .                         (38)
                       1 − φ1 z − φ2 z

We know that the coefficient on z j for j = 0 must be 1, requiring [2α cos(0) + 2β sin(0)] = 1 or

α = 1/2. We likewise know that the coefficient on z j for j = 1 is given by φ1 , so R[cos(m) +

2β sin(m)] = φ1 , which from (14) gives R2β sin(m) = φ1 /2 or 2β sin(m) = cos(m) so 2β =

cot(m). Substituting these values for α and β into (38) gives (10).

   Proof of Proposition 3. Recall the identity

                                                     Ph
                                       yt+h = yt +      j=1   ∆yt+j                                    (39)

                                                   29
which immediately gives the result of Proposition 3 for the case d = 1 as stated in (16). We

likewise have the identity
                                                                    Pj
                                              ∆yt+j = ∆yt +            s=1   ∆2 yt+s .                       (40)

Substituting (40) into (39) gives

                                                                   Ph               (h)
                                             yt+h = yt + ∆yt           j=1   1 + wt

      (h)       Ph     Pj
for wt      =    j=1    s=1   ∆2 yt+s as claimed in (17) for the case d = 2. We can proceed recursively
                                               Ps
using the identity ∆k yt+s = ∆k yt +               r=1   ∆k+1 yt+r and substituting into the preceding expression.
                                 (h)
For any d the resulting wt             is a finite sum of stationary variables and therefore is itself stationary.

    Proof of Proposition 4. Note that the fitted values and residuals implied by the coefficients

in (19) are numerically identical to those if we were to do the (infeasible) regression yt+h =

x̃0t α + vt+h for

                            x̃t = (ũt , ũt−1 , ..., ũt−p+d+1 , 1, ∆d−1 yt , ∆d−2 yt , ..., ∆yt , yt )0

with ũt = ∆d yt − µ. The latter regression is infeasible because we do not know the true values

of µ and d. But because the fitted values are the same, once we find the properties of the second

regression, we will also know the properties of the first. For example, for d = 2 and p = 4,
                                                             
                                            1 −2   0 −µ  
                                                          1                     yt 
                                                                                 
                                                                                 
                                        
                                            0 1 −2 1 µ 
                                                                              yt−1 
                                                                                    
                                                                                 
                                                                                 
                                  x̃t =     0 0  0 0 1                             ≡ Hxt
                                                                               yt−2                         (41)
                                                           
                                                        
                                                                                 
                                                                                 
                                        
                                            1 −1 0 0 0 
                                                                              yt−3 
                                                                                    
                                                                                 
                                                                                 
                                             1 0  0 0 0                         1


                                                                30
and α̂ = ( Hxt x0t H 0 )−1 ( Hxt yt+h ) so β̂ = H 0 α̂ for every sample.
          P                 P
                                                                                                                 When p = d we define

the (p + 1) × 1 vector as x̃t = (1, ∆d−1 yt , ∆d−2 yt , ..., ∆yt , yt )0 , that is, none of the ũt−j variables

appear in x̃t when p = d.
                                                                                      (h)         (d)   (d−1)             (1)            (h)
   Define q to be the (p + 1) × 1 vector q = (0, ..., 0, E(wt ), κh , κh                                        , ..., κh )0 , so that w̃t     =
 (h)          (h)
wt − E(wt ) = yt+h − x̃0t q and

                                                             −1                                   (h)
                                                    x̃t x̃0t )              x̃t (x̃0t q + w̃t )
                                               P                   P
                                      α̂ = (

                                                                           −1               (h)
                                                            x̃t x̃0t )
                                                      P                         P
                                        = q+(                                        x̃t w̃t .                                               (42)


   We first consider the case when (18) holds for ∆d yt when there is further no drift and the initial
                                                                                                                                               (j)
value for all of the difference processes is zero, namely, the case when µ = 0 and ∆d−j yt = ξ t
        (1)     Pt              (s)     Pt       (s−1)
where ξ t =         j=1 ũj and ξ t =    j=1   ξj         for s = 2, 3, ..., d. For this case define
                                                                                                       
                                         1/2
                                       T Ip−d+1                      0         0      ··· 0 
                                                                                             
                                                                                             
                                      
                                            0                     T            0      ··· 0 
                                                                                             
                                                                                             
                                 ΥT = 
                                            0                        0 T2             ··· 0 .                                             (43)
                                                                                             
                                            ..                       ..        ..         .. 
                                      
                                             .                        .         .     ··· . 
                                                                                             
                                                                                             
                                             0                        0         0       0 Td

Adapting the approach in Sims, Stock and Watson (1990), we have from (42) that

                                                               −1 P       (h)
                    T −1/2 ΥT (α̂ − q) = T −1/2 ΥT ( x̃t x̃0t )
                                                    P
                                                                    x̃t w̃t
                                                                    −1 −1 P       (h)
                                        = T −1/2 Υ−1   x̃t x̃0t Υ−1
                                                    P
                                                  T              T     ΥT    x̃t w̃t
                                                                                 −1 h                                (h)
                                                                                                                            i
                                                 Υ−1         x̃t x̃0t Υ−1                T −1/2 Υ−1
                                                      P                                                    P
                                        =         T                    T                         T              x̃t w̃t         .            (44)




                                                                 31
Consider first the last term in (44):
                                                                                               
                                                                            −1
                                                                               P         (h)
                                                                       T          ũt w̃t      
                                                                                               
                                                                              ..               
                                                                   
                                                                               .               
                                                                                                
                                                                                               
                                                                                               
                                                                    T −1 P ũ          (h)
                                                                              t−p+d+1 w̃t
                                                                                                
                                                                                               
                                                                                               
                                                                              P (h)            
                                                            (h)
                                                                         T −1 w̃t              
                                     T −1/2 Υ−1
                                                  P                                            
                                             T        x̃t w̃t     =
                                                                   
                                                                                                .
                                                                                                                (45)
                                                                    T −3/2 P ξ (1) w̃(h)       
                                                                                  t  t         
                                                                                               
                                                                             P (2) (h)         
                                                                    T −5/2 ξ t w̃t
                                                                                               
                                                                                                
                                                                                               
                                                                               ..              
                                                                                 .
                                                                                               
                                                                                               
                                                                                               
                                                                              P (d) (h)        
                                                                     T −d−1/2 ξ t w̃t

The first p − d terms are just the sample means of stationary variables, which by the Law of
                                                                                               (h)
Large Numbers converge in probability to their expectation E(ũt−j w̃t ). Term p − d + 1 likewise
                         (h)
converges to E(w̃t ) = 0. Calculations analogous to those behind Lemma 1(e) in Sims, Stock

and Watson (1990) show that the last d terms in (45) also all converge in probability to zero.20

       Turning next to the first term in (44), the upper-left (p − d) × (p − d) block of Υ−1                x̃t x̃0t Υ−1
                                                                                                        P
                                                                                          T                           T


is characterized by
                                                                                                     
                             −1                         −1
                                     ũ2t
                                P                          P
                        T                    ··· T         ũt ũt−p+d+1     γ0      · · · γ p−d−1   
                                                                                                     
                               ..                            ..           p 
                                                                          →       ..            ..    
              
                                .            ···              .                   .  ···        .    
                                                                                                        
                                                                                                     
                                                                                                     
                  T −1                                 T −1 ũ2t−p+d+1
                         P                                 P
                               ũt−p+d+1 ũt · · ·                              γ p−d−1 · · ·    γ0

for γ j = E(ũt ũt−j ). From Sims, Stock and Watson Lemma 1(a) and 1(b), the lower-right

  20
    That is, before multiplying by T −1/2 the terms are all Op (1); for similar calculations see Lemma 1(b) in
Choi (1993) and Proposition 17.3(e) in Hamilton (1994).




                                                                     32
(d + 1) × (d + 1) block satisfies
                                                                                                    
                             −3/2
                                  P (1)             −5/2
                                                         P (2)                    −d−1/2
                                                                                         P (d)
            1             T            ξt        T            ξt       ··· T                ξt 
                                                                                                    
                                                                                                    
     T −3/2 P ξ (1)       T −2 [ξ t ]2
                                P      (1)                  (1)
                                                 T −3 ξ t ξ t
                                                      P           (2)                      (1)
                                                                        · · · T −d−1 ξ t ξ t 
                                                                                      P          (d)
                  t                                                                                 
                                                                                                    
                                                                                                    
     T −5/2 ξP    (2)
                          T −3
                                P
                                   ξ
                                     (2)
                                         ξ
                                            (1)
                                                  T −4
                                                       P
                                                           [ξ
                                                              (2)
                                                                  ] 2
                                                                        ·  · · T −d−2
                                                                                      P
                                                                                         ξ
                                                                                           (2)
                                                                                               ξ
                                                                                                 (d) ⇒
                  t                 t      t                 t                            t     t   
                                                                                                    
             .
              .                   .
                                  .                      .
                                                         .                             .
                                                                                       .
                                                                                                     
    
             .                   .                      .              ·  · ·         .             
                                                                                                     
                                                                                                    
                                                                                                    
                     (d)               (d) (1)                (d) (2)                       (d)
      T −d−1/2 ξ t       T −d−1 ξ t ξ t         T −d−2 ξ t ξ t          · · · T −2d [ξ t ]2
                 P              P                      P                              P
                                                                                                                            
                                 R1                                  R1                                  R1
             1               ω 0 W (1) (r)dr                   ω 0 W (2) (r)dr           ···          ω 0 W (d) (r)dr       
                                                                                                                            
     R                                                                                                                      
     ω 1 W (1) (r)dr           R1
                             ω 2 0 [W (1) (r)]2 dr
                                                              R1                                     R1
                                                         ω 2 0 W (1) (r)W (2) (r)dr · · · ω 2 0 W (1) (r)W (d) (r)dr         
        0                                                                                                                   
                                                                                                                            
     R                                                                                                                      
     ω 1 W (2) (r)dr ω 2 R 1 W (2) (r)W (1) (r)dr                  R1
                                                              ω 2 0 [W (2) (r)]2 dr
                                                                                                     R1
                                                                                          · · · ω 2 0 W (2) (r)W (d) (r)dr   
        0                    0                                                                                              
                                                                                                                            
             ..                         ..                               ..                                  ..             
               .                          .                                .              ···                  .
                                                                                                                            
                                                                                                                            
                                                                                                                            
     R                                                                                                                      
          1                 R1                                R1                                        R1
      ω 0 W (d) (r)dr ω 2 0 W (d) (r)W (1) (r)dr ω 2 0 W (d) (r)W (2) (r)dr · · ·                    ω 2 0 [W (d) (r)]2 dr
                                                                                      Rr
where W (1) (r) denotes Standard Brownian Motion and W (j) (r) = 0 W (j−1) (s)ds. For the

off-diagonal block of Υ−1          x̃t x̃0t Υ−1
                               P
                       T                     T we see using calculations analogous to Sims, Stock and


Watson’s Lemma 1(e) that
                                                                                    
                                 −1                           −1
                                    P                            P
                            T      ũt         ···       T          ũt−p+d+1       
                                                                                    
                                                                                    
                        T −3/2 P ξ (1) ũ      ···    T −3/2
                                                                P    (1)
                                                                    ξ t ũt−p+d+1    
                                   t     t                                          
                                                                                    
                                                                                     p
                        T −5/2 P ξ (2) ũ      ···    T −5/2
                                                                P    (2)
                                                                    ξ t ũt−p+d+1     → 0.
                                   t     t                                          
                                                                                    
                                ..                                  ..              
                       
                                 .             ···                   .              
                                                                                     
                                                                                    
                                 P (d)                                              
                                                                          (d)
                         T −d−1/2 ξ t ũt       · · · T −d−1/2
                                                                 P
                                                                     ξ t ũt−p+d+1

Bringing all these results together, it follows that
                                                                           

                                                             p 
                                                                 g 
                                          T −1/2 ΥT (α̂ − q) → 
                                                                
                                                                                                             (46)
                                                                 0

                                                        33
                                                                −1                             
                                                                                          (h)
                                          γ0      · · · γ p−d−1                E(ũt w̃t )     
                                                                                               
                                 
                               g=          ..               ..                      ..         
                                                                                                  .
                                            .     ···        .  
                                                                 
                                                                          
                                                                                       .         
                                                                                               
                                                                                               
                                                                                            (h)
                                         γ p−d−1   ···      γ0                E(ũt−p+d+1 w̃t )
                                                                                                       (h)
Note that g corresponds to the coefficients from a population linear projection of w̃t                       on

(ũt , ũt−1 , ..., ũt−p+d+1 )0 .

     Writing out (46) explicitly using (43) gives
                                                                             
                               Ip−d+1   0   0            ···        0        
                                                                             
                                                                             
                               0
                                      T 1/2 0            ···        0        
                                                                              
                                                                                            
                              
                              
                                                                              
                                                                                        p 
                                                                                             g 
                               0        0 T 3/2          ···        0         (α̂ − q) →  .
                                                                                          
                                                                                           0
                                  ..    ..  ..                      ..       
                              
                                   .     .   .           ···         .       
                                                                              
                                                                             
                                                                             
                                   0     0   0             0     T d−1/2

This equation shows that the first p − d elements of α̂ converge to the stationary population
                                                                     (h)
projection coefficients g, the p − d + 1 term to E(wt ), and the last d elements of α̂ converge to
        (j)
the κh terms in q. Indeed, the latter estimates are superconsistent– they still converge to the

terms in q even when multiplied by some positive power of T.

     Taking again the p = 4 and d = 2 example (41), the coefficients β̂ from the actual regression




                                                                34
of yt+h on (yt , yt−1 , yt−2 , yt−3 , 1)0 have plim
                                                                                                      
                 1     0    0
                           1     1 g1                                             g1 + h + 1           
                                                                                                      
                                                                                                      
             
                −2 1 0 −1 0 
                             
                                    g2                             
                                                                                   g2 − 2g1 − h          
                                                                                                           
                                                                                                      
           p                                                                                          
        β̂ → 
                 1 −2 0 0 0 
                               µh(h + 1)/2
                                                                  =
                                                                                     g1 − 2g2            .
                                                                                                           
                                                                                                      
                                                                                                      
             
                 0  1 0 0 0 
                             
                                     h                             
                                                                                           g2            
                                                                                                           
                                                                                                      
                                                                                                      
                 −µ −µ 1 0 0          1                                    µ{[h(h + 1)/2] − g1 − g2 }

    The above derivation assumed µ = 0 so that there was no drift in ∆d yt . If instead we had
                            Pt            Pt                 (1)
µ 6= 0, then ∆d−1 yt =       s=1   us =    s=1   ũs + tµ = ξ t + tµ, which is dominated for large t by the
                                                             (1)                       (j)
drift term tµ rather than the random walk term ξ t , and ∆d−j yt = ξ t + (1/j!)tj µ + op (tj ). In

this case we would simply replace ΥT in the above derivations with
                                                                  
                                      1/2
                                    T Ip−d+1   0   0                  ···     0       
                                                                                      
                                                                                      
                                   
                                         0   T 3/2 0                  ···     0       
                                                                                       
                                                                                      
                                                                                      
                             Υ̃T = 
                                         0     0 T 5/2                ···     0       .
                                                                                                                (47)
                                                                                      
                                         ..    ..  ..                          ..     
                                   
                                          .     .   .                 ···       .     
                                                                                       
                                                                                      
                                                                                      
                                          0     0   0                  0     T d+1/2

We would then arrive at the identical conclusion (46) this time using results (a), (c), and (g)

from Sims, Stock and Watson Lemma 1.
                                                                                        (1)          (1)   (1)     (1)
    Alternatively, adding a nonzero initial condition, e.g. replacing ξ t                     with ξ t + ξ 0 for ξ 0
                                                                                                   (1)
any fixed constant produces a term that is still dominated asymptotically by ξ t , and as in Park

and Phillips (1989), the original convergence claims again all go through.

    Finally, the derivations are very similar for the case of purely deterministic time trends,


                                                        35
       Pd
yt =    j=0   δ j tj + ut . For this case we have µ = E(∆d yt ) = δ d and
                                                          1
                                                          X                                    d−1
                                                                                               X                      d
                                                                                                                      X
                                                                  (d−1) j                               (1)
x̃t = (∆d ut − δ d , ..., ∆d ut−p+d+1 − δ d , 1,                δj        t + ∆d−1 ut , ...,         δ j tj + ∆ut ,         δ j tj + ut )0
                                                          j=0                                  j=0                    j=0
        Pd−s        (s)          Pd−s+1        (s−1) j    Pd−s+1        (s−1)                     (0)
where        j=0   δ j tj =          j=0    δj      t −     j=0      δj         (t − 1)j and δ j        = δ j . Then for Υ̃T as in

(47), we again have

                                       (h) p
            T −1/2 Υ̃−1                    → (E(∆d ut − δ d )w̃t+h , ..., E(∆d ut−p+d+1 − δ d )w̃t+h , 0, ..., 0)0 .
                             P
                     T           x̃t w̃t


The matrix Υ̃−1               x̃t x̃0t Υ̃−1
                          P
             T                           T likewise has a block-diagonal plim, so for g the coefficients of the


population linear projection of w̃t+h on (∆d yt − δ d , ..., ∆d yt−p+d+1 − δ d )0 we have

                                                     p            (h)       (d)       (1)
                                                  α̂ → (g, E(wt ), κh , ..., κh )0 .                                                 (48)

                                                                                            (h)
   Derivation of equation (23). For σ 2 the variance of εt , w̃t                                  = εt+h − εt , and vt = εt − εt−1

we have
                                                                                                             −1                           
                  E(vt2 )
                      E(vt vt−1 )      E(vt vt−2 )   · · · E(vt vt−p+2 )                                          E[vt (εt+h − εt )]       
                                                                                                                                          
                                                                                                                                          
                             2
     E(v v )
         t−1 t         E(vt−1 )      E(vt−1 vt−2 ) · · · E(vt−1 vt−p+2 )                                     
                                                                                                              
                                                                                                                   E[v (ε − ε )]
                                                                                                                       t−1 t+h     t
                                                                                                                                             
                                                                                                                                             
                                                                                                                                          
                                                                                                                                          
g = 
     E(vt−2 vt )    E(vt−2 vt−1 )     E(vt−22
                                                 )   · · · E(vt−2 vt−p+2 )                                    
                                                                                                              
                                                                                                                   E[v (ε − ε )]
                                                                                                                       t−2 t+h     t
                                                                                                                                             
                                                                                                                                             
                                                                                                                                          
          ..              ..              ..                    ..                                                         ..             
            .               .               .        ···          .                                                            .
                                                                                                                                          
                                                                                                                                          
                                                                                                                                          
                                                                                                                                          
                                                                2
      E(vt−p+2 vt ) E(vt−p+2 vt−1 ) E(vt−p+2 vt−2 ) · · ·   E(vt−p+2  )                                             E[vt−p+2 (εt+h − εt )
                                 −1                            
                                                 2
                                   
         2 −1 0 · · · 0               −σ   −(p − 1)/p 
    
                                   
    
                                   
                                    
    
                                                               
    
                                 
                                    
                                                                  
    
        −1 2 −1 · · · 0             0   −(p − 2)/p 
    
                                                               
    
                                 
                                    
                                                                  
        
       2
                                                                  
  =   σ  0 −1 2 · · · 0           
                                         0  =  −(p − 3)/p 
                                                                    
    
                                                               
                                   
    
           .
             .    .
                  .   .
                      .          .
                                 .
                                   
                                             .
                                               .
                                                            .
                                                              .
                                                                      
    
    
    
        .
                 .   . · · · .  
                                    
                                        .  
                                                           .       
                                                                      
    
                                 
                                                                  
     
                                  
                                                                  
            0     0   0 ··· 2                0             −1/p
    
                                   


                                                                     36
where the last equation can be verified by premultiplying by

                                                                         
                                              2        −1
                                                         ··· 0 
                                                              0
                                                               
                                                               
                                          
                                              −1 2 −1 · · · 0 
                                                               
                                                               
                                          
                                               0 −1 2 · · · 0 
                                                               
                                               .. .. ..     .. 
                                          
                                                .  .  . ··· . 
                                                               
                                                               
                                                0  0  0 ··· 2

and confirming that the resulting vector is indeed (−1, 0, ..., 0)0 . Hence the plim in (48) for this

example is
                               p
                           α̂ → (−(p − 1)/p, −(p − 2)/p, ..., −1/p, hδ 1 , 1)0 .

Also for this case we have
                                                                             
                                         1 −1           0    ··· 0   0−δ 1 
                                                                           
                                                                           
                                  
                                         0        1    −1 · · · 0 0 −δ 1  
                                                                           
                                                                           
                                  
                                         0        0    1 · · · 0 0 −δ 1   
                                                                           
                                  
                                H=       ..       ..    ..      .. ..  .. 
                                          .        .     . ··· . .      . 
                                                                           
                                                                           
                                          0        0    0 · · · 1 −1 −δ 1 
                                                                           
                                  
                                                                           
                                                                           
                                          0        0    0 ··· 0 0       1 
                                                                           
                                  
                                                                           
                                                                           
                                          1        0    0 ··· 0 0       0

so β̂ = H 0 α̂ has plim (1/p, 1/p, ..., 1/p, δ 1 [h + (p − 1)/p + (p − 2)/p + · · · + 1/p])0 implying a fitted




                                                             37
value


        β 0 xt = (1/p)(yt + yt−1 + · · · + yt−p+1 ) + δ 1 [h + 1/p + 2/p + · · · + (p − 1)/p]

              = δ 1 h + (1/p){yt + [yt−1 + δ 1 ] + [yt−2 + 2δ 1 ] + · · · + [yt−p+1 + (p − 1)δ 1 ]}

              = δ 1 h + (1/p){[δ 0 + δ 1 t + εt ] + [δ 0 + δ 1 t + εt−1 ] + · · · + [δ 0 + δ 1 t + εt−p+1 ]}

              = δ 0 + δ 1 (t + h) + (1/p)(εt + εt−1 + · · · + εt−p+1 ).




                                                       38
Table 1. Maximum likelihood estimates of parameters of state-space formalization of the HP filter for
assorted quarterly macroeconomic series.

                                    σ2c             σ2v             λ

GDP                                0.115           0.468          0.245

Consumption                        0.163           0.174          0.940

Investment                         4.187          12.196          0.343

Exports                            5.818           3.341          1.741

Imports                            4.423           4.769          0.927

Government spending                0.221           1.160          0.191

Employment                         0.006           0.250          0.023

Unemployment rate                  0.014           0.092          0.152

GDP Deflator                       0.018           0.081          0.216

S&P 500                           21.284          15.186          1.402

10-year Treasury yield             0.135           0.054          2.486

Fed Funds Rate                     0.633           0.116          5.458

Real Rate                          0.875           0.091          9.596




                                                   39
Table 2. Standard deviation of cyclical component and correlation with cyclical component of GDP for
assorted macroeconomic series.

                                   Regression Residuals                Random walk                  Sample
                                 St. Dev.      GDP Corr.         St. Dev.     GDP Corr.
 GDP                               3.38           1.00             3.69         1.00             1947:1-2016:1
 Consumption                       2.85           0.79             3.04         0.82             1947:1-2016:1
 Investment                       13.19           0.84            13.74         0.80             1947:1-2016:1
 Exports                          10.77           0.33            11.33         0.30             1947:1-2016:1
 Imports                           9.79           0.77             9.98         0.75             1947:1-2016:1
 Government spending               7.13           0.31             8.60         0.38             1947:1-2016:1
 Employment                        3.09           0.85             3.32         0.85             1947:1-2016:2
 Unemployment rate                 1.44          -0.81             1.72        -0.79             1948:1-2016:2
 GDP Deflator                      2.99           0.04             4.11        -0.13             1947:1-2016:1
 S&P 500                          21.80           0.41            22.08         0.38             1950:1-2016:2
 10-year Treasury yield            1.46          -0.05             1.51         0.08             1953:2-2016:2
 Fed funds rate                    2.78           0.33             3.03         0.40             1954:3-2016:2
 Real rate                         2.25           0.39             2.60         0.42             1958:1-2014:3


Notes to Table 2. Filtered series were based on the full sample available for that variable, while
correlations were calculated using the subsample of overlapping values for the two indicators. Note
that the regression residuals lose the first 11 observations and the random-walk calculations lose the
first 8 observations.




                                                   40
Figure 1. Values for φ1 and φ2 implied by different values of λ.

        0.2
                      λ=0
          0

        -0.2

        -0.4
   φ2




        -0.6
                                                                λ = 1600
        -0.8
                                                                    λ=∞
         -1

        -1.2
               -0.5   0          0.5         1            1.5       2      2.5
                                            φ1




                                                     41
Figure 2. Autocorrelations and cross-correlations for first-difference of stock prices and real
consumption spending.
                           S&P 500 with own lags                                 Consumption with own lags
             1                                                      1



           0.5                                                    0.5



             0                                                      0



           -0.5                                                   -0.5
                  0   1    2    3    4    5    6    7    8               0   1     2    3    4    5    6     7   8
                                     Lag                                                  Lag
                      S&P 500 with lags of Consumption                       Consumption with lags of S&P 500
             1                                                      1



           0.5                                                    0.5



             0                                                      0



           -0.5                                                   -0.5
                  0   1    2    3    4    5    6    7    8               0   1     2    3    4    5    6     7   8
                                    Lag                                                     Lag



Notes to Figure 2. Upper left: autocorrelations of log growth rate of end-of-quarter value for S&P 500.
Upper right: autocorrelations of log growth rate of real consumption spending. Lower panels: cross
correlations.




Figure 3. Autocorrelations and cross-correlations for HP cyclical component of stock prices and real
consumption spending.
                           S&P 500 with own lags                                 Consumption with own lags
             1                                                      1



           0.5                                                    0.5



             0                                                      0



           -0.5                                                   -0.5
                  0   1    2    3    4    5    6    7    8               0   1     2    3    4    5    6     7   8
                                     Lag                                                  Lag
                      S&P 500 with lags of Consumption                       Consumption with lags of S&P 500
             1                                                      1



           0.5                                                    0.5



             0                                                      0



           -0.5                                                   -0.5
                  0   1    2    3    4    5    6    7    8               0   1     2    3    4    5    6     7   8
                                    Lag                                                     Lag



Notes to Figure 3. Upper left: autocorrelations of HP cycle for log of end-of-quarter value for S&P 500.
Upper right: autocorrelations of HP cycle for log of real consumption spending. Lower panels: cross
correlations.

                                                             42
Figure 4. Comparison of one-sided and two-sided HP filters.

    800                                      Two-sided filter

    700

    600

    500

    400

    300

    200
             1950        1960         1970         1980         1990        2000        2010


    800                                      One-sided filter

    700

    600

    500

    400

    300

    200
             1950        1960         1970         1980         1990        2000        2010


Notes to Figure 4. Red line in both panels plots 100 times natural log of S&P 500 stock price index. The
black curve in the top panel plots the HP estimate of trend as inferred using the usual two-sided filter
(calculated using the Kalman smoother for the state-space model in Proposition 1), whereas the black
curve in the bottom panel plots trend from a one-sided HP filter (calculated using the Kalman filter for
the same model). Shaded regions denote NBER recession dates.




                                                   43
Figure 5. Regression and 8-quarter-change filters applied to seasonally adjusted and seasonally
unadjusted employment data.
                  Employment (seasonally adjusted)                                           Employment (not seasonally adjusted)
   1200                                                                       1200


   1180                                                                       1180


   1160                                                                       1160


   1140                                                                       1140


   1120                                                                       1120


   1100                                                                       1100


   1080                                                                       1080


   1060                                                                       1060
           1950    1960       1970   1980   1990    2000     2010                     1950      1960   1970    1980   1990    2000     2010


                          Cyclical component (SA)                                                  Cyclical component (NSA)
    12.5                                                                      12.5
                                                           RANDOM_WALK                                                               RANDOM_WALK
                                                           REGRESSION                                                                REGRESSION
    10.0                                                                      10.0

     7.5                                                                       7.5

     5.0                                                                       5.0

     2.5                                                                       2.5

     0.0                                                                       0.0

    -2.5                                                                       -2.5

    -5.0                                                                       -5.0

    -7.5                                                                       -7.5

   -10.0                                                                      -10.0
           1950    1960       1970   1980   1990    2000     2010                     1950      1960   1970    1980   1990    2000     2010




Notes to Figure 5. Upper left: 100 times the log of end-of-quarter values for seasonally adjusted
nonfarm payrolls. Lower left: black plots yt − βˆ0 − βˆ1 yt −8 − βˆ2 yt −9 − βˆ3 yt −10 − βˆ4 yt −11 as a function of t
while red plots yt − yt −8 . Right panels show results when the identical procedure is applied instead to
seasonally unadjusted data.




                                                                         44
Figure 6. Results of applying regression (black) and 8-quarter-change (red) filters to 100 times the log of
components of U.S. national income and product accounts.
                                GDP                                                                     Exports
   20                                                    RANDOM_WALK
                                                                            40                                                    RANDOM_WALK
                                                         REGRESSION                                                               REGRESSION
                                                                            30
   15
                                                                            20
   10
                                                                            10

    5                                                                        0

                                                                            -10
    0
                                                                            -20
    -5
                                                                            -30

   -10                                                                      -40
         1950   1960   1970       1980     1990   2000      2010                  1950   1960   1970       1980     1990   2000      2010


                          Consumption                                                                   Imports
   15                                                    RANDOM_WALK
                                                                            40                                                    RANDOM_WALK
                                                         REGRESSION                                                               REGRESSION
                                                                            30
   10
                                                                            20

    5                                                                       10

                                                                             0
    0                                                                       -10

                                                                            -20
    -5
                                                                            -30

   -10                                                                      -40
         1950   1960   1970       1980     1990   2000      2010                  1950   1960   1970       1980     1990   2000      2010


                              Investment                                                               Government
   50                                                    RANDOM_WALK
                                                                            60                                                    RANDOM_WALK
                                                         REGRESSION                                                               REGRESSION
                                                                            50

                                                                            40
   25
                                                                            30

                                                                            20
    0
                                                                            10

                                                                             0
   -25                                                                      -10

                                                                            -20

   -50                                                                      -30
         1950   1960   1970       1980     1990   2000      2010                  1950   1960   1970       1980     1990   2000      2010




                                                                       45
