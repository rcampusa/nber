                                NBER WORKING PAPER SERIES




                ESTIMATING DISTRIBUTIONS OF TREATMENT EFFECTS
               WITH AN APPLICATION TO THE RETURNS TO SCHOOLING
                       AND MEASUREMENT OF THE EFFECTS
                      OF UNCERTAINTY ON COLLEGE CHOICE

                                            Pedro Carneiro
                                          Karsten T. Hansen
                                          James J. Heckman

                                         Working Paper 9546
                                 http://www.nber.org/papers/w9546


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                     March 2003




The views expressed herein are those of the authors and not necessarily those of the National Bureau of
Economic Research.

©2003 by Pedro Carneiro, Karsten T. Hansen, and James J. Heckman. All rights reserved. Short sections of
text not to exceed two paragraphs, may be quoted without explicit permission provided that full credit
including ©notice, is given to the source.
Estimating Distributions of Treatment Effects with an Application to the Returns to Schooling
and Measurement of the Effects of Uncertainty on College Choice
Pedro Carneiro, Karsten T. Hansen and James J. Heckman
NBER Working Paper No. 9546
March 2003
JEL No. C31

                                           ABSTRACT

This paper uses factor models to identify and estimate distributions of counterfactuals. We extend
LISREL frameworks to a dynamic treatment effect setting, extending matching to account for
unobserved conditioning variables. Using these models, we can identify all pairwise and joint

treatment effects. We apply these methods to a model of schooling and determine the intrinsic
uncertainty facing agents at the time they make their decisions about enrollment in school. Reducing

uncertainty in returns raises college enrollment. We go beyond the “Veil of Ignorance” in evaluating
educational policies and determine who benefits and who loses from commonly proposed
educational reforms.



Pedro Carneiro                                       Karsten T. Hansen
Department of Economics                              Kellogg School of Management
The University of Chicago                            Northwestern University
1126 E. 59th Street                                  2001 Sheridan Rd.
Chicago, IL 60637                                    Evanston, IL 60208
pmcarnei@midway.uchicago.edu                         karsten-hansen@kellogg.northwestern.edu

James J. Heckman
Department of Economics
The University of Chicago
and The American Bar Foundation
1126 East 59th Street, Chicago, IL 60637
and NBER
jjh@uchicago.edu
1     Introduction

The recent literature on evaluating social programs finds that persons (or firms or institutions) respond to the same policy
diﬀerently (Heckman, 2001). The distribution of responses is usually summarized by some mean. A variety of means can be
defined depending on the conditioning variables used. Diﬀerent means answer diﬀerent policy questions. There is no uniquely
defined “eﬀect” of a policy.
    The research reported here moves beyond means as descriptions of policy outcomes and determines joint counterfactual
distributions of outcomes for alternative interventions. From knowledge of the joint distributions of counterfactual outcomes
it is possible to determine the proportion of people who benefit or lose from making a particular policy choice (taking or not
taking particular treatments), the origin and destination outcomes of those who change states because of policy interventions
and the amount of gain (or loss) from various policy choices by persons at diﬀerent deciles of an initial prepolicy distribution.
Our work builds on previous research by Heckman and Smith (1993, 1998) and Heckman, Smith and Clements (1997) that
uses experimental data to bound or point-identify joint counterfactual distributions. We extend the analysis of Aakvik,
Heckman and Vytlacil (1999, 2003) who use factor models to identify counterfactual distributions to consider indicators for
unobservables, implications from choice theory and to exploit the benefits of panel data.
    From the joint distribution of counterfactuals, it is possible to generate all mean, median or other quantile gains, to identify
all pairwise treatment eﬀects in a multi-outcome setting, and to determine how much of the variability in returns across persons
comes from variability in the distributions of the outcome selected and how much comes from variability in opportunity
distributions. Using the joint distribution of counterfactuals, it is possible to develop a more nuanced understanding of
the distributional impacts of public policies, and to move beyond comparisons of aggregate overall distributions induced by
diﬀerent policies to consider how people in diﬀerent portions of an initial distribution are aﬀected by public policy. We
extend the analysis of DiNardo, Fortin and Lemieux (1996) to consider self-selection as a determinant of aggregate wage and
earnings distributions.
    Using our methods, we reanalyze the model of Willis and Rosen (1979), who apply the Roy model (1951) to the economics
of education. We extend their model to account for uncertainty in the returns to education. We also distinguish between
present value income maximizing and utility maximizing evaluations of schooling choices and we estimate the net non-
pecuniary benefit of attending college. We use information on the choices of agents to determine how much of the ex post
heterogeneity in the return to schooling is forecastable at the time agents make their schooling choices. This procedure
extends the analysis of Flavin (1981) to a discrete choice setting. This allows us to identify the eﬀect of uncertainty on
schooling choices. Ex ante, there is a great deal of uncertainty regarding the returns to schooling (in utils or dollars). Ex
post, 8% of college graduates regret going to college.
    The plan of this paper is as follows. Section 2 presents the essential idea underlying the identification strategy used in
this paper and how our approach is related to previous work. Section 3 presents a general policy evaluation framework for
counterfactual distributions with multiple treatments followed over time. The strategy pursued in this paper is based on
using low dimensional factors to generate distributions of potential outcomes. We show how our methods generalize the
method of matching by allowing some or all of the variables that generate the conditional independence assumed in matching



                                                                 1
to be unobserved by the analyst. Section 4 introduces the factor models used in this paper. Section 5 presents proofs of
semiparametric identification. Section 6 applies the analysis to extend the Rosen-Willis model of college choice to account
for uncertainty and to estimate the information about future earnings available to agents at the time schooling decisions are
made. Section 7 reports estimates of the distributions of returns to schooling, the components unforecastable by the agent
at the time schooling decisions are made, and the nonpecuniary net benefits from attending college. Section 8 applies our
estimates to evaluate a reform of the U.S. educational system. It illustrates the power of our method to lift the commonly
invoked Veil of Ignorance and move beyond aggregate distributions of outcomes to understand the consequences of public
policies on persons in various parts of the overall distribution. Section 9 concludes. We first provide a brief introduction to
the literature to put this paper in context.



2     Estimating Distributions of Counterfactual Outcomes
In order to place the approach used in this paper in the context of an emerging literature on heterogeneous treatment eﬀects,
it is helpful to motivate our work by a two outcome, two-treatment cross section model. For simplicity, in this section it is
assumed that the outcomes are continuous random variables. The analysis in the rest of this paper is for multiple treatments
and multiple outcomes followed over time and the outcomes may be discrete, continuous or mixed discrete-continuous.
    The agent can experience one of two possible counterfactual states with associated outcomes (Y0 , Y1 ). The states are
schooling levels in our empirical analysis. X is a determinant of the counterfactual outcomes (Y0 , Y1 ); S = 1 if the agent is in
state 1; S = 0 otherwise. The observed outcome is Y = SY1 + (1 − S)Y0 . There may be an instrument (or set of instruments)
Z such that (Y0 , Y1 ) ⊥
                       ⊥ Z | X and P r(S = 1 | Z , X ) depends on Z for all X (i.e., it is a nontrivial function of Z), i.e., Z is
in the choice probability but not the outcome equation. (A ⊥⊥ B | C means A is independent of B given C). We show below
that such a Z is not strictly required in our approach. The standard treatment eﬀect model assumes policies (Z ) that aﬀect
choices of treatment but not potential outcomes (Y0 , Y1 ). General equilibrium eﬀects are ignored.5
    The goal of our analysis is to recover F (Y0 , Y1 | X). As noted in Heckman (1992), Heckman and Smith (1993, 1998)
and Heckman, Smith, and Clements (1997), from this joint distribution it is possible to estimate the proportion of people
who benefit (in terms of gross gains) from participation in the program (P r(Y1 > Y0 | X)), gains to participants at selected
levels of the no treatment (F (Y1 − Y0 | Y0 = y0 , X)) or treatment distribution (F (Y1 − Y0 | Y1 = y1 , X)), the option value of
social programs, and a variety of other questions that can be answered using distributions of potential outcomes including
conventional mean treatment eﬀects and quantiles of the gains (Y1 − Y0 ) for those who receive treatment.
    The problem of recovering joint distributions arises because we observe Y0 if S = 0 and Y1 if S = 1. Thus we know
F (Y0 | S = 0, X), F (Y1 | S = 1, X) but not F (Y0 | X) or F (Y1 | X). In addition, we do not observe the pair (Y0 , Y1 ) for
anyone. Thus we cannot directly obtain F (Y1 , Y0 | S, X) from the data. Additional information is required to identify the
joint distribution.
    There are, then, two separate problems. The first is a selection problem. From F (Y1 | S = 1, X) and F (Y0 | S = 0, X),
under what conditions can one recover F (Y1 | X) and F (Y0 | X), respectively? The second problem is how to construct the
joint distribution F (Y0 , Y1 | X) from the two marginals.
    Assuming that the selection problem can be surmounted, classical probability results due to Fréchet (1951) and Hoeﬀding

                                                                2
(1940) show how to bound F (Y1 , Y0 | S, X) using the marginal distributions. In practice these bounds are very wide, and
the inferences based on the bounding distributions are often not useful.6
      The traditional (pre-1985) approach to program evaluation in economics assumed that F (Y0 , Y1 | X) is degenerate because
conditional on X, Y1 and Y0 are deterministically related:


(1)                                                    Y1 ≡ Y0 + ∆(X) .


This is the “common eﬀect” assumption that postulates that conditional on X, treatment has the same eﬀect on everyone.
From the means of F (Y0 | S = 0, X) and F (Y1 | S = 1, X) corrected for selection, one can identify E(∆( X)) = E(Y1 |
X)−E(Y0 | X). ( See Heckman and Robb, 1985; 1986 (reprinted 2000) for a variety of estimators for this case and for discussion
of more general cases.) Heckman and Smith (1993, 1998) and Heckman, Smith, and Clements (1997) relax this assumption
by assuming perfect ranking across diﬀerent counterfactual outcome distributions. Assuming absolutely continuous and
                                                                                                        −1
strictly increasing marginal distributions, they postulate that quantiles are perfectly ranked so Y1 = F1,X (F0,X (Y0 )) where
F1,X = F1 (y1 | X) and F0,X = F0 (y0 | X). This assumption generates a deterministic relationship which turns out to be
the tight upper bound of the Fréchet bounds. An alternative assumption is that people are perfectly inversely ranked so the
                                                          −1
best in one distribution is the worst in the other: Y1 = F1,X (1 − F0,X (Y0 )) . This is the tight Fréchet lower bound. More
generally, one could associate quantiles across distributions more freely. Heckman, Smith and Clements (1997) use Markov
transition kernels which stochastically map quantiles of one distribution into quantiles of another.     They define a pair of
Markov kernels M (y1 , y0 | X) and M̃ (y0 , y1 | X) such that
                                                            Z
                                            F1 (y1 | X) =       M (y1 , y0 | X)dF0 (y0 | X)

                                                            Z
                                           F0 (y0 | X) =        M̃ (y0 , y1 | X)dF1 (y1 | X).

Allowing these operators to be degenerate produces a variety of deterministic transformations, including the two previously
presented, as special cases of a general mapping. Diﬀerent (M, M̃ ) pairs produce diﬀerent joint distributions.7 These
stochastic or deterministic transformations supply the missing information needed to construct the joint distributions.
      A perfect ranking (or perfect inverse ranking) assumption is convenient. It generalizes the perfect-ranking, constant-
shift assumptions implicit in the conventional literature. It allows us to apply conditional quantile methods to estimate the
distributions of gains.8 However, it imposes a strong and arbitrary dependence across distributions. Our empirical analysis
shows that this assumption is at odds with data on the returns to education.
      An alternative approach to constructing joint distributions due to Heckman and Honoré (1990), Heckman (1990) and
Heckman and Smith (1998) uses the economics of the model by assuming that


(2)                                                    S = 1(µs (Z) ≥ es )


where µs (Z) is a mean net utility, Z ⊥
                                      ⊥ es , and “1” is a logical indicator (= 1 if the argument is valid; = 0 otherwise). In



                                                                    3
addition they assume that


                                               Y1     = µ1 (X) + U1,       E(U1 ) = 0

                                               Y0     = µ0 (X) + U0 ,     E(U0 ) = 0


                 ⊥ (X, Z).9 In the special case where S = 1( Y1 ≥ Y0 ) (the Roy model), Heckman and Honoré (1990) present
where (U1 , U0 ) ⊥
conditions on µ1 , µ0 and X such that F (U1, U0 ) and µ1 (X), µ0 (X) and hence F (Y0 ,Y1 |X) are identified from data on choices
(S), characteristics (X) and observed outcomes Y = SY1 + (1 − S)Y0. Buera (2002) extends their approach to non-separable
models with weaker exclusion restrictions.
   Heckman (1990) and Heckman and Smith (1998) consider more general decision rules of the form (2) under the assumption
that (Z, X) ⊥
            ⊥ (U0 , U1 , es ) and the further conditions (i) µs (Z) is a nontrivial function of Z conditional on X and (ii) full
support assumptions on µ1 ( X), µ0 (X) and µs (Z). They establish nonparametric identification of F (U0 , es ), F (U1 , es ) up to
a scale for es and µ1 (X) , µ0 (X) and µs (Z).10 Hence, under their assumptions, they can identify F ( Y0 , S | X, Z) and F
( Y1 , S | X, Z) but not the joint distributions F ( Y0 , Y1 | X) or F ( Y0 , Y1 , S | X, Z) unless the U0 , U1 , es dependence is
restricted.
   Aakvik, Heckman and Vytlacil (1999, 2003) build on Heckman (1990) and Heckman and Smith (1998) by postulating a
factor structure connecting (U0 , U1 , es ). Our work builds on their analysis so we describe its essential idea. Suppose that the
unobservables follow a factor structure:


                                           U0 = α0 θ + ε0, U1 = α1 θ + ε1 , es = αs θ + εs


        ⊥ (ε0 , ε1 , εs ) and the ε’ s are mutually independent. In their setup, θ is a scalar. θ can be an unobservable trait
where θ ⊥
like ability or motivation that aﬀects all outcomes. Because the factor loadings, α0 , α1 , αs , may be diﬀerent, the factors may
aﬀect outcomes and choices diﬀerently. Recall that one can identify F (U0 , es ) and F (U1 , es ) under the conditions specified
in Heckman and Smith (1998) and generalized in Theorems 1-3 below. Thus, one can identify COV (U0 , es ) = α0 αs σ 2θ and
COV (U1 , es ) = α1 αs σ 2θ assuming finite variances and assuming E(θ) = 0, E(θ2 ) = σ 2θ . With some normalizations (e.g.,
σ 2θ = 1, αs = 1), under conditions specified in Section 5, we can nonparametrically identify the distribution of θ and the
distributions of ε0 , ε1 , εs (the last up to scale). With the α1 , α0 , αs , and the distributions of θ, ε0 , ε1 , εs in hand, we can
construct the joint distribution F (Y0 , Y1 | X).11
   This paper builds on this basic idea and extends it to a more general setting. We consider a model with multiple factors,
multiple treatments and multiple time periods. Outcome measures may be discrete or continuous. We follow the psychometric
literature by adjoining measurement equations to outcome equations to pin down the distribution of θ. With this framework
we can estimate all pairwise treatment eﬀects in a multiple outcome setting. We also consider the benefits for identification
of having access to imperfect measurements on vector θ which are observed for all persons independent of their treatment
status. This model integrates the LISREL framework of Jöreskog (1977) into a model of discrete choice and a model of
multiple treatment eﬀects. We develop this model in Section 4 after presenting a more general framework for counterfactuals
and treatment eﬀects in a multi-outcome, possibly dynamic setting.


                                                                  4
3       Policy Counterfactuals for the Multiple Outcome Case
This section defines policy counterfactuals for the multiple treatment case. For specificity, think of states as schooling levels
and diﬀerent ages as periods in the life cycle. Associated with each state s (schooling level) is a vector of outcomes at age a
for person ω ∈ Ω (a set of indices) with elements:


(3)                                             Ys,a (ω)      s = 1, ..., S̄, a = 1, ..., Ā


where there are S̄ states and Ā ages. Associated with each person ω is a vector X(ω) of explanatory variables.
      The ceteris paribus eﬀect (or individual treatment eﬀect) of a move from state s0 at age a00 to state s at age a is


(4)                                         ∆((s, a), (s0 , a00 ), ω) = Ys,a (ω) − Ys0 ,a00 (ω).


Since it is usually not possible to observe the same person in both s and s0 , analysts often focus on estimating various
population level versions of these parameters for diﬀerent conditioning sets.12 In this paper, we estimate distributions of
potential outcomes and parameters derived from these distributions, including the Average Treatment Eﬀect:


                                        ATE ((s, a) , (s0 , a00 ), x) = E(Ys,a − Ys0 ,a00 | X = x)


and the Marginal Treatment Eﬀect, the average gain from moving from s0 to s for those on the margin of indiﬀerence between
s and s0 . We are interested in determining the joint distributions of the counterfactual distributions of ∆((s, a), (s0 , a00 ), x)
for diﬀerent conditioning sets.
      Associated with each treatment or state (schooling choice) is a choice equation associated with a level of lifetime utility:
Vs (ω), s = 1, ..., S̄. Utilities are assumed to be absolutely continuous. Agents select treatment states (schooling levels) s̃ to
maximize utility:


(5)                                                   s̃ = argmax {Vs (ω)}S̄s=1 .
                                                                 s


Associated with choices are explanatory variables Z(ω). A distinctive feature of the econometric approach to program eval-
uation is that it evaluates policies both in terms of objective outcomes (the Ys,a (ω)) and in terms of subjective outcomes
(the utilities of the agents making the choices). Both subjective and objective evaluations are useful in evaluating policy.
Choice theory is also used to guide and rationalize specific choices of estimators. It enables us to separate out variability
from intrinsic uncertainty, as we demonstrate below.
      This framework is suﬃciently general to encompass a variety of choice processes including sequential dynamic programming
models13 and ordered choice models,14 as well as more general unordered choice models. We let Ds = 1 if treatment s is
                          _                            P
                                                       S̄
selected. Since there are S mutually exclusive states,    Ds = 1.
                                                           s=1




                                                                     5
      In this notation, the marginal treatment eﬀect for choices s and s0 is


(6)                                M T E (a, V s,s0 ) = E(Ys,a − Ys0 ,a | Vs = Vs0 = V s,s0 ≥ Vj , j 6= s, s0 ).
                                  Vs →Vs0


It is the average gain of going from s0 to s at age a for persons indiﬀerent between s and s0 given that s and s0 are the best
two choices in the choice set, and that their level of utility is V s,s0 .
      Aggregating over choices s0 = 1, ..., S̄; s0 6= s, we may define the marginal treatment eﬀect over all origin states as



                                            S̄
                                            X                          ³                                                                                  ´
(7) M T Es (a, {V s,s0 }Ss0 =1,s0 6=s ) =            M T E (a, V s,s0 ) f (Vs , Vs0 | Vs = Vs0 = V s,s0 ≥ Vj , j 6= s, s0 )/ψ(a, {V s,s0 }Ss0 =1,s0 6=s )
                                                     Vs →Vs0
                                            s0 =1
                                            s0 6=s


the weighted average of the pairwise marginal treatment eﬀects from all source states to s (at a given level of utility V s,s0 )
with the weights being the density of persons at each relevant margin for specified values of utility where

                                ³                         ´ XS
                               ψ a, {V s,s0 }Ss0 =1,s0 6=s =   f (Vs , Vs0 | Vs = Vs0 = V s,s0 ≥ Vj , j 6= s, s0 )
                                                                    s0 =1
                                                                    s0 6=s



is a normalizing constant (the population density of people at all margins given a and {V s,s0 }Ss0 =1,s0 6=s ), assumed to be
positive.
      We next present a framework for estimating the distributions of the treatment eﬀects and the parameters derived from
them, which allows us to estimate the parameters defined in this section as well as other parameters. To simplify notation,
we suppress the ω argument in the rest of the paper.



4       Factor Structure Models
The strategy adopted in this paper identifies the distribution of counterfactuals by postulating a low dimensional set of
factors θ so that, conditional on them, and the covariates X and Z, the Ys,a and Vs0 are jointly independent for all s, s0 and
a. The distributions of the components of θ are nonparametrically identified under conditions specified below. With these
distributions in hand, it is possible to construct the distribution of counterfactuals. Under the conditions specified in Section
5, it is possible with low dimensional factors to nonparametrically identify the counterfactual distributions and to estimate
all of the treatment eﬀects in the literature suitably extended to multidimensional versions.
      Throughout this paper we analyze a separable-in-the-errors system. Thus preferences can be described by




(8)                                                            Vs = µs (Z) − es        s = 1, .., S.


It is conventional to assume that µs (Z) = Z 0 βs with s = 1, .., S. Linear approximations to value functions are advocated


                                                                                 6
by Heckman (1981) and Eckstein and Wolpin (1989) and are developed systematically in Geweke, Houser and Keane (2001).
Our approach does not require linearity but critically relies on separability between the deterministic portion of the model
and the errors es . Following Heckman (1981), Cameron and Heckman (1987, 1998), and McFadden (1984), write


(9)                                                                       es = α0s θ + εs


                                                             ⊥θ 0 , 6= 0 ) and define εs = (ε1 , ..., εS )
where θ is a K × 1 vector of mutually independent factors (θ ⊥


(10)                                                   ⊥εs
                                                      θ⊥         εs ⊥
                                                                    ⊥εs0         ∀s, s0 = 1, ..., S̄ and s 6= s0

                                                                                     S
E(θ) = 0;        E(εs ) = 0;       Dk = 1 if Vk is maximal in {Vs (Z)}s=1 .15
                                     ∗
      Potential outcomes at age a, Ys,a , are stochastically dependent among each other and the choices only through their
dependence on the observables X,Z and the factors θ :

                                                                  ∗
(11)                                                            Ys,a = µs,a (X) + α0s,a θ + εs,a


where E(εs,a ) = 0.
      Potential outcomes are separable in observables and unobservables. A linear-in-parameters version writes µs,a (X) =
X 0 βs,a . Define εY =(ε1,1 , ..., ε1,A , ..., εs,1 , ..., εs,A , ..., εS,A )




(12)                                                                              ⊥εY
                                                                                 θ⊥



(13)                                                          εs,a ⊥
                                                                   ⊥εs0 ,a00 ;     ∀ s 6= s0 ;   ∀ a, a00


and


(14)                                                  εs,a ⊥
                                                           ⊥εs0 ;      ∀ s0 , s = 1, ..., S̄;    a = 1, ..., Ā.



(15)                                                                         ⊥(θ, εY , εs )
                                                                       (Z, X)⊥

      ∗
The Ys,a may be vector valued.
                                                                                                      ∗
      When the outcome is continuous, the observed value corresponds to the latent variable (Ys,a = Ys,a ). When the outcome
                                                      ∗
is discrete (e.g., employment status), we interpret Ys,a in (11) as a latent variable. In that case, Ys,a is an indicator function
           ∗
Ys,a = 1(Ys,a ≥ 0). Tobit and other censored cases can be accommodated. Other mixed discrete-continuous cases can be
handled in a conventional fashion.16
      One motivation for the factor representation is that agents may observe components of θ (or variables that span those


                                                                                     7
components) and act on them (e.g., choose schooling levels), while the econometrician does not observe θ. Below, we present
methods for testing whether agents observe some or all components of θ. Conditional on θ and X, the potential outcomes
are independent. If (12)-(15) accurately describe the data generating process, we obtain the conditional independence
assumptions used in matching (see, e.g., Cochrane and Rubin, 1973; Rosenbaum and Rubin, 1983).
   In matching it is assumed that Ys,a ⊥⊥Ds | X, Z, θ for all s.17 From this assumption, we can identify ATE from the right
hand side of the following expression for continuous observed outcomes, which can be constructed if θ is observable:


                      E(Ys,a − Ys0 ,a | X, Z, θ) = E(Ys,a | X, Z, θ, Ds = 1) − E(Ys0 ,a | X, Z, θ, Ds0 = 1).


In this case treatment on the treated, ATE and MTE are the same parameter conditional on θ, X and Z (Heckman, 2001;
Aakvik, Heckman and Vytlacil, 2003). Our framework diﬀers from matching by allowing the factors that generate the
conditional independence that underlies matching to be unobserved by the analyst. In this sense, our approach is more
robust than matching. The price for this robustness is the assumed independence between θ and (X, Z).
   Factor structure models are notorious for being identified by arbitrary normalization and exclusion restrictions. To
reduce this arbitrariness and render greater interpretability to estimates obtained from our model, we adjoin a measurement
system to choice equations (8) and outcome system (11). Various measurements can be interpreted as indicators of specific
factors (e.g., test scores may proxy ability). Having measurements on the factors also facilitates identifiability under weaker
assumptions as we demonstrate in Section 5. However, measurements are not strictly required for identification in our model.
Outcome, measurement, and choice equations are interchangeable sources of identification in a sense that we make precise in
Section 5.
   Consider a system of L measurements on the K factors, initially assumed to be for continuous outcome measures:


(16)                                    M1 = µ1 (X ) + β 11 θ1 + .... + β 1K θK + εM
                                                                                   1
                                         ..
                                          .

                                       ML     = µL (X ) + β L1 θ1 + .... + β LK θK + εM
                                                                                      L



εM = (εM         M       M
                                                      ⊥εM , θ ⊥
       1 , ..., εL ), E(ε ) = 0 and where we assume θ ⊥       ⊥εs , εs ⊥
                                                                       ⊥(εM , εY ), εM ⊥εM
                                                                                     i ⊥ j ∀i 6= j, and i, j = 1, . . . , L.

For interpretability, we assume θi ⊥
                                   ⊥θj , ∀i 6= j, i, j = 1, ..., K. We develop the case with discrete measurements on latent
continuous variables in Section 5. One can think of the outcome measures as an s-dependent measurement system.            The
measures (16) are the same across all s.
   Measurement system (16) allows for fallible measures of outcomes. Thus in our schooling choice analysis we are not com-
mitted to the infallibility of test scores as measurements of ability. Measurement system (16) allows us to proxy unobservables
accounting for measurement error and hence enables us to improve on the proxy procedure of Olley and Pakes (1996) which
assumes no measurement error.

                                                      Choice Equations

   Our analysis applies to both ordered discrete choice models and unordered choice models as analyzed by Cameron and


                                                                8
Heckman (1998) and Hansen, Heckman and Mullen (2001). In this paper, we focus attention on a new ordered choice model.
Other choice models can easily be accommodated in our framework and richer models are a source of additional identifying
information.18
   For an ordered discrete choice model, let utility index I be written as

                                                                                      X
(17)                                   I = ϕ(Z) + εW , εW = γ 0 θ + εI , σ 2W = γ 0           γ + σ2I
                                                                                          θ

                            P
where E(ε2I ) = σ2I , and    θ   is the covariance matrix of θ. A linear-in-parameters version which is the one developed in this
paper writes ϕ(Z) = Zη. Choices are generated by index ϕ (Z) falling in various intervals.


(18)                                       D1   = 1 if − ∞ < I ≤ c1

                                           Ds   = 1 ⇔ cs−1 < I ≤ cs           s = 2, ..., S̄ − 1

                                           DS   = 1 if cS−1 < I < ∞


where c0 = −∞. It is required that cs ≥ cs−1 for all s ≥ 2. This is a special case of random utility model (8) in which states
are ordered and pairwise contrasts possess a special structure.19
   We can parameterize the cs to be functions of state-specific regressors, e.g., cs = Qs ρs where we restrict cs ≥ cs−1 . We
could also follow a suggestion in Heckman, LaLonde and Smith (1999) and incorporate one sided shocks ν s and work with
stochastic thresholds c̃s in place of cs : c̃s = cs + ν s , s = 1, ..., S − 1 where ν s ≥ ν s−1 and ν s ≥ 0.20
   Conditioning on Qs = qs , s = 1, ..., S̄, and assuming that the Support(Z | Qs = qs , s = 1, ..., S̄) = Support(εW ), we
can apply the conditions presented in Cameron and Heckman (1998) to identify the distribution of FεW , η, c1 , ...., cS−1 up
to scale. We can nonparametrically identify cs (Qs ) over the support of Qs under conditions specified in Theorem 2 below.
Unlike the case of the more general unordered discrete choice model (see Elrod and Keane, 1995; Ben Akiva et al., 2001),
without further restrictions on the distribution of εW , we cannot identify the factors generating εW using only choice data.
Hansen, Heckman and Mullen (2003) present an analysis parallel to the one given here for a multinomial probit model. In
that model, the distributions of factors can be identified from choice data.


4.1    Models for Factors

Factor models are notorious for being identified through arbitrary assumptions about how factors enter in diﬀerent equations.
This led to their disuse after their introduction into economics by Jöreskog and Goldberger (1975), Goldberger (1972),
Chamberlain and Griliches (1975) and Chamberlain (1977a, b).
   The essential identification problem in factor analysis is clearly stated by Anderson and Rubin (1956). If there are L
measurements on K mutually independent factors arrayed in a vector θ, we may write outcomes G in terms of latent variables
θ as


(19)                                                       G = µ + Λθ + ε



                                                                   9
where G is L × 1, θ ⊥
                    ⊥ ε, µ is an L × 1 vector of means, which may depend on X, θ is K × 1, ε is L × 1 and Λ is L × K.
εi ⊥
   ⊥ εj , i, j = 1, ., L, i 6= j. At this point, ε is a general notation which will be linked to specific ε’s in Section 5. Even if
θi ⊥
   ⊥ θj , i 6= j, i, j = 1.., K, the model is underidentified. As we shall see, the G in this paper is a more general system than
the system based solely on measurements invariant across states M so we distinguish (16) and (19). It will include M as well
                               ∗
as state dependent outcomes (Ys,a ) and the indices generating choice equations.
     Using only the information in the covariance matrices, as is common in factor analysis,


(20)                                                    COV (G) = ΛΣθ Λ0 + Dε


where Σθ is a diagonal matrix of the variances of the factors, and Dε is a diagonal matrix of the “uniqueness” variances. We
observe G but not θ or ε, and we seek to identify Λ, Σθ and Dε . Without some restrictions, this is clearly an impossible task.
Conventional factor-analytic models make assumptions to identify parameters. The restriction that the components of θ are
independent is one restriction that we have already made, but it is not enough. The diagonals of COV (G) combine elements
of Dε with parameters from the rest of the model. Once those other parameters are determined, the diagonals identify Dε .
                                         L(L−1)
Accordingly, we can only rely on the        2     non-diagonal elements to identify the K variances (assuming θi ⊥
                                                                                                                 ⊥ θj , ∀i 6= j),
and the L × K factor loadings. Since the scale of each θi is arbitrary, one factor loading devoted to each factor is normalized
to unity to set the scale. Accordingly, we require that

                                          L (L − 1)
                                                                        ≥        (L × K − K)             +        K
                                                                                                                 |{z}
                                          | {z2 }                                |    {z   }
                                                                             Numb er of unrestricted Λ       Variances of θ
                           Numb er of oﬀ-diagonal covariance elements


so
                                                               L ≥ 2K + 1

is a necessary condition for identification.
     The strategy pursued in this paper is transparent and assumes that there are two or more elements of G devoted exclusively
to factor θ1 , and at least three elements of G that are generated by factor θ1 , two or more other elements of G devoted only
to factors θ1 and θ2 , with at least three elements of G that depend on θ1 and θ2 , and so forth. This strategy is motivated by
our access to psychometric and longitudinal data. Test scores may only proxy ability (θ1 ). Other measurements may proxy
only (θ1 , θ2 ). Measurements on earnings from panel data may proxy (θ1 , θ2 , θ3 ), etc.
     Order G under this assumption so that we get the following pattern for Λ (we assume that the displayed λij are not zero):




                                                                        10
                                                                                                                     
                                                                                                 ..
                                                     1       0        0          0               .   ... ...   0     
                                                                                             ..                      
                                                                                                                     
                                                   λ21       0        0          0            .      ... ...   0     
                                                                                                                     
                                                                                              ..                     
                                                   λ31       1        0          0             .     ... ...   0     
                                                                                                                     
                                                                                           ..                        
                                                   λ41      λ42       0          0          .        ... ...   0     
                                                                                                                     
                                                                                            ..                       
                                                                                                                     
                                                   λ51      λ52       1          0           .       ... ...   0     
(21)                                       Λ =
                                                                                         ..
                                                                                                                      .
                                                                                                                      
                                                   λ61      λ62      λ63         0        .          ... ...   0     
                                                                                                                     
                                                                                          ..                         
                                                                                                                     
                                                   λ71      λ72      λ73         1         .         0   ...   0     
                                                                                                                     
                                                                                        ..                           
                                                   λ81      λ82      λ83     λ84         .           0   ...   0     
                                                                                                                     
                                                                                       ..                            
                                                    ...      ...      ...    ...        .            ... ...   ...   
                                                                                                                     
                                                                                               ..                    
                                                    λL,1     λL,2    λL,3         ...            .    ... ... λL,K

Assuming nonzero covariances


                                         COV (gj , gl ) = λj1 λl1 σ 2θ1 , l = 1, 2; j = 1, ..., L; j 6= l.


In particular


                                                           COV (g1 , g ) = λ 1 σ2θ1

                                                           COV (g2 , g ) = λ 1 λ21 σ 2θ1 .


Assuming λ      1   6= 0, we obtain
                                                                  COV (g2 , g )
                                                                                = λ21 .
                                                                  COV (g1 , g )
Hence, from COV (g1 , g2 ) = λ21 σ 2θ1 , we obtain σ 2θ1 , and hence λ 1 ,                            = 1, . . . , L. We can proceed to the next set of two
measurements and identify


                                      COV (gl , gj ) = λl1 λj1 σ 2θ1 + λl2 λj2 σ 2θ2 , l = 3, 4; j ≥ 3; j 6= l.


Since we know the first term on the right hand side by the previous argument, we can proceed using COV (gl , gj ) − λl1 λj1 σ 2θ1
and identify the λj2 , j = 1, ..., L using the previous line of reasoning (some of these elements are fixed to zero). Proceeding
in this fashion, we can identify Λ and Σθ subject to diagonal normalizations. This argument works for all but the system for
the K th and final factor. Observe that for all of the preceding factors there are at least three measurements that depend on
θj , j = 1, . . . , K − 1, although only two of the measurements need to depend solely on θ1,..., θK−1 . To obtain the necessary
three measurements for the K th and final factor, we require that there be at least three outcomes with measurements that
depend on θ1 , . . . , θ K .



                                                                             11
    Knowing Λ and Σθ , we can identify Dε . Use of dedicated measurement systems for specific factors and panel data helps
to eliminate much of the arbitrariness that plagued factor analysis in its 1970’s introduction in economics. While many
other restrictions on the model are possible, the one we adopt has the advantage of simplicity and interpretability in many
contexts.21
    Our analysis uses a version of (19), coupled with the exclusion restrictions exemplified in (21), to identify the joint
distributions of counterfactuals. We extend conventional factor analysis in three ways. First, following Heckman (1981) and
                                                                                                                        ∗
Muthen (1984), we allow the G to include latent index functions like I (associated with the choice equations) or like Ys,a
as well as their manifestations (the random variables they generate). Thus the G may include discrete or censored random
variables generated by latent random variables. We can identify components associated solely with the discrete case only
up to unknown scale factors — the familiar indeterminacy in discrete choice analysis. Choice indices, measurements and state
contingent outcomes are all informative on θ. The factor analysis in this paper is conducted on the latent continuous variables
that generate the manifest outcomes. Second, we extend factor analysis to a case with counterfactuals where certain variables
are only observed if state s is observed. This extension enables us to identify the full joint distribution of counterfactuals.
Third, we prove nonparametric identification of the distributions of θ and ε, and do not rely on any normality assumptions.



5     Identification of Semiparametric Factor Models with Discrete Choices and
      Discrete and Continuous Outcomes
In order to establish identification, we need to be clear about the raw data with which we are working. For each set of s-
                                                            e s = (M, Ys , Ds ) where Ys is a vector of state contingent outcomes.
contingent potential outcomes, there is a system like (19): G
                     es are of two types: (a) continuous variables and (b) discrete or censored random variables, including
Outcome variables in G
binary strings associated with durations (e.g., unemployment). When the random variables are discrete or censored, we work
                                                                               e s and the index functions generating the
with the latent variables generating them. We array the continuous portions of G
discrete portions into Gs .
    Let M c denote the continuous measurements, and let Ysc be the continuous counterfactual outcomes. Let M d be the
discrete components of M , while Ysd are the discrete components of Ys . Table 1 defines the variables used in our analysis.
    Under separability the continuous variables can be written as


                                                    Mc     = µcm (X) + Um
                                                                        c


                                                     Ysc   = µcs (X) + Usc .


Associated with the discrete variables are latent continuous variables


                                                    M ∗d   = µdm (X) + Um
                                                                        d


                                                    Ys∗d   = µds (X) + Usd




                                                               12
       d
where Um , Usd are assumed to be continuous.22 The indicator variable is generated by latent variable I as defined in (17).
       The data used for the factor analysis are Gs = (M c , M ∗d , Ysc , Ys∗d , I). For simplicity, in this paper we assume
that the “discrete” variables are in fact binary valued. Extensions to censored random variables and to binary strings are
                                                               e s when Ds = 1. For each s, we have a system of outcome
straightforward and are developed in a later paper. We observe G
variables. While the outcomes are s-dependent, the measurements are observed independently of the value assumed by Ds .
       The distinction between measurements (M ) whose values do not depend on the value assumed by Ds , and the state
contingent outcomes Ys that depend on the state s that is observed, is essential. There is no selection bias in observing M
                                                        P
                                                        S
but in general there is selection bias in observing Y =   Ds Ys .
                                                             s=1
       M, Y, and Ds , s = 1, .., S all contain information on θ. The information from M is easier to access, and traditional factor
analysis is based on such measurements. Nonetheless, the identification of counterfactual states does not require M . If M is
available, however, the interpretation of θ is more transparent.
       Before turning to our factor analysis, we first establish conditions under which we can identify the joint distribution of
M , M ∗d , Ysc , Ys∗d , I, which constitute the data for the factor analysis. To understand the basic ideas, we break this task into
   c
                                                              ¡         ¢
three parts: (a) identification of the joint distribution of M c , M ∗d ; (b) identification of the parameters in choice system (17)
                                                                      ¡                      ¢
and (18) and (c) identification of the full joint distribution of M c , M ∗d , Ysc , Ys∗d , I . This full distribution is subsequently
factor analyzed.
       We assume that
      ¡ c    d
                               ¢
(A-1) Um  , Um , Usc , Usd , εW have distribution functions that are absolutely continuous with respect to Lebesgue measure with
         means zero23 with support Ucm × Udm × Ucs × Uds × EW with upper and lower limits being Ūm
                                                                                                  c     d
                                                                                                    , Ūm , Ūsc , Ūsd , εW and
         U cm , U dm , U cs , U ds , εW , respectively, which may be bounded or infinite. Thus the joint system is measurably separable
         (variation free).24 We assume finite variances.25 The cumulative distribution function of εW is assumed to be strictly
         increasing over its full support (εW , εW ).

                                       c    d
                ⊥ (U, εW ) where U = (Um
(A-2) (X, Z, Q) ⊥                        , Um , Usc , Usd ), where Q is a vector of state-specific regressors Q= (Q1 , . . . , QS ) .

       We denote by “~” normalized values where the normalizations in our context are usually standard deviations of latent
index errors. We first consider identification of the joint distribution of M . Our results are contained in Theorem 1.
                                                                            ¡ c    d
                                                                                     ¢
Theorem 1 From data on F (M | X), one can identify the joint distribution of Um , Um   (the latter component only up
to scale), the function µdm (X) is identified and µcm (X) is identified over the support of X (up to scale) provided that the
following assumptions, in addition to the relevant components of (A-1) and (A-2), are invoked.

(A-3) Order the discrete measurement components to be first. Suppose that there are Nm,d discrete components, followed by
                                                  ³                               ´         ¡ d                  ¢
      Nm,c continuous components. Assume Support µd1,m (X) , . . . , µdNm,d ,m (X) ⊇ Support U1,m         d
                                                                                                  , ..., UN m ,m
                                                                                                                  .
                                            ³                                 ´
       Conditions (A-1) and (A-3) imply that µd1,m (X) , . . . , µdNm,d ,m (X) is measurably separable (variation free) in all of its
coordinates when “⊇” is replaced by “= .”

(A-4) For each l = 1, ..., Nm,d µdl,m (X) = Xβ dl,m .

                                                                   13
(A-5) The X lives in a subset of RNX . There exists no linear proper subspace of RNX having probability 1 under FX , the
       distribution function of X.

Proof: See Appendix A.
    Condition (A-4) is conventional (See Cosslett, 1983, or Manski, 1988). Weaker conditions are available using the analysis of
Matzkin (1992,1993). Support condition (A-3) appears in Cameron and Heckman (1998) and Aakvik, Heckman and Vytlacil
(1999). The easiest way to satisfy it is to have exclusions: one continuous component in µdl,m (X) that is not an argument in
the others. But that is only a suﬃcient condition. Even without exclusion, this condition can be satisfied if there are enough
continuous regressors in X and the µdl,m (X) have a full rank Jacobian - with respect to the derivatives of the continuous (X)
variables. Intuitively, if the rank condition is satisfied, we can hold µdl,m (X) at µ̄dl,m and vary the other arguments. Formally,
                                                                                                                e d , into a Nm,d
this rank condition requires that if we array the coeﬃcients of the continuous variables coeﬃcients of β dl,m , β l,m
                                                                                     n d oNm,d
by NX matrix, where NX is the number of continuous components of X, that Rank β l,m    e         ≥ Nm,d . This requires Nm,d
                                                                                                  l=1
continuous variables. It also requires that the coeﬃcients are linearly independent. If the number of continuous components
                                                                          d
is NX < Nm,d , we can only identify NX components of the distribution of Um . We can trace out the distribution of the
latent variables even if the X are not of full rank, so (A-5) is not strictly required. Observe that we can identify the joint
distribution of the Um even if all components of β are not identified because of a failure of a rank condition. See Cameron
and Heckman (1998), Aakvik, Heckman and Vytlacil (1999) or Hansen, Heckman and Mullen (2003) for more discussion of
this case of identification without conventional exclusion restrictions.
    We next turn to identification of the generalized ordered discrete choice model (17). This extends the proof in Cameron
and Heckman (1998) by parameterizing the cut points. A more general version of this model appears in Hansen, Heckman
and Mullen (2001).

Theorem 2 For the relevant subsets of the conditions (A-1), and (A-2) (specifically, assuming absolute continuity of the
distribution of εW with respect to Lebesgue measure and εW ⊥
                                                           ⊥ (Z, Q)), and the additional assumptions:

(A-6) cs (Qs ) = Qs ηs , s = 1, ..., S, ϕ(Z) = Z 0 β

(A-7) (Q1 , Z) is full rank (there is no proper subspace of the support (Q1 , Z) with probability 1). The Z contains no intercept.
                                                                             ¡ Q ¢
(A-8) Qs for s = 2, . . . , S is full rank (there is no proper subspace of    R s with probability 1).

(A-9) Support (c(Q1 ) − ϕ(Z)) ⊇ Support (εW )

       Then the distribution function FεW is known up to a scale normalization on εW and cs (Qs ), s = 1, ...s̄, and ϕ(Z) are
       identified up to a scale normalization.

Proof: See Appendix A.
    Our choice system can be made nonparametric using the type of restrictions introduced in Matzkin, although we eschew
that generality here. Matzkin and Lewbel (2002) weaken (A-6) generalizing the analysis of Matzkin (1992) assuming that
the cs are constants.



                                                                  14
      We next turn to the identification of the joint system (M c , M ∗d , Ysc , Ys∗d , I). The data for each choice system (including
  the data on choice probabilities) generate the left hand side

                           ¡                                                                 ¢
  (22)                   Pr M c ≤ mc , M ∗d ≤ 0, Ysc ≤ ysc , Ys∗d ≤ 0|Ds = 1, X, Z, Qs , Qs−1 Pr(Ds = 1|Z, Qs , Qs−1 )

                Z    mc −µ cm (X) Z −e
                                     µdm (X)
                                                     Z   ysc −µ cs (X)   Z    µ d (X)
                                                                             −e         Z    cs (Qs )−ϕ(Z)
                                                                                                  σW              ³                  ´
                                                                                                                    c ed    c ed        c ed        esd de
            =                           d                                    d
                                                                                                                 f Um, Um, Us, Us, e
                                                                                                                                   εW dUm dUm dUsc dU    εW .
                Uc                  e
                                    U                  U cs              e
                                                                         U
                                                                                            cs−1 (Qs−1 )−ϕ(Z)
                                      m                                                            σW


                                                                                                             d
                                                                  e ) and the joint distribution of (U c U
                                                    e dm (X) (= X β
      From Theorem 1 we know µcm (X) (= Xβ cm ) and µ                                                    ed
                                                                    m                                 m, m ). From Theorem
                    cs (Qs )−ϕ(Z)           Qs ηs −Z 0 β
  2, we know             σW         =           σW       ,    s = 1, ..., S and the coeﬃcients η s , β and the distribution FeεW . Notice that cs (Qs ) ≥
  cs−1 (Qs−1 ) is a requirement of the ordered choice model. We maintain the following assumptions:
                  ³                    ³                                  ´´
(A-10) Support −e    µdm (X), −e
                               µds (X), cs (Qsσ)−ϕ(Z) − cs−1 (Qs−1 )−ϕ(Z)              d
                                                                             ⊇Support(Um , Usd , e                   e W ).
                                                                                                 εW ) = (Udm × Uds × E
                                                W              σW

(A-11) There is no proper linear subspace of (X, Z, Qs , Qs−1 ) with probability one so the model is full rank.

      As a consequence of (A-6) and (A-10) we can find values of Qs , Qs−1 , Q̄s , Qs−1 respectively so that


                                                                             lim Pr (Ds = 1|Z, Qs , Qs−1 ) = 1.
                                                                       Qs →Q̄s
                                                                     Qs−1 →Q
                                                                                 s−1




      In these limit sets (which may depend on Z), under the stated conditions (A-1) — (A-11), we can identify the joint
  distribution of (M c , M ∗d , Ysc , Ys∗d ), s = 1, . . . , S using an argument parallel to the one used to prove Theorem 1. These limit
  sets produce S diﬀerent joint distributions (corresponding to each value of s) but do not generate joint distributions across
  the s (i.e., the joint distribution of M c , M ∗d , Ysc , Ys∗d across s values). However, M is common across these systems. Using
  the dependence of M and Ys , s = 1, . . . , S on a common θ we can sometimes identify the joint distribution. See Carneiro,
  Hansen and Heckman (2001) for an example. Thus with a measurement system M we do not strictly require information on
  the choice index I to identify the model.
      Following an argument of Heckman (1990), Heckman and Honoré (1990) and Heckman and Smith (1998), we can identify
  µcs (X) up to an additive constant without passing to the limit set where Pr(Ds = 1|Z, Qs , Qs−1 ) = 1. This is not possible
  for the identification of µ̃ds (X) because there is no counterpart to the variation in ysc for the discrete component. This is the
  content of the following theorem which combines the key ideas of Theorems 1 and 2 to produce an identification theorem for
  the general case.

                                                                                                                 e dm (X),
  Theorem 3 Under assumptions (A-1), (A-2), (A-4), (A-6), (A-7),(A-8),(A-9),(A-10) and (A-11), µcm (X), µcs (X), µ
  e ds (X), ϕ
  µ                                                                                           c
            e (Z), cs (Qs ) s = 1, ..., S − 1 are identified as is the joint distribution F (Um     d
                                                                                                , Ũm , Usc , Ũsd , e
                                                                                                                     εW ).

  Proof: See Appendix A.
      As noted in the discussion following Theorem 1, without standard exclusion restrictions we may only be able to identify
  subcomponents of the joint distribution if NX < Nm,d where NX is the number of continuous regressors. Note that the
          eds,l may only be defined over their supports. Under an additional rank or variation-free condition on the regressors we
  µcs,l , µ
  recover these functions everywhere over the support of X.

                                                                                                 15
5.1    Factor Analysis

The thrust of Theorems 1-3 is that under the stated conditions we know the joint distributions of (Us , Um , ε̃W ) s = 1, ..., S
           ¡        ¢
where Us = Usd , Usc . We factor analyze them under assumptions like those invoked in matrix (21) with two or more of these
elements dependent solely on θ1 , an additional two or more elements dependent solely on (θ1 , θ2 ) and so forth but at least
three final elements dependent on θK . There are a total of A × R outcomes in each state where R is the number of outcome
measures in each state at each age (e.g., wages, employment, occupation), there are M non-state-contingent measurements
and ε̃W is a scalar. Thus L in (21) is (A × R) + M + 1 in dimension for each system s, s = 1, ..., S.
   We write the unobservables in factor structure form


                                      Us,a   = α0s,a θ + εs,a with s = 1, ..., S a = 1, ..., A

                                       Um    = α0m θ + εm with m = 1, ..., Nm

                                       ε̃W   = γ 0 θ + εI .


The αs,a may be diﬀerent across s-states so that each s system may depend on diﬀerent elements of θ. The αm are not,
nor is the γ. There may be multiple measurements of outcomes so in principle αs,a may be a matrix and εs,a a vector of
mutually independent components. Our empirical analysis is for the vector case.
   The choice of how to select the blocks of (21) may appear to be arbitrary, but in many applications there are natural
orderings. Thus in the empirical work reported below we estimate a two factor model. We have a vector of five test scores
that proxy latent ability (θ1 ). The state contingent outcomes (earnings) equations and choice equations plausibly depend on
both θ1 and θ2. In many applications there are often natural allocations of factors to various measurements. However, to
avoid arbitrariness a carefully reasoned defense of any allocation is required. We now formalize identification in this system.

Theorem 4 Under the normalizations on the factor loadings of the type in (21) for one system s under the conditions
of Theorems 1-3, given the normalizations for the unobservables for the discrete components and given at least 2K + 1
measurements (Y, M, I), the unrestricted factor loadings and the variances of the factors (σ 2θi , i = 1, ..., K) are identified for
all systems.

Proof: The proof is implicit in the discussion surrounding equation (21). ¥
Observe that since the σ 2θi , i = 1, ..., K are identified in one system, normalizations of specific factor loadings to unity are
only required in that system since we can apply the knowledge of these variances to the other systems.26 Thus for the other
systems (values of the state other than s) we do not need to normalize any factor loading to unity.
   We can also nonparametrically identify the densities of the uniquenesses and the factors. This follows from mutual
independence of the θi , i = 1, ..., K and an application of Kotlarski’s Theorem (1967). We first state Kotlarski’s Theorem
and then we apply it to our problem.
   Write ({Um }N            A
                                  εW ) in vector form as T s . Order the vectors so that the first B1 (≥ 2) elements depend only
               m=1 , {Us,a }a=1 , e
                m



on θ1 , the next B2 − B1 (≥ 2) elements depend on (θ1 , θ2 ) and so forth. Let T1s and T2s be the first two elements of T s . (This
is purely a notational convenience). We order the elements of T s so that the first block depends solely on θ1 , (assuming that


                                                                16
there are B1 such measurements) the second block depends solely on θ1 , θ2 (there are B2 − B1 such measurements) and so
forth, following the convention established in equation (21). We require B1 ≥ 2, B2 − B1 ≥ 2, and BK − BK−1 ≥ 3.

Theorem 5 If
                                                                    T1s = θ1 + v1

and
                                                                    T2s = θ1 + v2

and θ1 ⊥
       ⊥v1 ⊥
           ⊥v2 , the means of all three generating random variables are finite, E(v1 ) = E(v2 ) = 0, and the conditions of
Fubini’s theorem are satisfied for each random variable, and the random variables possess nonvanishing ( a.e.) characteristic
functions, then the densities of (θ1 , v1 , v2 ) , g(θ1 ), g1 (v1 ), g2 (v2 ), respectively, are identified.

Proof : Kotlarski (1967). See also Rao (1992). ¥
    Applied to our context, consider the first two equations of T and suppose that the components depend only on θ1 . We
use our notation for the factor loadings to write


                                                      T1s    = λs11 θ1 + εs1 where λs11 = 1

                                                      T2s    = λs21 θ1 + εs2 where λs21 6= 0.


Here we use a notation associating the subscript of εsi with its position in the T s vector. Applying Theorem 4, we can identify
λs21 (subject to the normalization λs11 = 1).27 Thus we can rewrite these equations as




                                                                 T1s   = θ1 + εs1
                                                                T2s
                                                                       = θ1 + ε∗,s
                                                                               2 ,
                                                                λs21

where ε∗,s  s   s                                                                                                s            ∗,s
       2 = ε2 /λ21 . Applying Kotlarski’s Theorem, we can nonparametrically identify the densities g(θ 1 ), g1 (ε1 ) and g2 (ε2 ).

Since we know λs21 we can identify g(εs2 ). Let B1 denote the number of measurements (elements of T s ) which depend only
on θ1 . Proceeding through the first B1 measurements, we can identify g(εsi ), i = 1, ..., B1 .
    Proceeding to equations B1 + 1 and B1 + 2 (corresponding to the first two measurements in the next set of equations that
depend on θ1 and θ2 ), we may use the normalization adopted in Theorem 4 to write




                                                TBs 1 +1    = λsB1 +1,1 θ1 + θ2 + εsB1 +1

                                                TBs 1 +2    = λsB1 +2,1 θ1 + λsB1 +2,2 θ2 + εsB1 +2 .


Rearranging, we may write these equations as




                                                                          17
                                                    s
                                                   TB+1 − λsB1 +1,1 θ1    = θ2 + εsB1 +1
                                                    s
                                                   TB+2 −  λsB1 +2,1 θ1
                                                                          = θ2 + ε∗,s
                                                       λsB1 +2,2                  B1 +2



                   εsB1 +2
where ε∗,s
       B1 +2 =    λsB +2,2 ,   and the εsB1 +1 and ε∗,s
                                                    B1 +2 are mutually independent. Hence by Theorem 5, we can identify densities
                     1

g(θ2 ), g(εsB1 +1 ), g(εsB1 +2 ). Exploiting the structure (21), we can proceed sequentially to identify the densities of θ, g(θi ), i =
1, ..., K and the uniqueness, g(εsi ) for all the components of vector T s . For the components of εsi corresponding to discrete
measurements, we do not identify the scale. Armed with knowledge of the densities of the θi and the factor loadings for
other values of s, we can apply standard deconvolution methods to nonparametrically identify the uniqueness of the εi ’s for
the other systems. Thus we can nonparametrically identify the error terms for the model. Notice that in principle we can
estimate separate distributions of the θi for each s system and thus can test the hypothesis of equality of these distributions
across systems.
    The essential idea in this paper is to obtain identification of the joint counterfactual distributions through the dependence
across s of Ys = (Ysd , Ysc ) on the common factors that also generate M or I. In this sense measurements and choices are
both sources of identifying information, and can be traded oﬀ in terms of identification. We next apply our framework to a
well-posed economic model.



6     Generalizing The Willis-Rosen Model
We revisit Willis and Rosen’s application of the Roy model (1979) to the economics of education, adding uncertainty,
nonpecuniary net returns to schooling and identifying counterfactual distributions of gross and net returns. In this paper the
outcomes are utility outcomes, present value outcomes and rates of return.
    Suppose that agents cannot lend or borrow and possess log preferences (utility = ln C, where C is consumption). Suppose
that agents are choosing between high school and college so S = 2. The utility of attending college is

                                                                 XA
                                                                      ln Ya1
                                                       V (1) =               a − ln P
                                                                 a=0
                                                                     (1 + ρ)

where ln P is the “cost” of going to school. These include tuition costs and the psychic benefits from working in sector 1
(relative to sector 0). Thus costs may be negative. ρ is a subjective rate of time preference. The utility of completing only
high school is
                                                                    XA
                                                                         ln Yao
                                                          V (0) =
                                                                    a=0
                                                                        (1 + ρ)a

where Ya1 and Ya0 are earnings from high school and college, respectively, at age a. The psychic costs or benefits in logs for
high school are normalized to zero. We can only identify relative psychic “costs” or benefits.




                                                                     18
   Latent variables and costs are generated by a factor structure. The equations are:

                                                     ¡ ¢0
                                 ln Yaj    = µj (X) + αja θ + εja                   j = 0, 1, a = 1, ..., A.
                                                     ¡ ¢0
                                  ln P     = µP (Z) + αP θ + εP .
                                                                                                          ·³ ´                              ¸
                                                                                                                  I
In addition we have measurements on test scores M = µM (x ) + α0M θ + εM , where θ ⊥
                                                                                   ⊥                        εji,a          ,1j=0 ,A
                                                                                                                                  a=0 , ε P
                                                                                                                                              .
                                                                                                                     i=1
   The agent makes decisions about schooling under uncertainty about diﬀerent components of the model. Iθ is the infor-
mation set. The expected value V of going to college is :
                                                                                                                    
                                                                    P
                                                                    A                                 0
                                                                          µ1a (X)−µ0a (X)+(α 1a −α 0a ) θ+ε1a −ε0a
                                                                                        (1+ρ)a                      
                            V = E (V (1) − V (0) | Iθ ) = EIθ      a=0                                              .
                                                                            £                    ¤
                                                                           − µP (Z) + α0P θ + εP

                                    ¡         ¢
   If future innovations in earnings ε1a , ε0a , a = 0, .., A are not known at the time schooling decisions are made but innova-
tions in costs are known, we may write the agent’s preference function as

                             Ã A                        ! " A ¡          ¢      #
                              X µl (X) − µo (X)            X α1a − α0a 0
                                  a        a
                         V =               a    − µP (Z) +             a
                                                                              0
                                                                           − αP EIθ (θ) − εP .
                              a=0
                                    (1 + ρ)                a=0
                                                                (1 + ρ)

As we shall see, this assumption about agent knowledge of future innovations in earnings is testable. Assume that σ P =
¡    ¡ ¢¢ 1
 V ar εP 2 < ∞. Then

                               Ã A                              !       Ã A ¡       ¢                     !
                     V     1    X µ1 (X) − µ0 (X)                        X α1a − α0a 0                         1           εP
                                      a         a
                        =                       a    − µP (Z)       +                      a    −   α0P          EIθ (θ) −
                     σP   σP    a=0
                                          (1 + ρ)                          a=0
                                                                                  (1 + ρ)                     σP           σP

            V
Ds = 1 if   σP   > 0 ; Ds = 0 otherwise.
   Specifying alternative information sets (Iθ ) and examining the resulting fit of the model to data, we can determine which
information sets agents act on. Exact econometric specifications are presented in Section 7. We test whether agents act
on components of θ that also appear in outcome equations realized after the choices are made. The estimated dependence
between schooling choices and subsequent realizations of earnings enables us to identify the components in the agent’s
information set at the time schooling decisions are being made. This extends the method of Flavin (1981) and Hansen,
Roberds and Sargent (1991) to a discrete choice setting. If agents do not act on these components, then those components
are intrinsically uncertain at the time agents make their schooling decisions unless nongeneric cancellations occur.28 Because
we can identify the joint distributions of unobservables, we can answer questions Willis and Rosen could not such as: (1)
How highly correlated are latent skills (utilities) across sectoral choices? (2) How much intrinsic uncertainty do agents face?
(3) How important is uncertainty for explaining schooling choices? (4) What fraction of the population regrets its ex ante
schooling choice ex post? We can also separate out net psychic components of the returns to schooling (the ln P ) from
monetary components.
   Observe that as a consequence of the log specification of preferences (including the additive separability of the θ and ε),
mean preserving spreads in εja , θ and εP produce no change in mean utility. The probability of selection Ds = 1 is also



                                                                  19
invariant to mean preserving spreads in εja but not for θ and εP since their variance enters the choice probability if these
components are known to the agent.
    In addition, a mean preserving spread in ln Y is not the same as a mean preserving spread in Y . Mean preserving spreads
in Y have an eﬀect on utility since E (Y ) = eµ E (eε ). Define the residual from the mean as H, H = eµ eε − eµ E (eε ) so
              ³ ¡ ¢                ´
                                 2
V ar (H) = e2µ E e2ε − [E (eε )] . A mean preserving spread keeps the mean of Y fixed at constant k = E (Y ) = eµ E (eε ).
    For a perturbation in the variance of ε that changes ε to ∆ε, and defining f (ε) as the density of ε, locally 0 = dµ +
 R                               R
[ εeε f (ε)dε]                  [ εeε f (ε)dε]
   E(eε )      d∆   so dµ = −      E(eε )     Moreover, because E (ε) = 0 and εeε is convex increasing in ε, the derivative
                                               d∆.
                                                      σ2    ¡ ¢          2
                                                                                          ³ 2        2
                                                                                                       ´        σ2
is positive. In a log normal example, E (eε ) = e 2 , E e2ε = e2σ , V ar (H) = e2µ e2σ − eσ , k = eµ e 2 , ln k = µ +
σ2            d(σ 2 )
 2 , (−dµ) =    2     so an increase in the variance is equivalent to a decrease in the mean utility. We consider the eﬀects of
mean preserving spreads on both mean log utility and on the probability that V is positive (college is selected). We now
turn to the empirical analysis of this paper.



7     Empirical Results
We use the NLSY data for white males described in Appendix B and augmented with the PSID data to estimate the Willis-
Rosen Model. Main features of the data are presented in Table 2. We focus on two schooling decisions; graduating from a
four year college or graduating from high school. We thus abstract from the full multiplicity of choices of schooling. This is
clearly a bold simplification but it allows us to focus on the main points of this paper.
    As a measurement system (M ) for cognitive ability we use five components of the ASVAB test battery (arithmetic
reasoning, word knowledge, paragraph composition, math knowledge and coding speed). We dedicate the first factor (θ1 ) to
the ability measurement system and exclude the other factors from that system (recall the normalizations in equation (21)).
We include family background variables as additional covariates in the ASVAB test equations (the µM (X)).
    To simplify the empirical analysis, we divide the lifetimes of individuals into two periods. The first period covers ages 19
to 29, and the second covers ages 30 to 65. We compute annual earnings by multiplying the hourly wage by hours worked
each year for each individual.29 We impute missing wages and project earnings for the ages not observed in the NLSY data
using the procedure described in Appendix B. The NLSY data do not contain information on the full life cycle of earnings.
We project the missing NLSY earnings using estimates of lifetime earnings from the PSID data.
    Tables 2a-b present the sample statistics. They show that while college graduates have higher earnings than high school
graduates, all of the gain to attending college comes after age 30. College graduates also have much higher test scores and
come from better family backgrounds than high school graduates. They are more likely to live in locations where a college
is present and where college tuition is lower.
    In the notation of Section 5, S̄ = 2 (two choices), R̄ = 1 (there is one outcome per person, earnings), M̄ = 5 (there are five
test scores that are generated solely by θ1 ) and Ā = 2 (there are two periods in the life cycle). In addition, there is utility index
I. The test scores depend solely on θ1 . The outcomes and index are allowed to depend on (θ1 , θ2 ). Since K = 2, assuming
non-zero factor loadings, we satisfy the conditions for identification presented in Theorem 4. We have five measurements
generated solely by θ1 . There are three measurements generated by θ1 and θ2 for each schooling level. (Outcomes and



                                                                  20
choices are defined for each choice system). Exclusion restrictions are given in Table 2c along with specification of each of the
equations. Tuition and family background identify the parameters of the schooling equations. Local labor market variables
identify the parameters of utility equations. Assuming that test scores are continuous outcomes, no exclusions are needed for
identification of the test score equations and their distribution.
   In this section, to facilitate the exposition we denote the college state (choice 1) by c, while high school (choice 0) is
denoted by h. We model log earnings (utility of earnings) at each age as:


(23)                      ln Ya,s = δ a,s + X 0 β a,s + η1,s ∗ experiencea + η2,s ∗ experience2a + α0a,s θ + εa,s


where Ya,s is earnings in period (age) a if the schooling level is s, X is a vector of covariates, θ is a vector of factors and η1,s
and η2,s are calculated by the procedure described in Appendix B. We compute the present value of log earnings (lifetime
utility) in the first period (ages 19 to 29) and in the second period (ages 30 to 65). Let V1,s be the period 1 gross utility
of achieving schooling level s, and V2,s be the period 2 gross utility of obtaining schooling level s. Using (23), we write the
gross utilities as


                                                  V1,s     = δ̄ 1,s + X 0 β̄1,s + ᾱ01,s θ + ε̄1,s

                                                  V2,s     = δ̄ 2,s + X 0 β̄2,s + ᾱ02,s θ + ε̄2,s .


These are the outcome equations for the model that we estimate. To see this, notice that

                      XA1
                            ln Ya,s
          V1,s   =
                      a=19
                           (1 + ρ)a
                   XA1
                        δ a,s + X 0 βa,s + α0a,s θ + εa,s + η1,s ∗ experiencea + η 2,s ∗ experience2a
                 =                                                 a
                   a=19
                                                            (1 + ρ)
                    A1
                                                                                         " A             #      A1
                   X    δ a,s + X 0 βa,s + η1,s ∗ experiencea + η 2,s ∗ experience2a      X 1
                                                                                                 α0a,s         X       εa,s
                 =                                          a                         +                a   θ +
                   a=19
                                                   (1 +  ρ)                               a=19
                                                                                               (1 + ρ)         a=19
                                                                                                                    (1 + ρ)a
                 = δ̄ 1,s + X 0 β̄1,s + ᾱ01,s θ + ε̄1,s




                                                                          21
where


                                         A1      = 29

                                            ρ = 0.03 (the prespecified discount rate)
                                                     XA1
                                                          δ a,s + η 1,s ∗ experiencea + η 2,s ∗ experience2a
                                        δ̄ 1,s   =                                    a
                                                     a=19
                                                                               (1 + ρ)
                                                      A1
                                                      X     βa,s
                                        β̄1,s    =
                                                     a=19
                                                          (1 + ρ)a
                                                      A1
                                                      X     α0a,s
                                       ᾱ01,s    =
                                                     a=19
                                                          (1 + ρ)a
                                                      A1
                                                      X      εa,s
                                        ε̄1,s    =                a
                                                     a=19
                                                          (1 + ρ)

and terms for the second period of data (30-65) are defined analogously. The “cost” or psychic net return of going to college
is written as:
                                                           ln P = δ P + Z 0 γ + α0P θ + εP .

These “costs” can be negative as they entail both psychic and tuition components. Assuming that the agents know X, Z, θ
and εP , the criterion for the choice of schooling is:


           V     = E (V1,c + V2,c − V1,h − V2,h |X, θ) − E(ln P |Z, X, θ,εP )

                 = δ̄ 1,c + X 0 β̄ 1,c + ᾱ01,c θ + δ̄ 2,c + X 0 β̄2,c + ᾱ02,c θ − δ̄ 1,h − X 0 β̄1,h − ᾱ01,h θ − δ̄ 2,h − X 0 β̄2,h − ᾱ02,h θ

                   −δ P − Z 0 γ − α0P θ − εP
                   ¡                                      ¢     ¡                              ¢
                 = δ̄ 1,c + δ̄ 2,c − δ̄ 1,h − δ̄ 2,h − δ P + X 0 β̄1,c + β̄2,c − β̄ 1,h − β̄2,h − Z 0 γ
                      ¡                                       ¢
                   + ᾱ01,c + ᾱ02,c − ᾱ01,h − ᾱ02,h − α0P θ − εP .


Individuals go to college if V > 0. We test (and do not reject) the hypothesis that at the time they make their college decision
agents know their cost function and both factors θ, but not the uniquenesses in the outcome equations. These expressions
can be modified in an obvious way to accommodate other information sets.
   The test score equations have a similar structure. Let Tj be test score j:


                                                            Tj = X 0 ω j + α0test j θ + εtest j


where X is the vector of covariates in the test score equation, and ω j is the covariate vector. The distributions of the θ and ε
are nonparametrically identified under the assumptions supporting Theorems 1-5. In this paper, we assume that each factor




                                                                            22
is generated by a mixture of normals distribution,

                                               Jk
                                               X           ¡                ¢
(24)                                    θk ∼         pk,j φ fk |µj,k , τ j,k ,      k = 1, . . . , K.
                                               j=1


Mixtures of normals with a large enough number of components approximate any distribution of θk and the ε arbitrarily
well (Ferguson, 1983). We assume that the ε’s are normal although in principle they are nonparametrically identified from
the analysis of Theorem 5.
   We estimate the model using Markov Chain Monte Carlo methods as described in Appendix C for 55,000 iterations,
discarding the first 5,000 iterations to allow the chain to converge to its stationary distribution. We retain every 10th of the
remaining 50,000 iterations for a total of 5,000 iterations.30 The Markov Chain mixes well with most autocorrelations dying
out at around lag 25 to 50.
   We estimate models with one factor and with two factors. The estimated coeﬃcients are presented as Tables A1 through
A5 in the supplementary tables on the website (http://lily.src.uchicago.edu/CHH_estimating.html). The two factor model
specifies that the first factor only appears in test scores and choice equations while the second factor appears in all equations.
                                                                                                                          ¡ ¢
No additional factors are necessary to fit our data. Thus we conclude that the innovations in the earnings process εja are
not in the agent’s information set at the time schooling decisions are made. If they were, they would be an additional source
of covariance (i.e., they would generate additional factors) between the choice equation and future earnings. If we use only
one factor that enters in all equations, the quality of the fit is much poorer (results available on request). From this testing
procedure we infer that agents know both components of θ at the time they enroll in college. Figure 1 shows the fit of the
density of the present value of log earnings (or lifetime utility of earnings excluding psychic costs and benefits) for everyone in
the population. It graphs the actual and predicted densities of gross utility. The fit is very good. Results for each schooling
group are available in the supplement on the website and are equally good (χ2 goodness of fit tests are passed overall as
well as for the distribution of utility for each schooling group; see Table A6). In order to achieve this good fit it is necessary
to allow for non-normal factors. Figure 2 shows the density of each of the estimated factors and compares them with a
benchmark normal with the same mean and standard deviation. Neither factor is normal.31 There is evidence of selection
on ability (factor 1), with the less able less likely to attend college. There is weaker evidence of selection on factor 2 (see
graphs A-1 and A-2 posted at the website).
   Tables 3a-b presents the factor loadings in the outcome, choice and measurement equations.32 Both factors have a positive
eﬀect on gross utility for both schooling levels in each period and on schooling attainment (the I). Factor 1 explains most
of the variance in the test score system (see Table 3b) while factor 2 explains most of the variance in the utility outcome
system (see Table 3a). The return to college in terms of gross utility (gross utility diﬀerences) is given by:

                                                     ¡                                 ¢     ¡                               ¢
                  V1,c + V2,c − V1,h − V2,h   =       δ̄ 1,c + δ̄ 2,c − δ̄ 1,h − δ̄ 2,h + X 0 β̄1,c + β̄ 2,c − β̄1,h − β̄2,h
                                                         ¡                                ¢
                                                     + ᾱ01,c + ᾱ02,c − ᾱ01,h − ᾱ02,h θ + (ε̄1,c + ε̄2,c − ε̄1,h − ε̄2,h ) .


Both factors raise returns (see the base of Table 3a). While the second factor explains much more of the variance in utility



                                                                     23
than the first factor, the first factor explains more of the variance in returns than the second factor although it only explains
30% of the variance in returns. We infer that agents know θ (the factors) based on the superior fit of a model that includes
nonzero factor loadings on both factors in the choice equation but not the innovations in outcomes (the ε’s in the outcome
equations) at the time they make their schooling decisions.
   Our results indicate that the unpredictability in gross utility gains (i.e. diﬀerences) of going to college is much larger
than the unpredictability in utility levels. Both factors have a negative impact on “costs” (the factor loadings are positive
in the “cost” or psychic return function). Therefore, both factors positively influence the likelihood of going to college since
both contribute positively to returns and negatively to costs.
   Figure 3 plots the estimated factual and counterfactual gross college utility densities for college graduates and high school
graduates, respectively (see Figure A3 on the website for the corresponding figure for high school utility). College graduates
have the highest level of gross utility both as high school graduates and as college graduates. They also have the highest
gross gains of going to college as demonstrated in Figure 4.33,34 Figure 5 presents the marginal treatment eﬀect as defined
in equation (6) using utils as the outcome. This is the gross gain in utils of going to college as a function of εW , which is an
index of variables that increase the likelihood of enrollment in college. It shows that individuals who are likely to enroll in
college have higher returns to college than those who are unlikely to enroll in college who have lower values of εW . Figure 5
also shows the distribution of εW in the population. Most of the mass of this distribution is at values of εW around 0. Many
individuals have negative gross utility returns (excluding psychic benefits of going to college). Even among those deciding
to go to college, 39.53% would have higher utility (ignoring psychic components) had they not gone to college. There is a
definite falloﬀ in utility gains as college enrollment is expanded to the less college prone. Table 4 confirms Figure 3 and
shows that college graduates have higher potential high school and college utility than high school graduates in high school
and in college (these are gross utilities). Table 5 shows that the gross returns of going to college are higher for those who
choose to go to college. These results are expected given the pattern shown in Figure 4. The returns for attending college
for the average high school graduate are negative. The returns to college for the individual at the margin (V = 0) are about
0.59% of total high school utility. Since these individuals are exactly at the margin, these gains correspond exactly to the
cost they are facing. Once we account for the nonmonetary costs and benefits of going to college (net returns reported in the
bottom two rows of Table 5) the relative returns of going to college become more negative for high school graduates and more
positive for college graduates. Since ln P can be allocated as either a cost or a return, there are two ways to compute returns
depending on whether ln P is treated as a cost (row 2) or a return (row 3). We present two sets of net return estimates
depending on how “costs” or “gains” (ln P ) are allocated. These are bounds since the actual allocation between cost and
benefit is indeterminate.
   The patterns of Figures 3-5 are essentially reproduced for present value of earnings in Figures 6-8. Table 6 shows that
college graduates have earnings 57.6% higher than they would have had (or $608,372 higher, on average) if they did not go
to college. High school graduates have a gross gain of 43% (or $362,987) if they go to college. Notice that even though the
utility gains of going to college are negative for high school graduates, the money returns are positive and large. Table 7
shows that even though 39.66% of the persons going to college would have had a higher utility in high school than in college
(ignoring psychic gains), only 6.9% of this population had higher earnings in high school than in college. Once we account



                                                               24
for psychic benefits, the proportion of college students regretting their decisions is roughly the same whether we measure
regret in present value or utils. This shows the importance of accounting for psychic returns in analyzing schooling choices.
Among high school graduates, 95.90% do not regret not going to college (measured in utils), but 85.26% regret the decision
financially. The marginal treatment eﬀect has the same general shape when present values of earnings are used instead of
gross utility (see Figure 8).
   Table 8 shows the probability of being in decile i of the college potential discounted earnings distribution conditional
on being in decile j of the high school potential earnings distribution. (These are gross earnings.) It shows that neither
an independence assumption across counterfactual outcomes, which is the Veil of Ignorance assumption used in applied
welfare theory, (see, e.g., Sen, 1973) or in aggregate income inequality decompositions (DiNardo, Fortin, and Lemieux, 1996)
nor a perfect ranking assumption, which are sometimes used to construct counterfactual joint distributions of outcomes,
(see e.g. Heckman, Smith, and Clements, 1997 or Athey and Imbens, 2002) are satisfied in the data. There is a strong
positive dependence between potential outcomes in each counterfactual state, but there is not perfect dependence. There are
substantial nonzero elements outside the diagonal. We get similar results for utils (discounted log earnings). See Table A-13
at our website.
   We have already shown that there is a large dispersion in the distribution of utilities, utility returns, earnings, and
earnings returns to college. However, this dispersion can be due to heterogeneity that is known at the time the agent makes
schooling decisions, or it can be due to heterogeneity that is not predictable by the agent at that time. Figure 9 plots the
densities of the unforecastable component of college gross utilities at the time college decisions are made for fixed X values,
under three diﬀerent information sets. (The X are fixed at their means.) The solid line corresponds to the case where the
agent does not know his factor (θ) nor his innovations (the ε’s in the outcome equations). The other two lines correspond
respectively to the cases where the agent knows θ2 only, or both θ1 and θ2 .35 Knowledge of θ2 dramatically decreases the
uncertainty faced, but knowledge of factor 1 (associated with cognitive ability) has only a small eﬀect on the amount of
uncertainty faced by the agent. We obtain a similar figure in terms of gross utility in high school.36 However, even though
knowledge of θ2 reduces dramatically the amount of uncertainty faced in terms of levels of gross utility in each counterfactual
state, it has only a small eﬀect on the uncertainty faced in terms of returns (see Figure 10). Table 9 reports the variances
of gross and net utility and gross and net present value of earnings under diﬀerent information sets of agents. Giving agents
more information (knowledge of factors) reduces the variance in utilities or present values as perceived by agents. However,
reducing uncertainty barely budges the forecast returns to schooling measured in dollars or utils—the message of Figure 10.
Analogous results are obtained for present value of earnings. See Figures A-15 and A-16 posted at our website.
   The fact that a two factor model is adequate to fit the data implies that the agents cannot forecast future shocks of log
earnings (ε̄1,c , ε̄2,c , ε̄1,h , ε̄2,h ) at the time they make their schooling decision. (If they did, they would enter as additional
factors in the estimated model.) Even though the factors (θ) explain most of the variance in levels of utilities, they explain
less than half of the variance in returns, which may lead the reader to conclude that the reason so many college graduates
would have higher gross utility in high school than in college (39%) is because they cannot accurately forecast their returns
of going to college. However this is not the case. As shown in Table 7 once we account for psychic benefits or costs of
attending college (P ) relative to attending high school, only 8% of college graduates regret going to college. This suggests a



                                                                 25
substantial part of the gain to college is due to non-pecuniary components. Furthermore, Table 10 shows that if individuals
had knowledge of (ε̄1,c , ε̄2,c , ε̄1,h , ε̄2,h ), keeping their average expected earnings the same, very few of them would change their
schooling decision. Uncertainty in gains to schooling is substantial but knowledge of this uncertainty has a very small eﬀect
on the choice of schooling because the variance of gains is so much smaller than the variance of psychic costs or benefits, and
it is the latter that drives most of the heterogeneity in schooling decisions. In addition, there is uncertainty about the level
of both college and high school earnings. See the variances reported for each in Table 9. The uncertainty in the return comes
from both sources although the literature emphasizes the uncertainty in college earnings. When conducting this experiment,
we make sure that the average expected earnings are the same because a mean preserving reduction in the uncertainty faced
by the agents in terms of utility is not the same as a mean preserving change in uncertainty in terms of levels of earnings (see
Appendix D).37 In particular a change in the variance of (ε̄1,c , ε̄2,c , ε̄1,h , ε̄2,h ) would not change the expected utility in each
schooling level but would change expected earnings in each schooling level. The numbers reported in Table 10 take this into
account. When agents know their (ε̄1,c , ε̄2,c , ε̄1,h , ε̄2,h ) , they face less uncertainty. Knowing these components is equivalent
to setting ∆ = 0 in the expression at the end of Section 6, a special case of mean preserving shrinkage where variances are
set to zero. The expected utility at each schooling level increases.38



8     Some Evidence on an Educational Reform
Using the estimated model, we evaluate the eﬀect of a full subsidy to college tuition. We move beyond the Veil of Ignorance
which is based on an anonymity assumption and evaluates reforms considering only their overall impact on inequality, to
consider which individuals are benefited by the reform. We consider only partial equilibrium treatment eﬀects and do not
consider the full cost of financing the reforms. Table 4 shows the average lifetime gross utility of participants before the
policy change and Table 5 shows their pre-policy average return to college. These tables compare these levels and returns
with what the marginal participant attracted into schooling by the policy would earn. The marginal person has lower utility
in college and lower returns to college than the average person in college (also see Figure 5). Since the policy aﬀects the
schooling decisions of the individuals at the margin, the policy will produce a decline in the quality of college graduates after
the policy is implemented, since the new entrants are of lower average quality than the incumbents.
    Despite the substantial size of the policy changes we consider, the induced eﬀects on participation are small. The full
tuition subsidy only increases graduation from four-year college by 4%.39 The policies operate unevenly over the deciles
of the initial outcome distribution. Figure 11 shows the proportion of high school people in each decile of the high school
present value of earnings distribution induced to graduate from four-year college by the tuition subsidy. The figure shows
that providing a free college education mostly aﬀects people at the top end of the high school earnings distribution.40 The
policy does not benefit the poor. A calculation based on the Veil of Ignorance using the Gini coeﬃcient would show no eﬀect
of the policy up to two decimal points. Our analysis relaxes the Veil of Ignorance, and lets us study the impact of policies on
persons at diﬀerent positions of the income distribution. It goes beyond the counterfactual simulations used in the inequality
literature (see, e.g. DiNardo, Fortin and Lemieux, 1996) to account for self selection by agents into sectors in response to
policy changes.



                                                                  26
9     Summary and Conclusions
    This paper uses low dimensional factor models to generate counterfactual distributions of potential outcomes. It ex-
tends matching by allowing some of the variables that determine the conditional independence assumed in matching to be
unobserved by the analyst. Semiparametric identification is established.
    We apply our methods to a problem in the economics of education. We extend the Willis-Rosen model to explicitly
account for dependence in potential outcomes across potential schooling states, to account for psychic benefits in the return
to schooling and to measure the eﬀect of uncertainty on schooling choices. We extend the framework of Flavin (1981) and
Hansen, Roberds and Sargent (1991), who estimate the impact of uncertainty on consumption choices to a discrete choice
setting to estimate agent information sets. Our framework extends the inequality decomposition analysis of DiNardo, Fortin,
and Lemieux (1996) to account for self selection in the choice of sectors.
    Our analysis reveals substantial heterogeneity in the returns to schooling, much of which is unpredictable at the time
schooling decisions are made. We also find a substantial non-pecuniary return to college. Although there is substantial
uncertainty in forecasting returns at the time schooling decisions are made, eliminating it has modest eﬀects on schooling
choices. Uncertainty is inherent in both college and high school outcomes at the time schooling decisions are made. In
addition, nonpecuniary factors play a dominant role in schooling choices. The assumption of perfect ranking of potential
outcome across alternative choices is soundly rejected, although potential outcomes are strongly positively correlated.
    We simulate a tuition reduction policy to determine who benefits and loses from it. We go beyond the Veil of Ignorance
to see which persons are aﬀected by the policy. The policy favors those at the top of the income distribution. This simulation
illustrates the power of our method to lift the Veil of Ignorance, and to count the losers and gainers from any policy initiative.




                                                               27
Appendix A                  : Proofs of Theorems
Proof of Theorem 1: The case where M consists of purely continuous components is trivial. We observe M c for each X
and can recover the marginal distribution for each component. Recall that M is not state dependent.
   For the purely discrete case, we encounter the usual problem that there is no direct observable counterpart for µdm (X).
Under (A-1)-(A-5), we can use the analysis of Manski (1988) to identify the slope coeﬃcients β dl,m up to scale, and the marginal
                 d
distribution of Ul,m                                                     d
                     . From the assumption that the mean (or median) of Ul,m is zero, we can identify the intercept in β dl,m .
We can repeat this for all discrete components. Thus coordinate by coordinate we can identify the marginal distributions of
 c ed
Um                    edm (X) , the latter up to scale (“~” means identified up to scale).
   , Um , µcm (X) and µ
   To recover the joint distribution write:

                                                                                         ³                                 ´
                               Pr (Mc ≤ mc , Md = (0, ..., 0) | X) = FU c ,Ue d                              µdm (X)
                                                                                             mc − µcm (X) , −e
                                                                                 m   m



                                                                                                           c ed
by assumption (A-2). To identify FU c ,Ue d (t1 , t2 ) for any given evaluation points in the support of (Um , Um ), we know the
                                            m   m

         edm (X) and using (A-3) we can find an X where µ
function µ                                              edm (X) = t2 . Let x                       edm (b
                                                                           b denote this value, so µ    x) = t2 . In this proof,
t1 , t2 may be vectors. Thus


                                                                        b) = FU c ,Ue d (mc − µcm (b
                                   Pr (Mc ≤ mc , Md = (0, ..., 0) | X = x                          x) , t2 )
                                                                                          m     m




    b c = t1 − µcm (b
Let m               x) to obtain


                                                  b c , Md = (0, ..., 0) | X = x
                                         Pr (Mc ≤ m                            b) = FU c ,Ue d (t1 , t2 )
                                                                                                    m   m




We know the left hand side and thus identify FU c ,Ue d at the evaluation point t1 , t2 . Since (t1 , t2 ) is any arbitrary evaluation
                                                         m   m
                         c ed
point in the support of Um , Um we can thus identify the full joint distribution.¥41
Proof of Theorem 2:                                                        µ                                   ¶
                                                                               c1 (Q1 ) − ϕ (Z)   εW
                                           Pr (D1 = 1 | Z, Q1 ) = Pr                            >
                                                                                     σW           σW
                                                                        c1 (Q1 )−ϕ(Z)                                   εW
Under (A-1), (A-2), (A-6), (A-7) and (A-9), it follows that                  σW          and Fε̃W (where e
                                                                                                         εW =           σW   ) are identified (see Manski,
                                                                                                    c1 (Q1 )−ϕ(Z)                               c1 (Q1 )
1988 or Matzkin 1992, 1993). Under rank condition (A-7), identification of                               σW         implies identification of     σW       and
ϕ(Z)
σW      separately. Write

                                                                 µ                       ¶              µ                      ¶
                                                                     c2 (Q2 ) − ϕ (Z)                       c1 (Q1 ) − ϕ (Z)
                            Pr (D2 = 1 | Z, Q1, Q2 ) = FeεW                                   − FeεW                               .
                                                                           σW                                     σW
From the absolute continuity of e
                                εW and the assumption that the distribution function of e
                                                                                        εW is strictly increasing, we can
write                                        ·                                µ                  ¶¸
                            c2 (Q2 )                                            c1 (Q1 ) − ϕ (Z)      ϕ (Z)
                                     = Feε−1  Pr (D2 = 1 | Z, Q1 , Q2 ) + Fe
                                                                           εW                       +       .
                              σW          W
                                                                                      σW               σW
                        c2 (Q2 )                                                                                    cs (Qs )
Thus we can identify      σW       over its support and, proceeding sequentially, we can identify                     σW , s   = 3, .., S. Under (A-8) we


                                                                          28
can identify ηs , s = 2, .., S.¥ Observe that we could use the final choice (Pr(s = S)) rather than the initial choice to start oﬀ
the proof of identification using an obvious change in the assumptions.
Proof of Theorem 3: From (A-2), the unobservables are jointly independent of (X, Z, Q). For fixed values of (Z, Qs , Qs−1 ),
we may vary the points of evaluation for the continuous coordinates (ysc ) and pick alternative values of X = x
                                                                                                              b to trace out
the vector µc (X) up to intercept terms. Thus we can identify µcs,l (X) up to a constant for all l = 1, ..., Nc,s .(Heckman and
Honoré, 1990). Under (A-2), we recover the same functions for whatever values of Z, Qs , Qs−1 are prespecified as long as
cs (Qs ) > cs−1 (Qs−1 ), so that there is interval of εW bounded above and below with positive probability. This identification
result does not require any passage to a limit argument.
   For values of (Z, Qs , Qs−1 ) such that


                                                       lim      Pr (Ds = 1|Z, Qs , Qs−1 ) = 1.
                                                   Qs →Q̄s (Z)
                                                 Qs−1 →Q     (Z)
                                                          s−1




where Q̄s (Z) is an upper limit and Qs−1 (Z) is a lower limit, and we allow the limits to depend on Z, we essentially integrate
out e
    εW and obtain
                                                   e dm ≤ −Um
                                     Pr(M c ≤ mc , µ        d                       esd ≤ −e
                                                              , Usc ≤ ysc − µc (X), U      µds (X))

We know that this probability can be achieved by virtue of the support condition of assumption (A-10).
                                                                 e ds (X) coordinate by coordinate and we obtain the
   Then proceeding as in the proof of Theorem 1, we can identify µ
                                                                     e d (X). From the assumption of mean or median zero of the
constants in µcs,l (X), l = 1, ..., Nc,s as well as the constants in µ
unobservables. In this exercise, we use the full rank condition on X which is part of assumption (A-11).
                                                                                                                     cs (Qs )−ϕ(Z)
   With these functions in hand, under the full conditions of assumption (A-10) we can fix ysc , ym
                                                                                                  c
                                                                                                     eds , µ
                                                                                                    ,µ     edm ,          σW       ,
cs−1 (Qs−1 )−ϕ(Z)                                                              c     d
       σW           at diﬀerent values to trace out the joint distribution F (Um , Ũm , Usc , Ũsd , e
                                                                                                      εW ).¥42




                                                                        29
   Appendix B: Description of the Data
   We use white males from NLSY79. In the original sample there are 2439 individuals. We consider the information on
these individuals from age 19 to age 35. We discard 663 individuals because they have observations missing for at least one
of the covariate variables we use in the analysis. Tables 2a-b contain a description of the number of missing observations
per variable. For example, we discard 50 individuals because we do not observe whether they were living in the South when
they were 14 years old or not. Then we discard another 6 for not having information on whether they lived in urban area
at age 14, other 5 for not reporting the number of siblings, 221 for not indicating parental education and so on, as described
in Table 2a. We then restrict the NLSY sample to white males with a high school or college degree. We define high school
graduates as individuals having a high school degree or having completed 12 grades and never reporting college attendance.
We define participation in college as having a college degree or having completed more than 16 years in school. We exclude
the oversample of poor whites. Experience is Mincer experience (age-12 if high-school graduate, age-16 for college graduate).
The variables that we include in the outcome and choice equations are number of siblings, parental years of schooling, AFQT,
year of birth dummies, average tuition of the colleges in the county the individual lives in at 17 (we simulate the policy change
by decreasing this variable by $1000 for each individual), distance to the nearest college at 17, average local blue collar wage
in state of residence at 17 (or in 1979, for individuals entering the sample at ages older than 17) and local unemployment
rate in county of residence in 1979. For the construction of the tuition variable see Cameron and Heckman (2001). Distance
to college is constructed by matching college location data in HEGIS (Higher Education General Information Survey) with
county of residence in NLSY. State average blue collar wages are constructed using data from the BLS. For a description of
the NLSY sample see BLS (2001).
   In 1980, NLSY respondents were administered a battery of ten achievement tests referred to as the Armed Forces Vo-
cational Aptitude Battery (ASVAB) (See Cawley, Conneely, Heckman and Vytlacil (1997) for a complete description). The
math and verbal components of the ASVAB can be aggregated into the Armed Forces Qualification Test (AFQT) scores.43
Many studies have used the overall AFQT score as a control variable, arguing that this is a measure of scholastic ability. We
argue that AFQT is an imperfect proxy for scholastic ability and use the factor structure to capture this. We also avoid a
potential aggregation bias by using each of the components of the ASVAB as a separate measure.
   For our analysis, we use the random sample of the NLSY and restrict the sample to 1161 white males for whom we have
information on schooling, several parental background variables, test scores and behavior. Distance to nearest college at each
date is constructed in the following way: Take the county of residence of each individual and all other counties within the
same state. The distance between two counties is defined as the distance between the center of each county. If there exists a
college (2 year or 4 year) in the county of residence where a person lives then the distance to the nearest college (2 year or 4
year) variable takes the value of zero. Otherwise we compute distance (in miles) to the nearest county with a college. Then
we construct distance to nearest college at 17 by using the county of residence at 17. However for people who were older
than 17 in 1979 we use the county of residence in 1979 for the construction of this variable.
   Tuition at age 17 is average tuition in colleges in the county of residence at 17. If there is no college in the county then
average tuition in the state is taken instead. For details on the construction of this variable see Cameron and Heckman
(2001).



                                                               30
   Local labor market variables for the county of residence are computed using information in the 5% sample of the 1980
Census. For each county group in the census we compute the local unemployment rate and average wage for high school
dropouts, high school graduates, individuals with some college and four year college graduates. We do not have this variable
for years other than 1980 so, for each county, we assume that it is a good proxy for local labor market conditions in all the
other years where NLSY respondents are assumed to be making the schooling decisions we consider in this paper.
   We also use the variable log annual labor earnings. We extract this variable from the NLSY79 reported annual earnings
from wages and salary. Earnings (in thousands of dollars) are discounted to 1993 using the Consumer Price Index reported
by the Bureau of Labor Statistics. Missing values for this variable may occur here for two reasons: First, because respondents
do not report earnings for wages/salary, and second, because the NLSY becomes biannual after 1994 and this prevents us
from observing respondents when they reach certain ages. For example, because the NLSY79 was not conducted in 1995, we
do not observe individuals born in 1964 when they are 31 year-old. In this case we input missing values.
   To predict missing log earnings between ages 19 and 35 and extrapolate from age 36 to age 65 we pool NLSY and PSID
data. From the latter, we use the sample of white males that are household heads and that are either high-school or college
graduates according to the definition given above. This produces a sample of 3,043 individuals from PSID. To get annual
earnings, we multiply the reported CPI-adjusted (1993 =100) hourly wage rate by the annual hours worked and divide the
outcome by 1000. Then we take logs to have an NLSY-comparable variable. Similarly to NLSY, we generate the Mincerian
Experience according to the rule given above. We also generate dummy variables for cohorts. The first (omitted) cohort
consists of individuals born between 1896 and 1905, the second consists of individuals born between 1906 and 1915, and so
on up to the last cohort which is made up of PSID respondents born between 1976 and 1985. We pool NLSY and PSID by
merging the NLSY respondents in the PSID cohort born between 1956 and 1965.
   Let Yia denote log earnings of agent i at age a. For each schooling choice s, we model the earnings-experience profile as

                                                                            2
(25)                                           Yia (s) = α + β 0 Xia + β 1 Xia + Dγ + εia



(26)                                                        εia = ηi + via



(27)                                                      via = ρvia−1 + κia


where X is Mincer Experience, D is a set of dummy variables that indicate cohort, η i is the individual eﬀect, and κia is
white noise. In Table A-14 posted at http://lily.src.uchicago.edu/CHH_estimating.html we report the OLS estimates for
α, β 0 , β 1 , γ, ρ based on the pooled data set.




                                                                  31
   Now, let ε̂ia be the estimated residual of the earnings-experience profile. An estimator of the individual eﬀect η i is

                                                                       65
                                                                       X
                                                             1
                                              η̂ i   =                        φia ε̂ia ,
                                                          P
                                                          65
                                                                 φia   a=19
                                                         a=19
                                    where φia        = 1(if individual i is observed at age a)


Then, we can obtain an estimator of via by computing


                                                            v̂ia = ε̂ia − η̂i


Now, given vbia we can run equation (27) and then compute ρ. From this we obtain an estimator of κia according to


                                                          κ̂ia = v̂ia − ρ̂v̂ia−1


   We can then predict earnings for missing observations for ages 19 to 35 and perform the extrapolation from 36 to 65 by
computing for each individual

                                                                   2
                                  Ŷia (s) = α̂ + β̂ 0 Xia + β̂ 1 Xia + Dγ̂ + ε̂ia
                                                                     2
                                             = α̂ + β̂ 0 Xia + β̂ 1 Xia + Dγ̂ + η̂i + ρ̂v̂ia−1 + κ̂ia


Note that to get ε̂ia we do not set κ̂ia equal to zero. Instead, we sample ten draws from its distribution and average them
for each individual, for each time period.
   The next step is to get the present value of log earnings at age 19 for each agent. In order to do it we discount log earnings
at each period using a discount rate of 3%. For identification purposes we then break each individual’s working-life in two
periods. The first one goes from age 19 to age 29. The second period goes from age 30 all the way to age 65. This produces
a panel in which the first observation for each agent is the present value of log earnings from age 19 to 29 and the second
is the present value of log earnings from 30 to 65. This means that lifetime present value of log earnings is just the sum of
these two components. Table 2b contains descriptive statistics for the present value of log earnings for the entire working-life
period and also for the two subperiods used in the analysis.




                                                                       32
    Appendix C: Markov Chain Monte Carlo Simulation Methods
    Due to the complex nature of the likelihood function we will rely on Markov Chain Monte Carlo techniques to estimate
the model. These are computer-intensive algorithms based on designing an ergodic discrete time continuous state Markov
chain with a transition kernel having invariant measure equal to the posterior distribution of the parameter vector ψ, see
Robert and Casella (1999) for details. In particular, we will be using the Gibbs sampling algorithm.44
    We first describe how the Gibbs sampler can be used to estimate models in the general set-up laid out in section 4. Let
ψ s,a be parameters specific to the distribution of outcomes with schooling level s at age a, let ψ m be parameters specific
to the distribution of measurements, let ψ c be parameters specific to the distribution of schooling choice and let ψ θ be
parameters specific to the factor distributions. Let n be the number of observations. Let the outcome matrix over all ages
                                    c      ∗d
with schooling level s be Ys,i = (Ys,i , Ys,i ) and the vector of measurements is M .
    The complete data likelihood for completed schooling level S = s is

                                                   ¡              ¢              Y          ¡                        ¢
                                                  f M, Ys , I, θ|ψ =                       f Mi , Ys,i , Ii , θ i |ψ
                                                                            i:Di,s =1

         £                       ¤
where ψ = ψ s,a , ψ m , ψ c , ψ θ , “i” denotes a subscript for individual i and

                                                                                   Ā
                                                                                   Y
                              f (Mi , Ys,i , Ii , θi |ψ) = f (Mi |ψ m , θi ) ×             f (Ys,a,i |ψ s,a , θi )f (Ii |θ i , ψ c )f (θi |ψ).
                                                                                  a=1


    The complete data posterior is

                                                                                   S̄
                                                                                   Y
                                                f (M, Y, I, θ, ψ|data) ∝                   f (θ,M, Ys∗ , I|ψ)f (ψ)
                                                                                   s=1


where Y = (Y1 , ..., YS̄ ).
    In what follows the conditional posteriors that constitute the transition kernel of the Gibbs sampler will be derived.

                                                                     Choice equations

    Conditional on the factors we have
                                                          (                                      )
                              ¯                               n
                                                              Y
                              ¯
                   f (η, γ, ρ ¯ψ −(η,γ,ρ) , θ ) ∝                 f (Ii |Zi0 η         0
                                                                                 + γ θi , 1)
                                                            i=1
                                                                                                                                                
                                                          Xs̄                                                                                   
(28)                                                                1(ci,j−1 < Ii < ci,j )Di,j }1(ci1 < · · · < cis̄ )f (η, γ)f (ρ)                  .
                                                                                                                                                
                                                              j=1


    This marginal can be factored into two conditionals. Conditional on ρ we

                                                                                 n
                                                                                 Y
                                            f (η, γ|ρ, ψ −(η,γ,ρ) , θ) ∝               f (Ii |Zi0 η + γ 0 θi , 1)f (η, γ).
                                                                                 i=1




                                                                                  33
   This is the posterior for a normal regression model with covariates Zi , θ i and precision fixed at one. With f (η, γ)
multivariate normal this is a multivariate normal distribution.
   The second conditional (for ρ) is

                                                             n X
                                                             Y s̄
                            f (ρ|η, γ, ψ −(η,γ,ρ) , θ) ∝                 1(ci,j−1 < Ii < ci,j )Di,j 1(ci1 < · · · < cis̄ )f (ρ)
                                                             i=1 j=1


We sample ρs one at a time conditional on the ρ1 , . . . , ρs−1 , ρs+1 , . . . , ρs̄ . The conditional for ρs is

                                 ¯                                           Y
                                 ¯
                           f (ρs ¯ρ−s , η, γ, ψ −(η ,γ ,ρ) , θ)      ∝               1(ci,s−1 < Ii < Qis ρs )
                                                                           i:si =s
                                             Y                                               n
                                                                                             Y
(29)                                   ×               1(Qi,s ρs     < Ii < cis+1 )                1(cis−1 < Qis ρs < cis+1 )f (ρ).
                                           i:si =s+1                                         i=1

                                               Q
   As a prior for ρ we choose f (ρ) =             s U(−B, B)        where B = 1000, i.e., a uniform distribution with very large support.
   Let Ks be the number of elements in Qs . We sample ρhs , h = 1, . . . , Ks one at a time. The conditional for ρhs is a uniform
distribution with boundary points which can be derived from a series of inequalities. Without loss we can assume that Qihs
is positive. From (29) it follows that

                                                n           Ii − c̃is        cis−1 − c̃is     o
                                       ρhs > max maxi:si =s           , maxi              , −K ≡ g hs
                                                              Qihs              Qihs

                                                 n             Ii − c̃is        cis+1 − c̃is    o
                                        ρhs < min mini:si =s+1           , mini              , K ≡ ḡhs ,
                                                                 Qihs              Qihs
               PKs
where c̃is =    j=1,j6=h   Qijs ρjs . Hence ρhs is uniform with boundaries (g hs , ḡhs ).
   Conditional on the factors we have

                                                        nY
                                                         n                                    s̄
                                                                                             ©X   ¡                  ¢    ª
                              f (I|ψ −(η,γ,ρ) , θ) ∝            f (Ii |Zi0 η + γ 0 θi , 1)       1 ci,j−1 < Ii < ci,j Di,j .
                                                          i=1                                 j=1


   This factors into n independent truncated normals,

                                                                       n
                                                                       Y
                                           f (I|ψ −(η,γ,ρ) , θ) =            TN(ci,j−1 ,cij ) (Ii |Zi0 η + γ 0 θi , 1).
                                                                       i=1


So we sample Ii , i = 1, . . . , N , one at a time from truncated normals.

                                                             Measurement equations

   The continuous measurement equations are of the form


(30)                                                           0
                                                       Mi,j = Xm,i,j βcm,j + αcm,j 0 θi + εcm,i,j .


Given Xm,i,j , θ i this is a linear regression model. With multivariate normal priors on (β cm,j , αcm,j ) and a gamma prior on the

                                                                                34
precision of εcm,i,j this is in the form of the standard conjugate Bayesian linear regression model, with a conditional normal
distribution for β cm,j given the precision of εcm,i,j and a gamma distribution for the precision conditional on β cm,j .
   Let the last m − m1 elements of the measurement vector M be binary indices generated as


                                                     Mjd = 1(Mj∗d ≥ 0),                  j = m1 + 1, . . . , m.


The parameters in the binary measurements are samples as above with two exceptions. First, a separate step samples the
latent measurements, Mj∗d , as

                                               
                                                      ¡                        d 0
                                                                                           ¢
                                               TN(0,∞) M ∗d |X 0        d                         d
                                       ∗d                  i,j   m,i,j β m,j + αm,j θ i , 1    if Mi,j = 1,
                                      Mi,j   ∼           ¡                                   ¢
                                               
                                               TN(−∞,0) M ∗d |X 0         d     d 0               d
                                                             i,j  m,i,j β m,j + αm,j θ i , 1   if Mi,j = 0.

Second, the precision is not sampled but fixed at one.

                                                                   Outcome equations

   Let Ys,a be the outcome vector at age a with schooling level s. Suppose both employment and wage outcomes are modeled.
      c                            d                                     ∗,d
Let Ys,a be the wage outcome and Ys,a the employment outcome. Also let Ys,a  be the latent employment index. By the
factor structure assumption we have
                                                              c      ∗,d            c          ∗,d
                                                         f (Ys,a , Ys,a  |θ) = f (Ys,a |θ)f (Ys,a  |θ),

for a person working.
   The model for wages is
                                                            c
                                                          Ys,a,i    0
                                                                 = X1,a,i βcs,a + αcs,a 0 θi + εca,s,i ,

where εca,s,i ∼ N(0, τ cs,a ). This is in the form of a standard linear regression model under normality and (β cs,a , αcs,a , τ cs,a ) is
sampled as above (using multivariate normal and gamma priors).
   We can allow for general state dependence by modeling the latent employment transition indices as
                                                    
                                                    
                                                    X 0        d        d    0       d              d
                                          d,∗         2,a,s,i β a,s,0 + αa,s,0 θ i + εa,s,i,0 , if Ys,a−1,i = 0,
                                        Ys,a,i =
                                                    
                                                    X 0        d        d    0       d              d
                                                      2,a,s,i β a,s,1 + αa,s,1 θ i + εa,s,i,1 , if Ys,a−1,i = 1,



where εda,s,i,0 and εda,s,i,1 are both standard normal.
   The conditional of (β2,a,s,0 , α2,a,s,0 ) and (β2,a,s,1 , α2,a,s,1 ) is

                  ¡                                                                         Y            ¡ d,∗                              0      ¢
                 f βda,s,0 , αda,s,0 |ψ −β da,s,0 ,αda,s,0 ) ∝ f (βda,s,0 , αda,s,0 )                   f Ys,a,i   0
                                                                                                                 |X2,a,s,i βda,s,0 + αda,s,0 θi , 1
                                                                                            d
                                                                                        i:Ys,a−1,i =0


               ¡                                                                             Y           ¡ d,∗                               0       ¢
              f β d a,s,1 , αd a,s,1 |ψ −β da,s,1 ,αda,s,1 ) ∝ f (βda,s,1 , αda,s,1 )                   f Ys,a,i   0
                                                                                                                 |X2,a,s,i βda,s,1 + +αda,s,1 θ i , 1
                                                                                            d
                                                                                        i:Ys,a−1,i =1




                                                                                 35
Both of these are normal regression models with the precision fixed at one. The latent employment indices are sampled as in
the usual binary choice framework (see Albert and Chib (1993)).

                                                                        Factors

   The conditional for θ factors into n conditionals for θ1 , . . . , θ n . To see what the conditional for θi is note that all
contributions of θi originate from linear regression models,


                                        Ii − Zi0 η = γ 0 θi + εI,i ,                                            (choice model)
                                  0
                            Mj − Xm,i,j βm,j = α0m,j θi + εm,j ,                                                (measurements)
                            c
                          Ys,a,i    0
                                 − X1,a,i βc s,a = αc 0s,a θ i + εca,s,i ,                                      (wages)
                       d,∗                                  0
                     Y2,s,a,i    0
                              − X2,a,s,i β d a,s,l = αd a,s,l θ i + εda,s,i,l ,                                 (employment).


   This equation system is of the form
                                                                   Ŷi = Ai θi + ui ,

where ui ∼ N (0, Σi ), where Σi is a diagonal precision matrix. The conditional posterior for θi is then

                                                            n    1                                   o
                                        f (θi |ψ) ∝ exp         − (Ŷi − Ai θ i )0 Σi (Ŷi − Ai θ i ) f (θ i ),
                                                                 2

where
                                                                   JK
                                                                 K X
                                                                 Y               ¡                 ¢
                                                    f (θ i ) =             pk,j N θik |µk,j , τ k,j .
                                                                 k=1 j=1

We sample θik |{θij }j6=k one at a time from their respective conditionals which can be shown to be a mixture of normals with
updated (data dependent) mixture weights and parameters.
   Conditional on the factor vector θ, we have

                                                      Jk
                                                      X           ¡                    ¢
                                              θik ∼         p ,j N θi |µ ,j , τ   ,j    ,   i = 1, . . . , n.
                                                      j=1


   Conditional on θ we can treat the factors as known and update the mixture parameters (pk , µk , τ k ). We follow the “group
indicator” approach in Diebolt and Robert (1994) and augment the parameter vector by a sequence of latent group indicators
defined as gi = j if a θi,j originates from mixture component j. Conditional on the mixture group indicators the mixture
parameters are easily sampled and conditional on the mixture parameters the group indicators are simple multinomials. To
preserve identification of intercepts we constrain the mixture to have mean zero using the method proposed in Richardson et
al., (2000).
   The estimation of the structural models in section 7 are done as above with a few modifications. The choice model is a
probit so the cut point is c = 0, and no ρ parameters are estimated. The cross equation restrictions are imposed as follows.
Let Ỹi = (V1,h,i , V2,h,i , V1,c,i , V2,c,i , Vi ), i.e., the stacked outcomes under high school and college and the choice index. We


                                                                            36
can then write the model as


                                                         Ỹi = Wi ψ + Γθi + εi ,

                                                           i   = Xi ω + αtest θi + εtest ,

         ©                                               ª
where ψ = {δ̄ 1,s , δ̄ 2,s , β̄ 1,s , β̄ 2,s }s , δ P , γ , and Wi and the loading matrix Γ = Γ({ᾱ1,s , ᾱ2,s }s , αP ) are defined appropriately.
This model is now in the form of the system described above and the required conditionals are derived as above.




                                                                         37
   Appendix D: Mean Preserving Spread
   For the model described in Section 7, assume that εa,s are independent and identically normally distributed within each
period:
                                                 εa,s ∼ N (0, σ 2s,1 ) for ages 19-29



                                                 εa,s ∼ N (0, σ 2s,2 ) for ages 30-65.

Then:
                                                                   29
                                                                   X      σ 2s,1
                                                    ε1,s ∼ N (0,                 )
                                                                   a=19
                                                                        (1 + ρ)a

                                                                   65
                                                                   X      σ 2s,2
                                                    ε2,s ∼ N (0,                 ).
                                                                   a=30
                                                                        (1 + ρ)a

At each age:


                ln Ya,s = δ a,s + X 0 β a,s + α0a,s θ + εa,s + η1,s ∗ experiencea + η2,s ∗ experience2a = µa,s + εa,s


where
                            µa,s = δ a,s + X 0 βa,s + α0a,s θ + η 1,s ∗ experiencea + η 2,s ∗ experience2a

then
                                              E(Ya,s |X,θ) = exp(µa,s )E[exp(εa,s )].

We do a mean preserving spread at each age a by giving the individual knowledge of εa,s :

                                                                  ¡           ¢     ¡      ¢
                                       E (Ya,s |X, θ, εa,s ) = exp µa,s + εa,s = exp µ0a,s


Then,
                                                  ¡     ¢
                                               exp µ0a,s = exp(µa,s )E[exp(εa,s )]

Since the εa,s are iid we can drop the age subscript on the ε:

                                                  ¡     ¢     ¡    ¢
                                               exp µ0a,s = exp µa,s E (exp (εs ))


The mean preserving spread is actually a combination of a age by age mean preserving spreads. Finally, compute:

                                                                29
                                                                X      µa,s
                                                       µ1,s =
                                                                a=19
                                                                     (1 + ρ)a

                                                                65
                                                                X      µa,s
                                                       µ2,s =
                                                                a=30
                                                                     (1 + ρ)a



                                                                   38
                                                                  29
                                                                  X      µ0a,s
                                                      µ01,s =
                                                                  a=19
                                                                       (1 + ρ)a

                                                                  65
                                                                  X      µ0a,s
                                                      µ02,s   =                 .
                                                                  a=30
                                                                       (1 + ρ)a

   Define


                        V   = µ1,C + µ2,C − µ1,a − µ2,a − Zγ − α0p θ − εp

                       V0   = µ01,C + µ02,C − µ01,a − µ02,a + Zγ − α0p θ − εp + ε̄1,C + ε̄2,C − ε̄1,a − ε̄2,a


The probability of going to college is simply given by


                                                              Pr (V > 0)


for the first case and for the second case
                                                              Pr (V 0 > 0) .

The experiment for the case where we remove θ1 from the information set of the agent, keeping age by age mean earnings
constant, is analogous to the one just described.




                                                                    39
References
 [1] Aakvik, A., J. Heckman and E. Vytlacil, “Training Eﬀects on Employment when the Training Eﬀects are Heterogeneous:
    An Application to Norwegian Vocational Rehabilitation Programs,” manuscript, University of Chicago, 1999.

 [2] _____, “Treatment Eﬀects For Discrete Outcomes when Responses To Treatment Vary Among Observationally Iden-
    tical Persons: An Application to Norwegian Vocational Rehabilitation Programs,” NBER Working Paper No.TO262,
    forthcoming in Journal of Econometrics, 2003.

 [3] Albert, J. and S. Chib, “Bayesian Analysis of Binary and Polychotomous Response Data,” Journal of the American
    Statistical Association 88 (1993): 669-679.

 [4] Anderson, T.W. and H. Rubin, “Statistical Inference in Factor Analysis,” in J. Neyman, ed., Proceedings of Third
    Berkeley Symposium on Mathematical Statistics and Probability, University of California Press, 5, 1956, 111-150.

 [5] Athey, S and G. Imbens, “Identification and Inference in Nonlinear Diﬀerence-In-Diﬀerences Models,” NBER Technical
    Working Paper T0280, 2002.

 [6] Ben Akiva, Moshe; Bolduc, Denis; and Walker, Joan (2001). “Specification, Identification and Estimation of the Logit
    Kernel (or Continuous Mixed Logit Model). Manuscript, Department of Civil Engineering, MIT, February.

 [7] Buera, Francisco Javier, “Testable Implications and Identification of Occupational Choice Models,” unpublished manu-
    script, University of Chicago, 2002.

 [8] Bureau of Labor Statistics (2001). NLS Handbook 2001. Washington, D.C.: U.S. Department of Labor.

 [9] Cameron, S. and J. Heckman, “Son of CTM: The DCPA Approach Based on Discrete Factor Structure Models,”
    unpublished manuscript, University of Chicago, 1987.

[10] _____, “Life Cycle Schooling and Dynamic Selection Bias,” Journal of Political Economy 106(2)(1998), 262-333.

[11] _____, “The Dynamics of Educational Attainment for Blacks, Whites and Hispanics,” Journal of Political Economy
    109(3) (2001), 455-499.

[12] Carneiro, P., K. Hansen and J. Heckman, ”Removing the Veil of Ignorance in Assessing the Distributional Impacts of
    Social Policies,” Swedish Economic Policy Review, Vol. 8, (2001).

[13] Cawley, J., K. Conneely, J. Heckman and E. Vytlacil., “Cognitive Ability, Wages, and Meritocracy,” in Intelligence
    Genes, and Success: Scientists Respond to the Bell Curve, edited by B. Devlin, S. E. Feinberg, D. Resnick and K.
    Roeder, (Copernicus: Springer-Verlag, 1997), 179-192.

[14] Chamberlain, G., “Education, Income, and Ability Revisited,” Journal of Econometrics v5, n2 (March 1977a): 241-57

[15] _____, “An Instrumental Variable Interpretation of Identification in Variance Components and MIMIC Models,” in
    Paul Taubman, ed., Kinometrics: Determinants of Socio-Economic Success Within and Between Families, Amsterdam:
    North-Holland, 1977b

                                                            40
[16] Chamberlain, G. and Z. Griliches, “Unobservables with a Variance-Components Structure: Ability, Schooling, and the
    Economic Success of Brothers,” International Economic Review v16, n2 (June 1975): 422-49.

[17] Chib, Siddhartha and Hamilton, Barton. “Bayesian Analysis of Cross Section and Clustered Data Treatment Models”
    Journal of Econometrics, (2000), 97, 25-50.

[18] _____. (2002) “Semiparametric Bayes Analysis of Longitudinal Data Treatment Models,” Journal of Econometrics,
    110(1)(2002):67—89.

[19] Cochrane, W.G. and D. Rubin, “Controlling Bias in Observational Studies: a review.” Sankhya A 35(1973), 417-446.

[20] Cosslett, Stephen R., “Distribution-Free Maximum Likelihood Estimator of the Binary Choice Model,” Econometrica,
    v51, n3 (May, 1983): 765-82.

[21] Diebolt, J. and C.P. Robert, “Estimation of Finite Mixture Distributions Through Bayesian Sampling,” Journal of the
    Royal Statistical Society, Series B, 56(1994): 363-375.

[22] DiNardo, J., N. M. Fortin and T. Lemieux, “Labor Market Institutions and the Distribution of Wages, 1973-1992: A
    Semiparametric Approach,” Econometrica, 64(1996): 1001-1044

[23] Eckstein, Z. and K. Wolpin, “The Specification and Estimation of Dynamics Stochastic Discrete Choice Models: A
    Survey,” Journal of Human Resources, 24(1989), 562-598.

[24] _____. (1999). “Dynamic Labour Force Participation of Married Women and Endogenous Work Experience.” Review
    Economic Studies 56 (July): 375-90.

[25] Elrod and M. Keane, “A Factor-analytic Probit Model for Representing the Market Structure in Panel Data,” Journal
    of Marketing Research 32(1995), 1-16.

[26] Ferguson, T. S., “Bayesian Density Estimation by Mixtures of Normal Distributions,” in M. Rizvi, J. Rustagi, and D.
    Siegmund, eds., Recent Advances in Statistics, New York: Academic Press, 1983, 287-302.

[27] Flavin, M., “The Adjustment of Consumption to Changing Expectations about Future Income,” Journal of Political
    Economy 89(1981), 974-1009.

[28] Florens, J., M. Mouchart and J. Rolin. Elements of Bayesian Statistics. New York : M. Dekker, 1990.

[29] Fréchet, M., “Sur les tableaux de corrélation dont les marges sont donneés,” Annals Université Lyon, Sect.A, Series 3,
    14(1951), 53-77.

[30] Geweke, J., D. Houser and M. Keane, “Simulation based inference for dynamic multinomial choice models”, in B.H.
    Baltaji, ed., Companion for Theoretical Econometrics, 2001, Basil Blackwell, London.

[31] Goldberger, A.S., “Structural Equation Methods in the Social Sciences.” Econometrica, 40(1972), 979-1001.




                                                              41
[32] Hansen, K., J. Heckman, and K. Mullen, “Ordered Discrete Choice Models with Stochastic Shocks,” manuscript, Uni-
    versity of Chicago, 2001.

[33] _____, “The Eﬀect of Schooling and Ability on Achievement Test Scores, forthcoming in Journal of Econometrics,
    2003.

[34] Hansen, K., J. Heckman, and S. Navarro, “Nonparametric Identification of Time to Treatment Models and The Joint
    Distributions of Counterfactuals,” Unpublished manuscript, University of Chicago, 2003.

[35] Hansen, L., W. Roberds and T. Sargent, “Time Series Implications of Present Value Budget Balance and of Martingale
    Models of Consumption and Taxes,” in L. Hansen and T. Sargent, eds., Rational Expectations Econometrics. Boulder,
    CO: Westview Press, 1991.

[36] Heckman, J.,“Statistical Models for Discrete Panel Data,” in C. Manski and D. McFadden, eds., Structural Analysis of
    Discrete Data With Econometric Applications. M.I.T. Press: 1981.

[37] _____, “Varieties of Selection Bias.” American Economic Review 80(2) (May 1990), 313-18.

[38] _____, “Randomization and Social Policy Evaluation,” in Evaluating Welfare and Training Programs, edited by
    Charles F. Manski and Irwin Garfinkel, Cambridge, Mass.: Harvard University Press, 1992.

[39] _____, “Micro Data, Heterogeneity, and the Evaluation of Public Policy: Nobel Lecture,” Journal of Political Econ-
    omy. 109(4)(2001), 673-748.

[40] Heckman, J. and B. Honoré, “The Empirical Content of the Roy Model,” Econometrica, 58(5)(1990),1121-1149.

[41] Heckman, J., R. LaLonde, and J. Smith, “The Economics and Econometrics of Active Labor Market Programs,” In O.
    Ashenfelter and D. Card, eds, Handbook of Labor Economics. Volume 3. (Amsterdam: Elsevier, 1999).

[42] Heckman, J., L. Lochner and C. Taber (1998a). “Explaining Rising Wage Inequality: Explorations With A Dynamic
    General Equilibrium Model of Earnings With Heterogeneous Agents.” Review of Economic Dynamics, 1:1-58.

[43] _____, (1998b). “General Equilibrium Treatment Eﬀects: A Study of Tuition Policy,” American Economic Review,
    88(2):381-6.

[44] _____, (1998c). “Tax Policy and Human Capital Formation,” American Economic Review, 88(2):293-297.

[45] _____, (2000). “General Equilibrium Cost Benefit Analysis of Education and Tax Policies,” in G. Ranis and L.K.
    Raut, eds., Trade, Growth and Development: Essays in Honor of T. N. Srinivasan, Chapter 14. Amsterdam: Elsevier
    Science, B.V., 291-393.

[46] Heckman, J. and S. Navarro. “Using Matching, Instrumental Variables and Control Functions to Estimate Economic
    Choice Models,” forthcoming in Review of Economics and Statistics, 2003.

[47] Heckman, J., and R. Robb. “Alternative Methods for Evaluation the Impact of Interventions.” In Longitudinal Analysis
    of Labor Market Data, J. Heckman and B. Singer, eds. (New York: Cambridge University Press,1985).

                                                           42
[48] _____, (1986). “Alternative Methods for Solving the Problem of Selection Bias in Evaluating the Impact of Treatments
    on Outcomes,” in Drawing Inferences from Self-Selected Samples, H. Wainer, ed. (New York: Springer-Verlag, 1986;
    reprinted in 2000 by Lawrence Erlbaum Associates).

[49] Heckman, J. and J. Smith, “Assessing the Case for Randomized Evaluation of Social Programs.” in Measuring Labour
    Market Measures: Evaluating The Eﬀects of Active Labour Market Policy Initiatives, K. Jensen and P.K. Madsen, eds.
    (Copenhagen: Ministry Labour, 1993).

[50] _____, “Evaluating the Welfare State,” in S. Strom, ed., Econometrics and Economic Theory in the 20th Century: The
    Ragnar Frisch Centennial, Econometric Society Monograph Series, (Cambridge: Cambridge University Press, 1998).

[51] Heckman, J., J. Smith, and N. Clements, “Making the Most out of Program Evaluations and Social Experiments:
    Accounting for Heterogeneity in Program Impacts,” Review of Economic Studies 64(1997), 487-535.

[52] Hoeﬀding, W., “Masstabinvariante Korrelationtheorie,” Schriften des Mathematischen Instituts und des Instituts für
    Angewandte Mathematik der Universität Berlin, 5(1940), 1979-233.

[53] Jöreskog, K., “Structural Equations Models In The Social Sciences: Specification, Estimation and Testing.” In Applica-
    tions of Statistics, edited by P.R. Krishnaih. (Amsterdam: North Holland, 1977), 265-287.

[54] Jöreskog, K.G., and Goldberger, A.S.. “Estimation of a Model with Multiple Indicators and Multiple Causes of a Single
    Latent Variable,” Journal of the American Statistical Association, 70(351)(1975):631-639.

[55] Keane, M., and K. Wolpin, “The Career Decisions of Young Men,” Journal of Political Economy 105(3) (June 1997):
    473-522.

[56] Kotlarski, Ignacy.“On characterizing the gamma and normal distribution,” Pacific Journal of Mathematics, Volume 20
    (1967), pp. 69-76.

[57] Manski, Charles F. “Identification of Binary Response Models,” Journal of the American Statistical Association,
    83(403)(1988), 729-38.

[58] Matzkin, R., “Nonparametric and Distribution-Free Estimation of the Binary Threshold Crossing and the Binary Choice
    Models,” Econometrica v60, n2 (March 1992): 239-70.

[59] _____,. “Nonparametric Identification and Estimation of Polychotomous Choice Models,” Journal of Econometrics,
    58, 137-68, 1993.

[60] Matzkin, R. and A. Lewbel, “Notes on Single Index Restrictions,” unpublished manuscript, Northwestern University,
    2002.

[61] McFadden, D., “Econometric Analysis of Qualitative Response Models,” In Handbook of Econometrics, Vol. II, Z.
    Griliches and M. Intrilligator, eds. (Amsterdam: North Holland, 1984).



                                                            43
[62] Muthen, B., “A General Structural Equation Model With Dichotomous, Ordered Categorical and Continuous Latent
    Variable Indicators,” Psychometrika 49(1984): 115-132.

[63] Olley, S. G. and A. Pakes, “The Dynamics of Productivity in the Telecommunications Equipment Industry,” Economet-
    rica, 64(6)(1996), 1263-1297.

[64] Rao, Prakasa B.L.S., Identifiability in Stochastic Models: Characterization of Probability Distributions, (Boston: Acad-
    emic Press, 1992).

[65] Richardson, S., L. Leblond, I. Jaussent and P. J. Green, “Mixture Models in Measurement Error Problems, with Reference
    to Epidemiological Studies,” Working paper, 2000.

[66] Robert, C.P. and G. Casella, Monte Carlo Statistical Methods, (New York: Springer, 1999).

[67] Rosenbaum, P. and D. Rubin, “The Central Role of the Propensity Score in Observational Studies for Causal Eﬀects,”
    Biometrika 70 (April 1983), 41-55.

[68] Roy, A., “Some Thoughts on the Distribution of Earnings,” Oxford Economic Papers, 3(1951), 135-146.

[69] Rozanov,Y.A., Markov Random Fields, (Berlin: Spring Verlag, 1982).

[70] Sen, Amartya Kumar, On Economic Inequality, Oxford, Clarendon Press, 1973.

[71] Willis, R. and S. Rosen, “Education and Self-Selection,” Journal of Political Economy, 87(5, part 2)(1979), S7-S36.




                                                             44
Notes
 1
       Address: Department of Economics, University of Chicago, 1126 E. 59th Street, IL 60637. E-mail: pmcarnei@mid-
way.uchicago.edu. Pedro Carneiro’s research was supported by Fundacao Ciencia and Tecnologia and Fundacao Calouste
Gulbenkian.
   2
       Address: Kellogg School of Management, Northwestern University, Evanston, Illinois 60657.      E-mail: karsten-hansen
@northwestern.edu
   3
       Address: Department of Economics, The University of Chicago, 1126 East 59th Street, Chicago, IL 60637, and The
American Bar Foundation. E-mail: jjh@uchicago.edu. This research is supported by NSF 97-09-873, SES-0099195, and
NICHD-40-4043-000-85-261. Heckman’s work was also supported by the American Bar Foundation and the Donner Founda-
tion.
   4
       Previous versions of this paper were given at the Midwest Econometrics Group, Chicago, October 2000, Washington
University St. Louis, May, 2001, the Nordic Econometrics Meetings, May, 2001 and workshops at Chicago August, 2002 and
Stanford, January, 2003. A simple version of this paper is presented in Carneiro, Hansen and Heckman (2001). A version of
this paper was presented by Heckman as the Klein Lecture at the University of Pennsylvania, September, 28, 2001 and also
at the IFAU conference in Stockholm Sweden, October 2001. We are grateful to all workshop participants. We especially
thank Mark Duggan, Orazio Attanasio and Michael Keane for comments on the first draft of this paper. We have benefited
from discussions with Ricardo Barros, Richard Blundell, Francisco Buera, Flavio Cunha, Mark Duggan, Lars Hansen, Steven
Levitt, Bin Li, Luigi Pistaferri and Sergio Urzua on subsequent drafts. We single out Salvador Navarro and Edward Vytlacil
for especially helpful comments. We are grateful to Flavio Cunha and Salvador Navarro for exceptional research assistance
and hard work.
   5
       See Heckman, Lochner and Taber (1998a, 1998b, 1998c; 2000) for a treatment of general equilibrium policy evaluation.
   6
       See Heckman and Smith (1993, 1998) and Heckman, Smith and Clements (1997).
   7
       Conditions under which (M, M̃ ) determine the joint distribution are presented in Rozanov (1982).
   8
       See e.g, Heckman, Smith, and Clements (1997), or Athey and Imbens, (2002).
   9
       Mean or median zero assumptions on (U0 , U1 ) are also used.
  10
       See their papers for exact conditions. Heckman and Smith (1998) present the most general set of conditions.
  11
       Aakvik, Heckman and Vytlacil (1999) present other sets of identifying assumptions.
  12
       Heckman and Smith (1998) and Heckman, LaLonde and Smith (1999) discuss conditions under which it is possible to
estimate (4).
  13
       See Eckstein and Wolpin (1999) and Keane and Wolpin (1997).
  14
       See Cameron and Heckman (1998), and Hansen, Heckman and Mullen (2001).
  15
       In the case of ties, use the choice with the lowest index.
  16
       See Hansen, Heckman, and Navarro (2003) for duration models with general forms of dependence functions generated by
this type of model.
  17
       Strictly speaking, matching models do not distinguish X and Z. See Heckman and Navarro (2003).
  18
       See Hansen, Heckman and Mullen (2001) for a comparison among alternative models of completed schooling. Hansen,
Heckman and Mullen (2003) develop a parallel analysis for a one factor-multinomial choice model.


                                                                    45
  19
       Specifically, it is assumed for (8) that µs (Z) is concave in s for each Z (Cameron and Heckman, 1998), that


                                                     es − es−1 = τ        all s = 2, .., S

with e1 as an initial condition, that


                                               (∗)      µs (Z) − µs−1 (Z) = ϕ(Z) + cs−1

with µ1 (Z) as an initial condition and that cs ≥ cs−1 for all s = 1, . . . , s̄. Changes in utilities across states are independent
of s, except for an intercept. Then in (17) εW = τ + e1 . If we set all of the iid components of (9) to zero (the uniquenesses
εs ) we get the ordered probit model. As noted in the text, and developed in Hansen, Heckman and Mullen (2003), we can
generalize this model to allow es − es−1 = τ + χs where χs ≥ 0 is a one sided random variable and still secure identification.
The requirement (∗) precludes a strict random utility model because preferences are state specific. ( The strict random utility
model requires that µs (Z) not depend on s but Z can vary across s. See, e.g., Matzkin, 1993).
 20
                 P
                 s
    Write ν s =                   ⊥ ρj 0 (j 6= j 0 ), ρj ⊥
                    ρj , where ρj ⊥                                ⊥ (Z, Q), ρj ≥ 0, ϕ(Z) = Z 0 η. This model is identified
                                                         ⊥ εW , ρj ⊥
                   j=2
under the assumptions in Cameron and Heckman (1998) even without any exclusion restrictions, so Qs can just include an
intercept. The proof is trivial. Normalize ρ1 = 0. From the first choice we compute,

                                                Pr(D1 = 1 |Z ) = Pr(Z 0 η + εW ≤ c1 )

so we can identify f (εW ), and η up to scale σ W , assuming εW and the ρj have densities with respect to Lebesgue measure
and nonvanishing characteristic function in addition to other standard regularity conditions. We suppress the intercept in Z.
One cannot distinguish the intercept from c1 . Proceeding to further choices we obtain




                                        Pr(D1 + D2 = 1|Z) = Pr(Z 0 η + εW ≤ c2 + ν 2 )

                                                                = Pr(εW − ν 2 ≤ c2 − Z 0 η).


Therefore we can identify f (εW −ν 2 ) and c2 up to scale σ εW −ν 2 . The scale is determined by the first normalization (relative to
                                   µ 2            ¶1/2
                        σ εW −ν 2     σ ε +σ 2ν 2
σ εW ). We can estimate σε        =      W
                                         σ2            by taking the ratio of the normalized η from the second choice probability
                              W           εW

to the normalized ratio of η from the first choice probability for any coordinate of η. Define ψ(X) as the characteristic
function of X. From the assumed independence of εW and ν 2 , ψ(εW − ν 2 ) = ψ(εW )ψ(−ν 2 ). Therefore we can identify
ψ(εW − ν 2 )
             = ψ(−ν 2 ), and we can determine f (−ν 2 ) from the convolution theorem adopting a normalization for σ W .
   ψ(εW )
Proceeding sequentially, we obtain Pr(D1 + D2 + D3 + ... + Dk = 1 |Z ) = Pr(Z 0 η + εW ≤ ck + ν k ), and can identify ck and
f (ν k ) up to the normalization given in the first step. From f (ν k ), we can use deconvolution to identify f (ρj ), j = 2, .., S. See
Hansen, Heckman and Mullen (2001) for further details and extensions to factor models. Nowhere in this analysis do we use
the assumption that Qs contains regressors.
  21
       Other normalizations are possible. All require that there be at least three measurements on each factor, although we can
get by with only one dedicated measurement. Consider the following example (due to Salvador Navarro):

                                                                     46
       Let L = 5, K = 2.


                                           g1    = θ1 + ε1 , g2 = λ21 θ1 + θ2 + ε2

                                           g3    = λ31 θ1 + λ32 θ2 + ε3 , g4 = λ41 θ1 + λ42 θ2 + ε4

                                           g5    = λ51 θ1 + λ52 θ2 + ε5 .


Assuming nonvanishing covariances and factor loadings,

                                               COV (g1 , g5 ) COV (g3 , g4 ) − COV (g3 , g5 ) COV (g1 , g4 )
                                       λ32 =
                                               COV (g2 , g4 ) COV (g1 , g5 ) − COV (g1 , g4 ) COV (g2 , g5 )

if λ22 λ42 λ51 − λ41 λ52 6= 0.
       Then
                                                            COV (g3 , g4 ) − COV (g2 , g4 ) λ32
                                                    λ41 =
                                                            COV (g1 , g3 ) − COV (g1 , g2 ) λ32

if λ31 6= λ32 λ21 .

                                        COV (g1 , g2 ) λ41             COV (g1 , g3 ) λ41          COV (g1 , g5 ) λ41
                           λ21     =                       , λ31 =                        , λ51 =
                                         COV (g1 , g4 )                  COV (g1 , g4 )             COV (g1 , g4 )
                                                                                            2
                                        COV (g1 , g4 )           COV (g2 g3 ) − λ21 λ31 σ θ1
                           σ 2θ1   =                   , σ 2θ2 =
                                           λ41                                λ32
                                        COV (g2 , g4 ) − λ21 λ41 σ 2θ1          COV (g2 , g5 ) − λ21 λ51 σ 2θ1
                           λ42     =                                   ,  λ52 =                                .
                                                   σ 2θ2                                      σ2θ2

  22                   d
       In particular, Um , Usd are assumed to have a distribution that is absolutely continuous with respect to Lebesgue measure.
  23
       Alternatively, we could normalize the medians to be zero.
  24
       For a definition of measurable separability, see Florens, Mouchart and Rolin (1990), section 5.2. The key idea is that we
can vary each of the coordinates of the vector freely.
  25
       This assumption can be relaxed. It only aﬀects certain normalizations.
  26
       In the discussion of equation (21) we could have normalized the variances of the σ 2θi , i = 1, ..., K to one rather than
certain factor loadings, although this is less straightforward and requires the imposition of certain sign restrictions.
  27
       To be able to identify λs21 we need a third measurement on this factor, which we can get from equation B1 + 1. Since
there is no equation BK+1 , we require that BK − BK−1 ≥ 3 in order to be able to identify the loadings on θK .
  28
                             ¡         ¢
     In principle, the future α1a , α0a can be uncertain at the date decisions are made. Assuming that these factor loadings
                                                                   ¡ ¢        ¡ ¢
are independent of θ, we can replace these expressions by EIθ α1a , EIθ α0a without aﬀecting the identifiability of the
¡ 1 0¢
 αa , αa , provided the conditions of Theorem 4 are met, but it aﬀects the identifiability and interpretation of αP . A more
general version of this model would postulate two random variables θ and θ ∗ . Agents act on θ ∗ while θ is the true value.
It would be interesting to identify the joint distributions of θ and θ∗ under (e.g.) a rational expectations assumption. We
leave this for a later occasion.
  29
       We set zero earnings to 1 in this paper.



                                                                        47
  30
       The run time was about 122 minutes on a 1.2Ghz AMD Athlon PC.
  31
       The distributions of the factors by schooling level are shown in Figures A1 and A2 on the website.
  32
       The coeﬃcient estimates for the model are posted on the website.
  33
       If we consider net gains by subtracting costs the diﬀerence between college graduates and high school graduates will be
even higher because costs are lower for college graduates.
  34
       We can also compute gross utility gains as a percentage of the gross utility in high school as:

                                                               V1,c + V2,c
                                                          R=               − 1.
                                                               V1,h + V2,h

See Figure A4 on the website.
  35
       If the agent knows θ1 , θ2 , εcollege and εhighschool then he faces no uncertainty.
  36
       These results are available on request from the authors, and are posted on the website.
  37
       See the numbers posted at the website.
  38
       We compute the compensation (which can be negative or positive) required by each individual to keep average earnings
the same after the uncertainty is reduced. Then we provide the individual with this compensation together with knowledge
of (ε̄1,c , ε̄2,c , ε̄1,h , ε̄2,h ) and finally we compute the percentage of individuals who would change their schooling decision if
they had knowledge of (ε̄1,c , ε̄2,c , ε̄1,h , ε̄2,h ) but had the same present value of earnings in each schooling level. We use the
procedure described at the end of Section 4 applied to each period to adjust utility for the eﬀects of mean preserving spreads
in earnings (see Appendix D).
  39
       This comes from a simulation available on request from the authors.
  40
       The same result holds when we consider distributions of utilities instead of distributions of lifetime earnings. See Figure
A-15 on the website.
  41
       We thank Edward Vytlacil for simplifying and clarifying the statements and proofs of all three theorems in this section.
  42
       Using a standard definition of identification, a model (FU , β) is identified iﬀ for any alternative parameters (FU∗ , β ∗ ) 6=
(FU , β), there exists some ε > 0 such that


                                                   Pr (|FU (β) − FU∗ (β ∗ ) | > ε) > 0,


where the probability is computed with respect to the density of the data generating process. Our use of limit set arguments
may appear to contradict the standard definition because of zero probability at the limit sets. However, this intuition is false.
See the argument in Aakvik, Heckman and Vytlacil (1999), Theorem 1, which justifies the appeal to limit arguments used in
this paper in terms of standard definitions of identification.
  43
       Implemented in 1950, the AFQT score is used by the army to screen draftees.
  44
       For other uses of Markov Chain Monte Carlo techniques in models and applications related to ours, see Chib and Hamilton
(2000) who implements MCMC methods for a panel version of a generalized Roy model and Chib and Hamilton (2002) who
consider various cross sectional treatment models.




                                                                    48
                                                                   Figure 1                                                                                                                             Figure 2
                                                            Density of Gross Utility                                                                                                          Factor and Normal Densities*
                    0.35                                                                                                                                1
                                                                                                                        Actual                                                                                                                 T1
                                                                                                                        Predicted                                                             variance = 0.3019                                normal version of T1
                                                                                                                                                      0.9                                                                                      T2
                     0.3                                                                                                                                                                                                                       normal version of T2

                                                                                                                                                      0.8

                    0.25
                                                                                                                                                      0.7


                                                                                                                                                      0.6
                     0.2
Dens ity(Utility)




                                                                                                                                         Density(T)
                                                                                                                                                      0.5                                                                              variance = 0.5747

                    0.15
                                                                                                                                                      0.4


                     0.1                                                                                                                              0.3


                                                                                                                                                      0.2
                    0.05
                                                                                                                                                      0.1


                      0                                                                                                                                 0
                           0                2                4                6                 8                 10                12                  -2             -1.5          -1          -0.5             0          0.5           1             1.5          2
                                                                            Utility                                                                                                                               T
                      All densities are estimated using a 100 point grid over the domain and a Gaussian kernel with bandwidth of 0.12.                      * Normal densities are defined to be normal with same mean and variance as the corresponding T.
                                                                                                                                                            All dens ities are es timated us ing a 100 point grid over the domain and a G aus s ian kernel with
                                    1
                      Utility=6a          log(Y a,s )                                                                                                       bandwidth of 0.12.
                                 (1+0.03)a




                                                All densities are
                                                All densities are
                                                estimated using a
                                                100 point grid
                                                over the domain
                                                and a Gaussian
                                                kernel with
                                                                        Figure 3                                                                                                                               Figure 4
                                                              Density of College Gross Utility                                                                                        Density of Gross Utility Differences (College-High School)
                   0.09                                                                                                                                                0.7
                                                                                                                    High School*
                                                                                                                    College**                                                                                                                                    High School*
                   0.08                                                                                                                                                                                                                                          College**
                                                                                                                                                                       0.6

                   0.07

                                                                                                                                                                       0.5
                   0.06




                                                                                                                                        Density(Utility Differences)
Density(Utility)




                   0.05                                                                                                                                                0.4



                   0.04
                                                                                                                                                                       0.3

                   0.03

                                                                                                                                                                       0.2
                   0.02


                                                                                                                                                                       0.1
                   0.01


                      0                                                                                                                                                  0
                          0                    2                4            6                8                10                  12                                    -3                   -2             -1                 0                1                 2                 3
                                                                           Utility
                                                                                                                                                                              * E(V c-Vh|Choice=High School)             Utility Differences
                          * Counterfactual: E(V c |C hoice=High S chool)                                                                                                      ** E(V c-Vh|Choice=College)
                          ** Predicted: E(V c |C hoice=C ollege)
                                                                                                                                                                         All densities are estimated using a 100 point grid over the domain and a Gaussian kernel with bandwidth of 0.12.
                    All densities are estimated using a 100 point grid over the domain and a Gaussian kernel with bandwidth of 0.12.                                                     1
                                                                                                                                                                         Utility=6a            log(Y a,s )
                                          1                                                                                                                                           (1+0.03)a
                          Utility=6a            log(Y a,s )
                                       (1+0.03)a
                                                                                                                                                                                                                     Figure 6
                                                                  Figure 5                                                                                                              Density of Gross Lifetime Earnings Differences (College-High School)
                                     Density of ε W and Marginal Treatment Effect: (E(V c-Vh|ε W) )*                                                                                     -3
                                                                                                                                                                                      x 10
                            1                                                                                             0.04                                                  1.4
                                                                                                          MTE                                                                                                                                                             High School*
                                                                                                          Density(ε W)                                                                                                                                                    College**

                                                                                                                                                                                1.2




                                                                                                                                                                                  1




                                                                                                                                                Density(Earnings Differences)
Marginal Treatment Effect




                                                                                                                                                                                0.8




                                                                                                                                 Density(ε W)
                            0                                                                                             0.02

                                                                                                                                                                                0.6




                                                                                                                                                                                0.4




                                                                                                                                                                                0.2




                            -1                                                                                             0                                                     0
                             -10        -8       -6        -4       -2   0     2         4          6         8          10                                                      -500                  0                500                 1000                  1500                   2000
                                                                         εW                                                                                                                                               Earnings Differences (1000's)

                                *ε =(α' + α' - α' - α' - α'' )θ - H                                                                                                             *E(PVc-PVh|Choice=High School)
                                  W    1,c  2,c  1,h  2,h   p       p
                                                                                                                                                                                **E(PVc-PVh|Choice=College)
           All dens ities are es timated us ing a 100 point grid over the domain and a G aus s ian kernel with bandwidth of 0.12.
                                                                                                                                                                                PVh=6a       1     Yh,a
                                                                                                                                                                                         (1+0.03)a

                                                                                                                                                                          All densities are estimated using a 100 point grid over the domain and a Gaussian kernel with bandwidth of 0.12.
                                                                                                                                                                                                                                       Figure 8
                                                               Figure 7                                                                  Density of ε W and Relative Marginal Treatment Effect for Present Value of Gross Earnings E((PVc/PVh)-1| ε W)
                               Density of Relative Gross Earnings Differences (College-High School)
                         1                                                                                                                                                          8                                                                                                  0.04
                                                                                                                    High School*                                                                                                                                     MT E
                                                                                                                    College**                                                                                                                                        Dens ity(ε W )
                   0.9


                   0.8                                                                                                                                                              6


                   0.7


                   0.6




                                                                                                                                               Relative Marginal Treatment Effect
                                                                                                                                                                                    4
Density(Returns)




                                                                                                                                                                                                                                                                                              Density(ε W)
                   0.5                                                                                                                                                                                                                                                                 0.02


                   0.4                                                                                                                                                              2


                   0.3


                   0.2                                                                                                                                                              0


                   0.1


                    0                                                                                                                                                               -2                                                                                                  0
                        -0.5               0              0.5               1                1.5                2                  2.5                                               -10       -8      -6          -4             -2      0       2   4       6          8            10
                                                                           Returns
                                                                                                                                                                                                                                         εW
                     *E((PVc/PVh)-1|Choice=High School)
                     **E((PVc/PVh)-1|Choice=College)                                                                                                                                    *ε =(α' + α' - α' - α'           '
                                                                                                                                                                                          W    1,c  2,c  1,h     2,h - α' p)θ   - Hp
                         PVh=6a      1      Yh,a
                                  (1+0.03)a                                                                                                                         All dens ities are es timated us ing a 100 point grid over the domain and a G aus s ian kernel with bandwidth of 0.12.
                    All densities are estimated using a 100 point grid over the domain and a Gaussian kernel with bandwidth of 0.12.
                                                             Figure 9                                                                                                      Figure 10
                                   Density of Gross College Utility Under Different Information Sets                            Density of Gross Utility Difference (College-High School) Under Different Information Sets
                   0.7
                                                                                                                                                                   0.7
                                                                                                               I=
                                                                                                               I={T2}                                                                                                                                                 I=
                                                                                                              I={T1,T2}                                                                                                                                               I={T2}
                   0.6                                                                                                                                                                                                                                               I={T1,T2}
                                                                                                                                                                   0.6



                   0.5
                                                                                                                                                                   0.5




                                                                                                                                     Density(Utility Difference)
Density(Utility)




                   0.4
                                                                                                                                                                   0.4



                   0.3
                                                                                                                                                                   0.3



                   0.2
                                                                                                                                                                   0.2



                   0.1
                                                                                                                                                                   0.1



                     0
                         0                2              4          6                 8                10                  12                                        0
                                                                                                                                                                     -3                   -2             -1                  0               1                 2                 3
                                                                  Utility                                                                                                                                             Utility Difference
        All densities are estimated using a 100 point grid over the domain and a Gaussian kernel with bandwidth of 0.12.                                           All densities are estimated using a 100 point grid over the domain and a Gaussian kernel with bandwidth of 0.12.
                     Utility=6a      1     log(Y a,s )                                                                                                                               1
                                                                                                                                                                     Utility=6a            log(Y a,s )
                                  (1+0.03)a                                                                                                                                       (1+0.03)a
                                                           Figure 11
                         Proportion of People Induced Into College by Full Subsidy to College Tuition
                      When Information Set = {θ1, θ 2} by Decile of Initial High School Earnings Distribution
             0.1800
             0.1600
             0.1400
             0.1200
Proportion




             0.1000
             0.0800
             0.0600
             0.0400
             0.0200
             0.0000
                         1        2        3         4        5            6    7        8                                                9                      10
                                                                  Decile
                                                                                              Lqglfdwru ri vwdwh
                                                                                              Yduldeohv ghqhg iru v
                                                                                              Yduldeohv ghqhg wr eh wkh vdph iru doo v

                                                                                                                                              Frpsrqhqwv ri J
                                                                                                                                                  Wdeoh 4
                                                                                                                                                            v
                                                                                             Frqwlqxrxv
                                                                                                Pf
                                                                                                \vf
                                                                                                 


                                                                                              Glvfuhwh
                                                                                                Pg
                                                                                                Gv
                                                                                                \vg
                                                                                Table 2a
                                                                   Descriptive Statistics of Variables
                                                                        NLSY 79 - White Males
                                                             Overall                                  High School                                    College
                  Variable                     Obs    Mean Std. Dev.     Min      Max    Obs    Mean Std. Dev.      Min        Max    Obs      Mean Std. Dev.            Min       Max
Tuition at age 17(Hundreds of Dollars)        1161    20.68     7.70    0.00     53.59   704    21.17     7.85     0.00       47.06   457      19.92     7.40           0.00      53.59
Urban at age 14                               1161     0.74     0.44    0.00      1.00   704     0.69     0.46     0.00        1.00   457       0.82     0.38           0.00       1.00
Broken at age 14                              1161     0.13     0.34    0.00      1.00   704     0.15     0.36     0.00        1.00   457       0.11     0.31           0.00       1.00
Number of Siblings                            1161     2.83     1.77    0.00     15.00   704     3.03     1.85     0.00       15.00   457       2.51     1.60           0.00      11.00
South at age 14                               1161     0.19     0.39    0.00      1.00   704     0.19     0.39     0.00        1.00   457       0.19     0.39           0.00       1.00
Mother Education                              1161    12.39     2.20    3.00     20.00   704    11.69     1.86     3.00       20.00   457      13.47     2.26           6.00      20.00
Father Education                              1161    12.71     3.16    0.00     20.00   704    11.61     2.70     0.00       20.00   457      14.40     3.08           4.00      20.00
Age in 1980                                   1161    19.27     2.19   16.00     23.00   704    19.27     2.17    16.00       23.00   457      19.28     2.21          16.00      23.00
Distance to College at age 17                 1161     7.68   15.58     0.00    100.20   704     8.87    16.01     0.00      100.20   457       5.84   14.75            0.00      96.59
Dummy for Birth in 1957                       1161     0.10     0.30    0.00      1.00   704     0.10     0.31     0.00        1.00   457       0.09     0.29           0.00       1.00
Dummy for Birth in 1958                       1161     0.10     0.30    0.00      1.00   704     0.09     0.28     0.00        1.00   457       0.12     0.33           0.00       1.00
Dummy for Birth in 1959                       1161     0.11     0.31    0.00      1.00   704     0.11     0.31     0.00        1.00   457       0.10     0.30           0.00       1.00
Dummy for Birth in 1960                       1161     0.14     0.35    0.00      1.00   704     0.15     0.36     0.00        1.00   457       0.13     0.34           0.00       1.00
Dummy for Birth in 1961                       1161     0.14     0.34    0.00      1.00   704     0.14     0.34     0.00        1.00   457       0.13     0.34           0.00       1.00
Dummy for Birth in 1962                       1161     0.16     0.37    0.00      1.00   704     0.16     0.37     0.00        1.00   457       0.16     0.36           0.00       1.00
Dummy for Birth in 1963                       1161     0.13     0.34    0.00      1.00   704     0.12     0.33     0.00        1.00   457       0.14     0.35           0.00       1.00
Education Status (0 if HS, 1 if College)      1161     0.39     0.49    0.00      1.00   704     0.00     0.00     0.00        0.00   457       1.00     0.00           1.00       1.00
In School at AFQT test date                   1161     0.67     0.47    0.00      1.00   704     0.49     0.50     0.00        1.00   457       0.94     0.23           0.00       1.00
Arithmetic Reasoning                          1161     0.15     0.95   -2.39      1.42   704    -0.22     0.91    -2.39        1.42   457       0.73     0.70          -1.96       1.42
Word Knowledge (ASVAB 3)                      1161     0.14     0.88   -3.71      1.16   704    -0.19     0.92    -3.71        1.16   457       0.64     0.50          -2.24       1.16
Paragraph Composition (ASVAB 4)               1161     0.14     0.89   -3.50      1.21   704    -0.17     0.96    -3.50        1.21   457       0.62     0.47          -1.62       1.21
Coding Speed (ASVAB 6)                        1161     0.15     0.95   -3.03      2.79   704    -0.12     0.90    -2.89        2.09   457       0.57     0.87          -3.03       2.79
Math Knowledge (ASVAB 7)                      1161     0.14     0.97   -2.14      1.58   704    -0.36     0.80    -2.14        1.58   457       0.91     0.68          -1.83       1.58
Present Value of Earnings*                    1161   956.13  730.87    18.12   7861.67   704   694.56   321.93    18.12     1885.85   457    1359.07  964.47           77.02    7861.67
* Earnings in Thousands of Dollars

                                                                              Table 2b
                                               Descriptive Statistics - Present Value of Log Earnings (Discount Rate = 3%)
                                                                        NLSY 79 - White Males
                                                            Overall                                   High School                                     College
                 Variable                     Obs    Mean Std. Dev.     Min      Max Obs       Mean Std. Dev.        Min      Max Obs          Mean Std. Dev.            Min        Max
Present Value of Earnings (Working Life)*     1161   956.13  730.87    18.12   7861.67 704     694.56    321.93     18.12   1885.85 457       1359.07  964.47           77.02    7861.67
Present Value of Earnings in Period 1*        1161   157.25    72.02    3.91    509.21 704     162.46     74.66      3.91    457.00 457        149.22    67.04          13.55     509.21
Present Value of Earnings in Period 2*        1161   798.88  694.55    14.21   7533.31 704     532.10    251.57     14.21   1582.89 457       1209.84  922.21           63.48    7533.31
*Earnings in Thousands of Dollars
Working Life = From age 19 to age 65
Period 1 = From age 19 to age 29, inclusive
Period 2 = From age 30 to age 65
                                                                                                                                6RXWK
                                                                                                                                8UEDQ
                                                                                                                                ,QWHUFHSW




                                                                                                                                $JHLQ
                                                                                                                                /RFDO7XLWLRQ



                                                                                                                                %URNHQ)DPLO\
                                                                                                                                &RKRUW'XPPLHV




                                                                                                                                )DWKHU V(GXFDWLRQ
                                                                                                                                1XPEHURI6LEOLQJV
                                                                                                                                0RWKHU V(GXFDWLRQ
                                                                                                                                $YHUDJH/RFDO:DJH




                                                                                                                                (QUROOHGLQ6FKRRODW7HVW'DWH
                                                                                                                                0HDQ/RFDO8QHPSOR\PHQW5DWH




                                                                                                                                          
                                                                                                                                           
                                                                                                                                           



                                                                                                                                           
                                                                                                                                           
                                                                                                                                           
                                                                                                                                           
                                                                                                                                         <HV
                                                                                                                                         <HV
                                                                                                                                         <HV
                                                                                                                                         <HV
                                                                                                                                         <HV
                                                                                                                                         <HV
                                                                                                                                                                                                                     7DEOH 2c




                                                                                                                                                                 
                                                                                                                                                                 
                                                                                                                                                                 




                                                                                                                                                                 
                                                                                                                                                                 
                                                                                                                                                               <HV
                                                                                                                                                               <HV
                                                                                                                                                               <HV
                                                                                                                                                               <HV
                                                                                                                                                               <HV
                                                                                                                                                               <HV
                                                                                                                                                               <HV
                                                                                                                                                               <HV
                                                                                                                                                                                            &RYDULDWHVIQFOXGHGLQOXWFRPHCKRLFHDQGTHVWETXDWLRQV




                                                                                                                                                                                    
                                                                                                                                                                                    

                                                                                                                                                                                    
                                                                                                                                                                                    




                                                                                                                                                                                  <HV
                                                                                                                                                                                  <HV
                                                                                                                                                                                  <HV
                                                                                                                                                                                  <HV




                                                                                                                                                                                  <HV
                                                                                                                                                                                  <HV
                                                                                                                                                                                  <HV
                                                                                                                                                                                  <HV
                                                                                                                                                                                  <HV
                                                                                                                                8WLOLW\RI(DUQLQJV 8WLOLW\&RVWRI6FKRROLQJ 7HVW6FRUHV
                                                                                                                                                                                                         Table 3b
                                                       Table 3a                                                                                                                                      Factor Loadings
                                                   Factor Loadings                                                                                                                                       AFQT
                                                 Post-School Utility                                                                                                         Factor Loading                                        Standard Error
                                     Factor Loading                        Standard Error                                             Arithmetic Reasoning             1           1                                                     0
Potential First Period         1        0.1419                                0.0324                                                                                        Total Variance                         Proportion of Total Variance Explained by 1
Utility in High School         2           1                                     0                                                                                              0.7764                                                0.7391
                                     Total Variance           Proportion of Total Variance Explained by
                                         0.3460                  1                           2                                      Coding Speed                     1         0.9672                                               0.0275
                                                              0.0351                        0.8717                                                                            Total Variance                        Proportion of Total Variance Explained by 1
                                                                                                                                                                                  0.7340                                               0.7308
Potential Second Period        1        0.2277                                        0.0519
Utility in High School         2        1.6432                                        0.0262                                         Math Knowledge                   1         0.6313                                               0.0350
                                     Total Variance                   Proportion of Total Variance Explained by                                                               Total Variance                        Proportion of Total Variance Explained by 1
                                         0.8951                         1                            2                                                                          0.8049                                               0.2843
                                                                      0.0349                        0.8717
                                                                                                                                      Word Knowledge                   1         0.7508                                               0.0317
Potential First Period         1        0.1888                                        0.0559                                                                                 Total Variance                        Proportion of Total Variance Explained by 1
Utility in College             2        0.9402                                        0.0676                                                                                     0.6193                                               0.5219
                                     Total Variance                   Proportion of Total Variance Explained by
                                         0.3455                         1                            2                              Paragraph Composition            1         0.8080                                               0.0345
                                                                      0.0634                        0.7718                                                                   Total Variance                         Proportion of Total Variance Explained by 1
                                                                                                                                                                                  0.7061                                               0.5301
Potential Second Period        1        0.3908                                         0.0979                                        Total variance for test w is 2w>1 21 +2w>2 22 +2%w =
Utility in College             2        1.7217                                         0.1203                                                                                                         2w>n  2
                                                                                                                                      Proportion of total variance explained by factor n is         Total Variance =
                                                                                                                                                                                                                  n
                                     Total Variance                    Proportion of Total Variance Explained by
                                                                                                                                                                                                            Choice
                                         1.0860                           1                           1
                                                                      0.0848                         0.8241                                                                  Factor Loading                                        Standard Error
                                                                                                                                      Cost Function*                   1        -2.1250                                               0.5042
Total variance for schooling v in period d is 2v>d>1 21 +2v>d>2  22 + 2%v>d =
                                                                                              2v>d>n  2
                                                                                                                                                                       2        -1.0278                                               0.3799
Proportion of total variance explained by factor n, in schooling v in period d is           Total Variance =
                                                                                                           n
                                                                                                                                                                             Total Variance                           Proportion of Total Variance Explained by
                                                       Gross Returns                                                                                                              0.8951                           1                                 1
                                     Factor Loading                             Standard Error                                                                                                                  0.0349                              0.9096
Xf  Xk                        1        0.2099                                     0.1553
                               2        0.0188                                     0.1786                                            Choice**                         1         2.3349                                              0.4904
                                     Total Variance              Proportion of Total Variance Explained by                                                             2         1.0466                                              0.4277
                                         0.1031                    1                                   1                                                                    Total Variance                         Proportion of Total Variance Explained by
                                                                 0.3027                               0.0889                                                                      6.1544                          1                                 1
                                                  2 2                                    2 2
Total variance = (f>2>1 +f>1>1 k>2>1 k>1>1 ) 1 +(f>2>2 +f>1>2 k>2>2 k>1>2 )  2 + %f>2 + 2%f>1  2%k>2  2%k>1 =
                                                                                                  2                                                                                                             0.5297                             0.0604
                                                          (f>2>n +f>1>n k>2>n k>1>n )2  2                                     * Cost = s +s>1  1 +s>2 2 +%s =
Proportion of total variance explained by factor n =                Total Variance
                                                                                                    n
                                                                                                        =
                                                                                                                                      ** Choice = f>2 +f>1 k>2 k>1 +(f>2>1 +f>1>1 k>2>1 k>1>1 s>1 )1 +(s>2>2 +s>1>2 k>2>2 k>1>2 s>2 ) 2 s %s =
                                                                                                                                      Total variance of cost = 2s>1 21 + 2s>2  22 +  2%s =
                                                                                                                                                                                                                       2s=n  2
                                                                                                                                      Proportion of total variance of cost explained by factor n = Total Variancen of Cost =
                                                                                                                                                                                                        2                                             2
                                                                                                                                      Total variance of choice = (f>2>1 +f>1>1 k>2>1 k>1>1 s>1 ) 21 +(f>2>2 +f>1>2 k>2>2 k>1>2 f>2 ) 22 +2%s =
                                                                                                                                                                                                    (f>2>n +f>1>n k>2>n k>1>n s>n )2  2
                                                                                                                                      Proportion of total variance explained by factor n is                Total Variance of Choice
                                                                                                                                                                                                                                                    n
                                                                                                                                                                                                                                                        =
                           Table 4                                                                             Table 5
     Average Gross Utility In Dierent States (Factual or                                   Factual and Counterfactual Returns for Persons
 Counterfactual) For Persons Who Go To High School                                       Who Go To High School, College, or Are At The Margin
 or Who Go to College and for People At The Margin                                                       (Does Not Include Utility Cost In College)
        (Does Not Include Utility “Cost” or Pyschic Returns to College)                                                     High School1    College2   Utility for People
 Factual or Counterfactual    High School  1
                                               College2
                                                          Utility for People    Gross Return:                                                             at Margin3
      Schooling Level                                        at Margin3         College Vs. High School (Relative)+            -0.0180       0.0126          0.0059
                                                                                Std. Error                                      0.1590       0.0178          0.0227
 High School+                    7.8580         8.6125          8.2991
 Std. Error                      0.0604         0.0737          0.1363          Net Returns:
                                                                                College Vs. High School (Relative)++           -0.2398       0.3161         -0.0402
        ++
 College                          7.7262        8.6885          8.3118          Std. Error                                      0.2502       0.3178          0.0077
 Std. Error                       0.0638        0.0763          0.1413
 +                                 ++
1 E(Vk | choice=high school) and 1     E(Vf )|choice=high school)               College Vs. High School (Relative)+++        -0.4227         0.1892         -0.0416
 +                             ++
2 E(Vk | choice=college) and 2     E(Vf |choice=college)                        Std. Error                                    0.5770         0.0144          0.0229
3+ E(Vk | V=0) and 3++ E(Vf |V=0)                                               +
                                                                               1 E((Vf /Vk )-1|choice=high school)
                                                                                +
                                                                               2 E((Vf /Vk )-1|choice=college)
                                                                               3+ E((Vf /Vk )-1|V=0)
                                                                               1++ E((Vf -Vk -p)/(Vk +p)|choice=high school)
                                                                               2++ E((Vf -Vk -p)/(Vk +p)|choice=college)
                                                                               3++ E((Vf -Vk -p)/(Vk +p)|V=0)
                                                                               1+++ E((Vf -Vk -p)/(Vk )|choice=high school)
                                                                               2+++ E((Vf -Vk -p)/(Vk )|choice=college)
                                                                               3+++ E((Vf -Vk -p)/(Vk )|V=0)
                                                                               We make the distinction between the second and third line in this table because in our
                                                                               framework we cannot separate nonmonetary costs from nonmonetary benefits of going to
                                                                               college, so we allocate ln P both ways.
                              Table 6                                                                             Table 7
                   Returns to College In Terms of                                                   Percentage of People with Negative
           Lifetime Earnings Excluding Tuition For People                                           Returns to College (Net and Gross)
           Who Go To H.S., College, or Are At The Margin                                                                   Gross                   Net
                                  High School1     College2     Earnings for People                                 Utility Earnings        Utility Earnings
                                                                    at Margin3         High School Graduates        56.22%    13.62%        95.91%    14.74%
 Gross Returns:                                                                        College Graduates            39.66%     6.90%        8.32%      7.28%
 College vs. High School+            0.4379         0.5764            0.5274          * Net means net of total cost for utility and net of tuition costs for earnings.
 Std. Error                          0.0228         0.0365            0.0634

 Net Returns:
 College vs. High School++         0.4162        0.5607               0.5092
 Std. Error                        0.0213        0.0366               0.0605
 +
1 E((PVf /PVk )-1|choice=high school)
2+ E((PVf /PVk )-1|choice=college)
3+ E((PVf /PVk )-1|V=0)
1++ E((PVf /(PVk +PVwxlwlrq ))-1|choice=high school)
2++ E((PVf /(PVk +PVwxlwlrq ))-1|choice=college)
3++ E((PV
     P f /(PVk +PVwxlwlrq ))-1|V=0)
PVm = d (1/(1+0.03))d Yd>m , that is, the interest rate is 3%
Earnings are measured in $1000s
                                                       7DEOH8
            3U GLVc≤GL_GMVh≤GM ZKHUHGLLVWKHLWKGHFLOHRIWKHFROOHJHHDUQLQJVGLVWULEXWLRQ
                        DQGGMLVWKHMWKGHFLOHRIWKHKLJKVFKRROHDUQLQJVGLVWULEXWLRQ
+LJK6FKRRO                                               &ROOHJH'HFLOHV
  'HFLOHV                                                                                                                                                                                                                                                                                                                                                
                                                                                                                                                                                                                                                                                                         
                                                                                                                                                                                                                                                                                                         
                                                                                                                                                                                                                                                                                                         
                                                                                                                                                                                                                                                                                                         
                                                                                                                                                                                                                                                                                                         
                                                                                                                                                                                                                                                                                                         
                                                                                                                                                                                                                                                                                                         
                                                                                                                                                                                                                                                                                                         
                                                                                                                                                                                                                                                                                                         
                                                                                                                                                                                                                                                                                                        
 7KXVWKHQXPEHULQURZMFROXPQLLVWKHSUREDELOLW\WKDWDSHUVRQZLWKSRWHQWLDOKLJKVFKRROHDUQLQJVLQWKHMWKGHFLOHRIWKHKLJKVFKRROHDUQLQJVGLVWULEXWLRQKDVSRWHQWLDO
FROOHJHHDUQLQJVLQWKHLWKGHFLOHRIWKHFROOHJHHDUQLQJVGLVWULEXWLRQ.




                                                                       I=
                                                                                                                                                                                                                        I=
                                                                                                                                                                                                                                                                                              I=
                                                                                                                                                                                                                                                                                                                                                   I=




                                                                       I = {T2}
                                                                                                                                                                                                                        I = {T2}
                                                                                                                                                                                                                                                                                              I = {T2}
                                                                                                                                                                                                                                                                                                                                                   I = {T2}




                                                                       I = {T1, T2}
                                                                                                                                                                                                         I = {T1, T2}
                                                                                                                                                                                                                                                                               I = {T1, T2}
                                                                                                                                                                                                                                                                                                                                    I = {T1, T2}




                                                                                        2.68x10
                                                                                                                                                                                                                                                                               4.4763
                                                                                                                                                                                                                                                                                                    7.5549
                                                                                                                                                                                                                                                                                                    7.9354
                                                                                                                                                                                                                                                                                                                                    0.4824
                                                                                                                                                                                                                                                                                                                                                       0.5036
                                                                                                                                                                                                                                                                                                                                                       0.5134




                                                                                        1.18x105
                                                                                                5
                                                                                                                                                                                                         9.74x105
                                                                                                                                                                                                                          1.18x105
                                                                                                                                                                                                                          2.68x105




                                                                       9.73x104
                                                                                                                                                                                                                        Variance(Yc-Yh)
                                                                                                                                                                                                                                                                                                                                                   Variance(Vc-Vh)




                                                                                                                                                                                                                                                                                              Variance(Vc-Pc-Vh)




                                                                                                              7.69x10
                                                                                                                                                                                                                         7.69x10
                                                                                                                                                                                                                                                                               4.2959
                                                                                                                                                                                                                                                                                                   8.6418
                                                                                                                                                                                                                                                                                                                                    0.3020
                                                                                                                                                                                                                                                                                                                                                     0.5068
                                                                                                                                                                                                                                                                                                                                                     2.6462




                                                                                                              1.30x105
                                                                                                                      5
                                                                                                                                                                                                         8.21x104
                                                                                                                                                                                                                         1.30x105
                                                                                                                                                                                                                                 5




                                                                       8.20x104
                                                                                                                                                                                                                                                                                                  12.7911




                                                                                                                                                                                                                        Variance(Yc)
                                                                                                                                                                                                                                                                                                                                                   Variance(Vc)
                                                                                                                                                                                                                                                                                                                                                                                                                                                   Table 9




                                                                                                                                                                                                                                                                                              Variance(Vc-Pc)
                                                                                                                                                                                                                                                                                                                      Net Utility
                                                                                                                                                                                                                                                                                                                                                                                                       Gross Utility




                                                                                                                               2.70x10
                                                                                                                                                                                                                        2.70x10




                                                                                                                                                                         Net Present Value of Earnings


                                                                                                                               2.36x104
                                                                                                                                       5
                                                                                                                                                                                                         1.53x104
                                                                                                                                                                                                                        2.36x105
                                                                                                                                                                                                                                5




                                                                       1.53x104
                                                                                                                                                                                                                                                                               0.1804
                                                                                                                                                                                                                                                                                                0.2632
                                                                                                                                                                                                                                                                                                2.3714
                                                                                                                                                                                                                                                                                                                                    0.1804
                                                                                                                                                                                                                                                                                                                                                     0.2632
                                                                                                                                                                                                                                                                                                                                                     2.3714




                                                                                                                                                                                                                                             Gross Present Value of Earnings
                                                                                                                                                                                                                                                                                                                                                                                             Information Sets for the Agents




                                                                                                                                                                                                                        Variance(Yh)
                                                                                                                                                                                                                                                                                              Variance(Vh)
                                                                                                                                                                                                                                                                                                                                                   Variance(Vh)




                                                                                                                                                                                                         0




                                                                       0
                                                                                                                                                                                                                                                                               0
                                                                                                                                                                                                                                                                                                                                    0




                                                                                                                                                   0.3232
                                                                                                                                                   0.8458
                                                                                                                                                                                                                           0.3234
                                                                                                                                                                                                                           0.8458
                                                                                                                                                                                                                                                                                                     0.4476
                                                                                                                                                                                                                                                                                                     0.6561
                                                                                                                                                                                                                                                                                                                                                        0.3648
                                                                                                                                                                                                                                                                                                                                                        0.8990




                                                                                                                                                                                                                        Correlation(Yc,Yh)
                                                                                                                                                                                                                                                                                                                                                   Correlation(Vc,Vh)




                                                                                                                                                                                                                                                                                              Correlation(Vc-Pc,Vh)
                                                                                                                                                                                                                                                                                                                                                                        Agent's Forecast Variance of High-School-College Returns Under Different




                                                                                  Variance(Yc-Tuition-Yh) Variance(Yc-Tuition) Variance(Yh) Correlation(Yc-Tuition,Yh)
                         7DEOH10
3HRSOHZKRFKRRVHGLIIHUHQWO\XQGHUGLIIHUHQWLQIRUPDWLRQVHWV
             FRPSHQVDWLQJIRUWKHFKDQJHLQULVN
       2ULJLQDO&KRLFH                  )UDFWLRQWKDWFKDQJHFKRLFH
                                      , ^TTH&H+6`      , ^T`
        +LJK6FKRRO                                  
           &ROOHJH                                   
            7RWDO                                    
