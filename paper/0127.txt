                     NB. WOPJN PAP SERIES




          IDTIFIcATION TFEORY FOR TD-VARYING MODELS


                          Thomas F. Coo 1ey
                            Kent D. Wafl

                       Working Paper No. 127




  COMPUTER RESEARCH CENTER FOR ECONOacs AND AGT SCIENCE
               National Bureau of Econcnic Research, Inc.
                         575 Tecbriolo' Square
                     Cambridge, Massachusetts 02139




                             March 1975


                   Fe1ininary:   Not   for quotation

 NBER working papers are     distributed   infonrally and in limited.
 numbers for cormients only. They should       not be quoted without
 written permission.

 This report has not undergone the review accorded official NBER
 publications; in particar, it has not yet been submit-ted for'
 approval by the Board of Directors.

*NBER  Computer Research Center' and Tufts University. Research
 supoorted in part by National Science Foundation Grant GJll5LX3
 to the National Bureau of Economic Research, Inc.
;:NBER Computer Research Center. Research supported in part by
  National Science Foundation Grant SJ-l15X3 to the National
 Bureau   of   Economic Research, Inc.
                                   Abstract

The identification of te-varying coefficient regression models is
investigated using an analysis of the classical information matrix.
The variable coefficients are characterized by autoregressive stochastic

processes,   allowing   the entire model   to be case in state space form.

Thus the unimown stochastic specification parameters and priors can
be interpreted in terms of the coefficient matrices and initial state
vector. Concentration of the lixelihood function on these quantities
allows the identification of each to be considered separately. Suitable
restriction of the form of the state space model, coupled with the
concept of controllabili-j, lead to sufficient conditions for the identi-
fication of the coefficient transition parameters. Partial identification
of the variance-covariance matrix for the random disturbances on the
coefficients is established in a lixe rrnner. Introducing the additional
concept of observabili-ty then provides for necessary and sufficient
conditions for identification of the un)mown priors. The results so
obtained are   completely analogous to those already establishad in the
econometric   literature, namely, that the coefficients of the    reduced

   are always identified subj ect to the absence of multicollineari-ty.
form

Some consistency results are also presented which derive from the above
approach.
                                   1. INROJICN

Identification     is an issue which      arises   in connection with      all parametric
statistical rrodels.      Simply   stated,   the issue   is   w:riether one can   infer
from observed samples the existence of a uricue underlying                 theoretical

structure. Econometricians         have   long c-onoer'ned thamsieves with establish—
ing the conditions for the identifiability of structures whose parameters
are assujnecl to he constant over time. In this paPer we address the
seemingly more complex issue of the identifialili-tyof structures                 when

the regression coefficients themselves are varying stochastically over
tine. This is a relevant problem           because   in recent years increasing
attention   has been focused on the problem of estimating time               varying

structures.'       Although estimation methods have been suggested by several
authors, little attention has        been    paid to the problem   identification
                                                                    of

or to   the asmptotic theory for          these estimatoxs. Many of the issues
we address in this paper have been investigated by others (Tse £ Anton

[1972] and Mebra [1974] for example) bt the context and the results,

as we shall elaborate, are quite different.

        The identification problem for the traditional linear econcretric

model with    uncorrelated errors was first         recognized   by Kooprrans and
Reiersol [1950] and      solutions were provided by Xoopmans          et   al. [1950].
This    theory was later extended and elaborated upon by Fisher [l96]

in his comprehensive book      on the subject. o imoortant papers by

Harinan   [1969,   1971] generalize the earlier theory to encompass ir'dels

with moving average      error processes. ost of          this prior theory

concentrates on conditions which guarantee unique solutions to -the set
                                       —2—




of equations which characterize the structural form parameters in terms
of the reduced form parameters as manifest by Hannan's     solution.

Rothenberg [1971] takes a different approach in characterizing the
identifiability criteria in terms of the infonration matrix of classical
mathematical statistics. Rothenberg's approach has been nicely extended
to a more general representation by Bowden [1973]. It is this latter
approach which is most appropriate to problems we are considering because
of its relative independence from concepts related to sta-tionar9
stochastic process theory.
        The problem we are addressing can best be illustrated by consider-
ing the    state space representation of a model with   stochastically varying
coefficients. We characterize the problem in terms of a regression relation

(or observation equation) and a "state" equation which describes the

evolution of the coefficients over time:


(1.1)                   +
                            e
(1.2)       ti =        +



The   variables   y arid X represent the obser'vables of the system    is a (KxK)

matrix   which governs the transitions of the K component   coefficient         and

e and    are independently and identically distributed random variables
w th mean zero and covariance matrices 2 and Q respectively. It is
clear that   the identification is quite complex in this context      because
we must establish the conditions for the existence of a inique stochastic

characterizaticn of the process governing the coefficients. Identifica-

tion of the coefficients        depends on the identification of the transition
                                         —3—




matrix ,     the covariancematrix Q arid the initial conditions of the
coefficient process               The literature on varying parameter estima-
tion has focused on the problem of       estinating the initial       conditions,

but there has been no discussion of the conditions under which the other

parameters will be identified

       In the following section we formulate the general estimation problem

for   time varying   coefficients arid   present   the recursive (Kalman filtering)

solution. The     initial     condition problem is discussed and      the 1i2:elfl-iood

function, concentrated with respect        to the initial conditions, is presented

to facilitate the derivation of the identification conditions. In

Section 3 the Information Natrix is derived and analyzed to give simple

sufficient conditions for the identification of            and   Q.   It   is shown

that   there are restrictions on the forns of         nich can be identified.
                                                       and Q

       Section   briefly states the conditions for the identifiability of

     using the results of Section 3. As.totic properties of the estimators
are   also   discussed. The final     section   sujiciarizes the results arid draws
some   conclusions.



             2. Thi ESTB4ATICN TI-CRY FOR TL. ARYL G STRUOYJFES


In    the introduction   we   represented the rcblem    of time varying structures

in teis of a single equation regression relationship arid an equation
which characterizes the evolution of the coefficients as a first order

Narkov process. As a point of departure for this section let us consider
how we might generalize this representation. Ideally, we would like to
                                       _14_




be   able to consider general sinailtanecus equation regression relation-
ships. In practice, however, we must restrict ourselves to the considera-
tion of reduced form relationships because the estimation theory for time-
varying structural forms of simultaneous equation systems has not yet

been   developed.
       In many instances one might expect to observe variation that is
systematic   but non-stochastic, or variation that is purel        random. To
include   these possibilities we can   modify our   state equation to the form


(2.1)       ti          +     +w

which admits variation of all three types. If w is equal to zero then

the   variation is   purely systematic. Thus, if the parameters      follow   a

time    trend, a sinusoidal pattern, or are correlated with exogenous var!-
ables it can be represented in this fashion. Similan models have been

considered by Beisley [1973]. If          is a unit vector, w is nonzero

while        0, then the formulation is equivalent to the random coefficients

model    considered by Swamy [1970] and others   where   the   parameters are

regarded as random drawings from a multivariate distribution with mean
vector D in the above representation. Although this is not properly a
state space formulation it can still be handled within this framework.
Thus, the evolution of the state of the system represented by equation
(2.1) is a general one ich encompasses many possibilities.2            In this

paper   we concentrate on stochastic coefficient variation because it is
this    which presents the most difficult problems of identification.3
                                                    -.5—




        wish to ectend the basic state sace model to permit the
         We

coefficient3 to be characterized by more general stochastic processes.
Let    each   of the         obey an autoregressive                process of order      }cl,2).. .K.
Thus


 (2.2)
                     klk,t-l + k2k,t-2                                      + k,t-l

where               0 and        is       a normally distributed zero mean sequentially

independent       random process with           E{nnJ          =         The model can   then
be represented compactly as

(2.3) YX+e1
(2.L)
              t Hz.
(2.5a)        z =   zi       +


where         is the state vector of the model                 describing   the evolution of



                        1_ (                  ...   I     K—
              z      [(zr)       (zt)
                                      2
                                                        (Zr.

V!i-ch   z.k representing the r state viae (tne
                        .                                  .        .
                                                                        substate \lector    for

H is a Kxn (n           rik) matrix of        the       form




                                          A=H
                        2
                                              —5—




and h is a row vector of zeros except for a one in the 1 +               + •+n:kl
column.    The iratrix   is now assumed to be of the form


                 ll12B
                 k1                kk
where




The assumed form of
                    [    2k
                          is a natural one given the autoregressive repre-

sentation of the process governing each coefficient. In the fo1lowin

discussion we assume that the off-diagonal blocks are            null   ratrices

(c.. = 0   ;      since this.     is   a   restriction which must be irnoseâ to.
  1J
derive a sufficient condition for identification. In Section 3.3 we
present a counter example         showing that a model without this restriction

is underidentified.

       To further simplify derivation of the identifiability conditions

we replace the stochastic term of the state equation, Ant by an eçuivalent

term ru where       is Kxl vector of stochastic elements such that


           E[u] =   E[rlt]0   ,
                                   E[utu]           I


and
                                             —7—




             E[n]EQ        E[(HArIt)(HArlY]            [(Hru)(HFu)'].

r is an nxlK matrix      of   the same   stn.icture as A with the exception that the
nonzero rows contain the         corresponding rows of the unique lower triangular

factorization of Q. Henceforth, (2.Sa) will                be replaced   by

(2.5)        z z1         ÷ ru1.

       Models like the one described by ecuations (2.3) -                (2.5) have   been

extonsively explored in the engineering literature following the york

of YiTh'an    [19605] arid a1nsn arid Bucy [l5i]. The first recognition of

the applicability of state space          representations and Kalman filtering
solutions to the problem of estinating eonnetric relationships
with time varying structure was by Rosererg [1968]. Other approaches to
estimating models similar        to   the one described above have been suggested
by   Cocley arid Prescott [1973, 1976] and Sarris [1973]. Here, however,

we shall briefly review only the optiral recursive                estimation     method
because   it is the most convenient for establishing the. identifiability
criteria.

       The   estimation proSier is to obtain estimates of the states,               z,
based on     the observations [yl           yJ. If we let              be an estimate

of z based on observations [y1                         There   tt and define the error
covariasice   matrix of   the    estimated staes as

(2.6)                                              I




then   the solution is easily obtained wnen             z0, , 2 and V are known.
The form     of   the solution   is known   as the     Kalman filter arid   is   rerresented
as
                                                   —8—



(2.7)
                          Ztl/tl
(2.8)
                         t-1it-1            + rr

(2.9)

(2.10)                                  ÷
                         i-1
(2.11)        Kt Pti-1               Mt'

(2.12)
                       -it_      +


(2.13)             =
                       (I_Kt) i-i
where          X H

        Although   the   KaJ.man Filter     has appeared many other         places in the

                                                     be



                                                     '
literature a brief interpretation may                     useful. Equation (2.7) represents

the one step ahead prediction of the states based on obseniations thrcugh

period t when t =         t-1.       The quantity         which   is   ca.lled the "innovations"

series, is obviously the one period      prediction error for the                       The

quantity        is called the gain of the Kalman Filter and Mt is                 the   covariance
matrix of the innovations. In this light                  it is   easy to see   that   the gain
of   the   filter is simply the optiiial prediction correction factor.
        It   is obvious that z, p0,            a2 and r     will   not   be 3iown in most
applications.      The    log    licelihood of the system represented by (2.7) -              (2.13),
however,     is (see Mebra [1972]);


(2.1k)       £(z,P,®) -1/2 1[logIMI +
                                          —9—




where B (2 r, ). Thus.,           estimation proceeds by selecting initial

values of z, F, 0 and using the equations of the KaLrn Filter to

define    the likelihood function.    This process proceeds iteratively and
is    known in the engineering literature as 'fturiing the filter'1.. The
engineering literature, however, has not        in general been sensitive to
problems of     estina-ein the initial state vector z0. Most of the literature
assumes that        has a proper prior distribution which eliminates the
problem. That this is seldom the case, however, is        not a serious Droblem
in    dealing with real time systems with many observations (as in most

engineering apiicatiors)      because it is easily sho-i that   under    the
aDpropriate condi-cions the discrete lKaJman Filter is asymptoticajly
stable and the effects of the initial conditions are
                                                      ultimately forgotten
(see Jazwinski [1970, pp. 20-23]). In econometrics, however, the
situation     is somewhat different in    that we do not deal with real tine
systems,    our observation   intervals          relatively short, arid we
                                          are often

are   often   primarily interested in how the structure of the system evolves
over tine. For all of these reasons it is particularly iuortant to be
sensitive to the stattisig problems. The first correct solution to the
starting problem was propcsed by Rosenberg [2958] arid later generalized
by   him [1973b].    The solution involves concentration of the likelihood
function with respect to the initial state vector z0. This permits

naximum likelihood estimation of z conditional on ci2,         arid r.

recursive     equations for z0 are

(2.16)
           o/o
(2.16) tit_i
                                                        —10—




(2.1'/)          t/t      t/t-l -


(2.18)          Ht        ti_' M1                          it-p
(2 • 19) h           ("
                  t_ 't     '
                        t/t_1         )    M -1
                                            t     lit

                        T             T
     •
                             At             h
where                 and       are   as   defined in equations (2.7) -           (2.13).   The matrix

'   then is simply a function of the transition matrix which extrapolates
the initial parameter vector into the. future.
           Equations (2.15)           (2.20) show that the likelihood function of
the system can be concentrated with respect to the initial state vector
arid thus,       the identifiability of z0 simply requires the invertability
           T
of              Ht) ich in tuni depends on the identification of and r
arid the        properties of the X. Consequently we can                     approach   the problem
at       hand by first looking at the conditions for the identification of
and       F.

           It   is worth noting that the initial condition approach outlined
above          does   not provide estimates of the initial covariance matrix P0.
The consequences            of this have recently been discussed in a paper by

Garbade [1975a].   In econometric applications one should be most
interested in obtaining "smoothed" estimates of the states (zt/T),
that       is, estimates which        use all of the information in the sample.

A smoothing algoritIun which avoids all                        of   the initial condition roblenis
has been derived in Cooley and                 Wall       [1976].
                                       -ii-

                    3. IDEfIIFTOATION OONJ::0':s FOP !1D F


The   identifiability of the unkiown stochastic specification parameters
can be determined through an investiga-nion. of the classical Information
matrix. This approach has wo advantages: First, it permits the
identification    problem to be studied wihin the general framework of
statistical Lrlfonration theo -      a poinn well emçnasized by Bowden          [1973].

Also, it provides a useful connection heteen cermain concepts in control
systems   theory and mather-aical statistics.


3.1 The Information Natrix
       The   classical Information Matrix   of   P. A.   Fisher is defined as

(see Rothenberg [1971] or owden      [1971]):


                                            C




where, i     is the Nxl vector of u kTlown caraneters      ith true value   0;
an n p is the natural logarithm of the density function foP the jointly
observed outputs over the interval 1    < T. Thus, the first step is

to derive the density function for the ointlv observed outputs.
       Combining the state anä odtput euacicns (2.3) — (2.5) permits
the formation of an expression for y ( 1,2,... ,T) exclici-tly in

terms of the vector of unJo;       parameters.




(.1)                           t 1
                 XW + e
                                                               -12-


where,



                  [ ! •••

         wt       —
                      (z_1Y1
                       0


                       :
                                   A


                                             0

                                             2   ]
                                           (zt.1)j...i
                                             :
                                                         I




                                                         I•.1
                                                             •..
                                                                   I
                                                                        0

                                                                        0


                                                                        :
                                                                            -
                                                                                I




                                                                                I:
                                                                                    0
                                                                                        —
                                                                                             0


                                                                                                 U2 ". 0
                                                                                                             0


                                                                                                             0
                                                                                                                       0




                       OtQ.t(zyI
                                   I                                                         :

                  —                I                     I         J    LJ..
                                                                                I
                                                                                                         0            u.
                                                                                                                      t-

   is thus a KxN ntrix with its first n columns                                         exhibiting a block       diagonal
stricture, the (k,k)th block having dension lXnk arid containing the sub—
state vector associated with the kth                                   regression coefficient. The last K(K+1)/2

columns form a matrix in the                     elements              of   u1, with the last K columns of the

last row consisting of u1.                           The       joint observation can now be written
compactly in terms of (3.1):

(3.2)         z                +


where,




                                       I         I
                               0           •••       0
                  xl
                  0        I   X2 i...           10
                           --—I---H---
                   0
                                              —-


                              wJ-

                              w
               w



                              WT.


                       [e1,e2,. .

         The
               e                 iden-ioally dierihuted normal random
                     are independently and

variables so the probability density fictiori for conditional on XT
and      is;

               PT;                =
                                      ()i/2(2)T/2
                                                     e
Taking natural logarithms and then partially differentiating with respect
to it gives

(,3•4)
                            —
                   9n p(Ym;X.. it))




                                              —.
                                                    -m_y    ij
                                                    _—.j_


Finally,       the   above eression may be substituted into the definition for
I(i) to    yield


(3.5)
               I(p) —E{(- w)( c)}
The replacement of YT.Wit) by ET follc'.:s           from the    evaluation   of
at =i°.        The
               Infonmatioh matrix is seen to depen on the exDectation of
a product of random matrices.
                                                    -l'4-


        In order to facilitate tne              evaluation      of the   expectation operation,
we resort    to consideration of the (1,)tn element of l(i):

(3.6)       {I()}..                                  ejwjj]L          m esmwri1)
                                     nT              TK         irT                  1
                          — E1               er
                                                                ij[51 e    1TX




                          r                 E{e}
                                                      E( xtw.)            (1j      )1
                          a tl
                                     E {
                                           (xw.1 )    (xw.)
                                                            3




                          1      T
                                     E{x w.)(wx )}
                                           —t




                          1T
                          __5_
                          o      t=i
Here —t
     x     denotes the t w
                         —of >—Iand w.1 with .th colu of                          . It is     now

possible to cons'uct the Inonration Mtri, element-by-elenent once the
expectation of the outer product ww is computed.
    Appendix A contains the details of the element-by-element construction
of I(i),       with some additional steps required to put l()
            along                                                                     into   a rrre
useful form for analysis. The end result is:

(3.7)       I(p) =---
                      a   t=l t                 t
                                    —15—




t           t is a nonsisigular elementary rasfonition of the data rtrices:

                                              0
                  z1z2




                       0                          V

                xI
        VR x)ck
              I

                               01


                  L0 DKJ
The nxn ma S1 is the genera1ize varance-covariace caix fcr the
state-space process z1, i.e. S1 E{z1zJ (See Bryson Ho [1959]
pp. 320—325). Thc }((K+i)/2 x K(K+1)/2 cetrix D, has a block diagorl
stn.cture

                  AK
                         K—i
                                              -16-


where each, O K, K-i,..., 1) is a kXk rnatrx with unity in
every location. In view of (3.7), it is clear that the identificaticn
of the ui)o-ion-is in           and I depends upon the rank (or, equivalently,
the   positive definiteness) of both
                                               S1 and Dk.
3.2     Identification Conditions

        Two points      are immediately     evident from      (3.7).     First, DK is never

of. full rank          since   each     has only   one linearly    independent column.

Thus all the unJown             elements   in F can   never   be identified simultarec:siy,

but K linear        combinations      of these elements are      identified.      Second,

the identifiability of the parameters depends on whether or not S,

is positive definite. If conditions can                be   found which establish this,

then the unJmown         elements     of   will be identified.

        The question of identification of can                 readily   be resolved with

the   aid of the concept of controllability ;5


        Definition      1. The state-space model
                                             (2.5) is said to be
        uriiforiifty   ccmpletely
                             controllable (UCC) with respect to
        the disturbances, ut_i, if arid only if there exists an
        integer N1 > 0 arid constants c, C2 > 0 such that6
              0 <
                    cI         C(t,t-N1) cI <
        for all t >      N1, where the controllability                    C(t,t-N1)
        is   defined by

(3.8)
                                 t1 t1—T-,
                                        ill (Ot1T
             C(t,-t-N1)

Definition 1 and the restricted structure of                    are    all that   is needed to

prove    the rr5 result of this paper:
        Theorem 1. If the time-varying coefficients of (2.3) have
        their transition relationships realized by (2.5) with
        arid if(2.5) is UCO then: (1) the n-o-o-i sochastic specifica-
        tion parameters in are globally enthiled; and (ii) only X
        linear combinations of the ur.or.s in F are identified.

     Proof: The identifiability resul- ico F bas already been
established from our observations conceco-ing the D, matrix, so
we shall concentrate on the proof of (i . Yrrn      the   state equations for
z, the generalized varience-covariance nanrioes are seen to obey the equation

            St    St_i    + 11

which   has a unique solution given by.

(3.9)       S L
                  t—l
                    S1(
                         t—l
                                E t- --T
                            ) + t—l           F( -T)•.
                                 TO
The second term on the righthand side ci (3. ) is nothing more then the
controllability matrix C(t,O) defined in (3.8) with N1t. rrom the UCC
of (2.5) there will always exist a t=7 such that C(t,O) > 0 for all
                                                                          tt1.
Thus for all tt1 (3.9) will be posifive definite and the identification
of is established.

3.3     Remarks
        The identifiability of   relies abnos exclusively on the special

structureunderlying the state-space noiel, ith the principle condition
being the block diagonal form. This resus in an with its upper left-
hand block identically equal to S1. ne       controllability condition     is
then imposed to giarantee that S1 > fr          t>t1+l. Controllability
                                              all

alone is not a sufficient condition :Tcr iienification of - it mist be
                                               —18-



accomanied by anpropriate          structure in . Actelly,               any ( j) air
which yields    this   structure   in      *   has       linear in p, and is    UOC will
give exactly the same conclusions as Theoreri.l.
       The controllability     requirement ny apocar             imoossible   to verify
a priori since it is stated in teis of the                       owns.
                                                            practice,    In

however, this is no real lijnitation since the block diagonaltiy off
perrilts (2.5) to be viewed as a grouping of K independent subsystems
(sec. Luenberger [1967]. Each "subsystem" will he UCC if arid only if
        0 and at least one nonzero element apocars in the corresponding


row   of r. If each subsystem is         UOC,    then    the overall state-space model

will be     UOC. The   first   requirement is met         if   the specified order, ri1,,
is   less than or    equal to the "tnie         autoregressive      order, while the
second    is met if there is any trace of randomness in each coefficient.

It is difficult      to concieve of     a realistic situation where such conditions

will   be a.b:irt.
       In   the case where all K coefficients obey              first order autoregressive

processes    each lagged         becomes an element of the           state vector (i.e.,HI),

and our results regarding the           lack of complete identification of r agree
 ith   the   results of 'ebra [±9i] co-'.ce: ang -               _:entfablrty      r Q
His   other results are not generally comparable to ours cecause ne consrders

only models with stationary regression relationships, i.e. X constant

for all t.

       The results in the control theory literature (see Tse and \e±nert [1975])

suggest that more general forms for                   can be identified (specifically, block

triangular 'I). The following counter examrle, hcwever, demonstrates that

this specification for will not be identified in the tire-varying coefficients

problem.
                                                      —19—



     Consider the special case where                    Kz2,     r1ri2
                                                                         1 (which more   closely

reserr1es the control thec case), ar let some interccn1irg between

coefficients be pers'itted through 21




                    L21 22
This lower (block) trianguier                     is not inentified. as can be seen by

constructing    the associated 1 () following the steo contained                         in the

Appendix:



                                    E
                            a t=l t          t t


                    Ixiti



                -   I

            =t_                         lxii

                    L                   ii
                                        TiZ2t
                                        I     I




                    H11 11                  i2
                        S11    ll           l2
                                                             0



                    1s12       S12          S22
            t
                                                         110
                    I-



                                0                        110
                        L                         I
                                          —20--




Clearly the upper-left 3X3 block of 2÷ is singular so that all ..
elements are not identified. Whereas the control theory srate model

has    one substate—vector associated with each element of :÷ the time-
varying coefficient model has one subsrate—vector associated with each
      element. 7



              4.    CONTROLLABILITY, OBSERVAEILITY ATD CONSISTCY8


In    Section 2 it was shown that      the likelihood function   can   be concentrated
with respect to            the initial vector. This allowed us o consider the
identification of          and F separately. We now t1rn to the establishment
of the identification conditions for . Conditions which establish the
identification of any ft can easily be derived with the aid of certain
qualitative concepts from control theory as in the previous section.
In addition to controllability, the concept of Uniform Complete Observability
(UCO) is helpful. It is introduced by a second definition:9

        Dfinition 2. The model (2.3) - (2.5) is said to be uniforray
        completely observabie (UCO) witn respec to the ou Lrt,
        and only if there exists an integer N2>O and constanrs c3,c>O
        such that

(4.1)         0 <
                    c31 . O(t,t—N2)    c41 <

        for all          where   the observahility   matrix O(t,t-N2) is
        defined     by

(4.2)         O(t,t—N,)
                     L
                             t
                           Tt—N2
                                    (T—t'
                                      ) T—t
                                                      —21--




Taken together, controllability arid obseiability imrly the identification
of each point on the trai, ectcry for . The nin result is given by the
following theorem

      Theorem 2. If the system defined by (2.3) - (2.5) is both
      UCO and UOC, then    is completely identified.

       Proof: Each   zT can be exresse                          terms   of zL via   solution of

the   underlying state equations, i.e.,

           Z
                  T—t
                          +       T-l -T-l—S
                                  St
Substitution of this exression into that for the observed outputs yields,

           YT XZ + 2.T1 Tl_ST. +
               Az,
                TI: +'•       T


The jointly observed      process, iith                       a an unJrncn parameter    vector, can

no be represented as in regression theri: (let mx

           YAz+V
where

                                                      -
                '-t—N'                          't1
           A    [A
                  tN      L_j
                                       I   ..   A]    —




           V
                         1t-:r+1'


The standand conditions for
                                           z    to be unique are that        both


           AA            (o1ty
                                             -22--




ad the veriarice-covariance matrix


                          11 12
                      H 21          22
           E{Vv}



                                   m..
                   R6.. +     X.
                               12T
                                         m. .—-1
                                                   ri ( m.ij .--1.)
           ra..
            13
                   mm {i,j}

           ó..      Kronecker delta
            3.3


be nonsingular. The first of these is just UCO, while the                 second follows

irrmediately from UCC. Finally, since                I.
                                                          Hzt and H is full rank,       L

is unique whenever z÷         is    identified.
        Theorem   2 can inmediately be specialized to            the problem   of   estimating
unc-io-ipriors. In such a situation, the observation interval ns from
the point of interest, tzO, forward to I. Thus, by setting N- e
find that is identified if and only if

 (.3)      0 <
                  TT+l   ()TT_l              <




              i
The above condition is equivalent to requiring that the matrix

                         2j
                                     (TlyyJ
be of (full) rank K. The full rank interDpetatjOn of (4.L)                can be
interpreted as a generalized muiticollinearity condition.
     Observability and controflabi]i-y are also quite useful in e<arnining
the consistency of tine-varying coefficient esttes. Both
                                                             C(t,--N) and O(,t-.N)
can be used to establish bounds on the estimation erTol'
                                        —23—




rtrix and thus to study the behavior of the err as T - . The essence
othase steps is contained in the following theorem.

      Theorem 3. Let the tine-varying coefficient model (2.3) -
      (2.5) be both UCO and UCO. If t . N max {N1,N2} then the
      best ]J-iear unbiased    estima:e             is never consistent.


      Proof: First, consider     the behavior        of the filter estimation error

variance-covariance    matrix P. Together UCO and UCC                 guarantee   the

existence, uniqueness, and stability of                 as t0,   the   initial tim.e
tends toward -°° Furthermore, for any               prior P > 0, UCO guarantees

tnat P > 0.         (See JaZWLnSki [1970],     Lerrma 7.3,   .       238-239).   Thus
the   filtered   estimate's error variance-covariance        matrix can      never   decay
to zero no matter how much ata, up through tLrne t, has been emplc-ied.
      Next consider   the behavior of the smoother estimaticn error ;ariance-
covariance matrix, F4, ,, which uses all data in the sanijle. Fraser arid
Potter   [1989] have shown that

          t/T       [(f)l +
             is the filtered ermor variance-covanjance of a
tu filter begri at t          1 ama rmnning :orard to  i, while  =

is the one-stan prediction error variance-ccvariam:e of a "back?ard"
filter begun at t T and runnim ackward uivtil               Fixing t
and letting T -*     reveals that cnly    P?/1 will change since only it                is
dependent on T. Now, from       the   oositive definite roperty of
no matter how     fr "back" it   was started (i.e. how large T is), it is
clear that Pt/t+j     > 0   and hence ? >      C.     The smoothed estimate will
always be inconsistent
                                       —24—




     Specializing     the theorem to the estination of n}oc.•r prior:,
    it is clear that inconsistency persists. For diffuse prior:,

           P
            O/T
                  - [ + (pb
                  —
                         "
                           0/i
                                 )_ll - pOil
                                      —




which reveals the lack of consistency under stocbastic excitation for
the s's.


                          5. SULARY AND COi'CLUSlC S



The growing literature on the estimation of models with time Vao-.1ing
regression coefficients has largely ignored the issue of the identif i-
ability of such models and consequently has left in doubt the generality

with which they cart be specified. This paper has used the classical
information matrix of statistics to establish sufficient conditions for
identifiability. The main result of the paper shows that the peramet-r
transition matrix        will be completely identified if it is in block
diagonal form. This special form of the transition matrix pe±mdts
generality     in the specification of the process gcverTling eaoh coefficient
in the   regression relation but riles out the estimation of intercouplings

aIrng    coefficients.    This is an important restriction in that n-any

theoretical considerations which       lead   one to expect stochastic vErieion

in coefficients also suggest that the nvements in the coefficients will

be related. The restriction, however, doe not preclude a priori specifi-

cation    of )iown off diagonal transition parameters, arid it may often be the
case that theory will suggest a priori values for parameters.         The
                                      —25-



identifiability conditions also show that only 1< linear coiriatinns
of the elements of the variance-covariance matrix of the coefficients
can be identified. Diagonal Q matrices will therefore always be identified
      The results of Section 3 are equally aeplicable to the muitivariae
output mod--I where y is an Lxi vector. The observation error variance
     is replaced by an LxL matrix R, which, as before, is always identified.
In addition, the unciown priors   are   identified subjeci to exactly the
same conditions as given in Section          (these conditions being oboalned
independent of the dimension of y÷). The only conolication is in the
evelopment    or the expression for   I ().    R   rep±aces trie scalar a/a2
in   the earlier stages of the algehrac naniou±atcns and        cannot be so
easily tractcred out" of the ensuing cerivaton -        With the ai 0: the
Kronecker product, however, an expression similar       to   (3.9) can   be
obtained   which yields exactly the   same conclusions   as before:


(5.1)      I()           [ØR]
                           L




Since   the Kronecker pn>duct of to positive definite        matrices   is itself
positive definite, the conditions for identification once again derive
from an analysis   of    defined in (3.9).      The    matrices above have

exactly the same form given in the apoandix with the exception -hat t:e
scalars x are replaced by LXl column vectors.
                                      —26—



       Themioo priors ae always identified sabj act to the generalized
multicollii-iearity condition tnduced in Section . If is iccn
a priori then the identification of the i(Xl prior   and hence any point
on the       trajectory, can be established by exaaining the rank of the
associated Observabili-ty Natrix of (.3). Note that with knor, this
check can always be carTied out before estimation is attempted. The
consistency of the prior estimate cannot, however, be established. The
analysis of the dynadic properties of the estiirtion error variance-
covarance matrlx reveals that random excitation of the coefficients
always prevents the limiting distribution for        from attaining a zero
dispersion. Given     comDiete observabili-ty (identification) the most

that   can be achieved is an asymptotically finite error distribution for
the   estimates of the randomly   excited coefficients.
                                    AP?ENDIZ

             DF.EOTATIOI EVALUATION ANP               FOFJATION OF
                          i n\TcRwTIo:

The   Inforniation Matrix construction resented in Seotien               3.1   reduces
to the evaluation of {w.(w.) '} where w.               and
                                                             w denote   ne i     and    th
colurru-is, respectively, of           (see   ecuation (3•3))•12 Since 1 <        i,j < N,
where N is the total nuier of u                n7n stochastic specification para-
meters for the B. process, this airnts to the evaluation of                       matrix
expectations forned from various vector outer products -                 These evalua—

tions are     straightforward       if care is taken to avoid the potential for
confusion. Tras can be achieved               by   decomtosing the evaluations to
three pants, depending on the relati.'e value of the subscripts i arid j.
To this end let denote the set ci integers containing the colurrri
nunibers of the states in W.associacei 7ith the k coefficient in
other words, Ck contains the coluirr ncbers in which the                       states
          are located. For exarle,

              C1     1, 2, ..., n1
              C2     n1-1-l, ...,

                     nKi+i, ..., nl+n2+..

        First,     consider the case in which i,j < n,          i.e., in which w. arid
w.    are both taken from the first n colujru-is of U . Tr:s let
                                                               t               .1 c C arid
                                                                                    £
 J
j c Cm   ,   then
                             E
                                 [1 1           [O...EJ
                                                            H
                                                                  O...iJ
                                                                           I
                                                          Zj,_1


                                 Hi
                             KxX rrtrix      of zeros with the ()th            -
                             position   replaced b' {z1,_i z }
The
      eectaticn E{z±ti z,_1}                 is noth         than the
element of the generalized variance             covariance function St
E(zt z associated with the COT. lete state representatic:. for
the coefficient transitions defined by (2.5). If this elemenT is
denoted by          then14

           E{w.(w.)'}
               3-   i
                             KxKtrjx of zeroes with the (9)th
           el&ent       replaced by s.
                                        1]


       Second, consider the      case   in which i         while j   is ass:ciated
with any of   the last K(K+l)/2 coluTns of U. Then it is easy to see

that


             Ew.(wi'}
               :L KxK null rracrix,                                                (A.2)

since z1 and u1 are independent. The sare result holds with the
roles of I and j exchanged.
     Finally, consider the case in fnich both i and 5 are taan fr:nt
the last 1K(K+l)/2 colurmis in W. In general the matrix expectations
become,
             Ew1(w)?} Z                   UDt        [O...u0_1...o]


                                         LU     J

Obviously    if q q the end result is a KxK null matrix. If                               q then
the expectation operation results in a Kx}( matrix of zeros with one ele-
mart reolaced by unity. The exact location of unity depends, of course,
on the resmactve row -csitions Of u                        arid   u        .    A concise chax'ac-
                                                    ,t—l           q,c-l
terization    analogous to (A.l) does not seem possible. This dffficulty,
however, is of little consequence to the final expression for I(i) as
will beco:r.e apparent below when resort is made to the use of elementary
row and colutn-i transformations.
       The above results concerning the evaluation of                    the expectatic:s      can
no be co;abined with the Oe1nit1ori of the                  (i    j).th elamen-t o () to per—
mit   an elemant-by-elernen- construction             of   I().       More specificeflu;
       1. If i>j        belong    to the first n     columns of W, then
                                     T
                             —V
                                o2   tl x. s.;<.
                                            1] jt.
                                           it                                               (A.3)

       2. If i or j belongs              to the first   n colutris of           while
            the   other is associated with the last K(K+L)/2 colutne
            of W, then


            i.    .()      0.                                                              (A.L)

      .3.   If i and      j ar-s. both    associated with the         last     KG(+1)/2
            columns of U, then
                                        x Efu                       uQ     x1                   (A.5)


                                  T
                     —-
                         1
                      0 Ll        2.    XX ;                    pq


                                                                q.
The   element-by-element construction can                            finally   be corrined   give
the complete InfonTiation Matrix:

           I()   =   -
                                        E    t           Et                                     (A.6)



where,


                         z1                                    1




                                            V1
                                                 V
                                                     2
                              o


                                                              --S




                              o




                                             0
                                  V
                                       kt                     :kxk
                              0
                                       •1
                       st...1
                                  Tk




Although Zk an Vk contain the sane elements cn the diagonal, they have
different dimensions. Thus Z1 is an n1xn1 ma-trix whereas V1 is merely
a scalar (lxi matrix).    The K(Y+l)/2      x K(K+1)/2 matrix   is difficult
to write down in general; it con-tains only zero-es arid oneTs following
the pattern set out below.

                    11010010 001
                    11010010 001.....
                    00101001000
                    11010010001
                    00101001000
                    00000100100
                    110 100 10 0 01
                    00101001000
                    00000100 100
                    00000000010
                    11010010001


     The InSormat ion matrix of (A. S) can be written in a more conrenierit
form for analysis of identification by resorting to elanentarr row and.
column transformations. In Darticular there will always exist nonsingular
matrices P and Q,   composed   of eiemantary excherges of rows an colum-is
respectively, such that
                                       0


                            ————I------
                             0




 whre

                            I.
                             K


                  K



                                      j
 with        a kxk matrix with 1' s everyhre. :rl addition, the srcture
• of     reve:i1s that for evory rc.-: exchange recuired to bring T,, to
 there is a corresponding co1urn-         exchange.   Thus QP', and the final
 expression for the inforrration ratrix becones

                                      t                            0
                                                                            (A.7)

 Since   P    is nonsingular the rank of I(4i), and hence its definiteness,
 derend      on   the rarJ and definiteness of
[1] Aoki, N. [1967], pt5mizatior!       of S:cbasoin   Systems, New Yor::
        Academic Press.

[2] Beisley, David [1973], "On the Date           ation of Systematic Far—meter
       Variation in the Linear Regrassic-       Ycia1', Aunais of Ecorcnfo and
         Social   Measurement, Vol.   3, October, 1973.

[3J Bowden, R. [1973], "The Theory of rarecric :deritification'. Fconometrioa,
       Vol. 1l, No. 6, pp. 1069—1o7.

[]     Bryson, A. E., and Y. C. Ho [1969]. Auclied Cc]nal Control, Naitham,
        Mass.:     Girci—Blaisdell.

[5] Cooley, T. F. and . Prescott [197, Varyirr Preceter P,ecression",
       A Theory and Some Appiications'. Annals ci gccnorric and Social
        MeasurEiTnsnt, Vol. 3, October.
[6]    __________ and _________ L::EJ, 'istirnation in the Presence
         of Stochastic Parameter Variation' Zconornet-'ica,
                                                     —
                                                            Vol. , Un. 1,
        January.

[7] __________        K. B. Wall [l97. On the Identification of Tine
      Varying Stroctures', NBR, \Ocrkin: Pacer Mc. 85, Mar'.
[8] ___________ and _________ [l97E]. "A Mote On Optimal Smoothing
         In Varying Coefficient Reges cn'. ME'E,, March.
[9] Deyst, J. J. [1973], Correction to 'Unn Onions for Asyrrtotic StaLi1it
      of the Discrete i       -Var---        cs_Tatcr', IEEE Tbn. Auto.
        Control, Vol. AC—IS, no. 5, OctcUnr, ct. 5"A563.
[10]                   and C. P. Price [l96. 'Ccodicicns for Asyi'np'rr':ic StabiUnty
        O. L11 JLS.ELC                                           T'-—,        rdC.
        Cortrol, Vol. AC—13, no. 6, Decander, ZD. 7:2—705.
[lii Fisher, F. N. [1966], The Identidicenion Problem in Econonics.         rIe York:
       McGraw—Hill.

[121 Fraser, B. C. and  5. H. Potter [19], The Ontimun Linear imocther as
        a Corrination of o Optinm Ninear FiUners', IEEE Trans. Auto Control.
[13]   Garbade, K. [1975a1, "The Initiali:.eoion Probem In Variable Parameter
         Regression", Working Paper, Nan; Unrk University, August.
[14]                 [1975b], "To Methods f:r Pxandr:ncr the Stabi1i of
         Regression Coeffacaents", Research      ranc'i No. 186, Eccnometrccs
         Research Program, Princeton Unioe:-isry, October.
[15] Hanrian, H. J.    [1969],   "The Identification of Vector Mixed AucoregressOne-
        Moving Average Systems", Bicretrina, 57, po. 223-225.
[16] Uaruian, E. J. [1971], "The Identificatic Problem for :'N1tiie Ectic
         Systems 71th
         751—765.
                         Noving A1erage ors",       Econometrica,   9, Seri r, p.
[17]   Jazwinski, A. H. [1970], Stochastic Focesses and        Filterir:
         New YorK: Acaderc ri7eSS.
                                                                           Theorz,

[18] KJjmem, R. E. [1969a], 'Cn the Ceneral Theory of Control SY5emS!?,
        Proceedings First int'l Congress IFAC, Nbscow, U.S.S.R.
[19]   ___________ [1960b], "A New Anproach to Linear Filcerincr and Prediction
         Problems", 'fransactions of ASME, Series D, Jourr'ial of Basic gineering,
         82, pp. 35—5.
[20] ___________ and R. S. Buoy [1961], Ne Resul-s in Linear Filtering and
       Preiiction Thoory',. Transactions of ASi-, Series D, Journal of 3asic
        giruering, 83, po. 95—108.
[21] ___________ [1963], 'New Nethods in Wiener Filtering Theory", Prc>o.
       Symp. F:nc,. Appl. Ran±xn Function Theory and Prohab lity (Bc'gdanoff
       arid Kozja-i, eds.) Ne York: .7iiey.

[22] Kooprnans, T.C. and C. Reiersol [1950], "The Identification
        Characteristics", Annals of Ilatherratical Statistics, Vol.
                                                                           f   STructural
                                                                           21, p.  165-181.
[23]     _______, H. Rubin ant R..       Leipnik [1950], "Measuring te Equation
                                         B.
       ystams      of £jnainic   Economics", Statistical Inference in Eqmarfc Eo-xnrriic
        Models, Cowles Corrmission Ncnograrh, No. 10
[2] Luenberger, D. G. [1967], 'Canornical Forms for Linear Multivariable
      Systems", IEEE Trans. Auto. Contcol, vol. AC—12, no. 3, June, to. 29)-
         293.

[25] Mebra,     R. K.   [1970], "On   the Identification of Variances ani Adaptive
        KJmn Filtering', lEE Trans.scticns on Automatic Control, Aoril,
        pp. 175_18L.
[26] ____________ [1971], "On Lins Identification of Linear nar±c Susteirs
       with Applications to Kairan Filtering", IEEE ansactions on Automatic
       Conol, AC-16, No. 1, February.
[27] ________   [1972], 'Arroaches to Adaptive Fi1terin', I Transactions
       on Automatic Controi, October'. p. 693-98.
[28] ___________ [197LJ, "Identification in Control arid Econometrics: Similar-
       ities and Differences", Annals of Economic arid Social Measurement, Vol. 3,
        pp. 21_L.7.

[29] Pagan, A. R. [197L], "A Note on the Ectrac-tion of Comnonents from Time
       Series", Econome-trica, L3, pp. 163-168.
[30] Rosenberg, B. N. [1968], "Varying-Parameter Estimaticn, Unpublished Ph.D.
       dissertation, Deparirent of Economics, }-Larvard Uni'.'ersity.
[31] ____________ [1973a1, "A Survey of 3-ochasric Paramecer Regresion",
       Annals of            Econoraic and So: iaJ    il__renenr, 7o1 3, October.
[32] ____________ [1973b],        Analysis of a Cross Section of Tin.e Series
                                       "The
         by Sooc-as-tccel1y Oonvergc-'- nete -e'essior" n-a1s c
         and Social easuroraen-, Vol. 3, lotoher.
[33] Rosenbrock, I-i. H.          [1970], StatSoe.ca and Nlnivaria1.e Theory, New York:
         Jobn Wiley 6 Sons.
[3] Rothenberg, T. J. [1971], ?Identificaton in ?arame-raic Models",
         Econome-ioa,           39., pp.   577.

[35] Sais, A. [1973], "A Bayesian     Arrvoao to E:tion of Tine-VarvinC
         Regrcssicn
         r-         Coefficients', Anain cf Econco-:lo and Social Neanorct,
         JCLO     ,    p. Ju—.
                       —' cm

[36]   Swamy,   P. A. V. B. [1970]. "Fffiofin-c :nferenoe in a                Random   Coefficient
         Regression          Model", Econometrica.      39, p:. 31—323.
[37] Tse,   E. and J. Anton [1972],               'Cr. .ce idenoifi-obility   of Pararetero",
         IEEE Thans. Auto. Control,          Vol. -l7, No. , October.
       _______        and.    H. Weinert [1973],   ntire :etcmin.tion anf ?arareter
         Identification for Nultivaniabje Snoohastic Linaayy Systens?!, i
         Thans. Auto. Control, vol. AC-2.. no. 6. 2eoember, on
                                    FOOTflCTi'S




1. See, for   exaniple, Garbade [1975a, 1975b], Cooley [197], Pager. [197],
     Rosenberg [1973a, 1973b], Cooley and Prescott [19761.

2. For an excellent survey     of   generic relaticus among   models with rot-
     constant coefficients see Rosenberg [137 3a1.
3.   An earlier version of this paper (Cooley and Wall [1375]) anely:oad
     the differences bet :eeri t ie seque:::ial nonsoochas tic variatito pr'obinT
     arid the stochastic variation proh1am.
i.   The ccrdrtucus are that the systr is u              cceolete:j olervable
     and uniformly cort1ete1y controllable.
5. The derivation of ccrro1 lability cotta ned in the folliwin; definition
     is beyond the scope of this panor. The reader nay consult any
     of it            tects such as Zadeb and besoer [1963; pp. 506-509] for
     an excefle::: de'lcbmcnt. It shc:dui be noted that there are. rrmmy
     definitions cf controllability, each with its. own subtle twist (sac
     Rosenbrock   [1970; EThpt. 5   0 6]).

6.   Let x be any arbitr p>l vector and A an pXp matrix. Than ci <             21
     is taken to mean xxr < xAx 6xx<       where  I is the op identify matrix.
7.   There is a basic difference bet:een the identification problem posed in
     this paper and that considered the control engineering literature. There
     is more     ornationi available in the latter case because cash diagonal
     block in the crisiticn matrix (i.e. each state subgroup, z) is
     a soc± to        a d±fferonr. coreme oto ( it )

8.    The authors  wish to ac}cowledge an anorvres referee whose heluful
      corn-ants have greatly facilitated the presentation of Theorem 2.
 9. Obsarvability was another qualitative SVa L-err rromertv firsr defined he
     Kalman [l96[)ai in a purely deteitidnistic framework. Stochastic versions
     of this concept were also aitrociuceG by Ka2nan [1963], Ache [1967], arid
     Jazwinski [l370]. The sole difference between the stocheemi: and determin-
    istic versions is the insertion of R—1, the variarica-covariance of e-,
    bet :aen X- and X. This distinction is, however, immaterial so long as
    R is assumed positive definite.

10. This fundamental result was first obtained by }(abran [1903] for the r
      contlnuoes time  case The e_screte—tarc case as fxst   coos_oared       :eysa
      arid Price [1968] arid Deyst [1973], with subsequent cedagogical oresenca-
      tion given by Jazwirski [19701. The interested readar cs referred to
      any of these for a proof.
ii. iJte-ati'e1y,
      end
                  the exoressio: couli have bee:. itter. usir
                 with equal validity. In a ccninuc us-time frame mr-.
     lcilosyncracy disappeers, I . e. both fil-er variance-covarian:ss -e
     em?loyed.
12. The. time dependence of each colupm of    is supresed in cYder to
     avoid the use of dobie subscrjD-ts :hich are reserved for eements
     os matrices.
13. Fm the definj-tjor of the overall state vecor, z-, giveri in Section 2,
     it is cleaT -that 4
     Ith colunri of W-. fikewise,Zj,
                                   if p is such that z,t_1 appca in the
                                  z,-:-l z ,-t-: if q is such tt z,t_i
     aprears in the jth colua-u-i of W:.
14. The supersorapt t is used hare to aoid triue subscripts while still
    retaining explicit indication of the time de:endence of
