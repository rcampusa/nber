                                NBER WORKING PAPER SERIES




 THE EXACT LAW OF LARGE NUMBERS FOR INDEPENDENT RANDOM MATCHING

                                            Darrell Duffie
                                             Yeneng Sun

                                        Working Paper 17280
                                http://www.nber.org/papers/w17280


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                     August 2011




This work was presented as "Independent Random Matching" at the International Congress of Nonstandard
Methods in Mathematics, Pisa, Italy, May 25-31, 2006, the conference on Random Matching and Network
Formation, University of Kentucky in Lexington, USA, October 20-22, 2006, and the 1st PRIMA
Congress, Sydney, Australia, July 6-10, 2009. We are grateful for helpful comments from an anonymous
associate editor of this journal, two anonymous referees, and from Haifeng Fu, Xiang Sun, Pierre-Olivier
Weill, Yongchao Zhang and Zhixiang Zhang. The views expressed herein are those of the authors
and do not necessarily reflect the views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2011 by Darrell Duffie and Yeneng Sun. All rights reserved. Short sections of text, not to exceed
two paragraphs, may be quoted without explicit permission provided that full credit, including © notice,
is given to the source.
The Exact Law of Large Numbers for Independent Random Matching
Darrell Duffie and Yeneng Sun
NBER Working Paper No. 17280
August 2011
JEL No. C02,D83

                                              ABSTRACT

This paper provides a mathematical foundation for independent random matching of a large population,
as widely used in the economics literature. We consider both static and dynamic systems with random
mutation, partial matching arising from search, and type changes induced by matching. Under independence
assumptions at each randomization step, we show that there is an almost-sure constant cross-sectional
distribution of types in a large population, and moreover that the multi-period cross-sectional distribution
of types is deterministic and evolves according to the transition matrices of the type process of a given
agent. We also show the existence of a joint agent-probability space, and randomized mutation, partial
matching and match-induced type-changing functions that satisfy appropriate independence conditions,
where the agent space is an extension of the classical Lebesgue unit interval.


Darrell Duffie
Graduate School of Business
Stanford University
Stanford, CA 94305-5015
and NBER
duffie@stanford.edu

Yeneng Sun
1DWLRQDO8QLYHUVLW\RI6LQJDSRUH
Department of Economics
1 Arts LinN
6LQJDSRUH
5HSXEOLFRI6LQJDSRUH
ynsun@nus.edu.sg
1       Introduction

A deterministic (almost surely) cross-sectional distribution of types in independent random
matching models for continuum populations had been widely used in several literatures, without
a foundation. Economists and geneticists, among others, have implicitly or explicitly assumed
the law of large numbers for independent random matching in a continuum population, by which
we mean an atomless measure space of agents. This result is relied upon in large literatures
within general equilibrium theory (e.g. [24], [25], [42], [56]), game theory (e.g. [6], [8], [11],
[23], [30]), monetary theory (e.g. [14], [28], [31], [36], [37], [49], [53]), labor economics (e.g. [13],
[32], [45], [46], [48]), illiquid financial markets (e.g. [17], [18], [38], [54], [55]), and biology (e.g.
[9], [29], [41]). Mathematical foundations, however, have been lacking, as has been noted by
Green and Zhou [28].
         We provide an exact law of large numbers for independent random matching, under which
there is an almost-sure constant cross-sectional distribution of types in a large population. We
address both static and dynamic systems with random mutation, partial matching arising
from search, and type changes induced by matching. Based on a suitable measure-theoretic
framework, an exact law of large numbers is proved for each case under an independence
assumption1 on each of the randomization steps: matching, mutation, and matching-induced
type changes. We also show the existence of a joint agent-probability space, and randomized
mutation, partial matching and match-induced type-changing functions that satisfy appropriate
independence conditions, where the agent space is an extension of the classical Lebesgue unit
interval.2
         The mathematical abstraction of an atomless measure space of agents not only provides
a convenient idealization of an economy with a large but finite number of agents, but is of-
ten relied upon for tractability, especially in dynamic settings. It is intractable, at best, to
propagate finite-agent approximations in every time step, given the many underlying state
variables that would be required to capture the payoff relevant states of the economy. This
may partially explain why a plethora of papers in economics have been based on independent
random matching of a continuum of agents, even without a mathematical foundation. In our
setting, the continuum model allows us to show that the time evolution of the cross-sectional
distribution of types is completely determined by the agent-level Markov chain for type, with
    1
      The independence condition we propose is natural, but may not be obvious. For example, a random matching
in a finite population may not allow independence among agents since the matching of agent i to agent j implies
of course that j is also matched to i, implying some correlation among agents. The effect of this correlation
is reduced to zero in a continuum population. A new concept, “Markov conditional independence in types,” is
proposed for dynamic matching, under which the transition law at each randomization step depends on only the
previous one or two steps of randomization.
    2
      A rich measure-theoretic extension of the Lebesgue unit interval was already considered by Kakutani in [34].



                                                        1
explicitly calculated transition matrices. This convenient property is not even considered for
models with a large but finite number of agents.
       For a simple illustration of our results, suppose that each agent within a fraction p of
a continuum population has an item for sale, and that the agents in the remaining fraction
q = 1 − p are in need of the item. If the agents “pair off independently,” a notion that we
formalize shortly, then each would-be seller meets some would-be buyer with probability q.
At such a meeting, a trade occurs. One presumes that, almost surely, in a natural model,
exactly a fraction q of the seller population would trade, implying that a fraction qp of the
total population are sellers who trade, that the same fraction pq of the total population are
buyers who trade, and that the fraction of the population that would not trade is 1 − 2pq.
Among other results, we show that this presumption is correct in a suitable mathematical
framework. Moreover, we prove in Section 5 below that such a model exists.
       Hellwig [31] is the first, to our knowledge, to have relied on the effect of the exact law of
large numbers for random pairwise matching in a market, in a study of a monetary exchange
economy.3 Much earlier reliance can be found in genetics. In 1908, G.H. Hardy [29] and W.
Weinberg (see [9]) independently proposed that with random mating in a large population, one
could determine the constant fractions of each allele in the population. Hardy wrote: “suppose
that the numbers are fairly large, so that the mating may be regarded as random,” and then
used, in effect, an exact law of large numbers for random matching to deduce his results.4 For a
simple illustration, consider a continuum population of gametes consisting of two alleles, A and
B, in initial proportions p and q = 1 − p. Then, following the Hardy-Weinberg approach, the
new population would have a fraction p2 whose parents are both of type A, a fraction q 2 whose
parents are both of type B, and a fraction 2pq whose parents are of mixed type (heterozygotes).
These genotypic proportions asserted by Hardy and Weinberg are already, implicitly, based on
an exact law of large numbers for random matching in a large population. In order to consider
the implications for the steady-state distribution of alleles, suppose that, with both parents of
allele A, the offspring are of allele A, and with both parents of allele B, the offspring are of
allele B. Suppose that the offspring of parents of different alleles are, say, equally likely to be of
allele A or allele B. The Hardy-Weinberg equilibrium for this special case is a population with
steady-state constant proportions p = 60% of allele A and q = 40% of allele B. Provided that
   3
      Diamond [12] had earlier treated random matching of a large population with, in effect, finitely many
employers, but not pairwise matching within a large population. The matching of a large population with a
finite population can be treated directly by the exact law of large numbers for a continuum of independent
random variables. For example, let N (i) be the event that worker i is matched with an employer of a given
type, and suppose this event is pairwise independent and of the same probability p, in a continuum population
of such workers. Then, under the conditions of [50], the fraction of the population that is matched to this type
of employer is p, almost surely.
    4
      Later in his article, Hardy did go on to consider the effect of “casual deviations,” and the issue of stability.



                                                          2
the law of large numbers for random matching indeed applies, this is verified by checking that,
if generation k has this cross-sectional distribution, then the fraction of allele A in generation
k + 1 is almost surely 0.62 + 0.5 × (2 × 0.6 × 0.4) = 0.6. This Hardy-Weinberg Law, governing
steady-state allelic and genotypic frequencies, is a special case of our results treating dynamics
and steady-state behavior.
       In applications, random-matching models have also allowed for random mutation of
agents, obviously in genetics, and in economics via random changes in preferences, productivity,
or endowments. Typical models are also based on “random search,” meaning that the time at
which a given agent is matched is also uncertain. With random search, during each given time
period, some fraction of the agents are not matched. Finally, in some cases, it is important
that the impact of a match between two agents on their post-match types is itself random, as in
[37] and [46]. For instance, trade sometimes depends on a favorable outcome of a productivity
shock to the buyer, allowing the buyer to produce, or not, the output necessary to pay the
seller. In some models, once paired by matching, agents use mixed strategies for their actions,
causing another stage of random type changes. It is also often the case that one wishes not only
an (almost surely) deterministic cross-sectional distribution of types as a result of each round
of matching, but also a cross-sectional distribution of types that is constant over time, as in the
Hardy-Weinberg Equilibrium. It may also help if one knows the cross-sectional distribution of
the type process almost surely. We provide a collection of results treating all of these cases.
       Our results include the potential for random birth and death, because we allow for
random mutation of types, which can include “alive” or “dead.” We do not, however, consider
population growth, which could be handled by relatively straightforward extensions of the
results here, that we leave for future work. It would also be straightforward to extend our
results in order to consider the effect of “aggregate shocks,” for example common adjustments
to the parameters determining speed of matching, according to a Markov chain, as in the
business-cycle effects on employer-worker matching studied by5 Mortensen and Pissarides [46].
When we treat dynamic models, we take only the discrete-time case, although continuous-time
models of random matching are also popular (e.g. [17], [46], [53], [55]). Using different methods,
we are in the process of extending our results to continuous-time settings.
       Because there are fundamental measurability problems associated with a continuum of
independent random variables,6 there has up to now been no theoretical treatment of the exact
law of large numbers for independent random matching among a continuum population. In
[50], various versions of the exact law of large numbers and their converses are proved by direct
  5
     Ljungqvist and Sargent [39] present a discrete-time version of the Mortensen-Pissarides model, which is
further treated in discrete time by Cole and Rogerson [10] and by Merz [43].
   6
     See, for example, [4], [15], [16], [20], [33] and discussions in [50].



                                                     3
application of simple measure-theoretic methods in the framework of an extension of the usual
product probability space that retains the Fubini property.7 This paper adopts the measure-
theoretic framework of [50] to provide the first theoretical treatment of the exact law of large
numbers for a general independent random matching among a continuum population. The
existence of such an independent random matching is shown in [19] for the case of a hyperfinite
number of agents via the method of nonstandard analysis. Since the unit interval and the class
of Lebesgue measurable sets with the Lebesgue measure provide the archetype for models of
economies with a continuum of agents, we show in this paper that one can take an extension of
the classical Lebesgue unit interval as the agent space for the construction of an independent
random matching in both static and dynamic settings.
       In comparison, earlier papers on random matching, such as [7], [26], and [42], consider
either the non-existence of random matching with certain desired properties, or provide for
an approximate law of large numbers for some particular random matching with a countable
population (and with a purely finitely additive sample measure space in [26]). Section 6 provides
additional discussion of the literature.8 A continuum of agents with independent random types
is never measurable with respect to the completion of the usual product σ-algebra, except in
the trivial case that almost all the random types in the process are constants.9 Instead, we
work with extensions of the usual product measure spaces (of agents and states of the world)
that retain the Fubini property, allowing us to resolve the measurability problem in this more
general measure-theoretic framework.
       The remainder of the paper is organized as follows. Section 2 is a user’s guide, going
immediately to the form and implications of the main results, and putting off most of the
underlying mathematical developments. In Section 3, we consider random full and partial
matchings in the static case. Section 3.1 includes a brief introduction of the measure-theoretic
framework (a Fubini extension). A random full matching is formally defined in Section 3.2 and
its properties shown in Theorem 1. Random partial matching (the case of search models) is
considered in Section 3.3.
   7
      While it is relatively straightforward to construct examples of a continuum of independent random variables
whose sample means or distributions are constant (see, for example, Anderson [4], Green [27], or Judd [33]), one
can also construct other pathological examples of a continuum of independent random variables whose sample
functions may not be measurable, or may behave in “strange” ways. (For example, the sample function can be
made equal to any given function on the continuum almost surely, as in [33] and [50].) By working with a Fubini
extension of the usual product probability space, however, one is able to obtain general results on the exact law
of large numbers, as in [50], while at the same time ruling out such pathologies.
    8
      To prove our results, we cannot use the particular example of an iid process constructed from the coordinate
functions of the Kolmogorov continuum product, as in [33]. While the Kolmogorov continuum product space
gives a product measure on the continuum product easily, there is no simple way to define a useful measure on
the space of matching functions (which are special one-to-one and onto mappings on the agent space) that will
lead to an independent random matching.
    9
      See, for example, Proposition 2.1 in [50].



                                                        4
          Section 4 considers a dynamical system for agent types, allowing at each time period for
random mutation, partial matching, and match-induced random type changes. We introduce
the condition of Markov conditional independence to model these three stages of uncertainty.
Markov conditional independence allows us to show that the individual type processes of al-
most all agents are essentially pairwise independent Markov chains. Using this last result, we
can then show that there is an almost-sure constant cross-sectional distribution of types in a
large population (including stationarity of the cross-sectional distribution of agent types), and
moreover, that the time evolution of the cross-sectional distribution of types is (almost surely)
completely determined as that of a Markov chain with known transition matrices. All of these
results are included in Theorem 3.
          Existence results for random matching, in static settings and in dynamic settings that are
(Markov conditionally) independent in types, are stated in Section 5. Although the Lebesgue
unit interval fails to be an agent space suitable for modeling a continuum of agents with
independent random matching, we show that an extension of the Lebesgue unit interval does
work well in our setting.
          A brief discussion of the relevant literature is given in Section 6. Proofs of Theorems 1,
2 and 3 (as stated in Sections 3 and 4) are given in Appendix 1 (Section 7). Proofs of Theorem
4 and Corollaries 1 and 2 (as stated in Section 5) are given in Appendix 2 (Section 8).

2        User’s Guide

This section gives a simple understanding of some of the key results, without detailing most of
the definitions and arguments that we later use to formalize and prove these results.
          We fix a probability space (Ω, F, P ) representing uncertainty, an atomless probability
space (I, I, λ) representing the set of agents,10 and a finite agent-type space S = {1, . . . , K}.11
As shown in Section 5 below, one may take the agent space (I, I, λ) to be an extension of the
Lebesgue unit interval (L, L, η) in the sense that I = L = [0, 1], the σ-algebra I contains the
Lebesgue σ-algebra L, and the restriction of λ to L is the Lebesgue measure η.
          In order to discuss independent random matching, we consider a product probability
space (I × Ω, W, Q) such that W contains the product σ-algebra I ⊗ F, and such that the
marginals of Q on (I, I) and (Ω, F) are λ and P respectively. This extension (I × Ω, W, Q) of
    10
     A probability space (I, I, λ) is atomless if there does not exist A ∈ I such that λ(A) > 0, and for any
I-measurable subset C of A, λ(C) = 0 or λ(C) = λ(A).
  11
     In order to study independent random partial matching systematically in the static and dynamic settings,
we focus on the finite type case here. It is pointed out in Remarks 1 – 4 below that for the case of independent
random full matching, some results in [19] and this paper can be readily restated to the setting of a complete
separable metric type space. However, an independent random partial matching with random mutation and
match-induced random type changes in such a type space would require transition probabilities in a general
setup. Some new tools will be needed to handle that case, which is beyond the scope of this paper.


                                                       5
the product of the two underlying spaces must have the basic Fubini property in order for the
following law-of-large numbers results to make sense.
        A cross-sectional or probability distribution of types is an element of ∆ = {p ∈ RK
                                                                                          + :
p1 + · · · + pK = 1}.
        For each time period n ≥ 1, we first have a random mutation, and then a random partial
matching, followed by a random type changing for matched agents. The random mutation is
modeled by some W-measurable function hn : I × Ω → S that specifies a mutated type for
agent i at state of nature ω.
        As for the random partial matching at time n, there is some random matching function
πn   : I × Ω → I ∪ {J}, where {J} is a singleton representing ‘unmatched,’ that specifies either
an agent j = π n (i, ω) 6= i in I to whom i is matched in state ω, or specifies the outcome
π n (i, ω) = J that i is not matched. It must be the case that if i is matched to j, then j is
matched to i. Specifically, for all ω, i, and j, π n (i, ω) = j if and only if π n (j, ω) = i. Let g n be
a W-measurable matching type function on I × Ω into S ∪ {J}, such that g n (i, ω) is the type
of the agent j = π n (i, ω) who is matched with agent i in state of nature ω, or g n (i, ω) = J if
π n (i, ω) = J.
        When agents are not matched, they keep their types. Otherwise, the types of two
matched are randomly changed, with a distribution that depends on their pre-match types.
Some W-measurable αn : I × Ω → S specifies the type αn (i, ω) of agent i ∈ I in state of the
world ω after the type changing. The associated cross-sectional distribution of types at time n
is the ∆-valued random variable pn (ω) (also denoted by pnω ) defined by

                                 pnk (ω) = λ({i ∈ I : αn (i, ω) = k}).

The initial type function α0 : I → S is non-random. As usual, we let gin and αin denote
the random variables whose outcomes in state ω are g n (i, ω) and αn (i, ω), respectively. For
a realized state of nature ω, hnω , πωn , gωn and αωn denote, respectively, the realized mutation,
matching, matching type and type functions on I.
        The parameters of a random matching model with type space S are

     1. Some initial cross-sectional distribution p0 ∈ ∆ of types (the type distribution induced
        by α0 ).

     2. A K × K transition matrix b fixing the probability bkl that an agent of type k mutates
        to an agent of type l in a given period, before matching.

     3. Some q ∈ [0, 1]S specifying, for each type k, the probability qk that an agent of type k is
        not matched within one period. An agent who is not matched keeps her type in a given
        period, but may mutate to another type at the beginning of next period.

                                                   6
   4. Some ν : S × S → ∆ specifying the probability νkl (r) that an agent of type k who is
      matched with an agent of type l will become, after matching, an agent of type r.

       Fixing the parameters (p0 , b, q, ν) of some random matching model, under a natural
definition of “Markov conditional independence for mutation, matching, and type changing”
which we provide later in this paper, one conjectures the following results.12

    • At each time n ≥ 1, the realized cross-sectional type distribution pn (ω) is P -almost
      surely (henceforth, “a.s.”) equal to the expected cross-sectional type distribution pn =
      R n
       Ω p (ω) dP (ω).

    • After the random mutation step at time n, the fraction pnl (ω) of the population of a given
      type l is almost surely K     n−1
                                        bkl , denoted by p̃nl .
                             P
                              k=1 p k

    • At each time n ≥ 1 and for any type k, the fraction of the population of type k that are
      not matched13 at period n is

                                   λ({i ∈ I : hnω (i) = k, gωn (i) = J}) = p̃nk qk                a.s.                    (1)

      For any types k, l ∈ S, the fraction of the population who are agents of type k that are
      matched with agents of type l is
                                                                           p̃nk (1 − qk )p̃nl (1 − ql )
                          λ({i : hnω (i) = k, gωn (i) = l}) =                  PK n                       a.s.            (2)
                                                                                  r=1 p̃r (1 − qr )

    • At the end of each time period n ≥ 1 (after match-induced type changing), for each type
      r, the new fraction of agents of type r is
                                                                K
                                                               X    νkl (r)p̃nk (1 − qk )p̃nl (1 − ql )
                         pnr (ω)   =   p nr   =   p̃nr qr   +             PK n                            a.s.            (3)
                                                              k,l=1            t=1 p̃t (1 − qt )
                                          PK      n−1
      Using the fact that p̃nl =            k=1 p k   bkl , one has a recursive formula                    for p n in terms of
      p n−1 , and thus p n (and         also pn ) can be computed directly from p0 .

    • For λ-almost every agent i ∈ I, the type process αi0 , αi1 , αi2 , . . . is an S-valued Markov
      chain,14 with a K × K transition matrix z n specifying the probability of transition from
   12
      Models with random full matching, or with deterministic match-induced type changing, or without random
mutation, are special cases of our model. To avoid random mutation, one can simply take bkk to be one for all
k ∈ S. If qk = 0 for all k ∈ S, then an agent will be matched with probability one. For k, l ∈ S, if νkl (r) is one
for some r, then the match-induced type change is deterministic.
   13                      PK n
      We note that            r=1 p̃r(1 − qr ) is the fraction of population who are matched, while
                    K
(p̃n                      n
                   P
   l (1 − q l )) /     p̃
                    r=1 r   (1 − q r )   is the relative fraction of the population who are matched agents of
type l among all matched agents.
   14
      For a complete statement of what constitutes a Markov process, one must fix a filtration {F0 , F1 , . . .} of
sub-σ-algebras of F. For our purposes, it is natural, and suffices for this result, to take Ft to be the σ-algebra
generated by {αis : s ≤ t}.


                                                                    7
       type k at time n − 1 to type l at time n (for n ≥ 1), given by
                                                                        K
                                                                        X               (1 − qr )(1 − qt )p̃nt
                    n
                   zkl   =   P (αin   =l|   αin−1   = k) = ql bkl +          νrt (l)bkr PK                      ,   (4)
                                                                                                             n
                                                                       r,t=1               r0 =1 (1 − qr0 )p̃r0


       provided the event {αin−1 = k} has positive probability.

    • For P -almost every state of nature ω ∈ Ω, the cross-sectional type process α0 , αω1 , αω2 , . . .
       is an S-valued Markov chain with the transition matrix z n at time n − 1 and initial
       type distribution p0 . Thus, almost surely, the evolution of the fractions of each type is
       deterministic and coincides with the evolution of the probability distribution of type for
       a given agent, except for the initial distributions.15

       Given the mutation, search and match-induced type changing parameters (b, q, ν), one
also conjectures that, under the assumption of “Markov conditional independence,” there is
some steady-state constant cross-sectional type distribution p∗ in ∆, in the sense that, for
the parameters (p∗ , b, q, ν) we have, almost surely, for all n ≥ 0, pnω = p∗ . Moreover, the
Markov chains for the type process αi0 , αi1 , αi2 , . . . (for λ-almost every agent i ∈ I) and for
the cross-sectional type process α0 , αω1 , αω2 , . . . (for P -almost every state of nature ω ∈ Ω) are
time-homogeneous, and the latter has p∗ as a stationary distribution. That is, for some fixed
transition matrix z, we have z n = z for all n ≥ 1, and we have, for all ` in S,
                                                     K
                                                     X
                                                           zkl p∗k = p∗` .
                                                     k=1

       We will demonstrate all of the results stated above, based on the following version of the
exact law of large numbers, proved in Sun [50]. Given some W-measurable f : I × Ω → X,
where X is a finite set (we state the result for general X in Section 7.1), the random variables
{fi : i ∈ I}, defined by fi (ω) = f (i, ω), are said to be essentially pairwise independent16 if
for λ-almost all i ∈ I, the random variables fi and fj are independent for λ-almost all j ∈ I.
For brevity, in this case we say that f itself is essentially pairwise independent. With the
assumption of the Fubini property on (I × Ω, W, Q), the exact law of large numbers in [50]
(which is stated as Lemma 1 in Section 7.1, for the convenience of the reader) says that if
f is essentially pairwise independent, then the sample functions fω have essentially constant
  15
     We do not take the initial probability distribution of agent i’s type to be p0 , but rather the Dirac measure
at the type α0 (i). See Footnote 30 for a generalization.
  16
     This condition is weaker than pairwise/mutual independence since each agent is allowed to have correlation
with a null set of agents (including finitely many agents since a finite set is null under an atomless measure). For
example, the agent space I is divided into a continuum of cohorts, with each cohort containing a fixed number
L of agents (L ∈ N). If the agents across cohorts act independently (correlation may be allowed for agents in
the same cohort), then the essential pairwise independence condition is satisfied.


                                                               8
distributions. Then, the notion of Markov conditional independence is used to derive the
essential pairwise independence of the n-th period mutation, matching and type processes hn ,
g n and αn , as well as the essential pairwise independence of the Markov chains αi0 , αi1 , αi2 , . . .,
which imply all the results stated above. In addition, we show the existence of an independent
random matching satisfying all the above properties when the agent space is an extension of
the classical Lebesgue unit interval.

3        Exact law of large numbers for independent random matchings

In this section, we consider independent random matchings, full or partial, in a static setting.
Some background definitions are given in Section 3.1. Exact laws of large numbers for random
full and partial matchings are presented, respectively, in Sections 3.2 and 3.3, and their proofs
are given in Section 7.2 of Appendix 1.

3.1        Some background definitions

Let (I, I, λ) and (Ω, F, P ) be two probability spaces that represent the index and sample
spaces respectively.17 In our applications, (I, I, λ) is an atomless probability space that is
used to index the agents. If one prefers, I can be taken to be the unit interval [0, 1], I an
extension of the Lebesgue σ-algebra L, and λ an extension of the Lebesgue measure η. Let
(I × Ω, I ⊗ F, λ ⊗ P ) be the usual product probability space. For a function f on I × Ω (not
necessarily I ⊗ F-measurable), and for (i, ω) ∈ I × Ω, fi represents the function f (i, · ) on Ω,
and fω the function f ( · , ω) on I.
            In order to work with independent type processes arising from random matching, we
need to work with an extension of the usual measure-theoretic product that retains the Fubini
property. A formal definition, as in [50], is as follows.

Definition 1 (Fubini extension)               A probability space (I×Ω, W, Q) extending the usual product
space (I × Ω, I ⊗ F, λ ⊗ P ) is said to be a Fubini extension of (I × Ω, I ⊗ F, λ ⊗ P ) if for any
real-valued Q-integrable function g on (I × Ω, W), the functions gi = g(i, · ) and gω = g( · , ω)
are integrable respectively on (Ω, F, P ) for λ-almost all i ∈ I and on (I, I, λ) for P -almost all
                           R             R
ω ∈ Ω; and if, moreover, Ω gi dP and I gω dλ are integrable, respectively, on (I, I, λ) and on
                 R             R R                R R          
(Ω, F, P ), with I×Ω g dQ = I Ω gi dP dλ = Ω I gω dλ dP . To reflect the fact that the
probability space (I × Ω, W, Q) has (I, I, λ) and (Ω, F, P ) as its marginal spaces, as required by
the Fubini property, it will be denoted by (I × Ω, I  F, λ  P ).
    17
         All measures in this paper are complete, countably additive set functions defined on σ-algebras.




                                                           9
       An I  F-measurable function f will also be called a process, each fi will be called a
random variable of this process, and each fω will be called a sample function of the process.
       We now introduce the following crucial independence condition. We state the definition
using a complete separable metric space X for the sake of generality; in particular, a finite
space or an Euclidean space is a complete separable metric space.

Definition 2 (Essential pairwise independence)               An I  F-measurable process f from I × Ω
to a complete separable metric space X is said to be essentially pairwise independent if for
λ-almost all i ∈ I, the random variables fi and fj are independent for λ-almost all j ∈ I.18

3.2    An exact law of large numbers for independent random full matchings

We follow the notation in Section 3.1. Below is a formal definition of random full matching.

Definition 3 (Full matching)

   1. Let S = {1, 2, . . . , K} be a finite set of types, α : I → S an I-measurable type function
       of agents. Let p denote the distribution on S. That is, for 1 ≤ k ≤ K and Ik = {i ∈ I :
       α(i) = k}, let pk = λ(Ik ) for each 1 ≤ k ≤ K.

   2. A full matching φ is a one-to-one mapping from I onto I such that, for each i ∈ I,
       φ(i) 6= i and φ(φ(i)) = i.

   3. A random full matching π is a mapping from I × Ω to I such that (i) πω is a full
       matching for each ω ∈ Ω; (ii) the type process g = α(π) is a measurable map from
       (I × Ω, I  F, λ  P ) to S;19 (iii) for λ-almost all i ∈ I, gi has distribution p.

   4. A random full matching π is said to be independent in types if the type process g is
       essentially pairwise independent.20

       Condition (1) of this definition says that a fraction pk of the population is of type k.
Condition (2) says that all individuals are matched, there is no self-matching, and that if i is
matched to j = φ(i), then j is matched to i. Condition (3) (iii) means that for almost every
agent i, the probability that i is matched to a type-k agent is pk , the fraction of type-k agents
in the population. Condition (4) says that for almost all agents i and j ∈ I, the event that
  18
     Two random variables φ and ψ from (Ω, F, P ) to X are said to be independent, if the σ-algebras σ(φ) and
σ(ψ) generated respectively by φ and ψ are independent.
  19
     In general, we only require the measurability condition on the type process g rather than on the random
full matching π; the latter implies the former. This allows one to work with a more general class of random full
matchings.
  20
     This weaker condition of independence (see Footnote 16) allows one to work with a more general class of
independent random matchings.


                                                      10
agent i matched to a type-k agent is independent of the event that agent j matched to a type-l
agent, for any k and l in S.
        Because agents of type k have a common probability pl of being matched to type-l agents,
Condition (4) allows the application of the exact law of large numbers in [50] (which is stated
as Lemma 1 in Section 7.1 below) in order to claim that the relative fraction of agents matched
to type-l agents among the type-k population is almost surely pl (or, intuitively, frequency
coincides with probability). This means that the fraction of the total population consisting of
type-k agents that are matched to type-l is almost surely pk · pl . This result is formally stated
in the following theorem, whose proof is given in Section 7.2.

Theorem 1 Let α : I → S be an I-measurable type function with type distribution p =
(p1 , . . . , pK ) on S. Let π be a random full matching from I × Ω to I. If π is independent in
types, then for any given types (k, l) ∈ S × S,

                                   λ({i : α(i) = k, α(πω (i)) = l}) = pk · pl                                        (5)

holds for P -almost all ω ∈ Ω.21

3.3    An exact law of large numbers for independent random partial matchings

We shall now consider the case of random partial matchings, starting with the formal definition.

Definition 4 Let α : I → S be an I-measurable type function with type distribution p =
(p1 , . . . , pK ) on S. Let π be a mapping from I × Ω to I ∪ {J}, where J denotes “no match.”

   1. We say that π is a random partial matching with no-match probabilities q1 , . . . , qK in
       [0, 1] if (i) for each ω ∈ Ω, the restriction of πω to I − πω−1 ({J}) is a full matching on
       I − πω−1 ({J});22 (ii) after extending the type function α to I ∪ {J} so that α(J) = J, and
       letting g = α(π), we have g measurable from (I × Ω, I  F, λ  P ) to S ∪ {J}; (iii) for
       λ-almost all i ∈ Ik , P (gi = J) = qk and23
                                                         (1 − qk )pl (1 − ql )
                                            P (gi = l) = PK                    .
                                                            r=1 pr (1 − qr )
  21
      It means that the joint distribution of α and gω is the product distribution p ⊗ p on S × S. As noted in
Remarks 1 and 3, one can readily generalize the finite type case to the case of a complete separable metric type
space for the case of independent random full matching.
   22
      This means that an agent i with πω (i) = J is not matched, while any agent in I − πω−1 ({J}) is matched.
This produces a partial matching on I.
   23
      Note that if an agent of type k is matched, its probability of being matched to a type-l agent should be
proportional to the type distribution        of matched agents. The fraction of the population of matched agents
among the total population is K
                                   P
                                     r=1  p r (1 − qr ). Thus, the relative fraction of type l matched agents to that of
all the matched agents is (pl (1 − ql ))/ K
                                              P
                                                 r=1 pr (1 − qrP
                                                               ). This implies that the probability that a type-k agent
is matched to a type-l agent is (1 − qk )(pl (1 − ql ))/ K
                                                                                          PK
                                                                  r=1 pr (1 − qr ). When    r=1 pr (1 − qr ) = 0, we have
pk (1 − qk ) = 0 for allP1 ≤ k ≤ K, in which case almost no agents are matched, and we can interpret the ratio
((1 − qk )pl (1 − ql ))/ Kr=1 pr (1 − qr ) as zero.



                                                           11
     2. A random partial matching π is said to be independent in types if the process g (taking
          values in S ∪ {J}) is essentially pairwise independent.24

          The following result, proved in Section 7.2, generalizes Theorem 1 to the case of random
partial matchings.

Theorem 2 If π is an independent-in-types random partial matching from I × Ω to I ∪ {J}
with no-match probabilities q1 , . . . , qK then, for P -almost all ω ∈ Ω:

     1. The fraction of the total population consisting of unmatched agents of type k is

                                   λ({i ∈ I : α(i) = k, gω (i) = J}) = pk qk .                            (6)


     2. For any types (k, l) ∈ S 2 , the fraction of the total population consisting of type-k agents
          that are matched to type-l agents is
                                                               pk (1 − qk )pl (1 − ql )
                             λ({i : α(i) = k, gω (i) = l}) =     PK                     .                 (7)
                                                                    r=1 p r (1 − q r )

4        A dynamical system with random mutation, partial matching, and type
         changing that is Markov conditionally independent in types

In this section, we consider a dynamical system with random mutation, partial matching and
type changing that is Markov conditionally independent in types. We first define such a dy-
namical system in Section 4.1. Then, we formulate in Section 4.2 the key condition of Markov
conditional independence in types, and finally present in Theorem 3 of Section 4.3 an exact
law of large numbers and stationarity for the dynamical system.

4.1       Definition of a dynamical system with random mutation, partial matching
          and type changing

Let S = {1, 2, . . . , K} be a finite set of types. A discrete-time dynamical system D with random
mutation, partial matching and type changing in each period can be defined intuitively as
follows. The initial distribution of types is p0 . That is, p0 (k) (denoted by p0k ) is the initial
fraction of agents of type k. In each time period, each agent of type k first goes through a
stage of random mutation, becoming an agent of type l with probability bkl . In models such as
[17], for example, this mutation generates new motives for trade. Then, each agent of type k
is either not matched, with probability qk , or is matched to a type-l agent with a probability
proportional to the fraction of type-l agents in the population immediately after the random
    24
     This means that for almost all agents i, j ∈ I, whether agent i is unmatched or matched to a type-k agent
is independent of a similar event for agent j.


                                                     12
mutation step. When an agent is not matched, she keeps her type. Otherwise, when a pair of
agents with respective types k and l are matched, each of the two agents changes types; the
type-k agent becomes type r with probability νkl (r), where νkl is a probability distribution on
S, and similarly for the type-l agent. Under appropriate independence conditions, one would
like to have an almost-surely deterministic cross-sectional type distribution at each time period.
        We shall now define formally a dynamical system D with random mutation, partial
matching and type changing. As in Section 3, let (I, I, λ) be an atomless probability space
representing the space of agents, (Ω, F, P ) a sample probability space, and (I × Ω, I  F, λ  P )
a Fubini extension of the usual product probability space.
        Let α0 : I → S = {1, . . . , K} be an initial I-measurable type function with distribution
p0 on S. For each time period n ≥ 1, we first have a random mutation that is modeled by a
process hn from (I × Ω, I  F, λ  P ) to S, then a random partial matching that is described by
a function π n from (I × Ω, I  F, λ  P ) to I ∪ {J} (where J represents no matching), followed
by a random assignment of types for the matched agents, given by αn from (I ×Ω, I F, λP )
to S.
        For the random mutation step at time n, given a K × K probability transition matrix25
b, we require that, for each agent i ∈ I,

                                       P hni = l | αin−1 = k = bkl ,
                                                            
                                                                                                             (8)

the specified probability with which an agent i of type k at the end of time period n−1 mutates
to type l.
        For the random partial matching step at time n, we let p̃n be the expected cross-sectional
type distribution immediately after random mutation. That is,
                                      Z
                         n     n
                       p̃k = p̃ (k) =   λ({i ∈ I : hnω (i) = k}) dP (ω).                                     (9)
                                               Ω

The random partial matching function π n at time n is defined by:

   1. For any ω ∈ Ω, πωn ( · ) is a full matching on I − (πωn )−1 ({J}), as defined in Section 3.3.

   2. Extending hn so that hn (J, ω) = J for any ω ∈ Ω, we define g n : I × Ω → S ∪ {J} by

                                            g n (i, ω) = hn (π n (i, ω), ω),

        and assume that g n is I  F-measurable.
  25
     Here, bkl is in [0, 1], with K
                                 P
                                   l=1 bkl = 1 for each k. We do not require that the mutation probability bkl is
strictly positive. Thus, as noted in Footnote 12, the degenerate case of no random mutation is allowed.




                                                       13
   3. Let q ∈ [0, 1]S . For each agent i ∈ I,

                                     P (gin = J | hni = k) = qk ,
                                                              (1 − qk )(1 − ql )p̃nl
                                      P (gin = l | hni = k) = PK                     .                               (10)
                                                                                 n
                                                                  r=1 (1 − qr )p̃r

Equation (10) means that, for any agent whose type before the matching is k, the probability of
being unmatched is qk , and the probability of being matched to a type-l agent is proportional
to the expected cross-sectional type distribution for matched agents. When g n is essentially
pairwise independent (as under the Markov conditional independence condition used in Section
4.3 below), the exact law of large numbers in [50] (see Lemma 1 below) implies that the
realized cross-sectional type distribution λ(hnω )−1 after random mutation at time n is indeed
the expected distribution p̃n , P -almost surely.26
         Finally, for the step of random type changing for matched agents at time n, a given
ν : S × S → ∆ specifies the probability distribution νkl = ν(k, l) of the new type of a type-k
agent who has met a type-l agent. When agent i is not matched at time n, she keeps her type
hni with probability one. We thus require that the type function αn after matching satisfies,
for each agent i ∈ I,

                                    P (αin = r | hni = k, gin = J) = δkr ,
                                     P (αin = r | hni = k, gin = l) = νkl (r),                                       (11)

where δkr is one if r = k, and zero otherwise.
         Thus, we have inductively defined a dynamical system D with random mutation, partial
matching, and match-induced type changing with parameters (p0 , b, q, ν).

4.2     Markov conditional independence in types

In this section, we consider a suitable independence condition on the dynamical system D. In
order to formalize the intuitive idea that, given their type function αn−1 , the agents randomly
mutate to other types independently at time n, in such a way that their types in earlier periods
have no effect on this mutation, we say that the random mutation is Markov conditionally
independent in types if, for λ-almost all i ∈ I and λ-almost all j ∈ I,

      P (hni = k, hnj = l | αi0 , . . . , αin−1 ; αj0 , . . . , αjn−1 ) = P (hni = k | αin−1 )P (hnj = l | αjn−1 )   (12)

holds for all types k, l ∈ S.27
  26
     As noted in Footnote 23, if the denominator in equation (10) is zero, then almost no agents will be matched
and we can simply interpret the ratio as zero.
  27
     We could include the functions hm and g m for 1 ≤ m ≤ n − 1 as well. However, it is not necessary to do
so since we only care about the dependence structure across time for the type functions at the end of each time
period.


                                                             14
         Intuitively, the random partial matching at time n should depend only on agents’ types
immediately after the latest random mutation step. One may also want the random partial
matching to be independent across agents, given events that occurred in the first n − 1 time
periods and the random mutation at time n. We say that the random partial matching π n is
Markov conditionally independent in types if, for λ-almost all i ∈ I and λ-almost all
j ∈ I,

   P (gin = c, gjn = d | αi0 , . . . , αin−1 , hni ; αj0 , . . . , αjn−1 , hnj ) = P (gin = c | hni )P (gjn = d | hnj )   (13)

holds for all types c, d ∈ S ∪ {J}.
         The agents’ types at the end of time period n should depend on the agents’ types im-
mediately after the random mutation stage at time n, as well as the results of random partial
matching at time n, but not otherwise on events that occurred in previous periods. This mo-
tivates the following definition. The random type changing after partial matching at time n
is said to be Markov conditionally independent in types if for λ-almost all i ∈ I and
λ-almost all j ∈ I, and for each n ≥ 1,

                          P (αin = k, αjn = l | αi0 , . . . , αin−1 , hni , gin ; αj0 , . . . , αjn−1 , hnj , gjn )
                     = P (αin = k | hni , gin )P (αjn = l | hnj , gjn )                                                   (14)

holds for all types k, l ∈ S.
         The dynamical system D is said to be Markov conditionally independent in types
if, in each time period n, each random step (random mutation, partial matching, and type
changing) is so.28

4.3      Exact law of large numbers and stationarity

With the goal of a stationarity result for the cross-sectional type distribution, we now define a
mapping Γ from ∆ to ∆ such that, for each p = (p1 , . . . , pK ) ∈ ∆, the r-th component of Γ is

                                                 νkl (r)(1 − qk )(1 − ql ) K
                              K              K                            P                 PK
                                                                               m=1 pm bmk    j=1 pj bjl
                              X             X
   Γr (p1 , . . . , pK ) = qr     pm bmr +                   PK                PK                       .                 (15)
                              m=1          k,l=1                 t=1 (1 − qt )   j=1 pj bjt

We note that the second term of this expression for Γr (p1 , . . . , pK ) can be written as
                           K                 K              PK                      PK
                                                               l=1 νkl (r)(1 − ql )   j=1 pj bjl
                           X                 X
                                 (1 − qk )         pm bmk       PK              PK               ,
                           k=1               m=1                  l=1 (1 − ql )    j=1 pj bjl
  28
     For the conditions of Markov conditional independence in equations (12), (13) and (14), one could state the
Markov type property and the independence condition separately. However, this would double the number of
equations.



                                                                15
                                      PK              PK
which is less than or equal to          l=1 (1 − ql )  j=1 pj bjl . This means that one can define
Γr (p1 , . . . , pK ) to be qr m=1 pm bmr when l=1 (1 − ql ) K
                              PK                PK              P
                                                                   j=1 pj bjl = 0, in order to have con-
tinuity of Γ on all of ∆.
        We let pn (ω)k = λ({i ∈ I : αωn (i) = k}) be the fraction of the population of type k at the
end of time period n in state of nature ω, and let p nk be it’s expectation. That is,
                               Z                   Z
                          n
                         pk =     p (ω)k dP (ω) = P (αin = k) dλ(i),
                                   n
                                                                                                             (16)
                                      Ω                         I

where the last equality follows from the Fubini property.
        The following theorem provides an exact law of large numbers and shows stationarity
for a dynamical system D with random mutation, partial matching, and type changing that is
Markov conditionally independent in types. Its proof is given in Section 7.3 of Appendix 1.

Theorem 3 Let D be a dynamical system with random mutation, partial matching and type
changing whose parameters are (p0 , b, q, ν).29 If D is Markov conditionally independent in types,
then:

   1. For each time n ≥ 1, the expected cross-sectional type distribution is given by p n =
      Γ(p n−1 ) = Γn (p0 ), and p̃nk = K         n−1
                                                     , where Γn is the composition of Γ with itself
                                      P
                                       l=1 blk p l
        n times, and where p̃n is the expected cross-sectional type distribution after the random
        mutation (see equation (9)).

   2. For λ-almost all i ∈ I, {αin }∞                                             n
                                    n=0 is a Markov chain with transition matrix z at time n − 1
        defined by
                                                  K
                                  n
                                                  X                (1 − qr )(1 − qj )p̃nj
                                 zkl = ql bkl +         νrj (l)bkr PK                      .                 (17)
                                                                                        n
                                                  r,j=1               r0 =1 (1 − qr0 )p̃r0

   3. For λ-almost all i ∈ I and λ-almost all j ∈ I, the Markov chains {αin }∞         n ∞
                                                                             n=0 and {αj }n=0
        are independent (which means that the random vectors (αi0 , . . . , αin ) and (αj0 , . . . , αjn ) are
        independent for all n ≥ 0).

   4. For P -almost all ω ∈ Ω, the cross-sectional type process {αωn }∞
                                                                      n=0 is a Markov chain with
        transition matrix z n at time n − 1.

   5. For P -almost all ω ∈ Ω, at each time period n ≥ 1, the realized cross-sectional type
        distribution after the random mutation λ(hnω )−1 is its expectation p̃n , and the realized
  29
     It is straightforward to restate the results in this theorem to the case of a dynamical system of random full
matching with a complete separable metric type space S and deterministic match induced type changes, and
without random mutation; see Remark 2. As noted in Footnote 11, the study of independent random partial
matching with random mutation and match induced random type changes in such a general type space is beyond
the scope of this paper.


                                                         16
       cross-sectional type distribution at the end of period n, pn (ω) = λ(αωn )−1 , is equal to its
       expectation p n , and thus, P -almost surely, pn = Γn (p0 ).

    6. There is a stationary distribution p∗ . That is, with initial cross-sectional type distribution
       p0 = p∗ , for every n ≥ 1, the realized cross-sectional type distribution pn at time n is
       p∗ P -almost surely, and z n = z 1 . In particular, all of the relevant Markov chains are
       time-homogeneous with a constant transition matrix having p∗ as a fixed point.30

5    Existence of random matching models that are independent in types

We now show the existence of a joint agent-probability space, and randomized mutation, partial
matching and match-induced type-changing functions that satisfy Markov conditional indepen-
dence, where the agent space (I, I, λ) is an extension of the classical Lebesgue unit interval
(L, L, η) in the sense that I = L = [0, 1], the σ-algebra I contains the Lebesgue σ-algebra L,
and the restriction of λ to L is the Lebesgue measure η. We also obtain as corollaries existence
results for random partial and full matchings in the static case.
       First, we present the existence result for the dynamic case. Its proof is given in Appendix
2 (Section 8).

Theorem 4 Fixing any parameters p0 for the initial cross-sectional type distribution, b for
mutation probabilities, q ∈ [0, 1]S for no-match probabilities, and ν for match-induced type-
change probabilities, there exists a Fubini extension (I × Ω, I  F, λ  P ) such that

    1. The agent space (I, I, λ) is an extension of the Lebesgue unit interval (L, L, η).

    2. There is defined on the Fubini extension a dynamical system D with random mutation,
       partial matching and type changing that is Markov conditionally independent in types with
       the parameters (p0 , b, q, ν).31

       Next, by restricting the dynamic model of Theorem 4 to the first period without random
mutation, we obtain the following existence result for independent random partial matching.

Corollary 1 For any type distribution p = (p1 , . . . , pK ) on S, and any q = (q1 , . . . , qK ) as
no-match probabilities, there exists a Fubini extension (I × Ω, I  F, λ  P ) such that
   30
      Our initial type function α0 is assumed to be non-random. It is easy to generalize to the case in which α0
is a function from I × Ω to S such that for λ-almost all i ∈ I, j ∈ I, αi0 and αj0 are independent. Let p 0 be the
expected cross-sectional type distribution. Then, all the results in Theorem 3 remain valid. In the case that for
λ-almost all i ∈ I, αi0 has distribution p 0 , the evolution of the fractions of each type is essentially deterministic
and coincides exactly with the evolution of the probability distribution of type for almost every given agent (in
comparison with Footnote 15).
   31
      The existence of a dynamical system of random full matching with a complete separable metric type space
S and deterministic match induced type changes is noted in Remark 4 below.


                                                          17
    1. The agent space (I, I, λ) is an extension of the Lebesgue unit interval (L, L, η).

    2. There is defined on the Fubini extension an independent-in-types random partial matching
       π from (I × Ω, I  F, λ  P ) to I with type distribution p and with q as the no-match
       probabilities.

       By taking the no-match probabilities to be zero, we can obtain an existence result for
independent random matching where almost all agents are matched. We cannot, however,
claim the existence for an independent random full matching, for which all agents are matched.
On the other hand, the general constructions in the proof of Theorem 4 can also be used to
prove the following corollary.

Corollary 2 There exists a Fubini extension (I × Ω, I  F, λ  P ) such that

    1. The agent space (I, I, λ) is an extension of the Lebesgue unit interval (L, L, η).

    2. For any type distribution p = (p1 , . . . , pK ) on S, there exists an independent-in-types
       random full matching π from (I × Ω, I  F, λ  P ) to I with type distribution p.

       The proofs for Corollaries 1 and 2 are given in Appendix 2 (Section 8). The dynamic
and static matching models described in Theorem 4 and its Corollaries 1 and 2 satisfy the
respective conditions of Theorems 3, 2, and 1. Thus, the respective conclusions in Theorems
1, 2, and 3 also hold for these matching models.

6    Discussion

As noted in the introduction, this is the first theoretical treatment of the exact law of large
numbers for independent random matching among a continuum population (modeled by an
atomless, countably additive probability measure space). All three basic issues concerning in-
dependent random matching for a continuum population, namely, mathematical formulation
of the analytic framework, proof of general results on the exact law of large numbers for inde-
pendent random matching, and existence of independent random matching with an extension
of the Lebesgue unit interval as the agent space, are addressed for both static and dynamic
systems. Our results on dynamical systems with random mutation, random partial matching,
and random type changing provide an understanding of the time evolution of the cross-sectional
type process, identifying it as a Markov chain with known transition matrices.
       Based on the classical asymptotic law of large numbers, Boylan constructed an example
of random full matching for a countable population in [7, Proposition 2] with the properties
that an individual’s probability of matching a type-k agent is the fraction pk of type-k agents in

                                                18
the total population, and that the asymptotic fraction of type-k agents matching type-l agents
in a realized matching approximates pk pl almost surely.32 A repeated matching scheme is then
considered in [7] for the dynamic setting.
       A special example of random full matching is constructed in [3, Theorem 4.2] for a
given type function on the population space [0, 1] by rearranging intervals in [0, 1] through
measure-preserving mappings. For repeated matching schemes with an infinite number of time
periods, it is recognized in [3, p. 262] that one may run into problems when the matching
in the next period follows from the type function in a previous period.33 It is then proposed
to arbitrarily rearrange agents with the same types into half-open intervals. Aside from the
question of a natural interpretation of this rearrangement of agents’ names using intervals,
the random full matching considered in [3] does not satisfy the intuitive idea that agents are
matched independently in types. That is, this example is not a model for independent random
matching. It is made clear in [3, p. 266] that “This paper should not be viewed as a justification
for the informal use of a law of large numbers in random matching with a continuum of agents.”
       Gilboa and Matsui [26] constructed a particular example for a matching model of two
countable populations with a countable number of encounters in the time interval [0, 1), where
the space N of agents is endowed with a purely finitely additive measure µ extending the usual
density. They showed that their matching model satisfies a few desired matching properties in
their setting, including the fact that an agent is matched exactly once with probability one.
Their matching model for a countable population with a countable number of encounters within
a continuous time framework is quite different from our static or dynamic matching models for
a continuum population. As they also point out, a disadvantage of their approach is that the
underlying state of the world is “drawn” according to a purely finitely-additive measure.
       In comparison with the particular examples of a non-independent random matching with
some matching properties in [7], [3], and [26],34 we prove the exact law of large numbers for
general independent random matchings,35 which can be applied to different matching schemes.
The papers [1] and [2] also formalize a link between matching and informational constraints,
  32
     It is not clear whether this example satisfies the kind of condition, independence in types, considered by us.
  33
     In the dynamic random matching model defined in the proof of Theorem 3.1 in [19] (and of Theorem 4 here),
every step of randomization uses the realized type function generated in the step of randomization immediately
before.
  34
     As noted in Subsection 3.2 of [44], there are many non-independent random matchings with some matching
properties even for finitely many agents.
  35
     Independence is in general viewed as a behavioral assumption. When agents make their random choices
without explicit coordinations among themselves, it is reasonable to assume independence. It is important to
distinguish an ad hoc example with some particular correlation structure from a general result in the setting of
                                                                                                     ∞
law of large numbers. For example, one can take a sequence of bounded random variables        P {φn }n=1 with mean
zero. When all the odd terms in the sequence equal φ1 and even terms equal −φ1 , then ( n       k=1 k )/n converges
                                                                                                    φ
to zero almost surely. Such kind of result will not be useful at all in situations that require the use of the law of
large numbers.



                                                         19
which do not consider random matching under the independence assumption as in the mod-
els considered here. Random mutation, random partial matching and random type changing
induced by matching are not considered in any of those earlier papers.
          In addition, our results in Section 5 also provide the first existence results for independent
random matching where an extension of the Lebesgue unit interval is used as the agent space.
This goes beyond the existence results for independent random matching with a hyperfinite
number of agents as studied in [19].

7        Appendix 1 – Proof of Theorems 1, 2 and 3

7.1       Exact law of large numbers for a continuum of independent random variables

The following general version of the exact law of large numbers is shown by Sun in [50], and is
stated as a lemma here for the convenience of the reader.36

Lemma 1 Let f be a measurable process from a Fubini extension (I × Ω, I  F, λ  P ) of
the usual product probability space to a complete separable metric space X. Assume that the
random variables fi are essentially pairwise independent in the sense that for λ-almost all i ∈ I,
the random variables fi and fj are independent for λ-almost all j ∈ I.

     1. For P -almost all ω ∈ Ω, the sample distribution λfω−1 of the sample function fω is the
          same as the distribution (λ  P )f −1 of the process.37

     2. For any A ∈ I with λ(A) > 0, let f A be the restriction of f to A × Ω, λA and λA  P
          the probability measures rescaled from the restrictions λ and λ  P to {D ∈ I : D ⊆ A}
          and {C ∈ I  F : C ⊆ A × Ω} respectively. Then for P -almost all ω ∈ Ω, the sample
          distribution λA (f A )−1                         A
                                ω of the sample function (f )ω is the same as the distribution of
          (λA  P )(f A )−1 of the process f A .

     3. If there is a distribution µ on X such that for λ-almost all i ∈ I, the random variable
          fi has distribution µ, then the sample function fω (or (f A )ω ) also has distribution µ for
          P -almost all ω ∈ Ω.
    36
     Part (2) of the lemma is part of Theorem 2.8 in [50]. That theorem actually shows that the statement in
Part (2) here is equivalent to the condition of essential pairwise independence. While Parts (1) and (3) of the
lemma are special cases of Part (2), they are stated respectively in Corollary 2.9 and Theorem 2.12 of [50]. In
addition, it is noted in [51] that under the condition of essential pairwise independence on the process f , the
statement in Part (2) here is equivalent to the existence of a Fubini extension in which f is measurable. Thus,
in a certain sense, a Fubini extension provides the only right measure-theoretic framework for working with
independent processes (and independent random matchings).
  37
     Here, (λ  P )f −1 is the distribution ν on X such that ν(B) = (λ  P )(f −1 (B)) for any Borel set B in X;
λfω−1 is defined similarly.




                                                      20
       By viewing a discrete-time stochastic process taking values in X as a random variable
taking values in X ∞ , Lemma 1 implies the following exact law of large numbers for a continuum
of discrete-time stochastic processes, which is formally stated in Theorem 2.16 in [50].

Corollary 3 Let f be a mapping from I × Ω × N to a complete separable metric space X such
that for each n ≥ 0, f n = f (·, ·, n) is an I F-measurable process. Then, for λ-almost all i ∈ I,
{fin }∞                                                                                 n ∞
      n=0 is a discrete-time stochastic process. Assume that the stochastic processes {fi }n=0 , i ∈
I are essentially pairwise independent, i.e., for λ-almost all i ∈ I, λ-almost all j ∈ I, the
random vectors (fi0 , . . . , fin ) and (fj0 , . . . , fjn ) are independent for all n ≥ 0. Then, for P -almost
all ω ∈ Ω, the empirical process fω = {fωn }∞
                                            n=0 has the same finite-dimensional distributions
as that of f = {f n }∞            0            n         0           n
                     n=0 , i.e. (fω , . . . , fω ) and (f , . . . , f ) have the same distribution for any
n ≥ 0.

7.2    Proofs of Theorems 1 and 2

Proof of Theorem 1: If pk = 0, equation (5) is automatically satisfied. Consider pk > 0. Let
Ik = {i ∈ I : α(i) = k} and g = α(π). Since the random variables gi are essentially pairwise
independent, Lemma 1 (3) implies that the sample function (g Ik )ω on Ik has distribution p on
S for P -almost all ω ∈ Ω. This means that λ({i ∈ Ik : gω (i) = l})/pk = pl for P -almost all
ω ∈ Ω. Hence equation (5) follows.

Remark 1 One can restate Definition 3 for the case in which the type space S is a complete
separable metric space. It is clear that Theorem 1 still holds in the setting that α is a I-
measurable type function from I to such a general type space S. Let p be the induced probability
distribution of α on the type space S. Then, if π is independent in types, the joint distribution
λ(α, gω )−1 is simply the product distribution p ⊗ p on S × S for P -almost all ω ∈ Ω. This is also
a direct consequence of the exact law of large numbers in Lemma 1. One can simply consider
the process G(i, ω) = (α(i), g(i, ω)), which still has essentially pairwise independent random
variables. By Lemma 1 (1), we have for P -almost all ω ∈ Ω, λ(α, gω )−1 = (λ  P )G−1 , which
is simply p ⊗ p.

Proof of Theorem 2: The proof is similar to that of Theorem 1; we adopt the same notation
and consider only pk > 0. Lemma 1 says that for P -almost all ω ∈ Ω, the sample function gωIk
on Ik has the same distribution as g Ik on Ik × Ω. Hence for P -almost all ω ∈ Ω,

                           λIk (gωIk )−1 ({J}) = λIk  P (g Ik )−1 ({J}) ,
                                                                      




                                                      21
which means that
                                                         Z Z                              Z
             λ({i ∈ I : α(i) = k, gω (i) = J}) =                    1(gi =J) dP dλ =               qk dλ = pk qk ; 38
                                                           Ik   Ω                             Ik

and also for any 1 ≤ l ≤ K,
                                                                       Z Z
              λ(Ik ∩ gω−1 ({l})) = (λ  P )((Ik × Ω) ∩ g −1 ({l})) =           1(gi =l) dP dλ
                                                                        Ik Ω
                                       (1 − qk )pl (1 − ql )     pk (1 − qk )pl (1 − ql )
                                   Z
                                 =      PK                   dλ = PK                      .
                                    Ik    r=1 pr (1 − qr )            r=1 pr (1 − qr )

Thus, equations (6) and (7) follow.

7.3      Proof of Theorem 3

Before proving Theorem 3, we need to prove a few lemmas. The first lemma shows how to
compute the expected cross-sectional type distributions p n and p̃n .

Lemma 2 (1) For each n ≥ 1, p n = Γ(p n−1 ), and hence p n = Γn (p0 ), where Γn is the
composition of Γ with itself n times.
        (2) For each n ≥ 1, the expected cross-sectional type distribution p̃n immediately af-
                                                                               PK          n−1
ter random mutation at time n, as defined in equation (9), satisfies p̃nk =      l=1 blk p l   =
PK          n−1 0
   l=1 blk Γl  (p ).

Proof. Equations (8) and (9) and the Fubini property imply that
                                 Z                              K
                                                              Z X
                     p̃nk   =         P (hni   = k) dλ(i) =             P (hni = k, αin−1 = l) dλ(i)
                                  I                             I l=1
                                   K
                                 Z X
                            =               P (hni = k | αin−1 = l)P (αin−1 = l) dλ(i)
                                 I l=1
                                 K Z
                                 X                                        K
                                                                          X
                            =               blk P (αin−1 = l) dλ(i) =           blk p n−1
                                                                                      l   .                             (18)
                                 l=1    I                                 l=1

Then, we can express p n in terms of p̃n by equations (10) and (11) as
              Z
       n
      pr =       P (αin = r) dλ(i)
                      I
                      K h
                    Z X                                                 K
                                                                        X                                  i
                =               P (αin = r, hni = k, gin = J) +               P (αin = r, hni = k, gin = l) dλ(i)
                      I k=1                                             l=1
                      K h
                    Z X
                =               P (αin = r | hni = k, gin = J)P (gin = J | hni = k)P (hni = k)
                      I k=1

 38
      For a set C in a space, 1C denotes its indicator function.


                                                              22
                      K
                      X                                                                   i
                  +         P (αin = r | hni = k, gin = l)P (gin = l | hni = k)P (hni = k) dλ(i)
                      l=1
                               K
                              X    νkl (r)p̃nk (1 − qk )p̃nl (1 − ql )
             = p̃nr qr +                 PK n                          .                                 (19)
                             k,l=1            t=1 p̃t (1 − qt )

By combining equations (18) and (19), it is easy to see that p n = Γ(p n−1 ), and hence that
p n = Γn (p0 ), where Γn is the composition of Γ with itself n times. Hence, part (1) of the
lemma is shown. Part (2) of the lemma follows from part (1) and equation (18).

       The following lemma shows the Markov property of the agents’ type processes.

Lemma 3 Suppose the dynamical system D is Markov conditionally independent in types.
Then, for λ-almost all i ∈ I, the type process for agent i, {αin }∞
                                                                  n=0 , is a Markov chain with
transition matrix z n at time n − 1, where zkl
                                            n is defined in equation (17).


Proof. Fix n ≥ 1. Equation (12) implies that for λ-almost all i ∈ I, λ-almost all j ∈ I,

                 P (hni = kn , hnj ∈ S | αi0 = k0 , . . . , αin−1 = kn−1 ; αj0 ∈ S, . . . , αjn−1 ∈ S)
             = P (hni = kn | αin−1 = kn−1 )P (hnj ∈ S | αjn−1 ),                                         (20)

holds for any (k0 , . . . , kn ) ∈ S n+1 . Thus, for λ-almost all i ∈ I,

                               P (hni = k | αi0 , . . . , αin−1 ) = P (hni = k | αin−1 )                 (21)

holds for any k ∈ S. By grouping countably many null sets together, we know that for λ-almost
all i ∈ I, equation (21) holds for all k ∈ S and n ≥ 1.
       Similarly, equations (13) and (14) imply that for λ-almost all i ∈ I,

                             P (gin = c | αi0 , . . . , αin−1 , hni ) = P (gin = c | hni )
                      P (αin = k | αi0 , . . . , αin−1 , hni , gin ) = P (αin = k | hni , gin )          (22)

hold for all k ∈ S, c ∈ S ∪ {J} and n ≥ 1. A simple computation shows that for λ-almost
all i ∈ I, P (αin = k | αi0 , . . . , αin−1 ) = P (αin = k | αin−1 ) for all k ∈ S and n ≥ 1. Hence, for
λ-almost all i ∈ I, agent i’s type process {αin }∞
                                                 n=0 is a Markov chain; it is also easy to see that
the transition matrix z n from time n − 1 to time n is
     n
    zkl = P (αin = l | αin−1 = k)
              K
              X       X
         =                    P (αin = l | hni = r, gin = c)P (gin = c | hni = r)P (hni = r | αin−1 = k).(23)
              r=1 c∈S∪{J}
                                                               n in equation (17) holds.
Then, equations (8), (10) and (11) imply that the formula for zkl

       Now, for each n ≥ 1, we view each αn as a random variable on I × Ω. Then {αn }∞
                                                                                     n=0 is
a discrete-time stochastic process.

                                                            23
Lemma 4 Assume that the dynamical system D is Markov conditionally independent in types.
Then, {αn }∞                                                  n
           n=0 is also a Markov chain with transition matrix z at time n−1 given by equation
(17).

Proof. We can compute the transition matrix of {αn }∞
                                                    n=0 at time n − 1 as follows. For any
k, l ∈ S, we have
                                                   Z
                          n
             (λ  P )(α = l, α    n−1
                                        = k) =           P (αin = l | αin−1 = k)P (αin−1 = k) dλ(i)
                                                   ZI
                                               =          n
                                                         zkl P (αin−1 = k) dλ(i)
                                                     I
                                                  n
                                               = zkl · (λ  P )(αn−1 = k),                                   (24)

which implies that (λ  P )(αn = l | αn−1 = k) = zkl
                                                  n.

         Next, for any n ≥ 1, and for any (a0 , . . . , an−2 ) ∈ S n−1 , we have

              (λ  P )((α0 , . . . , αn−2 ) = (a0 , . . . , an−2 ), αn−1 = k, αn = l)
              Z
            =    P ((αi0 , . . . , αin−2 ) = (a0 , . . . , an−2 ), αin−1 = k, αin = l) dλ(i)
              ZI
            =    P (αin = l | αin−1 = k)P ((αi0 , . . . , αin−2 ) = (a0 , . . . , an−2 ), αin−1 = k) dλ(i)
                  I
               n
            = zkl · (λ  P )((α0 , . . . , αn−2 ) = (a0 , . . . , an−2 ), αn−1 = k),                         (25)

which implies that (λ  P )(αn = l | (α0 , . . . , αn−2 ) = (a0 , . . . , an−2 ), αn−1 = k) = zkl
                                                                                               n . Hence

the discrete-time process {αn }∞                                                    n
                               n=0 is indeed a Markov chain with transition matrix z at time
n − 1.

         To prove that the agents’ type processes are essentially pairwise independent in Lemma
6 below, we need the following elementary lemma.

Lemma 5 Let φm be a random variable from (Ω, F, P ) to a finite space Am , for m = 1, 2, 3, 4.
If the random variables φ3 and φ4 are independent, and if, for all a1 ∈ A1 and a2 ∈ A2 ,

                      P (φ1 = a1 , φ2 = a2 | φ3 , φ4 ) = P (φ1 = a1 | φ3 )P (φ2 = a2 | φ4 ),                 (26)

then the two pairs of random variables (φ1 , φ3 ) and (φ2 , φ4 ) are independent.

Proof. For any am ∈ Am , m = 1, 2, 3, 4, we have

                        P (φ1 = a1 , φ2 = a2 , φ3 = a3 , φ4 = a4 )
                  = P (φ1 = a1 , φ2 = a2 | φ3 = a3 , φ4 = a4 )P (φ3 = a3 , φ4 = a4 )
                  = P (φ1 = a1 | φ3 = a3 )P (φ2 = a2 | φ4 = a4 )P (φ3 = a3 )P (φ4 = a4 )
                  = P (φ1 = a1 , φ3 = a3 )P (φ2 = a2 , φ4 = a4 ).                                            (27)

                                                          24
Hence, the pairs (φ1 , φ3 ) and (φ2 , φ4 ) are independent.

         The following lemma is useful for applying the exact law of large numbers in Corollary
3 to Markov chains.

Lemma 6 Assume that the dynamical system D is Markov conditionally independent in types.
Then, the Markov chains {αin }∞
                              n=0 , i ∈ I, are essentially pairwise independent. In addition, the
processes hn and g n are also essentially pairwise independent for each n ≥ 1.

Proof. Let E be the set of all (i, j) ∈ I × I such that equations (12), (13) and (14) hold for
all n ≥ 1. Then, by grouping countably many null sets together, we obtain that for λ-almost
all i ∈ I, λ-almost all j ∈ I, (i, j) ∈ E, i.e., for λ-almost all i ∈ I, λ(Ei ) = λ({j ∈ I : (i, j) ∈
E}) = 1.
         We can use induction to prove that for any fixed (i, j) ∈ E, (αi0 , . . . , αin ) and (αj0 , . . . , αjn )
are independent, so are the pairs hni and hnj , and gin and gjn . This is obvious for n = 0. Suppose
that it is true for the case n − 1, i.e., (αi0 , . . . , αin−1 ) and (αj0 , . . . , αjn−1 ) are independent, so
are the pairs hn−1
               i   and hn−1
                        j   , and gin−1 and gjn−1 . Then, the Markov conditional independence
condition and Lemma 5 imply that (αi0 , . . . , αin−1 , hni ) and (αj0 , . . . , αjn−1 , hnj ) are independent,
so are the pairs (αi0 , . . . , αin−1 , hni , gin ) and (αj0 , . . . , αjn−1 , hnj , gjn ), and (αi0 , . . . , αin−1 , hni , gin , αin )
and (αj0 , . . . , αjn−1 , hnj , gjn , αjn ). Hence, the random vectors (αi0 , . . . , αin ) and (αj0 , . . . , αjn ) are
independent for all n ≥ 0, which means that the Markov chains {αin }∞         n ∞
                                                                    n=0 and {αj }n=0 are in-
dependent. It is also clear that for each n ≥ 1, the random variables hni and hnj are independent,
so are gin and gjn . The desired result follows.

Proof of Theorem 3: Properties (1), (2), and (3) of the theorem are shown in Lemmas 2, 3,
and 6 respectively.
         By the exact law of large numbers in Corollary 3, we know that for P -almost all ω ∈ Ω,
(αω0 , . . . , αωn ) and (α0 , . . . , αn ) (viewed as random vectors) have the same distribution for all
n ≥ 1. Since, as noted in Lemma 4, {αn }∞                                             n
                                        n=0 is a Markov chain with transition matrix z at
time n − 1, so is {αωn }∞
                        n=0 for P -almost all ω ∈ Ω. Thus (4) is shown.
         Since the processes hn and g n are essentially pairwise independent as shown in Lemma
6, the exact law of large numbers in Lemma 1 implies that at time period n, for P -almost all
ω ∈ Ω, the realized cross-sectional distribution after the random mutation, pn (ω) = λ(hnω )−1 is
the expected cross-sectional distribution p̃n , and the realized cross-sectional distribution at the
end of period n, pn (ω) = λ(αωn )−1 is the expected cross-sectional distribution p n . Thus, (5) is
shown.
         To prove (6), note that Γ is a continuous function from ∆ to itself. Hence, Brower’s
Fixed Point Theorem implies that Γ has a fixed point p∗ . In this case, p n = Γn (p∗ ) = p∗ ,

                                                                  25
 n = z 1 for all n ≥ 1. Hence the Markov chains {αn }∞                            n ∞
zkl   kl                                          i n=0 for λ-almost all i ∈ I, {α }n=0 ,
{αωn }∞
      n=0 for P -almost all ω ∈ Ω are time-homogeneous.


Remark 2 It is simple to define inductively a dynamical system of random full matching with
a complete separable metric type space S and deterministic match-induced type changes, and
without random mutation. Let α0 be an initial measurable type function from I to S with
distribution p0 on S, and ν a deterministic assignment of types for the matched agents that is
given by a Borel measurable function from S × S to S. For each time n ≥ 1, we first have a
random full matching that is described by a function π n from (I × Ω, I  F, λ  P ) to I, and
then a new type function αn from (I × Ω, I  F, λ  P ) to S.
        Let p̄n−1 be the expected cross-sectional type distribution on S at time n−1, i.e., p̄n−1 (·) =
      n−1 −1                   The random full matching function π n at time n is defined by:
R
 Ω λ(αω ) (·) dP (ω).

    1. For any ω ∈ Ω, πωn ( · ) is a full matching on I.

    2. Define g n : I × Ω → S by g n (i, ω) = αn−1 (π n (i, ω), ω), and assume that g n is I  F-
       measurable.

    3. For λ-almost all agent i ∈ I, the conditional distribution

                                               P (gin )−1 | αin−1 = p̄n−1 .
                                                                 
                                                                                                                   (28)

Item 3 means that given agent i’s type at time n − 1, the conditional probability of agent i being
matched in time n to an agent with type in a measurable subset B of S is simply the expected
proportion of agents with types in B at time n − 1. Thus, for λ-almost all agent i ∈ I, the
random variables αin−1 and gin are independent with distributions P (αin−1 )−1 and p̄n−1 . We
also have αn = ν(αn−1 , g n ).
        We assume that the random full matching π n is Markov conditionally independent
in types in the sense that for λ-almost all i ∈ I and λ-almost all j ∈ I,

    P (gin ∈ C, gjn ∈ D | αi0 , . . . , αin−1 ; αj0 , . . . , αjn−1 ) = P (gin ∈ C | αin−1 )P (gjn ∈ D | αjn−1 )   (29)

holds for measurable subsets C and D of S.
        Define a mapping Γ from the space ∆(S) of Borel probability measures on S by letting
Γ(p) = (p ⊗ p)ν −1 for each p ∈ ∆(S).
        For the expected cross-sectional type distribution p̄n on S at time n ≥ 1, we have
                          Z                       Z
              p̄n ( · ) =    λ(αωn )−1 ( · ) dP = P (αin )−1 ( · ) dλ
                          ZΩ                        I
                                  n−1 −1
                                                     −1
                                                n−1
                                                      ν ( · ) dλ = p̄n−1 ⊗ p̄n−1 ν −1 ( · ),
                                                                                
                        =    P (αi ) ⊗ p̄                                                                          (30)
                                I


                                                           26
where the second equality is obtained by the Fubini property and the third equality follows from
the definition of αn and equation (28). Thus, we have p̄n = Γ(p̄n−1 ) = Γn (p0 ).
          For s ∈ S, let νs be the function ν(s, ·) from S to S. Define a transition probability z n
by letting zsn be the probability distribution p̄n−1 νs−1 for s ∈ S. Then, the same proof as above
allows us to claim that (1) for λ-almost all i ∈ I, {αin }∞
                                                          n=0 is a Markov chain with transition
probability z n at time n−1; (2) for λ-almost all i ∈ I and λ-almost all j ∈ I, the Markov chains
{αin }∞         n ∞
      n=0 and {αj }n=0 are independent; (3) for P -almost all ω ∈ Ω, the cross-sectional type
process {αωn }∞                                             n
              n=0 is a Markov chain with transition matrix z at time n − 1; (4) for P -almost
all ω ∈ Ω, the realized cross-sectional type distribution at each period n ≥ 1, pn (ω) = λ(αωn )−1 ,
is equal to its expectation p n , and thus, P -almost surely, pn = Γn (p0 ).
          In addition, if S is compact and ν is continuous, then Γ is a continuous mapping from
the the compact and convex space ∆(S) to itself. The classical Tychonoff Fixed Point Theorem
implies the existence of a stationary distribution p∗ with Γ(p∗ ) = p∗ . That is, with initial cross-
sectional type distribution p0 = p∗ , for every n ≥ 1, the realized cross-sectional type distribution
pn at time n is p∗ P -almost surely, and z n = z 1 . In particular, all of the relevant Markov chains
are time-homogeneous with transition probability z 1 having p∗ as a fixed point.

8        Appendix 2 – Proofs of results in Section 5

In this appendix, the unit interval [0, 1] will have a different notation in a different context.
Recall that (L, L, η) is the Lebesgue unit interval, where η is the Lebesgue measure defined on
the Lebesgue σ-algebra L.
          The following result is Theorem 3.1 in [19]. Note that the agent space used in the proof of
Theorem 3.1 in [19] is a hyperfinite Loeb counting probability space. Using the usual ultrapower
construction as in [40], the hyperfinite index set of agents can be viewed as an equivalence class
of a sequence of finite sets with elements in natural numbers, and thus this index set of agents
has the cardinality of the continuum.39

Proposition 1 Fixing any parameters p0 for the initial cross-sectional type distribution, b for
mutation probabilities, q ∈ [0, 1]S for no-match probabilities, and ν for match-induced type-
                                                                     ˆ Î, λ̂) of agents, where the
change probabilities, there exist (1) an atomless probability space (I,
index space Iˆ has cardinality of the continuum; (2) a sample probability space (Ω, F, P ); and
(3) a Fubini extension (Iˆ × Ω, Î  F, λ̂  P ) on which is defined a dynamical system D̂ with
random mutation, partial matching and type changing that is Markov conditionally independent
in types with these parameters (p0 , b, q, ν).
    39
    The notation for the agent space in the proof of Theorem 3.1 in [19] is (I, I, λ), which is replaced by the
          ˆ Î, λ̂) in Proposition 1 here. The notation (I, I, λ) will be used below for a different purpose.
notation (I,


                                                      27
       The purpose of Theorem 4 in this paper is to show that one can find some extension of
the Lebesgue unit interval as the agent space so that the associated version of Proposition 1
still holds.
       Fix a set Iˆ with cardinality of the continuum as in Proposition 1.40 The following lemma
is a strengthened version of Lemma 2 in [34]; see also Lemma 419I of Fremlin [21] and Lemma
3 in [52].41 The proof given below is a slight modification of the proof of Lemma 2 in [34].

Lemma 7 There is a disjoint family C = {Cî : î ∈ I}      ˆ of subsets of L = [0, 1] such that
S                                  ˆ
  î∈Iˆ Cî = L, and for each î ∈ I, Cî has the cardinality of the continuum, η∗ (Cî ) = 0 and
η ∗ (Cî ) = 1, where η∗ and η ∗ are, respectively, the inner and outer measures of the Lebesgue
measure η.

Proof. Let c be the cardinality of the continuum. As usual in set theory, c can be viewed as
the set of all ordinals below the cardinality of the continuum. Let H be the family of closed
subsets of L = [0, 1] with positive Lebesgue measure. Then, the cardinality of H is c, and hence
the cardinality of H × c is c as well. Enumerate the elements in H × c as a transfinite sequence
{(Fξ , αξ )}ξ<c , where ξ is an ordinal.
       Define a transfinite sequence {xξ }ξ<c by transfinite induction as follows. Suppose that
for an ordinal ξ < c, {xβ }β<ξ is defined. Note that the set of elements {xβ }β<ξ has cardinality
strictly less than the continuum. Since Fξ has the cardinality of the continuum, one can take
any xξ from the nonempty set Fξ \ {xβ }β<ξ . One can continue this procedure to define the
whole transfinite sequence {xξ }ξ<c . Note that the elements in the transfinite sequence {xξ }ξ<c
are all distinct.
       For each ordinal α < c, let Aα be the set of all the xξ with ξ < c and αξ = α, that is,
Aα = {xξ : ξ < c, αξ = α}. It is clear that the sets Aα , α < c are disjoint.
       Next, fix an ordinal α < c. Since {(Fξ , αξ )}ξ<c enumerates the elements in H × c,
the set {Fξ : ξ < c, αξ = α} equals H, which has cardinality of the continuum. Thus,
{ξ : ξ < c, αξ = α} has the cardinality of the continuum. Since the elements in {xξ }ξ<c are all
distinct, the set Aα = {xξ : ξ < c, αξ = α} has the cardinality of the continuum as well.
       Suppose that the inner measure η∗ (L \ Aα ) is positive. Then there is a Lebesgue mea-
surable subset E of L \ Aα with η(E) > 0, which implies the existence of F ∈ H with
F ⊆ E ⊆ L \ Aα . Let ξ < c be the unique ordinal such that Fξ = F and αξ = α. By
the definitions of xξ and Aα , we have xξ ∈ Fξ and xξ ∈ Aα , and hence xξ ∈ F ∩ Aα . However,
  40                                                                                            ˆ Î, λ̂) in this
     Note that we replace the corresponding notation (K, K, κ) used in the Appendix of [52] by (I,
paper. The reason is that the notation K has been used earlier as the number of agent types.
  41
     The original version of Lemma 2 of
                                      S [34] and Lemma 419I of Fremlin [21] requires neither that each Cî has
cardinality of the continuum nor that î∈Iˆ Cî = L.



                                                       28
F ⊆ L\Aα , which means that F ∩Aα = ∅. This is a contradiction. Hence, η∗ (L\Aα ) = 0, which
means that the outer measure η ∗ (Aα ) = 1. It is clear that 0 ≤ η∗ (Aα ) ≤ η∗ (L \ Aα+1 ) = 0.
Therefore η∗ (Aα ) = 0.
      Finally, since Iˆ has the cardinality of the continuum, there is a bijection ξˆ between Iˆ and
                 ˆ let C = A . Let B = L \ S ˆ C . Since the cardinality of B is at most
c. For each î ∈ I,       î    ξ̂(î)               î∈I î
the cardinality of the continuum, we can redistribute at most one point of B into each Cî in
                         ˆ The rest is clear.
the family C = {C : î ∈ I}.
                    î


       Kakutani [34] provided a non-separable extension of the Lebesgue unit interval by adding
subsets of the unit interval directly. As in the Appendix of [52], we follow some constructions
used in the proof of Lemma 521P(b) of [22], which allows one to work with Fubini extensions in a
more transparent way. The spirit of the Lebesgue extension itself is similar in the constructions
used in [34] and here. Define a subset C of L × Iˆ by letting C = {(l, î) ∈ L × Iˆ : l ∈ C , î ∈ I}.
                                                                                               î
                                                                                                   ˆ
         ˆ L ⊗ Î, η ⊗ λ̂) be the usual product probability space. For any L ⊗ Î-measurable set
Let (L × I,
U that contains C, C ⊆ U for each î ∈ I,   ˆ where U = {l ∈ L : (l, î) ∈ U } is the î-section of
                         î   î                          î
U . The Fubini property of η ⊗ λ̂ implies that for λ̂-almost all î ∈ I,ˆ U is L-measurable, which
                                                                               î
means that η(Uî ) = 1 (since η ∗ (Cî ) = 1). Since η ⊗ λ̂(U ) = Iˆ η(Uî ) dλ̂, we have η ⊗ λ̂(U ) = 1.
                                                                 R

Therefore, the η ⊗ λ̂-outer measure of C is one.
       Since the η ⊗ λ̂-outer measure of C is one, the method in [16] (see p. 69) can be used to
extend η ⊗ λ̂ to a measure γ on the σ-algebra U generated by the set C and the sets in L ⊗ Î
                                           n                                       o
with γ(C) = 1. It is easy to see that U = (U 1 ∩ C) ∪ (U 2 \C) : U 1 , U 2 ∈ L ⊗ Î , and that
γ[(U 1 ∩ C) ∪ (U 2 \C)] = η ⊗ λ̂(U 1 ) for any measurable sets U 1 , U 2 ∈ L ⊗ Î. Let T be the
σ-algebra {U ∩ C : U ∈ L ⊗ Î}, which is the collection of all the measurable subsets of C in
U. The restriction of γ to (C, T ) is still denoted by γ. Then, γ(U ∩ C) = η ⊗ λ̂ (U ), for every
measurable set U ∈ L ⊗ Î. Note that (L × I,   ˆ U, γ) is an extension of (L × I,
                                                                               ˆ L ⊗ Î, η ⊗ λ̂).
      Consider the projection mapping pL : L× Iˆ → L with pL (l, î) = l. Let ψ be the restriction
of pL to C. Since the family C is a partition of L = [0, 1], ψ is a bijection between C and L.
It is obvious that pL is a measure-preserving mapping from (L × I,  ˆ L ⊗ Î, η ⊗ λ̂) to (L, L, η)
in the sense that for any B ∈ L, (pL )−1 (B) ∈ L ⊗ Î and η ⊗ λ̂[(pL )−1 (B)] = η(B); and thus
pL is a measure-preserving mapping from (L × I,  ˆ U, γ) to (L, L, η). Since γ(C) = 1, ψ is a
measure-preserving mapping from (C, T , γ) to (L, L, η), that is, γ[ψ −1 (B)] = η(B) for any
B ∈ L.
      To introduce one more measure structure on the unit interval [0, 1], we shall also denote
it by I. Let I be the σ-algebra {S ⊆ I : ψ −1 (S) ∈ T }. Define a set function λ on I by
letting λ(S) = γ[ψ −1 (S)] for each S ∈ I. Since ψ is a bijection, λ is a well-defined probability
measure on (I, I). Moreover, ψ is also an isomorphism from (C, T , γ) to (I, I, λ). Since ψ

                                                   29
is a measure-preserving mapping from (C, T , γ) to (L, L, η), it is obvious that (I, I, λ) is an
extension of the Lebesgue unit interval (L, L, η).
       We shall now follow the procedure used in the proof of Proposition 2 in [52] to construct
a Fubini extension based on the probability spaces (I, I, λ) as defined above, and (Ω, F, P ) as
in our Proposition 1 here.
     First, consider the usual product space (L × Iˆ × Ω, L ⊗ (Î  F), η ⊗ (λ̂  P )) of the
Lebesgue unit interval (L, L, η) with the Fubini extension (Iˆ × Ω, Î  F, λ̂  P ). The following
lemma is shown in Step 1 of the proof of Proposition 2 in [52].

Lemma 8 The probability space (L × Iˆ × Ω, L ⊗ (Î  F), η ⊗ (λ̂  P )) is a Fubini extension of
                                     ˆ × Ω, (L ⊗ Î) ⊗ F, (η ⊗ λ̂) ⊗ P ).
the usual triple product space ((L × I)

      Next, as shown in Step 2 of the proof of Proposition 2 in [52], the set C ×Ω has η⊗(λ̂P )-
outer measure one. Based on the Fubini extension (L × Iˆ × Ω, L ⊗ (Î  F), η ⊗ (λ̂  P )), we can
construct a measure structure on C × Ω as follows. Let E = {D ∩ (C × Ω) : D ∈ L ⊗ (Î  F)}
(which is a σ-algebra on C × Ω), and τ be the set function on E defined by τ (D ∩ (C × Ω)) =
η ⊗ (λ̂  P )(D) for any measurable set D in L ⊗ (Î  F).42 Then, τ is a well-defined probability
measure on (C × Ω, E) since the η ⊗ (λ̂  P )-outer measure of C × Ω is one. The result in the
following lemma is shown in Step 2 of the proof of Proposition 2 in [52].

Lemma 9 The probability space (C × Ω, E, τ ) is a Fubini extension of the usual product prob-
ability space (C × Ω, T ⊗ F, γ ⊗ P ).

       Let Ψ be the mapping (ψ, IdΩ ) from C × Ω to I × Ω, where IdΩ is the identity map on
Ω. That is, for each (l, î) ∈ C, ω ∈ Ω, Ψ((l, î), ω) = (ψ, IdΩ )((l, î), ω) = (ψ(l, î), ω). Since ψ is a
bijection from C to I, Ψ is a bijection from C ×Ω to I ×Ω. Let W = {H ⊆ I ×Ω : Ψ−1 (H) ∈ E};
then W is a σ-algebra of subsets of I × Ω. Define a probability measure ρ on W by letting
ρ(H) = τ [Ψ−1 (H)] for any H ∈ W. Therefore, Ψ is an isomorphism from the probability space
(C × Ω, E, τ ) to the probability space (I × Ω, W, ρ). The following lemma is shown in Step 3
of the proof of Proposition 2 in [52].

Lemma 10 The probability space (I × Ω, W, ρ) is a Fubini extension of the usual product
probability space (I × Ω, I ⊗ F, λ ⊗ P ).

       Since (I × Ω, W, ρ) is a Fubini extension, we shall follow Definition 1 to denote (I ×
Ω, W, ρ) by (I × Ω, I  F, λ  P ).
  42
     We replace here the notation “ν” used in the Appendix of [52] with “τ ” here, because “ν” has been used
earlier here for match-induced type-change probabilities.


                                                    30
     Now, define a mapping ϕ from I to Iˆ by letting ϕ(i) = î if i ∈ Cî . Since the family
                ˆ is a partition of I = [0, 1], ϕ is well-defined.
C = {Cî : î ∈ I}

Lemma 11 The following properties of ϕ hold.

                                                            ˆ Î, λ̂), in the sense that for any
  1. The mapping ϕ is measure preserving from (I, I, λ) to (I,
      A ∈ Î, ϕ−1 (A) is measurable in I with λ[ϕ−1 (A)] = λ̂(A).

  2. Let Φ be the mapping (ϕ, IdΩ ) from I × Ω to Iˆ × Ω, that is, Φ(i, ω) = (ϕ, IdΩ )(i, ω) =
      (ϕ(i), ω) for any (i, ω) ∈ I × Ω. Then Φ is measure preserving from (I × Ω, I  F, λ  P )
      to (Iˆ × Ω, Î  F, λ̂  P ) in the sense that for any V ∈ Î  F, Φ−1 (V ) is measurable in
      I  F with (λ  P )[Φ−1 (V )] = (λ̂  P )(V ).

Proof. Property (1) obviously follows from (2) by considering those sets V in the form of
                                                                                             ˆ
A × Ω for A ∈ Î. Thus, we only need to prove (2). Consider the projection mapping pI×Ω :
                          ˆ                                                       ˆ
L × Iˆ × Ω → Iˆ × Ω with pI×Ω (l, î, ω) = (î, ω). Let Ψ1 be the restriction of pI×Ω to C × Ω.
      Fix any (i, ω) ∈ I × Ω. There is a unique î ∈ Iˆ such that i ∈ Cî . Thus, ϕ(i) = î, and
(i, î) ∈ C by the definition of C. We also have ψ(i, î) = i, ψ −1 (i) = (i, î), and Ψ−1 (i, ω) =
((i, î), ω). Note that Ψ−1 is a well-defined mapping from I × Ω to C × Ω since Ψ is a bijection
from C × Ω to I × Ω. Hence, we have

                          Ψ1 [Ψ−1 (i, ω)] = (î, ω) = (ϕ(i), ω) = Φ(i, ω).

Therefore Φ is the composition mapping Ψ1 [Ψ−1 ].
      Fix any V ∈ Î  F. We have Φ−1 (V ) = Ψ[Ψ−1
                                                1 (V )]. By the definition of Ψ1 , we obtain
that Ψ−1
      1 (V ) = (L×V )∩(C ×Ω), which is obviously measurable in E. For simplicity, we denote
the set Ψ−1
         1 (V ) by E . It follows from the definition of τ that τ (E) = η ⊗ (λ̂  P )(L × V ) =
(λ̂P )(V ). Since Ψ is an isomorphism from the probability space (C×Ω, E, τ ) to the probability
space (I × Ω, W, ρ), we know that Ψ(E) is measurable in W and ρ[Ψ(E)] = τ (E) = (λ̂  P )(V ).
It is clear that Ψ(E) = Φ−1 (V ). Therefore, Φ−1 (V ) is measurable in W with ρ[Φ−1 (V )] =
(λ̂P )(V ). The rest follows from the fact that (I ×Ω, W, ρ) is denoted by (I ×Ω, I F, λP ).


      For notational convenience, we let D̂ denote the dynamical system with random mutation,
partial matching and type changing that is Markov conditionally independent in types with
parameters (p0 , b, q, ν), as presented in Proposition 1 here and Theorem 3.1 in [19]. For D̂, we
add a hat to the relevant type functions, random mutation functions, and random assignments
of types for the matched agents. Let α̂0 : Iˆ → S = {1, . . . , K} be an initial Î-measurable type
function with distribution p0 on S.


                                                31
      For each time period n ≥ 1, ĥn is a random mutation function from (Iˆ × Ω, Î  F, λ̂  P )
                                   ˆ and for any types k, l ∈ S,
to S such that for each agent î ∈ I,
                                                          
                                  P ĥnî = l | α̂în−1 = k = bkl .                          (31)

The expected cross-sectional type distribution immediately after random mutation p̃n follows
from the recursive formula in part (1) of Theorem 3.
       The random partial matching at time n is described by a function π̂ n from Iˆ × Ω to
Iˆ ∪ {J} such that

   1. For any ω ∈ Ω, π̂ωn ( · ) is a full matching on Iˆ − (π̂ωn )−1 ({J}). For simplicity, the set
      Iˆ − (π̂ωn )−1 ({J}) will be denoted by Ĥωn .

   2. ĝ n is a Î  F-measurable mapping from Iˆ × Ω to S ∪ {J} with ĝ n (î, ω) = ĥn (π̂ n (î, ω), ω),
      where we assume that ĥn (J, ω) = J for any ω ∈ Ω.

   3. For each agent i ∈ I and for any types k, l ∈ S,
                                                    
                            P ĝîn = J | ĥnî = k    = qk ,
                                                       (1 − qk )(1 − ql )p̃nl
                             P ĝîn = l | ĥnî = k   = PK                     .                         (32)
                                                                             n
                                                              r=1 (1 − qr )p̃r

       A random assignment of types for the matched agents at time n is a function α̂n from
(Iˆ × Ω, Î  F, λ̂  P ) to S such that for each agent î ∈ I,
                                                             ˆ
                                                                  
                               P α̂în = r | ĥnî = k, ĝîn = J    = δkr ,
                                                                  
                                P α̂în = r | ĥnî = k, ĝîn = l   = νkl (r).                           (33)


Proof of Theorem 4: Based on the dynamical system D̂ on the Fubini extension (Iˆ × Ω, Î 
F, λ̂  P ), we shall now define, inductively, a new dynamical system D on the Fubini extension
(I × Ω, I  F, λ  P ).
       We first fix some bijections between the î-sections of the set C. For any î, î0 ∈ Iˆ with
               0                                             0                                      0
î 6= î0 , let Θî,î be a bijection from Cî to Cî0 , and Θî ,î be the inverse mapping of Θî,î . This is
possible since both Cî and Cî0 have cardinality of the continuum, as noted in Lemma 7.
       Let α0 be the mapping α̂0 (ϕ) from I to S. By the measure preserving property of ϕ in
Lemma 11, we know that α0 is I-measurable type function with distribution p0 on S.
       For each time period n ≥ 1, let hn and αn be the respective mappings ĥn (Φ) and α̂n (Φ)
from I × Ω to S. Define a mapping π n from I × Ω to I ∪ {J} such that for each (i, ω) ∈ I × Ω,
                                                           if π̂ωn (ϕ(i)) = J,
                                 
                        n                    J
                      π (i, ω) =     ϕ(i), π̂ n (ϕ(i))
                                   Θ          ω        (i) if π̂ωn (ϕ(i)) 6= J.

                                                      32
When π̂ωn (ϕ(i)) 6= J, π̂ωn defines a full matching on Ĥωn = Iˆ − (π̂ωn )−1 ({J}), which implies that
π̂ωn (ϕ(i)) 6= ϕ(i). Hence, π n is a well-defined mapping from I × Ω to I ∪ {J}.
         Since Φ is measure-preserving and ĥn is a measurable mapping from (Iˆ× Ω, Î  F, λ̂  P )
to S, hn is I  F-measurable. By the definitions of hn and αn , it is obvious that for each i ∈ I,

                                        hni = ĥnϕ(i) and αin = α̂ϕ(i)
                                                                  n
                                                                       ,                                       (34)

which, together with equation (31), implies that
                                                                  
                       n       n−1               n         n−1
                                       
                   P hi = l | αi    = k = P ĥϕ(i) = l | α̂ϕ(i) = k = bkl .                                    (35)

      Next, we consider the partial matching property of π n .

   1. Fix any ω ∈ Ω. Let Hωn = I − (πωn )−1 ({J}); then Hωn = ϕ−1 (Ĥωn ). Pick any i ∈
      Hωn and denote πωn (i) by j. Then, ϕ(i) ∈ Ĥωn . The definition of π n implies that j =
              n                           n
      Θϕ(i), π̂ω (ϕ(i)) (i). Since Θϕ(i), π̂ω (ϕ(i)) is a bijection between Cϕ(i) and Cπ̂ωn (ϕ(i)) , it follows
      that ϕ(j) = ϕ(πωn (i)) = π̂ωn (ϕ(i)) by the definition of ϕ. Thus, j = Θϕ(i), ϕ(j) (i). Since the
      inverse of Θϕ(i), ϕ(j) is Θϕ(j), ϕ(i) , we know that Θϕ(j), ϕ(i) (j) = i. By the full matching
      property of π̂ωn , ϕ(j) 6= ϕ(i), ϕ(j) ∈ Ĥωn and π̂ωn (ϕ(j)) = ϕ(i). Hence, we have j 6= i, and
                                                      n
                                πωn (j) = Θϕ(j), π̂ω (ϕ(j)) (j) = Θϕ(j), ϕ(i) (j) = i.

      This means that the composition of πωn with itself on Hωn is the identity mapping on
      Hωn , which also implies that πωn is a bijection on Hωn . Therefore πωn is a full matching on
      Hωn = I − (πωn )−1 ({J}).

   2. Extending hn so that hn (J, ω) = J for any ω ∈ Ω, we define g n : I × Ω → S ∪ {J} by
      g n (i, ω) = hn (π n (i, ω), ω). Denote ϕ(J) = J. As noted in the above paragraph, for any
      fixed ω ∈ Ω, ϕ(πωn (i)) = π̂ωn (ϕ(i)) for i ∈ Hωn . When i ∈
                                                                 / Hωn , we have ϕ(i) ∈
                                                                                      / Ĥωn , and
      πωn (i) = J, π̂ωn (ϕ(i)) = J. Therefore, ϕ(πωn (i)) = π̂ωn (ϕ(i)) for any i ∈ I. Then,

            g n (i, ω) = ĥn (ϕ(π n (i, ω)), ω) = ĥn (π̂ n (ϕ(i), ω), ω) = ĝ n (ϕ(i), ω) = ĝ n (Φ)(i, ω).

      Hence, the measure-preserving property of Φ implies that g n is I  F-measurable. The
      above equation also means that

                                              gin (·) = ĝϕ(i)
                                                          n
                                                               (·),   i ∈ I.                                   (36)

   3. Equations (32), (34) and (36) imply that for each agent i ∈ I,
                                                                
              P (gin = J | hni = k) = P ĝϕ(i)
                                            n
                                                = J | ĥnϕ(i) = k = qk ,
                                                                (1 − q )(1 − q )p̃n
                    n        n              n          n                   k         l l
               P (gi = l | hi = k) = P ĝϕ(i) = l | ĥϕ(i) = k = PK                       .                    (37)
                                                                                        n
                                                                         r=1 (1 − qr )p̃r


                                                          33
     Now, we consider the type-changing function αn for the matched agents. Since Φ is
measure-preserving and α̂n is a measurable mapping from (Iˆ × Ω, Î  F, λ̂  P ) to S, αn is
I  F-measurable. Equations (33), (34) and (36) imply that for each agent i ∈ I,
                                                                               
       P (αin = r | hni = k, gin = J) = P α̂ϕ(i)
                                             n                         n
                                                  = r | ĥnϕ(i) = k, ĝϕ(i) = J = δkr ,
                                                                              
        P (αin = r | hni = k, gin = l) = P α̂ϕ(i)
                                             n                         n
                                                  = r | ĥnϕ(i) = k, ĝϕ(i) = l = νkl (r).                                                         (38)

          Therefore, D is a dynamical system with random mutation, partial matching and type
changing and with the parameters (p0 , b, q, ν).
      It remains to check the Markov conditional independence for D. Since the dynamical
system D̂ is Markov conditionally independent in types, for each n ≥ 1, there is a set Iˆ0 ∈ Î
with λ̂(Iˆ0 ) = 1, and for each î ∈ Iˆ0 , there exists a set Êî ∈ Î with λ̂(Êî ) = 1, with the following
properties being satisfied for any î ∈ Iˆ0 and any ĵ ∈ Ê :                              î


    1. For all types k, l ∈ S,

           P (ĥnî = k, ĥnĵ = l | α̂î0 , . . . , α̂în−1 ; α̂ĵ0 , . . . , α̂ĵn−1 ) = P (ĥnî = k | α̂în−1 )P (ĥnĵ = l | α̂ĵn−1 ).
                                                                                                                                                   (39)

    2. For all types c, d ∈ S ∪ {J},

          P (ĝîn = c, ĝĵn = d | α̂î0 , . . . , α̂în−1 , ĥnî ; α̂ĵ0 , . . . , α̂ĵn−1 , ĥnĵ ) = P (ĝîn = c | ĥnî )P (ĝĵn = d | ĥnĵ ).
                                                                                                                                                   (40)

    3. For all types k, l ∈ S,

                                   P (α̂în = k, α̂ĵn = l | α̂î0 , . . . , α̂în−1 , ĥnî , ĝîn ; α̂ĵ0 , . . . , α̂ĵn−1 , ĥnĵ , ĝĵn )
                             = P (α̂în = k | ĥnî , ĝîn )P (α̂ĵn = l | ĥnĵ , ĝĵn ).                                                       (41)

          Let I 0 = ϕ−1 (Iˆ0 ). For any i ∈ I 0 , let Ei = ϕ−1 (Ê                               ϕ(i) ).    Since ϕ is measure-preserving,
λ(I 0 )
     = λ(Ei ) = 1. Fix any i ∈                    I 0,   and any j ∈ Ei . Denote ϕ(i) by î and ϕ(j) by ĵ. Then, it is
obvious that î ∈ Iˆ0 and ĵ ∈ Êî .
          By equations (34) and (36), we can rewrite equations (39), (40) and (41) as follows. For
all types k, l ∈ S,

                                          P (hni = k, hnj = l | αi0 , . . . , αin−1 ; αj0 , . . . , αjn−1 )
                                    = P (ĥnî = k, ĥnĵ = l | α̂î0 , . . . , α̂în−1 ; α̂ĵ0 , . . . , α̂ĵn−1 )
                                    = P (ĥnî = k | α̂în−1 )P (ĥnĵ = l | α̂ĵn−1 )
                                    = P (hni = k | αin−1 )P (hnj = l | αjn−1 ).                                                                    (42)

                                                                            34
For all types c, d ∈ S ∪ {J},

                             P (gin = c, gjn = d | αi0 , . . . , αin−1 , hni ; αj0 , . . . , αjn−1 , hnj )
                       = P (ĝîn = c, ĝĵn = d | α̂î0 , . . . , α̂în−1 , ĥnî ; α̂ĵ0 , . . . , α̂ĵn−1 , ĥnĵ )
                       = P (ĝîn = c | ĥnî )P (ĝĵn = d | ĥnĵ )
                       = P (gin = c | hni )P (gjn = d | hnj ).                                                                     (43)

For all types k, l ∈ S,

                       P (αin = k, αjn = l | αi0 , . . . , αin−1 , hni , gin ; αj0 , . . . , αjn−1 , hnj , gjn )
                 = P (α̂în = k, α̂ĵn = l | α̂î0 , . . . , α̂în−1 , ĥnî , ĝîn ; α̂ĵ0 , . . . , α̂ĵn−1 , ĥnĵ , ĝĵn )
                 = P (α̂în = k | ĥnî , ĝîn )P (α̂ĵn = l | ĥnĵ , ĝĵn )
                 = P (αin = k | hni , gin )P (αjn = l | hnj , gjn ).                                                               (44)

Therefore the dynamical system D is Markov conditionally independent in types.

Proof of Corollary 1: In the proof of Theorem 4, take the initial type distribution p0 to be
p. Assume that there is no genuine random mutation in the sense that bkl = δkl for all k, l ∈ S.
Then, it is clear that p̃1k = pk for any k ∈ S. Consider the random partial matching π 1 in
period one.
      Fix an agent i with α0 (i) = k. Then equation (35) implies that P (h1i = k) = 1. By
equation (37),
                                                      (1 − qk )(1 − ql )pl
                           P gi1 = J = qk , P gi1 = l = PK
                                    
                                                                            .                                                      (45)
                                                          r=1 (1 − qr )pr

Similarly, equation (43) implies that the process g 1 is essentially pairwise independent. By
taking the type function α to be α0 , the partial matching function π to be π 1 , and the associated
process g to be g 1 , the corollary holds.


Remark 3 For the proof of Corollary 2, we state Theorem 2.4 of [19] here using the notation
 ˆ Î, λ̂) for the agent space, instead of the notation (I, I, λ) from [19]. In particular, Theorem
(I,
2.4 of [19] shows the existence of an atomless probability space (I, ˆ Î, λ̂) of agents with Iˆ having
the cardinality of the continuum, a sample probability space (Ω, F, P ), a Fubini extension (Iˆ ×
Ω, Î  F, λ̂  P ), and a random full matching π̂ from (Iˆ× Ω, Î  F, λ̂  P ) to Iˆ such that (i) for
                                                                     ˆ P (π̂ −1 (A)) = λ̂(A) for any
each ω ∈ Ω, λ̂(π̂ω−1 (A)) = λ̂(A) for any A ∈ Î; (ii) for each î ∈ I,     î
A ∈ Î; (iii) for any A1 , A2 ∈ Î, λ̂(A1 ∩ π̂ω−1 (A2 )) = λ̂(A1 )λ̂(A2 ) holds for P -almost all ω ∈ Ω.
Since the random matching considered here does not depend on type functions, it is universal
in the sense that it can be applied to any type functions.


                                                                    35
             ˆ Î, λ̂) is taken to be the unit interval with the Borel algebra and Lebesgue mea-
       When (I,
sure, Footnote 4 of McLennan and Sonnenschein [42] shows the non-existence of a random full
matching π̂ that satisfies (i)-(iii). Theorem 2.4 of [19] resolves this issue posed by McLennan
and Sonnenschein by working with a suitable agent space; see the main theorem of [47] for
another proof of such a result.43
       To be consistent with the general terminology in the paper, the statement of Theorem 2.4
in [19] stated the independence condition in terms of independence in types. However, it is
                                                                        ˆ (π̂ , π̂ ) is a measure-
shown in the proof of Theorem 2.4 of [19, p. 399] that for î 6= ĵ in I,    î ĵ
                                        ˆ  ˆ
preserving mapping from (Ω, F, P ) to (I × I, Î  I, λ̂  λ̂), which implies that π̂ and π̂ are   î       ĵ
independent as measurable mappings.44 The idea of the proof of [19] can be used to show that
for finitely many different agents, the mappings of their random partners are independent. This
stronger independence property is shown explicitly in [47] for the particular universal random
matching considered there.
       As noted in Remark 1, the exact law of large numbers for an independent random full
matching of Theorem 1 generalizes trivially to a setting in which α̂ is a Î-measurable type func-
tion from Iˆ to a complete separable metric type space S. The existence of such an independent
random full matching follows immediately from Theorem 2.4 of [19] by working with the type
process ĝ = α̂(π̂).
       Since the finiteness of S is not used in the proof of Corollary 2 below, Corollary 2 also
holds in the setting of a complete separable metric type space S.

Proof of Corollary 2: We follow the notation in Remark 3. For any given type distribution
p on S, take a Î-measurable type function α̂ from Iˆ to S with type distribution p.
       By following the same constructions used before the proof of Theorem 4, we can obtain
(1) an atomless probability space (I, I, λ) which is an extension of the Lebesgue unit interval
(L, L, η); (2) a Fubini extension (I × Ω, I  F, λ  P ); and (3) a measure preserving mapping
                     ˆ Î, λ̂).
ϕ from (I, I, λ) to (I,
                                                                                     0
       As in the proof of Theorem 4, for any î, î0 ∈ Iˆ with î 6= î0 , let Θî,î be a bijection from Cî
                 0                                      0
to Cî0 , and Θî ,î be the inverse mapping of Θî,î .
  43
     The agent space in Theorem 2.4 of [19] is a hyperfinite probability space. It is a well-known property that
hyperfinite probability spaces capture the asymptotic properties of large but finite probability spaces; see [40].
So the use of such a probability space does provide some advantages. In contrast, the agent space as considered
in [47] is the space of all transfinite sequences of 0 or 1 whose length is the first uncountable ordinal and whose
terms are constant 1 except for countably many terms; it is not known how such a probability space can be linked
to large but finite probability spaces. Unlike [19], random partial matching and dynamic random matching are
not considered in [47]. In addition, we note that as indicated in the last paragraph of the proof of Theorem
2.4 of [19, p. 400], property (iii) above is simply a special case of the exact law of large numbers (as stated in
Lemma 1) for an i.i.d. process in a Fubini extension with a common distribution on the two-pint space {0, 1};
Proposition 3 of [47] provided another proof for such a special case.
  44
     We add the hat notation here.


                                                        36
        Define a mapping π from I × Ω to I such that for each (i, ω) ∈ I × Ω,

                                       π(i, ω) = Θϕ(i), π̂ω (ϕ(i)) (i).

        Fix any ω ∈ Ω. Pick any i ∈ I and denote πω (i) by j. The definition of π implies that
j = Θϕ(i), π̂ω (ϕ(i)) (i). Since Θϕ(i), π̂ω (ϕ(i)) is a bijection between Cϕ(i) and Cπ̂ω (ϕ(i)) , it follows
that ϕ(j) = ϕ(πω (i)) = π̂ω (ϕ(i)) by the definition of ϕ. Thus, j = Θϕ(i), ϕ(j) (i). Since the
inverse of Θϕ(i), ϕ(j) is Θϕ(j), ϕ(i) , we know that Θϕ(j), ϕ(i) (j) = i. By the full matching property
of π̂ω , ϕ(j) 6= ϕ(i) (and thus j 6= i), and π̂ω (ϕ(j)) = ϕ(i). Hence,

                            πω (j) = Θϕ(j), π̂ω (ϕ(j)) (j) = Θϕ(j), ϕ(i) (j) = i.

This means that the composition of πω with itself is the identity mapping on I, which also
implies that πω is a bijection on I. Hence, πω is a full matching on I.
        Let the type function α on I be defined as the composition α̂(ϕ). Since ϕ is measure
preserving, the distribution of α is still p. Let ĝ = α̂(π̂) and g = α(π). Then, it is easy to see
that for any (i, ω) ∈ I × Ω,

                         g(i, ω) = α̂(ϕ(π(i, ω))) = α̂(π̂ω (ϕ(i))) = ĝ(ϕ(i), ω).

Hence, the essential pairwise independence of g follows immediately from that of ĝ and the
measure-preserving property of ϕ.

Remark 4 To obtain the existence of a dynamical system D̂ with random full matching for a
complete separable metric type space S and deterministic match induced type changing function
ν that is Markov conditionally independent in types, we can follow the proof of Theorem 3.1 in
[19].
        Let M be a fixed unlimited hyperfinite natural number in ∗ N∞ , Iˆ = {1, 2, ..., M }, Î0 be
the internal power set on I, ˆ and λ̂0 be the internal counting probability measure on Î0 . Let
 ˆ Î, λ̂) be the Loeb space of the internal probability space (I,
(I,                                                             ˆ Î0 , λ̂0 ).
        For n ≥ 1, let (Ωn , Fn , Qn ) be the internal sample measurable space (Ω, F0 , P0 ) as in the
proof of Theorem 2.4 of [19]. Let π̂n be the random full matching defined by π̂n (î, ωn ) = ωn (î)
for î ∈ Iˆ and ωn ∈ Ωn . Let Pn be the corresponding Loeb measure of Qn on (Ωn , σ(Fn )).
      Follow the notation in Subsection 5.2 of [19]. We can construct generalized infinite
product spaces (Ω∞ , A∞ , P ∞ ) and (Iˆ× Ω∞ , Î  A∞ , λ̂  P ∞ ), where Ω∞ = ∞ Ωn . Let π̂ n be
                                                                              Q
                                                                                       n=1
defined by π̂ n (î, {ωm }∞                        ˆ          ∞      ∞
                          m=1 ) = ωn (î) for î ∈ I and {ωm }m=1 ∈ Ω . For simplicity, we shall also
use (Ω, F, P ) and (Iˆ× Ω, Î  F, λ̂  P ) to denote (Ω∞ , A∞ , P ∞ ) and (Iˆ× Ω∞ , Î  A∞ , λ̂  P ∞ )
respectively.


                                                     37
       Let α0 be any initial measurable type function from Iˆ to S with distribution p0 on S. The
type function α̂n from (Iˆ × Ω, Î  F, λ̂  P ) to S can be defined inductively by letting α̂n (î, ω) =
ν(α̂n−1 (î, ω), α̂n−1 (π̂ n (î, ω), ω)). Define ĝ n : Iˆ × Ω → S by ĝ n (î, ω) = α̂n−1 (π̂ n (î, ω), ω); then
we have α̂n = ν(α̂n−1 , ĝ n ). It can be checked that equations (28) and (29) are satisfied (with
the hat notation). Thus, the desired existence result for D̂ follows.
       As in the proof of Theorem 4, we can use the dynamical system D̂ on the Fubini extension
(Iˆ × Ω, Î  F, λ̂  P ) to construct a new dynamical system D on the Fubini extension (I ×
Ω, I  F, λ  P ).
       For the dynamical system D, define mappings π n from I × Ω to I, αn from I × Ω to S
such that for each (i, ω) ∈ I × Ω,
                                                n
                          π n (i, ω) = Θϕ(i), π̂ω (ϕ(i)) (i), αn (i, ω) = α̂n (ϕ(i), ω).

Define g n : I × Ω → S by g n (i, ω) = αn−1 (π n (i, ω), ω). As shown above, π n is a random
full matching, and g n = ĝ n (Φ). It is then easy to see that αn = α̂n (Φ) = ν(α̂n−1 , ĝ n )(Φ) =
ν(αn−1 , g n ). It is also easy to check that equations (28) and (29) are satisfied. This shows
the existence of a dynamical system D of random full matching with a complete separable
metric type space S and with initial type distribution p0 and deterministic match induced type
changing function ν that is Markov conditionally independent in types, where the agent space
is an extension of the Lebesgue unit interval.




                                                        38
References
  [1] C. D. Aliprantis, G. Camera, and D. Puzzello, Matching and anonymity, Economic Theory 29
      (2006), 415–432.

  [2] C. D. Aliprantis, G. Camera, and D. Puzzello, A random matching theory, Games and Economic
      Behavior 59 (2007), 1–16.

  [3] C. Alós-Ferrer, Dynamical systems with a continuum of randomly matched agents, Journal of
      Economic Theory 86 (1999), 245–267.

  [4] R. M. Anderson, Non-standard analysis with applications to economics, in Handbook of Mathe-
      matical Economics IV (W. Hildenbrand and H. Sonnenschein eds.), North-Holland, New York,
      1991.

  [5] T. F. Bewley, Existence of equilibria in economies with infinitely many commodities, Journal of
      Economic Theory 4 (1972), 514–540.

  [6] K. Binmore and L. Samuelson, Evolutionary drift and equilibrium selection, Review of Economic
      Studies 66 (1999), 363–393.

  [7] R. T. Boylan, Laws of large numbers for dynamical systems with randomly matched individuals,
      Journal of Economic Theory 57 (1992), 473–504.

  [8] K. Burdzy, D. M. Frankel, and A. Pauzner, Fast equilibrium selection by rational players living
      in a changing world, Econometrica 69 (2001), 163–189.

  [9] L. L. Cavalli-Sforza, and W. F. Bodmer, The Genetics of Human Population, Freeman, San
      Francisco, 1971.

 [10] H. Cole and R. Rogerson, Can the Mortenson-Pissarides matching model match the business-cycle
      facts?, International Economic Review 40 (1999), 933–959.

 [11] E. Dekel and S. Scotchmer, On the evolution of attitudes towards risk in winner-take-all games,
      Journal of Economic Theory 87 (1999), 125–143.

 [12] P. Diamond, A model of price adjustment, Journal of Economic Theory 3 (1971), 156–168.

 [13] P. Diamond, Aggregate demand management in search equilibrium, Journal of Political Economy
      90 (1982), 881–894.

 [14] P. Diamond and J. Yellin, Inventories and money holdings in a search economy, Econometrica 58
      (1990), 929–950.

 [15] J. L. Doob, Stochastic processes depending on a continuous parameter, Transactions of the Amer-
      ican Mathematical Society 42 (1937), 107–140.

 [16] J. L. Doob, Stochastic Processes, Wiley, New York, 1953.

 [17] D. Duffie, N. Gârleanu, and L. H. Pedersen, Valuation in over-the-counter markets, Review of
      Financial Studies 20 (2007), 1865–1900.

 [18] D. Duffie, N. Gârleanu, and L. H. Pedersen, Over-the-counter markets, Econometrica 73 (2005),
      1815–1847.

 [19] D. Duffie and Y. N. Sun, Existence of independent random matching, Annals of Applied Probability
      17 (2007), 386–419.



                                                 39
[20] M. Feldman and C. Gilles, An expository note on individual risk without aggregate uncertainty,
     Journal of Economic Theory 35 (1985), 26–32.

[21] D. H. Fremlin, Measure Theory, Volume 4: Topological Measure Spaces, second ed., Torres Frem-
     lin, Colchester, 2006.

[22] D. H. Fremlin, Measure Theory, Volume 5:            Set-theoretic Measure Theory (version
     8.5.03/29.6.06), 2005. Available at http://www.essex.ac.uk/maths/staff/fremlin/mt.htm

[23] D. Fudenberg and D. Levine, Steady-state learning and Nash equilibrium, Econometrica 61
     (1993), 547–573.

[24] D. Gale, Bargaining and competition, Part I: characterization, Econometrica 54 (1986), 785–806.

[25] D. Gale, Bargaining and competition, Part II: existence, Econometrica 54 (1986), 807–818.

[26] I. Gilboa and A. Matsui, A model of random matching, Journal of Mathematical Economics 21
     (1992), 185–197.

[27] E. J. Green, Individual-level randomness in a nonatomic population, Working Paper, University
     of Minnesota, 1994.

[28] E. J. Green and R. Zhou, Dynamic monetary equilibrium in a random matching economy, Econo-
     metrica 70 (2002), 929–970.

[29] G. H. Hardy, Mendelian proportions in a mixed population, Science 28 (1908), 49–50.

[30] J. E. Harrington, The social selection of flexible and rigid agents, American Economic Review 88
     (1998), 63–82.

[31] M. Hellwig, A model of monetary exchange, Econometric Research Program, Research Memoran-
     dum Number 202, Princeton University, 1976.

[32] A. Hosios, On the efficiency of matching and related models of search and unemployment, Review
     of Economic Studies 57 (1990), 279–298.

[33] K. L. Judd, The law of large numbers with a continuum of iid random variables, Journal of
     Economic Theory 35 (1985), 19–25.

[34] S. Kakutani, Construction of a non-separable extension of the Lebesque measure space, Proc.
     Imp. Acad. Tokyo 20 (1944), 115-119.

[35] H. J. Keisler, Hyperfinite model theory, in Logic Colloquium 76 (R. O. Gandy and J. M. E. Hyland
     eds.), North-Holland, Amsterdam, 1977.

[36] N. Kiyotaki and R. Wright, On money as a medium of exchange, Journal of Political Economy
     97 (1989), 927–954.

[37] N. Kiyotaki and R. Wright, A search-theoretic approach to monetary economics, American Eco-
     nomic Review 83 (1993), 63–77.

[38] J. Krainer and S. LeRoy, Equilibrium valuation of illiquid assets, Economic Theory 19 (2002),
     223–242.

[39] L. Ljungqvist and T. Sargent, Recursive Macroeconomic Theory, Boston, MIT Press, 2000.

[40] P. A. Loeb and M. Wolff, eds. Nonstandard Analysis for the Working Mathematician, Kluwer
     Academic Publishers, Dordrecht, 2000.


                                                40
[41] J. Maynard-Smith, Evolution and the Theory of Games, Cambridge University Press, Cambridge,
     UK, 1982.

[42] A. McLennan and H. Sonnenschein, Sequential bargaining as a noncooperative foundation for
     Walrasian equilibrium, Econometrica 59 (1991), 1395–1424.

[43] M. Merz, Heterogeneous job matches and the cyclical behavior of labor turnover, Journal of
     Monetary Economics 43 (1999), 91–124.

[44] R. Molzon, D. Puzzello, On the observational equivalence of random matching, Journal of Eco-
     nomic Theory 145 (2010), 1283–1301

[45] D. Mortensen, Property rights and efficiency in mating, racing, and related games, American
     Economic Review 72 (1982), 968–979.

[46] D. Mortensen and C. Pissarides, Job creation and job destruction in the theory of unemployment,
     The Review of Economic Studies 61 (1994), 397–415.

[47] K. Podczeck and D. Puzzello, Independent random matching, Economic Theory, forthcoming.

[48] R. Rogerson and R. Wright, Search-theoretic models of labor markets, Journal of Economic
     Literature 43 (2005), 959–988.

[49] P. Rupert, M. Schindler, and R. Wright, The search-theoretic approach to monetary economics:
     a primer, Journal of Monetary Economics 48 (2001), 605–622.

[50] Y. N. Sun, The exact law of large numbers via Fubini extension and characterization of insurable
     risks, Journal of Economic Theory 126 (2006), 31–69.

[51] Y.N. Sun, On the characterization of individual risks and Fubini extension, National University
     of Singapore, mimeo., 2007.

[52] Y. N. Sun and Y. C. Zhang, Individual risk and Lebesgue extension without aggregate uncertainty,
     Journal of Economic Theory 144 (2009), 432–443.

[53] A. Trejos and R. Wright, Search, bargaining, money, and prices, Journal of Political Economy
     103 (1995), 118–140.

[54] D. Vayanos and T. Wang, Search and endogenous concentration of liquidity in asset markets,
     Journal of Economic Theory 136 (2007) 66–104.

[55] P.-O. Weill, Liquidity premia in dynamic bargaining markets, Journal of Economic Theory 140
     (2008), 66–96.

[56] A. Wolinsky, Information revelation in a market with pairwise meetings, Econometrica 58 (1990)
     1–23.




                                                41
