                                NBER WORKING PAPER SERIES




  A SIMPLE NONPARAMETRIC ESTIMATOR FOR THE DISTRIBUTION OF RANDOM
                            COEFFICIENTS

                                           Patrick Bajari
                                           Jeremy T. Fox
                                            Kyoo il Kim
                                          Stephen P. Ryan

                                        Working Paper 15210
                                http://www.nber.org/papers/w15210


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                     August 2009




Bajari thanks the National Science Foundation, grant SES-0720463, for generous research support.
Fox thanks the National Science Foundation, the Olin Foundation, and the Stigler Center for generous
funding. Thanks to helpful comments from seminar participants at the AEA meetings, Chicago, Chicago
GSB, UC Davis, European Commission antitrust, Far East Econometric Society meetings, LSE, Mannheim,
MIT, Northwestern, Paris I, Quantitative Marketing and Economics, Queens, Rochester, Rutgers, Stanford,
Stony Brook, Toronto, UCL, USC, Virginia, and Yale. Thanks to comments from Xiaohong Chen,
Andrew Chesher, Philippe Fevrier, Amit Gandhi, Han Hong, David Margolis, Andrés Musalem, Peter
Reiss, Jean-Marc Robin, Andrés Santos, Azeem Shaikh and Harald Uhlig. Thanks to research assistance
from Chenchuan Li. The views expressed herein are those of the author(s) and do not necessarily
reflect the views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2009 by Patrick Bajari, Jeremy T. Fox, Kyoo il Kim, and Stephen P. Ryan. All rights reserved. Short
sections of text, not to exceed two paragraphs, may be quoted without explicit permission provided
that full credit, including © notice, is given to the source.
A Simple Nonparametric Estimator for the Distribution of Random Coefficients
Patrick Bajari, Jeremy T. Fox, Kyoo il Kim, and Stephen P. Ryan
NBER Working Paper No. 15210
July 2009
JEL No. C01,C14,C25,C31,C35,I21,I28,L0,O1,O15

                                               ABSTRACT

We propose a simple nonparametric mixtures estimator for recovering the joint distribution of parameter
heterogeneity in economic models, such as the random coefficients logit. The estimator is based on
linear regression subject to linear inequality constraints, and is robust, easy to program and computationally
attractive compared to alternative estimators for random coefficient models. We prove consistency
and provide the rate of convergence under deterministic and stochastic choices for the sieve approximating
space. We present a Monte Carlo study and an empirical application to dynamic programming discrete
choice with a serially-correlated unobserved state variable.


Patrick Bajari                                        Kyoo il Kim
Professor of Economics                                Department of Economics
University of Minnesota                               University of Minnesota
4-101 Hanson Hall                                     4-129 Hanson Halll
1925 4th Street South                                 1925 4th Street South
Minneapolis, MN 55455                                 Minneapolis, MN 55455
and NBER                                              kyookim@umn.edu
bajari@econ.umn.edu
                                                      Stephen P. Ryan
Jeremy T. Fox                                         MIT Department of Economics
Department of Economics                               E52-262C
University of Chicago                                 50 Memorial Drive
1126 East 59th Street                                 Cambridge, MA 02142
Chicago, IL 60637                                     and NBER
and NBER                                              sryan@mit.edu
fox@uchicago.edu
1    Introduction
In economics, it is common to observe that otherwise identical agents behave differently when
faced with identical choice environments, due to such factors as heterogeneity in preferences. A
growing econometric literature has addressed this problem by providing estimators that allow the
coefficients of the economic model to vary across agents (for recent work in demand estimation,
see Berry, Levinsohn, and Pakes (1995), Nevo (2001), Petrin (2002), and Rossi, Allenby, and
McCulloch (2005)). In this paper, we describe a general method for estimating such random
coefficient models that is both nonparametric with respect to the underlying distribution of het-
erogeneity and easy to compute. Our estimator exploits a reparameterization of the underlying
model so that the parameters enter linearly. This is in contrast to previous approaches in the
literature, such as the EM algorithm, Markov Chain Monte Carlo (MCMC), simulated maximum
likelihood and simulated method of moments, which are highly nonlinear. Linearity simplifies
the computation of the parameters.
    To motivate our approach, consider the following simple example. Suppose that the econo-
metrician is interested in estimating a binary logit model with a scalar random coefficient. Let
y = 1 if the first option is chosen and let y = 0 otherwise. Suppose that this model has a single
independent variable, x. Furthermore, suppose that the random coefficient is known to have
support on the [0,1] interval. Fix a large but finite grid of R equally spaced points. Suppose
                                           1 2        R−1                                      r
that the grid points take on the values   R , R , ..., R , 1. The parameters of our model are θ ,   the
                                          r
frequencies of each random coefficient    R . It then follows that the empirical probability that   the
dependent variable y is 1 conditional on x can be approximated by the linear combination

                                                R
                                                      exp Rr · x
                                                                 
                                                X
                                                      r
                             Pr (y = 1 | x) ≈     θ                .
                                              r=1
                                                    1 + exp Rr · x

The key insight of our approach is that the dependent variable in our model, Pr (y = 1 | x) , is
linearly related to the model parameters θr , irrespective of the nonlinear model used to compute
the probability under a given type r. Instead of optimizing over that nonlinear model, we com-
pute the probability under each type as if it were the true parameter, and then find the proper
mixture of those models that best approximates the actual data. In the paper, we demonstrate
that the θr parameters can be consistently estimated using inequality constrained least squares.
This estimator has a single, global optimum and widely-available specialized minimization ap-
proaches are guaranteed to converge to that point. This contrasts with alternative approaches



                                                  2
to estimating random coefficient models, where the objective functions can have multiple local
optima and the econometrician is not guaranteed to find the global solution.
   We also note that our approach does not require a parametric specification for the distribution
of the random coefficients. We can closely approximate any well behaved distribution on [0,1]
by making R large and by choosing θr appropriately. Intuitively, our framework allows us to
manipulate the cell size in a histogram. Many alternative estimators require computationally
expensive nonlinear optimization, and as a result researchers frequently use tightly specified
parametric models in applied work because of computational constraints. For example, applied
researchers frequently assume that the random coefficients are mutually independent and normal.
Our approach allows us to estimate the joint distribution of random coefficients without having
to impose restrictions on the family of distributions.
   We show how to extend this simple intuition to a more general framework. First, modeling the
random coefficients using equally spaced grid points does not lead to a smooth estimated density.
We suggest alternative methods for discretizing the model that give smooth densities, while still
maintaining the linear relationship between the dependent variable and the model parameters.
Second, we discuss how our approach can be extended to more complex economic choice models.
As an example, we show how to include a nonparametric distribution of random coefficients in a
dynamic programming discrete choice model such as Rust (1987). In our empirical application,
we demonstrate that our approach can dramatically reduce the computational burden of these
models. Third, we extend our approach to accommodate the case where the support of the basis
functions is not known, and the econometrician must search over the parameter space.
   Our approach can include random coefficients on multiple independent variables. Because of
the curse of dimensionality in estimating infinite dimensional objects, the rate of convergence of
the estimator of the distribution (with a deterministic choice of grid points) is inversely related to
the number of random coefficients. With a stochastic choice of grid points, the rate of convergence
is invariant to the number of random coefficients.
   We do not claim that our estimator dominates all existing methods in all potential demand-
estimation applications. In the paper, we discuss some strengths and weaknesses of our approach.
First, if the researcher has aggregate data on market shares with price endogeneity, the approach
of Berry, Levinsohn and Pakes (1995) is likely to be preferable. Our approach can accommodate
aggregate data, even if market shares are measured with error. However, we need to specify
a reduced-form pricing function as in Petrin and Train (2009) in order to account for price
endogeneity. While this approach has worked well in some examples, it is not possible to prove
the existence of this pricing function in the general case. Second, in small samples, Bayesian

                                                  3
methods that use prior information will likely have better finite-sample performance (Rossi,
Allenby and McCulloch 2005; Burda, Harding and Hausman 2008). Our methods are intended
for applications where the researcher has access to large sample sizes.
    Our approach is a general, nonparametric mixtures estimator. The most common frequentist,
nonparametric estimator is nonparametric maximum likelihood or NPMLE (Laird 1978, Böhning
1982, Lindsay 1983, Heckman and Singer 1984). Often the EM algorithm is used for computation
(Dempster, Laird and Rubin 1977), but this approach is not guaranteed to find the global
maximum. The literature worries about the strong dependence of the output of the EM algorithm
on initial starting values and well as the difficulty in diagnosing convergence (Seidel, Mosler and
Alker 2000, Verbeek, Vlassis and Kröse 2002, Biernacki, Celeux and Govaert 2003, Karlis and
Xekalaki 2003).1 Further, the EM algorithm has a slow rate of convergence even when it does
converge to a global solution (Pilla and Lindsay 2001). Li and Barron (2000) introduce another
alternative, but again our approach is computationally simpler.2 The discrete-grid idea (called
the “histogram” approach) is found outside of economics in Kamakura (1991), who uses a discrete
grid to estimate an ideal-point model. He does not discuss identification, the nonparametric
statistical properties of his approach, or any of our extensions. Of course, mixtures themselves
have a long history in economics (Quandt and Ramsey 1978).
    We prove the consistency of our estimator for the distribution of random parameters, in
function space and under a potential ill-posed inverse problem. We note that many if not most
of the alternative estimators discussed above do not have general, nonparametric consistency
theorems for the estimator for the distribution of random parameters. Our consistency theorem
is not specific to the economic model being estimated.
    The outline of our paper is as follows. In section 2, we start by precisely describing our
model and estimator. We focus on the motivating example of discrete choice with individual
data. However, we provide other examples such as discrete choice with aggregate data, dynamic
   1
     Another drawback of NPMLE that is specific to mixtures of normal distributions, a common approximating
choice, is that the likelihood is unbounded and hence maximizing the likelihood does not produce a consistent
estimator. There is a consistent root but it is not the global maximum of the likelihood function (McLachlan and
Peel 2000).
   2
     There is a literature on customized estimators for particular economic models as opposed to general mixtures
estimators. For discrete choice models, Ichimura and Thompson (1998) and Gautier and Kitamura (2008) provide
estimators for models with two choices and where covariates enter linearly. Lewbel (2000) considers the identifica-
tion and estimation of mean preferences using a special regressor and a mean independence assumption. Manski’s
(1975) maximum score estimator is consistent for mean preferences in the presence of random coefficients for the
case of two choices only. Briesch, Chintagunta and Matzkin (2007) allow utility to be a nonparametric function of
x and a scalar unobservable. Hoderlein, Klemelä and Mammen (2008) estimate the distribution of heterogeneity
in a linear regression model.



                                                        4
discrete choice, mixed continuous and discrete choice (selection), and discrete choice models with
endogenous regressors. Following Chen and Pouzo (2009a, 2009b), we view our estimator as a
sieve estimation problem. In section 3, we prove consistency in a function space if the true
distribution is any arbitrary distribution function. In section 4, under additional assumptions
we derive a pointwise rate of convergence for our distribution estimator. In section 5, we discuss
how to conduct inference under the assumption that the set of types used in estimation is
the true set of types. In section 6, we conduct a Monte Carlo experiment to investigate the
finite sample performance of our estimator. In section 7, we apply our estimator to a dynamic
programming, discrete choice empirical problem studied in Duflo, Hanna and Ryan (2008). The
dynamic programming problem has a serially correlated, unobserved state variable. We allow
for a nonparametric distribution of random parameters. Code that implements our estimator is
available on the internet.3


2        The Estimator
2.1        General Notation
We first introduce general notation. The econometrician observes a real valued vector of co-
variates x. The dependent variable in our model is denoted y. In our examples, we will focus
primarily on the case where the range of y is a finite number of integer values as is customary in
discrete choice models. However, much of our analysis extends to the case where y is real valued.
        Let A denote a (measurable) set in the range of y, Y. We let PA (x) denote the probability
that y ∈ A when the decision problem has characteristics x. Let β denote a random coefficient. In
our framework, this is a finite-dimensional, real-valued vector. We let gA (x, β) be the probability
of A conditional on the random coefficients β and characteristics x. The density and CDF of
the random coefficients are denoted as f (β) and F (β) respectively. The function gA is specified
as a modeling primitive. In our simple example in the introduction, gA corresponds to the logit
model. Given these definitions it follows that
                                            Z
                                  PA (x) = gA (x, β) dF (β) .                                   (1)

On the right hand side of the above equation, gA (x, β) gives the probability of A conditional
on x and β. We average over the distribution of β using the CDF F (β) to arrive at PA (x), the
    3
        The code can be accessed at http://home.uchicago.edu/~fox .



                                                         5
population probability of the event A conditional on x.
       In our framework, the object the econometrician wishes to estimate is F (β), the distribution
of random coefficients. Identification means that a unique F (β) solves (1) for all x and all A.4
       More technical readers may prefer functional operator notation. Let L be an operator that
takes the distribution function F as an argument and produces the probability function P =
{PA (·) ∀ A}, where P is PA (x) for all valid A and x. With a limiting dataset, the econometrician
can identify P using observations on pairs (x, y). Let (1), for all valid x and A, be written as
P = L (F ). Identification of the distribution means L−1 is a function. A restatement of the
definition of identification is that L is one-to-one in F : each valid F produces a different P.
       This paper focuses on estimating F using a finite dataset (xi , yi ) for i = 1, . . . , N and not
identification. The proof of consistency of our estimator will address the potential ill-posedness
of L: the inverse of L, L−1 , may not be continuous in x, when A is fixed.

2.2       The Logit Model
As we discussed in the introduction, the motivating example for our paper is the logit with
random coefficients. We shall begin by discussing this example in detail. Afterwards, we will
show how our approach extends to other random coefficient models including dynamic discrete
choice and demand models where the dependent variable is represented by a vector of discrete
and continuous variables. The estimation method that we propose, however, can in principal be
applied to any model that can be written in the form (1).
       In the logit model, agents i = 1, ..., N can choose between j = 1, ..., J mutually exclusive
alternatives. The exogenous variables for choice j are in the K × 1 vector xi,j . In the example
of demand estimation, xi,j might include the non-price product characteristics, the price of good
j and the demographics of agent i. We shall let xi = (x0i,1 , ..., x0i,J ) denote the stacked vector of
the J xi,j ’s, the observable characteristics.
       In the model, there are r = 1, ..., R types of agents. The unobservable preference parameters
of type r are equal to the K × 1 vector β r . We shall discuss how to choose the types β r below.
For the moment, assume that the β r are fixed and exogenously specified. As in our example
in the introduction, it might be helpful to think of the β r being defined using a fixed grid on
a compact set. Later we will view the R types of consumers as giving a nonparametric, sieve
approximation to an arbitrary distribution for β, F (β). The random variable β is distributed
   4
    This is the definition used in certain relevant papers on identification in the statistics literature, see Teicher
(1963).



                                                          6
independently of x. The probability of type r in the population is θr . Let θ = (θ1 , ..., θR )0 denote
the corresponding vector. This will be a parameter in our model. The θ must lie on the simplex,
or
                                                    R
                                                    X
                                                          θr = 1                                      (2)
                                                    r=1

                                                          θr ≥ 0.                                     (3)

If agent i is of type r, her utility for choosing good j is equal to

                                               ui,j = x0i,j β r + i,j .

There is an outside good with utility ui,0 = εi,0 . Assume that i,j is distributed as Type I extreme
value and is independent of xi and β r . Agents in the model are assumed to be utility maximizers.
The observable dependent variable yi,j is generated as
                                            
                                            1 if u > u 0 for all j 0 6= j
                                                   i,j  i,j
                                   yi,j   =
                                            0 otherwise.

Let yi be the vector (yi,1 , . . . , yi,J ). The probability of type r picking choice j at xi is
                                                                     
                                                        exp x0i,j β r
                                   gj (xi , β r ) =                           .
                                                    1 + Jj0 =1 exp x0i,j 0 β r
                                                       P


Because the error term is extreme value, it follows that
                                                                                       
                                          R
                                          X                     R
                                                                X         exp x0i,j β r
               Pr (yi,j   = 1 | xi ) =     θr gj (xi , β r ) =     θr                           .   (4)
                                                                      1 + Jj0 =1 exp x0i,j 0 β r
                                                                         P
                                       r=1                     r=1


2.3    Linear Regression
We study the estimation problem: the researcher has i = 1, . . . , N observations on (xi , yi ). A
first method for estimating the parameters θ is by ordinary least squares. To construct the
estimator, begin by adding yi,j to both sides of (4) and moving Pr (yi,j = 1 | xi ) to the right side



                                                           7
of this equation, which gives

                                   R
                                                             !
                                   X
                         yi,j =          θr gj (xi , β r )       + (yi,j − Pr (yi,j = 1 | xi )) .
                                   r=1

Define the R × 1 vector zi,j = (zi,j,1 , ..., zi,j,R )0 with individual elements zi,j,r = gj (xi , β r ) and let
zi = (zi,1 , . . . , zi,J ) be the stacked vector of zi,j ’s. Recall that β r is fixed and is not a parameter
to be estimated. As a result, given xi , the term zi,j,r is a known constant. Next, by the definition
of a choice probability,
                                     E [yi,j − Pr (yi,j = 1 | xi ) | zi ] = 0.                                  (5)

This implies that the following ordinary least squares problem is a consistent estimator of the
θr ,
                                                N J
                                              1 XX          0
                                                                 2
                                  θ = arg min
                                  b                 yi,j − zi,j θ .
                                           θ NJ
                                                             i=1 j=1

Let Y denote the N J × 1 vector formed by stacking the yi,j and let Z be the N J × R matrix
formed by stacking the zi,j . Then our estimator is θb = (Z 0 Z)−1 Z 0 Y .5
       Equation (5) implies that the standard exogeneity condition is satisfied for least squares.
The “error term” in our least squares regression is yi,j − Pr (yi,j = 1 | xi ). This is the difference
between the yi,j and the probability that yi,j = 1. This prediction error only depends on the
realization of the εi,j ’s, which are i.i.d. and as a consequence independent of zi . The least squares
estimator will have a unique solution so long as Z has rank R.

2.4      Inequality Constrained Linear Least Squares
A limitation of the ordinary least squares estimator is that θb need not satisfy (2) and (3). In
practice, one might wish to constrain θb to be a well-defined probability measure. This would
be useful in making sure that our model predicts probabilities that always lie between zero and
one. Also, this may be important if the economist wishes to interpret the distribution of β as a
   5
     We focus on the least squares criterion, rather than a likelihood or pseudo-likelihood, for computational
simplicity. However, it is not necessarily the case that the least squares criterion will result in a less efficient
estimator. The least squares objective function enforces all of the parametric assumptions inherent in the logit
model, which arises in the definition of zi,j,r . There is no sense in which the likelihood alternative uses more
structure. Second, in a nonparametric world, the true distribution lies in an infinite-dimensional space, and
the limiting distribution of any estimator for the infinite-dimensional object is not known. Therefore, we avoid
discussing efficiency, which relies on the estimator having an asymptotically normal distribution.




                                                                 8
structural parameter. We suggest estimating θ using inequality constrained least squares,

                                               N J
                                             1 XX          0
                                                                 2
                                θb = arg min       yi,j − zi,j θ                                              (6)
                                          θ NJ
                                                           i=1 j=1
                                         subject to (2) and (3).

This minimization problem is a quadratic programming problem subject to linear inequality
constraints.6 The minimization problem is convex and routines like MATLAB’s lsqlin guarantee
finding a global optimum. One can construct the estimated cumulative distribution function for
the random coefficients as
                                                     R
                                                     X
                                          F̂ (β) =         θ̂r 1 [β r ≤ β] ,
                                                     r=1

where 1 [β r ≤ β] is equal to 1 when β r ≤ β. Thus, we have a structural estimator for a distribution
of random parameters in addition to a flexible method for approximating choice probabilities.

2.5        Smooth Basis Densities
A limitation of the method above is that the CDF of the random parameters will be a step
function. In applied work, it is often attractive to have a smooth distribution of random param-
eters. In this subsection, we describe one approach to estimating a density instead of a CDF.
Our approach is easily extended to this case: instead of modeling the distribution of the random
parameters as a mixture of point masses, we instead model the density as a mixture of normal
densities.
         Let a basis r be a normal distribution with mean the K × 1 vector µr and standard deviation
the K × 1 vector σ r . Let N (βk | µrk , σkr ) denote the normal density of the kth random parameter.
Under normal basis functions, the joint density for a given r is just the product of the marginals,
or
                                                          Y
                                    N (β | µr , σ r ) =        N (βk | µrk , σkr ) .
                                                           k

Let θr denote the probability weight given to the rth basis, N (β | µr , σ r ). As in the previous
     6
    Lindsay (1983) discusses how if there are at most M distinct values of the N observations (xi , yi ), then the
nonparametric maximum likelihood estimator can maximize the likelihood using at most M points of support for
the beta space, β. Adding more points of support would not increase the statistical fit given the finite sample.
Here, we fix the points of support and estimate only the weights, so we do not appeal to Lindsay’s results. In our
simulations and empirical work, we often find that only a few of the many points of support have nonzero weights
in the final estimates.


                                                           9
subsection, it is desirable to constrain θ to lie in the simplex, that is, (2) and (3).
       For a given r, make S simulation draws from N (β | µr , σ r ). Let a particular draw s be
denoted as β r,s . We can then simulate Pr (yi,j = 1 | xi ) as:
                                                                                                              
                          R            S
                                                            !     R           S       exp     x 0 β r,s
                          X          1X                           X        1 X                  i,j
  Pr (yi,j = 1 | xi ) ≈         θr       gj (xi , β r,s )       =     θr                                       .
                                     S                                     S     1 +
                                                                                     PJ
                                                                                              exp     x 0 β r,s
                          r=1          s=1                        r=1        s=1       j 0 =1           i,j 0


We use the ≈ to emphasize from now on that we work with sieve approximations in finite samples.
We can then estimate θ using the inequality constrained least squares problem

                                   N J        R                              S
                                                                                                  !!2
                                 1 XX         X                            1X
                    θb = arg min       yi,j −   θr                             gj (xi , β r,s )
                              θ NJ                                         S
                                          i=1 j=1               r=1            s=1

                     subject to (2) and (3).

This is once again inequality constrained, linear least squares, a globally convex optimization
problem.7 The resulting density estimate is

                                                     R
                                                     X
                                          fˆ (β) =         θ̂r N (β | µr , σ r ) .
                                                     r=1


2.6      Location Scale Model
In many applications, the econometrician may not have good prior knowledge about the support
region where most of the random coefficients β r lie. This is particularly true in models where
the covariates are high dimensional. There are three approaches to finding a region of support
in such settings.
       First, the econometrician may use a preliminary estimate of a logit model with fixed coeffi-
cients to determine a region of support for the random coefficients. For example, the econome-
trician may center the grid for the β r at the initial logit estimates.
       A second, related approach is that the econometrician may experiment with alternative sets
of grid points to see how the choice of grid points influences estimation results. A limitation
   7
    If the true distribution has a continuous density function, using an estimator that imposes that the density
estimate is smooth may provide better statistical performance than an estimator that imposes no restrictions on
the true distribution function. We do not formally derive the rate of convergence of the estimator for densities or
compare the rate to the rate for the earlier estimator for CDFs, which is derived in Section 4.



                                                            10
of this approach is that this introduces a pre-test bias and the standard errors that we present
later in the paper will need to be adjusted. For example, a procedure where a researcher uses a
diffuse grid with a wide support and then re-estimates the model after increasing the detail of
the grid where mass appears to be located is, in its full statistical structure, a two-step estimator.
Rigorous standard errors should account for both of the estimation-steps.8
       A third approach is to introduce location and scale parameters. To illustrate the idea, let
the unscaled basis vectors {β r }R                        K
                                 r=1 lie in the set [0, 1] , that is, the K-fold Cartesian product of
the unit interval. We include a set of location and scale parameters µk and σk , k = 1, ..., K and
define the rth random coefficient for the kth characteristic as µk + σk βkr .
       In numerical optimization, we now search over R + 2K parameters corresponding to θ and
µ = (µ1 , ..., µK )0 and σ = (σ1 , ..., σK )0 . Market shares predictions for type r are

                                                             K
                                                                                    
                                                                          σk βkr )
                                                             P
                                                   exp      xk,i,j (µk +
                                                          k=1
                     gj (xi , µ + σβ r ) =                  K                          ,
                                                PJ           P                      r
                                                                                      
                                             1 + j 0 =1 exp       xk,i,j 0 µk + σk βk
                                                                    k=1

where σβ r is assumed to represent element-by-element multiplication. Our estimator for the
weights solves the nonlinear least squares problem

                                        N J        R
                                                                        !2
                                     1 XX          X
                                                      r               r
                              min           yi,j −   θ gj (xi , µ + σβ )
                              µ,σ,θ N J
                                        i=1 j=1               r=1
                              subject to (2) and (3).                                                       (7)

The appropriate MATLAB routine is lsqnonlin, and there is no theorem that the routine will
converge to a global minimum. Note that the model has 2K nonlinear parameters and the
remaining R parameters still enter linearly. Also, µ and σ do not enter the constraints, which are
still linear. Using exact, rather than numerical derivatives will improve the speed and convergence
of the estimator.9 The derivatives to our objective function can be expressed in closed form up to
any order, at least for the logit example. In numerical experimentation with the logit example,
   8
     A more sophisticated approach would be to choose a massive grid and then add in a penalty function that
increases the objective function for values of θ that assign nonzero weight to a large number of points. Our
consistency theorem fits into the sieve framework of Chen and Pouzo (2009a). Their theorems, which we reference
in our proof of consistency, explicitly allow for penalty functions and so our estimator is consistent if such a
penalty function is used. We will not discuss penalty functions for conciseness.
   9
     Automatic differentiation is an option to compute derivatives.



                                                         11
we have found the convergence of the estimator to be robust.
   In many applied problems, the number of characteristics K may be large and it may not
be practical to estimate a K-dimensional nonparametric distribution of random coefficients.
Therefore, it may be desirable to only let a subset of the most important characteristics have
random coefficients. This is possible by a trivial modification of (7).

2.7     Choosing R and the β r ’s
We have discussed the location and scale model for when the support of β r is not known and the
β r are picked on a grid. The β r in this case can be chosen by using a random number generator
to make draws from [0, 1]K . We have also experimented with Halton and Weyl sequences; they
tend to improve Monte Carlo performance over evenly-spaced grids. The rate of convergence
derived in section 4 uses results from the theory of quadrature. Another rate of convergence,
derived in section 4.1, relies on choosing a grid using random draws from a chosen distribution.
   It is also natural to ask how to choose R, the number of basis components. The nonparametric
rates of convergence in section 4 are not useful for choosing R in a finite sample. However, the
rates do suggest that R should not increase too quickly with N , the number of observations. In
                                                          N
Monte Carlo experiments in section 6, we set R =          40 .   Our Monte Carlos show a small R (say a
few hundred basis vectors) can give a good approximation to F (β) in a finite sample.

2.8     Additional Examples
As we discussed in the introduction to this section, our estimator can in principal be applied to
any setting where the model can be written in the form (1). Below, we discuss some additional
examples that fit into in our framework that may be of interest to applied researchers.

2.8.1    Aggregate Data

Our estimator can still be used if the researcher has access to data on market shares sj,t , rather
than individual-level choice data yi,j . Index markets by t = 1, ..., T . In this framework, we assume
that the utility of person i when the type of i is r is

                                         uri,j = x0j,t β r + i,j .

In this model, the utility of individuals is a function of product- and market-varying attributes
xj,t . In applied work, characteristics will vary across markets in the sample. If a market has a


                                                    12
continuum of consumers, our modeling framework implies that the market share of product j
should satisfy
                               R                           R
                               X                           X              exp (xj,t β r )
                      sj,t ≈         θr gj (xt , β r ) =         θr      PJ                      ,
                               r=1                         r=1        1 + j 0 =1 exp xj 0 ,t β r

where we let xt = (x01,t , ..., x0J,t ) denote the stacked vector of the all the xj,t and the ≈ sign just
indicates that the set R of types is a sieve approximation. Suppose that the economist only
observes a noisy measure sbj,t of the true share. This is common in applied work. For example,
there may be a finite number of consumers in a market. Let the actual share be denoted as sbj,t .
Simple algebra implies that

                                            R
                                                                      !
                                            X
                                                   r             r
                                sbj,t ≈           θ gj (xt , β )             sj,t − sj,t ) .
                                                                          + (b
                                            r=1

                               sj,t − sj,t | xt ] = 0. That is, the difference between the measured
Under standard assumptions, E [b
shares and the true shares is independent of the product characteristics in our model xt . This
would be the case if the difference between sbj,t and sj,t is accounted for by random sampling.
Then we can estimate θ using the regression

                                        T X
                                          J         R
                                                                    !2
                                      1 X           X
                                                       r          r
                         θb = arg min       sbj,t −   θ gj (xt , β )
                                   θ JT
                                                  t=1 j=1                 r=1

                          subject to (2) and (3).

One advantage of our approach is that it can accommodate measurement error in the market
shares. We note that the method of Berry, Levinsohn and Pakes (1995) assumes that sj,t is
observed without error by the economist. If the number of persons in a survey or in a market is
small, this assumption may be a poor approximation.
   A drawback of our approach is that it may require less attractive assumptions than Berry,
Levinsohn and Pakes (1995) to account for price endogeneity and aggregate demand shocks. One
could use the control function approach of Petrin and Train (2009) to account for this problem.
Petrin and Train argue that the control function works well in the applied examples that they
consider. However, it has two potential drawbacks. The first is that this approach assumes that
there is a reduced-form pricing equation that permits the econometrician to infer the omitted
product attributes from the observed prices. In general, it is unknown whether this reduced form
exists. The second limitation is that the control function approach requires inserting a generated


                                                            13
regressor that depends on a nonparametric function into our estimator. This complication is
beyond the scope of our asymptotic theory.

2.8.2       Dynamic Programming Models

Our approach can be applied to dynamic discrete choice models as in Rust (1987, 1994). We
generalize the framework he considers by allowing for a nonparametric distribution of random
coefficients. Suppose that the flow utility of agent i in a period t from choosing action j is

                                                  ui,j,t = x0i,j,t βi + i,j,t .

The error term i,j,t is a preference shock for agent i’s utility to choice j at time period t. For
simplicity, the error term is i.i.d. extreme value across agents, choices and time periods. Agent
i’s decision
            problem isdynamic because there is a link between the current and future values of
xi,t = xi,1,t , . . . , x0i,J,t through current decisions. Let π (xi,t+1 | xi,t , ji,t ) denote the transition
           0

probability for the state variable xi,t as a function of the action of the agent, ji,t . This does not
involve random coefficients and we assume that it can be estimated in a first stage.
       The goal is to estimate F (β), the distribution of the random coefficients. Again we pick
R basis vectors β r . For each of the R basis vectors, we can solve the corresponding single-
agent dynamic programming problem for the state xi,t value functions, V r (xi,t ). Once all value
functions V r (xi,t ) are known, the choice probabilities gj (xi , β r ) for all combinations of choices
j and states xi,t can be calculated as
                                                                                
                              exp x0i,j,t β r + δE [V r (xi,t+1 ) | xi,t , j]                   exp (v r (j, xi,t ))
       gj (xi,t , β r ) = P                                                              = PJ                          ,
                           J             0          r + δE [V r (x                     0]          exp  (v r (j 0 , x ))
                            0
                           j =1 exp   x  i,j 0 ,t β                i,t+1 ) | x i,t , j         0
                                                                                              j =1                   i,t


where v r (j, xi,t ) is Rust’s choice-specific continuation value, here implicitly defined in the above
equation. The scalar δ ∈ [0, 1) is a discount factor fixed by the researcher before estimation, as
is usually done in empirical practice.10
       We use panel data on N panels of length T each. Suppose that we observe yi,j,t , an indicator
variable equal to one if j is chosen at time t by agent i. Let xi,t be the state of agent i at time
  10
    Alternatively, the discount factor could be a random parameter and the goal would be to estimate the
distribution of (δ, β).




                                                               14
t. We can then estimate

                                     N T J          R
                                                                                             !2
                                  1 XXX             X
                    θb = arg min           yi,j,t −   θr gj (xi,t , β r )
                              θ N JT
                                            i=1 t=1 j=1             r=1
                            subject to (2) and (3).

As in the earlier examples, we could use normal distributions as basis functions in order to smooth
the estimates of the random coefficients. However, it is not desirable to use the location scale
model because modifying these parameters would require us to re-solve the model. The idea of
presolving a complex economic model for only R types before optimization commences is also
found in Ackerberg (2009), although Ackerberg’s approach is parametric instead of nonparametric
on the distribution of random coefficients.11
    It is often the case that the information on a panel can provide more information on het-
erogeneity than T repeated cross sections. We can explicitly incorporate this use of panel data
into our estimator. Let w index a sequence of choices for each time period t = 1, . . . , T called
w1 , . . . , wT . For example, a choice sequence w could be w1 = 5, w2 = 2, w3 = 3, . . . If there are
J choices per period, there are W = J T sequences that could occur. Let yi,w be equal to 1 if
agent i takes action sequence w over the T periods. This minimization problem when panel data
is used for extra information on heterogeneity is

                                 N W                         T R
                                                                                             !!2
                               1 XX                          Y X
                                                                          r            r
                  θb = arg min                      yi,w −               θ gwt (xi,t , β )
                            θ NW
                                          i=1 w=1            t=1   r=1
                          subject to (2) and (3).                                                          (8)

In this case, the estimator matches sequences of choices.

2.8.3    Joint Discrete and Continuous Demand as in Fox and Gandhi (2009b)

We can also estimate a model with a joint discrete and continuous choice using our methods.
Fox and Gandhi (2009b) introduce identification results for this type of model. Suppose that a
consumer purchases a particular type of air conditioner according to the logit model. Conditional
on purchase, we observe the electricity consumption of the air conditioner, a measure of usage.
Let the notation for the discrete choice be the same as before. The electricity usage equation of
  11
     Ackerberg cannot estimate his model using linear regression, although an advantage is the ease of allowing
homogeneous parameters that enter into a linear index that also has an additive, heterogeneous error.


                                                      15
consumer i of type r for air conditioner j is

                                                          0
                                                 ari,j = wi,j γjr + ηjr ,                                              (9)

where wi,j is a vector of observable characteristics that affect electricity demand. There can be
overlap between the elements of xi,j and wi,j . The parameter γjr is a potentially choice-specific
random coefficient vector for type r in the outcome equation. The scalar ηjr is a choice-specific
error term. Let wi = (wi,1 , . . . , wi,J ), γ = (γ1 , . . . , γJ ) and η = (η1 , . . . , ηJ ).
    Because the dependent variable includes
                                        h a continuous
                                                      element, we need to exploit the general
model in (1) and work with a set A = alj , al+1
                                            j         for the real-valued dependent variable. The
                                                                                         h     
researcher must discretize the continuous outcome variable aj by choosing L bins: a0j , a1j ,
h                   h          
  a1j , a2j , through aL−1
                       j   , aL . A higher L increases the computational burden and the closeness
                              j
of the approximation to the continuous outcome model.
         l be 1 when consumer i purchases air conditioner j and consumes electricity between
    Let yi,j
the lower and upper bounds alj and al+1
                                    j   . Then

                                         
       l
   Pr yi,j = 1 | xi , wi ; β r , γ r , η r = gj,l (xi , wi , β r , γ r , η r ) =
                                                                       
                                                     exp x0i,j β r                  h                              i
                                                                                       l     0    r     r      l+1
                                                                                1   aj ≤ w    γ
                                                                                             i,j j  + η i,j < aj     . (10)
                                             1 + Jj0 =1 exp x0i,j 0 β r
                                                   P


The unknown object of interest is the joint distribution F (β, γ, η) of the multinomial choice
random coefficients β, the electricity usage random coefficients γ, and the additive errors in utility
                                                                            r r r R
usage, η. In this case one can choose a grid of taste parameters     n {βo, γ , η }r=1 . A statistical
                                                                                                  
observation is (ji , ai,j , xi , wi ), which can be transformed into    l
                                                                       yi,j              , xi , wi by a
                                                                                          1≤j≤J,0≤l<L
change of variables. Data on ai,k for k 6= ji is not needed for this transformation. The estimate
of θ minimizes the objective function

                                N J L        R
                                                                                                    !2
                              1 XXX l        X
                 θb = arg min         yi,j −   θr gj,l (xi , wi , β r , γ r , η r )                                  (11)
                           θ NJ
                                           i=1 j=1 l=1             r=1
                          subject to (2) and (3),

                                     r ’s from (10). Estimating θ provides a nonparametric
where zi,j,l is the vector of the R zi,j,l



                                                           16
estimator of F (β, γ, η),

                                             R
                                             X
                            F̂ (β, γ, η) =         θ̂r 1 [β r ≤ β, γ r ≤ γ, η r ≤ η] .             (12)
                                             r=1

This distribution completely determines the model.
   This joint continuous and discrete demand model is an example of a selection model. In
this example, the continuous outcome ai,j is selected because the researcher only observes the
electricity usage for air conditioner j for those individuals who purchase air conditioner j, or
ji = j. Regressing ai,j on wi,j for those individuals who choose j will not consistently estimate
E [γj ] if γj is not statistically independent of β. Those agents who choose air conditioner j,
yi,j = 1, will have certain β preferences, and the correlation of β with γ and the correlation of x
with w will induce correlation between wi,j and γi in the sample of those who pick a particular air
conditioner j. However, jointly modeling both the choice of air conditioner and electricity usage
removes this selection problem. Note that we allow random coefficients in both the selection
(multinomial choice) and outcome (usage) parts of the model. Commonly, selection models
focus on allowing random coefficients in only the outcome equation. Again, see Fox and Gandhi
(2009b) for identification results for this type of selection model.

2.8.4    Endogenous Regressors as in Fox and Gandhi (2009a)

In some cases, the regressors may be correlated with omitted factors. Consider the multinomial
choice model. Fox and Gandhi (2009a) address the correlation of xi,j with βi using instrumental
variables and an auxiliary equation, in which the values of the endogenous regressors are given as
a function of exogenous regressors and instruments. For example, if price ai,j is an endogenous
regressor and wi,j are the instruments for price, then the auxiliary equation for type r is ari,j =
 0 γ r +η r , which is the same notationally as (9). The difference here is that price a
wi,j j   j                                                                               i,j is observed
for all J choices for each agent. There is no selection problem. Instead, there is a traditional
omitted variables problem where ai,j enters the logit demand model as a component of xi,j and
the random variable ai,j is not independent of βi , the vector of random coefficients in the logit
model.
   Our estimator can be applied to the model where Fox and Gandhi (2009a) study identification.
Estimation works with the reduced form of the model. Let xi,j,1 = ai,j , or the endogenous
regressor price is the first characteristic in the vector of product characteristics xi,j . Let β1
reflect the random coefficient on price in the discrete choice utility. Let β̃i be the K − 1 other,


                                                        17
non-price random coefficients and let x̃i,j be the K − 1 other, non-price product characteristics
for product j. We reuse a lot of the notation from the example of mixed discrete and continuous
choice above. With the borrowed notation,
                                        
      l
  Pr yi,j = 1 | xi , wi ; β r , γ r , η r = gj,l (xi , wi , β r , γ r , η r ) =
                                                                  
                         exp x̃0i,j β̃ r + β1r wi,j0 γ r + ηr
                                                         j        j                  h                              i
                                                                                        l     0    r     r      l+1
                                                                               1   aj ≤ w    γ
                                                                                              i,j j  + η i,j < aj     . (13)
                1 + Jj0 =1 exp x̃0i,j 0 β̃ r + β1r wi,j     0 γ r + ηr
                      P
                                                              0 j0          j 0



The key idea here is that we work with the reduced form of the model: we replace the actual J
                                                     0 γ r +η r from the auxiliary pricing equation.
prices in the data ai,j with the J predicted prices wi,j j   i,j
The actual data ai,j that are allowed to be statistically dependent with βi do not appear in (13).
Only data on x̃i,j , the exogenous regressors, and wi,j , the instruments       that enter the pricing
                                                         0               0
equation, are used. We assume (x̃i , wi ), where x̃i = x̃i,1 , . . . , x̃i,J and wi = (wi,1 , . . . , wi,J ), is
independent of (βi , γi , ηi ), the heterogeneity realizations.
    Estimation proceeds as in (11), except gj,l (xi , wi , β r , γ r , η r ) is now defined by (13). Like in
the selection example, the omitted variables example has as its object of interest F (β, γ, η). The
estimator of F (β, γ, η) is (12), with the estimates θ̂ coming from the omitted variable bias and
not the selection model. Note that the framework of Fox and Gandhi (2009a) allows random
coefficients both in the pricing and discrete choice decisions, and those random coefficients have
an unrestricted joint distribution F (β, γ, η).

2.8.5    Endogenous Regressors as in Berry and Haile (2008)

Berry and Haile (2008) investigate a multinomial choice model where the utility to each choice j in
market t includes an additive ξj,t term, perhaps reflecting an unobserved product characteristic.
The endogeneity problem arises from the correlation of price with ξj,t . The authors use a “special
regressor” xi,j,K that varies within markets, while the endogenous regressor price varies at the
market level. The coefficient on the special regressor xi,j,K has the same sign for all consumers.
    Berry and Haile propose a three-step identification strategy. In step 1, the special regressor
is used to trace out the marginal CDF for the utility for each of the J choices in each market.
The key idea is that the random disturbance ξj,t that causes the endogeneity problem is the
same for all agents in the same market, while the special regressor varies across agents in the
same market. With this step, the authors can compute the mean payoff across consumers in
the same market for each choice. In step 2, the mean utility of each choice in each market is

                                                           18
nonparametrically regressed on product characteristics, using instruments. An observation in
this second stage is a product in a particular market, not an individual consumer. The error
term in the nonparametric regression is ξj,t . As the mean utility value is a continuous variable,
the value of ξj,t is recovered. In the third stage, ξj,t is an included product characteristic and
normal demand estimation can commence. In other words, ξj,t can be added to the vector xj,t
and our standard discrete-choice estimator can be used.

2.8.6   Endogenous Regressors as in Petrin and Train (2008)

We mentioned Petrin and Train (2008) above in the discussion of aggregate data. Their method
can also be used with individual data. Like Fox and Gandhi (2009a), they specify an auxiliary
pricing equation. However, there are no random coefficients in the pricing equation (γi = γ ∀ i)
                                                   0 γ +η
and the error term in the pricing equation ai,j = wi,j    i,j is labeled as an omitted product
characteristic. In a first stage, an OLS regression of a on the instrument vector w produces
consistent estimates of the error terms ηi,j . These error terms ηi,j are then treated as observed
and then included in xi,j , the vector of product characteristic. All endogeneity problems are
assumed to arise from not observing ηi,j , so measuring it in a first stage and including it as an
observed product characteristic solves the endogeneity problem.
    The Petrin and Train (2008) approach is computationally simple and indeed the model is a
special case of the model in Fox and Gandhi (2009a), as it does not allow random coefficients
in the first stage. Both methods require the assumption of an auxiliary equation that gives the
value of the endogenous regressors as a function of instruments. Berry and Haile (2008) avoid
using an auxiliary equation by assuming that the endogeneity problem arises from a variable that
is constant for all agents within the same market. Berry and Haile can trace out the marginal
utility of each choice, holding the variable causing endogeneity problems constant, using a special
regressor that varies across agents within the same market.


3    Consistency for the Distribution in Function Space
In this section, we assume that the true distribution function F0 lies in the space F of distribution
functions on a parameter space B. Let R (N ) be the number of grid points chosen with N obser-
                                                                            PR(N )
vations. We wish to show that the estimated distribution function F̂N (β) = r=1 θ̂r 1 [β r ≤ β]
converges to the true F0 ∈ F. To prove consistency, we use the recent results for sieve estimators
developed by Chen and Pouzo (2009a), hereafter referred to as CP. We view each set of R points


                                                 19
as a sieve space

                                                          R                           R
                      (                                                                                 )
                                                          X                           X
               FR =     F | ∃ θ1 , . . . , θR , θr ≥ 0,         θr = 1 s.t. F (β) =         θr 1 [β r ≤ β] ,
                                                          r=1                         r=1


for some fixed choice of grid B R = β 1 , . . . , β R . We require B R ⊆ B S for S > R. Note that
                                   

this sieve space automatically imposes that the estimated function is a cumulative distribution
function. CP suggest other bases for unknown functions; we prefer our choice of sieve spaces in
the interests of computational simplicity and the ease of constraining the estimated function to
be a probability distribution function over Borel measurable sets.
      Based on CP, one can prove that the estimator F̂N converges to the true F0 in function space,
i.e. in the space of distributions. CP study sieve minimum distance estimators that involve a
two-stage procedure. Our estimator is a one-stage sieve least squares estimator (Chen, 2007)
and we show its consistency based on CP’s general consistency theorem, their Lemma B.1.
      Let y be the dependent variable, which represents some discrete outcome. We suppress the
notation A for the outcome. We define our sample criterion function as

                                         N      R
                                                               !2
                                       1 X      X
                                                   r         r
                            Q̂N (F ) ≡     yi −   θ g (xi , β ) , F ∈ FR .
                                       N
                                             i=1            r=1

We minimize over the unknown weights θr and subject to the constraints θr ≥ 0 ∀ r and
PR(N ) r
 r=1 θ = 1. One can add to the least squares objective function a function that penal-
izes values of F that are less smooth, but we prove consistency using only the properties of the
sieve space, not the penalization function. See CP for more on optional penalization functions.
      We must choose a metric to show the consistency of F̂N for F0 . We choose the Lévy-Prokhorov
metric, denoted by dLP (·), a metrization of the weak topology for the space of multivariate
distributions. The Lévy-Prokhorov metric in the space of F is defined on a metric space (B, d).
      We say two distributions F1 and F2 are distinguishable in terms of the Lévy-Prokhorov
metric if dLP (F1 , F2 ) = 0 implies F1 = F2 .12 The space F of distributions on B is compact in
the weak topology if B itself is compact (Parthasarathy 1967, Theorem 6.4). We do not formally
use compactness of the true function space F in our proof of consistency. It is possible that
compactness rules out the ill-posed inverse problem that CP investigate and may be present in
the heterogeneity problem under other metrics. We require that the grid of points be chosen so
 12
      See Zolotarev (2001) for a formal definition of the Lévy-Prokhorov metric.



                                                                20
that the grid B R(N ) becomes dense in B. The theorem is one of consistency and so does not offer
any additional insight on how choices of grids relate to statistical performance.

Theorem 3.1

   • Let F be the space of all distribution functions on a finite-dimensional real space B, where
      B is compact. Further let F ⊆ F be the space of distributions distinguishable in terms of
      the the Lévy-Prokhorov metric. We assume the true F0 ∈ F.
                                                       R(N ) log R(N )
   • Let B R(N ) become dense in B as N → ∞ where           √
                                                               N
                                                                         → 0. Let {yi , xi }N
                                                                                            i=1 be i.i.d.

   • Let the space of x, X , be compact and let any open ball in X have positive measure.

   • Let each g (x, β) be continuous and bounded in both x and β.

   • Assume the model {g (x, β) | β ∈ B} is identified with positive probability, meaning F0 and
      any F1 6= F0 , F1 ∈ F give P0 (x) 6= P1 (x) for x ∈ X̃ , where X̃ ⊆ X is a set with positive
      probability.
                              h                    0
                                                       i
   • The R(N ) × R(N ) matrix E g (X, β r ) g X, β r                         is positive definite and its
                                                             1≤r,r0 ≤R(N )
     smallest eigenvalue is bounded away from zero uniformly in R(N ).
                 
Then dLP F̂N , F0 −→ oP (1), where dLP (·) represents the Lévy-Prokhorov metric.

3.1   Comments on Identification
We assume that the model is identified in order to focus on this paper’s contribution to esti-
mation. In Bajari, Fox, Kim and Ryan (2009), we prove that our lead example, the random
coefficients logit, is nonparametrically identified. Our proof relies on the linear index x0 β and
the differentiability of g (x, β) in β. Our approach can be adapted to other economic models
satisfying these restrictions.
   Fox and Gandhi (2009a, 2009b) provide a general framework for proving identification in
a class of economic models. They develop conditions on the behavior of the heterogeneous
economic agents at different choice situations. These conditions are sufficient but not necessary
for identification. The conditions are easier to verify than the “linear independence” condition
emphasized in the statistics literature (Teicher 1963). Indeed, stating that a model is “linearly
independent” is only a small step from stating that the model is identified.



                                               21
    We have discussed Berry and Haile’s (2008) contribution to discrete choice analysis with un-
observed product characteristics previously. Ichimura and Thompson (1997) study binary choice
with exogenous characteristics and use the Cramér-Wold (1936) theorem for identification. Gau-
tier and Kitamura (2008) present alternative arguments that give equivalent results. Hoderlein,
Klemelä, and Mammen (2008) study the identification and estimation of the density of random
coefficients in the linear regression model. They allow endogenous regressors using auxiliary
equations (without random coefficients) for the endogenous regressors. Chiappori and Komunjer
(2009) recently investigate identification in the multinomial choice model.


4     Asymptotic Bounds
We derive asymptotic bounds both for the approximation of conditional choice probabilities,
Pr (yi,j = 1 | xi ), and for the approximation of the underlying distribution of random coefficients
F (β) obtained from the estimation of θ in Section 2.
   Here we explicitly account for a number of J product choices or choice probabilities. To
                                  0 , . . . , X0
simplify our notation, let Xi = X1,i           J,i be the random variable that is the characteristics
of all J choices and denote by X ≡ X1 × . . . × XJ the support of the distribution of Xi . Here
we focus on the logit but the asymptotic theory we develop in this section can be applied to
other cases as long as equivalents of Assumption 4.1 and 4.2 below hold. Our results in this
section relate to those in Newey (1997) and Chen (2007), although the Riemann and quadrature
arguments are more novel.
    We maintain the assumption of the nonparametric random utility model with logit errors and
consider only distributions with locally integrable densities. The conditional choice probability
for alternative j is
                                                       
                        Z         exp x0j,i β
                                                          f (β)dβ for any xi = x01,i , . . . , x0J,i ∈ X ,
                                                                                                      
    P (j | xi , f ) =       
                                 PJ                                                                           (14)
                              1 + j 0 =1 exp x0j 0 ,i β

where f is a density of the random coefficients and the true conditional choice probability is de-
noted by P (j | xi , f0 ). We further let the true choice probabilities in the data be Pr (yi,j = 1 | xi ) ≡
P (j | xi ) = P (j | xi , f0 ).
    Then, we have

                                yi,j = P (j | xi ) + ej,i and E [ej,i | X1 , . . . , XN ] = 0


                                                             22
by definition of a choice probability for individual choices. For aggregate data such as market
shares, we assume

                  ŝj,t = P (j | xt ) + ej,t and E [ej,t = ŝj,t − sj,t | X1 , . . . , XT ] = 0

where sj,t = P (j | xt ) and xt = (x01,t , ..., x0J,t ), so ej,t is a pure measurement error as in Section
2.8.1. Here we focus on individual choice problems and the asymptotics we develop in this section
also hold for the aggregate data case with slight modification.
   We introduce additional simplifying notation. Define a simplex
                                                                              
                                                                R(N )       
                                                                   X
                      ∆R(N )   = θ = θ1 , . . . , θR(N ) | θr ≥ 0,       θr = 1 .
                                                                              
                                                                             r=1

                                        PR(N )
Recall that we require θr ≥ 0 and       θr = 1 as the θr ’s are type frequencies in our approxi-
                                           r=1
                                        PR r                                        exp(x0j,i β r )
mation. We approximate P (j | xi ) using r=1 θ gj (xi , β r ) and gj (xi , β r ) ≡ PJ        “
                                                                                                 0  r
                                                                                                      ”.
                                                                                               1+   j 0 =1   exp xj 0 ,i β
Let the approximation to choice probabilities be

                                             R(N )
                                             X
                             µθ (j, xi ) =           θr gj (xi , β r ) for θ ∈ ∆R(N ) .
                                             r=1

The estimated weights are

                                                 N J
                                               1 XX
                          θb = argminθ∈∆R(N )        (yi,j − µθ (j, xi ))2 .                                            (15)
                                              NJ
                                                          i=1 j=1


   Let K be the number of random coefficients. Then we define the parameter space for choice
j as the collection of market shares or conditional choice probabilities,

                                                                                      K h
         (                                                   Z                                                               )
                                                                                      Y                i
H(j) =    P (j | xi , f ) | M ≥ max sup |Ds f (β)| ,               f (β)dβ = 1, B =         β (k) , β (k) , f ∈ C s [B] ,
                                s=0,...,s β∈B                  B                      k=1

                    ∂s
where Ds =      α1
              ∂β(1) ...∂β(K)
                                                        0
                         αK , s = α1 + · · · + αK with D f = f , giving the collection of all                derivatives
of order s.   Also, C s [B] is a space of s-times continuously differentiable functions                      defined on
B. We assume P (j | xi , f0 ) ∈     H(j)   for j = 1, . . . , J. Therefore we assume any element of the
class of density functions that generates H(j) is defined on a Cartesian product B, is uniformly


                                                          23
bounded by M , is s-times continuously differentiable, and its (all own and partial) derivatives
are uniformly bounded by M . We assume that the true f0 is in each H(j) , that there exist the
scalars M and s, and that s̄ is indeed an integer. The definition of H(j) depends on M and s̄; s̄
will show up in our bounds.
    We define the space of approximating functions to choice probabilities,

                                        (j)    
                                       HR(N ) = µθ (j, xi ) | θ ∈ ∆R(N ) ,                                                 (16)

with R(N ) tending to infinity as N → ∞. In order to show that any P (j | xi , f ) ∈ H(j) can be
                                           (j)
approximated by an element of HR(N ) , we can put an additional structure on θ such that

                                              c(β r )f (β r )
                                      θ r = PR                  , r = 1, . . . , R,
                                                      r       r
                                             r=1 c(β )f (β )

for some chosen weights c(β r )’s and where we pick the R grid points {β 1 , . . . , β R } to become
dense in B as R (N ) → ∞. We also require that the grid points accumulate: the set of R (N )
                                                                                                                 (j)
grid points contains the R (N − 1) grid points. Then, formally, we have H(j) = H∞ since any
               (j)
element in HR(N ) is a corresponding Riemann sum (where we let c(β r ) = 1/R) or more generally
a quadrature approximation for an integral like (14) such that

                             R                                                R
                             X                                       1        X
             µθ (j, xi ) =         θr gj (xi , β r ) = PR                           c(β r )f (β r )gj (xi , β r ) ,
                                                                   r     r
                             r=1                            r=1 c(β )f (β )   r=1

                             PR          r )f (β r )
                                                                                    PR           r )f (β r )g    (xi , β r ) −→
                                                                R
and where we expect            r=1 c(β                 −→            f (β)dβ = 1,      r=1 c(β               j
                                                       R→∞                                                                R→∞
P (j | xi , f ), and so µθ (j, xi ) −→ P (j            | xi , f ).   We, therefore, can approximate the observed
                                       R→∞
choice probabilities P (j | xi ) arbitrarily well using the logit functions gj (xi , β r ).
    In what follows, even though our notation is specific to the logit example, we develop the
asymptotic theory for general basis functions that satisfy a set of conditions. We let khkL2,N =
q P                          qR
   1  N     2 (x ), khk            2
  N   i=1 h     i       L2 =    X h (x)d$(x) (the norm in L2 ), and khk∞ = supx∈X |h(x)| for
any function h : X → R, where $(·) denotes the distribution of X.

Assumption 4.1 (i) {ej,i , . . . , eJ,i } are independent across i = 1, . . . , N ; (ii) E [ej,i | X1 , . . . , XN ] =
0 for all j = 1, . . . , J; (iii) {X1,i , . . . , XJ,i } are i.i.d. across i = 1, . . . , N ; (iv) X is compact; (v)
The density of X is bounded above and is bounded away from zero on X .



                                                               24
Assumption 4.2 (i) kP (j | xi , f0 )k∞ ≤ ζ0 and kgj (xi , β r )k∞ ≤ ζ0 for some constant ζ0 > 0
uniformly over r ≤ R(N ) and for all j; (ii) kgj (xi , β r )kL2 ≥ c0 > 0 uniformly over r ≤ R(N )
                                              h                          0
                                                                             i
and for all j; (iii) the R(N ) × R(N ) matrix E gj (Xi , β r ) gj Xi , β r       0
                                                                                        is positive
                                                                                           1≤r,r ≤R(N )
definite and its smallest eigenvalue is bounded away from zero uniformly in R(N ) and for all j.

    Assumption 4.1 is about the structure of the data. In Assumption 4.1 (i), we allow for
heteroskedasticity of {ej,i , . . . , eJ,i }, across different i’s. For the logit case, we satisfy Assumption
4.2 (i) trivially since
                                                  
                                       exp x0j β r
                  gj (xi , β r ) =                          ≤ 1 uniformly over β and x
                                   1 + Jj0 =1 exp x0j 0 β r
                                      P


and P (j | xi , f0 ) ≤ 1 uniformly by construction. In the assumption, the same constant ζ0
applies for all sample sizes N .          We also satisfy Assumption 4.2 (ii) trivially unless X has
no
 positive  mass. In practice,
                                  0
                                    Assumption 4.2 (iii) requires that the (R(N ) × R(N )) matrix
  1 PN             r            r
 N   i=1 gj (xi , β ) gj xi , β           0
                                                is nonsingular for sufficiently large N .
                                        1≤r,r ≤R(N )
    Under these regularity conditions, the bound of the approximation to choice probabilities is
obtained by the following theorem.

                                                                                             R(N )2 log R(N )
Theorem 4.1 Suppose Assumptions 4.1 and 4.2 hold. Further suppose                                   N           → 0.
Then, we have

      1 XJ                                      2
                µθb (j, xi ) − P (j | xi , f0 ) L
      J    j=1
                                            
                                                  2,N
                                                                                                              
                                
           R(N ) log (R(N )N )                                  1 X J
    = OP                            + OP  inf                          kµθ (j, xi ) − P (j | xi , f0 )k2L2,N  .
                    N                          µθ ∈H
                                                     (j)        J   j=1
                                                        R(N )



                     R(N )2 log R(N )
    The condition           N           → 0 is required so that the (R(N ) × R(N )) matrix

                                    N
                                                                       !
                                  1 X                           0
                                                                   
                                      gj (xi , β r ) gj xi , β r
                                  N
                                        i=1                                1≤r,r0 ≤R(N )


is nonsingular with sufficiently large N (see Lemma B.3 in the Appendix). Roughly speak-
ing, in view of asymptotic theory for sieve estimators, Theorem 4.1 tells us that the first term
corresponds to an asymptotic variance (≈ (R(N )/N ) and the second term corresponds to an

                                                        25
asymptotic bias term (Chen 2007). To derive convergence rates, we need to obtain the order of
bias, i.e., the approximation error rate of our sieve approximation to arbitrary conditional choice
probabilities.

Lemma 4.1 For all x ∈ X and for any P (j | xi , f ) ∈ H(j) , there exist θ∗ ∈ ∆R such that
                                                                                  
                                                                            −s/K
                                   |µ (j, x) − P (j | x, f )| = O R
                                     θ∗

               PJ
           1
                            (j, xi ) − P (j | xi , f )k2L2,N = OP R−2s/K .
                                                                        
and that   J     j=1 kµθ∗

   The approximation error rate in Lemma 4.1 shows that we have faster convergence with a
smoother true density function and slower convergence with a higher dimensional β.
   Now we consider approximating the true conditional choice probability function. From The-
                                                                                            (j)
orem 4.1 and Lemma 4.1, we can find θ0∗ ∈ ∆R(N ) (i.e. µθ0∗ (j, ·) ∈ HR(N ) ) and R(N ) such
that
                                                                                                              
   1 XJ                                         2
                                                         
                                                               −2s/K
                                                                                     R(N ) log (R(N )N )
          µθ0∗ (j, xi ) − P (j | xi , f0 )      L2N
                                                      = OP R                OP                                    .   (17)
   J  j=1                                                                                      N

With this choice of R(N ), one will obtain the optimal convergence rate that balances the bias
and variance of the sieve estimator. We conclude:
                                                                                            R(N )2 log R(N )
Theorem 4.2 Suppose Assumptions 4.1 and 4.2 hold. Further suppose                                  N           → 0 and
s > K/2. Then, we have
                                                                                              
           1 XJ                                   2                        R(N ) log (R(N )N )
                  µθb (j, xi ) − P (j | xi , f0 ) L2         = OP                                and
           J  j=1                                   N                               N
                                                                                r                !
                                XR(N )                                             log (R(N )N )
                                         θbr − θ0∗r          = OP          R(N )                   .
                                          r=1                                            N

   One can let R(N ) = C · N ρ . Combining the rate conditions on R(N ), we note that ρ should
                              K                                                           R(N )2 log R(N )
be slightly larger than     2s+K   (from (17)) and be smaller than 1/2 (from                     N           → 0). Also
note that s > K/2 is required to have such a ρ exist.
    Theorem 4.2 establishes the bounds of the distance between the true conditional choice prob-
ability and the approximated conditional choice probability as well as the L1 distance between θb
and θ0∗ . The L1 metric for the discrepancy between θb and θ0∗ is reasonable because θb and θ0∗ are
frequency parameters (Devroye and Gyorfi 1985, Chapter I). From these bounds, the convergence
rates are trivially obtained.

                                                        26
                                                                                           0
   We can also estimate the distribution function using our estimates θb = θb1 , . . . , θbR . Denote
by F0 (β) the true distribution of β. Also let B0 be the support (or compact subset of the support)
of F0 (β). One can estimate the distribution of the random coefficients using the following
empirical distribution based on our estimates
                                                XR(N )
                                   FbN (β) =             θbr 1 [β r ≤ β] .
                                                   r=1


We show that FbN (β) can approximate the true distribution function F0 (β). The approximation
rate holds pointwise in β rather than in a function space.

Theorem 4.3 Suppose Assumptions in Theorem 4.2 hold. Then,
                                            r                  !
                                                log(R(N )N )                   
         FbN (β) − F0 (β) = OP      R(N )                          + O R(N )−s/K a.e. β ∈ B0 .
                                                     N



   The bound is divided into a rate involving the statistical sampling error and one involving the
approximation of a true distribution by a finite number of grid points. Again, from this bound,
we have faster convergence with a smoother true density function and slower convergence with
a higher dimensional β.

4.1    Breaking the Dimensionality Problem using Random Grids
We have obtained the approximation error (bias term) in Lemma 4.1 by exploiting the smoothness
of the density function and the logit function where the degree of smoothness breaks the curse
of dimensionality. Similarly to the arguments of Rust (1997) for dynamic programming, we can
also handle the dimensionality problem using a random grids approach. Suppose the R number
of grid points are drawn independently from the multivariate uniform distribution such that
                                                              
                                    r
                                   β(k) = β (k) + β (k) − β (k) ur(k)

for k = 1, . . . , K where ur = (ur(1) , . . . , ur(K) )0 follows the multivariate uniform distribution on
[0, 1]K . Further let
                                              (1/R)f (β r )
                                       θ∗r = PR                .                                     (18)
                                                  (1/R)f (β r)
                                              r=1




                                                    27
Then we have
                                R                                                                 R
                                X
                                       ∗r                r                    1             1 X
               µ (j, xi ) =
                θ∗                    θ gj (xi , β ) =                       PR                 f (β r )gj (xi , β r ) .     (19)
                                                                     (1/R)               r
                                                                                 r=1 f (β )
                                                                                            R
                                r=1                                                            r=1

Note that applying the law of large numbers,
                                                                                                                  
   R
 1 X
                                            Z                                         exp x0j,i β + (β − β)u
     f (β r )gj (xi , β r )
                                                                       
                                 −→                      f β + (β − β)u                                               du
 R                              R→∞             [0,1]K                           1 + Jj0 =1 exp x0j 0 ,i β + (β − β)u
                                                                                     P
    r=1
                                                                                                   
                                                             1
                                                                          Z             exp x0j,i β
                                  =         K 
                                                                             f (β)                           dβ,
                                                                                   1 + Jj0 =1 exp x0j 0 ,i β
                                                                                     P
                                            Q                              B
                                                     β (k) − β (k)
                                            k=1

where the equality holds by applying the change of variables in the integral. Similarly we have

                        R                                                        Z
                        X
                                  r                              1                                         1
               (1/R)          f (β ) −→           K 
                                                                                      f (β)dβ =   K                    .
                                      R→∞         Q                          
                                                                                  B               Q
                        r=1                                  β (k) − β (k)                              β (k) − β (k)
                                                  k=1                                             k=1

Applying above two results to (19), we conclude
                                                                   
                                            Z           exp x0j,i β
                     µθ∗ (j, xi ) −→          f (β)                           dβ = P (j | xi , f ) ,
                                 R→∞
                                                    1 + Jj0 =1 exp x0j 0 ,i β
                                                       P
                                            B


where we pick θ∗ as in (18). Finally we obtain the approximation error rate (bias term), applying
the Lindeberg-Levy central limit theorem and the law of large numbers.

Lemma 4.2 For all x ∈ X and for any P (j | x, f ) ∈ H(j) , with θ∗ as in (18), we have that
                                                                            
                                      |µθ∗ (j, x) − P (j | x, f )| = OP R−1/2 .

               PJ
           1
                              (j, x) − P (j | x, f )k2L2,N = OP R−1 .
                                                                   
and that   J     j=1 kµθ∗


    The key conclusion is that these rates do not depend on K, the number of random coefficients.
We can replace Lemma 4.1 with Lemma 4.2 and obtain alternative bounds for the conditional
choice probability and the distribution of random coefficients.

                                                                       28
5        Estimating Confidence Regions with Finite Types
The literature on sieve estimation (Chen and Pouzo 2009a) does not have asymptotic distribution
theory for the infinite-dimensional unknown parameter, F (β). Given the state of the literature,
we focus on computing standard errors using a parametric method, which requires the assumption
that the set of R types used in estimation is the true set of R types that generates the data. Under
this assumption, one can use the common OLS heteroskedasticity-consistent standard errors for
the unknown weights, θ1 , . . . , θR . The traditional OLS confidence intervals are computed using
the unconstrained point estimates in section 2.3 instead of the point estimates with constraints
from section 2.4. However, the confidence regions so constructed give correct coverage (more
than 95%) for both the estimates with and without the constraints (Andrews and Guggenberger
2009b, footnote 7). We use the common standard error formulas in our empirical example, below.
    One needs to use heteroskedasticity-consistent standard errors because the errors in a linear
probability model such as (6) are heteroskedastic. One also should cluster the standard errors at
the level of the “regression observations” j = 1, . . . , J for each statistical observation i = 1, . . . , N .
Recall that for each i, there are J “regression observations” in (6) because each inside good j has
a separate term in the least-squares objective function. After constructing the OLS confidence
intervals for θ̂r , one should remove infeasible values by reporting, for a 95%, two-sided confidence
interval,                         h                                    i
                          [0, 1] ∩ θ̂r − 1.96 · SE θ̂r , θ̂r + 1.96 · SE θ̂r ,
         
where SE θ̂r is the standard error adjusted for heteroskedasticity and clustered across the J
“regression observations” for each statistical observation i.
    Researchers are often not directly interested in confidence intervals for the weights θ but
rather for functions m (θ; X), where X is some arbitrary data. For     example,
                                                                             Presearchers may
wish to construct confidence intervals for the distribution F̂ (β) = m θ̂; X = R       r    r
                                                                                 r=1 θ̂ 1 [β ≤ β]

       ata particular value of β (X does not enter m here). To construct standard errors
evaluated
for m θ̂; X , one first constructs the distribution of θbr − θ0r as above and then uses the delta
method. A 95% confidence interval is then
                                   
        min m (θ; X) , max m (θ; X) ∩
         θ              θ
                          h                                                i
                           m θ̂; X − 1.96 · SE m θ̂; X , m θ̂; X + 1.96 · SE m θ̂; X     .

Here the minimum and maximum are taking over the values of θ that satisfy (2) and (3). This is a


                                                      29
compact set, so the minimum and maximum are obtained. In many examples, it will be possible
to deduce the feasible upper and lower bounds for m (θ; X) without resorting to computation.
    The common heteroskedasticity-consistent standard errors give more than 95% coverage but,
based on our Monte Carlo evidence, are often quite conservative in that the coverage is much
more than 95%.13 In empirical work, we often find that many of the included R types have
estimated weights of θ̂r = 0. Thus, in principle one can construct less conservative confidence
intervals by recognizing that the parameters on the boundary of the parameter space cannot
have an asymptotically normal distribution.14 While Andrews (1999, 2002) and Andrews and
Guggenberger (2009b) study related cases, the reality is that this recent literature has not devel-
oped general-enough results that could allow us to estimate confidence intervals for our problem
in a way that gives asymptotically correct coverage as defined by Andrews and Guggenberger.
Indeed, Andrews and Guggenberger study only the case of a regression with one inequality con-
straint and i.i.d. observations. Traditional OLS confidence intervals using fixed-critical values
and based on the point estimates imposing the one constraint are recommended by those authors,
but there is no suggestion that traditional OLS methods with fixed critical values and based on
the point estimates imposing the constraints give asymptotically correct coverage if there are
two or more inequality constraints, as in our estimator.
    Resampling procedures are a possibility, but one that Andrews and Guggenberger do not
currently recommend when a true parameter may lie on a boundary. Andrews (2000) shows that
the standard bootstrap is inconsistent but that subsampling and the m-out-of-n bootstrap are
pointwise consistent. Andrews and Guggenberger (2010) show that the latter two resampling
procedures are not uniformly consistent and may have poor finite-sample coverage. Andrews
and Guggenberger (2009a, 2009b) discuss a hybrid method where the maximum of a traditional
critical value for a t-statistic and a subsampled critical value is used to construct confidence
intervals for θ. The hybrid subsampling method has correct asymptotic coverage under the
definition of Andrews and Guggenberger for the special case of one inequality constraint, but,
as Andrews and Guggenberger (2009b) show, so does one of its ingredients, the traditional fixed
critical value method that Andrews and Guggenberger recommend. Subsampling by itself does
not have correct asymptotic coverage.15
  13
     We performed Monte Carlo studies of using Tikhonov regularization / ridge regression (and Golub, Heath
and Wahba’s (1979) generalized cross validation method to pick the perturbation value) to compute standard
errors. Tikhonov regularization reduced the size of the confidence regions some but the coverage was still much
more than 95%.
  14
     Statistical inference for linear regression subject to a set of inequality constraints has been studied by Judge
and Takayama (1966), Liew (1976), Geweke (1986), and Wolak (1987).
  15
     In an unreported Monte Carlo study, we also found that subsampling could undercover the true parameters:


                                                         30
    In an appendix available upon request, we show consistency and derive the sampling distri-
bution of the inequality-constrained nonlinear least squares estimator of the location and scale
model. The nonlinear distribution also applies if some parameters in the model are homogeneous.
As the sampling distribution is derived, we have verified the only regularity condition needed for
the pointwise consistency of subsampling (Politis, Romano and Wolf 1999).


6    Monte Carlo Experiments
We conduct a Monte Carlo experiment in order to study the finite sample properties of our
estimator. We suppose that the true data generating process is indeed a random coefficients
logit. In our Monte Carlo study, xj is a 2 × 1 vector. We generate xj,1 using the distribution
xj,1 ∼ N (1, 1). Also, xj,2 ∼ N −0.8, 0.82 + 0.1xj,1 . There are J = 10 products in each of our
                                          

markets. J does not include the outside option. We use independent normal basis functions
in order to approximate a density function rather than a CDF, as section 2.5 discusses. The
sample size is J · T , the number of products times the number of markets. We use a sample size
of 20,000, corresponding to 2000 markets with 10 products each. The number of basis points
         T ·J
is R =    40    = 500. There is little measurement error in shares; we calculate market shares for
aggregate data generated by 1 million consumers.16
    Rather than a criterion such as integrated mean squared error, we prefer to test the structural
use of our estimates. For each run, after we compute the estimate fˆ (β), we evaluate its predictive
power by drawing new product characteristics (from the same distributions) and predicting
shares. We compare our results to those using the true f 0 (β). This tests the structural use of
discrete choice models to predict the demand for new goods.
    We generate data using three alternative distributions of the random coefficients f (β). In
the first design, the tastes for characteristics are generated from a mixture of two normals,
                          "        # "                 #!               "        # "             #!
                              3          0.2    −0.1                        −1         0.3 0.1
                0.4 · N             ,                       + 0.6 · N             ,                   .
                              −1         −0.1   0.4                         1          0.1 0.3

All distributions of random coefficients will have a non-trivial variance matrix, while our basis
functions are independent distributions of normal random coefficients. In the second design, the
it has coverage less than 95%.
   16
      We choose the grid of points using a Halton sequence over the relevant support. We compute numerical
integrals using 200 simulated draws.




                                                        31
true coefficients are generated by a mixture of four normals,
                         "          # "                     #!              "           # "                    #!
                                3             0.2    −0.1                           0            0.2    −0.1
              0.2 · N                   ,            + 0.4 · N                              ,
                                0      −0.1 0.4                                     3    −0.1 0.4
                                "      # "         #!                           "      # "         #!
                                    1      0.3 0.1                                  −1     0.3 0.1
              + 0.3 · N                 ,             + 0.1 · N                         ,             .
                                    −1     0.1 0.3                                   1     0.1 0.3

In the third design and final design, the true coefficients are generated by the mixture of six
normals,
                        "       # "                        #!               "       # "                       #!
                            3                0.2    −0.1                        0               0.2    −0.1
            0.1 · N                 ,                           + 0.2 · N               ,
                            0               −0.1    0.4              −0.1 0.4   3
                            "               # "                  #!   # "         #!"
                               1        0.2 −0.1                  −1      0.3 0.1
             + 0.2 · N             ,                 + 0.1 · N         ,
                              −1       −0.1 0.4                    1      0.1 0.3
                             " # "           #!            " # "            #!
                              2      0.3 0.1                  1     0.3 0.1
             + 0.3 · N           ,              + 0.1 · N       ,              .
                              1      0.1 0.3                  2     0.1 0.3

    We summarize our results in Table 1. The first column states the distribution used to generate
the random coefficients. The second column is the root mean squared error (RMSE) of our out
of sample prediction for market shares. The RMSE is averaged over many new, counterfactual
sets of products. It is not averaged over the statistical sampling error in the original estimator;
we estimate for each design only once. The final column is the number of basis functions that
have positive weight.
    The results suggest that our estimator of f (β) is excellent for predicting the market shares
of new goods, with large samples.17 To understand the units, with J = 10 and a mean share
of 0.10, a prediction error of 0.01 (10% of 0.1) on each product corresponds to a RMSE of
p
  10 · 0.012 /10 = 0.01. Observed prediction errors are small with this sample size: 0.0001. Note
that we are able to fit the observed market shares well with a fairly small number of basis func-
tions. The mean number of nonzero basis functions ranges from 32 to 99 in the results reported
below. More complex true densities require more nonzero basis points for approximation. Even
when we make R large, most of these basis functions will have zero probability. This result
is consistent with the literature on mixtures, which demonstrates that even quite complicated
  17
     Although not shown, our estimator is quite good at fitting market share equations with much smaller samples.
Larger samples are needed for excellent performance in density estimation, which is not surprising given the curse
of dimensionality in estimating an infinite-dimensional object.


                                                                 32
distributions can be approximated with a surprisingly small number of mixture components.
    We perform 20 replications with different datasets in order to simulate the finite sample
properties of our estimator. We use the 20 replications to construct the finite-sample properties
of the estimator of the marginal densities of both β1 and β2 . Figure 1 includes one plot for each
of the three designs and each of the two random coefficients. The solid line is the true density as
recorded above, while the dashed line is the fitted density for one of the twenty replications. We
use the 20 replications to construct the 5th and 95th quantiles of the estimated densities at each
point of evaluation. We see that the variability across replications is quite low at this sample
size. Our estimator can recover the true marginal densities.
    Figure 2 plots the true versus the estimated joint density of f (β), for one out of our 20
replications. A visual inspection of the densities shows that we are able to generate excellent
fits to the true densities of preferences using our estimator. Our Monte Carlo experiments
demonstrate that it is possible to generate good estimates of market shares and the density of
random coefficients with a large number of observations. In fact, we get the modes almost spot
on, even for the challenging case of six modes.


7    Empirical Application to Dynamic Programming
As an illustration of our estimator, we apply it to the dynamic labor supply setting from Duflo,
Hanna, and Ryan (2008), hereafter referred to as DHR. They consider the problem of incentiviz-
ing teachers to go to work, and estimate a dynamic labor supply model using the method of
simulated moments. The model is a single agent, dynamic programming problem with a serially
correlated, unobserved state variable. In order to accommodate unobserved heterogeneity across
teachers, they estimate a parametric, two-type mixture model. We apply the present estima-
tor to this setting, and show that our approach allows for a more flexible approximation of the
underlying heterogeneity. Further, our new estimator is quicker to run and easier to implement.
    Teacher absenteeism is a major problem in India, as nearly a quarter of all teachers are
absent nationwide on any given day. The absenteeism rate is nearly 50 percent among non-
formal education centers, NGO-run schools designed to provide education services to rural and
poor communities. To address absenteeism, DHR ran a randomized field experiment where
teachers were given a combination of monitoring and financial incentives to attend school. In a
sample of 113 rural, single-teacher schools, 57 randomly selected teachers were given a camera
and told to take two pictures of themselves with their students on days they attend work. On
top of this monitoring incentive, teachers in the treatment group also received a strong financial

                                                  33
incentive: for every day beyond 10 they worked in a month, they received a 50 rupee bonus on
top of their baseline salary of 500 rupees a month. The typical work month in the sample was
about 26 days, so teachers that went to school at every opportunity would receive greater wages
than the control group, which was paid a flat amount of 1000 rupees per month. The program
ran for 21 months, and complete work histories were collected for all teachers over this period.
    DHR evaluates this program and find the combination of incentives was successful in reducing
absenteeism from 42 percent to 21 percent. In order to disentangle the confounding effects of
the monitoring and financial incentives, they exploit nonlinearities in the financial incentive to
estimate the labor supply function of the teachers.
    A convenient aspect of the intervention is that the financial incentives reset each month.
Therefore, the model focuses on the daily work decision of teachers within a month. Denote the
number of days it is possible to work in each month as C. Let t denote the current day, and
let the observed state variable d denote the number of days already worked in the month. Each
day a teacher faces a choice of going to school or staying home. The payoff to going to school is
zero. The payoff to staying home is equal to µi + i,t , where µi is specific to teacher i and i,t is
a shock to payoffs. The shock i,t is serially correlated.
    After the end of the month (C), the teachers receive the following payoffs, denominated in
rupees, which are a function of how many days that teacher worked in the month:
                                        
                                         500 + (10 − d) · 50               if d ≥ 10,
                                 π(d) =                                                                                 (20)
                                         500                               otherwise .

Let r be the type of the teacher in our approximation. The choice decision facing each teacher
in the form of a value function for periods t < C is

 V r (t, di , i,t ) = max {E [V r (t + 1, di + 1, i,t+1 ) | i,t ] , µr + i,t + E [V r (t + 1, di , i,t+1 ) | i,t ]} .
                                                                                                                        (21)
At time C, the value function simplifies to

                           V r (C, di , i,C ) = max {βπ(di + 1), µr + i,C + βπ(di )} ,                                (22)

where β is the marginal utility of an additional rupee. There is no continuation value in the
right side of (22) as the stock of days worked reset to zero at the end of the month. These
value functions illustrate the tradeoff teachers face early in each month between accruing days



                                                            34
worked, in the hopes of receiving an extra payoff at the end of the month, and obtaining instant
gratification by skipping work.18
       DHR propose a simulated method of moments estimator that matches sequences of days
worked at the beginning of each month. They find parameters that generate predicted probabil-
ities for all the possible sequences of work histories in the first five days of each month as close
as possible to their empirical counterparts.19                 This procedure results in 25 − 1 = 31 linearly
independent moments to be matched in each month. They estimate several specifications of the
above model using this approach; we focus on their preferred model, in which the shock to the
outside option follows an AR(1) process with correlation parameter ρ, and the teacher-specific,
deterministic component of the outside option µi is drawn from a bimodal normal distribution.
Note that this is a very computationally intensive problem, as the researcher must integrate out
over both the distribution of outside options µi and the serially correlated unobservable i,t in
order to produce the probabilities of sequences of days worked. This is not a computationally
trivial task, as the model requires several hundred thousand simulations in order to produce
estimates of the probabilities of working sequences with low variance for each type. Using the
parametric mixture, DHR estimate that in a given month 97.6 percent of the teachers have out-
side options drawn from a normal distribution with mean -0.428 and variance of 0.007, with the
remaining 2.4 percent drawing from a normal distribution with mean 1.781 and variance 0.050.
At the estimated parameters of this model, workers drawing the first distribution generally go
to school every day, while workers drawing from second distribution are likely to never attend
school during a given month.
       There are natural bounds on the level of the outside option, as low values of µi lead to
teachers always working and high values lead to teachers never working. The autocorrelation
parameter is bounded between negative one and positive one. The beta parameter is also sharply
bounded, as it has similar effects on work behavior as the outside option outside a narrow range.
       We apply the present paper’s estimator to this setting, allowing for a nonparametric dis-
tribution of heterogeneity in the outside option. We hold the marginal utility of income and
  18
      The day of the month t and stock of worked days d are naturally discrete state variables. For each combination
of t and d, the continuous state  is discretized into 200 bins. For each simulated value of , the closest bin value
is taken to be the actual state for the purposes of dynamic programming. For numerical integration in the
calculation of expectations such as E [V r (t + 1, di + 1, i,t+1 ) | i,t ], the distribution of i,t+1 is used to calculate
the probability i,t+1 lies in each of the 200 bins, and those probabilities weight V r (t + 1, di + 1, i,t+1 ) in the
approximation to the expectation.
   19
      Considering data on only the first five days in each month both simplifies the computational burden and
breaks much of the statistical dependence across the beginning of months (as the correlation of i,t across months
is low). We treat data from different months as statistically distinct from the viewpoint of the correlation in i,t .



                                                             35
the autocorrelation parameter at their values estimated under the two-type parametric model
estimated in DHR: β = 0.013 and ρ = 0.449.20 In other words, we do not estimate those
parameters.
    We estimate the model with a discrete approximation to the underlying distribution of het-
erogeneity in the outside option. We let the number of basis functions range between R = 5
and R = 40, with the types uniformly distributed across the economic bounds on the outside
option. At the lower extreme, µi = −2.5, the teachers almost always go to work, and at the
upper extreme, µi = 4.0, teachers almost never go to work. We solve the model under each
of those R draws for every month in the data set. In addition to the intra-month time series
variation in the moments, our model is identified from variation in the number of days, the distri-
bution of workdays (teachers receive one day off on the weekends), and the number of holidays,
which count as a day worked in the teacher’s payoff function, across months. These exogenous
sources of variation produce different probabilities of working even under the same set of model
primitives.
    For each month in the data, we solve the dynamic program for all R types, and then compute
the probabilities of all possible sequences of days worked in the first five days of the month. We
collate these probabilities together to produce a matrix with R columns and 31 rows, where each
row corresponds to the probability of observing a specific work history for that month. We then
stack these matrices across months to obtain a large matrix with R columns and 31∗21 = 651 rows
corresponding to the 31 = 25 − 1 possible work histories and the 21 different months. We formed
the corresponding vector of empirical probabilities as the vector of dependent variables. We then
assigned weights to each of the R types using the inequality constrained OLS estimator.21 This
estimator is quite similar to the specification in equation (8), except that we do not use panel
data to construct sequences of choices for the same teacher across months, only within months.
     The estimated distributions of types are shown in Figure 3. We use a discrete-type approx-
imation to the distribution of µi . The vertical axis is the weight of that type in the probability
   20
      In a previous draft of the paper, we also experimented with allowing for unobserved heterogeneity in the other
two parameters, the marginal utility of income (β) and the degree of persistence in the AR(1) process (ρ), with
unrestricted covariance across the parameters. We found that the β and ρ parameters were always estimated
tightly around their point estimates, and therefore we focus on the distribution of µi in what follows.
   21
      We use a penalized (for the constraints) Newton’s method to minimize the least squares objective function.
We use the assumption that the true types are the types included to construct confidence intervals, as in section
5. Confidence intervals are constructed using the standard OLS formulas. An observation is a teacher / month.
Five days are used for each teacher / month observation. The number of observations is 1123 teacher / months.
The correlation in i,t across the first five days should be low, so we do not account for autocorrelation across
teacher / months for the same teacher.



                                                        36
mass function that is an approximation to the true distribution of types. The squares represent
the point estimates; the sum of these weights is always 1. The figures also show the 90% confi-
dence intervals for the weight on each type. The confidence intervals are smaller in the center
of the distribution. Keep in mind these are univariate confidence regions for individual weights;
the confidence regions for functions of all of the weights, such as F̂ (β) at a particular β, may be
relatively narrower. In these and subsequent figures, we present four approximations, for R = 5,
R = 10, R = 20, and R = 40. The R = 40 estimates suggest a double-peaked distribution in
the range a utility of staying home from -0.5 to 0.0. Three basis points in the range -0.5–0.0 are
given substantial weight. The right tail is thin: most weights are 0 but the type of µ = 2.83 has
an estimated frequency of 3%. The middle of this support, -0.25, gives a value of staying home
of −0.25/β = −0.25/0.013 = −19 rupees a day. This means that, at i,t = 0, a modal teacher
will go to work for only a standard incentive like the threat of being fired or simply an attitude
of professionalism. However, there is a positive probability of teachers with positive values of
staying home, µi . Our nonparametric estimates do not exactly match those from the parametric
results in DHR, but they capture the same finding that most teachers are between -0.5 and 0.0
with a small fraction of teachers who always prefer to skip work in the right tail.
   Figure 4 shows the fit of the discrete approximation models to the distribution of days worked
in the data. Keep in mind we only used the first five days of each month in estimation. The mean
predicted distribution matches the observed distribution relatively well. In the specifications with
R > 5, our model tends to underpredict the peak by one day and overpredict the proportion of
days worked in the 15–20 range. We note that these fit results are particularly encouraging, as
the model does not use the distribution of days worked directly in the estimation; as such, these
results are a partial out-of-sample test.
   A feature of the teacher data is that there is a completely independent second experiment
which was conducted after the first intervention, in which the incentives facing teachers were
changed. Instead of working 10 days to get a marginal per-day bonus of 50 rupees, teachers in
the second experiment had to work 12 days to get a marginal per-day bonus of 70 rupees. Figure
5 shows the fits of the discrete approximation models in the out-of-sample test. These data are a
bit noisier than the original data set, due to a smaller sample size, but the nonparametric model
also does a fairly good job of matching the distribution of days worked. Our approach tends
to underpredict the proportion of zero days worked and overpredict the number of days worked
above 18.
   While the advantage of a nonparametric approach is clear, it is also worth emphasizing that
the computational burden of the present estimator is much lower than the parametric alternative.

                                                37
Even with good starting values, the parametric approach requires several thousand evaluations
of the objective function in a typical Newton-based optimizer. For each guess of the parameters,
the dynamic model requires several million forward simulations to produce reliable estimates
of the predicted probabilities of work histories, which is a significant computational burden.
Furthermore, specification testing in the parametric environment is costly, and the parameter
estimates under one type were very different than under two types. One clear benefit of the
present approach is that one is able to run literally dozens of different models in less time than
it takes to optimize a single parametric specification.


8    Conclusion
In this paper, we have proposed a new method for estimating general mixtures model. Our
method allows the researcher to drop standard parametric assumptions, such as independent
normal random coefficients, that are commonly used in applied work. In terms of computer
programming and execution time, our linear regression estimator is easier to work with than
simulated likelihood or method of moments estimators for parametric models. Convergence of
an optimization routine to the global optimum is guaranteed under linear regression with linear
constraints, something that cannot be said for other statistical objective functions. Also, our
estimator is much easier to program and to use than alternatives such as the EM algorithm.
    Our estimator is useful for estimating a wide variety of models. For example, in a dynamic
programming setting, the estimator allows for a nonparametric distribution of random coefficients
while simultaneously cutting the computational time compared to the no-random coefficients
model. The computational savings arise because we must solve the dynamic program only once
for each basis vector.
    We explored the asymptotic properties of the linear regression estimator. We showed con-
sistency in the function space of all distributions by viewing our estimator as a sieve estimator
and applying techniques in Chen and Pouzo (2009a). Under additional assumptions, we derive
the pointwise rate of convergence of the estimated distribution function to the true distribution
function. Many alternative mixtures estimators lack asymptotic results in such generality.
    We apply our estimator in an empirical example of estimating the distribution of agent pref-
erences in a dynamic programming model with an unobserved, serially correlated state variable.
Our estimator use dramatically less programming and execution time than a parametric alter-
native.



                                                38
A     Proof of Consistency: Theorem 3.1
There are two cases. If the true distribution F0 ∈ FR(N ) is in some sieve space, then the estimator
becomes parametric after some R (N ), and the estimator is consistent. The remainder of the
argument assumes F0 is not in any sieve space.
    We resort to CP’s Lemma B.1 in our consistency proof and verify the conditions of CP’s
Lemma B.1. To provide completeness to readers, we first present our simplified version of CP’s
Lemma B.1 and its proof.

Lemma A.1 Lemma B.1 of CP: Let F̂N be such that Q̂N (F̂N ) ≤ inf F ∈FR(N ) Q̂N (F )+OP (ηN )
with ηN = o(1). Suppose the following conditions (B.1.1)-(B.1.4) hold:

    • (B.1.1) (i) Q(F0 ) < ∞; (ii) there is a positive function δ(N, R(N ), ε) such that

                                     inf              Q(F ) − Q(F0 ) ≥ δ (N, R(N ), ε)
                           F ∈FR(N ) :dLP (F,F0 )≥ε


      for each N ≥ 1, R ≥ 1, ε > 0, and lim inf N →∞ δ (N, R(N ), ε) ≥ 0 for all ε > 0.

    • (B.1.2) (i) F ⊆ F and (F, dLP (·)) is a metric space; (ii) FR ⊆ FR+1 ⊆ F for all R ≥ 1,
      and there exists a sequence ΠN F0 ∈ FR(N ) such that dLP (ΠN F0 , F0 ) → 0 as N → ∞.

    • (B.1.3) (i) Q̂N (F ) is a measurable function of the data {yi , xi }N
                                                                          i=1 for all F ∈ FR(N ) ; (ii)
      F̂N is well-defined and measurable.

    • (B.1.4) (i) Let ĉQ (R(N )) = supF ∈FR(N ) Q̂N (F ) − Q(F ) = oP (1); (ii)

                    max ĉQ (R(N )), ηN , |Q (ΠN F0 ) − Q (F0 )|
                       
                                                                 = o(1) for all ε > 0.
                                 δ (N, R(N ), ε)

                
Then dLP F̂N , F0 −→ oP (1).

Proof. Under condition (B.1.3) (ii) F̂N is well-defined and measurable. It follows that for any
ε > 0,




                                                        39
                            
      Pr dLP (F̂N , F0 ) ≥ ε
                                                                           !
  ≤ Pr               inf              Q̂N (F ) ≤ Q̂N (ΠN F0 ) + O(ηN )
           F ∈FR(N ) :dLP (F,F0 )≥ε
                                                                                                                 !
                                      n                       o                                      
  ≤ Pr               inf               Q(F ) + Q̂N (F ) − Q(F )   ≤ Q(ΠN F0 ) + Q̂N (ΠN F0 ) − Q(ΠN F0 ) + O(ηN )
           F ∈FR(N ) :dLP (F,F0 )≥ε
                                                                                                                  !
                                      n                        o
  ≤ Pr               inf               Q(F ) − Q̂N (F ) − Q(F ) ≤ Q(ΠN F0 ) + Q̂N (ΠN F0 ) − Q(ΠN F0 ) + O(ηN )
           F ∈FR(N ) :dLP (F,F0 )≥ε
                                                                                      !
                                                    Q
  ≤ Pr               inf              Q(F ) ≤ 2ĉ (R(N )) + Q(ΠN F0 ) + O(ηN )
           F ∈FR(N ) :dLP (F,F0 )≥ε
                                                                                                   !
  ≤ Pr               inf              Q(F ) − Q(F0 ) ≤ 2ĉQ (R(N )) + Q(ΠN F0 ) − Q(F0 ) + O(ηN )
           F ∈FR(N ) :dLP (F,F0 )≥ε

  ≤ Pr δ(N, R(N ), ε) ≤ 2ĉQ (R(N )) + |Q(ΠN F0 ) − Q(F0 )| + O(ηN )
                                                                    


which goes to zero by condition (B.1.4).
   In our problem we let

                                        N      R
                                                                 !2
                                      1 X      X
                           Q̂N (F ) =     yi −   θr g (xi , β r ) , F ∈ FR(N )
                                      N
                                           i=1             r=1

                                                  2 
                            PR       r         r
and Q(F ) = E        Yi −       r=1 θ g (Xi , β )          for F ∈ FR(N ) . Similarly we define

                                               N     Z                   2
                                             1 X
                                 Q̂N (F0 ) =      yi − g (xi , β) dF0 (β)
                                             N
                                                 i=1
                 h          R                     2 i
and Q(F0 ) = E       Yi −       g (Xi , β) dF0 (β)     . Note that since we do not use CP’s optional penal-
ization, we let QN (·) = Q(·) = Q(·) in CP’s original Lemma B.1. Also note that by definition of
our estimator as an extremum estimator, we have

                        Q̂N (F̂N ) ≤       inf      Q̂N (F ) + OP (ηN ) with ηN = o(1).
                                        F ∈FR(N )




                                                             40
   We start with the condition (B.1.1). The condition Q(F0 ) < ∞ holds because Q(F ) ≤ 1 for
all F ∈ F. Next we will verify the condition

                                inf              Q(F ) − Q(F0 ) ≥ δ(N, R(N ), ε)               (23)
                      F ∈FR(N ) :dLP (F,F0 )≥ε


for each N ≥ 1, ε > 0, and some function δ(N, R(N ), ε) to be defined where lim inf N →∞ δ(N, R(N ), ε) >
0 (strictly positive). We will use our assumption of identification in Theorem 3.1. Let m (x, F ) =
P (x) − g (x, β) dF (β) and with abuse of notation, let g (x, β) dF (β) = R           r       r
         R                                                  R                 P
                                                                                 r=1 θ g (x, β ) for
F ∈ FR . First note that we have
                         "        Z                  2 #
                                                           = E (Y − P (X) + m(X, F ))2
                                                                                      
          Q(F ) = E          Y −       g (X, β) dF (β)
                     h            i
                  = E (Y − P (X))2 + 2E [(Y − P (X)) m(X, F )] + E m(X, F )2
                                                                            
                     h            i
                  = E (Y − P (X))2 + E m(X, F )2
                                                


where the last equality is obtained by applying the law of iterated expectation noting E [Y − P (X)|X] =
0. Therefore, for each F ∈ FR , we have

              Q(F ) − Q(F0 ) = E m(X, F )2 − E m(X, F0 )2 = E m(X, F )2
                                                                   
                                                                                               (24)

because m(X, F0 ) = 0 and the condition (23)
                                          h holds due i to our assumption of identification as
                                                    2
the following argument shows. Consider E m (X, F ) , with m (X, F ) defined above, as a map
                                                                            h          i
from FR(N ) to R+ . As the true F0 is not in the sieve space FR(N ) , then E m (X, F )2 takes
on positive values for each F ∈ FR(N ) , because the model is identified on a set X̃ with positive
                                            h         i
probability. But FR(N ) is compact, so E m (X, F )2 attains some minimum strictly positive
value on FR(N ) . Therefore, we can take δ(N, R(N ), ε) > 0 for all R(N ) ≥ 1 with ε > 0.
   Next we consider (B.1.2). First note that (F, dLP ) is a metric space and we have FR ⊆
FR+1 ⊆ F for all R ≥ 1 by construction of our sieve space. Then we claim that there exist
a sequence of functions ΠN F0 ∈ FR(N ) such that dLP (ΠN F0 , F0 ) → 0 as N → ∞ as follows.
First, B R(N ) becomes dense in B by assumption of the theorem. Second, FR(N ) becomes dense
in F because the set of distributions on a dense subset B R(N ) ⊂ B is itself dense. To see
this, remember that the class of all distributions with finite support is dense in the class of all
distributions (Aliprantis and Border 2006, Theorem 15.10). Any distribution with finite support



                                                      41
can be approximated using a finite support in a dense subset B R(N ) (Huber 2004).
    Next, to show (B.1.3) holds, we resort to Remark B.1. (1) (a) of CP. First note that FR is a
compact subset of F for each R. This is because the simplex
                                                                                    
                                                                      R(N )       
                                                                         X
                            ∆R(N )   = θ = θ1 , . . . , θR(N ) | θr ≥ 0,       θr = 1
                                                                                    
                                                                                     r=1


itself is compact in the definition of FR . Second we need to show that for any data {yi , xi }N
                                                                                               i=1 ,
Q̂N (F ) is lower semicontinuous on FR for each R ≥ 1. Again with abuse of notation, let
R                PR r            r
  g (x, β) dF =     r=1 θ g (x, β ) for F ∈ FR . Then, for any F1 , F2 ∈ FR(N ) , applying the
triangle inequality gives us

                     Q̂N (F1 ) − Q̂N (F2 )
                                                                   Z                          Z                      
           PN                                           PN
    2 N1                                            1
                     R
≤           i=1 yi       g(xi , β)d |F1 − F2 | +    N    i=1             g(xi , β)d (F1 + F2 )     g(xi , β)d |F1 − F2 | .

                                R
Because yi , g(xi , β), and         g(xi , β)dF (β) are uniformly bounded, the above implies Q̂N (F1 ) − Q̂N (F2 ) →
0 as F1 → F2 . This means that Q̂N (F ) is continuous on FR ; continuity is stronger than lower
semicontinuity.
    Next there are two conditions to verify in (B.1.4). We first focus on the uniform convergence
of Q̂N (F ) to Q(F ) for F ∈ FR(N ) ,

                                          sup       Q̂N (F ) − Q(F ) = oP (1).                                    (25)
                                        F ∈FR(N )


    It is convenient to view Q̂N (F ) and Q(F ) as functions of θ ∈ ∆R(N ) and so to write Q̂N (θ)
and Q(θ), respectively. Then we can show that the class of measurable functions
                                                                               !2                  
                                                            R
                                                             X                                      
                            G = l(y, x, θ) =            y−         θr g (x, β r )    : θ ∈ ∆R(N )
                                                                                                   
                                                             r=1


is P-Donsker due to Theorems 2.5.6 and 2.7.11 of van der Vaart and Weller (1996), noting that




                                                               42
(i) l(y, x, θ) is uniformly bounded by 1 and is Lipschitz in θ, i.e.,

                                                  R
                                                  X
             |l(y, x, θ1 ) − l(y, x, θ2 )| ≤ 2y         g (x, β r ) |θ1r − θ2r |
                                                  r=1
                                                  R
                                                  X                                          R
                                                                                             X
                                                                   r
                                              +         (g (x, β       ) (θ1r   +   θ2r ))         (g (x, β r ) |θ1r − θ2r |)
                                                  r=1                                        r=1
                                                     R
                                                     X                         √
                                           ≤ M (·)         |θ1r − θ2r | ≤ M (·) R kθ1 − θ2 kE
                                                     r=1


with some function E M (·)2 < ∞ and k·kE denotes the Euclidean norm22 and (ii) ∆R(N ) is a
                          

compact subset of RR(N ) .
       As measures of complexity of spaces, let N (ε, T , k·k) denote the covering number of the set
T with balls of radius ε with a norm k·k and let N[] (ε, T , k·k) denote the bracketing number of
the set T with ε-brackets.
                                                           PR                       r ) θr
       We take M (·) = 4 (noting y, g(·), and                   r=1 g (x, β                   are uniformly bounded by 1).
Then due to Theorem 2.7.11 of van der Vaart and Wellner (1996), we have N[] (ε, G, k·k) ≤
                    √ R
N 8√ε R , ∆R , k·kE = 8 ε R for any norm k·k. It follows that

                                 Z    1q
                                        log N[] (ε, G, k·k)dε = O (R log R) ;
                                  0

                                      R(N ) log R(N )                      R(N ) log R(N )
therefore we find ĉQ (R(N )) =            √
                                              N
                                                        and need                √
                                                                                   N
                                                                                                    → 0 to satisfy the uniform
convergence condition (25).
       To satisfy the second condition in (B.1.4), it suffices to show all three terms in max{·} go to
zero, provided that lim inf N →∞ δ (N, R(N ), ε) > 0. We have shown the uniform convergence of
the sample criterion function and we can take ηN small enough. We also have

                                        |Q (ΠN F0 ) − Q (F0 )| = o(1),

which is trivially satisfied from the continuity of Q(F ) and ΠN F0 → F0 from (B.1.2) (ii).
                                                                                           ∗
       Next we formally prove lim inf N →∞ δ (N, R(N ), ε) > 0. Note that there exists an FR(N ) ∈
  22
    In the above, the first inequality is obtained by the triangle inequality and the second inequality holds due
to the Cauchy-Schwarz inequality.




                                                           43
                               
                      ∗
FR(N ) such that dLP FR(N ) , F0 ) ≥ ε and

                                 ∗
                                FR(N ) = arg min F ∈FR(N ) :dLP (F,F0 )≥ε Q(F ) − Q(F0 )


since FR(N ) is compact. Then from (24), we obtain

                                           2                                   2 
         ∗                               ∗                            ∗
Q       FR(N )       − Q (F0 ) = E m X, FR(N )       = E m X, FR(N ) − m (X, F0 )
                                                                 2      h                                i
                                           ∗                                                                 2
                               = E m X, FR(N    )   −  m (X, Π  F
                                                               N 0 )      + E  (m (X, Π   F
                                                                                        N 0  ) − m (X, F 0 ))
                                    h                                                            i
                                               ∗
                                 +2E m X, FR(N      ) − m (X, ΠN F0 ) (m (X, ΠN F0 ) − m (X, F0 ))
                                                                2 
                                           ∗
                               = E m X, FR(N ) − m (X, ΠN F0 )            + o(1),
                                                                              2 
                                            R(N )
                                            X
                                = E                 g (X, β r ) (θ∗r − ΠN θ0r )  + o(1)
                                                r=1
                                                       R(N )
                                                        X
                                ≥ ξmin (R(N ))                 (θ∗r − ΠN θ0r )2 + o(1)
                                                        r=1

       ∗
                             PR      r      r
                                                    PR         r    r
where FR(N ) (β) =                ≤ β], ΠN F0 (β) =
                                r=1 θ∗ 1 [β          r=1 Π  N θ 1 [β ≤ β], and ξmin (R(N )) de-
                                             h             0 0 i
notes the smallest eigenvalue of the matrix E g (X, β r ) g X, β r        0
                                                                                . In the above,
                                                                                              1≤r,r ≤R(N )
we obtain the fourth equality because ΠN F0 → F0 and m(X, F ) is continuous in F , applying
the dominated convergence theorem.                                                           
                                                                                         ∗
    We will prove lim inf N →∞ δ (N, R(N ), ε) > 0 by contradiction. Now suppose that Q FR(N ) −
Q (F0 ) → 0. Then
                h    we must         |θ∗r − ΠN θ0r | → 0 for all r = 1, . . . , R(N ) because we assume
                            have0 i
the matrix E g (X, β r ) g X, β r                       is positive definite and its smallest eigenvalue is
                                          1≤r,r0 ≤R(N )                                    
bounded away from zero uniformly in R(N ). It follows that dLP FR(N          ∗     , Π   F    → 0, which con-
                                                                                       N  0
                                                                              )                          
                             ∗
tradicts the fact that dLP FR(N   , F      ≥ ε  >  0 because  d      F ∗    , F      ≤ d      F ∗    , Π   F    +
                                )     0                         LP    R(N )    0         LP    R(N )     N  0
dLP (ΠN F0 , F0 ). Therefore, we conclude that lim inf N →∞ δ (N, R(N ), ε) is strictly positive.
    We have verified all the conditions in Lemma B.1 of CP and this completes the consistency
proof.




                                                                   44
B      Proof of the Asymptotic Bounds
We let R = R(N ). We also let C, C
                                  1 , hC2 , . . . denotegeneric i
                                                                  positive constants. We let ξmin (j, R)
                                                                  0
denote the smallest eigenvalue of E gj (Xi , β r ) gj Xi , β r                     . We use diag (A) to de-
                                                                       1≤r,r0 ≤R
note a diagonal matrix composed of diagonal elements of a matrix A. We often use the fol-
lowing inequality (denoted by RCS for the Cauchy-Schwarz inequality for R terms in a sum):
PR          √ qPR
                          2
  r=1 Wr ≤   R      r=1 Wr for a sequence Wr ’s.
     We first prove preliminary lemmas that are useful to prove Theorem 4.1 and Theorem 4.2.
We define
         X                               
          1 N             r
                               
                                       r0
                                                               h                          0
                                                                                              i
ΨN,R =          gj (xi , β ) gj xi , β                and ΨR = E gj (Xi , β r ) gj Xi , β r                 ,
          N i=1
                                            1≤r,r0 ≤R                                             1≤r,r0 ≤R


where we suppress ΨN,R and ΨR ’s dependence on j for notational simplicity.

Lemma B.1 Suppose Assumption 4.1 and 4.2 hold. Then
       h n                                               o    n                                             oi
    min Pr kgj (·, β r )k2L2,N ≤ 2 kgj (·, β r )k2L2 , ∀r , Pr kgj (·, β r )kL2 ≤ 2 kgj (·, β r )kL2,N , ∀r ≥
                                                                                                     N c40
                                                                                                          
                                                                                1 − R exp −C1 4 .
                                                                                                      ζ0



Proof. The claim follows from the union bound applied to the r-specific events and Hoeffding’s
inequality.
     Lemma B.1 suggests that as N increases, kgj (·, β r )kL2,N and kgj (·, β r )kL2 get closer to each
other with probability exponentially approaching to one. Lemma B.1 implies that diag (ΨN,R ) ≤
2diag (ΨR ) holds with probability approaching to one because the kgj (·, β r )k2L2,N ’s are diagonal
elements of ΨN,R and the kgj (·, β r )k2L2 ’s are diagonal elements of ΨR .
     The purpose of the following Lemma B.2 is to show that ΨN,R ≥ ΨR /2 holds with probability
approaching to one. This in turn shows that ΨN,R is nonsingular with probability approaching
to one.

Lemma B.2 Let G = span gj (·, β 1 ), . . . , gj (·, β R ) be the linear space spanned by some func-
                      




                                                     45
                                                           j            j
tions gj ·, β 1 , . . . , gj ·, β R . Note that obviously HR
                                  
                                                             ⊂ G where HR , is defined in (16). Then

                                                       kµk2L2
                                      (                                 )                     
                                                                                     2     N
                                 Pr            sup                 >2          ≤ R exp −C2 4 2
                                          µ∈G\{0}    kµk2L2,N                             ζ0 R

for some constant C.

Proof. Let φ1 , . . . , φM be an orthonormal basis of G in L2 with M ≤ R. Also let ρ(D) denote
the following quantity for a symmetric matrix D:
                                                                X              X
                                                ρ(D) = sup             |al |         |al0 | Dl,l0 ,
                                                                   l           l0


where the sup is taken over sequences {al }M
                                                                                PM        2
                                           l=1 with                                  l=1 al   = 1. Then, following Lemma 5.2 in
Baraud (2002), we have

                                      kµk2L2
                        (                               )                                                             !
                                                                              ($0 − c−1 )2
                   Pr       sup                      >c     ≤ M 2 exp −N                                                            (26)
                                      kµk2L2,N
                                                                                
                        µ∈G\{0}                                          4$1 max ρ2 (A) , ρ (B)
                  q
where A   l,l0   = E[φ2l φ2l0 ] and Bl,l0 = kφl φl0 k∞ for l, l0 = 1, . . . , M and $0 and $1 denote the lower
bound and upper bound of the density of X, respectively. We find Al,l0 ≤ ζ02 and Bl,l0 ≤ ζ02 .
It follows that
                                                                                    !2
                            X             X                            X                               X
     ρ(A) ≤ ζ02 sup               |al |         |al0 | = ζ02 sup            |al |        ≤ ζ02 sup M       |al |2 = ζ02 M ≤ ζ02 R
                             l            l0                           l                               l


where ( l |al |)2 ≤ M l |al |2 holds by the Cauchy-Schwarz inequality (and note   |al |2 =1 in
       P             P                                                          P
                                                                                                                          l
our construction). Similarly we have ρ(B) ≤ ζ02 R. Noting M ≤ R, from (26), we conclude

                                                      kµk2L2
                                      (                                )                   
                                                                                    2   N
                                 Pr        sup                  >2          ≤ R exp −C2 4 2 .
                                          µ∈G\{0}    kµk2L2,N                          ζ0 R


   Next combining Lemma B.1 and Lemma B.2, we obtain a bound for the probability that the
matrix ΨN,R is not smaller than a diagonal matrix in the semi-positive definite sense.




                                                                       46
Lemma B.3 Suppose Assumption 4.1 and 4.2 hold. Then,

                                                       N c40
                                                                          
              ξmin (R)                                            2        N
    Pr ΨN,R −          diag (ΨN,R ) ≥ 0 ≥ 1 − R exp −C1 4      − R exp −C2 4 2 .
                4ζ02                                    ζ0                ζ0 R



                                       ξmin (R)
Proof. First, note that ΨR −             ζ02
                                                diag (ΨR )   ≥ 0 (positive semi-definite) because ΨR is a
positive definite matrix by Assumption 4.2 (ii) and a well-known result involving eigenvalues.
Now let Gbe the linear space spanned by gj ·, β 1 , . . . , gj ·, β R . Now note that under the event
                                                                    

kgj (·, β r )k2L2,N ≤ 2 kgj (·, β r )k2L2 ∀r = 1, . . . , R, we have diag (ΨN,R ) ≤ 2diag (ΨR ) and note that
                                                  
                                        kµk2L
under the event supµ∈G\{0} kµk2               2
                                                > 2 , we have ΨN,R ≥ ΨR /2. Therefore, under the event
                                           L2,N
                                                                                          
                                                                                kµk2L
n                                           o
             r  2                    r   2
 kgj (·, β )kL2,N ≤ 2 kgj (·, β )kL2 , ∀r = 1, . . . , R ∩ supµ∈G\{0} kµk2           2
                                                                                       > 2 , we have
                                                                               L2,N



                            ξmin (R)                        ξmin (R)
                  ΨN,R −         2   diag (ΨN,R ) ≥ ΨR /2 −          2diag (ΨR ) ≥ 0
                              4ζ0                             4ζ02

             ξmin (R)
since ΨR −     ζ02
                      diag (ΨR )   ≥ 0. It follows that
                                                      
                         ξmin (R)
      1 − Pr ΨN,R −               diag (Ψ N,R   ) ≥  0   ≤
                            4ζ02
                                                                                                    kµk2L2
                                                                                    (                         )
            n                                                             o
      1 − Pr kgj (·, β r )k2L2,N ≤ 2 kgj (·, β r )k2L2 , ∀r = 1, . . . , R + 1 − Pr   sup                    >2
                                                                                       µ∈G\{0}   kµk2L2,N
               N c40
                                      
                          2          N
  ≤ R · exp −C1 4      + R · exp −C2 4 2
                ζ0                  ζ0 R

where the last inequality is obtained from Lemma B.1 and Lemma B.2.

Lemma B.4 Suppose Assumptions 4.1 and 4.2 hold. Then, for given positive sequence ηN

                          N
                     (                                                                              )
                      1 X
                Pr           ej,i gj (xi , β r ) ≤ ηN kgj (·, β r )kL2,N for all r = 1, . . . , R
                     N
                         i=1
                                      2 c2 
                                                                  N c40
                                                                      
                                 N ηN     0
              ≥ 1 − R · exp −                   − R · exp −C1 4 .
                                    8ζ02                           ζ0




                                                       47
Proof. Hoeffding (1963)’s inequality implies that
                    "     (                                                                         )#
                     1 X
            EX Pr         ej,i gj (Xi , β r ) ≥ ηN ||gj (·, β r ) ||L2,N , ∀r ≤ R X1 , . . . , XN        (27)
                     N
                        i
               " R                                         #
                X                                       
                              2             r 2        2
          ≤ EX     exp −2N ηN kgj (·, β )kL2,N /4ζ0
                        r=1

because E [ej,i gj (Xi , β r ) | X1 , . . . , XN ] = 0 and because choice probabilities lie in [0, 1], −ζ0 ≤
ej,i gj (Xi , β r ) ≤ ζ0 uniformly. n                                                     o
      Now note under the event kgj (·, β r )kL2 ≤ 2 kgj (·, β r )kL2,N , ∀r = 1, . . . , R ,

  R
  X                                           R
                                                X                                  
                2
        exp −N ηN kgj (·, β r )k2L2,N /2ζ02   ≤           2
                                                  exp −N ηN kgj (·, β r )k2L2 /8ζ02                      (28)
  r=1                                                r=1
                                                     R
                                                     X
                                                                   2 2
                                                                     c0 /8ζ02 = R exp −N ηN
                                                                                          2 2
                                                                                            c0 /8ζ02 .
                                                                                                   
                                                 ≤         exp −N ηN
                                                     r=1
                                          n                                                           o
From (27)-(28), combining the bound for Pr kgj (·, β r )kL2 > 2 kgj (·, β r )kL2,N , ∀r = 1, . . . , R =
            N c4
                
R exp −C1 ζ 40 in Lemma B.1, the claim follows.
                0
    The following Lemma B.5 decomposes the approximation error into a bias term and a variance
term.

Lemma B.5 Suppose Assumptions 4.1 and 4.2 hold. Then, for any N ≥ 1, R ≥ 2, and a > 1,
we have for all θ ∈ ∆N ,

  J                                                 J
1X                                   2        a+1 1 X                                           ζ02     8a2 2
    µθb(j, xi ) − P (j | xi , f0 )          ≤         kµθ (j, xi ) − P (j | xi , f0 )k2L2,N +               η R,
J                                    L2,N     a−1J                                            ξmin (R) a − 1 N
  j=1                                                  j=1


where the inequality holds with probability greater than 1 − pN,R ,

                                2 c2 
                                                    N c40
                                                                          
                             N ηN  0                           2          N
           pN,R   = RJ exp −           + 2RJ exp −C1 4      + R J exp −C2 2 2 .
                              8ζ02                   ζ0                  ζ0 R




                                                      48
Proof. Because µθb(·) (i.e., θ)
                             b is the solution of the minimization problem in (15), we have

                                 J                                            J
                                                                 2
                                 X                                            X
                                            µθb(j, xi ) −   yi,j L       ≤          kµθ (j, xi ) − yi,j k2L2,N           (29)
                                                                   2,N
                                  j=1                                         j=1


for any θ ∈ ∆R(N ) . Now note that

    J                                           J
                                   2                                                                             2
    X                                           X
           µθb(j, xi ) − yi,j      L2,N
                                            =         µθb(j, xi ) − P (j | xi , f0 ) + P (j | xi , f0 ) − yi,j   L2,N
    j=1                                         j=1
    J                                                        J                                             J
    X                                             2
                                                             X 2 X                                         X
=           µθb(j, xi ) − P (j | xi , f0 )        L2,N
                                                         −         ej,i (µθb(j, xi ) − P (j | xi , f0 )) +   kej,i k2L2,N ,
                                                               N
    j=1                                                      j=1         i                                       j=1
                                                                                                                         (30)

where we use the definition ej,i = yi,j − P (j | xi , f0 ). Similarly we obtain

    J
    X
          kµθ (j, xi ) − yi,j k2L2,N =
    j=1
    J                                                        J                                             J
    X                                                        X 2 X                                         X
          kµθ (j, xi ) − P (j | xi , f0 )k2L2,N −                  ej,i (µθ (j, xi ) − P (j | xi , f0 )) +   kej,i k2L2,N .
                                                               N
    j=1                                                      j=1         i                                       j=1
                                                                                                                         (31)

Subtracting (31) from (30) and noting (31) ≥ (30) from (29), we obtain

                     J
                                                                    2
                     X
                                µθb(j, xi ) − P (j | xi , f0 )      L2,N
                      j=1
                     J                                                         J
                     X                                                         X 2 X
                ≤           kµθ (j, xi ) − P (j |        xi , f0 )k2L2,N     +       ej,i (µθb(j, xi ) − µθ (j, xi )).
                                                                                 N
                      j=1                                                      j=1          i

                            1   PN
Now let VN,r (j) =          N       i=1 ej,i gj   (xi , β r ). Then, we have

                                                                        R
                                 1 X                                    X
                                     ej,i (µθb(j, xi ) − µθ (j, xi )) =   VN,r (j)(θbr − θr )
                                 N
                                        i                                             r=1




                                                                         49
by construction of µθb and µθ and we obtain by the triangle inequality,

J                                                 J                                                  J X
                                                                                                       R
                                       2
X                                                 X                                                  X
      µθb(j, xi ) − P (j | xi , f0 )   L2,N
                                              ≤         kµθ (j, xi ) − P (j |   xi , f0 )k2L2,N +2             |VN,r (j)| θbr − θr .
j=1                                               j=1                                                j=1 r=1
                                                                                                                            (32)
                                      TR n                                               o
Define two events E0,j = r=1 VN,r (j) ≤ ηN kgj            L2,N          (·, β r )k
                                                                 for some positive sequence ηN
           n                                o
and E1,j = ΨN,R − ξmin   (R)
                       4ζ 2
                             diag(ΨN,R ) ≥ 0 . Then, under E0,j ∩ E1,j , we note
                                  0


           XR                       2      XR                                      2
                    2
                   VN,r (j) θbr − θr       2
                                        ≤ ηN          kgj (·, β r )k2L2,N θbr − θr                                          (33)
               r=1                               r=1
                                           2 1
                                                XN XR                         2
                                        = ηN                           r
                                                                     θ −θ
                                                                     b       r
                                                                                  gj (xi , β r )2
                                             N       i=1     r=1
                                             (θ − θ)0 diag(ΨN,R (j))(θb − θ)
                                           2 b
                                        = ηN
                                                              −1
                                           2
                                        ≤ ηN   ξmin (R)/4ζ02        (θb − θ)0 ΨN,R (j)(θb − θ)
                                           2
                                                              −1
                                        = ηN   ξmin (R)/4ζ02        ||µθb(j, xi ) − µθ (j, xi )||2L2,N .

The above uses Lemma B.3. From (32) and (33), it follows that on ∩Jj=1 (E0,j ∩ E1,j )

       XJ                                         2
                  µθb(j, xi ) − P (j|xi , f0 )    L2,N
        j=1
       XJ                                                     XJ        XR
  ≤              kµθ (j, xi ) − P (j|xi , f0 )k2L2,N + 2                             |VN,r (j)| θbr − θr
           j=1                                                    j=1        r=1
                                                                        √
                                                                             r                 2
       XJ                                                     XJ              XR    
  ≤              kµθ (j, xi ) −   P (j|xi , f0 )k2L2,N   +2        R           2 (j) θ
                                                                            VN,r      br − θr
           j=1                                               j=1        r=1
                                                                    s 
                                                                        ξmin (R) −1
       XJ                                                   XJ                   
  ≤              kµθ (j, xi ) − P (j|xi , f0 )k2L2,N     +2      ηN R                ||µθb(j, xi ) − µθ (j, xi )||L2,N
           j=1                                               j=1          4ζ02
       XJ
  ≤      kµθ (j, xi ) − P (j|xi , f0 )k2L2,N
           j=1
              s 
                       ξmin (R) −1 
      XJ                                                                                                                
  + 2      ηN R                          ||µ   (j, x i ) − P (j|xi , f0 )||L    + kµ θ (j, x i ) − P (j|x i , f0 )k L
       j=1               4ζ02                θ                              2,N                                      2,N
                                             b


by the Cauchy-Schwarz inequality, RCS, and the q      triangle inequality. Applying the inequal-
                 2     2
                                                                        −1
ity 2xy ≤ x a + y /a (any x,y, a > 0) to x = ηN R ξmin (R)/4ζ02              and y = ||µθb(j, xi ) −
                                  q
                                                     −1
P (j|xi , f0 )||L2,N and to x = ηN R ξmin (R)/4ζ02
                                                   
                                                        and y = kµθ (j, xi ) − P (j|xi , f0 )kL2,N , re-




                                                                50
                                        J
                                        Q
spectively, we obtain under                   E0,j ∩ E1,j ,
                                        j=1

                  XJ
                            ||µθb(j, xi ) − P (j|xi , f0 )||2L2,N
                   j=1
                  XJ                                                XJ
             ≤              kµθ (j, xi ) − P (j|xi , f0 )k2L2,N +          ||µθb(j, xi ) − P (j|xi , f0 )||2L2,N /a
                      j=1                                            j=1
                      XJ                                                                           −1
                  +            kµθ (j, xi ) − P (j|xi , f0 )k2L2,N /a + 2aηN
                                                                           2
                                                                             R ξmin (R)/4ζ02               ·J
                         j=1

                               J
                               T
It follows that under              (E0,j ∩ E1,j ), for all a > 1,
                             j=1

J                                                         J
X                                                 a+1X                                         ζ02     8a2 2
      ||µθb(j, xi ) − P (j|xi , f0 )||2L2,N ≤          kµθ (j, xi ) − P (j|xi , f0 )k2L2,N +               η RJ.
                                                  a−1                                        ξmin (R) a − 1 N
j=1                                                      j=1


The conclusion follows from
                  C 
             J              J                        2 c2 
                                                                         N c40
                                                                          
        \              X       C               N ηN  0
     Pr       E0,j      ≤   Pr E0,j = J R exp −           + R exp   −C1 4
       
        j=1           
                        j=1                       8ζ02                   ζ0

                                               C 
                                                                     N c4
                                   TJ                      n                                o
by Lemma B.4 and Pr                  j=1 E1,j           = J R exp −C1 ζ 40 + R2 exp −C2 ζ 4NR2    by Lemma
                                                                            0                          0
B.3.

B.1       Proof of Theorem 4.1
From Lemma B.5, we find that the best convergence rate will be obtained when the order of
ηNn is as small as possible  while
                                  keeping   the convergence    of pN,R
                                                                     o to zero. By inspecting pN,R =
                 2 c2
                                       N c40
                                                      
             N ηN                                  2              N
J R exp − 8ζ 2      0
                        + 2R exp −C1 ζ 4 + R exp −C2 ζ 2 R2 , we note that the optimal rate
                  0         q           0                      0
                                log(R(N )N )
is obtained when ηN = O              N         since the first term in pN,R dominates the second term
                                                                     q               
                                                                         log(R(N )N )
in pN,R when ηN is small enough and pN,R → 0 with ηN = O                      N         . The inspection of
                                                                                                    R(N )2 log(R(N ))
the third term in pN,R reveals that we also require R = R(N ) should satisfy                                N           →0
so that pN,R → 0.




                                                               51
    The result of Theorem 4.1 follows from these requirements and Lemma B.5 as

     J
  1X
        ||µθb(j, xi ) − P (j|xi , f0 )||2L2,N =
  J
    j=1
                                                                                                    
                                           R(N ) log(R(N )N ) 1 XJ                                   
                           OP max                            ,     kµθ (j, xi ) − P (j|xi , f0 )kL2,N 
                                                   N           J                                     
                                                                              j=1

                                                                                        q                  
          ζ02
                                                                
                 8a2 2                     log(R(N )N )                                      log(R(N )N )
since   ξmin (R) a−1 ηN R(N )   = O             N       R(N )        under ηN = O                 N             and because
Lemma B.5 holds for any θ ∈ ∆R(N ) . Combining the rate conditions on R(N ) = C · N ρ with
  K               R(N )2 log R(N )                           K
2s+K    < ρ and          N           → 0, we obtain        2s+K      < ρ < 1/2 . Also see that s > K/2 is required
for ρ to exist.

B.2      Proof of Lemma 4.1
First we construct approximating power series with the length of L as L different tensor products
of higher order polynomials of β(k) ’s in β

                                           {ϕ1 (β), . . . , ϕl (β), . . . , ϕL (β)} ,

where ϕl (β) is the lth element in the L number of tensor product polynomials. The tensor
                                                    l1 l2         lK                                     
products are defined by the functions ϕl (β) = β(1)   β(2) · · · β(K) with β = β(1) , β(2) , . . . , β(K) ∈
     K h
     Q                 i
B =       β (k) , β (k) and lk ’s are exponents of each β(k) . For example, we can let ϕ1 (β) = 1,
        k=1
                                      l1 l2           lK
ϕ2 (β) = β(1) , . . . , and ϕl (β) = β(1) β(2) · · · β(K) .
    Now note that gj (x, β) f (β) is a member of the Hölder class (s-smooth) of functions since
it is uniformly bounded and all of its own and partial derivatives up to the order of s are
also uniformly bounded by our restriction on f in H(j) and the logit specification assumption.
Therefore, we can approximate gj (x, β) f (β) well using power series (see Chen, 2007) and obtain
the approximation error rate due to Timan (1963) as

                                                          L
                                                          X
                             sup gj (x, β) f (β) −               al (x)ϕl (β) = O(L−s/K )                              (34)
                             β∈B                           l=1




                                                              52
all x ∈ X . Now define

                        β (k) = b(k),1 < b(k),2 < · · · < b(k),rj +1 = β (k) , k = 1, . . . , K,
                              h             i
be partitions of the intervals β (k) , β (k) , k = 1, . . . , K, into r1 , . . . , rK subintervals, respectively.
Then we can define the R = r1 r2 · · · rK number of subcubes
                                            YK                     
                            Cι1 ,...,ιK =       b(k),ιk , b(k),ιk +1 , ιk = 1, 2, . . . , rk ,
                                                k=1

which become a partition P (B) of B. For any choice of R points

                             {bι1 ,...,ιK ∈ Cι1 ,...,ιK | ιk = 1, 2, . . . , rk ,k = 1, . . . , K}
                                                                                                                     PL
(one bι1 ,...,ιK for each of R subcubes), now we can approximate a Riemann integral of                                  l=1 al (x)ϕl (β)
using a quadrature method with R distinct weights

                         {c(ι1 , . . . , ιK ) ≡ c(bι1 ,...,ιK ) | ιk = 1, . . . , rk , k = 1, . . . , K}

such that
   Z X                                                       Z
        L                                  XL
                al (x)ϕl (β)dβ =                    al (x)       ϕl (β)dβ
          l=1                                 l=1
                                           XL                X
                                   =                al (x)                            c(ι1 , . . . , ιK )ϕk (bι1 ,...,ιK ) + R(δR )
                                              l=1                Cι1 ,...,ιK ∈P (B)


where R(δR ) denotes a remainder term with δR = max{diam(Cι1 ,...,ιK ) : Cι1 ,...,ιK ∈ P (B)}.
Without loss of generality, we will pick δR = C · R−1/K . Noting that ϕl (β) is a product of poly-
nomials in β(k) ’s by construction, we can apply Theorem 6.1.2 (Generalized Cartesian Product
Rules) of Krommer and Ueberhuber (1998) and so we can approximate multivariate integrals
with products of univariate integrals. Note that
                                       Z                     YK Z
                                           ϕl (β)dβ =                    ϕl,k (β(k) )dβ(k)
                                                                 k=1

                  K
                  P                                                 R
with ϕl (β) =           ϕl,k (β(k) ). If we approximate                 ϕl,k (β(k) )dβ(k) using a univariate quadrature
                  k=1




                                                                 53
with weights {ck (1), . . . , ck (rk )}, we obtain
                           Z                             rk
                                                         X
                                ϕl,k (β(k) )dβ(k) =              ck (ιk )ϕl,k (b(k),ιk ) + Rl,k (δR )
                                                         ιk =1


where Rl,k (δR ) denotes a remainder term. Then we can make the univariate quadrature become
accurate (or exact) at least up to the order of rk (Theorem 5.2.1 in Krommer and Ueberhuber
             R p           Prk               p
(1998)) i.e., β(k) dβ(k) =    ιk =1 ck (ιk )b(k),ιk with suitable choice of ck (ιk ) ≡ ck (b(k),ιk ) for all
p ≤ rk .
    For notational simplicity, we take rk = r1 for all k. Then with the L = (r1 +1)K = (R1/K +1)K
number of power series, we can include powers and cross products of β(k) ’s at least up to the
order of r1 . With the choice of L = (r1 + 1)K and ck (ιk )’s that make the univariate quadrature
exact at least up to the order of r1 , we can let
                               Z                     YK Z
                                   ϕl (β)dβ =                         ϕl,k (β(k) )dβ(k)                      (35)
                                                           k=1
                                                                                                  
                                                     YK               r1
                                                                      X
                                             =                               ck (ιk )ϕl,k (b(k),ιk )
                                                           k=1
                                                                      ιk =1


for l = 1, . . . , L. By adding and subtracting terms, it follows that

                   P (j|x, f ) − µθ∗ (j, x) =
                   Z                       Z X
                                                L
                      f (β)gj (x, β) dβ −            al (x)ϕl (β)dβ                                          (36)
                                                l=1
                                                                                            
                     XL         Z             L
                                              X           YK       Xrk
                   +      al (x) ϕl (β)dβ −       al (x)              ck (ιk )ϕl,k (b(k),ι )           k
                                                                                                             (37)
                                                                              k=1
                       l=1                               l=1                         ιk =1
                                                                                   
                       L
                       X              YK         rk
                                                 X
                   +         al (x)                     ck (ιk )ϕl,k (b(k),ιk ) − µθ∗ (j, x) .             (38)
                                       k=1
                       l=1                       ιk =1


We first bound the terms in (36) as
                        Z                                Z X
                                                            L
                               f (β)gj (x, β) dβ −                       al (x)ϕl (β)dβ
                                                                   l=1
                       Z                           XL                                            
                  ≤            f (β)gj (x, β) −                al (x)ϕl (β) dβ = O L−s/K · vol(B)
                                                         l=1



                                                                 54
from (34). Second note that (37) becomes zero due to (35).
                     el (br ) r = 1, . . . , R such that
    Now we construct ϕ
                                                                                                 
                                   R
                                   X                         rk
                                                        YK  X                                    
                                          el (br ) =
                                          ϕ                                ck (ιk )ϕl,k (b(k),ιk )
                                                            k=1                                  
                                   r=1                           ιk =1


with R = r1 · · · rK and br = (br(1) , br(2), , . . . , br(K) ) in b ≡ {b : b = (b(1),ι1 , . . . , b(K),ιK ), ι1 =
1, . . . , r1 , . . . , ιK = 1, . . . , rK }. Then, we have

                                         el (br ) = c1 (br(1),ι1 ) · · · cK (br(K),ιK )ϕl (br )
                                         ϕ

for any br = (b(1),ι1 , . . . , b(K),ιK ) in b, r = 1, . . . , R = r1 r2 · · · rK . Define

                                                     c1 (br(1),ι1 ) · · · cK (br(K),ιK )f (br )
                                         ∗r
                                     θ        = PR                                                    .
                                                           r                   r           r
                                                  r=1 c1 (b(1),ι1 ) · · · cK (b(K),ιK )f (b )


It follows that
        L
        X
                     el (br ) − θ∗r γ(r, j, xj )
               al (x)ϕ                                                                                                                  (39)
         l=1
          PL                 r                   r             r
            l=1 al (x)c1 (b(1),ι1 ) · · · cK (b(K),ιK )ϕl (b )
  =            c1 (br    )···cK (br(K),ι )f (br )
          − PR (1),ιr1                r
                                        K
                                                   r
                                                     γ(r, j, xj )
              r=1 c1 (b(1),ι )···cK (b(K),ι )f (b )
                             1                K
                                               L
                                               X                                                  f (br )
  =     c1 (br(1),ι1 ) · · · cK (br(K),ιK )          al (x)ϕl (br ) − PR                                                        γ(r, j, xj )
                                                                                     r                   r           r
                                               l=1                          r=1 c1 (b(1),ι1 ) · · · cK (b(K),ιK )f (b )
                                               L
                                               X
  ≤     c1 (br(1),ι1 ) · · · cK (br(K),ιK )          al (x)ϕl (br ) − f (br )γ(r, j, xj )
                                               l=1
                                               PR          r                   r           r
                                                  r=1 c1 (b(1),ι1 ) · · · cK (b(K),ιK )f (b ) −             1
  +     c1 (br(1),ι1 ) · · · cK (br(K),ιK )      PR                                                             |f (br )γ(r, j, xj )|
                                                              r                  r           r
                                                    r=1 c1 (b(1),ι1 ) · · · cK (b(K),ιK )f (b )

       = O(R−1 L−s/K ),

First note that c1 (br(1),ι1 ) · · · cK (br(K),ιK ) is at most O(R−1 ), which is obtained if one uses
the uniform weights, i.e., ck (br(k),ιk ) = . . . = ck (bR
                                                         (k),ιk ) for k = 1, . . . , K. Second note that
 PL               r        r                     −s/K ) due to the bound in (34). Third note that
   l=1 al (x)ϕl (b ) − f (b )γ(r, j, xj ) = O(L


                                                                    55
PR         r                   r           r
                                                                                                              R
  r=1 c1 (b(1),ι1 ) · · · cK (b(K),ιK )f (b )   is another quadrature approximation of integral                B   f (β)dβ =
1. Since f (β) itself belongs to a Hölder class, the approximation error rate of this integral be-
comes O L−s/K . Combining these results, our conclusion of the bound in (39) follows.
                 

   Now by letting θ∗ = (θ∗1 , . . . , θ∗R ), we have

                                                                                       
                               L
                               X                    rk
                                               YK  X                                   
                                      al (x)                     ck (ιk )ϕl,k (b(k),ιk ) − µθ∗ (j, x)
                                                k=1                                    
                                l=1                  ιk =1
                               R X
                               X L                             R
                                                               X
                        =                              r
                                                el (b ) −
                                          al (x)ϕ                     θ∗r γ(r, j, xj )
                              r=1 l=1                          r=1
                              R X
                              X    L
                        ≤                       el (br ) − θ∗r γ(r, j, xj )
                                          al (x)ϕ
                               r=1 l=1
                                     −1 −s/K
                        = O(RR             L         ) = O(L−s/K )

where the second equality holds by (39). From this, we bound (38) as O(L−s/K ).
   Combining the bounds, we conclude
                                              
 |P (j|x, f ) − µθ∗ (j, x)| = O L−s/K · vol(B) + 0 + O(L−s/K ) = O(L−s/K )
                                                       !
                                            K −s/K                −s 
                            = O      R 1/K
                                           +1            =O    R 1/K
                                                                     +1       ≤ O(R−s/K ).


The second conclusion in the lemma is trivial since the above holds for all x ∈ X .

B.3      Proof of Lemma 4.2
Define
                                                                                                           
                1 X
                   R
                                                                       1
                                                                                    Z           exp x0j,i β
  Â(x) =             f (β r )γ(r, j, x), A(x) = K                                   f (β)                           dβ
                R                                Q                                
                                                                                    B       1 + Jj0 =1 exp x0j 0 ,i β
                                                                                               P
                  r=1                                             β (k) − β (k)
                                                           k=1
                        R                                                  Z
                        X                                  1
      B̂ = (1/R)              f (β r ), B =     K                            f (β)dβ.
                        r=1
                                                Q                          B
                                                      β (k) − β (k)
                                               k=1




                                                                 56
Then, by plugging in definition of terms and adding and subtracting terms, we have
           √                                 √                     
            R (µθ∗ (j, x) − P (j | x, f )) =   R Â(x)/B̂ − A(x)/B                                (40)
                                                               
                                             √     Â(x) −  A(x)   √ A(x)
                                           =   R                  − R      (B̂ − B).
                                                         B̂           B B̂
Applying the Lindeberg-Levy central limit theorem and the law of large numbers, we further
obtain
                               √
                                   R(Â(x) − A(x)) = OP (1)                                       (41)
                                              Â(x) = A(x) + oP (1)
                                                 B̂ = B + oP (1)
                                      √
                                          R(B̂ − B) = OP (1)

                                 exp(x0j,i β )
uniformly over x ∈ X since PJ             “         ” is uniformly bounded. Combining (40) and (41),
                           1+ j 0 =1 exp x0j 0 ,i β
          √
we obtain R (µθ∗ (j, x) − P (j | x, f )) = OP (1).
   The second claim in Lemma 4.2 follows by applying the dominated convergence theorem,
noting that Â(x) is uniformly bounded by M ≥ supβ∈B |f (β)|.

B.4      Proof of Theorem 4.2
The first result of Theorem 4.2 follows from Theorem 4.1 combined with Lemma 4.1 with R(N )
satisfying (17) so that the variance term and the bias term are balanced. Now we show the second
claim. Note that with probability approaching to one, we have 2 kgj (·, β r )kL2,N ≥ kgj (·, β r )kL2 ≥
c0 > 0 for all r = 1, . . . , R and j = 1, . . . , J by Assumption 4.2(ii) and Lemma B.1. It follows




                                                  57
that

       c0 XR br
               θ − θ0∗r
       2   r=1
                                                                    v
          J    R                                           J √ u        R
       1 XX                                           1  X          uX                                     2
  ≤              kgj (·, β r )kL2,N θbr − θ0∗r ≤                 Rt        kgj (·, β r )k2L2,N θbr − θ0∗r
       J                                              J
         j=1 r=1                                         j=1           r=1
       s
                           J
         ξmin (R) √ 1 X
  ≤                R            µθb(j, xi ) − µθ0∗ (j, xi ) L
            4ζ02      J
                          j=1
                                                               2,N

       s
                           J
         ξmin (R) √ 1 X                                                                                         
  ≤                R              µ   (j, x i ) − P (j|x  , f
                                                        i 0 L )         +  µ  ∗
                                                                             θ0 (j, x i ) −  P (j|x   , f
                                                                                                     i 0 L)
            4ζ02      J
                          j=1
                                    θb                             2,N                                       2,N

       s                v                                                 v                                               
                                 J                                                J
         ξmin (R) √ u
                         u                                                 u
                              1 X                                          u1 X
  ≤                R t                ||µθb(j, ·) − P (j|·, f0 )||2L2,N + t            ||µθ0∗ (j, ·) − P (j|·, f0 )||2L2,N 
            4ζ02              J
                                  j=1
                                                                              J
                                                                                     j=1


where the second inequality holds by RCS, the third inequality holds similarly with (33), the
fourth inequality holds by the triangle inequality, and the last inequality is due to the Cauchy-
Schwarz inequality. Therefore,
                           qfrom Lemma      4.1 and the first result of Theorem 4.2 with our
                               log(R(N )N )
choice of R(N ) and ηN = O          N         (so that the variance term balances with the bias
term), it follows that

       R
                                           r                             !                      r                  !
       X                                       R(N ) · log(R(N )N )                                 log(R(N )N )
             θbr − θ0∗r
                             p
                          =O  R(N )                                          = O R(N )                                 .
                                                         N                                               N
       r=1


B.5     Proof of Theorem 4.3
                                                                       PR      ∗r     r
Define a pseudo true distribution such that FR (β) =                      r=1 θ0 1 [β       ≤ β]. It is not difficult to
see that
                                                       XR                            XR
             sup Fb (β) − FR (β)          =     sup              θbr 1 [β r ≤ β] −            θ∗r 1 [β r   ≤ β]
             β∈B0                              β∈B0        r=1                             r=1 0
                                                       XR                  
                                          =     sup               θbr − θ0∗r 1 [β r ≤ β]
                                               β∈B0        r=1
                                                                                           r                   !
                                               XR                                              log (R(N )N )
                                          ≤               θbr − θ0∗r = OP      R(N )
                                                    r=1                                              N



                                                            58
where the last inequality holds by the triangle inequality and the last equality holds by Theorem
4.2. Note that                               Z
                                  F0 (β) =       f0 (b)1 [b ≤ β] db

and FR (β) becomes a quadrature approximation. We use a similar strategy with the proof of
                                           L
                                              af,l ϕl (b) such that supβ∈B0 f (b) − L
                                          P                                        P
Lemma 4.1 where we approximate f (b) with                                           l=1 af,l ϕl (b) =
                                          l=1
O L−s/K due to Timan (1963). We find
         

                                                    
                            F0 (β) − FR (β) = O R−s/K a.e. β ∈ B0 .

We conclude

         FbN (β) − F0 (β)    ≤ FbN (β) − FR (β) + |FR (β) − F0 (β)|
                                          r              !
                                            log(R(N )N )           
                             = OP R(N )                    + O R−s/K a.e. β ∈ B0 .
                                                 N




                                                  59
References
 [1] Ackerberg, Daniel A. (2009). “A New Use of Importance Sampling to Reduce Computational
    Burden in Simulation Estimation.” Quantitative Marketing and Economics.

 [2] Akhiezer, N.I. (1965), The classical moment problem and some related questions in analysis,
    Oliver & Boyd.

 [3] Aliprantis, C.D. and K.C. Border (2006), Infinite Dimensional Analysis: A Hitchhiker’s
    Guide, Springer.

 [4] Amemiya, T. (1983), “Non-Linear Regression Models,” Handbook of Econometrics I, edited
    by Z. Griliches and M.D. Intrilligator, 333-389.

 [5] Andrews, D.K.W. (1994), “Empirical Process Methods in Econometrics,” Handbook of
    Econometrics IV, edited by R.F. Engle and D.L. McFadden, 2247-2294.

 [6] Andrews, D.K.W. (2000), "“Inconsistency of the Bootstrap When a Parameter Is on the
    Boundary of the Parameter Space,” Econometrica, 68, 399-405.

 [7] Andrews D.K.W. (1999), “Estimation When a Parameter is on a Boundary,” Econometrica
    67, 1341-1383.

 [8] Andrews D.K.W. (2002), “Generalized Method of Moments Estimation When a Parameter
    is on a Boundary,” Journal of Business & Economics Statistics 20-4, 530–544.

 [9] Andrews D.K.W and P. Guggenberger (2009a), “Hybrid and Size-corrected Subsample Meth-
    ods,” Econometrica, 77.

[10] Andrews, D.K.W and P. Guggenberger (2009b), “Applications of Subsampling, Hybrid, and
    Size-Correction Methods,” Yale working paper.

[11] Andrews, D.K.W and P. Guggenberger (2010), “Asymptotic Size and a Problem with Sub-
    sampling and the m Out of n Bootstrap”, Econometric Theory, 26.

[12] Bajari, Patrick, Jeremy T. Fox, Kyoo il Kim and Stephen P. Ryan, “The Random Coefficients
    Logit Model Is Identified”, NBER working paper.

[13] Baraud, Y. (2002), “Model Selection for Regression on a Random Design”, ESAIM Proba-
    bility & Statistics 7, 127-146

                                              60
[14] Berry, S and Haile, P. (2008), “Nonparametric Identification of Multinomial Choice Models
    with Heterogeneous Consumers and Endogeneity”, working paper.

[15] Berry, S., J. Levinsohn, and A. Pakes (1995), “Automobile Price in Market Equilibrium”,
    Econometrica (63), July 1995.

[16] Biernacki, Christophe, Gilles Celeux and Gérard Govaert (2003), “Choosing starting values
    for the EM algorithm for getting the highest likelihood in multivariate Gaussian mixture
    models,” Computational Statistics & Data Analysis, 41, 561–575.

[17] Billingsley, Patrick (1995), Probability and Measure, 3rd Edition.

[18] Böhning, D. “Convergence of Simar’s Algorithm for Finding the Maximum Likelihood Esti-
    mate of a Compound Poisson Process”, The Annals of Statistics, 10(3), 1006–1008. 1982.

[19] Briesch, R.A., P.K. Chintagunta, and R.L. Matzkin (2007), “Nonparametric Discrete Choice
    Models with Unobserved Heterogeneity”, SMU working paper.

[20] Burda, Martin, Matthew Harding and Jerry Hausman (2008), “A Bayesian Mixed Logit-
    Probit for Multinomial Choice Demand Models”, University of Toronto working paper.

[21] Chen, X. (2007), “Large Sample Sieve Estimation of Semi-Nonparametric Models,” Handbook
    of Econometrics VI, Elsevier.

[22] Chen, X. and D. Pouzo (2009a), "Estimation of Nonparametric Conditional Moment Models
    With Possibly Nonsmooth Moments", Yale working paper.

[23] Chen, X. and D. Pouzo (2009b), "Efficient Estimation of Semiparametric Conditional Mo-
    ment Models with Possibly Nonsmooth Residuals", Journal of Econometrics, forthcoming.

[24] Chiappori, Pierre André and Ivana Komunjer, “On the Nonparametric Identification of
    Multiple Choice Models”, UCSD working paper.

[25] Cramér, H. and H. Wold (1936), “Some Theorems on Distribution Functions”, Journal of
    the London Mathematical Society, s1-11(4), 290–294.

[26] Dempster, A.P., N.M. Laird and D.B. Rubin (1977), "Maximum likelihood from incomplete
    data via the EM algorithm", Journal of the Royal Statistical Society, 39, 1, 1-38.




                                               61
[27] D’Haultfoeuille, Xavier (2009), “On the Completeness Condition in Nonparametric Instru-
    mental Problems”, Econometric Theory.

[28] Devroye, Luc, and Laszlo Gyorfi (1985), Nonparametric Density Estimation, The L1 View,
    New York, Wiley.

[29] Duflo, Esther, Rema Hanna and Stephen P. Ryan (2008), "Monitoring Works: Getting
    Teachers to Come to School", MIT working paper.

[30] Fox, Jeremy T. and Amit Gandhi (2009a), “Identifying Heterogeneity in Economic Choice
    Models”, NBER working paper.

[31] Fox, Jeremy T. and Amit Gandhi (2009b), “Full Identification in the Generalized Selection
    Model”, University of Chicago working paper.

[32] Gautier, Eric and Kitamura, Yuichi. (2008), “Nonparametric Estimation in Random Coeffi-
    cients Binary Choice Models”, CREST working paper.

[33] Geweke, J. (1986), “Exact Inference in the Inequality Constrained Normal Linear Regression
    Model,” Journal of Applied Econometrics, Vol. 1, No. 2, pp. 127-141.

[34] Golub, GH, M. Heath and G. Wahba (1979), “Generalized cross-validation as a method for
    choosing a good ridge parameter,” Technometrics, 215–223.

[35] Heckman, J. and Singer, B. “Method for Minimizing the Impact of Distributional Assump-
    tions in Econometric Models for Duration Data”, Econometrica 52(2), 271-320 .

[36] Hoderlein, Stefan, Jussi Klemelä and Enno Mammen (2008), “Analyzing the Random Co-
    efficient Model Nonparametrically”, Brown University working paper.

[37] Hoeffding, W. (1963), “Probability Inequalities for Sums of Independent Random Variables”,
    Journal of the American Statistical Association, 58, 13-30.

[38] Huber, J. (2004) Robust Statistics, Wiley.

[39] Ichimura, Hidehiko and T. Scott Thompson (1998), “Maximum likelihood estimation of a
    binary choice model with random coefficients of unknown distribution,” Journal of Econo-
    metrics, 86(2), 269–295.

[40] Judd, K. L. (1998), Numerical Methods in Economics. MIT Press.

                                                  62
[41] Judge, G.G. and Takayama, T. (1966), “Inequality Restrictions in Regression Analysis”,
    Journal of the American Statistical Association, Vol. 61, No. 313, pp. 166-181.

[42] Kamakura, W.A. (1991), “Estimating flexible distributions of ideal-points with external
    analysis of preferences”, Psychometrika, 56, 3, 419-431.

[43] Karlis, Dimitris and Evdokia Xekalaki (2003), “Choosing initial values for the EM algorithm
    for finite mixtures”, Computational Statistics & Data Analysis, 41, 577–590.

[44] Krein, M.G. and A.A. Nudel’man (1973, translation 1977), The Markov moment problem
    and extremal problems: ideas and problems of P. L. Čebyšev and A. A. Markov and their fur-
    ther development. Translations of mathematical monographs v. 50, American Mathematical
    Society, Providence.

[45] Krommer, A.R. and C.W. Ueberhuber (1998), Computation Integration, SIAM.

[46] Laird, Nan (1978), “Nonparametric Maximum Likelihood Estimation of a Mixing Distribu-
    tion”, Journal of the American Statistical Association, Vol. 73, No. 364, pp. 805–811.

[47] Lehmamm, E.L. and J.P. Romano (2005), Testing Statistical Hypotheses, 3rd Ed. Springer.

[48] Lewbel, Arthur. (2000) “Semiparametric qualitative response model estimation with un-
    known heteroscedasticity or instrumental variables”, Journal of Econometrics, 97, 1, 145-
    177.

[49] Li, Jonathan Q. and Andrew R. Barron (2000), “Mixture density estimation”, Advances in
    Neural Information Processing Systems, Vol. 12, pp. 279–285.

[50] Liew, C.K. (1976), “Inequality Constrained Least-Squares Estimation”, Journal of the Amer-
    ican Statistical Association, Vol. 71, No. 355, pp. 746-751.

[51] Lindsay, B.G. (1983) “The Geometry of Mixture Likelihoods: A General Theory”, The
    Annals of Statistics, 11(1), 86–94.

[52] Manski, Charles F. (1975) “Maximum Score Estimation of the Stochastic Model of Choice”,
    Journal of Econometrics, 3(3), 205–228.

[53] McFadden, Daniel and Kenneth Train (2000), “Mixed MNL models for discrete response”,
    Journal of Applied Econometrics, 15(5): 447–470.


                                               63
[54] McLachlan, G.J. and D. Peel (2000), Finite Mixture Models. Wiley.

[55] Nevo, Aviv. 2001, “Measuring Market Power in the Ready-to-Eat Cereal Industry”, Econo-
    metrica, 69(2): 307–342.

[56] Newey, W.K. (1997), “Convergence Rates and Asymptotic Normality for Series Estimators”,
    Journal of Econometrics 79, 147-168.

[57] Parthasarathy, K.R. (1967), Probability Measures on Metric Spaces, Academic Press.

[58] Petrin, Amil (2002), “Quantifying the Benefits of New Products: The Case of the Minivan”,
    Journal of Political Economy, 110:705-729, 2002.

[59] Petrin, Amil and Kenneth Train (2009), “Control Function Corrections for Omitted At-
    tributes in Differentiated Products Markets”, Journal of Marketing Research.

[60] Pilla, Ramani S. and Bruce G. Lindsay (2001), “Alternative EM methods for nonparametric
    finite mixture models”, Biometrika, 88, 2, 535–550.

[61] Quandt, R.E. and Ramsey, J.B. (1978), "Estimating Mixtures of Normal Distributions and
    Switching Regressions," Journal of the American Statistical Association, 73, 364, 730-738.

[62] Rossi, Peter E., Greg M. Allenby, and Robert McCulloch (2005), Bayesian Statistics and
    Marketing. West Sussex: John Willy & Sons.

[63] Rust, John (1987), “Optimal Replacement of GMC Bus Engines: An Empirical Model of
    Harold Zurcher”, Econometrica, 55(5): 999–1033.

[64] Rust, John (1994), “Structural Estimation of Markov Decision Processes”, in Handbook of
    Econometrics, vol. 4, edited by Robert F. Engle and Daniel L. McFadden. Amsterdam:
    North-Holland.

[65] Rust, John (1997), “Using randomization to break the curse of dimensionality”, Economet-
    rica, 65, 3, 487–516.

[66] Seidel, Wilfried, Karl Mosler and Manfred Alker (2000), “A cautionary note on likelihood
    ratio tests in mixture models”, Annals of the Institute of Statistical Mathematics, 52, 3,
    418-487,




                                              64
[67] Shohat, J.A. and Tamarkin, J.D. (1943), The Problem of Moments, American Mathematics
    Society, Providence, RI.

[68] Teicher, H. (1963), “Identifiability of Finite Mixtures”, Annals of Mathematical Statistics,
    34, 1265-1269.

[69] Tibshirani, R. (1996), “Regression shrinkage and selection via the Lasso”, Journal of the
    Royal Statistical Society, Series B. 58, 267-288.

[70] Timan, A.F. (1963), Theory of Approximation of Functions of a Real Variable, MacMilan,
    New York.

[71] Van der Vaart, W. and J.A. Wellner (1996), Weak Convergence and Empirical Processes,
    Springer Series in Statistics, New York.

[72] Verbeek, J.J., N. Vlassis and B. Kröse (2003), “Efficient Greedy Learning of Gaussian Mix-
    ture Models”, Neural Computation, Vol. 15, pp 469–485.

[73] Wolak, F. “Testing inequality constraints in linear econometric models”, Journal of Econo-
    metrics, Elsevier, vol. 41(2), pages 205-235, June 1989.

[74] Yakowitz, S.J. and Spragins, J.D. (1968) "On the identifiability of finite mixtures", The
    Annals of Mathematical Statistics, pages 209-214.

[75] Zolotarev,   V.M. (2001),    “Lévy-Prokhorov metric”,     Encyclopaedia of Mathematics,
    Hazewinkel, M. (Ed.), Kluwer.




                                               65
Figure 1: True, Estimated, and 95% Sampling Intervals for Marginal Densities from the Monte
Carlo Experiment

          0.5
                                                                                   0.5
                     True                                                                     True
         0.45        Fitted                                                       0.45        Fitted
                     5th percentile                                                           5th percentile
          0.4        95th percentile                                               0.4        95th percentile

         0.35                                                                     0.35


          0.3                                                                      0.3


         0.25                                                                     0.25


          0.2                                                                      0.2


         0.15                                                                     0.15


                                                                                   0.1
          0.1

                                                                                  0.05
         0.05

                                                                                    0
           0                                                                             −5     −4        −3    −2   −1   0    1   2   3   4   5
                −5     −4        −3    −2   −1   0    1   2   3   4   5
                                                                                                                          β2
                                                 β1




     Density of β1 : Two Normal Mixtures                                       Density of β2 : Two Normal Mixtures


          0.5                                                                      0.5
                     True                                                                     True
         0.45        Fitted                                                       0.45        Fitted
                     5th percentile                                                           5th percentile
          0.4        95th percentile                                               0.4        95th percentile

         0.35                                                                     0.35


          0.3                                                                      0.3


         0.25                                                                     0.25


          0.2                                                                      0.2


         0.15                                                                     0.15


          0.1                                                                      0.1


         0.05                                                                     0.05


           0                                                                        0
                −5     −4        −3    −2   −1   0    1   2   3   4   5                  −5     −4        −3    −2   −1   0    1   2   3   4   5
                                                 β1                                                                       β2




     Density of β1 : Four Normal Mixtures                                      Density of β2 : Four Normal Mixtures


          0.5                                                                      0.5
                     True                                                                     True
         0.45        Fitted                                                       0.45        Fitted
                     5th percentile                                                           5th percentile
          0.4        95th percentile                                               0.4        95th percentile

         0.35                                                                     0.35


          0.3                                                                      0.3


         0.25                                                                     0.25


          0.2                                                                      0.2


         0.15                                                                     0.15


          0.1                                                                      0.1


         0.05                                                                     0.05


           0                                                                        0
                −5     −4        −3    −2   −1   0    1   2   3   4   5                  −5     −4        −3    −2   −1   0    1   2   3   4   5
                                                 β1                                                                       β2




      Density of β1 : Six Normal Mixtures                                      Density of β2 : Six Normal Mixtures




                                                                          66
                  Figure 2: True and Estimated Joint Densities from the Monte Carlo Experiment


                                                                                                0.3
            0.3

           0.25                                                                                0.25

            0.2                                                                                 0.2




                                                                                    f(β1,β2)
f(β1,β2)




           0.15                                                                                0.15

            0.1                                                                                 0.1

           0.05                                                                                0.05

             0                                                                                   0
            10                                                                                  10
                    5                                                      6                            5                                                   6
                                                                       4                                                                                4
                          0                                        2                                        0                                       2
                                                               0                                                                                0
                                −5                   −2                                                          −5
                                                −4                                                                                    −2
                                     −10   −6                                                                                    −4
                           β2                                                                                         −10   −6
                                                          β1                                                β2                             β1

                    Two Normal Mixtures: Truth                                                        Two Normal Mixtures: Estimated




            0.3                                                                                 0.3

           0.25                                                                                0.25

            0.2                                                                                 0.2
f(β1,β2)




                                                                                    f(β1,β2)




           0.15                                                                                0.15

            0.1                                                                                 0.1

           0.05                                                                                0.05

             0                                                                                   0
            10                                                                                  10
                    5                                                      6                            5                                                   6
                                                                       4                                                                                4
                          0                                        2                                        0                                       2
                                                               0                                                                                0
                                −5                   −2                                                          −5                   −2
                                                −4                                                                               −4
                                     −10   −6                                                                         −10   −6
                           β2                             β1                                                β2                             β1


                   Four Normal Mixtures: Truth                                                        Four Normal Mixtures: Estimated




            0.3                                                                                 0.3

           0.25                                                                                0.25

            0.2                                                                                 0.2
f(β1,β2)




                                                                                    f(β1,β2)




           0.15                                                                                0.15

            0.1                                                                                 0.1

           0.05                                                                                0.05

             0                                                                                   0
            10                                                                                  10
                    5                                                      6                            5                                                   6
                                                                       4                                                                                4
                          0                                        2                                        0                                       2
                                                               0                                                                                0
                                −5                   −2                                                          −5                   −2
                                                −4                                                                               −4
                                     −10   −6                                                                         −10   −6
                           β2                             β1                                                β2                             β1


                        Six Normal Mixtures: Truth                                                    Six Normal Mixtures: Estimated




                                                                               67
              Figure 3: Estimated Distributions of Heterogeneity for the Benefits of Staying Home
                                                       R=5                                                                                                           R = 10
            1.0                                                                                                            1.0


            0.9                                                                                                            0.9


            0.8                                                                                                            0.8


            0.7                                                                                                            0.7


            0.6                                                                                                            0.6
Frequency




                                                                                                               Frequency
            0.5                                                                                                            0.5


            0.4                                                                                                            0.4


            0.3                                                                                                            0.3


            0.2                                                                                                            0.2


            0.1                                                                                                            0.1


            0.0                                                                                                            0.0
                  -2.5   -2.0   -1.5   -1.0   -0.5   0.0   0.5    1.0   1.5   2.0     2.5   3.0   3.5   4.0                      -2.5   -2.0   -1.5   -1.0   -0.5   0.0   0.5    1.0   1.5   2.0     2.5   3.0   3.5   4.0
                                                            Theta                                                                                                          Theta

                                       Mean      5th percentile     95th percentile                                                                   Mean      5th percentile     95th percentile


                                                      R = 20                                                                                                         R = 40
            1.0                                                                                                            1.0


            0.9                                                                                                            0.9


            0.8                                                                                                            0.8


            0.7                                                                                                            0.7


            0.6                                                                                                            0.6
Frequency




                                                                                                               Frequency




            0.5                                                                                                            0.5


            0.4                                                                                                            0.4


            0.3                                                                                                            0.3


            0.2                                                                                                            0.2


            0.1                                                                                                            0.1


            0.0                                                                                                            0.0
                  -2.5   -2.0   -1.5   -1.0   -0.5   0.0   0.5    1.0   1.5   2.0     2.5   3.0   3.5   4.0                      -2.5   -2.0   -1.5   -1.0   -0.5   0.0   0.5    1.0   1.5   2.0     2.5   3.0   3.5   4.0
                                                            Theta                                                                                                          Theta

                                       Mean      5th percentile     95th percentile                                                                   Mean      5th percentile     95th percentile




                                                                                                              68
Figure 4: Predicted Distribution of Days Worked in Months Where First Five Days Used for
Estimation
                                              R=5                                                                                         R = 10
              0.250                                                                                        0.250


              0.225                                                                                        0.225


              0.200                                                                                        0.200


              0.175                                                                                        0.175


              0.150                                                                                        0.150
  Frequency




                                                                                               Frequency
              0.125                                                                                        0.125


              0.100                                                                                        0.100


              0.075                                                                                        0.075


              0.050                                                                                        0.050


              0.025                                                                                        0.025


              0.000                                                                                        0.000
                   0.0   2.5   5.0   7.5   10.0    12.5     15.0   17.5   20.0   22.5   25.0                    0.0   2.5   5.0   7.5   10.0    12.5     15.0   17.5   20.0   22.5   25.0
                                                  Days Worked                                                                                  Days Worked

                                     Best Model Fit       Actual Data                                                             Best Model Fit       Actual Data


                                             R = 20                                                                                       R = 40
              0.250                                                                                        0.250


              0.225                                                                                        0.225


              0.200                                                                                        0.200


              0.175                                                                                        0.175


              0.150                                                                                        0.150
  Frequency




                                                                                               Frequency




              0.125                                                                                        0.125


              0.100                                                                                        0.100


              0.075                                                                                        0.075


              0.050                                                                                        0.050


              0.025                                                                                        0.025


              0.000                                                                                        0.000
                   0.0   2.5   5.0   7.5   10.0    12.5     15.0   17.5   20.0   22.5   25.0                    0.0   2.5   5.0   7.5   10.0    12.5     15.0   17.5   20.0   22.5   25.0
                                                  Days Worked                                                                                  Days Worked

                                     Best Model Fit       Actual Data                                                             Best Model Fit       Actual Data




                                                                                               69
Figure 5: Predicted Distribution of Days Worked Under Out-of-Sample Compensation Scheme
                                                 R=5                                                                                             R = 10
             0.250                                                                                            0.250


             0.225                                                                                            0.225


             0.200                                                                                            0.200


             0.175                                                                                            0.175


             0.150                                                                                            0.150
 Frequency




                                                                                                  Frequency
             0.125                                                                                            0.125


             0.100                                                                                            0.100


             0.075                                                                                            0.075


             0.050                                                                                            0.050


             0.025                                                                                            0.025


             0.000                                                                                            0.000
                  0.0   2.5   5.0      7.5    10.0    12.5   15.0   17.5     20.0   22.5   25.0                    0.0   2.5   5.0      7.5    10.0    12.5   15.0   17.5     20.0   22.5   25.0
                                                     Days Worked                                                                                      Days Worked

                                    Best Out-of-Sample Fit     Actual Data                                                           Best Out-of-Sample Fit     Actual Data


                                                R = 20                                                                                           R = 40
             0.250                                                                                            0.250


             0.225                                                                                            0.225


             0.200                                                                                            0.200


             0.175                                                                                            0.175


             0.150                                                                                            0.150
 Frequency




                                                                                                  Frequency




             0.125                                                                                            0.125


             0.100                                                                                            0.100


             0.075                                                                                            0.075


             0.050                                                                                            0.050


             0.025                                                                                            0.025


             0.000                                                                                            0.000
                  0.0   2.5   5.0      7.5    10.0    12.5   15.0   17.5     20.0   22.5   25.0                    0.0   2.5   5.0      7.5    10.0    12.5   15.0   17.5     20.0   22.5   25.0
                                                     Days Worked                                                                                      Days Worked

                                    Best Out-of-Sample Fit     Actual Data                                                           Best Out-of-Sample Fit     Actual Data




                                                                                                  70
Table 1: Market-Share Approximations and the Number of Positive Weights, for Monte Carlo
            Design                   RMSE                # of positive weights
                            Mean       Min      Max      Mean Min          Max
        Normal Mixture 2 0.000096 0.000042 0.000185        32    17         73

       Normal Mixture 4   0.000123   0.000045   0.000227   70     36      125

       Normal Mixture 6   0.000080   0.000043   0.000130   99     62      154




                                           71
