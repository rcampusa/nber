                       NBER WOR1NG PAPER SERIES

               The Maxirrnm Likelihx)d and the Nonlinear
                 TI-ree-Stage Least Squares Estimator
                in the General Nonlinear Simultaneous
                          tuation Model
                          Take shi P2fleIPiya*


                         Working Paper No. 90


p

     COMPUTER RESEARCH CENTER FOR ECONOMICS AND MANAGEMENT SCIENCE
             National Bureau of Econic Research, Inc.
                        575 Technology Square
                    Cambridge, Massachusetts 02139

                                June 1975

                     Preliminary: not for quotation
    NBER working papers are distributed informally and in limited
    numbers for connents only. Tl-y should not be quoted without
    written permi ssion.
    This report has not undergone the review accorded official NBER
    publications; in particular, it has not yet been su]initted for
    approval by the Board of Directors.
    *Stariford University and NBER Computer Research Center
     Research supported by NSF Grant DCR 70_03L56 A04 to
    the National Bureau of Economic Research, Inc.
                             Abstract


      The consistency and the asymptotic normality of the
maximum likelihood estintor in the general nonlinear
simultaneous equation model are proved. It is shown that
the proof depends on the assumption of normality unlike in
the linear simultaneous equation model. It is proved that
the maximum likelihood estimator is asymptotically more
efficient than the nonlinear three-stage least squares
estimator if the specification is correct, However, the
latter has   the advantage of being     consistent   even when the

normality assumption is removed. Hausrnan' s instrumental-variable-

interpretation   of the maximum likelihood estimator is       extended to

the   general nonlinear siiailtaneous equation liDdel.




                          Acknowledgement


      The author had stimulating discussions with David Belsley,
Michio   Hatanaka,   and Jerry Hausman.




                                                                            I
                              Contents


    1. Inoduction
    2. Model
    3. Mxirnum Likelil-ood Estimator
    tj   Iterative Meti-ods                            13

    5. Nonlinear Three-Stage Least Squares Estimator   17

    6. Conclusions                                     24

    References                                         25

    Appendix                                           Al




p
1. Introduction
     In this paper we obtain the asymptotic properties of the maximum
likelihood estimator in the general nonlinear simultaneous equation
model and compares them with those of the nonlinear three-stage least
squares estimator. The main results of the paper are the following:
    1) The proof of the consistency and the asymptotic normality
         of the maximum likelihood estimator in the general nonlinear
         simultaneous equation model crucially depends on the
         assumption of normality of the error term unlike in the
         linear case.
    2) All the th±od-order derivatives can be asymptotically
         ignored either in the iterative method for obtaining
         the maxihjrn likelihood    estimator or in the computation

         of    the asymptotic variarlce-covariance matrix.
    3) The maximim likelihood estimator is asymptotically more
         efficient than the nonlinear three-stage least squares
         estimator.
    L)   Hausmari's   iteration method for the computation of the
         maximum likelihood estimator in the linear case
         (see   Hausman   [1975]) is generalized to the nonlinear case
         Unlike in the    linear case,   it does not produce   an asymptotically
         efficient second-round estimator even if the initial estimator

         is consistent, but, like in the linear case, it illustrates

         the   similarity and   the difference between the maximum likelihood
         and   the nonlinear three-stage least squares estimator.
                                    —2—




2.   Model

       We will consider the nonlinear simultaneous equation model defined

by the following system of n equations:




                f1(y,x,a1) u., i1,2,.. . ,n                                     (2.1)




where     is a n-dimensional vector of endogenous variables, x a

vector of exogenous variables, and        is a vector of unl<nown parameters.

Not all of the elements of vectors      and x may actually appear in the

arguments of each        Define a n-dimensional vector u as (uit ,u2,       .




Then we assume {ut} is independently and identically distributed as

rruitivariate N(O,2). We assume that there are no constraints among CLs, but

the results we subsequently obtain are not affected by the removal of

this assumption as we will show at the end of Section 5. We assume either

that f1 defines a one-to-one mapping between y and Ut or that the researcher

can apriori specify a prticular root of y for a given value of u so

that   the density of   can   be obtained by the usual way as the product

of the Jacobian arid the density of u. Finally we assume that all the partial

derivatives of f1 with respect to      and y that appear in equation (3.5)
                                                              T
in Section 3 exist and are continuous and that -4 and             f f'
                                                                  t t

where f                 , are   norisingular, These assumptions enable us to

define the maximum likelihood estimator. The other conditions needed

for the consistency and asymptotic normality of the maximum likelihood

estimator are given in Section 3.
                                                     —3—



P   3.    Maximum Likelihood Estimator
            Because   of the basic assumptions of Section 2, we can write the

    logarithmic   likelihood function as

                                                                          f-4i I -
                       L*       -       log          +
                                                         T
                                                         E
                                                         t=l
                                                               log    I
                                                                           Yt
                                                                                          T
                                                                                          E
                                                                                         ti    f       t       (3.1)


    where we defined                             .
                                                     ,f)'       .    Equating the partial derivatives
    of L* with respect to to Zero, we obtain



                       a——-
                          T
                                       Eftt
                                          f'                                                                   (32)

                                          T
    where we will abbreviate E as E from now on. Putting (3.2) into
                                         t=1


    (3.1)   we obtain the concentrated likelihood function



                       L       E log I    II I -             log     IT     E                      .           (3.3)


    We define a vector                           and a matrix


    We will write the partial derivatives of L using these symbols below.

    To avoid the excessive subscripts, we will omit the subscript t from f,

    y, u, and g whenever they appear inside the suiunation. We have


                                        3g1    - T E g. f'(Eff'):'
                                   E                                                                           (3.4)
                       act.
                           1            3u.           1
                                          1

    where we used
                       ag.
                               =
                                       ag. 1fi'- and. wrote (
                                       —4 I—,-                                  )
                                                                                    1   for   the 1th column
                       au              ay

    of   the inverse of the matrix within the bracket. We have
                                                     —Lb—




                                      E              - T Z g.. f'(Eff')T1
                              1]              3u.
                                                1               ij     1

                                      ag.
                                -                    - T   (Eff') E g1 g
                                          J      1                   1]
                                                                                                     (3.5)
                                + T E g1 f'cEff'):'
                                                           J
                                                                 (ff')1
                                                                      I
                                                                            E fg
                                                                                I

                                +   T(Eff)           E g.
                                                        1
                                                          f,(EffYa            fg!
                                                                                1




where we used ::
                                    : [!1                  and wrote C )       for   the 1, th


element         of   the inverse of the matrix within the bracket.

           We   define   the maximum          likelihood       estimator of a as a root   of
equation                      0. Given assumptions A through E in the appendix,

one of the roots is consistent and if we denote the consistent root by
I.'



a     ,   we have




                                                           plim                                 .    (3.6)



The proof is given In the appendix. The above result is of course not

a surprising           one.    Our   main reason for writing down the assumptions
explicitly Is that checking some of these conditions, especially B and

E,        is instructive in our model: It will show that the consistency proof

depends crucially on the normaility assumption and                           that the   terms

involving            g   in (i.l..5) can      asymptotically         be ignored. Also, it will aid

                                                                                                             I
                                         —5—




  us later when we compare the maximum likelthood estimator with the nonlinear
  three-stage least squares estimator.
          We will consider each of the assumptions in the appendix arid
  indicate what conditions on the function f are implied by each. We
  will not make a great effort to find the minimum set of assumptions needed
  on f since that is not likely to be a useful exercise. As it was stated
  earlier, assumptions B and E are most interesting to verify and we
  will   devote    most of our time on   their   verification. But since assumption
  C requires the greatest number of conditions on f, we will           state a sweeping
  set   of conditions on f to make assumption C satisfied. After this is done,

  only a small number of additional conditions is needed to satisfy the

  remaining four assumptions. Thus we assume

  Condition_1. The probability limit of T1 times every surimation that

  occurs    in   the right-hand side of (3.5) is finite and is equal to the limit

  of T1    tines its expectation. Moreover, the convergence is uniform in
  a neighborhood of c.        In addition, pm T1 ff' is        nonsingular.
         Note that the uniform boundedness of the third-order derivatives may

  be    substituted for assumption C.
         Before proceeding further, we will prove the following important lenm
  which will be frequently used.
  Lenua. Suppose u1, u2,. . .u are jointly normal with mean 0 and h(u1 ,u2,.. .u)
                        — are finite. Then, iu1
  is such that E h and EDu.                                    i-i E   Du. a11
                                1                                       1
where   a11 is the covariance between u1 and u..
                                          —6—



Proof. Replaceu.inhwith — u1+w. for i2,...,nandtreathas

a   function of u1, w2, w3,..., w. Then, Ehu1 EE hu1 where

w    (w2,. ..   ,w).    But using integration by parrts we have




                                 I
                       Ehui               hu1   du1

                                                                                          (3.7)
                                     2                200dh
                                - a1 [h]00 +                       4
                                                                       du1



where    is the density of NC 0 ,a). But the first term                 of the rht-hand
side of (3.7) is zero because Eh is finite. Note                          p—
                                                                   du1 il u1 a2
                                                                                                  I
Therefore,      taking the expectation of both sides of (3.7) with

respect to w, we get the desired result.

      Now we will consider assumption B. Using (3L) we have



                   -k-                   -I-          - g.1   u'
                   1T ct1                                          J
                                0
                                                                                          (3.8)




where a' is the th column of Q. We irranediately see that the mean

of the first term of the right-hand side of (3.8) is zero since g1

satisfies the condition of the lenana because of condition 1. Also

using the leimia we have
                                                    —7—

                                                                       ag.
                        plim -           E   g u'        urn    E   E •-4 c                          (3.9)


    since         u} satisfies the conditions for a law of large numbers

    because     of conditon 1. Therefore, denoting the equivalence of the limit

    distribution by the symbol j, we have




                        '
                                               - il        +
                                                               i2                                    (3.10)




    where


                                                lag.                   .1
                            p.ii              E !—-.       - g.1   u' a'l                            (3.11)
                                                L'i


                                                     ag.
                            p. = lim          EF               —-— E (uu' -   c?)a' .                (3.12)
                             i2                      u


    Written     thus,   it is clear that            a certain essential       boundedness of g1

          ag.
    and          is   sufficient to let (3.10) follow a central limit theorem.


    For exanle, the following condition is certainly sufficient:


                                   3
    Condition 2. Egl and E                                 are uniformly bounded        for all t,
                                                    it

    where   g.itand
                        ag.
                             it        are evaluated at c.o
                        au.  it

I
                                          —8—


                                                                                                        I
        Next we   will verify assumption E,        Ta)dng the      probability limit
of T1    times (3.5) evaluated at c, we have




                   plim   T'      jr        plim T1 E [:                 - g1    u' o]


                                           g•
                           - pun T              ---J-
                                                        -   o plim T
                                            j      1
                                                                                               (3.13)

                           +   plan T-l        ji'
                                           g.1 ua a           •      .
                                                                  plan T
                                                                        —l
                                                                             ug.
                                                                                 J

                           +   a pun T1 E g1 u' •                  • plim   T1       ug


                                                                                                        I
Because of condition 1, we can replace plim in the right-hand side of

(3.13) with him E. But, then, the first term drops out provided g.
                                                                                          1]
satisfies   the condition of the leia.           So we impose

—                    ijt
Condition_3. E g.. is finite, where g.. is evaluated at a
                                                 ijt                  0
Thus,   either in performing       Newton's iteration to obtain the rnaximiur likelihood
estinator   or in Obtaining its asymptotic variance-covariance matrix

one   need not compute g...      Also we can apply the result of the lemma to

each term involving the product of u and g1. Thus we have




                                                                                                        I
                                                                —9—



                                                                                              ag.       g!
                      plim T1           act.aco
                                            1               I
                                                                ct0
                                                                        - urn T1 E E —i
                                                                                              . a.       1



                                       -            Urn T1 E E g1                                                      (3.1L)


                                       +   urn T1 E E                            •    urn T   1EE—-
                                                                                                 au.
                                                                        au5                              1

                                                                                ag.                              ag!
                                       +   a Liiii T1                  E E au
                                                                            —4           •    Urn T1 E E au




                                                                       1    1L I                    I       1
    We   must   compare the   above with                    urn T
                                                                           EL
                                                                                —
                                                                                act1
                                                                                     I        -- I
                                                                                                 jIo
    We ijiipose

    Condition .      E     ag1
                           it              g'       1
                                                                      fite.
                                            j(          I
                       [3u.                     I°J
    Then,   by the repeated application of the lemma, we have

                        r ag.
                       E'
                           au. - g. u'


                                                t    ag.
                                                        all


                                                           ag.    .
                                                                      [       - g'
                                                                                     J   u' a]

                                   --
                                   -                   1—--—--u'a1—o
                                                    au au  Bu.                                      g        g         (3.15)
                                                L                                                       ]
                                                ag.1         ag
                                                             au.
                                                    J             1




    Therefore,     from   (3.11)       and (3.15) we have


I
                                    — 10 —



                                              9g.
                                                      --
                                                      3g!
               Em E p.1 p.1 Em T1 E
                                                                 (3.16)

                        +       urn T1    E g g!




We have



               E (uu' —     Y   o   (uu' —   )lJ        + 9,.    (3.17)


where     is a n-dimensional vector with 1 in the th place and

0 elsewhere. Therefore, from (3.12) and      (3.17)   we have



[Continued on page U]




                                                                          I
                                            — 11      -



                 Urn E p2 p.2 a' Em T1 E E                                             •
                                                                                           Em T1 E E
                                                                                                              (3.18)
                                                            ag.                                ag!
                                + Em T                 E
                                                            au.
                                                                    •    Em     TEE            au.1
                                                                J




By the application of the leiima we have


                     rag.                   .1
                 E          -
                                g1 u' a' a (uu' -
                                              J                                                               (3.19)
                                                      ag.                     ag.
                                       -a'E-—-
                                            au'                               au. 1
                                                                               J


Therefore from   (3.11),    (3.12), and           (3.19)        we have




                 Urn E p11 p.2          -   a'        urn   T1          E E                •   lin T1   E E

                                                                                                              (3.20)

                                 -     Em   T1        E E                •
                                                                             Urn    T1 E E


Similarly we have


                                   •
                                       rag.
                 E(uu'   - Q) a'                  -
                                                      g    u'




                                - a1
                                                   a'                        a'
                                         2 E •cj•ii             £. E                       .                  (3.21)
                                      — 12      —




                                                                                                        I
Therefore we have


                    liin E p.2 p1 urn E p.1 p2 .                                               (3.22)


Finally, assumption     E follows from (3.10), (3, l'4), (3 .16), (3 .18), (3.20),

and (3.22).

      This   leaves   assiunptions A and D.         Assumption A requires only one

additional condition:

               5.                               exists
Condition           plim T1 E log I
                                      ITI   I
                                                         in a neighborhood of


a0.
As we will show in Section 5, assumption D is implied by

                               3f.              af.
Condition_6. him T E                        E             is finite and nonsingular



for every i.

To sum   up,    conditions 1 through 6 imply assumptions A through E in the

appendix.

     Note that the proof of both consistency and asymptotic normality
crucially depends on the normality assumption unlike in the linear case
where the maximum likelihood estiiitor can be              easily   shown to   be consistent

for general specifications      on the error term, This          fact increases     the
usefulness      of such an estimator as the nonlinear two-stage or three-stage

least squares estimator which has been shown to be consistent for general

specifications on the error term.
                                                      - 13   —




4.   Iterative Methods
     Consider the class of gradient nthods of iteration defined by

                      'S       A

                           2
                                                Ct1


where
         a1 is an initial estimator and A is some matrix which may be stochastic.
Using a Taylor expansion of                      .      around     ct, the true value, we have
                                                 Ct1

fran (4.1)


                      V (a2-a0)             -           A           +          -A                       1o    (4.2)
                                                                          [i
          *                         A
where cz lies between c and a0. Suppose that a1 is a consistent estima.tor

of       such that         P   (ct1-ct0)
                                           has a proper           limit       distribution. It is apparent

from (4.2) that        the     asymptotic distribution of the second-round estimator

does not depend        upon        the asymptotic distribution of the                 first-round estimator
if and   only if


                                                              2
                      plirn T A = plim            T'                      .                                  (4.3)
                                                                    a0


Moreover, it is apparent fran (4.2) that in this case the limit distribution
of ,7'    (ct2-ct0)   is the same as that of the maximum li]elihood estimator. We
will call the gradient method satisfying (4.3)                                the efficient Newton   iteration.
                                             — 14 —




       Next consider the iteration that can be derived from                      the   equation

obtained by putting (3.4) equal to zero. We can rewrite the equation as



                   r1        g.                       1       -l F'F).
                         ---- • F'       -       G!       F(T                0                    (4.'.i)

                   LT                                 j

where F' is the nxT na-trix whose i, tth element is f±(y,x,c±) and

is
•
     the matrix
               •
                   whose tthcolumn is ___________
                                                           cti
                                                                 —       .   Define



                    1    1
                             — T       E —4- • F7                                                 (4.5)




and


                                   0         .        .   0


                             o     Gt
                                    2



                             0




Also define f as the (nxT)-dinensional vector obtained by stacking the

columns of F. Then, all the n equations in (4.4) for i1,2,...,n

can   be   combined as




                   G(cr'®I) f            0                                                        (4.7)
                                        — 15 -




where we have written Q for f1F'F. Expanding f(a1) in a Taylor series

   nd ct we    finally obtain the iteration


                 a2   a1 —     ['(c'Qi) GY' G'(cr'QI) f                             (L.8)


where

                           0    .   .   0

                      o


                                                                                    (g)
                      o                 G
                                            n




arid   every variable in the right-hand side of (.8) is evaluated at
ct. Equation (. 8) is the generalization of the formula expounded by
Hausmari E1975] for the linear case.            Note that (-k8) belongs to the
class   of iteration defined by (.1) with A [G'(QØI)Gi

By the application of the lenuiia we can easily show



                 plim T'   G(c2I) G - a'                      lirn T1 E E g1g

                                                                                    (L.10)
                                                        ag.
                               + a' lim T-1         E          •   . lijn T-1   E
                                        — 16 —




By comparing   (3.14)   with (4.10) we see that condition (4.3) is violated.

Thus we conclude that the asymptotic distribution of the second-round

estimator in this iteration depends on the asymptottic distribution of the

initial   estimator   and   is not   asymptotically efficient. Note that       the
result is not changed if [Gi(Q'®I)G] is used instead because its

probability limit can       be shown to be equal to (4.10). Note also that           in
the linear   case   the sum   of   the first term and the third     term of   the

right-hand side of (3.14) is zero         so   that conditon (4.3) gets satisfied.

      Although (4.8) nay not be a good method of iteration, it does serve

a useful pedagogical purpose as Hausman's           linear case   does, for it demonstrates

a   certain similarity between the maximim         likelihood estimator   and the    nonlinear

three-stage least squares estimator.
                                             — 17 —




5.   NonLinear Three-Stage Least Squares Estintor
      Jorgenson arid Laffont [l97Ll] defined the nonlinear three-stage
least squares esthnator (henceforth to be abbreviated as NL3S) arid proved
its consistency and asymptotitc noniiality, extending the result of Amemiya
[1974]       obtained   for the nonlinear two-stage least squares estimator.
They defined the NL3S as the value of a that minimizes


                      f(a)'[c21Q X(x'x) X']f(a)                                            (5.1)


         A
where    c   is   some consistent est:iinate of 2 and X    is   a matrix of exogenous
variables which may not coincide with the exogenous variables that appear
originally in the arguments of f. Its asymptotic variarice-covariance
matrix is given by
                                                                       -i-i
                        p1iinT                    X(X'X)                      .            (5.2)
                                                           x'] aoj


In   this paper we will define the NL3S more generally as the value of

a that minimizes f' A f where A could take any              one   of the following three

forms:




                      A1 =      A S1(SS1Y' S A4       ,                                    (5.3)

                                    A   _-
                      A
                        2
                            —
                            —
                                2'2 2' 2
                                    A
                                                                                           (5.4)
                                                 — 18 —




and



                     A3 =      s3(S3    A s3Y-            S    A                                    (5.5)



where S1, S2, and S3 are matrices of at least asymptotically

nonstochas-tic      variables and   A   = Q
                                                 0 I.         The asymptotic variarice-covariance
matrix is given by

                     r                                    1-i
                         plim T1 --          A
                                                 b-.,                .                          (5.6)
                     L                  a0
                                                        aoj


All   the   three formulations are       equivalent           in the sense that
A1, A2, and    A3   can be made equal by appropriately choosing

Si, S2, and     S3. If we take

                                                 X0. .0
                                                 Ox
                    123                          :


                                                 0               X




all the three are reduced to the Jorgenson-Laffont NL3S. It is apparent

from (5.6)    that   for all A1, 11,         2, 3, its        lower bound is   equal   to
                                            — 19    —




                         r
                             limTE (crl®                     I)                                               (5.7)
                         L

                                                         A                       A
                                                              E-
                ,
The    lower-bound is attained when S1                               ,   S
                                                                             2
                                                                                          E


arid   S3 = E            , where   we are iixlicitly assuming that the a
that appears        in E .-,    must be estimated consistently. We will call the

resulting NL3S estimator where any of these optimal S's is used as the

best nonlinear three-stage least squares estimator (abbreviated as BNL3S).
                                                  af
This is often not a practical estimator because E     is usually difficult

to obtain in explicit form, but the consideration of BML3S is theoretically

useful as it provides something to aim at.

        One can also attain the lower bound (5.7) using the Jorgenson-Laffont

NL3S, but that is possible if and only if the space spanned by the column

vectors of X contains the union of the spaces spanned by the column
                    f.
vectors of      E             for i    1, 2,.. .   ,n.       This necessitates including many
                     i   a
columns in X, which is likely to increase the finite sample variance                                of
the    estimator although it has no affect asymptotically. This is the

disadvantage        of the Jorgenson-Laffont definition compared to the                          definition

of   this paper.
       We will next show that the BNL3S is asymptotically less efficient

than    the   maximum likelihood estixtor. Using the lemma                           we   have
                                       — 20 —




                       —
                       U.
                          - g.1 U
                          1
                                           a1                                  aEg;Eg              .   (5.8)
                 EfL                            1a1'uE}
Similarly   we have


                                       g.
                                                                              =-a   Eg Eg' .           (5.9)
                 EEg..ua1[                       -g u?a
We have



                                                                     E g. E g'                         (5.10)
                 E(E g1       u' a' a1 u E
                                                     g1)       a'1
                                                                              ii     .




We obviously have



                ENuu' -   Q)a' ai' u       E g.]           0                                           (5.11)




and




                 ECE g1       u' a' a1(uu' -          Q)]        0    .                                (5.12)


Therefore, from (3.10), (3.11), (3.12), and. (5.8) through                          (5.12)   we have



                 him E(p.1 +
                                p12
                                      - —-— E g. •              u' a')(p' + pt
                                                                               j1   j2/  -   ——a u E   g4)
                                                                                                        J



                                                                                                               (5.13)

                       limT_1E         I




                                                'I
                                                               iI         I-a 1imTEg1Eg'
                                                                          1



                                       L
                                              - 21   —




The first     term of   the   right-hand side        of (5.13) is the ij         th block
of the inverse of the asyirptotic variarice-covariance matrix of the maximum
likelihood estimator       and   the second term is that of the BNL3S as it is
evident   from (5.7).      But the matrix whose            i-j   th block is given in the
left-hand     side of (5.13) is clearly nonnegative definite. Moreover,

since the matrix is nonzero with probability                     one in   general, we conclude

that   the   BNL3S is asymptotically less efficient than                  the   rrximum

likelihood estimator.

       Although the NL3S is asymptotically less efficient than the maximum

likelihood estimator, it         is    more   robust     against non-normality because it        is
consistent provided the error term has mean zero and certain higher-order
finite moments whereas the concistency of the maximum likelihood estimator
in the nonlinear model depends crucially on the normality assumption as
we have seen in Section 3 above.

       A necessary arid sufficient condition for the matrix to be inverted
in   (5.7) to be nonsingular is easily seen to be condition 6

of Section 3. In the linear case this condition implies the usual rank

condition    of identifiability for each equation. However in                     the nonlinear
case   the above condition is likely to be met               even    if all the exogenous

variables appear in each f1 provided f1 is sufficiently nonlinear.

Because of (5.13), condition 6 implies assumption D of the appendix.

       The Gauss-Newton iteration to obtain the                  BNL3S can be written as


                  a2      a1 —   [G'    (cr1-s I)        GJ G' (clØ I) f                              (5.15)
                                           — 22    —




where
                                                                                                   I
                              G'
                              1
                                 0     .   .   0

                              o                                                           (5.16)


                              o
                                               n



arid




                       1
                           E G!                                                           (5.17)
                              1



Equation (5.15) differs from (Li..8) only in the respective "instrumental

variables" used defined by (5.17) and (Lf.5)              respectively.     Intuitively

speaking, . catches more of the essentially nonstochastic part                  of
than G. does. Note that by a Taylor expansion we have



                           Cut) g (0) +                   u       .                       (5.18)


But (14.5)     can be written     as

                                           1
                           (ut) g + T              au . ut
                                                   ____           .                       (5.19)


The    similarity between (5.18) and (5.19)            provides some justification of

          as   the alternative instrumental variable. The                 that appears
in 1(o)        must   be consistently estimated. The resulting NL3S is
                                                                                                   I
                                        — 23 —




asymptotically         less efficient than   the   BNL3S but is much nore practical.

An even nore practical choice of the instniment is to use

where;      is   calculated simply as the predictor of y obtained by the

linear   least    squares regression of y on all the exogenous variables.

A    definite comparison between this choice and the use             of g (0) can not

be   easily made.
       So far in this paper we have assumed that there are no constraints
among als.     removal of this assrtion, however, causes no difficult
                 The

problem. If there are constx.ints anong ct.js, we can express each a1
parametically as        a1() where the number      of   elennts in        is fewer than

those in c (aj c,. . . , cc1)'. Thus,        one can    simply premultiply the      inverse

of the asymptotic variance—covariance matrix             of   the maximum likelihood

estimator    or the NL3S by           and postmultiply by             .    Hence,   all the
results of the     paper hold.
                                      -2    -




6.   Conclusions

     We have proved that the maximum        likelihood   estimator is asymptotically

more efficient than the nonlinear three-stage least squares estimator.

However we have also shown that       the   consistency of the maximum   likelihood
estimator depends on the assumption of normality whereas that of the

nonlinear three-stage least squares is not. This fact increase the

attractiveness of the latter. The following are some important topics

for further research:

      1)   Evaluate   the degree of the relative inefficiency of the best

           nonlinear three-stage least squares estimator as compared to

           the maximum likelihood in specific models.

      2)   Evaluate the degree of the       realtive inefficiency of several versions

           of   the computationally practical nonlinear three-stage least
           squares estimator as compared to the best nonlinear three-stage
           least squares estimator in specific models
      3) Is there an estiirtor, possibly even better than the best
           nonlinear three-stage least squares estimator, which is
           computationally simpler than the maximum likelihhod estimator?
           Can that estimator remain consistent when the normality
           assumption   is removed?
                                          — 25   —




    References
        1.    Amemiya, T. [1974], uThe Nonlinear Two-Stage Least-Squares
              Est:ijiiator,' Journal of Econometrics, Vol. 2, 105-110.

         2.   Hausman, J. [1975], 'An Instrumental Variable Approach to
              Full Inform-tion Estimators for Linear and Non-Linear Econometric
              Models , r Econometrica, forthcoming.

        3. Jorgenson, D.W., and J. Laffont [1974], "Efficient Estimation
              of Nonlinear Siuuiltaneous Equations with Additive Disturbances,"
              Annals of Economic and Soc ialMeasurenien-t, Vol. 3, 615-640.




p
                                            -Al-



                                   APPENDIX




Assumpitons
     We nake   -the following assumptions in additon to the basic assumptions

of the model stated in Section 2.



    A.    plirn        (a) exists in a neighborhood of



     B.   —--              -N             in T1 E             '

          /T


     C.   plim    f   _____     exists       in a neighborhood of ct0

          and   the convergence is uniform in the neighborhood.


     D.   plim T'                        is negative   definite.
                              a0
                       —

     E.   him T1 E                   .              - pin T'
                                a0           a0                         a0




Theorem. Under the basic assumptions of the model stated in Section 2 and
assumptions A through E above, a root of the equation                    O   is consistent


and the consistent root a       satisfies
                                                (a-a0)              pin T [a
                                                              Nf_                    ao])
                                   -A2-



Proof.    Expanding T1   L(ct) in a Taylor series around           the   true   value
    we have



                 T1 (a)       f'           + f1 (a                 a0)


                                                                                        (A.l)

                          1
                          2        o
                                       -l
                                        aa
                                            2

                                                              (aa)
                                                                 o
                                                         aT




where      lies between a and a(). Taking the probability limit of both sides

of (A.l) and using assumptions A, B, and C, we have



                 plim T1 L(a) plim T1 L(a0)
                                                                                        (A.2)
                                                     2
                       1
                      +-(a-a) plT
                                       .



                                                              aT




Since          is continuous by a basic assumption stated in Section 2,


assumption C implies that plim T1               is   continuous in a neighborhood of


a0. Therefore, by CD), the second tern of the right-hand side of (A.1)
                                                                —l
is negative for all a in a neighborhood of a0. Therefore, pin T L(a)

attains a local rrximum at a0. This implies that a root of equation

        0 is consistent. The asmptotic normality follows easily from

assumptions B through E using the Taylor expansion

                                                                                                I
                                       -A3-


                                             a2i                c)
                                                    ** (c
                                       +                    —                     (A.3)
                         c.
                                  cx



where a is the consistent root         and         lies   between a

and       noting the left-hand side of (A. 3) is zero by         the definition
      A
of a.
