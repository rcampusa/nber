                                 NBER WORKING PAPER SERIES




                 SENSITIVITY TO MISSING DATA ASSUMPTIONS:
           THEORY AND AN EVALUATION OF THE U.S. WAGE STRUCTURE

                                             Patrick Kline
                                             Andres Santos

                                         Working Paper 15716
                                 http://www.nber.org/papers/w15716


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                     February 2010




We are grateful to David Card, Guido Imbens, Justin McCrary, Hal White, and seminar participants
at UC Berkeley, UC San Diego, the University of Michigan, USC, Stanford, the 2010 NBER Summer
Institute, the 2010 Western Economic Association Summer Meeting, the 2010 Seoul Summer Economics
Conference and the 2010 Econometric Society World Congress for useful comments and corrections.
We thank Ivan Fernández-Val for assistance in replicating the results of Angrist et al. (2006). A previous
version of this paper circulated under the title \Interval Estimation of Potentially Misspecified Quantile
Models in the Presence of Missing Data." The views expressed herein are those of the authors and
do not necessarily reflect the views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2010 by Patrick Kline and Andres Santos. All rights reserved. Short sections of text, not to exceed
two paragraphs, may be quoted without explicit permission provided that full credit, including © notice,
is given to the source.
Sensitivity to Missing Data Assumptions: Theory and An Evaluation of the U.S. Wage Structure
Patrick Kline and Andres Santos
NBER Working Paper No. 15716
February 2010, Revised February 2011
JEL No. C01,C1,J3

                                             ABSTRACT

This paper develops methods for assessing the sensitivity of empirical conclusions regarding conditional
distributions to departures from the missing at random (MAR) assumption. We index the degree of
non-ignorable selection governing the missingness process by the maximal Kolmogorov-Smirnov
(KS) distance between the distributions of missing and observed outcomes across all values of the
covariates. Sharp bounds on minimum mean square approximations to conditional quantiles are derived
as a function of the nominal level of selection considered in the sensitivity analysis and a weighted
bootstrap procedure is developed for conducting inference. Using these techniques, we conduct an
empirical assessment of the sensitivity of observed earnings patterns in U.S. Census data to deviations
from the MAR assumption. We find that the well-documented increase in the returns to schooling
between 1980 and 1990 is relatively robust to deviations from the missing at random assumption except
at the lowest quantiles of the distribution, but that conclusions regarding heterogeneity in returns and
changes in the returns function between 1990 and 2000 are very sensitive to departures from ignorability.


Patrick Kline
Department of Economics
UC, Berkeley
508-1 Evans Hall #3880
Berkeley, CA 94720
and NBER
pkline@econ.berkeley.edu

Andres Santos
Department of Economics
9500 Gilman Drive
La Jolla, CA 92093-0508
a2santos@ucsd.edu




An online appendix is available at:
http://www.nber.org/data-appendix/w15716
1         Introduction

Despite major advances in the design and collection of survey and administrative data, missingness
remains a pervasive feature of virtually every modern economic dataset. Hirsch and Schumacher
(2004), for instance, find that nearly 30% of the earnings observations in the Outgoing Rotation
Groups of the Current Population Survey are imputed. Similar allocation rates are present in other
major earnings sources such as the March CPS and Decennial Census with the problem growing
worse in more recent years.

    The dominant framework for dealing with missing data has been to assume that it is “missing
at random” (Rubin (1976)) or “ignorable” conditional on observable demographics; an assumption
whose popularity owes more to convenience than plausibility. Even in settings where it is reason-
able to believe that non-response is approximately ignorable, the extent of missingness in modern
economic data suggests that economists ought to assess the sensitivity of their conclusions to small
deviations from this assumption.

    Previous work on non-ignorable missing data processes has either relied upon parametric models
of missingness in conjunction with exclusion restrictions to obtain point identification (Greenlees
et al. (1982) and Lillard et al. (1986)) or considered the “worst case” bounds on population moments
that result when all assumptions regarding the missingness process are abandoned (Manski (1994,
2003)). Neither approach has garnered much popularity.1 It is typically quite difficult to find
variables which shift the probability of missingness but are uncorrelated with population outcomes.
And for most applied problems, the worst case bounds are overly conservative in the sense that
they consider missingness processes unlikely to be found in modern datasets.

   We propose here an alternative approach for use in settings where one lacks prior knowledge of
the missing data mechanism. Rather than ask what can be learned about the parameters of interest
given assumptions on the missingness process, we investigate the level of non-ignorable selection
necessary to undermine ones’ conclusions regarding the conditional distribution of the data obtained
under a missing at random (MAR) assumption. We do so by making use of a nonparametric
measure of selection – the maximal Kolmogorov-Smirnov (KS) distance between the distributions
of missing and observed outcomes across all values of the covariates. The KS distance yields a
natural parameterization of deviations from ignorability, with a distance of zero corresponding to
MAR and a distance of one encompassing the totally unrestricted missingness processes considered
in Manski (1994). Between these extremes lie a continuum of selection mechanisms which may be
studied to determine a critical level of selection above which conclusions obtained under an analysis
predicated upon MAR may be overturned.

   To enable such an analysis, we begin by deriving sharp bounds on the conditional quantile
function (CQF) under nominal restrictions on the degree of selection present. We focus on the
commonly encountered setting where outcome data are missing and covariates are discrete. In order
    1
        See DiNardo et al. (2006) for an applied example comparing these two approaches.



                                                           2
to facilitate the analysis of datasets with many covariates, results are also developed summarizing the
conclusions that can be drawn regarding “pseudo-true” parametric approximations to the underlying
nonparametric CQF of the sort considered by Chamberlain (1994). When point identification of the
CQF fails due to missingness, the identified set of pseudo true parameters consists of all coefficients
associated with minimum mean square approximations to functions lying within the CQF bounds.

    We obtain sharp bounds on the coordinates of the pseudo true parameter vector and propose
computationally simple estimators for them. We show that these estimators converge in distribution
to a Gaussian process indexed by the quantile of interest and the level of the nominal restriction on
selection and develop a weighted bootstrap procedure for consistently estimating that distribution.
This procedure enables inference on the entire pseudo-true quantile process as indexed both by the
quantile of interest and the level of the selection bound.

    Substantively these methods allow a determination of the critical level of selection for which
hypotheses regarding conditional quantiles, parametric approximations to conditional quantiles, or
entire conditional distributions cannot be rejected. For example we study the “breakdown” function
defined implicitly as the level of selection necessary for conclusions to be overturned at each quantile.
The uniform confidence region for this function effectively summarizes the differential sensitivity of
the entire conditional distribution to violations of MAR. These techniques substantially extend the
recent econometrics literature on sensitivity analysis (Altonji et al. (2005, 2008), Imbens (2003),
Rosenbaum and Rubin (1983), Rosenbaum (2002)), most of which has focused on the sensitivity of
scalar treatment effect estimates to confounding influences, typically by using assumed parametric
models of selection.

    Having established our inferential procedures, we turn to an empirical assessment of the sensi-
tivity of heavily studied patterns in the conditional distribution of U.S. wages to deviations from
the MAR assumption. We begin by revisiting the results of Angrist et al. (2006) regarding changes
across Decennial Censuses in the quantile specific returns to schooling. Weekly earnings informa-
tion is missing for roughly a quarter of the observations in their study, suggesting the results may
be sensitive to small deviations from ignorability. We show that despite extensive missingness in
the dependent variable, the well-documented increase in the returns to schooling between 1980 and
1990 is relatively robust to deviations from the missing at random assumption except at the lowest
quantiles of the conditional distribution. However, deterioration in the quality of Decennial Census
data renders conclusions regarding heterogeneity in returns and changes in the returns function be-
tween 1990 and 2000 very sensitive to departures from ignorability at all quantiles. We also show,
using a more flexible model studied by Lemieux (2006), that the apparent convexification of the
earnings-education profile between 1980 and 2000 is robust to modest deviations from MAR while
changes in the wage structure at lower quantiles are more easily obscured by selection.

   To gauge the practical relevance of these sensitivity results we analyze a sample of workers from
the 1973 Current Population Survey for whom IRS earnings records are available. This sample allows
us to observe the earnings of CPS participants who, for one reason or another, failed to provide


                                                   3
valid earnings information to the CPS. We show that IRS earnings predict non-response to the CPS
within demographic covariate bins, with very high and very low earning individuals most likely to
have invalid CPS earnings records. Measuring the degree of selection using our proposed KS metric
we find significant deviations from ignorability with patterns of selection that vary substantially
across demographic groups. Given recent trends in survey imputation rates, these findings suggest
economists’ knowledge of the location and shape of conditional earnings distributions in the U.S.
may be more tentative than previously supposed.

   The remainder of the paper is structured as follows: Section 2 describes our index of selection
and our general approach to assessing sensitivity. Section 3 develops our approach to assessing
the sensitivity of parametric approximations to conditional quantiles. Section 4 obtains the results
necessary for estimation and inference on the bounds provided by restrictions on the selection
process. In Section 5 we present our empirical study and briefly conclude in Section 6.



2     Assessing Sensitivity

Consider the random variables (Y, X, D) with joint distribution F , where Y ∈ R, X ∈ Rl and
D ∈ {0, 1} is a dummy variable that equals one if Y is observable and zero otherwise. Denote the
distribution of Y given X and the distribution of Y given X and D respectively as:

               Fy|x (c) ≡ P (Y ≤ c|X = x)        Fy|d,x (c) ≡ P (Y ≤ c|D = d, X = x) ,             (1)

where d ∈ {0, 1} and further define the probability of Y being observed conditional on X to be:

                                      p(x) ≡ P (D = 1|X = x) .                                     (2)

    In conducting a sensitivity analysis the researcher seeks to assess how the identified features
of Fy|x (c) depend upon alternative assumptions regarding the process generating D. In particular,
we will concern ourselves with the sensitivity of conclusions regarding q(τ |X), the conditional τ -
quantile of Y given X, which is often of more direct interest than the distribution function itself.
Towards this end, we impose the following assumptions on the data generating process:

Assumption 2.1. (i) X ∈ Rl has finite support X ; (ii) Fy|d,x (c) is continuous and strictly increas-
ing at all c such that 0 < Fy|d,x (c) < 1; (iii) D equals one if Y is observable and zero otherwise.

    The discrete support requirement in Assumption 2.1(i) simplifies inference as it obviates the
need to employ nonparametric estimators of conditional quantiles. While this assumption may be
restrictive in some environments, it is still widely applicable as illustrated in our study of quantile
specific returns to education in Section 5. It is also important to emphasize that Assumption 2.1(i)
is not necessary for our identification results, but only for our discussion of inference. Assumption
2.1(ii) ensures that for any 0 < τ < 1, the τ -conditional quantile of Y given X is uniquely defined.

    Most previous work on sensitivity analysis (e.g. Rosenbaum and Rubin (1983), Altonji et al.

                                                  4
(2005)) has relied upon parametric models of selection. While potentially appropriate in cases where
particular deviations from ignorability are of interest, such approaches risk understating sensitivity
by implicitly ruling out a wide class of selection mechanisms. We now develop an alternative
approach designed to allow an assessment of sensitivity to arbitrary deviations from ignorability
that retains much of the parsimony of parametric methods. Specifically, we propose studying a
nonparametric class of selection models indexed by a scalar measure of the deviations from MAR
they generate. A sensitivity analysis may then be conducted by considering the conclusions that
can be drawn under alternative levels of the selection index, with particular attention devoted to
determination of the threshold level of selection necessary to undermine conclusions obtained under
ignorability.

    Since ignorability occurs when Fy|1,x equals Fy|0,x , it is natural to measure deviations from MAR
in terms of the distance between these two distributions. We propose as an index of selection
the maximal Kolmogorov-Smirnov (KS) distance between Fy|1,x and Fy|0,x across all values of the
covariates.2 Thus, for X the support of X, we define the selection metric:

                                      S(F ) ≡ sup sup |Fy|1,x (c) − Fy|0,x (c)| .                                  (3)
                                                x∈X c∈R

Note that the missing at random assumption may be equivalently stated as S(F ) = 0, while
S(F ) = 1 corresponds to severe forms of selection where Fy|1,x and Fy|0,x fail to overlap at some
point x ∈ X . For illustrative purposes, Appendix A provides a numerical example mapping the
parameters of a bivariate normal selection model into values of S(F ) and plots of the corresponding
observed and missing data CDFs.

     Restrictions on S(F ) can be shown to yield sharp tractable bounds on the conditional distribu-
tion function Fy|x (·) as well as its value at any particular point of evaluation Fy|x (c). This facilitates
                                               −1                                            −1
the study of both conditional quantiles (Fy|x     (c)) and conditional quantile processes (Fy|x (·)) in fam-
ilies of non-ignorable selection mechanisms indexed by S(F ). By construction, any scalar metric of
selection will be less informative than a full description of the selection process. Researchers who
suspect particular forms of heterogeneity in the selection mechanism across covariate values may
wish to consider separate indices of selection for each point in the support of X or consider a max-
imum of weighted KS distances in (3). Likewise, if one has in mind particular classes of selection
mechanisms, it is possible to consider indices based upon weighted KS distances with weights that
vary across points of evaluation c. Though such approaches entail a simple extension of our meth-
ods, we do not pursue them here. Our approach is tailored to environments where prior knowledge
of the selection mechanism is not available. If substantial prior information is available, or if par-
ticular sorts of violations of MAR are of interest, it may be better to work with a semi-parametric
or even parametric model of selection.

    We note in passing that parametric models of selection often imply stronger restrictions on the
relationship between the observed and missing data distributions than is sometimes appreciated.
  2
      The Kolmogorov-Smirnov distance between two distributions H1 (·) and H2 (·) is defined as supc∈R |H1 (c)−H2 (c)|.



                                                           5
For example, the bivariate normal selection model considered in Appendix A tends to achieve maxi-
mal distances between missing and observed distributions at points near the center of the unselected
distribution while points in the tails of the distribution exhibit relatively small discrepancies. How-
ever, this model cannot accommodate patterns of selection that would lead the CDFs of missing
and observed outcomes to cross at some values as might occur if individuals with very high or very
low values of Y are most likely to have missing observations – a pattern conjectured to be present
in earnings data by Lillard et al. (1986) and corroborated in our later analysis of earnings validation
data in Section 5.3 In cases where crossing occurs, the maximal distance between distributions will
tend to be achieved far from the crossing point, often in the tails of the unselected distribution.
Since one typically does not know whether or where the CDFs cross, or how this behavior varies
across covariate bins, it can be difficult to develop a model of non-response suitable for the study
of conditional distributions. For this reason, the consideration of a nonparametric measure of devi-
ations from MAR of the sort indexed by our maximal KS metric S(F ), is likely to be of interest in
most settings where missing data are present.

   For q(τ |X) the conditional τ -quantile of Y given X, we examine what can be learned about the
conditional quantile function q(τ |·) under the nominal restriction:

                                                    S(F ) ≤ k .                                              (4)

Knowledge of a true value of k for which (4) holds is not presumed. Rather, we propose examining
the conclusions that may be drawn on the CQF given various candidate values of k. By consid-
ering multiple values of k, it is possible to deduce what level of selection is necessary to overturn
conclusions of interest obtained under a MAR analysis.

    In the absence of additional restrictions, the conditional quantile function ceases to be identified
under any deviation from ignorability (k > 0). Nonetheless, q(τ |·) may still be shown to lie within
a nominal identified set. This set consists of the values of q(τ |·) that would be compatible with the
distribution of observables were the putative restriction S(F ) ≤ k known to hold. We qualify such
a set as nominal due to restriction (4) being part of a hypothetical exercise only.

      The following Lemma provides a sharp characterization of the nominal identified set:
                                                                                 −              −1
Lemma 2.1. Suppose Assumptions 2.1(ii)-(iii) hold, S(F ) ≤ k and let Fy|1,x          (c) = Fy|1,x  (c) if
            −                            −
0 < c < 1, Fy|1,x (c) = −∞ if c ≤ 0 and Fy|1,x (c) = ∞ if c ≥ 1. Defining (qL (τ, k|x), qU (τ, k|x)) by:
                                                 τ − min{τ + kp(x), 1}(1 − p(x)) 
                                          −
                           qL (τ, k|x) ≡ Fy|1,x
                                                               p(x)
                                                 τ − max{τ − kp(x), 0}(1 − p(x)) 
                                          −
                           qU (τ, k|x) ≡ Fy|1,x                                     ,
                                                               p(x)
it follows that the identified set for q(τ |·) is C(τ, k) ≡ {θ : X → R : qL (τ, k|·) ≤ θ(·) ≤ qU (τ, k|·)}.

      The bounds in Lemma 2.1 are given by quantiles of the conditional distribution of observed
  3
      Crossing of CDFs may also occur in two-sided selection models of the sort considered by Neal (2004).


                                                          6
outcomes. The nominal identified set C(τ, k) is sharp for q(τ |·) in that for every function θ ∈ C(τ, k)
there exists a distribution F̃ of (Y, X, D) that matches the distribution of observables, satisfies
S(F̃ ) ≤ k and has conditional τ -quantile function θ. It is interesting to note that C(τ, k) provides
a smooth parametrization between identification under MAR (k = 0) and the bounds derived in
Manski (1994) which impose no restrictions on the selection mechanism (k = 1). Between these
two extremes, however, lie a continuum of identified sets corresponding to families of selection
mechanisms yielding different degrees of departure from ignorability.


2.1      Examples

We conclude this section by illustrating through examples how the bound functions (qL , qU ) may
be used to evaluate the sensitivity of conclusions obtained under MAR. For simplicity, we let X be
binary so that the conditional τ -quantile function q(τ |·) takes only two values.

Example 2.1. (Pointwise Conclusions) Suppose interest centers on whether q(τ |X = 1) equals
q(τ |X = 0) for a specific quantile τ0 . A researcher who finds them to differ under a MAR analysis
may easily assess the sensitivity of his conclusion to the presence of selection by employing the
functions (qL (τ0 |·), qU (τ0 |·)). Concretely, the minimal amount of selection necessary to overturn the
conclusion that the conditional quantiles differ is given by:

      k0 ≡ inf k : qL (τ0 , k|X = 1) − qU (τ0 , k|X = 0) ≤ 0 ≤ qU (τ0 , k|X = 1) − qL (τ0 , k|X = 0) .   (5)

That is, k0 is the minimal level of selection under which the nominal identified sets for q(τ0 |X = 0)
and q(τ0 |X = 1) contain a common value.

Example 2.2. (Distributional Conclusions) A researcher is interested in whether the con-
ditional distribution Fy|x=0 first order stochastically dominates Fy|x=1 , or equivalently, whether
q(τ |X = 1) ≤ q(τ |X = 0) for all τ ∈ (0, 1). She finds under MAR that q(τ |X = 1) > q(τ |X = 0)
at multiple values of τ leading her to conclude that first order stochastic dominance does not hold.
Employing the functions (qL , qU ), she may assess what degree of selection is necessary to cast doubt
on this conclusion by examining:

                    k0 ≡ inf k : qL (τ, k|X = 1) ≤ qU (τ, k|X = 0) for all τ ∈ (0, 1) .                  (6)

Here, k0 is the smallest level of selection for which an element of the identified set for q(·|X = 1)
(qL (·, k0 |X = 1)) is everywhere below an element of the identified set for q(·|X = 0) (qU (·, k0 |X =
0)). Thus, k0 is the threshold level of selection under which Fy|x=0 may first order stochastically
dominate Fy|x=1 .

Example 2.3. (Breakdown Analysis) A more nuanced sensitivity analysis might examine what
degree of selection is necessary to undermine the conclusion that q(τ |X = 1) 6= q(τ |X = 0) at each
specific quantile τ . As in Example 2.1, we can define the quantile specific critical level of selection:

      κ0 (τ ) ≡ inf k : qL (τ, k|X = 1) − qU (τ, k|X = 0) ≤ 0 ≤ qU (τ, k|X = 1) − qL (τ, k|X = 0) .      (7)

                                                      7
By considering κ0 (τ ) at different values of τ , we implicitly define a “breakdown” function κ0 (·)
which reveals the differential sensitivity of the initial conjecture at each quantile τ ∈ (0, 1).



3       Parametric Modeling

Analysis of the conditional τ -quantile function q(τ |·) and its corresponding nominal identified set
C(τ, k) can be cumbersome when many covariates are present as the resulting bounds will be of
high dimension and difficult to visualize. Moreover, it can be arduous even to state the features
of a high dimensional CQF one wishes to examine for sensitivity. It is convenient in such cases to
be able to summarize q(τ |·) using a parametric model. Failure to acknowledge, however, that the
model is simply an approximation can easily yield misleading conclusions.

    Figure 1 illustrates a case where the nominal identified set C(τ, k) possesses an erratic (though
perhaps not unusual) shape. The set of linear CQFs obeying the bounds provide a poor description
of this set, covering only a small fraction of its area. Were the true CQF known to be linear this
reduction in the size of the identified set would be welcome, the benign result of imposing additional
identifying information. But in the absence of true prior information these reductions in the size of
the identified set are unwarranted – a phenomenon we term “identification by misspecification”.

    The specter of misspecification leaves the applied researcher with a difficult choice. One can
either conduct a fully nonparametric analysis of the nominal identified set, which may be difficult
to interpret with many covariates, or work with a parametric set likely to overstate what is known
about the CQF. Under identification, this tension is typically resolved by estimating parametric
models that possess an interpretation as best approximations to the true CQF and adjusting the
corresponding inferential methods accordingly as advocated in Chamberlain (1994) and Angrist
et al. (2006). Following Horowitz and Manski (2006), Stoye (2007), and Ponomareva and Tamer
(2009), we extend this approach and develop methods for conducting inference on potentially mis-
specified parametric models under partial identification.

    We focus on linear parametric models and approximations that minimize a known quadratic
loss function. For S a known measure on X and ES [g(X)] denoting the expectation of g(X) when
X is distributed according to S, we define the pseudo true parameter to be:4

                                   β(τ ) ≡ arg min ES [(q(τ |X) − X 0 γ)2 ] .                                   (8)
                                                 γ∈Rl


    Lack of identification of the conditional quantile function q(τ |·) due to missing data implies lack
of identification of the pseudo true parameter β(τ ). We therefore consider the set of pseudo true
    4
     The measure S weights the squared deviations in each covariate bin. Its specification is an inherently context-
specific task depending entirely upon the researcher’s objectives. In Section 4 we weight the deviations by sample
size. Other schemes (including equal weighting) may also be of interest in some settings.




                                                         8
parameters which constitute a best approximation to some CQF in C(τ, k). Formally, we define:

              P(τ, k) ≡ {β ∈ Rl : β ∈ arg min ES [(θ(X) − X 0 γ)2 ] for some θ ∈ C(τ, k)} .                        (9)
                                                γ∈Rl

Figure 2 illustrates an element of P(τ, k) graphically. While intuitively appealing, the definition of
P(τ, k) is not necessarily the most convenient for computational purposes. Fortunately, the choice
of quadratic loss and the characterization of C(τ, k) in Lemma 2.1 imply a tractable alternative
representation for P(τ, k), which we obtain in the following Lemma.
Lemma 3.1. If Assumptions 2.1(ii)-(iii), S(F ) ≤ k and ES [XX 0 ] is invertible, then it follows that:

P(τ, k) = {β ∈ Rl : β = (ES [XX 0 ])−1 ES [Xθ(X)] s.t. qL (τ, k|x) ≤ θ(x) ≤ qU (τ, k|x) for all x ∈ X } .

    Interest often centers on either a particular coordinate of β(τ ) or the pseudo-true conditional
quantile at a specified value of the covariates. Both these quantities may be expressed as λ0 β(τ )
for some known vector λ ∈ Rl . Using Lemma 3.1 it is straightforward to show that the nominal
identified set for parameters of the form λ0 β(τ ) is an interval with endpoints characterized as the
solution to linear programming problems.5
Corollary 3.1. Suppose Assumptions 2.1(ii)-(iii), S(F ) ≤ k, ES [XX 0 ] is invertible and define:

  πL (τ, k) ≡     inf      λ0 β = inf λ0 (ES [XX 0 ])−1 ES [Xθ(X)] s.t. qL (τ, k|x) ≤ θ(x) ≤ qU (τ, k|x)          (10)
                β∈P(τ,k)           θ

  πU (τ, k) ≡    sup λ0 β = sup λ0 (ES [XX 0 ])−1 ES [Xθ(X)] s.t. qL (τ, k|x) ≤ θ(x) ≤ qU (τ, k|x) . (11)
                β∈P(τ,k)           θ

The nominal identified set for λ0 β(τ ) is then given by the interval [πL (τ, k), πU (τ, k)].

   Corollary 3.1 provides sharp bounds on the quantile process λ0 β(·) at each point of evaluation τ
under the restriction that S(F ) ≤ k. However, sharpness of the bounds at each point of evaluation
does not, in this case, translate into sharp bounds on the entire process. To see this, note that
Corollary 3.1 implies λ0 β(·) must belong to the following set:

                        G(k) ≡ {g : [0, 1] → R : πL (τ, k) ≤ g(τ ) ≤ πU (τ, k) for all τ } .                      (12)

While the true λ0 β(·) must belong to G(k), not all functions in G(k) can be justified as some
distribution’s pseudo-true process.6 Therefore, G(k) does not constitute the nominal identified set
for the process λ0 β(·) under the restriction S(F ) ≤ k. Fortunately, πL (·, k) and πU (·, k) are in the
identified set over the range of (τ, k) for which the bounds are finite. Thus, the set G(k), though
not sharp, does retain the favorable properties of: (i) sharpness at any point of evaluation τ , (ii)
containing the true identified set for the process so that processes not in G(k) are also known not to
be in the identified set; (iii) sharpness of the lower and upper bound functions πL (·, k) and πU (·, k);
and (iv) ease of analysis and graphical representation.
   5
     Since X has discrete support, we can characterize the function θ by the finite number of values it may take.
Because the weighting scheme S is known, so is λ0 (ES [XX 0 ])−1 , and hence the objectives in (10) and (11) are of the
form w0 θ where w is a known vector and θ is a finite dimensional vector over which the criterion is optimized.
   6
     For example, under our assumptions λ0 β(·) is a continuous function of τ . Hence, any g ∈ G(k) that is discontin-
uous is not in the nominal identified set for λ0 β(·) under the hypothetical that S(F ) ≤ k.

                                                          9
3.1    Examples

We now revisit Examples 2.1-2.3 from Section 2.1 in order to illustrate how to characterize the
sensitivity of conclusions drawn under MAR with parametric models. We keep the simplifying
assumption that X is scalar, but no longer assume it is binary and instead consider the model:

                                       q(τ |X) = α(τ ) + Xβ(τ ) .                                     (13)

Note that when X is binary equation (13) provides a non-parametric model of the CQF, in which
case our discussion coincides with that of Section 2.1.

Example 2.1 (cont.) Suppose that an analysis under MAR reveals β(τ0 ) 6= 0 at a specific quantile
τ0 . Employing the functions (πL , πU ) we may then define the critical level of selection k0 necessary
to cast doubt on this conclusion as:

                                k0 ≡ inf k : πL (τ0 , k) ≤ 0 ≤ πU (τ0 , k) .                          (14)

That is, under any level of selection k ≥ k0 it is no longer possible to conclude that β(τ0 ) 6= 0.

Example 2.2 (cont.) In a parametric analogue of first order stochastic dominance of Fy|x over
Fy|x0 for x < x0 , a researcher examines whether β(τ ) ≤ 0 for all τ ∈ (0, 1). Suppose that a MAR
analysis reveals that β(τ ) > 0 for multiple values of τ . The functions (πL , πU ) enable her to assess
what degree of selection is necessary to undermine her conclusions by considering:

                             k0 ≡ inf k : πL (τ, k) ≤ 0 for all τ ∈ (0, 1) .                          (15)

Note that finding πL (τ, k0 ) ≤ 0 for all τ ∈ (0, 1) does in fact cast doubt on the conclusion that
β(τ ) > 0 for some τ because πL (·, k0 ) is itself in the nominal identified set for β(·). That is, under
a degree of selection k0 , the process β(·) may equal πL (·, k0 ).

Example 2.3 (cont.) Generalizing the considerations of Example 2.1, we can examine what
degree of selection is necessary to undermine the conclusion that β(τ ) 6= 0 at each specific τ . In
this manner, we obtain a quantile specific critical level of selection:

                               κ0 (τ ) ≡ inf k : πL (τ, k) ≤ 0 ≤ πU (τ, k) .                          (16)

As in Section 2.1, the resulting “breakdown” function κ0 (·) enables us to characterize the differential
sensitivity of the entire conditional distribution to deviations from MAR.



4     Estimation and Inference

In what follows we develop methods for conducting sensitivity analysis using sample estimates of
πL (τ, k) and πU (τ, k). This section is primarily technical and applied readers may wish to skip to
the application in Section 5 before studying these methods in detail.



                                                    10
     Our strategy for estimating the bounds πL (τ, k) and πU (τ, k) consists of first obtaining estimates
q̂L (τ, k|x) and q̂U (τ, k|x) of the conditional quantile bounds and then employing them in place of
qL (τ, k|x) and qU (τ, k|x) in the linear programming problems given in (10) and (11). Thus, an
appealing characteristic of our estimator is the reliability and low computational cost involved in
solving a linear programming problem – considerations which become particularly salient when
implementing a bootstrap procedure for inference.

    Recall that the conditional quantile bounds qL (τ, k|x) and qU (τ, k|x) may be expressed as quan-
tiles of the observed data (see Lemma 2.1). We estimate these bounds using their sample analogues.
For the development of our bootstrap procedure, however, it will be useful to consider a represen-
tation of these sample estimates as the solution of a general M-estimation problem. Towards this
end, we define a family of population criterion functions (as indexed by (τ, b, x)) given by:

            Qx (c|τ, b) ≡ (P (Y ≤ c, D = 1, X = x) + bP (D = 0, X = x) − τ P (X = x))2 .                        (17)

Under appropriate restrictions on (τ, k), to be shortly specified, qL (τ, k|x) and qU (τ, k|x) then satisfy:

     qL (τ, k|x) = arg min Qx (c|τ, τ + kp(x))           qU (τ, k|x) = arg min Qx (c|τ, τ − kp(x)) .            (18)
                          c∈R                                               c∈R

Hence, there exists a direct relationship between the bounds qL (τ, k|x) and qU (τ, k|x) as indexed by
(τ, k) and the minimizers of Qx (c|τ, b) as indexed by (τ, b).

   We therefore employ the sample analogue to Qx (c|τ, b) for estimation, which we denote by:
                        n
                     1 X                                                                            2
   Qx,n (c|τ, b) ≡              {1{Yi ≤ c, Xi = x, Di = 1} + b1{Di = 0, Xi = x} − τ 1{Xi = x}}              .   (19)
                     n    i=1

Exploiting (17), the extremum estimators for the bounds qL (τ, k|x) and qU (τ, k|x) are then:

   q̂L (τ, k|x) ∈ arg min Qx,n (c|τ, τ + k p̂(x))        q̂U (τ, k|x) ∈ arg min Qx,n (c|τ, τ − k p̂(x)) ,       (20)
                         c∈R                                                c∈R
                P                      P
where p̂(x) ≡ ( i 1{Di = 1, Xi = x})/( i 1{Xi = x}). Finally, solving the sample analogues to
the linear programming problems given in (10) and (11) we obtain the estimators:

       π̂L (τ, k) ≡ inf λ0 (ES [XX 0 ])−1 ES [Xθ(X)]          s.t. q̂L (τ, k|x) ≤ θ(x) ≤ q̂U (τ, k|x)           (21)
                     θ

       π̂U (τ, k) ≡ sup λ0 (ES [XX 0 ])−1 ES [Xθ(X)]          s.t. q̂L (τ, k|x) ≤ θ(x) ≤ q̂U (τ, k|x)           (22)
                      θ


    For this approach to prove successful we focus our analysis on choices of (τ, k) for which (18)
holds, which is guaranteed by two restrictions. First, we require that (τ, k) be such that the bounds
πL (τ, k) and πU (τ, k) are finite. Second, we demand that (τ, k) be such that S(F ) ≤ k proves more
informative than the restriction that Fy|0,x lie between zero and one. Succinctly, for an arbitrary
fixed  > 0, we focus on values of (τ, k) that lie in the set:
       (                                                                                             )
                           (i) kp(x)(1 − p(x)) + 2 ≤  τ p(x)         (iii) k ≤ τ
 B ≡ (τ, k) ∈ [0, 1]2 :                                                                for all x ∈ X
                           (ii) kp(x)(1 − p(x)) + 2 ≤ (1 − τ )p(x)   (iv) k ≤ 1 − τ



                                                       11
Provided that the conditional probability of missing is bounded away from one and  is small, the
set B is nonempty since it contains the MAR analysis as a special case. In general, however, the set
B imposes that large or small values of τ must be accompanied by small values of k. This simply
reflects that the fruitful study of quantiles close to one or zero requires stronger assumptions on the
nature of the selection process than the study of, for example, the conditional median.

   We introduce the following additional assumption in order to develop our asymptotic theory:

Assumption 4.1. (i) B 6= ∅; (ii) Fy|1,x (c) has a continuous bounded derivative fy|1,x (c); (iii)
                                                0
fy|1,x (c) has a continuous bounded derivative fy|1,x (c); (iv) ES [XX 0 ] is invertible; (v) fy|1,x (c) is
bounded away from zero uniformly on all c satisfying  ≤ Fy|1,x (c)p(x) ≤ p(x) −  ∀x ∈ X .

    Letting π̂L and π̂U be the functions defined pointwise by (21) and (22), we obtain their asymptotic
distribution as elements of L∞ (B) (the space of bounded functions on B). Such a result is a key
step towards constructing confidence intervals for πL (τ, k) and πU (τ, k) that are uniform in (τ, k).
As we illustrate in Section 4.2, these uniformity results are particularly useful for conducting the
sensitivity analyses illustrated in Examples 2.1-2.3.

Theorem 4.1. If Assumptions 2.1, 4.1 hold and {Yi , Xi , Di }ni=1 is an i.i.d. sample, then:

                                        √  π̂L − πL  L
                                         n            −→ G ,                                          (23)
                                            π̂U − πU

where G is a Gaussian process on the space L∞ (B) × L∞ (B).

    We note that since G is a Gaussian process, its marginals G(τ, k) are simply bivariate normal
random variables. For notational convenience, we let G(i) (τ, k) denote the ith component of the vec-
tor G(τ, k). Thus, G(1) (τ, k) is the limiting distribution corresponding to the lower bound estimate
at the point (τ, k), while G(2) (τ, k) is the limiting distribution of the upper bound estimate at (τ, k).


4.1    Examples

We now return to the Examples of Section 2.1 and 3.1 and discuss how to conduct inference on the
various sensitivity measures introduced there. For simplicity, we assume the relevant critical values
are known. In Section 4.2 we develop a bootstrap procedure for their estimation.

Example 2.1 (cont.) Since under any level of selection k larger than k0 it is also not possible to
conclude β(τ0 ) 6= 0, it is natural to construct a one sided (rather than two sided) confidence interval
                                 (i)
for k0 . Towards this end, let r1−α (k) be the 1 − α quantile of G(i) (τ0 , k) and define:
                                               (1)                          (2)
                                                 r (k)                     r (k)
                     k̂0 ≡ inf k : π̂L (τ0 , k) − 1−α
                                                   √   ≤ 0 ≤ π̂U (τ0 , k) + 1−α
                                                                             √   .                    (24)
                                                     n                         n

The confidence interval [k̂0 , 1] then covers k0 with asymptotic probability at least 1 − α.


                                                     12
Example 2.2 (cont.) Construction of a one sided confidence interval for k0 in this setting is more
challenging as it requires us to employ the uniformity of our estimator in τ . First, let us define:
                                                           G(1) (τ, k)   
                          r1−α (k) = inf r : P       sup                ≤r ≥1−α ,                 (25)
                                                     τ ∈B(k) ωL (τ, k)

where B(k) = {τ : (τ, k) ∈ B} and ωL is a positive weight function chosen by the researcher. For
every fixed k, we may then construct the following function of τ :
                                                       r1−α (k)
                                        π̂L (·, k) −     √      ωL (·, k)                         (26)
                                                           n
which lies below πL (·, k) with asymptotic probability 1 − α. The function in (26) thus provides a
one sided confidence interval for the process πL (·, k). The weight function ωL allows the researcher
to account for the fact that the variance of G(1) (τ, k) may depend heavily on (τ, k). Defining:
                                                              r1−α (k)
                           k̂0 ≡ inf k : sup π̂L (τ, k) −       √      ωL (τ, k) ≤ 0 ,            (27)
                                       τ ∈B(k)                    n

it can then be shown that [k̂0 , 1] covers k0 with asymptotic probability at least 1 − α.

Example 2.3 (cont.) Employing Theorem 4.1 it is possible to construct a two sided confidence
interval for the function κ0 (·). Towards this end, we exploit uniformity in τ and k by defining:
                                         n |G(1) (τ, k)| |G(2) (τ, k)| o   
               r1−α   ≡ inf r : P sup max                ,                ≤r ≥1−α ,               (28)
                                  (τ,k)∈B    ωL (τ, k)     ωL (τ, k)

where as in Example 2.2, ωL and ωU are positive weight functions. In addition, we also let:
                                         r1−α                                   r1−α
         κ̂L (τ ) ≡ inf k : π̂L (τ, k) − √ ωL (τ, k) ≤ 0 , and 0 ≤ π̂U (τ, k) + √ ωU (τ, k)       (29)
                                            n                                     n
                                          r1−α                                 r1−α
         κ̂U (τ ) ≡ sup k : π̂L (τ, k) + √ ωL (τ, k) ≥ 0 , or 0 ≥ π̂U (τ, k) − √ ωU (τ, k) .      (30)
                                            n                                     n
It can then be shown that the functions (κ̂L (·), κ̂U (·)) provide a functional confidence interval for
κ0 (·). That is, κ̂L (τ ) ≤ κ0 (τ ) ≤ κ̂U (τ ) for all τ with asymptotic probability at least 1 − α.


4.2    Bootstrap Critical Values

As illustrated in Examples 2.1-2.3, conducting inference requires use of critical values that depend
on the unknown distribution of G, the limiting Gaussian process in Theorem 4.1, and possibly on
weight functions ωL and ωU (as in (25), (28)). We will allow the weight functions ωL and ωU to be
unknown, but require the existence of consistent estimators of them:

Assumption 4.2. (i) ωL (τ, k) ≥ 0 and ωU (τ, k) ≥ 0 are continuous and bounded away from zero
on B; (ii) There exist estimators ω̂L (τ, k) and ω̂U (τ, k) that are uniformly consistent on B.




                                                        13
   Given (ωL , ωU ), let Gω be the Gaussian process on L∞ (B) × L∞ (B) that is pointwise defined by:
                                                   G(1) (τ, k)/ω (τ, k) 
                                                                 L
                                      Gω (τ, k) =    (2)
                                                                           .                                       (31)
                                                    G (τ, k)/ωU (τ, k)

The critical values employed in Examples 2.1-2.3 can be expressed in terms of quantiles of some
Lipschitz transformation L : L∞ (B) × L∞ (B) → R of the random variable Gω . For instance, in
Example 2.2, the relevant critical value, defined in (25), is the 1 − α quantile of the random variable:

                                           L(Gω ) = sup G(1)
                                                         ω (τ, k) .                                                (32)
                                                      τ ∈B(k)

Similarly, in Example 2.3 the appropriate critical value defined in (28) is the 1 − α quantile of:

                                   L(Gω ) = sup max{G(1)        (2)
                                                     ω (τ, k), Gω (τ, k)} .                                        (33)
                                            (τ,k)∈B


   We therefore conclude by establishing the validity of a weighted bootstrap procedure for consis-
tently estimating the quantiles of random variables of the form L(Gω ). The bootstrap procedure is
similar to the traditional nonparametric bootstrap with the important difference that the random
weights on different observations are independent from each other. Specifically, letting {Wi }ni=1 be
an i.i.d. sample from a random variable W , we impose the following:

Assumption 4.3. (i) W is positive almost surely, independent of (Y, X, D) and satisfies E[W ] = 1
and V ar(W ) = 1; (ii) The functional L : L∞ (B) × L∞ (B) → R is Lipschitz continuous.

   A consistent estimator for quantiles of L(Gω ) may then be obtained through the algorithm:

Step 1: Generate a random sample of weights {Wi }ni=1 satisfying Assumption 4.3(i) and define:
                       n
                    1 X                                                                                     2
 Q̃x,n (c|τ, b) ≡             Wi {1{Yi ≤ c, Xi = x, Di = 1} + b1{Di = 0, Xi = x} − τ 1{Xi = x}}                   . (34)
                    n   i=1

Employing Q̃x,n (c|τ, b), obtain the following bootstrap estimators for qL (τ, k|x) and qU (τ, k|x):

    q̃L (τ, k|x) ∈ arg min Q̃x,n (c|τ, τ + k p̃(x))           q̃U (τ, k|x) ∈ arg min Q̃x,n (c|τ, τ − k p̃(x))      (35)
                         c∈R                                                     c∈R
               P                            P
where p̃(x) ≡ ( i Wi 1{Di = 1, Xi = x})/( i Wi 1{Xi = x}). Note that q̃L (τ, k|x) and q̃U (τ, k|x)
are simply the weighted empirical quantiles of the observed data evaluated at a point that depends
on the reweighted missingness probability. Note also that if we had used the conventional bootstrap
we would run the risk of drawing a sample for which a covariate bin is empty. This is not a concern
with the weighted bootstrap as the weights are required to be strictly positive.

Step 2: Using the bootstrap bounds q̃L (τ, k|x) and q̃U (τ, k|x) from Step 1, obtain the estimators:

           π̃L (τ, k) ≡ inf λ0 (ES [XX 0 ])−1 ES [Xθ(X)]        s.t. q̃L (τ, k|x) ≤ θ(x) ≤ q̃U (τ, k|x)            (36)
                              θ

           π̃U (τ, k) ≡ sup λ0 (ES [XX 0 ])−1 ES [Xθ(X)]         s.t. q̃L (τ, k|x) ≤ θ(x) ≤ q̃U (τ, k|x) .         (37)
                              θ

Algorithms for quickly solving linear programming problems of this sort are available in most modern

                                                         14
computational packages. The weighted bootstrap process for Gω is then defined pointwise by:

                                         √  (π̃L (τ, k) − π̂L (τ, k))/ω̂L (τ, k) 
                             G̃ω (τ, k) ≡ n                                         .                          (38)
                                             (π̃U (τ, k) − π̂U (τ, k))/ω̂U (τ, k)

Step 3: Our estimator for r1−α , the 1 − α quantile of L(Gω ), is then given by the 1 − α quantile
of L(G̃ω ) conditional on the sample {Yi , Xi , Di }ni=1 (but not {Wi }ni=1 ):
                                 n                                           o
                      r̃1−α ≡ inf r : P L(G̃ω ) ≥ r {Yi , Xi , Di }ni=1 ≥ 1 − α .            (39)

In applications, r̃1−α will generally need to be computed through simulation. This can be accom-
plished by repeating Steps 1 and 2 until the number of bootstrap simulations of L(G̃ω ) is large. The
estimator r̃1−α is then well approximated by the empirical 1 − α quantile of the bootstrap statistic
L(G̃ω ) across the computed simulations.

        We conclude our discussion of inference by establishing r̃1−α is indeed consistent for r1−α .

Theorem 4.2. Let r1−α be the 1 − α quantile of L(Gω ). If Assumptions 2.1, 4.1, 4.2, and 4.3 hold,
the cdf of L(Gω ) is strictly increasing and continuous at r1−α and {Yi , Xi , Di , Wi }ni=1 is i.i.d, then:
                                                        p
                                                 r̃1−α → r1−α .



5         Evaluating the U.S. Wage Structure

We turn now to an empirical assessment of the sensitivity of observed patterns in the U.S. wage
structure to deviations from the MAR assumption. A large literature reviewed by (among others)
Autor and Katz (1999), Heckman et al. (2006) and Acemoglu and Autor (2010) finds important
changes over time in the conditional distribution of earnings with respect to schooling levels.

    We begin our investigation of the sensitivity of these findings to alternative missing data assump-
tions by revisiting the results of Angrist et al. (2006) regarding changes across Decennial Censuses
in the quantile specific returns to schooling. We analyze the 1980, 1990, and 2000 Census samples
considered in their study but, to simplify our estimation routine, and to correct small mistakes
found in the IPUMS files since the time their extract was created, we use new extracts of the 1%
unweighted IPUMS files for each decade rather than their original mix of weighted and unweighted
samples.7 Sample sizes and imputation rates for the weekly earnings variable are given in Table 1.

        We estimate linear conditional quantile models for log earnings per week of the form:

                                        q(τ |X, E) = X 0 γ(τ ) + Eβ(τ ) ,                                      (40)
    7
    The sample consists of native born black and white men ages 40-49 with six or more years of schooling who
worked at least one week in the past year. Rather than dropping observations with allocated earnings we treat
them as missing. We also drop 10 observations falling in demographic cells with greater than 66% missing and 1,404
observations falling into demographic cells with less than 20 observations. Use of the original extracts analyzed in
Angrist et al. (2006) yields nearly identical results.


                                                        15
where X consists of an intercept, a black dummy, and a quadratic in potential experience, and E
represents years of schooling. Our analysis focuses on the quantile specific “returns” to a year of
schooling β(τ ) though we note that, particularly in the context of quantile regressions, the Mincerian
earnings coefficients need not map into any proper economic concept of individual returns (Heckman
et al. (2006)).

    Figure 3 provides estimates of the pseudo-true returns functions β(·) in 1980, 1990, and 2000
that result from assuming the data are missing at random. Uniform confidence regions for these
estimates were constructed by applying the methods of Section 3 subject to the restriction that
S(F ) = 0.8 In defining our parametric approximation metric we weight bin-specific deviations by
sample size (i.e. we choose S equal to empirical measure, see Section 3).

    Our MAR results are similar to those found in Figure 2A of Angrist et al. (2006). They suggest
that the returns function increased uniformly across quantiles between 1980 and 1990 but exhibited
a change in slope in 2000. The change between 1980 and 1990 is consistent with a general economy-
wide increase in the return to human capital accumulation as conjectured by Juhn et al. (1993).
However the finding of a shape change in the quantile process between 1990 and 2000 represents
a form of heteroscedasticity in the conditional earnings distribution with respect to schooling that
appears not to have been present in previous decades. This pattern of heteroscedasticity is consistent
with nonlinear human capital pricing models of the sort studied in Card and Lemieux (1996) and
more nuanced multi-factor views of technical change reviewed in Acemoglu and Autor (2010).


5.1     Sensitivity Analysis

A natural concern is the extent to which some or all of the conclusions regarding the wage structure
drawn under a missing at random assumption are compromised by limitations in the quality of
Census earnings data. As Table 1 shows, the prevalence of earnings imputations increases steadily
across Censuses with roughly a quarter of the observations allocated by 2000.9 With these levels
of missingness, the bounds on quantiles below the 25th percentile and above the 75th are not even
finite in the absence of restrictions on the missingness process.

     We begin by examining the sensitivity of conclusions regarding changes in the wage structure
between 1990 and 2000. Figures 4 shows the 95% uniform confidence regions for the set G(k), as
defined in (12), that result when we allow for a small amount of selection by setting S(F ) ≤ 0.05.
Though it remains clear the returns function increased between 1980 and 1990, we cannot reject
the null hypothesis that the quantile process was unchanged from 1990 to 2000. Moreover, there is
little evidence of heterogeneity across quantiles in the returns in any of the three Census samples –
   8
      In constructing uniform confidence intervals we set ωL (τ, k) = ωU (τ, k) = φ(Φ−1 (τ ))1/2 , where φ(·) and Φ(·) are
the standard normal density and CDF . These weights are inversely proportional to the square root of the variance
of the quantiles of a standard normal. The bootstrap weights {Wi }ni=1 were drawn from an exponential distribution.
    9
      It is interesting to note that only 7% of the men in our sample report working no weeks in the past year. Hence,
at least for this population, assumptions regarding the determinants of non-response appear to be more important for
drawing conclusions regarding the wage structure than assumptions regarding non-participation in the labor force.


                                                           16
a straight line can be fit through each sample’s confidence region.

    To further assess the robustness of our conclusions regarding changes between 1980 and 1990,
it is informative to find the level of k necessary to fail to reject the hypothesis that no change in
fact occurred between these years under the supposition that S(F ) ≤ k. Specifically, for πLt (τ, k)
and πUt (τ, k) the lower and upper bounds on the returns coefficients in year t, we aim to obtain a
confidence interval for the values of selection k under which:

                               πU80 (τ, k) ≥ πL90 (τ, k) for all τ ∈ [0.2, 0.8] .                       (41)

As in Example 2.2, we are particularly interested in k0 , the smallest value of k such that (41) holds,
as it will hold trivially for all k ≥ k0 . A search for the smallest value of k such that the 95% uniform
confidence intervals for these two decades overlap at all quantiles between 0.2 and 0.8 found this
“critical k” to be k̂0 = 0.175. Due to the independence of the samples between 1980 and 1990,
the one-sided interval [k̂0 , 1] provides an asymptotic coverage probability for k0 of at least 90%.
The lower end of this confidence interval constitutes a large deviation from MAR (see Appendix
A) indicating the evidence is quite strong that the returns process changed between 1980 and 1990.
Figure 5 plots the uniform confidence regions corresponding to the hypothetical S(F ) ≤ k̂0 .

    Though severe selection would be necessary for all of the changes between 1980 and 1990 to
be spurious, it is clear that changes at some quantiles may be more robust than others. It is
interesting then to conduct a more detailed analysis by evaluating the critical level of selection
necessary to undermine the conclusion that the returns increased at each quantile. Towards this
end, we generalize Example 2.3 and define κ0 (τ ) to be the smallest level of k such that:

                                           πU80 (τ, k) ≥ πL90 (τ, k) .                                  (42)

The function κ0 (·) summarizes the level of robustness of each quantile-specific conclusion. In this
manner, the “breakdown” function κ0 (·) reveals the differential sensitivity of the entire conditional
distribution to violations of the missing at random assumption.

    The point estimate for κ0 (τ ) is given by the value of k where π̂U80 (τ, k) intersects with π̂L90 (τ, k)
(see Figure 6). To obtain a confidence interval for κ0 (τ ) that is uniform in τ we first construct
95% uniform two sided confidence intervals in τ and k for the 1980 upper bound πU80 (τ, k) and the
1990 lower bound πL90 (τ, k). Given the independence of the 1980 and 1990 samples, the intersection
of the true bounds πU80 (τ, k) and πL90 (τ, k) must lie between the intersection of their corresponding
confidence regions with asymptotic probability of at least 90%. Since κ0 (τ ) is given by the intersec-
tion of πU80 (τ, k) with πL90 (τ, k), a valid lower bound for the confidence region of the function κ0 (·)
is given by the intersection of the upper envelope for πU80 (τ, k) with the lower envelope for πL90 (τ, k)
and a valid upper bound is given by the converse intersection.

    Figure 7 illustrates the resulting estimates of the breakdown function κ0 (·) and its corresponding
confidence region. Unsurprisingly, the most robust results are those for quantiles near the center
of the distribution for which very large levels of selection would be necessary to overturn the


                                                       17
hypothesis that the returns increased. However the curve is fairly asymmetric with the returns at
low quantiles being much more sensitive to deviations from ignorability than those at the upper
quantiles. Hence, changes in reporting behavior between 1980 and 1990 pose the greatest threat to
hypotheses regarding changes at the bottom quantiles of the earnings distribution.

    To conclude our sensitivity analysis we also consider the fitted values that result from the more
flexible earnings model of Lemieux (2006) which allows for quadratic effects of education on earnings
quantiles.10 Figure 8 provides bounds on the 10th, 50th, and 90th conditional quantiles of weekly
earnings by schooling level in 1980, 1990, and 2000 using our baseline hypothetical restriction
S(F ) ≤ 0.05. Little evidence exists of a change across Censuses in the real earnings of workers at
the 10th conditional quantile. At the conditional median, however, the returns to schooling (which
appear roughly linear) increased substantially, leading to an increase in inequality across schooling
categories. Uneducated workers witnessed wage losses while skilled workers experienced wage gains,
though in both cases these changes seem to have occurred entirely during the 1980s. Finally, we
also note that, as observed by Lemieux (2006), the returns to schooling appear to have gradually
convexified at the upper tail of the weekly earnings distribution with very well educated workers
experiencing substantial gains relative to the less educated.


5.2     Estimates of the Degree of Selection in Earnings Data

Our analysis of Census data revealed that the finding of a change in the quantile specific returns to
schooling process between 1990 and 2000 is easily undermined by small amounts of selection while
changes between 1980 and 1990 (at least above the lower quantiles of the distribution) appear to
be relatively robust. Employing a sample where validation data are present, we now turn to an
investigation of what levels of selection, as indexed by S(F ), are plausible in U.S. survey data.

    In order to estimate S(F ) we first derive an alternative representation of the distance between
Fy|0,x and Fy|1,x which illustrates its dependence on the conditional probability of the outcome being
missing. Towards this end, let us define the following conditional probabilities:

                                 pL (x, τ ) ≡ P (D = 1|X = x, Fy|x (Y ) ≤ τ )                                    (43)
                                 pU (x, τ ) ≡ P (D = 1|X = x, Fy|x (Y ) > τ ) .                                  (44)

By applying Bayes’ Rule, it is then possible to express the distance between the distribution of
missing and non-missing observations at a given quantile as a function of the selection probabilities:11
                                                  p
                                                    (pL (x, τ ) − p(x))(pU (x, τ ) − p(x))τ (1 − τ )
          |Fy|1,x (q(τ |x)) − Fy|0,x (q(τ |x))| =                                                    . (45)
                                                                   p(x)(1 − p(x))
Notice that knowledge of the missing probability P (D = 0|X = x, Fy|x (Y ) = τ ) is sufficient to
compute by integration all of the quantities in (45) and (by taking the supremum over τ and x)
  10
     The model also includes a quartic in potential experience. Our results differ substantively from those of Lemieux
both because of differences in sample selection and our focus on weekly (rather than hourly) earnings.
  11
     See Appendix B for a detailed derivation of (45).


                                                         18
of S(F ) as well.12 For this reason, our efforts focus on estimating this function in a dataset with
information on the earnings of survey non-respondents.

   We work with an extract from the 1973 March Current Population Survey (CPS) for which
merged Internal Revenue Service (IRS) earnings data are available. Our sample consists of black
and white men between the ages of 25 and 50 with six or more years of schooling who reported
working at least one week in the past year and had valid IRS earnings. We drop observations with
annual IRS earnings less than $1,000 or equal to the IRS topcode of $50,000.

    As in our study of the Decennial Census, we take the relevant covariates to be age, years of
schooling, and race. However, because our CPS sample is much smaller than our Census sample,
we coarsen our covariate categories and drop demographic cells with fewer than 50 observations.13
This yields an estimation sample of 13,598 observations distributed across 33 demographic cells.
Because weeks worked are only measured categorically in this CPS extract we simply take log IRS
earnings as our measure of Y and use response to the March CPS annual civilian earnings question
as our measure of D. This yields a missingness rate of 8.4%.

    We approximate the probability of non-response P (D = 0|X = x, Fy|x (Y ) = τ ) with the follow-
ing sequence of increasingly flexible logistic models:

          P (D = 0|X = x, Fy|x (Y ) = τ ) = Λ(b1 τ + b2 τ 2 + δx )                                     (M1)
          P (D = 0|X = x, Fy|x (Y ) = τ ) = Λ(b1 τ + b2 τ 2 + γ1 δx τ + γ2 δx τ 2 + δx )               (M2)
          P (D = 0|X = x, Fy|x (Y ) = τ ) = Λ(b1,x τ + b2,x τ 2 + δx )                                 (M3)

where Λ(·) = exp(·)/(1 + exp(·)) is the Logistic CDF. These models differ primarily in the degree of
demographic bin heterogeneity allowed for in the relationship between earnings and the probability
of responding to the CPS. Model M1 relies entirely on the nonlinearities in the index function Λ(·)
to capture heterogeneity across cells in the response profiles. The model M2 allows for additional
heterogeneity through the interaction coefficients (γ1 , γ2 ) but restricts these interactions to be linear
in the cell fixed effect δx . Finally, M3, which is equivalent to a cell specific version of M1, places no
restrictions across demographic groups on the shape of the response profile.

    Maximum likelihood estimates from the three models are presented in Table 2.14 A comparison
of the model log likelihoods reveals that the introduction of the interaction terms (γ1 , γ2 ) in Model
2 yields a substantial improvement in fit over the basic separable logit of Model 1 despite the in-
significance of the resulting parameter estimates. However, the restrictions of the linearly interacted
Model 2 cannot be rejected relative to its fully interacted generalization in Model 3 which appears
  12
                                                   Rτ
      Note that P (D = 0, Fy|x (Y ) ≤ τ |X = x) = 0 P (D = 0|Fy|x (Y ) = u, X = x)du because Fy|x (Y ) is uniformly
                                                                   Rτ
distributed on [0, 1] conditional on X = x. Thus pL (x, τ ) = 0 P (D = 0|Fy|x (Y ) = u, X = x)du/τ . Likewise
            R1                                                        R1
pU (x, τ ) = τ P (D = 0|Fy|x (Y ) = u, X = x)du/(1 − τ ) and p(x) = 0 P (D = 0|Fy|x (Y ) = u, X = x)du.
   13
      We use five-year age categories instead of single digit ages and collapse years of schooling into four categories:
<12 years of schooling, 12 years of schooling, 13-15 years of schooling, and 16+ years of schooling.
   14
      We use the respondent’s sample quantile in his demographic cell’s distribution of Y as an estimate of Fy|x (Y ).
It can be shown that sampling errors in the estimated quantiles have asymptotically negligible effects on the limiting
distribution of the parameter estimates.


                                                          19
to be substantially overfit.

    A Wald test of joint significance of the earned income terms (b1 , b2 ) in the first model easily
rejects the null hypothesis that the data are missing at random. Evidently, missingness follows a U-
shaped response pattern with very low and very high wage men least likely to provide valid earnings
information – a pattern conjectured (but not directly verified) by Lillard et al. (1986). This pattern
is also found in the two more flexible logit models as illustrated in the third panel of the table which
provides the average marginal effects of earnings evaluated at three quantiles of the distribution.
These average effects are consistently negative at τ = 0.2 and positive at τ = 0.8. It is important
to note however that Models 2 and 3 allow for substantial heterogeneity across covariate bins in
these marginal effects which in some cases yields response patterns that are monotonic rather than
U-shaped.

    It is straightforward to estimate the distance between missing and nonmissing earnings distribu-
tions in each demographic bin by integrating our estimates of P (D = 0|X = x, Fy|x (Y ) = τ ) across
the relevant quantiles of interest. We implement this integration numerically via one dimensional
Simpson quadrature. The bottom panel of Table 2 shows quantiles of the distribution of resulting
cell specific KS distance estimates. Model 1 is nearly devoid of heterogeneity in KS distances across
demographic bins because of the additive separability implicit in the model. Model 2 yields sub-
stantially more heterogeneity with a minimum KS distance of 0.02 and a maximum distance S(F )
of 0.12. Finally, Model 3, which we suspect has been overfit, yields a median KS distance of 0.11
and an enormous maximum KS distance of 0.44.

   Figure 9 provides a visual representation of our estimates from Model 2 of the underlying distance
functions |Fy|1,x (q(τ |x))−Fy|0,x (q(τ |x))| in each of the 33 demographic bins in our sample. The outer
envelope of these functions corresponds to the quantile specific level of selection considered in the
breakdown analysis of Figure 7, while the maximum point on the envelope corresponds to S(F ).
Note that while some of the distance functions exhibit an unbroken inverted U shaped pattern
others exhibit double or even triple arches. The pattern of multiple arches occurs when the CDFs
are estimated to have crossed at some quantile which yields a distance of zero at that point. A
quadratic relationship between missingness and earnings can easily yield such patterns. Because of
the interactions in Model 2, some cells exhibit effects that are not quadratic and tend to generate
CDFs exhibiting first order stochastic dominance. It is interesting to note that the demographic cell
obtaining the maximum KS distance of 0.12 corresponds to young (age 25-30), black, high school
dropouts for whom more IRS earnings are estimated to monotonically increase the probability of
responding to the CPS earnings question. This leads to a distribution of observed earnings which
stochastically dominates that of the corresponding unobserved earnings.

    Our estimates of selection in Figure 9, when compared to the breakdown function of Figure 7,
reinforce our earlier conclusion that most of the apparent changes in wage structure between 1980
and 1990 are robust to plausible violations of MAR but that conclusions regarding lower quantiles
could be overturned by selective non-response. Likewise, the apparent emergence of heterogeneity in


                                                   20
the returns function in 2000, may easily be justified by selection of the magnitude found in our CPS
sample. Though our estimates of selection are fairly sensitive to the manner in which cell specific
heterogeneity is modeled, we take the patterns in Table 2 and Figure 9 as suggestive evidence that
small, but by no means negligible, deviations from missing at random are likely present in modern
earnings data. These deviations may yield complicated discrepancies between observed and missing
CDFs about which it is hard to develop strong priors. We leave it to future research to examine
these issues more carefully with additional validation datasets. Given however the likely absence of
such prior knowledge for most prospective studies, we expect the sensitivity techniques developed
in this paper to be quite useful for applied research.



6    Conclusion

We have proposed assessing the sensitivity of estimates of conditional quantile functions with missing
outcome data to violations of the MAR assumption by considering the minimum level of selection,
as indexed by the maximal KS distance between the distribution of missing and nonmissing out-
comes across all covariate values, necessary to overturn conclusions of interest. Inferential methods
were developed that account for uncertainty in estimation of the nominal identified set and that
acknowledge the potential for model misspecification. We found in an analysis of U.S. Census data
that the well-documented increase in the returns to schooling between 1980 and 1990 is relatively
robust to alternative assumptions on the missing process, but that conclusions regarding hetero-
geneity in returns and changes in the returns function between 1990 and 2000 are very sensitive to
departures from ignorability.

    While we have focused on methods for gauging sensitivity to non-response in cross-sectional
datasets, a number of interesting extensions are possible. An obvious (and important) one is an
adaptation to environments where missingness arises due to non-participation in the labor force
as in Heckman (1974) and Blundell et al. (2007). Additional side restrictions may be appropriate
here, particularly in a panel data setting of the sort studied by Johnson et al. (2000) and Neal
(2004). Another extension is to treatment effects where, again, researchers may wish to combine
our nominal KS restriction with additional restrictions of the sort studied by Lee (2009) or (if
outcomes are discrete) those of Shaikh and Vytlacil (2005) and Bhattacharya et al. (2008). Adding
restrictions will shrink the nominal identified set and, in general, reduce sensitivity, but will also
substantially complicate inference. We leave the development of such methods to future work.




                                                 21
References
Acemoglu, D. and Autor, D. H. (2010). Skills, tasks, and technology: Implications for employment
  and earnings. In Handbook of Labor Economics, vol. 4. Elsevier, Forthcoming.

Altonji, J. G., Elder, T. and Taber, C. (2005). Selection on observed and unobserved variables:
  Assessing the effectiveness of catholic schools. Journal of Political Economy, 113 151–184.

Altonji, J. G., Elder, T. and Taber, C. (2008). Using selection on observed variables to assess bias
  from unobservables when evaluating swan-ganz catheterization. American Economic Review, Paper and
  Proceedings, 98 145–350.

Angrist, J., Chernozhukov, V. and Fernández-Val, I. (2006). Quantile regression under misspeci-
  fication, with an application to the u.s. wage structure. Econometrica, 74 539–563.

Autor, D. H. and Katz, L. F. (1999). Changes in the wage structure and earning inequality. In Handbook
  of Labor Economics, vol. 3A. North-Holland, 1463–1555.

Bhattacharya, J., Shaikh, A. and Vytlacil, E. (2008). Treatment effect bounds under monotonicity
  assumptions: An application to swan-ganz catheterization. American Economic Review, Papers and
  Proceedings, 98 351–356.

Blundell, R., Gosling, A., Ichimura, H. and Meghir, C. (2007). Changes in the distribution of male
  and female wages accounting for employment composition using bounds. Econometrica, 75 323–363.

Card, D. and Lemieux, T. (1996). Wage dispersion, returns to skill, and black-white wage differentials.
  Journal of Econometrics, 74 319–361.

Chamberlain, G. (1994). Quantile regression, censoring and the structure of wages. In Advances in
  Econometrics, Sixth World Congress (C. Sims, ed.). Elsevier, 171–209.

DiNardo, J., McCrary, J. and Sanbonmatsu, L. (2006). Constructive proposals for dealing with
  attrition: An empirical example. Working paper, University of Michigan.

Greenlees, J. S., Reece, W. S. and Zieschang, K. D. (1982). Imputations of missing values when
  the probability of response depends on the variable being imputed. Journal of the American Statistical
  Association, 77 251–261.

Heckman, J. J. (1974). Shadow prices, market wages, and labor supply. Econometrica, 42 679–694.

Heckman, J. J., Lochner, L. and Todd, P. (2006). Earnings functions, rates of return and treatment
  effects: The mincer equation and beyond. In Handbook of Education Economics (E. Hanushek and
  F. Welch, eds.), vol. 1, chap. 7. Elsevier.

Hirsch, B. T. and Schumacher, E. J. (2004). Match bias in wage gap estimates due to earnings
  imputation. Journal of Labor Economics, 22 689–722.

Horowitz, J. L. and Manski, C. F. (2006). Identification and estimation of statistical functionals using
  incomplete data. Journal of Econometrics, 132 445–459.


                                                   22
Imbens, G. W. (2003). Sensitivity to exogeneity assumptions in program evaluation. American Economic
  Review, Papers and Proceedings, 93 126–132.

Johnson, W., Neal, D. and Kitamura, Y. (2000). Evaluating a simple method for estimating black-
  white gaps in median wages. American Economic Review, Papers and Proceedings, 90 339–343.

Juhn, C., Murphy, K. M. and Pierce, B. (1993). Wage inequality and the rise in the return to skill.
  Journal of Political Economy, 101 410–442.

Lee, D. (2009). Training, wages, and sample selection: Estimating sharp bounds on treatment effects.
  Review of Economic Studies, 76 1071–1102.

Lemieux, T. (2006). Postsecondary education and increasing wage inequality. The American Economic
  Review, 96 195–199.

Lillard, L., Smith, J. P. and Welch, F. (1986). What do we really know about wages? the importance
  of nonreporting and census imputation. The Journal of Political Economy, 94 489–506.

Manski, C. F. (1994). The selection problem. In Advances in Econometrics, Sixth World Congress
  (C. Sims, ed.). Cambridge University Press, Cambridge, UK, 143–170.

Manski, C. F. (2003). Partial Identification of Probability Distributions. Springer-Verlag, New York.

Neal, D. (2004). The measured black-white wage gap is too small. Journal of Political Economy, 112
  S1–S28.

Ponomareva, M. and Tamer, E. (2009). Misspecification in moment inequality models: Back to moment
  equalities? Working paper, Northwestern University.

Rosenbaum, P. (2002). Observational Studies. Springer, New York.

Rosenbaum, P. and Rubin, D. (1983). Assessing sensitivity to an unobserved binary covarite in an
  observational study with binary outcome. Journal of the Royal Statistical Society, Series B, 45 212–218.

Rubin, D. B. (1976). Inference and missing data. Biometrika 581–592.

Shaikh, A. M. and Vytlacil, E. (2005). Threshold crossing models and bounds on treatment effects:
  A nonparametric analysis. Working paper, University of Chicago.

Stoye, J. (2007). Bounds on generalized linear predictors with incomplete outcome data. Reliable Com-
  puting, 13 293–302.

van der Vaart, A. W. and Wellner, J. A. (1996). Weak Convergence and Empirical Processes: with
  Applications to Statistics. Springer, New York.




                                                    23
          Appendix A - The bivariate normal selection model and KS distance


To develop intuition for our metric S(F ) of deviations from missing at random we provide here a
mapping between the parameters of a standard bivariate selection model, the resulting CDFs of
observed and missing outcomes, and the implied values of S(F ). Using the notation of Section 2,
our DGP of interest is:

                         (Yi , vi ) ∼ N (0, ρ1 ρ1 )
                                                 
                                                     Di = 1{µ + vi > 0} .                   (46)

In this model, the parameter ρ indexes the degree of non-ignorable selection in the outcome variable
Yi . We choose µ = .6745 to ensure a missing fraction of 25% which is approximately the degree of
missingness found in our analysis of earnings data in the US Census. We computed the distributions
of missing and observed outcomes for various values of ρ by simulation, some of which are plotted
in Figures A.1 and A.2. The resulting values of S(F ), which correspond to the maximum vertical
distance between the observed and missing CDFs across all points of evaluation, are given in the
table below:

                                                                     Table A.1: S(F ) as a function of ρ

                                              ρ           S(F )                     ρ      S(F )                       ρ         S(F )
                                             0.05 0.0337                           0.35 0.2433                       0.65 0.4757
                                             0.10 0.0672                           0.40 0.2778                       0.70 0.5165
                                             0.15 0.1017                           0.45 0.3138                       0.75 0.5641
                                             0.20 0.1355                           0.50 0.3520                       0.80 0.6158
                                             0.25 0.1721                           0.55 0.3892                       0.85 0.6717
                                             0.30 0.2069                           0.60 0.4304                       0.90 0.7377



  Figure A.1: Missing and Observed Outcome CDFs                                                           Figure A.2: Vertical Distance Between CDFs
                                 Missing and Observed Outcome CDFs                                                               Vertical Distance Between CDFs
   1                                                                                                0.4
          Observed (ρ=.1)                                                                                     ρ=.1
          Missing (ρ=.1)                                                                                      ρ=.5
  0.9
          Observed (ρ=.5)                                                                          0.35
          Missing (ρ=.5)
  0.8

                                                                                                    0.3
  0.7

                                                                                                   0.25
  0.6


  0.5                                                                                               0.2


  0.4
                                                                                                   0.15

  0.3
                                                                                                    0.1
  0.2

                                                                                                   0.05
  0.1


   0                                                                                                 0
  −2.5   −2      −1.5       −1        −0.5        0     0.5          1   1.5   2    2.5             −2.5     −2      −1.5   −1      −0.5       0       0.5        1   1.5   2   2.5




                                                                                          24
                               Appendix B - Derivations of Section 5.2


    The following Appendix provides a justification for the derivations in Section 5.2, in particular
of the representation derived in equation (45). Towards this end, observe first that by Bayes’ rule:
                                     P (D = 1|X = x, Y ≤ c) × Fy|x (c)
                          Fy|1,x (c) =
                                                  p(x)
                                     P (D = 1|X = x, Fy|x (Y ) ≤ Fy|x (c)) × Fy|x (c)
                                   =                                                  ,                          (47)
                                                        p(x)
where the second equality follows from Fy|x being strictly increasing. Evaluating (47) at c = q(τ |x),
employing the definition of pL (x, τ ) in (43), and noting that Fy|x (q(τ |x)) = τ yields:
                                                                pL (τ, x) × τ
                                          Fy|1,x (q(τ |x)) =                  .                                  (48)
                                                                    p(x)
Moreover, by identical arguments, but working instead with the definition of pU (τ, x), we derive:
                          P (D = 1|Y > q(τ |x), X = x) × (1 − Fy|1,x (q(τ |x)))   pU (τ, x) × (1 − τ )
 1 − Fy|1,x (q(τ |x)) =                                                         =                      (49)
                                                 p(x)                                     p(x)
Finally, we note that the same manipulations applied to Fy|0,x instead of Fy|1,x enable us to obtain:
                         (1 − pL (τ, x)) × τ                                      (1 − pU (τ, x)) × (1 − τ )
    Fy|0,x (q(τ |x)) =                                  1 − Fy|0,x (q(τ |x)) =                               .   (50)
                              1 − p(x)                                                    1 − p(x)
Hence, we can obtain by direct algebra from the results (47) and (50) that we must have:
                                                                     |p(x) − pL (x, τ )| × τ
                           |Fy|1,x (q(τ |x)) − Fy|0,x (q(τ |x))| =                           .                   (51)
                                                                        p(x)(1 − p(x))
Analogously, exploiting (47) and (50) once again, we can also obtain:

             |Fy|1,x (q(τ |x)) − Fy|0,x (q(τ |x))| = |(1 − Fy|1,x (q(τ |x))) − (1 − Fy|0,x (q(τ |x)))|
                                                        |p(x) − pU (x, τ )| × (1 − τ )
                                                    =                                  .                         (52)
                                                              p(x)(1 − p(x))
The desired equality in (45) then follows immediately from taking the square root of the product
of (51) and (52).




                                                           25
                                     Appendix C - Proof of Results

Lemma 6.1. Under Assumptions 2.1(ii)-(iii), if S(F ) ≤ k, then the nominal identified set C(τ, k) equals:

{θ : X → R : τ − min{τ + kp(x), 1} × {1 − p(x)} ≤ Fy|1,x (θ(x))p(x) ≤ τ − max{τ − kp(x), 0} × {1 − p(x)}}


Proof of Lemma 6.1: Letting KS(Fy|1,x , Fy|0,x ) = supc |Fy|1,x (c) − Fy|0,x (c)|, we first observe that:
                                   1
            KS(Fy|1,x , Fy|0,x ) =    × sup |F   (c) × p(x) + Fy|0,x (c) × {1 − p(x)} − Fy|0,x (c)|
                                  p(x) c∈R y|1,x
                                   1
                                =     × sup |F (c) − Fy|0,x (c)| .                                             (53)
                                  p(x) c∈R y|x
Therefore, if θ(x) = cτ (x), then it immediately follows from the hypothesis S(F ) ≤ k and result (53) that:

                      τ = Fy|1,x (θ(x)) × p(x) + Fy|0,x (θ(x)) × {1 − p(x)}
                        ≤ Fy|1,x (θ(x)) × p(x) + min{Fy|x (θ(x)) + kp(x), 1} × {1 − p(x)}
                        = Fy|1,x (θ(x)) × p(x) + min{τ + kp(x), 1} × {1 − p(x)} .                              (54)

By identical manipulations, Fy|1,x (θ(x)) × p(x) ≤ τ − max{τ − kp(x), 0} × {1 − p(x)} and hence θ ∈ C(τ, k).
To prove the bounds are sharp, let θ ∈ C(τ, k) and define the function κ : X → R by:
                                                  τ − Fy|1,x (θ(x)) × p(x)
                                         κ(x) ≡                            .                                   (55)
                                                         1 − p(x)
Further observe that by virtue of θ ∈ C(τ, k), the following two inequalities hold uniformly in x ∈ X :

          max{τ − kp(x), 0} ≤ κ(x) ≤ min{τ + kp(x), 1}                          |κ(x) − Fy|1,x (θ(x))| ≤ k .   (56)

We now aim to construct a distribution for Y conditional on X and Y being missing such that all assump-
tions are met and in addition θ is the conditional quantile of Y given X. Define:

            +                                            1
          F̃y|0,x (c) ≡1{c ≥ θ(x)} × max{Fy|1,x (c), min{ (Fy|1,x (c) − Fy|1,x (θ(x))) + κ(x), 1}}
                                                         2
                       + 1{c < θ(x)} × max{Fy|1,x (c), 2(Fy|1,x (c) − Fy|1,x (θ(x))) + κ(x)}
            −
          F̃y|0,x (c) ≡1{c ≥ θ(x)} × min{Fy|1,x (c), 2(Fy|1,x (c) − Fy|1,x (θ(x))) + κ(x)}
                                                           1
                       + 1{c < θ(x)} × min{Fy|1,x (c), max{ (Fy|1,x (c) − Fy|1,x (θ(x))) + κ(x), 0}}           (57)
                                                           2
and let the distribution of Y conditional on X and Y being unobservable be pointwise given by:

                                                       +                                       −
             F̃y|0,x (c) ≡ 1{κ(x) ≥ Fy|1,x (θ(x))} × F̃y|0,x (c) + 1{κ(x) < Fy|1,x (θ(x))} × F̃y|1,x (c) .     (58)

Note that F̃y|0,x (c) is strictly increasing and continuous at all c such that 0 < Fy|0,x (c) < 1 by virtue
of Fy|1,x (c) being strictly increasing and continuous. Since F̃y|0,x is bounded between zero and one, we
conclude it is a properly defined cdf. Denoting F̃y|x (c) = Fy|1,x (c) × p(x) + F̃y|0,x (c) × {1 − p(x)}, we obtain:

 F̃y|x (θ(x)) = Fy|1,x (θ(x))×p(x)+ F̃y|0,x (θ(x))×{1−p(x)} = Fy|1,x (θ(x))×p(x)+κ(x)×{1−p(x)} = τ , (59)




                                                          26
so that θ(x) is the conditional τ -quantile of Y given X. In addition, by construction and (56) we have:

                                  sup |F̃y|0,x (c) − Fy|1,x (c)| = |F̃y|0,x (θ(x)) − Fy|1,x (θ(x))| ≤ k ,                       (60)
                                  c∈R

uniformly in x ∈ X . It follows that S(F ) ≤ k and hence conclude the bounds are sharp as claimed.

Proof of Lemma 2.1: Follows immediately from Lemma 6.1.

Proof of Lemma 3.1: For any θ ∈ C(τ, k), the first order condition of the norm minimization problem
yields β(τ ) = (ES [Xi Xi0 ])−1 ES [Xi θ(Xi )]. The Lemma then follows from Corollary 2.1.

Proof of Corollary 3.1: Since P(τ, k) is convex by Lemma 3.1, it follows that the identified set for
λ0 β(τ ) is a convex set in R and hence an interval. The fact that πL (τ, k) and πU (τ, k) are the endpoints
of such interval follows directly from Lemma 3.1.

Lemma 6.2. Let Assumption 2.1 hold, Wi be independent of (Yi , Xi , Di ) with E[Wi ] = 1 and E[Wi2 ] < ∞
and positive almost surely. If {Yi , Xi , Di , Wi } is an i.i.d. sample, then the following class is Donsker:

         M ≡ {mc : mc (y, x, d, w) ≡ w1{y ≤ c, d = 1, x = x0 } − P (Yi ≤ c, Di = 1, Xi = x0 ), c ∈ R} .


Proof: For any 1 >  > 0, by Assumption 2.1(ii) there is an increasing sequence {y0 , . . . , yd 4 e } such that
                                                                                                                     
                    d 8 e
for   {[yj−1 , yj ]}i=1      partitions R and for any 1 ≤ j ≤       d 8 e   we have Fy|1,x (yj ) − Fy|1,x (yj−1 ) < /4. Let

                      lj (y, x, d, w) ≡ w1{y ≤ yj−1 , d = 1, x = x0 } − P (Yi ≤ yj , Di = 1, Xi = x0 )                          (61)
                      uj (y, x, d, w) ≡ w1{y ≤ yj , d = 1, x = x0 } − P (Yi ≤ yj−1 , Di = 1, Xi = x0 )                          (62)

                                             d8e
and notice the brackets {[lj , uj ]}j=1
                                     
                                        form a partition of the class Mc (since w ∈ R+ ). In addition, note:

         E[(uj (Yi , Xi , Di , Wi ) − lj (Yi , Xi , Di , Wi ))2 ]
                  ≤ 2E[Wi2 1{yj−1 ≤ Yi ≤ yj , Di = 1, Xi = x0 }] + 2P 2 (yj−1 ≤ Yi ≤ yj , Di = 1, Xi = x0 )
           ≤ 4E[Wi2 ] × (Fy|1,x (yj ) − Fy|1,x (yj−1 )) ,                                             (63)
                                                  q
and hence each bracket has norm bounded by E[Wi2 ]. Therefore, N[ ] (, M, k · kL2 ) ≤ 16E[Wi2 ]/2 , and
the Lemma follows by Theorem 2.5.6 in van der Vaart and Wellner (1996).

Lemma 6.3. Let Assumption 2.1 hold, Wi be independent of (Yi , Xi , Di ) with E[Wi ] = 1, E[Wi2 ] < ∞ and
positive almost surely. Also let S ≡ {(τ, b) ∈ [0, 1]2 : b{1 − p(x)} +  ≤ τ ≤ p(x) + b{1 − p(x)} −  ∀x ∈ X }
for some  satisfying 0 < 2 < inf x∈X p(x) and denote the minimizers:

                             s0 (τ, b, x) = arg min Qx (c|τ, b)          ŝ0 (τ, b, x) ∈ arg min Q̃x,n (c|τ, b) .               (64)
                                               c∈R                                           c∈R

Then s0 (τ, b, x) is bounded in (τ, b, x) ∈ S × X and if {Yi , Xi , Di , Wi } is i.i.d. then for some M > 0:
                                                                    
                                    P sup sup |ŝ0 (τ, b, x)| > M = o(1) .
                                                   x∈X (τ,b)∈S


Proof: First note that Assumption 2.1(ii) implies s0 (τ, b, x) is uniquely defined, while ŝ0 (τ, b, x) may be


                                                                    27
one of multiple minimizers. By Assumption 2.1(ii) and the definition of S, it follows that the equality:

                            P (Yi ≤ s0 (τ, b, x), Di = 1|Xi = x) = τ − bP (Di = 0|Xi = x)                                    (65)

implicitly defines s0 (τ, b, x). Let s̄(x) and s(x) be the unique numbers satisfying Fy|1,x (s̄(x))×p(x) = p(x)−
and Fy|1,x (s(x)) × p(x) = . By result (65) and the definition of the set S we then obtain that for all x ∈ X :

                       −∞ < s(x) ≤ inf s0 (τ, b, x) ≤ sup s0 (τ, b, x) ≤ s̄(x) < +∞ .                                        (66)
                                          (τ,b)∈S                  (τ,b)∈S

Hence, we conclude that sup(τ,b)∈S |s0 (τ, b, x)| = O(1) and the first claim follows by X being finite.

   In order to establish the second claim of the Lemma, we define the functions:

                               Rx (τ, b) ≡ bP (Di = 0, Xi = x) − τ P (Xi = x)                                                (67)
                                                  n
                                             1    X
                             Rx,n (τ, b) ≡              Wi (b1{Di = 0, Xi = x} − τ 1{Xi = x})                                (68)
                                             n
                                                  i=1

as well as the maximizers and minimizers of Rx,n (τ, b) on the set S, which we denote by:

             (τ n (x), bn (x)) ∈ arg max Rx,n (τ, b)                  (τ̄n (x), b̄n (x)) ∈ arg min Rx,n (τ, b) .             (69)
                                        (τ,b)∈S                                                      (τ,b)∈S

Also denote the set of maximizers and minimizers of Q̃x,n (c|τ, b) at these particular choices of (τ, b) by:
                                n                                                           o
                       S n (x) ≡ sn (x) ∈ R : sn (x) ∈ arg min Q̃x,n (c|τ n (x), bn (x))                  (70)
                                                             c∈R
                                n                                                          o
                       S̄n (x) ≡ s̄n (x) ∈ R : s̄n (x) ∈ arg min Q̃x,n (c|τ̄n (x), b̄n (x))               (71)
                                                                       c∈R

From the definition of Q̃x,n (c|τ, b), we then obtain from (69), (70) and (71) that for all x ∈ X :

                      inf         sn (x) ≤ inf ŝ0 (τ, b, x) ≤ sup ŝ0 (τ, b, x) ≤                   sup         s̄n (x) .   (72)
                 sn (x)∈S n (x)           (τ,b)∈S                  (τ,b)∈S                     s̄n (x)∈S̄n (x)

We establish the second claim of the Lemma, by exploiting (72) and showing that for some 0 < M < ∞:
                                                                            
              P      inf    sn (x) < −M = o(1)         P      sup   s̄n (x) > M = o(1) .         (73)
                   sn (x)∈S n (x)                                            s̄n (x)∈S̄n (x)

To prove that inf sn (x)∈S n (x) sn (x) is larger than −M with probability tending to one, note that:

            |Rx,n (τ n (x), bn (x)) + P (Xi = x)| = |Rx,n (τ n (x), bn (x)) − max Rx (τ, b)| = op (1) ,                     (74)
                                                                                           (τ,b)∈S

where the second equality follows from the Theorem of the Maximum and the continuous mapping theorem.
Therefore, using the equality a2 − b2 = (a − b)(a + b), result (74) and Lemma 6.2, it follows that:

                    sup |Q̃x,n (c|τ n (x), bn (x)) − (Fy|1,x (c)p(x) − )2 P 2 (Xi = x)| = op (1) .                          (75)
                    c∈R

Fix δ > 0 and note that since Fy|1,x (s(x))p(x) =  and /p(x) < 1, Assumption 2.1(ii) implies that:

                                         η≡         inf     (Fy|1,x (c)p(x) − )2 > 0 .                                      (76)
                                              |c−s(x)|>δ




                                                                 28
Therefore, it follows from direct manipulations and the definition of S n (x) in (70) and of s(x) that:
                                                                                                            
P |      inf       sn (x) − s(x)| > δ ≤ P      inf     Q̃x,n (c|τ n (x), bn (x)) ≤ Q̃x,n (s(x)|τ n (x), bn (x))
    sn (x)∈S n (x)                          |c−s(x)|>δ
                                                                                                                   
                                       ≤ P η ≤ sup 2|Q̃x,n (c|τ n (x), bn (x)) − (Fy|1,x (c)p(x) − )2 P 2 (Xi = x)| .
                                                          c∈R
                                                                           p
We hence conclude from (75) that inf sn (x)∈S n (x) sn (x) → s(x), which together with (66) implies that
inf sn (x)∈S n (x) sn (x) is larger than −M with probability tending to one for some M > 0. By similar
                                                                    p
arguments it can be shown that sups̄n (x)∈S̄n (x) s̄n (x) → s̄(x) which together with (66) establishes (73). The
second claim of the Lemma then follows from (72), (73) and X being finite.

Lemma 6.4. Let Assumption 2.1 hold, Wi be independent of (Yi , Xi , Di ) with E[Wi ] = 1, E[Wi2 ] < ∞ and
positive almost surely. Also let S ≡ {(τ, b) ∈ [0, 1]2 : b{1 − p(x)} +  ≤ τ ≤ p(x) + b{1 − p(x)} −  ∀x ∈ X }
for some  satisfying 0 < 2 < inf x∈X p(x) and denote the minimizers:

                     s0 (τ, b, x) = arg min Qx (c|τ, b)                 ŝ0 (τ, b, x) ∈ arg min Q̃x,n (c|τ, b) .          (77)
                                           c∈R                                             c∈R

If {Yi , Xi , Di , Wi } is an i.i.d. sample, then supx∈X sup(τ,b)∈S |ŝ0 (τ, b, x) − s0 (τ, b, x)| = op (1).


Proof: First define the criterion functions M : L∞ (S × X ) → R and Mn : L∞ (S × X ) → R by:

             M (θ) ≡ sup sup Qx (θ(τ, b, x)|τ, b)                       Mn (θ) ≡ sup sup Q̃x,n (θ(τ, b, x)|τ, b) .        (78)
                       x∈X (τ,b)∈S                                                x∈X (τ,b)∈S

For notational convenience, let s0 ≡ s0 (·, ·, ·) and ŝ0 ≡ ŝ0 (·, ·, ·). By Lemma 6.3, s0 ∈ L∞ (S × X ) while
with probability tending to one ŝ0 ∈ L∞ (S × X ). Hence, (77) implies that with probability tending to one:

                             ŝ0 ∈ arg        min       Mn (θ)           s0 = arg       min       M (θ) .                 (79)
                                         θ∈L∞ (S×X )                                θ∈L∞ (S×X )

By Assumption 2.1(ii) and (65), Qx (c|τ, b) is strictly convex in a neighborhood of s0 (τ, b, x). Furthermore,
since by (65) and the implicit function theorem s0 (τ, b, x) is continuous in (τ, b) ∈ S for every x ∈ X :

      inf       M (θ) ≥ inf     inf            inf        Qx (c|τ, b)
  kθ−s0 k∞ ≥δ           x∈X (τ,b)∈S |c−s0 (τ,b,x)|≥δ

                                      = inf          inf min{Qx (s0 (τ, b, x) − δ|τ, b), Qx (s0 (τ, b, x) + δ|τ, b)} > 0 , (80)
                                          x∈X (τ,b)∈S

where the final inequality follows by compactness of S which together with continuity of s0 (τ, b, x) implies
the inner infimum is attained, while the outer infimum is trivially attained due to X being finite. Since
(80) holds for any δ > 0, s0 is a well separated minimum of M (θ) in L∞ (S × X ). Next define:

                                           Gx,i (c) ≡ Wi 1{Yi ≤ c, Di = 1, Xi = x}                                        (81)

and observe that compactness of S, a regular law of large numbers, Lemma 6.2 and finiteness of X yields:
                        n
                  1X
  sup sup sup |      Gx,i (c) + Rx,n (τ, b) − E[Gx,i (c)] − Rx (τ, b)|
  x∈X (τ,b)∈S c∈R n    i=1
                                          n
                                  1X
                      ≤ sup sup |    Gx,i (c) − E[Gx,i (c)]| + sup sup |Rx,n (τ, b) − Rx (τ, b)| = op (1) , (82)
                        x∈X c∈R n        i=1                   x∈X (τ,b)∈S



                                                                   29
where Rx (τ, b) and Rx,n (τ, b) are as in (67) and (68) respectively. Hence, using (82), the equality a2 − b2 =
(a − b)(a + b) and Qx (c|τ, b) uniformly bounded in (c, τ, b) ∈ R × S due to the compactness of S, we obtain:

      sup       |Mn (θ) − M (θ)| ≤                  sup     sup sup |Q̃x,n (θ(τ, b, x)|τ, b) − Qx (θ(τ, b, x)|τ, b)|
  θ∈L∞ (S×X )                                 θ∈L∞ (S×X ) x∈X (τ,b)∈S

                                                                    ≤ sup sup sup |Q̃x,n (c|τ, b) − Qx (c|τ, b)| = op (1) . (83)
                                                                       x∈X (τ,b)∈S c∈R

The claim of the Lemma then follows from results (79), (80) and (83) together with Corollary 3.2.3 in
van der Vaart and Wellner (1996).

Lemma 6.5. Let Assumptions 2.1, 4.1 hold, Wi be independent of (Yi , Xi , Di ) with E[Wi ] = 1, E[Wi2 ] < ∞
and positive a.s. Also let S ≡ {(τ, b) ∈ [0, 1]2 : b{1 − p(x)} +  ≤ τ ≤ p(x) + b{1 − p(x)} −  ∀x ∈ X } for
some  satisfying 0 < 2 < inf x∈X p(x) and denote the minimizers:

                         s0 (τ, b, x) = arg min Qx (c|τ, b)                 ŝ0 (τ, b, x) ∈ arg min Q̃x,n (c|τ, b) .             (84)
                                                   c∈R                                          c∈R

For Gx,i (c) ≡ Wi 1{Yi ≤ c, Di = 1, Xi = x} and Rx,n (τ, b) as defined in (68), denote the criterion function:
                                      n
                                   1 X                                                                                 2
            Q̃sx,n (c|τ, b) ≡                 {E[Gx,i (c) − Gx,i (s0 (τ, b, x))] + Gx,i (s0 (τ, b, x))} + Rx,n (τ, b)        .   (85)
                                    n
                                        i=1

If {Yi , Xi , Di , Wi } is an i.i.d. sample, it then follows that:

                                                           dQ̃sx,n (ŝ0 (τ, b, x)|τ, b)          1
                                          sup sup                                       = op (n− 2 ) .                           (86)
                                         x∈X (τ,b)∈S                     dc

Proof: We first introduce the criterion function Mns : L∞ (S × X ) → R to be given by:

                                                  Mns (θ) ≡ sup sup Q̃sx,n (θ(τ, b, x)|τ, b) .                                   (87)
                                                            x∈X (τ,b)∈S

We aim to characterize and establish the consistency of an approximate minimizer of Mns (θ) on L∞ (S ×X ).
Observe that by Lemma 6.2, compactness of S, finiteness of X and the law of large numbers:
                     n
              1X
  sup sup |      {Gx,i (s0 (τ, b, x)) − E[Gx,i (s0 (τ, b, x))]} + Rx,n (τ, b) − Rx (τ, b)|
  x∈X (τ,b)∈S n     i=1
                                              n
                                 1X
                     ≤ sup sup |    {Gx,i (c) − E[Gx,i (c)]}| + sup sup |Rx,n (τ, b) − Rx (τ, b)| = op (1) , (88)
                       x∈X c∈R n          i=1                   x∈X (τ,b)∈S

where Rx (τ, b) is as in (67). Hence, by definition of S and Rx (τ, b), with probability tending to one:
                           n
                1X
    P (Xi = x) ≤       {E[Gx,i (s0 (τ, b, x))] − Gx,i (s0 (τ, b, x))} − Rx,n (τ, b)
  2              n
                   i=1
                         
               ≤ (p(x) − )P (Xi = x)                                                                       ∀(τ, b, x) ∈ S × X . (89)
                         2
By Assumption 2.1(ii), whenever (89) holds, we may implicitly define ŝs0 (τ, b, x) by the equality:
                                                              n
                                                  1X
    P (Yi ≤   ŝs0 (τ, b, x), Di   = 1, Xi = x) =    {E[Gx,i (s0 (τ, b, x))] − Gx,i (s0 (τ, b, x))} − Rx,n (τ, b) ,              (90)
                                                  n
                                                             i=1




                                                                       30
for all (τ, b, x) ∈ S × X and set ŝs0 (τ, b, x) = 0 for all (τ, b, x) ∈ S × X whenever (89) does not hold. Thus,

                             sup sup |Q̃sx,n (ŝs0 (τ, b, x)|τ, b) − inf Q̃sx,n (c|τ, b)| = op (n−1 ) .                    (91)
                            x∈X (τ,b)∈S                               c∈R

Let ŝs0 ≡ ŝs0 (·, ·, ·) and note that by construction ŝs0 ∈ L∞ (S × X ). From (91) we then obtain that:

                 Mns (ŝs0 ) ≤ sup sup inf Q̃sx,n (c|τ, b) + op (n−1 ) ≤          inf       Mns (θ) + op (n−1 ) .          (92)
                              x∈X (τ,b)∈S c∈R                                 θ∈L∞ (S×X )

In order to establish kŝs0 − s0 k∞ = op (1), let M (θ) be as in (78) and notice that arguing as in (83) together
with result (88) and Lemma 6.2 implies that:

       sup       |Mns (θ) − M (θ)| ≤        sup       sup sup |Q̃sx,n (θ(τ, b, x)|τ, b) − Qx (θ(τ, b, x)|τ, b)|
  θ∈L∞ (S×X )                           θ∈L∞ (S×X )   x∈X (τ,b)∈S

                                                             ≤ sup sup sup |Q̃sx,n (c|τ, b) − Qx (c|τ, b)| = op (1) . (93)
                                                                x∈X (τ,b)∈S c∈R

Hence, by (80), (92), (93) and Corollary 3.2.3 in van der Vaart and Wellner (1996) we obtain:

                                        sup sup |ŝs0 (τ, b, x) − s0 (τ, b, x)| = op (1) .                                 (94)
                                       x∈X (τ,b)∈S


    Next, define the random mapping ∆n : L∞ (S × X ) → L∞ (S × X ) to be given by:
                   n
                1X
   ∆n (θ) ≡        {(Gx,i (θ(τ, b, x)) − E[Gx,i (θ(τ, b, x))]) − (Gx,i (s0 (τ, b, x)) − E[Gx,i (s0 (τ, b, x))])} ,         (95)
                n
                  i=1
                                                                                                   1
and observe that Lemma 6.2 and finiteness of X implies that k∆n (s̄)k∞ = op (n− 2 ) for any s̄ ∈ L∞ (S × X )
such that ks̄ − s0 k∞ = op (1). Since Q̃x,n (ŝ0 (τ, b, x)|τ, b) ≤ Q̃x,n (s0 (τ, b, x)|τ, b) for all (τ, b, x) ∈ S × X , and
by Lemma 6.2 and finiteness of X , supx∈X sup(τ,b)∈S Q̃x,n (s0 (τ, b, x)|τ, b) = Op (n−1 ), we conclude that:

   sup sup {Q̃sx,n (ŝ0 (τ, b, x)|τ, b) − Q̃sx,n (ŝs0 (τ, b, x)|τ, b)}
   x∈X (τ,b)∈S
                                                                                                                      1
        ≤ sup sup {Q̃x,n (ŝ0 (τ, b, x)|τ, b) − Q̃sx,n (ŝs0 (τ, b, x)|τ, b)} + k∆2n (ŝ0 )k∞ + 2k∆n (ŝ0 )k∞ × Mn2 (ŝ0 )
             x∈X (τ,b)∈S

        ≤ sup sup {Q̃x,n (ŝs0 (τ, b, x)|τ, b) − Q̃sx,n (ŝs0 (τ, b, x)|τ, b)} + op (n−1 ) ,                               (96)
             x∈X (τ,b)∈S

where Mn (θ) is as in (78). Furthermore, since by (92) we have Mns (ŝs0 ) ≤ Mns (s0 ) + op (n−1 ) and by Lemma
6.2 and finiteness of X we have Mns (s0 ) = Op (n−1 ), similar arguments as in (96) imply that:

   sup sup {Q̃x,n (ŝs0 (τ, b, x)|τ, b) − Q̃sx,n (ŝs0 (τ, b, x)|τ, b)}
  x∈X (τ,b)∈S
                                                                                                          1
                                                          ≤ k∆n (ŝs0 )k2∞ + 2k∆n (ŝs0 )k∞ × [Mns (ŝs0 )] 2 = op (n−1 ) . (97)

Therefore, by combining the results in (91), (96) and (97), we are able to conclude that:

   sup sup {Q̃sx,n (ŝ0 (τ, b, x)|τ, b) − inf Q̃sx,n (c|τ, b)}
  x∈X (τ,b)∈S                                c∈R

                           ≤ sup sup {Q̃sx,n (ŝ0 (τ, b, x)|τ, b) − Q̃sx,n (ŝs0 (τ, b, x)|τ, b)} + op (n−1 ) ≤ op (n−1 ) . (98)
                             x∈X (τ,b)∈S




                                                                 31
                                                    1
   Let n & 0 be such that n = op (n− 2 ) and in addition satisfies:

                               sup sup |Q̃sx,n (ŝ0 (τ, b, x)|τ, b) − inf Q̃sx,n (c|τ, b)| = op (2n ) ,                    (99)
                               x∈X (τ,b)∈S                                 c∈R

which is possible by (98). A Taylor expansion at each (τ, b, x) ∈ S × X then implies:

            0 ≤ sup sup {Q̃sx,n (ŝ0 (τ, b, x) + n |τ, b) − Q̃sx,n (ŝ0 (τ, b, x)|τ, b)} + op (2n )
                   x∈X (τ,b)∈S
                               n     dQ̃sx,n (ŝ0 (τ, b, x)|τ, b) 2n d2 Q̃sx,n (s̄(τ, b, x)|τ, b) o
                = sup sup       n ×                             +   ×               2
                                                                                                     + op (2n ) ,         (100)
                   x∈X (τ,b)∈S                     dc              2               dc

where s̄(τ, b, x) is a convex combination of ŝ0 (τ, b, x) and ŝ0 (τ, b, x) + n . Since Lemma 6.4 and n & 0
imply that ks̄ − s0 k∞ = op (1), the mean value theorem, fy|1,x (c) being uniformly bounded and (83) yield:
                    n
                 1X
  sup sup           {E[Gx,i (s̄(τ, b, x)) − Gx,i (s0 (τ, b, x))] + Gx,i (s0 (τ, b, x))} + Rx,n (τ, b)
  x∈X (τ,b)∈S    n
                   i=1
                                                                                                             1
                                                ≤ sup fy|1,x (c)p(x)P (Xi = x) × ks̄ − s0 k∞ + Mn2 (s0 ) = op (1) . (101)
                                                    c∈R
                              0
Therefore, exploiting (101), fy|1,x (c) being uniformly bounded and by direct calculation we conclude:

                 d2 Q̃sx,n (s̄(τ, b, x)|τ, b)     2
  sup sup                                     − 2fy|1,x (s̄(τ, b, x))p2 (x)P 2 (Xi = x)
  x∈X (τ,b)∈S                 dc2
                                                              0
                                                  ≤ sup sup |fy|1,x (s̄(τ, b, x))p(x)P (Xi = x)| × op (1) = op (1). (102)
                                                       x∈X (τ,b)∈S

Thus, combining results (100) together with (102) and fy|1,x (c) uniformly bounded, we conclude:

                                                              dQ̃sx,n (ŝ0 (τ, b, x)|τ, b)
                                     0 ≤ n × sup sup                                      + Op (2n ) .                   (103)
                                                x∈X (τ,b)∈S                 dc

In a similar fashion, we note that by exploiting (99) and proceeding as in (100)-(103) we obtain:

           0 ≤ inf       inf {Q̃sx,n (ŝ0 (τ, b, x) − n |τ, b) − Q̃sx,n (ŝ0 (τ, b, x)|τ, b)} + op (2n )
                  x∈X (τ,b)∈S
                                 n            dQ̃x,n (ŝ0 (τ, b, x)|τ, b) 2n d2 Q̃sx,n (s̄(τ, b, x)|τ, b) o
                ≤ inf    inf         − n ×                              +   ×                               + op (2n )
                  x∈X (τ,b)∈S                              dc              2               dc2
                                           dQ̃x,n (ŝ0 (τ, b, x)|τ, b)
                ≤ −n × sup sup                                        + Op (2n ) .                                       (104)
                          x∈X (τ,b)∈S                   dc
                                     1
Therefore, since n = op (n− 2 ), we conclude from (103) and (104) that we must have:

                                              dQ̃sx,n (ŝ0 (τ, b, x)|τ, b)                     1
                                  sup sup                                  = Op (n ) = op (n− 2 ) .                       (105)
                                  x∈X (τ,b)∈S               dc

By similar arguments, but reversing the sign of n in (100) and (104) it possible to establish that:

                                                         dQ̃sx,n (ŝ0 (τ, b, x)|τ, b)          1
                                         sup sup −                                    = op (n− 2 ) .                       (106)
                                         x∈X (τ,b)∈S                   dc

The claim of the Lemma then follows from (105) and (106).

Lemma 6.6. Let Assumptions 2.1, 4.1 hold, Wi be independent of (Yi , Xi , Di ) with E[Wi ] = 1, E[Wi2 ] < ∞


                                                                     32
and positive a.s. Also let S ≡ {(τ, b) ∈ [0, 1]2 : b{1 − p(x)} +  ≤ τ ≤ p(x) + b{1 − p(x)} −  ∀x ∈ X } for
some  satisfying 0 <  < inf x∈X p(x) and denote the minimizers:

                      s0 (τ, b, x) = arg min Qx (c|τ, b)                  ŝ0 (τ, b, x) ∈ arg min Q̃x,n (c|τ, b) .             (107)
                                           c∈R                                                c∈R

If Gx,i (c) is as in (81), inf x∈X inf (τ,b)∈S fy|1,x (s0 (τ, b, x))p(x) > 0 and {Yi , Xi , Di , Wi } is i.i.d., then:

   sup sup       (ŝ0 (τ, b, x) − s0 (τ, b, x))
  x∈X (τ,b)∈S
                                n
                           1 X Gx,i (s0 (τ, b, x)) + Wi (b1{Di = 0, Xi = x} − τ 1{Xi = x})          1
                         −                                                                 = op (n− 2 ) . (108)
                           n                   P (Xi = x)p(x)fy|1,x (s0 (τ, b, x))
                               i=1


Proof: For Q̃sx,n (c|τ, b) as in (85), note that the mean value theorem and Lemma 6.5 imply:

                                                     d2 Q̃sx,n (s̄(τ, b, x)|τ, b) dQ̃sx,n (s0 (τ, b, x)|τ, b)          1
    sup sup       (ŝ0 (τ, b, x) − s0 (τ, b, x)) ×                  2
                                                                                 +                            = op (n− 2 )     (109)
   x∈X (τ,b)∈S                                                    dc                           dc

for s̄(τ, b, x) a convex combination of s0 (τ, b, x) and ŝ0 (τ, b, x). Also note that Lemma 6.2 implies:

                 dQ̃sx,n (s0 (τ, b, x)|τ, b)
   sup sup
  x∈X (τ,b)∈S                 dc
                                                                         n
                                                                    1X                                           1
  = sup sup        2fy|1,x (s0 (τ, b, x))p(x)P (Xi = x) × {            Gx,i (s0 (τ, b, x)) + Rn (τ, b)} = Op (n− 2 ) . (110)
     x∈X (τ,b)∈S                                                    n
                                                                         i=1

In addition, by (102), the mean value theorem and fy|1,x (c) being uniformly bounded we conclude that:

                 d2 Q̃x,n (s̄(τ, b, x)|τ, b)     2
   sup sup                                   − 2fy|1,x (s0 (τ, b, x))p2 (x)P 2 (Xi = 1)
  x∈X (τ,b)∈S                dc2
                 2                      2                                      0
     . sup sup |fy|1,x (s̄(τ, b, x)) − fy|1,x (s0 (τ, b, x))| + op (1) . sup |fy|1,x (c)| × ks̄ − s0 k∞ + op (1) . (111)
        x∈X (τ,b)∈S                                                                 c∈R

Since by assumption fy|1,x (s0 (τ, b, x))p(x) is bounded away from zero uniformly in (τ, b, x) ∈ S × X , it
follows from (111) and ks̄ − s0 k∞ = op (1) by Lemma 6.4 that for some δ > 0:

                                                             d2 Q̃x,n (s̄(τ, b, x)|τ, b)
                                               inf   inf                                 >δ                                    (112)
                                               x∈X (τ,b)∈S               dc2
with probability approaching one. Therefore, we conclude from results (109), (110) and (112) that we must
                                 1
have kŝ0 − s0 k∞ = Op (n− 2 ). Hence, by (109) and (111) we conclude that:

                                                                                              dQ̃sx,n (s0 (τ, b, x)|τ, b)          1
 sup sup                                       2
               2(ŝ0 (τ, b, x) − s0 (τ, b, x))fy|1,x (s0 (τ, b, x))p2 (x)P 2 (Xi = 1) +                                   = op (n− 2 )
 x∈X (τ,b)∈S                                                                                               dc
                                                                                                                                 (113)
The claim of the Lemma is then established by (110), (112) and (113).

Lemma 6.7. Let Assumptions 2.1, 4.1(ii)-(iii) hold, Wi be independent of (Yi , Xi , Di ) with E[Wi ] = 1,
E[Wi2 ] < ∞ and positive a.s. Let S ≡ {(τ, b) ∈ [0, 1]2 : b{1 − p(x)} +  ≤ τ ≤ p(x) + b{1 − p(x)} −  ∀x ∈ X }
for some  satisfying 0 < 2 < inf x∈X p(x) and for some x0 ∈ X , denote the minimizers:

                                                s0 (τ, b, x0 ) = arg min Qx0 (c|τ, b) .
                                                                      c∈R



                                                                    33
If inf (τ,b)∈S fy|1,x (s0 (τ, b, x0 ))p(x0 ) > 0 and {Yi , Xi , Di , Wi } is i.i.d., then the following class is Donsker:
   n                    w1{y ≤ s0 (τ, b, x0 ), d = 1, x = x0 } + bw1{d = 0, x = x0 } − τ w1{x = x0 }              o
F ≡ fτ,b (y, x, d, w) =                                                                              : (τ, b) ∈ S
                                              P (Xi = x0 )p(x0 )fy|1,x (s0 (τ, b, x0 ))

Proof: For  > 0, let {Bj } be a collection of closed balls in R2 with diameter  covering S. Further notice
that since S ⊆ [0, 1]2 , we may select {Bj } so its cardinality is less than 4/2 . On each Bj define:

                τ j = min(τ,b)∈S∩Bj τ                                  τ̄j = max(τ,b)∈S∩Bj τ

                bj = min(τ,b)∈S∩Bj b                                   b̄j = max(τ,b)∈S∩Bj b
                                                                                                                                 (114)
                sj = min(τ,b)∈S∩Bj s0 (τ, b, x0 )                      s̄j = max(τ,b)∈S∩Bj s0 (τ, b, x0 )

                f j = min(τ,b)∈S∩Bj fy|1,x (s0 (τ, b, x0 ))            f¯j = max(τ,b)∈S∩Bj fy|1,x (s0 (τ, b, x0 )) ,

where we note that all minimums and maximums are attained due to compactness of S ∩ Bj , continuity
of s0 (τ, b, x0 ) by (65) and the implicit function theorem and continuity of fy|1,x (c) by assumption 4.1(iii).
Next, for 1 ≤ j ≤ #{Bj } define the functions:
                            w1{y ≤ sj , d = 1, x = x0 } + bj w1{d = 0, x = x0 }      τ̄j w1{x = x0 }
        lj (y, x, d, w) ≡                                                       −                                                (115)
                                           P (Xi = x0 )p(x0 )f¯j                  P (Xi = x0 )p(x0 )f j
                            w1{y ≤ s̄j , d = 1, x = x0 } + b̄j w1{d = 0, x = x0 }      τ j w1{x = x0 }
        uj (y, x, d, w) ≡                                                         −                                              (116)
                                            P (Xi = x0 )p(x0 )f j                   P (Xi = x0 )p(x0 )f¯j

and note that the brackets [lj , uj ] cover the class F. Since f¯j−1 ≤ f −1
                                                                         j
                                                                            ≤ [inf (τ,b)∈S fy|1,x (s0 (τ, b, x0 ))]−1 < ∞
for all j, there is a finite constant M not depending on j so that M > 3E[W 2 ]P −2 (Xi = x0 )p−2 (x0 )fj −2 f¯−2
                                                                                               i                                    j
uniformly in j. To bound the norm of the bracket [lj , uj ] note that for such a constant M it follows that:

  E[(uj (Yi , Xi , Di , Wi ) − lj (Yi , Xi , Di , Wi ))2 ] ≤ M × (b̄j f¯j − bj f j )2 + M × (τ̄j f¯j − τ j f j )2

                             + M × E[(1{Yi ≤ sj , Di = 1, Xi = x0 }f j − 1{Yi ≤ s̄j , Di = 1, Xi = x0 }f¯j )2 ] (117)

Next observe that by the implicit function theorem and result (65) we can conclude that for any (τ, b) ∈ S:
                   ds0 (τ, b, x0 )              1                      ds0 (τ, b, x0 )         1 − p(x0 )
                                   =                                                   =−                         .              (118)
                        dτ           fy|1,x (s0 (τ, b, x0 ))                db            fy|1,x (s0 (τ, b, x0 ))

Since the minimums and maximums in (114) are attained, it follows that for some (τ1 , b1 ), (τ2 , b2 ) ∈ Bj ∩ S
we have s0 (τ1 , b1 .x0 ) = s̄j and s0 (τ2 , b2 , x0 ) = sj . Hence, the mean value theorem and (118) imply:
                                                                                                             √
                          1                                  1 − p(x)                                           2
 |s̄j − sj | =                            (τ1 − τ2 ) +                            (b1 − b2 ) ≤                                     (119)
               fy|1,x (s0 (τ̃ , b̃, x0 ))              fy|1,x (s0 (τ̃ , b̃, x0 ))              inf (τ,b)∈S fy|1,x (s0 (τ, b, x0 ))

where (τ̃ , b̃) is between (τ1 , b1 ) and (τ2 , b2 ) and the final inequality follows by (τ̃ , b̃) ∈ S by convexity of S,
(τ1 , b1 ), (τ2 , b2 ) ∈ Bj ∩ S and Bj having diameter . By similar arguments, and (119) we conclude:
                                                                                                 √
                   ¯                  0                                0                            2
                  |fj − f j | ≤ sup |fy|1,x (c)| × |s̄j − sj | ≤ sup |fy|1,x (c)| ×                                    .         (120)
                                c∈R                              c∈R                inf (τ,b)∈S y|1,x (s0 (τ, b, x0 ))
                                                                                               f




                                                                  34
Since bj ≤ b̄j ≤ 1 due to b̄j ∈ [0, 1] and |b̄j − bj | ≤  by Bj having diameter , we further obtain that:

                                                                                                           42
 (b̄j f¯j − bj f j )2 ≤ 2f¯j2 (b̄j − bj )2 + 2b2j (f¯j − f j )2 ≤ 2 sup fy|1,x
                                                                         2
                                                                               (c) × 2 +                2                      , (121)
                                                                 c∈R                        inf (τ,b)∈S fy|1,x (s0 (τ, b, x0 ))

where in the final inequality we have used result (120). By similar arguments, we also obtain:
                                                                                         42
                        (τ̄j f¯j − τ j f j )2 ≤ 2 sup fy|1,x
                                                       2
                                                             (c) × 2 +                2                      .                 (122)
                                                c∈R                       inf (τ,b)∈S fy|1,x (s0 (τ, b, x0 ))

Also note that by direct calculation, the mean value theorem and results (119) and (120) it follows that:

  E[(1{Yi ≤ sj , Di = 1,Xi = x0 }f j − 1{Yi ≤ s̄j , Di = 1, Xi = x0 }f¯j )2 ]

                             ≤ 2(f¯j − f j )2 + sup fy|1,x
                                                        2
                                                             (c) × P (Xi = x0 )p(x0 )(Fy|1,x (s̄j ) − Fy|1,x (sj ))
                                                 c∈R
                                                                                                    √
                                               42                        3                           2
                             ≤                                    + sup f    (c) ×                                      . (123)
                                             2
                               inf (τ,b)∈S fy|1,x (s0 (τ, b, x0 )) c∈R y|1,x        inf (τ,b)∈S fy|1,x (s0 (τ, b, x0 ))

Thus, from (117) and (121)-(122), it follows that for  < 1 and some constant K not depending on j:

                                  E[(uj (Yi , Xi , Di , Wi ) − lj (Yi , Xi , Di , Wi ))2 ] ≤ K .                               (124)

Since #{Bj } ≤ 4/, we can therefore conclude that N[ ] (, F, k · kL2 ) ≤ 4K/2 and hence by Theorem 2.5.6
in van der Vaart and Wellner (1996) it follows that the class F is Donsker.

Lemma 6.8. Let Assumptions 2.1, 4.1(ii)-(iii) hold, Wi be independent of (Yi , Xi , Di ) with E[Wi ] = 1,
E[Wi2 ] < ∞, positive a.s., S ≡ {(τ, b) ∈ [0, 1]2 : b{1 − p(x)} +  ≤ τ ≤ p(x) + b{1 − p(x)}∀x ∈ X } and
          Pn
                W 1{Di = 1, Xi = x}
  p̃(x) ≡ i=1 Pn i                          p(x) ≡ P (Di = 1|Xi = x)        s0 (τ, b, x) = arg min Qx (c|τ, b) .
                i=1 Wi 1{Xi = x}                                                               c∈R

If inf (τ,b,x)∈S×X fy|1,x (s0 (τ, b, x))p(x) > 0 and {Yi , Xi , Di , Wi } is an i.i.d. sample, then for a ∈ {−1, 1}:

  s0 (τ, τ + ak p̃(x), x) − s0 (τ, τ + akp(x), x)
                                                                                            n
                                                 (1 − p(x))ka             1X                         1
                              =−                                        ×    R(Xi , Wi , x) + op (n− 2 ) , (125)
                                 fy|1,x (s0 (τ, τ + akp(x), x))P (X = x) n
                                                                                            i=1

where R(Wi , Xi , x) = p(x){P (X = x) − Wi 1{Xi = x}} + Wi 1{Di = 1, Xi = x} − P (D = 1, X = x) and
(125) holds uniformly in (B × X ). Moreover, the right hand side of (125) is Donsker.


Proof: First observe that (τ, k) ∈ B implies (τ, τ + akp(x)) ∈ S for all x ∈ X , and that with probability
tending to one (τ, τ + ak p̃(x)) ∈ S for all (τ, k) ∈ B. In addition, also note that
                                                                   n
                                                    1     X                         1
                                p̃(x) − p(x) =              R(Xi , Wi , x) + op (n− 2 )                                         (126)
                                               nP (X = x)
                                                                  i=1

by an application of the Delta method and inf x∈X P (X = x) > 0 due to X having finite support. Moreover,




                                                                 35
by the mean value theorem and (118) we obtain for some b̄(τ, k) between τ + ak p̃(x) and τ + akp(x)
                                                                        (1 − p(x))ka
  s0 (τ, τ + ak p̃(x), x) − s0 (τ, τ + akp(x), x) = −                                          (p̃(x) − p(x))
                                                                  fy|1,x (s0 (τ, b̄(τ, k), x))
                                                                           (1 − p(x))ka                                  1
                                                        =−                                       (p̃(x) − p(x)) + op (n− 2 ) (127)
                                                                  fy|1,x (s0 (τ, τ + akp(x), x))

where the second equality follows from (τ, b̄(τ, k)) ∈ S for all (τ, k) with probability approaching one by
convexity of S, inf (τ,b,x)∈S×X fy|1,x (s0 (τ, b, x))p(x) > 0 and sup(τ,k)∈B |ak(p̃(x) − p(x))| = op (1) uniformly
in X . The first claim of the Lemma then follows by combining (126) and (127).

    Finally, observe that the right hand side of (125) is trivially Donsker since R(Xi , Wi , x) does not depend
on (k, τ ) and the function (1 − p(x))ka/(fy|1,x (s0 (τ, τ + akp(x), x))P (X = x)) is uniformly continuous on
(τ, k) ∈ B due to inf (τ,b,x)∈S×X fy|1,x (s0 (τ, b, x))p(x) > 0.

Proof of Theorem 4.1: Throughout the proof we exploit Lemmas 6.6 and 6.7 applied with Wi = 1
with probability one, so that Q̃x,n (c|τ, b) = Qx,n (c|τ, b) for all (τ, b) in S, where

                 S ≡ {(τ, b) ∈ [0, 1]2 : b{1 − p(x)} +  ≤ τ ≤ p(x) + b{1 − p(x)} −  ∀x ∈ X } .                                        (128)

Also notice that for every (τ, k) ∈ B and all x ∈ X , the points (τ, τ + kp(x)), (τ, τ − kp(x)) ∈ S, while with
probability approaching one (τ, τ + k p̂(x)) and (τ, τ − k p̂(x)) also belong to S. Therefore for s0 (τ, b, x) and
ŝ0 (τ, b, x) as defined in (107) we obtain from Lemmas 6.6 and 6.7, applied with Wi = 1 a.s., that:
                                                                                                                                    1
 |(ŝ0 (τ, τ + akp(x), x) − s0 (τ, τ + akp(x), x)) − (ŝ0 (τ, τ + ak p̂(x), x) − s0 (τ, τ + ak p̂(x), x))| = op (n− 2 ) (129)

uniformly in (τ, k, x) ∈ B × X and a ∈ {−1, 1}. Moreover, by Lemma 6.8 applied with Wi = 1 a.s.

  s0 (τ, τ + ak p̂(x), x) − s0 (τ, τ + akp(x), x)
                                                                                                   n
                                                     (1 − p(x))ka             1X                    1
                                  =−                                        ×    R(Xi , x) + op (n− 2 ) , (130)
                                     fy|1,x (s0 (τ, τ + akp(x), x))P (X = x) n
                                                                                                  i=1

where R(Xi , x) = p(x){P (X = x) − 1{Xi = x}} + 1{Di = 1, Xi = x} − P (D = 1, X = x) again uniformly
in (τ, k, x) ∈ B × X . Also observe that since (τ, τ + k p̂(x)) and (τ, τ − k p̂(x)) belong to S with probability
approaching one, we obtain uniformly in (τ, k, x) ∈ B × X that:

      qL (τ, k|x) = s0 (τ, τ + kp(x), x)                               qU (τ, k|x) = s0 (τ, τ − kp(x), x)
                                                                                                                                        (131)
                                                       − 21                                                             − 12
      q̂L (τ, k|x) = ŝ0 (τ, τ + k p̂(x), x) + op (n          )        q̂U (τ, k|x) = ŝ0 (τ, τ − k p̂(x), x) + op (n          ).

Therefore, combining results (129)-(131) and exploiting Lemmas 6.6, 6.7 and 6.8 and the sum of Donsker
classes being Donsker we conclude that for J a Gaussian process on L∞ (B × X ) × L∞ (B × X ):


                           √                                                q̂L (τ, k|x) − qL (τ, k|x) 
                                    L
                               nCn → J            Cn (τ, k, x) ≡                                            .                           (132)
                                                                             q̂U (τ, k|x) − qU (τ, k|x)


    Next, observe that since X has finite support, we may denote X = {x1 , . . . , x|X | } and define the matrix


                                                                      36
B = (P (Xi = x1 )x1 , . . . , P (Xi = x|X | )x|X | ) as well as the vector of weights:

                                                   w ≡ λ0 (ES [Xi Xi0 ])−1 B .                                                (133)

Since w is also a function on X , we refer to its coordinates by w(x). Solving the linear programming
problems in (10) and (11), it is then possible to obtain the closed form solution:
                                 X
                   πL (τ, k) =         {1{w(x) ≥ 0}w(x)qL (τ, k|x) + 1{w(x) ≤ 0}w(x)qU (τ, k|x)}
                                 x∈X
                                 X
                  πU (τ, k) =          {1{w(x) ≥ 0}w(x)qU (τ, k|x) + 1{w(x) ≤ 0}w(x)qL (τ, k|x)}                              (134)
                                 x∈X

with a similar representation holding for (π̂L (τ, k), π̂U (τ, k)) but with (q̂L (τ, k|x), q̂U (τ, k|x)) in place of
(qL (τ, k|x), qU (τ, k|x)). We hence define the linear map K : L∞ (B × X ) × L∞ (B × X ) → L∞ (B) × L∞ (B),
to be given by:

                                               ≥ 0}w(x)θ(1) (τ, k, x) + 1{w(x) ≤ 0}w(x)θ(2) (τ, k, x)} 
                            P
                             x∈X {1{w(x)
                        
        K(θ)(τ, k) ≡                                                                                                          (135)
                                                   0}w(x)θ(2) (τ, k, x)                   0}w(x)θ(1) (τ, k, x)}
                            P
                             x∈X {1{w(x)       ≥                          + 1{w(x) ≤

where for any θ ∈ L∞ (X × B) × L∞ (B × X ), θ(i) (τ, k, x) denotes the ith coordinate of the two dimensional
vector θ(τ, k, x). It then follows from (132), (134) and (135) that:


                                               √  π̂L − πL  √
                                                n            = nK(Cn ) .                                                      (136)
                                                     π̂U − πU

Moreover, employing the norm k · k∞ + k · k∞ on the product spaces L∞ (B × X ) × L∞ (B × X ) and
L∞ (B) × L∞ (B), we can then obtain by direct calculation that for any θ ∈ L∞ (B × X ) × L∞ (B × X ):
                                       X                                                X
                    kK(θ)k∞ ≤ 2              |w(x)| × sup sup |θ(τ, b, x)| = 2                |w(x)| × kθk∞ ,                 (137)
                                       x∈X             x∈X (τ,b)∈S                      x∈X

which implies the linear map K is continuous. Therefore, the theorem is established by (132), (136), the
linearity of K and the continuous mapping theorem.

Proof of Theorem 4.2: For a metric space D, let BLc (D) denote the set of real valued bounded Lipschitz
functions with supremum norm and Lipschitz constant less than or equal to c. We first aim to show that:

                                   sup        |E[h(L(G̃ω ))|Zn ] − E[h(L(Gω ))]| = op (1) ,                                   (138)
                                 h∈BL1 (R)

where Zn = {Yi , Xi , Di }ni=1 and E[h(Z̃)|Zn ] denotes outer expectation over {Wi }ni=1 with Zn fixed. Let

  ŝ0 (τ, b, x) ∈ arg min Qx,n (c|τ, b)        s̃0 (τ, b, x) ∈ arg min Q̃x,n (c|τ, b)         s0 (τ, b, x) ∈ arg min Qx (c|τ, b) .
                    c∈R                                           c∈R                                           c∈R
                                                                                                                              (139)
Also note that with probability approaching one the points (τ, τ + ak p̃(x)) ∈ S for all (τ, k, x) ∈ B × X




                                                                37
and a ∈ {−1, 1} for S as in (128). Hence, arguing as in (129) and (130) we obtain:

  q̃L (τ, k|x) − q̂L (τ, k|x) =s̃0 (τ, τ + kp(x), x) − ŝ0 (τ, τ + kp(x), x)
                                                                                    n
                                                 (1 − p(x))k             1X                          1
                               −                                       ×    ∆R(Xi , Wi , x) + op (n− 2 ) (140)
                                 fy|1,x (s0 (τ, τ + kp(x), x))P (X = x) n
                                                                                   i=1

  q̃U (τ, k|x) − q̂U (τ, k|x) =s̃0 (τ, τ − kp(x), x) − ŝ0 (τ, τ − kp(x), x)
                                                                                    n
                                                   (1 − p(x))k             1X                          1
                               +                                         ×    ∆R(Xi , Wi , x) + op (n− 2 ) (141)
                                   fy|1,x (s0 (τ, τ − kp(x), x))P (X = x) n
                                                                                   i=1

where ∆R(Xi , Wi , x) = (1 − Wi )(1{Xi = x}p(x) − 1{Di = 1, Xi = x}) and both statements hold uniformly
in (τ, k, x) ∈ B × X . Also note that for the operator K as defined in (135), we have:


               √  π̃L − π̂L  √                                             q̃L (τ, k|x) − q̂L (τ, k|x) 
                n             = nK(C̃n )                  C̃n (τ, k, x) ≡                                     .   (142)
                      π̃U − π̂U                                                q̃U (τ, k|x) − q̂U (τ, k|x)

By Lemmas 6.6, 6.7 and 6.8, results (140) and (141) and Theorem 2.9.2 in van der Vaart and Wellner
                   √
(1996), the process nC̃n converges unconditionally to a tight Gaussian process on L∞ (B × X ). Hence,
                                   √
by the continuous mapping theorem, nK(C̃n ) is asymptotically tight. Define,


                                                    √  (π̃L − π̂L )/ωL 
                                            Ḡω ≡    n                    ,                                       (143)
                                                         (π̃U − π̂U )/ωU

and notice that ωL (τ, k) and ωU (τ, k) being bounded away from zero, ω̂L (τ, k) and ω̂U (τ, k) being uniformly
                                         √
consistent by Assumption 4.2(ii) and nK(C̃n ) being asymptotically tight imply that:
                                    √                               √
                                      n(π̂L (τ, k) − π̃L (τ, k))      n(π̂L (τ, k) − π̃L (τ, k))
  |L(G̃ω ) − L(Ḡω )| ≤ sup M0                                   −
                       (τ,k)∈B               ω̂L (τ, k)                      ωL (τ, k)
                                        √                                √
                                           n(π̂U (τ, k) − π̃U (τ, k))      n(π̂U (τ, k) − π̃U (τ, k))
                          + sup M0                                    −                               = op (1) , (144)
                            (τ,k)∈B                ω̂ U (τ, k)                     ωU (τ, k)

for some constant M0 due to L being Lipschitz. By definition of BL1 , all h ∈ BL1 have Lipschitz constant
less than or equal to 1 and are also bounded by 1. Hence, for any η > 0 Markov’s inequality implies:
                                                           
      P     sup |E[h(L(G̃ω ))|Zn ] − E[h(L(Ḡω ))|Zn ]| > η
         h∈BL1 (R)
                                                          η        η                       η          
                              ≤ P 2P (|L(G̃ω ) − L(Ḡω )| > |Zn ) + P (|L(G̃ω ) − L(Ḡω )| ≤ |Zn ) > η
                                                           2        2                       2
                                4 h h n                       ηo     ii
                              ≤ E E 1 |L(G̃ω ) − L(Ḡω )| >       |Zn .                                  (145)
                                η                             2
Therefore, by (144), (145) and Lemma 1.2.6 in van der Vaart and Wellner (1996), we obtain:
                                                        4                       η
  P      sup |E[h(L(G̃ω ))|Zn ] − E[h(L(Ḡω ))|Zn ]| > η ≤ P |L(G̃ω ) − L(Ḡω )| >    = o(1) .                    (146)
      h∈BL1 (R)                                            η                       2

               L
    Next, let = stands for “equal in law” and notice that for J the Gaussian process in (132):
                                        L                                 √
                                  L(Gω ) = T ◦ K(J)           L(Ḡω ) =    nL ◦ K(C̃n ) ,                         (147)

                                                            38
                                                                                                         P
due to the continuous mapping theorem and (142). For w(x) as defined in (130) and C0 ≡ 2                     x∈X   |w(x)|,
it follows from linearity of K and (135), that K is Lipschitz with Lipschitz constant C0 . Therefore, for any
h ∈ BL1 (R), result (147) implies that h ◦ L ◦ K ∈ BLC0 M0 (L∞ (B × X )) for some M0 > 0 and hence

   sup       |E[h(L(Ḡω ))|Zn ] − E[h(L(Gω ))]| ≤             sup            |E[h(Ḡω )|Zn ] − E[h(J)]| = op (1) , (148)
 h∈BL1 (R)                                           h∈BLC0 M0 (L∞ (B×X ))

where the final equality follows from (140), (141), (147), arguing as in (145)-(146) and Lemmas 6.7, 6.8
and Theorem 2.9.6 in van der Vaart and Wellner (1996). Hence, (146) and (148) establish (138).

   Next, we aim to exploit (138) to show that for all t ∈ R at which the cdf of L(Gω ) is continuous:

                                     |P (L(G̃ω ) ≤ t|Zn ) − P (L(Gω ) ≤ t)| = op (1) .                              (149)

Towards this end, for every λ > 0, and t at which the cdf of L(Gω ) is continuous define the functions:

              hU
               λ,t (u) = 1 − 1{u > t} min{λ(u − t), 1}              hL
                                                                     λ,t (u) = 1{u < t} min{λ(t − u), 1} .          (150)

Notice that by construction, hL                     U                                     L            U
                              λ,t (u) ≤ 1{u ≤ t} ≤ hλ,t (u) for all u ∈ R, the functions hλ,t (u) and hλ,t (u)
are both bounded by one and they are both Lipschitz with Lipschitz constant λ. Also by direct calculation:

                                                                     −1
                     0 ≤ E[hU               L
                            λ,t (L(Gω )) − hλ,t (L(Gω ))] ≤ P (t − λ    ≤ L(Gω ) ≤ t + λ−1 ) .                      (151)

Therefore, exploiting that hL      U                                           −1
                            λ,t , hλ,t ∈ BLλ (R) and that h ∈ BLλ (R) implies λ h ∈ BL1 (R), we obtain:

  |P (L(G̃ω ) ≤t|Zn ) − P (L(Gω ) ≤ t)|
                 ≤ |E[hL                       U                    U                       L
                       λ,t (L(G̃ω ))|Zn ] − E[hλ,t (L(Gω ))]| + |E[hλ,t (L(G̃ω ))|Zn ] − E[hλ,t (L(Gω ))]|

                 ≤2      sup      |E[h(L(G̃ω ))|Zn ] − E[h(L(Gω ))]| + 2P (t − λ−1 ≤ L(Gω ) ≤ t + λ−1 )
                      h∈BLλ (R)

                 = 2λ     sup       |E[h(L(G̃ω ))|Zn ] − E[h(L(Gω ))]| + 2P (t − λ−1 ≤ L(Gω ) ≤ t + λ−1 ) .         (152)
                        h∈BL1 (R)

If t is a continuity point of the cdf of L(Gω ), then (149) follows from (138) and (151).

   Finally, note that since the cdf of L(Gω ) is strictly increasing and continuous at r1−α , we obtain that:

                               P (L(Gω ) ≤ r1−α − ) < 1 − α < P (L(Gω ) ≤ r1−α + )                                (153)

∀ > 0. Define the event An ≡ {P (L(G̃ω ) ≤ r1−α − |Zn ) < 1 − α < P (L(G̃ω ) ≤ r1−α + |Zn )} and notice

                                         P (|r̃1−α − r1−α | ≤ ) ≥ P (An ) → 1 ,                                    (154)

where the inequality follows by definition of r̃1−α and the second result is implied by (149) and (153).




                                                            39
Table 1: Fraction of Observations in Census Estimation Sample with Missing Weekly Earnings

                  Census Year        Number of Observations          Fraction Missing

                       1980                       80,128                 19.49%
                       1990                       111,070                23.09%
                       2000                       131,265                27.70%
                     Overall                      322,463                23.66%




   Table 2: Logit Estimates of P (D = 0|X = x, Fy|x (Y ) = τ ) in 1973 CPS-IRS Sample

                                                        Model 1          Model 2    Model 3
         b1                                                 -1.30          7.25
                                                            (0.42)       (15.76)
         b2                                                 1.33           -0.72
                                                            (0.41)        (9.75)
         γ1                                                                3.92
                                                                          (7.25)
         γ2                                                                -0.92
                                                                          (4.50)

         Log-Likelihood                                -3,865.69         -3861.78   -3825.17
         Parameters                                          35             37           99
         Number of observations                            13,598         13,598    13,598
         Demographic Cells                                   33             33           33
            ∂P (D=0|X=x,Fy|x (Y )=τ )
         E[          ∂τ
                                      |τ =0.2 ]             -0.06          -0.06        -0.04
            ∂P (D=0|X=x,Fy|x (Y )=τ )
         E[          ∂τ
                                      |τ =0.5 ]             0.00           0.00         0.00
            ∂P (D=0|X=x,Fy|x (Y )=τ )
         E[          ∂τ
                                      |τ =0.8 ]             0.06           0.06         -0.05

         Min KS Distance                                    0.03           0.02         0.02
         Median KS Distance                                 0.03           0.06         0.11
         Max KS Distance (S(F ))                            0.03           0.12         0.44
          Note: Asymptotic standard errors in parentheses.




                                                      40
Figure 1: Linear Conditional Quantile Functions (Shaded Region) as a Subset of the Identified Set

        6




        5




        4




        3




        2




        1


                     Quantile Bounds        Lowest Slope Line            Highest Slope Line
        0
               1            2          3           4            5             6           7




               Figure 2: Conditional Quantile and its Pseudo-True Approximation

        7




        6




        5




        4




        3




        2




        1


                   Q
                   Quantile
                       til BBounds
                                d      C diti
                                       Conditional
                                                 l Quantile
                                                   Q   til          P
                                                                    Pseudo‐True
                                                                        d T     Approximation
                                                                                A     i ti
        0
               1            2          3           4            5             6           7




                                                 41
          Figure 3: Confidence Intervals Under Missing at Random Assumption

   .2
   .15
                    Returns to Schooling by Quantile and Year
dlnE/dS
   .1
   .05
   0




          .1         .2         .3          .4        .5             .6      .7           .8           .9
                                                     Tau

                                     1980                     1990                 2000
          Note: Returns computed under missing at random assumption. Pseudo true projection weighted by
          empirical frequency of demographic groups. Shaded regions provide 95% uniform confidence bands.




                      Figure 4: Confidence Intervals Under S(F ) ≤ 0.05

                    Returns to Schooling by Quantile and Year
    .2
    .15
dlnE/dS
   .1
    .05
    0




          .1         .2         .3          .4           .5          .6       .7          .8           .9
                                                        Tau

                                     1980                     1990                 2000
          Note: Returns computed under KS band of 0.05. Pseudo true projection weighted by empirical
          frequency of demographic groups. Shaded regions provide 95% uniform confidence bands.




                                                   42
           Figure 5: Confidence Intervals Under S(F ) ≤ 0.175 (1980 vs. 1990)

   .2
   .15              Returns to Schooling by Quantile and Year
dlnE/dS
   .1
   .05
   0




          .2             .3             .4               .5           .6             .7             .8
                                                        Tau

                                                1980                  1990
          Note: Returns computed under KS band of 0.175. Pseudo true projection weighted by empirical
          frequency of demographic groups. Shaded regions provide 95% uniform confidence bands.




          Figure 6: Intersection of 1980 upper envelope and 1990 lower envelope




                                                   43
                                                  Figure 7: Breakdown Analysis
                                                        Breakdown Curve (1980 vs 1990)
                0.25




                        0.2




                0.15
     k




                        0.1




                0.05


                                                                    Lower Bound
                                                                    Point Estimate
                                                                    Upper Bound
                         0
                         0.1         0.2        0.3         0.4         0.5      0.6        0.7        0.8        0.9
                                                                        tau




                              Figure 8: Confidence Intervals for Fitted Values Under S(F ) ≤ 0.05

                                  Fitted Values by Quantile, Education, and Year
                    8
Log Weekly Earnings
  5    6  4  7




                          8                10          12             14         16                 18             20
                                                              Years of Schooling

                                       tau=.1, 1980                     tau=.5, 1980                 tau=.9, 1980
                                       tau=.1, 1990                     tau=.5, 1990                 tau=.9, 1990
                                       tau=.1, 2000                     tau=.5, 2000                 tau=.9, 2000
                          Note: Fitted values computed under KS bound of 0.05. Pseudo true projection weighted by
                          empirical frequency of demographic groups. Earnings quantiles modeled using specification in
                          Lemieux (2006). Covariates other than education set to sample mean. Pointwise 95% confidence
                          regions in shaded areas and intervals.




                                                                   44
               Figure 9: Logit Based Estimates of Selection in 1973 CPS-IRS File
           Distance Between Missing and Nonmissing CDFs by Quantile of IRS Earnings and Demographic Cell
    0.12




     0.1




    0.08




    0.06
k




    0.04




    0.02




      0
           0       0.1      0.2      0.3      0.4        0.5     0.6      0.7      0.8       0.9       1
                                                          τ




                                                    45
