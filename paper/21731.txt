                              NBER WORKING PAPER SERIES




                       DYNAMIC DIRECTED RANDOM MATCHING

                                         Darrell Duffie
                                           Lei Qiao
                                          Yeneng Sun

                                      Working Paper 21731
                              http://www.nber.org/papers/w21731


                    NATIONAL BUREAU OF ECONOMIC RESEARCH
                             1050 Massachusetts Avenue
                               Cambridge, MA 02138
                         November 2015, Revised January 2018




Part of this work was presented at the Asian Meeting of the Econometric Society in Singapore in
August 2013 and in Taipei in June 2014; at the PIMS Summer School on the Economics and
Mathematics of Systemic Risk and the Financial Networks in the Pacific Institute for the
Mathematical Sciences, Vancouver, July 2014; and at the World Congress of the Econometric
Society, Montreal, August 2015. We are grateful for comments from Peter Loeb. This version
owes substantially to the careful reading and expository suggestions of an editor, an associate
editor and the referees. The work was partially supported by the NUS grants R-122-000-227-112
and R146-000-215-112. The views expressed herein are those of the authors and do not
necessarily reflect the views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

© 2015 by Darrell Duffie, Lei Qiao, and Yeneng Sun. All rights reserved. Short sections of text,
not to exceed two paragraphs, may be quoted without explicit permission provided that full
credit, including © notice, is given to the source.
Dynamic Directed Random Matching
Darrell Duffie, Lei Qiao, and Yeneng Sun
NBER Working Paper No. 21731
November 2015, Revised January 2018
JEL No. C78,D83,E41,G12

                                         ABSTRACT

We develop a general and unified model in which a continuum of agents conduct directed random
searches for counterparties. Our results provide the first probabilistic foundation for static and
dynamic models of directed search (including the matching-function approach) that are common
in search-based models of financial markets, monetary theory, and labor economics. The agents'
types are shown to be independent discrete-time Markov processes that incorporate the effects of
random mutation, random matching with match-induced type changes, and with the potential for
enduring partnerships that may have randomly timed break-ups. The multi-period cross-sectional
distribution of types is shown to be deterministic and is calculated using the exact law of large
numbers.

Darrell Duffie                                  Yeneng Sun
Graduate School of Business                     National University of Singapore
Stanford University                             Department of Economics
Stanford, CA 94305-7298                         1 Arts Link
and NBER                                        Singapore 117570
duffie@stanford.edu                             Republic of Singapore
                                                ynsun@nus.edu.sg
Lei Qiao
National University of Singapore
10 Lower Kent Ridge Road
Singapore 119076
a0086330@nus.edu.sg
Contents
1 Introduction                                                                                                                                                            2

2 Guide to the Main Results                                                                                                                                               4
  2.1 The exact law of large numbers . . . . . . . . . . . . .            .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   5
  2.2 Static directed random matching . . . . . . . . . . . .             .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   6
  2.3 Matching functions . . . . . . . . . . . . . . . . . . . .          .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   7
  2.4 Markovian mutation and match-induced type changes                   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   8

3 Static Directed Random Matching                                                                                                                                          9
  3.1 Mathematical preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                            10
  3.2 Static directed random matching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                             11

4 Dynamic Directed Random Matching                                                                                                                                        13
  4.1 Definition of dynamic directed random matching . . . . . .                      .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   13
  4.2 Markov conditional independence (MCI) . . . . . . . . . . .                     .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   15
  4.3 The exact law of large numbers for MCI dynamical systems                        .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   15
  4.4 Existence of MCI dynamic directed random matching . . .                         .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   16

5 Discussion                                                                                                                                                              17

References                                                                                                                                                                21

A Dynamic Directed Random Matching with Enduring Partnerships                                                                                                             27
  A.1 Definition of dynamic directed random matching with enduring partnerships . . . . . .                                                           .   .   .   .   .   27
  A.2 Markov conditional independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                   .   .   .   .   .   31
  A.3 The exact law of large numbers for MCI dynamical systems with enduring partnerships                                                             .   .   .   .   .   32
  A.4 Existence of MCI dynamic directed random matching with enduring partnerships . . . .                                                            .   .   .   .   .   34

B Illustrative applications in monetary and labor economics                                                                                                               35
  B.1 Kiyotaki-Wright: Model A . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                            35
  B.2 Matsuyama, Kiyotaki and Matsui . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                              37
  B.3 Matching in labor markets with multi-period employment episodes . . . . . . . . . . . . . . . . .                                                                   39

C Proofs of Theorem 4 and Proposition 4                                                                                                                                   41

D A Brief Introduction to Nonstandard Analysis                                                                                                                            48
  D.1 Non-standard number system . . . . . . . . . . .        .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   49
  D.2 General framework of nonstandard analysis . . .         .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   51
  D.3 Construction of hyperfinite Loeb spaces . . . . .       .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   53
  D.4 Transition probabilities . . . . . . . . . . . . . .    .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   54
  D.5 Why hyperfinite agent spaces work . . . . . . . .       .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   55

E Proofs of the Existence Results                                                                                                                                         55
  E.1 Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                          55
  E.2 Proof of Theorem 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                          61
  E.3 Proofs of Propositions 2 and 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                            71




                                                         1
1       Introduction

The economics literature is replete with models that assume independent random matching
among a continuum of agents.1 The agents in these models are frequently motivated to conduct
“directed search,” that is, to focus their searches toward those types of counterparties that offer
greater gains from interaction, or toward those types that are less costly to find. For example,
Rogerson, Shimer, and Wright (2005) describe cases in which “search is directed – i.e., workers
do not encounter firms completely at random but try to locate those posting attractive terms
of trade.” Our central marginal contribution is to provide a mathematical foundation for the
existence and properties of directed search models.
         Independent directed random matching, which includes the popular “matching-function”
approach, is the key to achieving tractability in many search-based models of financial markets,
monetary theory, and labor economics.
         Previous work on mathematical foundations for random matching considers only search
that is “undirected,” in the sense that, conditional on a match by a given agent at a given
time, the probability that the match is with a particular “target” type of agents is merely the
fraction of agents of the target type. Directed search can arise, for example, when one side of
a market posts terms of trade that are especially attractive to specific types of agents.
         Despite heavy reliance in the economics literature on models of independent directed
search,2 until now there has actually been no demonstration of the existence of such search
models, nor of the assumed aggregate behavior of these models that is supposedly based on the
law of large numbers. This paper demonstrates the existence and properties of general models
of static and dynamic independent directed search, thus placing a complete mathematical
foundation under the directed-search models assumed in the prior literature. Our results include
new features and properties that may be useful in future research.
         Earlier foundational work on random matching in a dynamic setting, which we review
in Section 5, also presumes that partnerships break up immediately after matching. Here, we
allow for the potential of enduring partnerships, which may have randomly timed break-ups.
In order to meet the objectives of this paper, a completely new methodology is required, for
    1
      Hellwig (1976) is the first, to our knowledge, to have relied on the effect of the exact law of large numbers
for random pairwise matching in a market. Other examples include Binmore and Samuelson (1999), Currarini,
Jackson and Pin (2009), Duffie, Gârleanu, and Pedersen (2005), Green and Zhou (2002), Kiyotaki and Wright
(1989), Lagos and Rocheteau (2009), Vayanos and Weill (2008), and Weill (2007).
    2
      Among the many applications of directed search in the economics literature, in addition to those cited
elsewhere in this paper, are the models of Acemoglu and Shimer (1999), Albrecht, Gautier, and Vroman (2006),
Burdett, Shi, and Wright (2001), Camera and Selcuk (2009), Eeckhout and Kircher (2010), Faig and Jerez (2005),
Guerrieri, Shimer, and Wright (2010), Kiyotaki and Lagos (1993), Li, Rocheteau and Weill (2012) McAfee (1993),
Menzio (2007), Moen (1997), Peters (1991), Shi (2002), and Watanabe (2010).




                                                        2
both static and dynamic settings.3
       We first consider a static setting in which search is “directed,” in the sense that the
probability qkl that an agent of type k is matched to an agent of type l can vary with the
respective types k and l, from some type space S. We first show, in Theorem 1, the existence
of directed random matching in which counterparty types are independent across agents. It
follows from the exact law of large numbers that the proportion of type-k agents matched
with type-l agents is almost surely pk qkl , where pk is the proportion of type-k agents in the
population. By allowing the matching probabilities {qkl }k,l∈S to depend on the underlying
cross-sectional type distribution p, we also encompass the “matching-function” approach that
has frequently been applied in the labor literature, as surveyed by Petrongolo and Pissarides
(2001) and Rogerson, Shimer, and Wright (2005), as well as over-the-counter models of trade
in financial markets, as in Maurin (2015).
       In typical dynamic settings for random matching, once two agents are matched, their
types change according to some deterministic or random rule. For example, when an unem-
ployed worker meets a firm with a vacant job, the worker’s type changes to “employed.” When
a prospective buyer and seller meet, their status as asset owners can change, and they can
learn information from each other. Random mutation of agent types is also a common model
feature, allowing for shocks to preferences, productivity, or endowments.4
       In practice, and in an extensive part of the literature, once a pair of agents is matched,
they may stay matched for some time. Typical examples include the relationships between
employer and employee, or between two agents that take time to bargain over their terms
of trade.5 In this paper, we develop the first mathematical model for independent random
matching that allows for potentially enduring partnerships.
       Our general model of independent dynamic directed random matching incorporates the
effects of random mutation, random matching with match-induced type changes, and enduring
partnerships. The agents’ types are shown to be independent discrete-time Markov chains.
By the exact law of large numbers in the dynamic setting, the multi-period cross-sectional
distribution of agents’ types is deterministic, and has a period-to-period update mapping that
coincides with the transition function of the law of the Markov chain for individual agent types.
For the special time-homogeneous case, we obtain a stationary joint cross-sectional distribution
of agent types, incorporating both unmatched agent types and pairs of currently matched
   3
     See the discussions in the first two paragraphs of Subsection E.1 on the proof of the static results, and the
second paragraph of Subsection E.2 on the proof of the dynamic results, respectively.
   4
     See, for example, Duffie, Gârleanu, and Pedersen (2005) and Lester, Postlewaite and Wright (2012).
   5
     See, for example, Acemoglu and Wolitzky (2011), Andolfatto (1996), Diamond (1982), Mortensen and
Pissarides (1994), Tsoy (2014), and the references in the surveys of Petrongolo and Pissarides (2001), Rogerson,
Shimer, and Wright (2005) and Wright et al. (2017).



                                                        3
types. This stationary cross-sectional distribution coincides with the stationary probability
distribution of the individual agent type processes. Many previously studied search-based
models of money, over-the-counter financial markets, and labor markets have relied on these
and other properties, which we demonstrate here for the first time.
         We illustrate the applications of our model of directed random matching with four ex-
amples taken, respectively, from Duffie, Malamud and Manso (2014) in financial economics;
Kiyotaki and Wright (1989) and Matsuyama, Kiyotaki and Matsui (1993) in monetary eco-
nomics; and Andolfatto (1996) in labor economics. These examples show how our model can
be used to provide rigorous foundations for typical random-matching models used in these
respective literatures.
         The remainder of the paper is organized as follows. Section 2 is a brief guide to our
main results in an easily accessible form. In Section 3, we describe a static model of inde-
pendent directed random matching, including an existence result as well as an application to
a typical over-the-counter financial market model. In order to capture the effect of enduring
partnerships, we must separately treat legacy and newly matched pairs of agents. In partic-
ular, we keep track of agents and their matched partners at each step (mutation, matching,
and type changing), in every time period. Because the treatment of enduring partnerships
is considerably more involved, its exposition is postponed to Appendix A. In Section 4, we
treat the relatively simpler case of a dynamical system with random mutation, directed ran-
dom matching, and match-induced type changing, but without enduring partnerships. This
section includes results covering the existence and exact law of large numbers for a dynamical
system with Markov conditional independence. Appendix B contains the remaining illustrative
examples of applications of our main results, to models of monetary and labor economics.
         The proofs of the results on the exact law of large numbers and stationarity for a general
dynamic directed random matching are given in Appendix C.6 The proofs for the existence
results for static and dynamic directed random matching make extensive use of tools from
nonstandard analysis, of which a brief introduction is provided in Appendix D.7 Those proofs
are located in Appendix E. Section 5 offers a discussion of the prior foundational mathematical
research on random matching models, and some concluding remarks.

2       Guide to the Main Results

We first offer a brief guide to the main results at an informal level, unburdened by many
technical details that we postpone to later sections.
    6
    Nonstandard analysis is not needed in the proofs of those results.
    7
    The reader can also be referred to the first three chapters of Loeb and Wolff (2015) for basic nonstandard
analysis.


                                                      4
       We emphasize throughout the key effects of the exact law of large numbers. This law is
largely responsible for the popularity of random-matching models, because of the tractability
associated with deterministic, and explicitly computable, quantities of matches between given
types of agents. In multi-period settings, key additional tractability is obtained via the deter-
ministic and explicitly computable evolution of the cross-sectional distribution of agent types.
For example, consider the stochastic dynamic programming problem faced by a given agent in
an economy with interacting agents, whose respective types change randomly over time through
various shocks, including those induced by matching. Without the effect of the exact law of
large numbers, the state variable for a given agent’s problem would need to include not only
that agent’s current type, but also the randomly evolving cross-sectional distribution of types
of all other agents. In many settings, the high dimensionality of the resulting state variable
would rule out any reasonable progress toward a tractable solution. However, with indepen-
dent random matching and an application of the exact law of large numbers, a given agent
can safely assume that the cross-sectional distribution of types of the other agents evolves over
time deterministically (almost surely). This leaves a fixed-point problem, of finding agent-level
policy rules that are consistent in equilibrium with optimality by each agent. In this paper,
however, we take agent-level policy rules as given. We also provide supporting assumptions
for stationarity, under which the cross-sectional distribution of types is actually constant and
deterministic, further simplifying the analysis.
       In the context of random-matching models, the independence of matching outcomes is
generally viewed as a behavioral assumption. That is, when agents conduct searches without
explicit coordination, independence has been viewed as a natural assumption.

2.1    The exact law of large numbers

We fix a probability space (Ω, F, P ). An element of Ω is a state of the world. A measurable
subset B of Ω (that is, an element of F) is an event, whose probability is P (B). The agent
space is an atomless probability space (I, I, λ). An element of I represents an agent. The mass
of some measurable subset A of agents is λ(A). Because the total mass of agents is 1, we can
also treat λ(A) as the fraction of the agents that are in A. In fact, we can take I to be the unit
interval [0, 1] and λ to be an extension of the Lebesgue measure.8
       In order to obtain the exact law of large numbers (ELLN) for a collection {fi : i ∈ I}
of agent-level random variables, we model such a collection as a function f : I × Ω → R that
is measurable with respect to a sufficiently rich set of measurable subsets of I × Ω, denoted
   8
    For measure-theoretic reasons, however, we need the set I of measurable sets of agents to be richer than the
usual Lebesgue measurable sets. We also follow the convention that all probability spaces are countably additive
and complete, unless otherwise noted.



                                                       5
I  F, that extends the usual product σ-algebra I ⊗ F. The required properties of I  F
are given in the next section. The usual product σ-algebra I ⊗ F is not satisfactory for this
purpose. We will also use a weaker version of the notion of independence of the agent-level
random variables. An I  F-measurable function f from I × Ω to R is said to be essentially
pairwise independent if for λ-almost all i ∈ I, the agent-level random variables fi and fj
are independent for λ-almost all j ∈ I. As explicitly shown,9 this condition is weaker than
the usual conditions of mutual independence (any finite collection of random variables are
independent) and pairwise independence (any pair of random variables are independent). The
weaker condition allows one to state a more general version of the exact law of large numbers
for independent random matching.10 Unless otherwise noted, by “independence,” we mean
“essential pairwise independence,” throughout this paper.
      From the exact law of large numbers of Sun (2006, Corollary 2.10) (or see Lemma 1
                                                     R
below), if f is I  F-measurable, integrable (in that I E(|fi |) dλ(i) is finite), and essentially
pairwise independent, then
                         Z                    Z
                                 fi dλ(i) =       E(fi ) dλ(i)   almost surely.                   (1)
                             I                I

      For example, if the agent-level random variables {fi : i ∈ I} are not only pairwise
independent, but also have the same probability distribution with a finite expectation, then
                                                    R
(1) implies that the cross-sectional average outcome I fi dλ(i) is almost surely equal to the
expected outcome for any agent, E(fi ).

2.2   Static directed random matching

Each agent has some type in S = {1, 2, . . . , K}. These types are assigned by some measurable
function, α : I → S. The initial fraction of type-k agents is thus pk = λ ({i : α(i) = k}). The
cross-sectional type distribution p = (pk ) is thus an element of the space ∆ of probability
distributions on S.
      A random matching is a function π : I × Ω → I that assigns a unique randomly chosen
agent π(i) to agent i. In the event that π(i) = i, agent i is not matched. Otherwise, π(i) is the
agent to whom i is matched. We will consider matchings with the property that any agent of
type k is matched to an agent of type l with some given “directed-matching” probability qkl ,
for any (k, l) ∈ S 2 . Of course, these parameters (qkl ) must satisfy l ∈ S qkl ≤ 1. That is, for
                                                                      P

any agent i of type k, we have qkl = P (gi = l), where g(i) = α(π(i)) denotes the type of the
agent to whom i is matched. In the event that i is not matched, we denote g(i) = J.
  9
    See Footnote 13.
 10
    On the other hand, Hammond and Sun (2006) shows that the essential versions of pairwise and mutual
independence are equivalent even in the conditional setting.


                                                       6
      A special case is uniform random matching, which means that qkl = pl . For reasons
given in the Introduction, a rich body of prior research requires more generality than uniform
matching.
      For the specified matching probabilities (qkl ) to be feasible, we must have

                                              pk qkl = pl qlk ,                                (2)

because the left and right hand sides are both equal to the expected total quantity of matches
of agents of type k with agents of type l.
      A random matching π is said to be independent if the associated types {gi : i ∈ I} are
essentially pairwise independent. In this case, it follows immediately from the exact law of large
numbers that the quantity λ({i : α(i) = k, g(i) = l}) of agents of type k that are matched to
agents of type l is almost surely equal to the expected quantity pk qkl . One of our main results
in Section 4 states that for any given initial distribution p = (pk ) of types and any feasible
matching probabilities (qkl ), there exists an initial type function α, a random matching π, and
an associated I  F-measurable process g for partners’ types satisfying these key properties.
We will show additional useful properties of such a directed random matching model.

2.3   Matching functions

Proposition 1 and Theorem 1 of Section 4 also provide a rigorous probabilistic foundation for
the “matching-function” approach that is widely used in the literature of search-based labor
markets. Matching functions allow the probabilities of matching to be directed and to depend
on an endogenously determined cross-sectional distribution of types.
      In models of search-based labor markets, it is typical to suppose that firms and workers
are characterized by their types. A commonly used modeling device in this setting is a matching
function mkl : [0, 1]×[0, 1] → [0, 1] that specifies the quantity of type-k agents that are matched
with type-l agents, given any proportions of type-k agents and type-l agents. (See Petrongolo
and Pissarides (2001) for a survey of the matching-function approach.) Clearly one must require
that for any k and l in S and any p in ∆,
                                                           X
                        mkl (pk , pl ) = mlk (pl , pk ),          mkr (pk , pr ) ≤ pk .        (3)
                                                           r∈S

Let qkl = mkl (pk , pl )/pk for pk 6= 0, and let qkl = 0 for pk = 0. Then the requirements for a
matching probability function are satisfied by (qkl ). By our results in Section 4, there exists
an independent directed random matching π with parameters (p, q). Moreover, for any types
k and l, the mass λ({i : αi = k, gi = l}) of agents of type k that are matched to agents of type
l is almost surely
                                          pk qkl = mkl (pk , pl ),

                                                      7
as specified by the given matching function. This means that any matching function satisfying
Equation (3) can be realized through independent directed random matching, almost surely.
For the special case of only two types of agents (say, types 1 and 2), any nonnegative matching
function m(p1 , p2 ) with m(p1 , p2 ) ≤ min(p1 , p2 ) can be realized through independent directed
random matching. For this, one can simply take q12 = m(p1 , p2 )/p1 and q21 = m(p1 , p2 )/p2 .
More general cases are considered in Footnote 31.
        A common parametric specification is the Cobb-Douglas matching function, for which

                                        mU V (pU , pV ) = A pαU pβV ,

for parameters α and β in (0, 1), and a non-negative scaling parameter A. We emphasize that
for some parameters α, β, and A, the inequality A pαU pβV ≤ min(pU , pU ) may fail for some
(pU , pV ) ∈ ∆. In that case, one can let m(pU , pV ) = min(A pαU pβV , pU , pV ).

2.4     Markovian mutation and match-induced type changes

We now extend to a multi-period setting with time periods 0, 1, . . . Typically, models used in
the literature allow for the following additional probabilistic specifications:

      • Before random matching occurs in each period, a random mutation causes an agent of
        type k to become an agent of type l with a given probability bkl .

      • At any matching between agents of types k and l, the agent that was of type k becomes
        an agent of type r with probability νkl (r). Likewise, the agent that was of type l becomes
        an agent of type r with probability νlk (r).

        The complete list of model parameters is thus (p0 , q, b, ν), where the initial type distri-
bution p0 and the matching probabilities q = (qkl ) are as described above for the static model.
In the more general model of Section 4, we allow the parameters (q, b, ν) to vary with the time
period.
        In each period, the mutations, random matchings, and match-induced type changes are
assumed to be conditionally independent across agents, in the essential-pairwise sense. The
initial types {αi0 : i ∈ I} are assumed to be essentially pairwise independent, which includes
the special case of deterministic initial types.
        In period n, after any mutation and match-induced types changes that have occurred in
that period, let αin denote the type of agent i and let pnk = λ({i : αin = k}) denote the fraction
of agents of type k. Let p̈n be the expected type distribution E(pn ). The initial conditions
α0 and p̈0 ∈ ∆ are given. The objective is to calculate the probability distributions and other
properties of the agent-level type process αi = {αi0 , αi1 , . . .}, as well the cross-sectional type
distribution process p = {p0 , p1 , . . .}.

                                                     8
           In Section 4, we show that the cross-sectional distribution pn of agent types in period n
is almost-surely deterministic. We also demonstrate that, almost surely:

     1. pn = Γ(pn−1 ), where Γ : ∆ → ∆ is explicitly computed.11

     2. For λ-almost every agent i, the agent’s type process αi = {αi1 , αi2 , . . .} is a Markov chain
          with the probability transition function Γ. That is, letting win ∈ ∆ denote the probability
          distribution of αin , we have win+1 = Γ(win ).

     3. The agent-level type processes {αi : i ∈ I} are essentially pairwise independent.

     4. From the exact law of large numbers and the above three results, it follows that the cross-
          sectional type distribution pn is the same as its expectation p̈n with probability one. In
          addition, if almost every agent i has the same initial type probability distribution p̈0 ,
          then win = p̈n for almost every agent i. That is, we can always arrange for the probability
          distribution of each agent’s type to coincide with the cross-sectional distribution of types.
          (In Section 4, we state this equivalence at the level of distributions on sample paths in
          S ∞ .)

     5. There exists a stationary distribution p∗ , defined by p∗ = Γ(p∗ ). Thus, if p∗ is the initial
          probability distribution of αi0 for almost every agent i, then for almost every agent i, in
          every time period n, the type αin of agent i has a probability distribution win = p∗ equal
          to the cross-sectional type distribution pn = p∗ .

           Theorem 2 provides additional characterization of the close relationship in this Markovian
setting between agent-level type probability distributions and cross-sectional type distributions.
We later generalize to allow for enduring matchings, by which a pair of agents, once matched,
may stay paired for some duration whose probability distribution can depend on their respective
types, in a sense that we make precise.

3        Static Directed Random Matching

This section begins the statement of our results at a more complete level. We start with some
mathematical preliminaries. Then a static model of directed random matching is formally
given in Subsection 3.2, where we present the exact law of large numbers, the existence of
independent directed random matching, and an illustrative application to a model of over-the-
counter financial markets.
    11                          P                            P
         Letting ηr (p) = 1 −   l∈S   pl qlr and p̄k (p) =   l∈Spl blk , we have
                                                                X
                                       Γr (p) = pr ηr (p̄(p)) +      p̄k (p)qkl (p̄(p))νkl (r).
                                                                 k,l∈S




                                                                   9
3.1    Mathematical preliminaries

Let (Ω, F, P ) be a probability space. The agent space is an atomless probability space (I, I, λ).
       While a continuum of independent random variables, one for each of a large population
such as I, can be formalized as a mapping on I × Ω, such a function can never be measurable
with respect to the completion of the usual product σ-algebra I ⊗ F, except in the trivial
case in which almost all of the random variables are constants.12 As in Sun (2006), we shall
therefore work with an extension of the usual product probability space that retains the crucial
Fubini property.

Definition 1 A probability space (I × Ω, W, Q) extending the usual product space (I × Ω, I ⊗
F, λ⊗P ) is said to be a Fubini extension of this product space if, for any real-valued Q-integrable
function f on (I × Ω, W),

 (1) For λ-almost all i ∈ I, fi = f (i, · ) is integrable on (Ω, F, P ).

 (2) For P -almost all ω ∈ Ω, fω = f ( · , ω) is integrable on (I, I, λ).
       R                  R R                R R
 (3)       I×Ω f   dQ =    I ( Ω fi dP ) dλ = Ω ( I fω dλ) dP .

To reflect the fact that the probability space (I × Ω, W, Q) has (I, I, λ) and (Ω, F, P ) as its
marginal spaces, as required by the Fubini property, it will be denoted by (I × Ω, I  F, λ  P ).

       The Fubini extension could include a sufficiently rich collection of measurable sets to
allow applications of the exact law of large numbers that we shall need. An I  F-measurable
function f will be called a “process,” each fi will be called a random variable of this process,
and each fω will be called a sample function of the process. As shown in Section 2 of Sun
(2006), a sufficient condition for proving the exact law of large numbers is the condition of
essential pairwise independence. A formal definition is as follows.13

  12
     See, for example, Proposition 2.1 in Sun (2006).
  13
     Here we state the definition of essential pairwise independence using a complete separable metric space X
for the sake of generality; in particular, a finite space or an Euclidean space is a complete separable metric space.
Fix any i ∈ I. If the singleton set {i} is measurable in I, then it is clear that {i} has measure zero (since (I, I, λ)
is atomless). Note that a singleton set is not necessarily measurable in a general measurable space. However,
such measurability follows from the atomless property and the convention that a probability space is complete.
                                                                                     2n
In particular, oneScan take, for each n ≥ 1, a I-measurable partition {An                               n         n
                                                                                  k }k=1 of I with λ(Ak ) = 1/2 such
        n      n+1      n+1                n                                                          n
that Ak =    A
           T 2k−1n    A 2k  for 1 ≤  k ≤ 2   . For any  n ≥  1, there is a unique  k n such that i ∈ Akn , which implies
that i ∈ ∞
                              T∞       n
             n=1 Akn . Since    n=1 Akn has measure zero and (I, I, λ) is complete, the singleton set {i} is in I
with measure zero. If the pairwise independence condition holds for f , namely, for any i 6= j ∈ I, fi and fj are
independent, then for each i ∈ I, fi and fj are independent except for j in the λ-null set {i}, which means that
f satisfies the condition of essential pairwise independence. The usual condition of mutual independence (any
finite collection of random variables are independent) is even stronger than pairwise independence.




                                                          10
Definition 2 (Essential pairwise independence)           An I  F-measurable process f from I × Ω
to a complete separable metric space X is said to be essentially pairwise independent if for
λ-almost all i ∈ I, the random variables fi and fj are independent for λ-almost all j ∈ I.

3.2   Static directed random matching

We follow the notation in Subsection 3.1. Let S = {1, 2, . . . , K} be a finite space of agent types
and α : I → S be an I-measurable type function, mapping individual agents to their types.
For any k in S, we let pk = λ({i : α(i) = k}) denote the fraction of agents of type k. We can
view p = (pk )k∈S as an element of the space ∆ of probability measures on S. Because (I, I, λ)
has no atoms, for any type distribution p ∈ ∆, one can find an I-measurable type function
with distribution p.
      A function q : S × S → R+ is a matching probability function for the type distribution
p if, for any k and l in S,
                                                        X
                                   pk qkl = pl qlk ,          qkr ≤ 1.                          (4)
                                                        r∈S

The matching probability qkl specifies the probability that an agent of type k is matched to an
                              P
agent of type l. Thus ηk = 1 − l∈S qkl is the associated no-matching probability for an agent
of type k.

Definition 3 Let α, p, and q be given as above, and J a special type representing no-matching.

  (i) A full matching φ is a one-to-one mapping from I onto I such that, for each i ∈ I,
      φ(i) 6= i and φ(φ(i)) = i.

 (ii) A (partial) matching ψ is a mapping from I to I such that for some subset B of I, the
      restriction ψ|B of ψ to B is a full matching on B, and the restriction ψ|I\B of ψ to I \ B
      is the identity mapping. This means that agent i in B is matched to another agent ψ(i)
      in B, whereas any agent i not in B is unmatched, in that ψ(i) = i.

(iii) A random matching π is a mapping from I × Ω to I such that

       (a) πω is a matching for each ω ∈ Ω.
                     (
                      α(π(i, ω)) if π(i, ω) 6= i
       (b) g(i, ω) =
                      J            if π(i, ω) = i
           is measurable from (I × Ω, I  F, λ  P ) to S ∪ {J}.

 (iv) A random matching π from I × Ω to I is directed, with parameters (p, q) satisfying con-
      dition (4), if for λ-almost every agent i of type k, P (gi = J) = ηk and P (gi = l) = qkl .



                                                   11
  (v) A random matching π is said to be independent if the associated type process g is essen-
          tially pairwise independent.


          For an agent i ∈ I who is matched, the random variable gi = g(i, · ) is the type of her
matched partner. Part (iv) of the definition thus means that for λ-almost every agent i of type
k, her probability of being matched with a type-l agent is qkl , while her no-matching probability
is ηk .
          The following result is a direct application of the exact law of large numbers. In par-
ticular, letting Ik = {i ∈ I : α(i) = k}, the result follows from Theorem 2.8 of Sun (2006)
(see Lemma 1 below) by working with the process g Ik = g|Ik ×Ω on the rescaled agent space Ik ,
where g|Ik ×Ω is the restriction of g to Ik × Ω.

Proposition 1 Let π be an independent directed random matching with parameters (p, q).
Then, for P -almost every ω ∈ Ω, we have

  (i) For k ∈ S, λ({i ∈ I : α(i) = k, gω (i) = J}) = pk ηk .

 (ii) For any (k, l) ∈ S × S, λ({i : α(i) = k, gω (i) = l}) = pk qkl .

          Let κ be the probability measure on S × (S ∪ {J}) defined by letting κ(k, l) = pk qkl for
any (k, l) ∈ S × S and κ(k, J) = pk ηk for k ∈ S. Proposition 1 says that the cross-sectional
joint type distribution of (α, gω ) is κ with probability one.
          Now we state our main existence result for the static setting.

Theorem 1 For any type distribution p on S and any matching probability function q for p,
there exists a Fubini extension (I ×Ω, I F, λP ) on which is defined a type function α and an
independent directed random matching14 π with parameters (p, q), which is measure preserving
in the sense that for each ω ∈ Ω, λ(πω−1 (A)) = λ(A) for any A ∈ I.

          The proof of Theorem 1 will be given in Subsection E.1 for the case of a Loeb measure
space of agents via the method of nonstandard analysis.15 Since the unit interval and the class
of Lebesgue measurable sets with the Lebesgue measure provide the archetype for models of
economies with a continuum of agents, the next proposition (proved in Subsection E.3) shows
that one can take an extension of the classical Lebesgue unit interval as the agent space for the
construction of an independent directed random matching.
  14
     As shown in the last paragraph in the proof of Lemma 7, one can take a subset I˜ of I such that λ(I \ I)
                                                                                                           ˜ = 0,
and the random types {gi }i∈I˜ as constructed there satisfy a stronger independence condition in the sense that
any finitely many random variables from that collection are mutually independent.
  15
     A simple treatment of nonstandard analysis is given in Appendix D. We note that the proof of Theorem 1 is
substantially different from the corresponding existence result for the case of “undirected” search in Duffie and
Sun (2007); see the first paragraph of Subsection E.1.


                                                       12
Proposition 2 For any type distribution p on S and any matching probability function q for
p, there exists a Fubini extension (I × Ω, I  F, λ  P ) such that:

    1. The agent space (I, I, λ) is an extension of the Lebesgue unit interval (L, L, χ).

    2. There is defined on the Fubini extension a type function α and an independent directed
       random matching16 π with parameters (p, q).

       The following example provides an illustrative application of Theorem 1 and Proposition
1 to a model of over-the-counter financial markets.

Example 1 In Duffie, Malamud and Manso (2014), the economy is populated by a continuum
of risk-neutral agents. There are M different types of agents that differ according to the quality
of their initial information, their preferences for the asset to be traded, and the likelihoods with
which they meet each of other types of agents for trade. The proportion of type-l agents is
ml , where l = 1, . . . , M . Any agent of type l is randomly matched with some other agent
with probability λl ∈ [0, 1). This counterparty is of type-r with probability κlr . In the present
context, we can take the matching probability qlr = λl κlr for any l and r in S. Theorem 1
guarantees the existence of independent directed random matching with the given parameters
ml , qlr . Proposition 1 implies that the total quantity of matches of agents of a given type l
with the agents of a given type r is almost surely ml λl κlr = mr λr κrl . (See page 7 of Duffie,
Malamud and Manso (2014).)

4     Dynamic Directed Random Matching

In this section we show how to construct a dynamical system that incorporates the effects
of random mutation, directed random matching, and match-induced type changes with time-
dependent parameters. We first define such a dynamical system in Subsection 4.1. The key
condition of Markov conditional independence is formulated in Subsection 4.2. Based on that
condition, we state in Subsection 4.3 an exact law of large numbers for such a dynamical
system. The section ends with the existence of Markov conditionally independent dynamic
directed random matching.

4.1    Definition of dynamic directed random matching

As in Section 3, we fix an atomless probability space (I, I, λ) representing the space of agents,
a sample probability space (Ω, F, P ), and a Fubini extension (I × Ω, I  F, λ  P ). Let
    16
       In addition, there exists a sub-σ-algebra I 0 of I and a Fubini extension (I × Ω, I 0  F, λ  P ) such that
I 0 F ⊆ I F and π is an independent directed random matching with parameters (p, q) on (I ×Ω, I 0 F, λP ),
which is measure preserving in the sense that for each ω ∈ Ω, λ(πω−1 (A)) = λ(A) for any A ∈ I 0 . See the
penultimate paragraph of the proof of Proposition 5 in Subsection E.3.


                                                        13
S = {1, 2, . . . , K} be a finite set of types and let J be a special type representing no-matching.
We shall define a discrete-time dynamical system D0 with the property that at each integer
time period n ≥ 1, agents first experience a random mutation and then random matching with
directed probabilities. Finally, any pair of matched agents are randomly assigned new types
whose probabilities may depend on the pair of prior types of the two agents.
      At period n ≥ 1, each agent of type k ∈ S first experiences a random mutation, becoming
an agent of type l with a given probability bnkl , with r∈S bnkr = 1. At the second step, every
                                                       P

agent conducts a directed search for counterparties. In particular, for each (k, l) ∈ S × S, the
                                                           n on the space of type distributions
directed matching probability is determined by a function qkl
∆, with the property that, for all k and l in S, the function that maps the type distribution
             n (p) is continuous and satisfies, for all p ∈ ∆,
p ∈ ∆ to pk qkl
                                                         X
                                n            n               n
                            pk qkl (p) = pl qlk (p) and     qkr (p) ≤ 1.                          (5)
                                                            r∈S

The intention is that, if the population type distribution in the current period is p, then an
agent of type k is matched to some agent whose type is l with probability qkl   n (p). Thus,

ηkn (p) = 1 − l∈S qkl
                   n (p) is the associated probability of no match. When an agent of type k is
             P

matched at time n to an agent of type l, the agent of type k becomes an agent of type r with
             n (r), where        n
                          P
probability νkl             r∈S νkl (r) = 1. The primitive model parameters are (b, q, ν).
      Let α0 be the initial S-valued type process on the Fubini extension (I × Ω, I  F, λ  P ).
For each time period n ≥ 1, the agents’ types after the random mutation step are given by a
process hn from (I × Ω, I  F, λ  P ) to S. Then, a random matching is described by a function
π n from I ×Ω to I. The end-of-period types are given by a process αn from (I ×Ω, I F, λP )
to S. Thus the post-mutation type function hn satisfies

                                     P (hni = l | αin−1 = k) = bnkl .                             (6)

For the directed random matching step, let g n be an I  F-measurable function defined by
g n (i, ω) = hn (π n (i, ω), ω), with the property that for any type k ∈ S, for λ-almost every i and
P -almost every ω ∈ Ω,
                                P (gin = l | hni = k, p̌n ) = qkl
                                                               n n
                                                                  (p̌ (ω)),                       (7)

where p̌n (ω) = λ(hnω )−1 is the post-mutation type distribution realized in state ω. The end-of-
period agent type function αn satisfies, for λ-almost every agent i,

          P (αin = r | hni = k, gin = J) = δk (r) and P (αin = r | hni = k, gin = l) = νkl
                                                                                        n
                                                                                           (r),   (8)

where δk (r) is 1 if k = r and is zero otherwise. Thus, we have inductively defined the properties
required of a dynamical system D0 that incorporates the specified effects of random mutation,
directed random matching, and match-induced type changes with given parameters (b, q, ν).

                                                    14
4.2     Markov conditional independence (MCI)

We now add independence conditions on the dynamical system D0 , along the lines of those
in Duffie and Sun (2007, 2012). The idea is that each of the just-described steps (mutation,
random matching, match-induced type changes) are conditionally independent across almost
all agents.
        We say that the dynamical system D0 is Markov conditionally independent (MCI) if, for
λ-almost every i and λ-almost every j, for every period n ≥ 1, and for all types k and l in S,
the following four properties apply:

      • Initial independence: αi0 and αj0 are independent.

      • Markov and conditionally independent mutation:

         P (hni = k, hnj = l | αi0 , . . . , αin−1 ; αj0 , . . . , αjn−1 ) = P (hni = k | αin−1 )P (hnj = l | αjn−1 ).


      • Markov and conditionally independent random matching:

        P (gin = k, gjn = l | αi0 , . . . , αin−1 , hni ; αj0 , . . . , αjn−1 , hnj ) = P (gin = k | hni )P (gjn = l | hnj ).


      • Markov and conditionally independent matched-agent type changes:

                        P (αin = k, αjn = l | αi0 , . . . , αin−1 , hni , gin ; αj0 , . . . , αjn−1 , hnj , gjn )
                                  =     P (αin = k | hni , gin )P (αjn = l | hnj , gjn ).

4.3     The exact law of large numbers for MCI dynamical systems

We define a sequence Γn of mappings from ∆ to ∆ such that, for each p ∈ ∆,
                                                                           X
                      Γnr (p1 , . . . , pK ) = p̄nr (p)ηrn (p̄n (p)) +            p̄nk (p)qkl
                                                                                           n n         n
                                                                                              (p̄ (p))νkl (r),
                                                                          k,l∈S

where p̄nk (p) =             pl bnlk for k ∈ S.
                    P
                       l∈S
        The following theorem presents an exact law of large numbers for the agent type processes
at the end of each period, and gives a recursive calculation for the cross-sectional joint agent
type distribution pn at the end of period n.

Theorem 2 A Markov conditionally independent dynamical system D0 with parameters (b, q, ν),
for random mutation, directed random matching and match-induced type changes, satisfies the
following properties.



                                                                  15
 (1) For each time n ≥ 1, let pn (ω) = λ(αωn )−1 be the realized cross-sectional type distribution
       at the end of the period n. The expectation E(pn ) is given by
                                                                     X
                        E(pnr ) = Γnr (E(pn−1 )) = p̄nr ηrn (p̄n ) +   p̄nk qkl
                                                                             n n n
                                                                                (p̄ )νkl (r),
                                                                    k,l∈S

       where p̄nk =           E(pn−1 )bnlk .
                      P
                        l∈S      l

 (2) For λ-almost every agent i, the type process {αin }∞
                                                        n=0 of agent i is a Markov chain with
       transition matrix z n at time n − 1 defined by
                                                         X
                                   n
                                  zkl = ηln (p̄n )bnkl +   bnkr qrj
                                                                 n
                                                                    (p̄n )νrj
                                                                           n
                                                                              (l).
                                                           r,j∈S


 (3) For λ-almost every i and λ-almost every j, the Markov chains {αin }∞         n ∞
                                                                        n=0 and {αj }n=0 are
       independent.17

 (4) For P -almost every state ω, the cross-sectional type process {αωn }∞
                                                                         n=0 is a Markov chain
       with transition matrix z n at time n − 1.

 (5) For P -almost every state ω, at each time period n ≥ 1, pn (ω) = λ(αωn )−1 , and the realized
       cross-sectional type distribution after random mutation λ(hnω )−1 is p̄n .

 (6) If there is some fixed p̈0 ∈ ∆ that is the probability distribution of the initial type αi0
       of agent i for λ-almost every i, then the probability distribution ζ = p̈0 ⊗ (⊗∞    n
                                                                                      n=1 z ) on
       S ∞ is equal to the sample-path distribution of the Markov chain αi = {αin }∞
                                                                                   n=0 for λ-
       almost every agent i. For P -almost every state ω ∈ Ω, ζ is also the cross-sectional joint
       distribution λαω−1 of the sample paths of agents’ realized type process.

 (7) Suppose that the parameters (b, q, ν) are time independent. Then there exists a type
       distribution p∗ ∈ ∆ such that p∗ is a stationary distribution for any Markov conditionally
       independent dynamical system D0 with parameters (b, q, ν), in the sense that for every
       period n ≥ 0, the realized cross-sectional type distribution pn at time n is p∗ P -almost
       surely. all of the relevant Markov chains are time homogeneous with a constant transition
       matrix z 1 having p∗ as a fixed point. In addition, if the initial type process α0 is i.i.d.
       across agents, then, for λ-almost every agent i, P (αin )−1 = p∗ for any period n ≥ 0.

4.4    Existence of MCI dynamic directed random matching

The following theorem provides for the existence of a Markov conditionally independent (MCI)
dynamical system with random mutation, random matching, and match-induced type changes.
  17
    Two Markov chains with a state space S are said to be independent if they are independent as random
variables taking values in S ∞ .


                                                      16
Theorem 3 For any primitive model parameters (b, q, ν) and for any type distribution p̈0 ∈ ∆,
there exists a Fubini extension (I × Ω, I  F, λ  P ) on which is defined a dynamical system
D0 with random mutation, random matching, match-induced type changes, that is Markov con-
ditionally independent with these parameters (b, q, ν), and with the initial cross-sectional type
distribution p0 that is p̈0 with probability one. In addition, for any n ≥ 1, π n is measure pre-
serving in the sense that for each ω ∈ Ω, λ((πωn )−1 (A)) = λ(A) for any A ∈ I. These properties
can be achieved with an initial type process α0 that is deterministic, or i.i.d. across agents.18

          With the next proposition, we show that the agent space (I, I, λ) can be an extension
of the classical Lebesgue unit interval (L, L, χ). That is, we can take I = L = [0, 1] with a
σ-algebra I that contains the Lebesgue σ-algebra L, and so that the restriction of λ to L is
the Lebesgue measure χ.

Proposition 3 Fixing any model parameters (b, q, ν) and any initial cross-sectional type dis-
tribution p̈0 ∈ ∆, there exists a Fubini extension (I × Ω, I  F, λ  P ) such that:

    (1) The agent space (I, I, λ) is an extension of the Lebesgue unit interval (L, L, χ).

    (2) There is defined on the Fubini extension a dynamical system D0 that is Markov condi-
          tionally independent with the parameters (b, q, ν), where the initial cross-sectional type
          distribution p0 is p̈0 with probability one.

    (3) These properties can be achieved with an initial type process α0 that is deterministic, or
          i.i.d. across agents.19

5        Discussion

We finish with a discussion of the prior literature on the mathematics of random matching,
and then offer some concluding comments about our main new results and some immediately
available extensions of our results to more general type spaces, changing population sizes, or
background “macro-economic” processes that cause random changes in the evolution of the
cross-sectional type distributions.
          As mentioned in Section 2, when agents conduct searches without explicit coordination,
it is reasonable to impose the assumption of independence of their searches. However, Footnote
    18
      This means that the process α0 is essentially pairwise independent, and αi0 has distribution p̈0 for λ-almost
all i ∈ I.
   19
      In addition, there exists a sub-σ-algebra I 0 of I and a Fubini extension (I × Ω, I 0  F, λ  P ) such that
I 0  F ⊆ I  F, the dynamical system D0 on (I × Ω, I 0  F, λ  P ) is Markov conditionally independent, and
for any n ≥ 1, π n is measure preserving in the sense that for each ω ∈ Ω, λ((πωn )−1 (A)) = λ(A) for any A ∈ I 0 .
See the penultimate paragraph of the proof of Proposition 5 in Subsection E.3.



                                                        17
4 of McLennan and Sonnenschein (1991) showed the non-existence of a type-free static random
full matching that satisfies a number of desired conditions, when the agent space is taken
to be the unit interval with the Borel σ-algebra and Lebesgue measure. That problem was
resolved through the construction of an independent type-free static random full matching
with a suitable agent space, as in Duffie and Sun (2007).20 Xiang Sun (2016) extended the
results on independent static random partial matching in Duffie and Sun (2007) from finite type
spaces to general type spaces. Duffie and Sun (2007) and Duffie and Sun (2012) go beyond the
static case to present the existence of, and the exact law of large numbers for, discrete-time
independent random matching.
       When the independence assumption for random matching is not required, one can con-
struct many non-independent random full matchings with some desired matching properties,
even for finitely many agents.21
       Based on the classical asymptotic law of large numbers, Boylan (1992) constructed a
random full matching for a countable population. Gilboa and Matsui (1992) presented a par-
ticular matching model of two countable populations with a countable number of encounters in
the time interval [0, 1), where both the agent space N and the sample space are endowed with
purely finitely additive measures. A non-independent random full matching was constructed in
Alós-Ferrer (1999) for a given type function on the population space [0, 1] by rearranging the
intervals in [0, 1] through measure-preserving mappings. Rather than relying on particular ex-
amples of a non-independent random matching that have certain desired matching properties,
Duffie and Sun (2012) instead proved the exact law of large numbers for general independent
random matchings.
       An independent random matching automatically involves a process with a continuum of
independent random variables. The classical Kolmogrov consistency theorem (see, for example,
Bogachev (2007, p. 95)) implies the existence of a continuum of independent real-valued random
variables. Following Doob (1937), some measure extension leads to the fact that almost all
sample functions differ from an arbitrarily given function h (whether measurable or not) at
only countably many points.22 This implies that one can claim that the sample functions
(and thus the sample means and distributions) are essentially arbitrary.23 Though it is not
  20
     See the main theorem of Podczeck and Puzzello (2012) for another construction. It is clear that an indepen-
dent type-free static random full matching also provides a model of independent static random full matching for
general types; see Duffie and Sun (2012) and Podczeck and Puzzello (2012).
  21
     See Subsection 3.2 of Molzon and Puzzello (2010). However, the requirement that all the agents are matched
randomly then leads to correlation among the finitely many agents. It therefore does not make sense to consider
independent random matching in the finite-agent setting. For a detailed discussion of the literature on “non-
independent” random matching, see Section 6 of Duffie and Sun (2012).
  22
     See Subsection 6.1 in Sun (2006) for the detailed discussion.
  23
     That is, the sample means and distributions of an i.i.d. process can be essentially equal to the theoretical
means and distribution, respectively, or any other arbitrarily given means and distribution. See Judd (1985)


                                                       18
possible to require an i.i.d. process to have essentially constant sample means and distributions
at the coalitional level for an agent space based on the Lebesgue unit interval,24 one can
simply use the transfer principle25 in nonstandard analysis to restate the classical law of large
numbers for a sequence of i.i.d. random variables in a nonstandard model, and thus claim the
existence of a process with the required properties.26 The essential difficulty associated with a
continuum of independent random variables is that independence and joint measurability with
respect to the usual measure-theoretic product are never compatible with each other except
for the trivial case in which almost all of the random variables are constants.27 In Sun (2006),
various versions of the exact law of large numbers, and their converses in both static and
dynamic settings, are proved by direct application of simple measure-theoretic methods in the
framework of a Fubini extension. Such a framework is adopted in this paper (1) to construct
static and dynamic models of independent directed random matching that incorporate the
effects of random mutation, random matching with match-induced type changes, and with the
potential for enduring partnerships that may have randomly timed break-ups, and (2) to study
various properties of a general independent directed random matching via the exact law of
large numbers.28
       All of the papers on random matching mentioned above address the case of “undirected”
search, in the sense that the matching probabilities are proportional to the population sizes of
the matched agents with given types. The main purpose of this paper is to provide a suitable
search-based model of markets in which agents can direct their searches, causing relatively
higher per-capita matching probabilities with specific types of counterparties. Although mod-
els with directed search are common in the literatures covering money, labor markets, and
and Sun (2006, p. 53).
   24
      See Feldman and Gilles (1985).
   25
      See Proposition 7 in Subsection D.2.
   26
      See, for example, Hersh and Greenwood (1975), Keisler (1977), Hurd and Loeb (1985), Anderson (1991),
and Footnote 46 below for the transfer of a sequence of i.i.d. random variables and relevant results. Green
(1994) also provided an alternative construction for the existence of such a process with the required property.
Note that the agent space as considered in those papers are not the Lebesgue type space while the relevant
sample probability space is countably additive. On the other hand, one can still claim the existence of an i.i.d.
process to have any kind of sample means/distributions at the coalitional level based on the usual Lebesgue
agent space and some purely finitely additive sample probability space; see Berti, Gori and Rigo (2012). Given
the various possibilities in claiming the stability of sample functions in specific examples, what one needs is to
find a suitable measure-theoretic condition to guarantee aggregate sample stability under independence.
   27
      See Proposition 2.1 in Sun (2006).
   28
      Dynamic independent random matching, as considered in this paper, needs a specific construction based on
the framework of a Fubini extension, which allows us to apply the static and dynamic versions of exact law of
large numbers shown in such a general framework in Sun (2006). However, this construction does not fit any
earlier examples on the existence of an i.i.d. process having essentially constant sample means/distributions.
In particular, the condition of ∗-independence (see Hurd and Loeb (1985) and Footnote 46 below) holds for the
transfer of an i.i.d. sequence of random variables to the nonstandard model. As noted in Footnote 19 of Duffie
and Sun (2007), ∗-independence, which is much stronger than the usual independence condition, is not satisfied
even in the simple setting of static random full matching.



                                                       19
over-the-counter financial markets, prior work has simply assumed that the exact law of large
numbers would lead to a deterministic cross-sectional distribution of agent types, and that this
distribution would obey certain properties. We provide a model that justifies this assumed
behavior, down to the basic level of random contacts between specific individual agents. We
provide the resulting transition distribution for the Markov processes for individual agents’
types, and for the aggregate cross-sectional distribution of types in the population, and show
the close relationship between these two objects.
       By incorporating directed search, we are also able to provide the first rigorous proba-
bilistic foundation for the notion of a “matching function” that is heavily used in the search
literature of labor economics.
       A secondary objective of our paper is to allow for random matching with enduring
partnerships. The durations of these partnerships can be random or deterministic, and can
be type dependent. Earlier work providing mathematical foundations for random matching
presumes that partnerships break up immediately after matching. Enduring partnerships are
crucial for search-based labor-market models, such as those cited in Footnote 5, in which there
are episodes of employment resulting from a match between a worker and a firm, eventually
followed by a randomly timed separation.29 In some of these models, separation is i.i.d. across
periods of employment. This is the case, for example, in Cho and Matsui (2013), Merz (1999),
Pissarides (1985), Shi and Wen (1999), Shimer (2005), and Yashiv (2000), among many other
papers. In other cases, the separation probability depends on the vintage of the match, and
can depend on the quality of the match between the worker and the firm. Since the separation
probabilities in our general model depend on the types of the matched agents, our results can
cover such cases by introducing new types.
       We have verified that our results can be extended under mild revisions of the proofs
to settings in which agents have countably many types, and can enter and exit (for example,
through “birth” and “death”), allowing for a total population size that is changing over time
without a fixed bound, as in Yashiv (2000).30 It is also straightforward to allow for a background
Markov process that governs the parameters determining probabilities for mutation, matching,
and type change (as well as enduring match break-ups). In this case, the background Markov
state causes aggregate uncertainty, but conditional on the path of the background state, the
cross-sectional distribution of population types evolves deterministically, almost surely.

  29
      Further such situations arise in the models of Cho and Matsui (2013), Flinn (2006), Haan, Ramey and
Watson (2000), Hall (2005), Hosios (1990), Merz (1995), Merz (1999), Mortensen (1982), Pissarides (1985),
Shimer (2005), Shi and Wen (1999), and Yashiv (2000).
   30
      Our results cover cases in which there is a fixed bound for the total population size in all time periods. In
such cases, one can simply introduce a new type to represent the inactive agents and re-scale the total population
size.


                                                        20
References

Daron Acemoglu and Robert Shimer, Holdups and efficiency with search frictions, International
  Economic Review 40 (1999), 827-849.

Daron Acemoglu and Alexander Wolitzky, The economics of labor coercion, Econometrica 79
  (2011), 555–600.

James Albrecht, Peter Gautier and Susan Vroman, Directed search with multiple applications,
  Review of Economic Studies 73 (2006), 869-91.

Carlos Alós-Ferrer, Dynamical systems with a continuum of randomly matched agents, Journal
  of Economic Theory 86 (1999), 245–267.

Robert M. Anderson, Non-standard analysis with applications to economics, in Handbook of
  Mathematical Economics IV (W. Hildenbrand, H. Sonnenschein eds.), North-Holland, New
  York, 1991.

David Andolfatto, Business cycles and labor-market search, American Economic Review 86
  (1996), 112–132.

Patrizia Berti, Michele Gori and Pietro Rigo, A note on the absurd law of large numbers in
  economics, Journal of Mathematical Analysis and Applications 388 (2012), 98–101.

Ken Binmore and Larry Samuelson, Evolutionary drift and equilibrium selection, Review of
  Economic Studies 66 (1999), 363–393.

Vladimir I. Bogachev, Measure Theory, Volume 2, Springer-Verlag Berlin Heidelberg, 2007.

Richard T. Boylan, Laws of large numbers for dynamical systems with randomly matched
  individuals, Journal of Economic Theory 57 (1992), 473–504.

Kenneth Burdett, Shoyoung Shi and Randy Wright, Pricing and matching with frictions, Jour-
  nal of Political Economy 109 (2001), 1060-85.

Gabriele Camera and Cemil Selcuk, Price dispersion with directed search, Journal of the Eu-
  ropean Economics Association 7 (2009), 1193-1224.

In-Koo Cho and Akihiko Matsui, Search theory, competitive equilibrium, and the Nash bar-
  gaining solution, Journal of Economic Theory 148 (2013), 1659–1688.

Sergio Currarini, Matthew O. Jackson and Paolo Pin, An economic model of friendship: Ho-
  mophily, minorities, and segregation, Econometrica 77 (2009), 1003–1045.

                                             21
Peter Diamond, Aggregate demand management in search equilibrium, Journal of Political
  Economy 90 (1982), 881–894.

Joseph L. Doob, Stochastic processes depending on a continuous parameter, Trans. Amer.
  Math. Soc. 42 (1937), 107-140.

Darrell Duffie, Nicolae Gârleanu, and Lasse H. Pedersen, Over-the-counter markets, Economet-
  rica 73 (2005), 1815–1847.

Darrell Duffie, Semyon Malamud and Gustavo Manso, Information percolation in segmented
  markets, Journal of Economic Theory 153 (2014), 1–32.

Darrell Duffie and Yeneng Sun, Existence of independent random matching, Annals of Applied
  Probability 17 (2007), 386–419.

Darrell Duffie and Yeneng Sun, The exact law of large numbers for independent random match-
  ing, Journal of Economic Theory 147 (2012), 1105–1139.

Jan Eeckhout and Philipp Kircher, Sorting and decentralized price competition, Econometrica
  78 (2010), 539-574.

Miquel Faig and Belén Jerez, A theory of commerce, Journal of Economic Theory 22 (2005),
  60-99.

Mark Feldman and Christian Gilles, An expository note on individual risk without aggregate
  uncertainty, Journal of Economic Theory 35 (1985), 26-32.

Christopher Flinn, Minimum wage effects on labor market outcomes under search, matching,
  and endogenous contact rates, Econometrica, 74 (2006), 1013–1062.

Itzhak Gilboa and Akihiko Matsui, A model of random matching, Journal of Mathematical
  Economics 21 (1992), 185–197.

Edward J. Green, Individual-level randomness in a nonatomic population, Working paper,
  University of Minnesota, 1994.

Edward J. Green and Ruilin Zhou, Dynamic monetary equilibrium in a random matching
  economy, Econometrica 70 (2002), 929–970.

Veronica Guerrieri, Rob Shimer and Randy Wright, Adverse selection in competitive search
  equilibrium, Econometrica 78 (2010), 1823-1862.




                                             22
Wouter J. D. Haan, Garey Ramey and Joel Watson, Job destruction and propagation of shocks.
  American Economic Review 90 (2000), 482–498.

Robert E. Hall, Employment fluctuations with equilibrium wage stickiness, American Economic
  Review 95 (2005), 50–65.

Martin Hellwig, A model of monetary exchange, Econometric Research Program, Research
  Memorandum Number 202, Princeton University, 1976.

Peter J. Hammond and Yeneng Sun, The essential equivalence of pairwise and mutual condi-
  tional independence, Probability Theory and Related Fields 135 (2006), 415–427.

Reuben Hersh and Priscilla Greenwood, Stochastic differentials and quasi-standard random
  variables, in Probabilistic Methods in Differential Equations (Mark Pinsky ed.), Lecture Notes
  in Mathematics, No. 451, Springer-Verlag, 1975, pp. 35–62.

Arthur Hosios, On the efficiency of matching and related models of search and unemployment,
  Review of Economic Studies 57 (1990), 279–298.

Albert E. Hurd and Peter A. Loeb, An Introduction to Nonstandard Real Analysis, Academic
  Press, Orlando, FL, 1985.

Kenneth L. Judd, The law of large numbers with a continuum of iid random variables, Journal
  of Economic Theory 35 (1985), 19–25.

Timothy J. Kehoe, Nobuhiro Kiyotaki and Randall Wright, More on money as a medium of
  exchange. Economic Theory 3 (1993), 297–314.

H. Jerome Keisler, Hyperfinite model theory. In Logic Colloquium 76 (R. O. Gandy and J. M.
  E. Hyland, eds.) 5110. North-Holland, Amsterdam, 1977

M. Ali Khan and Yeneng Sun, On Loeb measure spaces and their significance for non-
  cooperative game theory, in Current and Future Directions in Applied Mathematics (M.
  Albers, B. Hu, J. Rosenthal eds.), Birkhauser, Boston, 1997, 183–218.

Nobuhiro Kiyotaki and Ricardo Lagos, A model of job and worker flows, Journal of Political
  Economy 115 (2007), 770–819.

Nobuhiro Kiyotaki and Randall Wright, On money as a medium of exchange, Journal of Po-
  litical Economy 97 (1989), 927–954.

Ricardo Lagos and Guillaume Rocheteau, Liquidity in asset markets with search frictions,
  Econometrica 77 (2009), 403–426.

                                              23
Benjamin Lester, Andrew Postlewaite and Randall Wright, Information, liquidity, asset prices,
  and monetary policy, Review of Economic Studies 79 (2012), 1209–1238.

Yiting Li, Guillaume Rocheteau and Pierre-Olivier Weill, Liquidity and the threat of fraudulent
  assets, Journal of Political Economy 120 (2012), 815–846.

Peter A. Loeb, Real Analysis, Birkhäuser, Switzerland, 2016.

Peter A. Loeb and Manfred Wolff, eds. Nonstandard Analysis for the Working Mathematician,
  second edition, Springer, 2015.

Kiminori Matsuyama, Nobuhiro Kiyotaki and Akihiko Matsui, Toward a theory of international
  currency, Review of Economic Studies 60 (1993), 283–307.

Vincent Maurin, Liquidity fluctuations in over the counter markets, Working paper, European
  University Institute, 2015.

Preston McAfee, Mechanism design by competing sellers, Econometrica 61 (1993), 1281-1312.

Andrew McLennan and Hugo Sonnenschein, Sequential bargaining as a noncooperative foun-
  dation for Walrasian equilibrium, Econometrica 59 (1991), 1395–1424.

Guido Menzio, A theory of partially directed search, Journal of Political Economy 115 (2007),
  748-769.

Monika Merz, Search in the labor market and the real business cycle, Journal of Monetary
  Economics 36 (1995), 269–300.

Monika Merz, Heterogeneous job matches and the cyclical behavior of labor turnover, Journal
  of Monetary Economics 43 (1999), 91–124.

Espen Moen, Competitive search equilibrium, Journal of Political Economy 105 (1997), 385-
  411.

Robert Molzon and Daniela Puzzello, On the observational equivalence of random matching,
  Journal of Economic Theory 145 (2010), 1283–1301.

Dale Mortensen, Property rights and efficiency in mating, racing, and related games, American
  Economic Review 72 (1982), 968–979.

Dale Mortensen and Christopher Pissarides, Job creation and job destruction in the theory of
  unemployment, Review of Economic Studies 61 (1994), 397–415.



                                              24
Michael Peters, Ex-ante price offers in matching games: Non-steady states, Econometrica 59
  (1991), 1425-1454.

Barbara Petrongolo and Christopher Pissarides, Looking into the black box: A survey of the
  matching function, Journal of Economic Literature 39 (2001), 390–431.

Christopher Pissarides, Short-run equilibrium dynamics of unemployment, vacancies, and real
  wages, American Economic Review 75 (1985), 676–690.

Konrad Podczeck and Daniela Puzzello, Independent random matching, Economic Theory 50
  (2012), 1–29.

Richard Rogerson, Robert Shimer and Randall Wright, Search-theoretic models of labor mar-
  kets, Journal of Economic Literature 43 (2005), 959–988.

Shouyong Shi, A directed-search model of inequality with heterogeneous skills and skill-biased
  technology, Review of Economic Studies, 69 (2002), 467-491.

Shouyong Shi and Quan Wen, Labor market search and the dynamic effects of taxes and
  subsidies, Journal of Monetary Economics 43 (1999), 457–495.

Robert Shimer, The cyclical behavior of equilibrium unemployment and vacancies, American
  Economic Review 95 (2005), 25–49.

Xiang Sun, Independent random partial matching with general types, Advances in Mathematics
  286 (2016), 683–702.

Yeneng Sun, The exact law of large numbers via Fubini extension and characterization of
  insurable risks, Journal of Economic Theory 126 (2006), 31–69.

Yeneng Sun and Yongchao Zhang, Individual risk and Lebesgue extension without aggregate
  uncertainty, Journal of Economic Theory 144 (2009), 432–443.

Anton Tsoy, Trade delay, liquidity, and asset prices in over-the-counter markets, Working paper,
  Department of Economics, MIT, October, 2014.

Dimitri Vayanos and Pierre-Olivier Weill, A search-based theory of the on-the-run phenomenon,
  Journal of Finance 63 (2008), 1361–1398.

Makoto Watanabe, A model of merchants, Journal of Economic Theory 145 (2010), 1865-1889.

Pierre-Olivier Weill, Leaning against the wind, Review of Economic Studies 74 (2007), 1329–
  1354.

                                              25
Randall Wright, Philipp Kircher, Benoit Julien, Veronica Guerrieri, Directed search: a guided
  tour, Working paper, 2017.

Eran Yashiv, The determinants of equilibrium unemployment, American Economic Review 90
  (2000), 1297–1322.




                                             26
Appendices
      The following appendices allow matches that result in enduring partnerships, offer some
illustrative examples of applications of our main results to models from the literature on mon-
etary and labor economics, provide a brief introduction to nonstandard analysis, and present
proofs of the results.


A     Dynamic Directed Random Matching with Enduring Partnerships

This appendix extends the model of dynamic directed random matching found in Section 4 so
as to allow for enduring partnerships and for correlated type changes of matched agents. Unlike
the more basic model of Section 4, in order to capture the effect of enduring partnerships we
now must consider separate treatments of existing matched pairs of agents and newly formed
matched pairs of agents.
      We first define such a dynamical system in Subsection A.1. The key condition of Markov
conditional independence is formulated in Subsection A.2. Based on that condition, Subsection
A.3 presents an exact law of large numbers for such a dynamical system. Subsection A.4
provides results covering the existence of Markov conditionally independent dynamical system
with directed random matching and with partnerships that have randomly timed breakups.
We will show that all of our results can be obtained for an agent space that is a Loeb measure
space as constructed in nonstandard analysis, or is an extension of the classical Lebesgue unit
interval. This section has self-contained notation. In particular, some of the notation used in
this section may have a meaning that differs from its usage in Section 4.
      The exact law of large numbers and stationarity for independent dynamic directed ran-
dom matching, as stated in Theorem 2 in Section 4, is a special case of Theorem 4 and Proposi-
tion 4 in Subsection A.3. The existence of independent dynamic directed random matching, as
stated in Theorem 5 and Proposition 5 in Subsection A.4, extend respectively Theorem 3 and
Proposition 3 in Section 4. Hence, the proofs of Theorems 2, 3, and Proposition 3 are omitted.
The proofs of Theorem 4 and Proposition 4 do not use nonstandard analysis, and are given in
Appendix C. The proofs of Theorem 5 and Proposition 5 need nonstandard analysis, and are
presented in Subsections E.2 and E.3 respectively, after a brief introduction to nonstandard
analysis in Appendix D.

A.1    Definition of dynamic directed random matching with enduring partnerships

As in Sections 3 and 4, we fix an atomless probability space (I, I, λ) representing the space of
agents, a sample probability space (Ω, F, P ), and a Fubini extension (I × Ω, I  F, λ  P ). Let
S = {1, 2, . . . , K} be a finite set of types and let J be a special type representing no-matching.

                                                27
The “extended type” space is Ŝ = S × (S ∪ {J}). An agent with an extended type of the form
(k, l) has underlying type k ∈ S and is currently matched to another agent of type l ∈ S. If the
agent’s extended type is instead of the form (k, J), then the type-k agent is “unmatched.” The
space ∆ˆ of extended type distributions is the set of probability distributions p̂ on Ŝ satisfying
p̂(k, l) = p̂(l, k) for all k and l in S.
       Each time period is divided into three steps: mutation, random matching, match-induced
type changing with break-up. We now introduce the primitive parameters governing each of
these steps.
       At the first (mutation) step of time period n ≥ 1, each agent of type k ∈ S experiences
a random mutation, becoming an agent of type l with a given probability bnkl , a parameter of
the model. By definition, for each type k we must have l∈S bnkl = 1.
                                                      P

      At the second step, any currently unmatched agent conducts a directed search for coun-
                                                              ˆ into R+ with the property that
terparties. For each (k, l) ∈ S × S, let q n be a function on ∆
                                                  kl
for all k and l in S, the function p̂kJ qkln ( p̂ ) is continuous in p̂ ∈ ∆ ˆ and satisfies, for any p̂ in ∆,
                                                                                                           ˆ
                                                               X
                                  n                 n                n
                            p̂kJ qkl (p̂) = p̂lJ qlk  (p̂) and      qkr (p̂) ≤ 1.                          (9)
                                                                     r∈S

Whenever the underlying extended type distribution is p̂, the probability31 that an unmatched
agent of type k is matched to an unmatched agent of type l is qkl   n (p̂). Thus, η n (p̂) = 1 −
                                                                                   k
        n (p̂) is the no-matching probability for an unmatched agent of type k.
P
      q
  l∈S kl
       At the third step, each currently matched pair of agents of respective types k and l
(including those who have just been paired at the matching step) breaks up with probability
 n , where
θkl
                                                        n     n
                                                       θkl = θlk .                                              (10)

If a matched pair of agents of respective types k and l stays in their partnership, they become
                                                                               n (r, s), where
a pair of agents of types r and s, respectively, with a specified probability σkl
                            X
                                  n                 n            n
                                σkl (r, s) = 1 and σkl (r, s) = σlk (s, r)                     (11)
                                 r,s∈S

  31
      Let ϕ be any continuous function from ∆    ˆ to itself. Assume that: (1) for any k, l ∈ S, ϕkl (p̂) ≥ p̂kl ; (2)
for any p̂ ∈ ∆, ˆ ϕ(p̂) and p̂ have the same marginal measure on S, that is, for any k ∈ S, P
                                                                                                    r∈S∪{J} ϕkr (p̂) =
P                          ˆ
   r∈S∪{J} p̂kr . If p̂ in ∆ is the underlying extended type distribution for the agents, then ϕ(p̂) represents the
extended type distribution after matching. For any k, l ∈ S, let qkl (p̂) = (ϕkl (p̂) − p̂kl ) /p̂kJ if p̂kJ > 0 and
qkl (p̂) = 0 if p̂kJ = 0. Then, the function q satisfies the continuity condition as well as Equation (9), as required
for a matching probability function. In fact, any matching probability function can be obtained in this way.
For the special case that all of the matched agents break up at the end of each period, we need only consider
continuous functions from ∆ to ∆.   ˆ Let φ be any such continuous function with the property that for any p ∈ ∆,
                                                                        P
ϕ(p) has the marginal measure p on S. That is, for any k ∈ S,             l∈S∪{J} ϕkl (p) = pk . For any k, l ∈ S, let
qkl (p) = φkl (p)/pk if pk > 0 and qkl (p) = 0 if pk = 0. Then, the function q satisfies the continuity condition as
well as Equation (5), as required for a matching probability function. Again, any matching probability function
for this particular setting can be obtained in this way.


                                                           28
for any k, l, r, s ∈ S. The second identity is merely a labeling symmetry condition. If a matched
pair of agents of respective types k and l breaks up, the agent of type k becomes an agent of
                         n (r), where
type r with probability ςkl
                                               X
                                                       n
                                                      ςkl (r) = 1.                             (12)
                                                r∈S

      We now give an inductive definition of the properties defining a dynamical system D for
the behavior of a continuum population of agents experiencing, at each time period: random
mutations, matchings, and match-induced type changes with break-up. We later state condi-
tions under which such a system exists. The state of the dynamical system D at the end of
each integer period n ≥ 0 is defined by a pair Πn = (αn , π n ) consisting of:

   • An agent type function αn : I × Ω → S that is I  F-measurable. The corresponding
      end-of-period type of agent i is αn (i, ω) ∈ S.

   • A random matching π n : I × Ω → I, describing the end-of-period agent π n (i) to whom
      agent i is currently matched, if agent i is currently matched. If agent i is not matched,
      then π n (i) = i. The associated partner-type function g n : I × Ω → S ∪ {J} provides the
      type                                      (
                                                 αn (π n (i))        if π n (i) 6= i
                                      g n (i) =
                                                 J                   if π n (i) = i
      of the agent to whom agent i is matched, if agent i is matched, and otherwise specifies
      g n (i) = J. As a matter of definition, we require that g n is I  F-measurable.


      We take the initial condition Π0 = (α0 , π 0 ) of D as given. The initial condition may, if
desired, be deterministic (constant across Ω). The joint cross-sectional extended type distribu-
tion p̂n at the end of period n is λ(β n )−1 , where β n = (αn , g n ) is the extended type process.
That is, when ω ∈ Ω is a sample realization, p̂nω (k, l) is the fraction of the population at the
end of period n that has type k and is matched to an agent of type l. Likewise, p̂nω (k, J) is the
fraction of the population that is of type k and is not matched.
      For the purpose of the inductive definition of the dynamical system D, we suppose that
Πn−1 = (αn−1 , π n−1 ) has been defined for some n ≥ 1, and define Πn = (αn , π n ) as follows.

Mutation.       The post-mutation type function ᾱn is I  F-measurable, and satisfies, for any
k1 , k2 , l1 , and l2 in S, for any r ∈ S ∪ {J}, and for λ-almost-every agent i,

                      P (ᾱin = k2 , ḡin = l2 | αin−1 = k1 , gin−1 = l1 ) = bnk1 k2 bnl1 l2   (13)



                                                        29
                      P (ᾱin = k2 , ḡin = r | αin−1 = k1 , gin−1 = J) = bnk1 k2 δJ (r),            (14)

where δJ (r) is one if r = J, and zero otherwise. Equation (13) means that a paired agent
and her partner mutate independently. The post-mutation partner-type function ḡ n is defined
by ḡ n (i, ω) = ᾱn (π n−1 (i, ω), ω), for any ω ∈ Ω. We assume that ḡ n is I  F-measurable.
The post-mutation extended-type function is β̄ n = (ᾱn , ḡ n ). The post-mutation extended type
                                                                 −1
distribution that is realized in state ω ∈ Ω is p̌n (ω) = λ β̄ωn     .

Matching.         Let π̄ n : I × Ω → I be a random matching with the following properties.

  (i) For each state ω ∈ Ω, let Aω = {i : π n−1 (i, ω) 6= i} be the set of agents who are matched.
      We have
                                          π̄ωn (i) = πωn−1 (i) for i ∈ Aω ,                          (15)

      meaning that those agents who were already matched at the end of period n − 1 remain
      matched (to the same partner) at this step, which implies that the post-matching partner-
      type function ḡ¯n , defined by
                                             (
                                              ᾱn (π̄ n (i, ω), ω)    if π̄ n (i, ω) 6= i
                               ḡ¯n (i, ω) =
                                              J                       if π̄ n (i, ω) = i,

      satisfies
                                      P (ḡ¯in = r | ᾱin = k, ḡin = l) = δl (r),                   (16)

      for any k and l in S and any r ∈ S ∪ {J}, where δc (d) is zero if c 6= d and is one if c = d.

 (ii) ḡ¯n is I  F-measurable.

(iii) Given the post-mutation extended type distribution p̌n , an unmatched agent of type k is
                                                                           n (p̌n ), in that, for
      matched to a unmatched agent of type l with conditional probability qkl
      λ-almost every agent i and P -almost every ω,

                                P (ḡ¯in = l | ᾱin = k, ḡin = J, p̌n ) = qkl
                                                                            n n
                                                                               (p̌ (ω)),             (17)

      which also implies that

                                P (ḡ¯in = J | ᾱin = k, ḡin = J, p̌n ) = ηkn (p̌n (ω)).            (18)

The extended type of agent i after the random matching step is β̄¯in = (ᾱin , ḡ¯in ).

Type changes of matched agents with break-up.                        This step determines an end-of-period
random matching π n , an I  F-measurable agent type function αn , and an I  F-measurable

                                                       30
partner-type function g n so that we have g n (i, ω) = αn (π n (i, ω), ω) for all (i, ω) ∈ I × Ω, and
so that, for λ-almost every agent i and for any k1 , k2 , l1 , l2 ∈ S and r ∈ S ∪ {J},
                                          (
                                           π̄ n (i), if π n (i) 6= i
                                π n (i) =                                                               (19)
                                           i,        if π n (i) = i


                        P (αin = l1 , gin = r | ᾱin = k1 , ḡ¯in = J) = δk1 (l1 ) δJ (r)               (20)


                 P (αin = l1 , gin = l2 | ᾱin = k1 , ḡ¯in = k2 ) = (1 − θkn1 k2 )σkn1 k2 (l1 , l2 )   (21)


                      P (αin = l1 , gin = J | ᾱin = k1 , ḡ¯in = k2 ) = θkn1 k2 ςkn1 k2 (l1 ).         (22)

Equation (19) says that agent i cannot change her partner if she remains matched. Equation
(20) mean that unmatched agents stay unmatched without changing types. Equations (21) and
(22) specify the type changing probabilities for a pair of matched agents who stay together or
break up. The extended-type function at the end of the period is β n = (αn , g n ).

      Thus, we have inductively defined the properties of a dynamical system D = (Πn )∞
                                                                                      n=1
incorporating the effects of random mutation, directed random matching, and match-induced
type changes with break-up, consistent with given parameters (b, q, θ, σ, ς). The initial condition
Π0 of D is unrestricted. We next turn to the key Markovian independence properties for such a
system, and then to the exact law of large numbers and existence of a dynamical system with
these properties.

A.2    Markov conditional independence

We now add independence conditions on the dynamical system D = (Πn )∞
                                                                    n=0 , along the lines
of those in Duffie and Sun (2007, 2012), and Section 4. The idea is that each of the just-
described steps (mutation, random matching, and match-induced type changes with break-up)
are conditionally independent across almost all agents. In the following definition, we will
refer to objects, such as the intermediate-step extended type functions β̄ n and β̄¯n , that were
constructed in the previous subsection.
      We say that the dynamical system D is Markov conditionally independent (MCI) if, for
λ-almost every i and λ-almost every j, for every period n ≥ 1, and for all k1 , k2 ∈ S, and
l1 , l2 ∈ S ∪ {J}, the following five properties apply:

   • Initial independence: βi0 and βj0 are independent.



                                                         31
   • Markov and independent mutation:

                 P β̄in = (k1 , l1 ), β̄jn = (k2 , l2 )                   (βit )n−1     t n−1
                                                                                               
                                                                                t=0 , (βj )t=0
                                                                                                                          
                                      = P β̄in = (k1 , l1 )               βin−1 P β̄jn = (k2 , l2 )                    βjn−1 .                        (23)


   • Markov and independent random matching:
                                                                                                                 
                       P β̄¯in = (k1 , l1 ), β̄¯jn = (k2 , l2 )                                   n−1
                                                                              β̄in , β̄jn , (βit )t=0         n−1
                                                                                                      , (βjt )t=0
                                                                                                                 
                                   = P β̄¯in = (k1 , l1 )                     β̄in P β̄¯jn = (k2 , l2 ) β̄jn .                                        (24)


   • Markov and independent matched-agent type changes with break-up:
                                                                                                   
                         P βin = (k1 , l1 ), βjn = (k2 , l2 ) β̄¯in , β̄¯jn , (βit )t=0
                                                                                    n−1         n−1
                                                                                        , (βjt )t=0
                                                                                                
                                   = P βin = (k1 , l1 ) | β̄¯in P βjn = (k2 , l2 ) | β̄¯jn .                                                          (25)

A.3    The exact law of large numbers for MCI dynamical systems with enduring
       partnerships

                                                   ˆ to ∆
For each period n ≥ 1, we define a mapping Γn from ∆    ˆ by
                        X                                                            X
   Γnkl (p̂) =                      p̃nk1 l1 (1 − θkn1 l1 )σkn1 l1 (k, l) +                       p̃k1 J qkn1 l1 (p̃n )(1 − θkn1 l1 )σkn1 l1 (k, l)
                    (k1 ,l1 )∈S 2                                                (k1 ,l1 )∈S 2
                                           X                                                X
  ΓnkJ (p̂) = p̃kJ ηkn (p̃) +                           p̃nk1 l1 θkn1 l1 ςkn1 l1 (k) +                   p̃nk1 J qkn1 l1 (p̃n )θkn1 l1 ςkn1 l1 (k),
                                        (k1 ,l1 )∈S 2                                    (k1 ,l1 )∈S 2

where p̃nkl =                       p̂k1 l1 bnk1 k bnl1 l and p̃nkJ =               p̂lJ bnlk .
                P                                                         P
                    (k1 ,l1 )∈S 2                                             l∈S


      The following theorem, which extends Theorem 4.3, presents an exact law of large num-
bers for the joint agent-partner type processes at the end of each period. The result also
provides a recursive calculation of the cross-sectional joint agent-partner type distribution p̂n
at the end of period n.

Theorem 4 Let D be a dynamical system with random mutation, random matching, and
match-induced type changes with break-up whose parameters are (b, q, θ, σ, ς). If D is Markov
conditionally independent, then:

 (1) For each time period n ≥ 1, the expected cross-sectional extended type distributions
      p̃n = E(p̌n ) after the mutation step and E(p̂n ) at the end of the period are given by,
                                               n−1 n                                      n−1 n
      respectively, E(p̌nkl ) =                              n            n
                                P                                                P
                                 k1 ,l1 ∈S E(p̂k1 l1 )bk1 k bl1 l and E(p̌kJ ) =  l∈S E(p̂lJ )blk , and by
      E(p̂n ) = Γn (E(p̂n−1 )).


                                                                         32
 (2) For λ-almost every agent i, the extended-type process {βin }∞
                                                                 n=0 is a Markov chain in Ŝ
       whose transition matrix z n at time n − 1 is given by
                                                                         X
                        n
                       z(kJ)(k 0 J) = bnkk0 ηkn0 (p̃n ) +                          bnkk1 qkn1 l1 (p̃n )θkn1 l1 ςkn1 l1 (k 0 ),
                                                                      k1 ,l1 ,∈S
                                               X
                         n
                        z(kl)(k 0 J)     =               bnkk1 bnll1 θkn1 l1 ςkn1 l1 (k 0 ),
                                             k1 ,l1 ∈S
                                               X
                        n
                       z(kJ)(k 0 l0 )    =               bnkk1 qkn1 l1 (p̃n )(1 − θkn1 l1 )σkn1 l1 (k 0 , l0 ),
                                             k1 ,l1 ∈S
                                               X
                         n
                        z(kl)(k 0 l0 )   =               bnkk1 bnll1 (1 − θkn1 l1 )σkn1 l1 (k 0 , l0 ).                          (26)
                                             k1 ,l1 ∈S


 (3) For λ-almost every i and λ-almost every j, the Markov chains {βin }∞         n ∞
                                                                        n=0 and {βj }n=0 are
       independent.

 (4) For P -almost every ω ∈ Ω, the cross-sectional extended-type process {βωn }∞
                                                                                n=0 is a Markov
       chain32 with transition matrix z n at time n − 1.

 (5) For P -almost all ω ∈ Ω, at each time period n ≥ 1, the realized cross-sectional extended
       type distribution after random mutation λ(β̄ωn )−1 is equal to its expectation p̃n , and the
       realized cross-sectional extended type distribution at the end of period n, p̂n (ω) = λ(βωn )−1 ,
       is equal to its expectation E(p̂n ).

                                  ˆ that is the probability distribution of the initial extended
 (6) If there is some fixed p̈0 ∈ ∆
       type βi0 of agent i for λ-almost every i, then for λ-almost every i the Markov chain
       βi = {βin }∞                                                      0    ∞    n
                  n=0 has the sample-path probability distribution ξ = p̈ ⊗ (⊗n=1 z ) on the
       space Ŝ ∞ . Moreover, in this case, ξ = λ(βω )−1 for P -almost every ω. That is, for any
       measurable rectangle A = ∞                ∞ of sample paths, the probability ξ(A) is equal,
                                  Q
                                     n=0 An ⊆ Ŝ
       for P -almost every ω ∈ Ω, to the fraction λ({i : βω (i) ∈ A}) of agents whose extended
       type process has a sample path in A in sample realization ω.

       For the time-independent case, in which the parameters (b, q, θ, σ, ς) do not depend on
the time period n ≥ 1, the following proposition shows the existence of a stationary extended
type distribution.

Proposition 4 Suppose that the parameters (b, q, θ, σ, ς) are time independent. Then there
                                           ˆ that is a stationary distribution for any MCI dy-
exists an extended-type distribution p̂∗ ∈ ∆
namical system D with parameters (b, q, θ, σ, ς), in the sense that:
  32
    For a given sample realization ω ∈ Ω, {βωn }∞
                                                n=0 is defined on the agent space (I, I, λ), which is a probability
space itself. Thus, {βωn }∞
                          n=0 can be viewed as a discrete-time process.




                                                                  33
 (1) For every n ≥ 0, the realized cross-sectional extended-type distribution p̂n at time n is p̂∗
        P -almost surely;

 (2) All of the relevant Markov chains in Theorem 4 are time homogeneous with a constant
        transition matrix z 1 having p̂∗ as a fixed point;

 (3) If the initial extended type process β 0 is i.i.d. across agents, then, for λ-almost every i,
        the extended type distribution of agent i at any period n ≥ 0 is P (βin )−1 = p̂∗ .

A.4      Existence of MCI dynamic directed random matching with enduring part-
         nerships

The following theorem provides for the existence of a Markov conditionally independent (MCI)
dynamical system with random mutation, random matching, and match-induced type changes
with break-up. Theorem 3 is a special case.

Theorem 5 For any primitive model parameters (b, q, θ, σ, ς) and for any extended type dis-
                ˆ there exists a Fubini extension (I × Ω, I  F, λ  P ) on which is defined a
tribution p̈0 ∈ ∆,
dynamical system D = (Πn )∞
                          n=0 with random mutation, random matching, and match-induced
type changes with break-up, that is Markov conditionally independent with these parameters
(b, q, θ, σ, ς), and with the initial cross-sectional extended type distribution p̂0 being p̈0 with
probability one. In addition, for any n ≥ 1, π n and π̄ n are measure preserving in the sense that
for each ω ∈ Ω, λ((πωn )−1 (A)) = λ((π̄ωn )−1 (A)) = λ(A) for any A ∈ I. These properties can
be achieved with an initial condition Π0 that is deterministic, or alternatively with an initial
extended type process β 0 that is i.i.d. across agents.33

        In the next proposition, we show that the agent space (I, I, λ) in Theorem 5 can be an
extension of the classical Lebesgue unit interval (L, L, χ). That is, we can take I = L = [0, 1]
with a σ-algebra I that contains the Lebesgue σ-algebra L, and so that the restriction of λ to
L is the Lebesgue measure χ.

Proposition 5 Fixing any model parameters (b, q, θ, σ, ς) and any initial cross-sectional ex-
                               ˆ there exists a Fubini extension (I × Ω, I  F, λ  P ) such
tended type distribution p̈0 ∈ ∆,
that:

 (1) The agent space (I, I, λ) is an extension of the Lebesgue unit interval (L, L, χ).
  33
    This means that the process β 0 is essentially pairwise independent, and that βi0 has distribution p̈0 for
λ-almost every agent i.




                                                     34
 (2) There is defined on the Fubini extension a dynamical system D = (Πn )∞
                                                                          n=0 that is Markov
       conditionally independent with the parameters (b, q, θ, σ, ς), where the initial cross-sectional
       extended type distribution p̂0 is p̈0 with probability one.

 (3) These properties can be achieved with an initial condition Π0 that is deterministic, or
       alternatively with an initial extended type process β 0 that is i.i.d. across agents.34

B     Illustrative applications in monetary and labor economics

This appendix provides three example applications, which are designed to illustrate how our
results provide a mathematical foundation for the dynamic matching models used in monetary
economics and labor economics. The first example is from Kiyotaki and Wright (1989) and
Kehoe, Kiyotaki and Wright (1993). The second example is from Matsuyama, Kiyotaki and
Matsui (1993). The last example treats the labor-market matching model of Andolfatto An-
dolfatto (1996), a setting that calls for enduring matches of the sort considered in Appendix
A.

B.1     Kiyotaki-Wright: Model A

As in Model A of Kiyotaki and Wright (1989), three indivisible goods are labeled 1, 2, and
3. There is a continuum of agents of unit total mass. A given type of agent consumes good
k and can store one unit of good l, for some l 6= k. This type is denoted hk, li. The economy
is thus populated by agents of 6 distinct types h1, 2i, h1, 3i, h2, 1i, h2, 3i, h3, 1i, and h3, 2i, which
form our type space35 S. In order to avoid confusion over differences in terminology36 with
Kiyotaki and Wright (1989), we say that an agent who consumes good k has “trait” k. There
are equal proportions of agents with the three respective traits. In each period n, every agent
is randomly matched with some other agent. When matched, two agents decide whether or
not to trade. If there is no trade between the matched pair, they keep their goods. If there is
a trade, and if the agent who consumes good k gets good k from the other, then that agent
immediately consumes good k and produces one unit of good k + 1 (modulo 3), so that his type
becomes hk, k +1i (modulo 3, as needed). If there is a trade and an agent with trait k gets good
l for l 6= k, then his type becomes hk, li. Kiyotaki and Wright (1989) and Kehoe, Kiyotaki and
    34
       The statement in Footnote 19 is still valid in this more general case. For the convenience of the reader, we
repeat it here. There exists a sub-σ-algebra I 0 of I and a Fubini extension (I × Ω, I 0  F, λ  P ) such that
I 0 F ⊆ I F, the dynamical system D on (I ×Ω, I 0 F, λP ) is Markov conditionally independent, and for any
n ≥ 1, π n and π̄ n are measure preserving in the sense that for each ω ∈ Ω, λ((πωn )−1 (A)) = λ((π̄ωn )−1 (A)) = λ(A)
for any A ∈ I 0 . See the penultimate paragraph of the proof of this proposition in Subsection E.3.
    35
       It is clear that our results, which are stated earlier in terms of S = {1, 2, . . . , K}, hold for any finite type
space with appropriate notational change.
    36
       In Kiyotaki and Wright (1989), the meaning of “type” is different from that in our present paper.



                                                           35
Wright (1993) consider this matching model with both stationary and non-stationary trading
strategies.
       We can use our model of dynamic directed random matching in Section 4 to give a mathe-
matical foundation for the matching models in Kiyotaki and Wright (1989) and Kehoe, Kiyotaki
and Wright (1993) by choosing suitable parameters (b, q, ν) governing random mutation, ran-
dom matching and match-induced type changing. At period n, let bnhk1 ,l1 ihk2 ,l2 i = δk1 (k2 )δl1 (l2 )
                                        n
be the mutation probabilities, and let qhk                  (p) = phk2 ,l2 i be the matching probabilities
                                           1 ,l1 ihk2 ,l2 i

for p ∈ ∆. We will need to specify the match-induced type changing probabilities in both cases.
       First, a stationary (pure) trading strategy in Kiyotaki and Wright (1989, p. 931) is
described by some τ : {1, 2, 3} × {1, 2, 3} → {0, 1} that implies a trade, τk (l, r) = 1, if a trait-k
agent actually wants to trade good l for good r, and results in no trade, τk (l, r) = 0, otherwise.
Thus τ determines the match-induced type changes. Because the consumption traits of agents
do not change, the type of a matched agent cannot change to a type with a different trait.
Thus, for the type-changing probability ν n of an agent with trait k1 , the probability for the
target types is concentrated on only two types, hk1 , k1 + 1i and hk1 , k1 + 2i. This means that
it suffices to define the type-changing probability for only the target type hk1 , k1 + 1i. Suppose
that an agent i of type hk1 , k1 + 1i is matched with an agent j of type hk2 , l2 i. For l2 = k1 + 1,
there is no need to trade. When l2 = k1 and there is a trade, agent i will consume good k1 ,
produces a unit of good k1 + 1, and keeps the same type hk1 , k1 + 1i. (This applies trivially for
the no-trade case.) When l2 = k1 + 2, the probability νhk1 ,k1 +1ihk2 ,l2 i ({hk1 , k1 + 1i}) that agent
i has a type change is the probability of no trade between agents i and j. The probability of
having a trade between agents i and j is τk1 (k1 + 1, l2 )τk2 (l2 , k1 + 1). We therefore have
                                                (
       n                                         1                                      if l2 6= k1 + 2
      νhk1 ,k1 +1ihk2 ,l2 i ({hk1 , k1 + 1i}) =
                                                 1 − τk1 (k1 + 1, l2 )τk2 (l2 , k1 + 1) if l2 = k1 + 2.

By similar arguments,
                                                    (
          n                                          0                                    if l2 = k1 + 2
         νhk 1 ,k1 +2ihk2 ,l2 i
                                ({hk1 , k1 + 1i}) =
                                                     τk1 (k1 + 2, l2 )τk2 (l2 , k1 + 2)         6 k1 + 2.
                                                                                          if l2 =

       Next, we consider the case of non-stationary trading strategies, as in Sections 3 and 6
of Kehoe, Kiyotaki and Wright (1993). Suppose that (s1 (n), s2 (n), s3 (n)) is a time-dependent
mixed strategy at period n, where sk (n) is the probability that an agent with trait k trades
                                                                                        n
good k +1 for k +2. Based on (s1 (n), s2 (n), s3 (n)), one can compute the probability Phk         (k3 )
                                                                                           1 ,k2 i

that an agent of type hk1 , k2 i trades for good k3 .
       We must define the match-induced type changing probabilities corresponding to the given
time-dependent mixed strategy (s1 (n), s2 (n), s3 (n)). Suppose that an agent of type hk1 , k1 + 1i
is matched with an agent of type hk2 , l2 i. For cases with l2 = k1 or l2 = k1 + 1, the type

                                                         36
                      n
changing probability νhk                   ({hk1 , k1 + 1i}) = 1. When l2 = k1 + 2, the probability that
                        1 ,k1 +1ihk2 ,l2 i

the match leads to a trade is Phk        n                 n
                                                     (l2 )Phk        (k1 + 1). Thus
                                           1 ,k1 +1i         2 ,l2 i
                                                   (
      n                                               1                                                if l2 6= k1 + 2
     νhk 1 ,k1 +1ihk2 ,l2 i
                            ({hk1 , k1 + 1i}) =               n                 n
                                                      1 − Phk   1 ,k1 +1i
                                                                          (l2 )Phk2 ,l2 i
                                                                                          (k1 + 1)     if l2 = k1 + 2.

Similarly,
                                                       (
           n                                            0                                            if l2 = k1 + 2
          νhk 1 ,k1 +2ihk2 ,l2 i
                                 ({hk1 , k1   + 1i}) =    n                 n
                                                        Phk 1 ,k1 +2i
                                                                      (l2 )Phk2 ,l2 i
                                                                                      (k1 + 2)             6 k1 + 2.
                                                                                                     if l2 =



B.2      Matsuyama, Kiyotaki and Matsui

Our next example is from Matsuyama, Kiyotaki and Matsui (1993). Here, agents are divided
into two groups. Agents are more likely to be matched to a counterparty of their own group
than to a counterparty of a different group.
        The economy is populated by a continuum of infinitely-lived agents of unit total mass.
Agents are from two regions, Home and Foreign. Let r ∈ (0, 1) be the size of the Home
population. There are K ≥ 3 kinds of indivisible commodities. Within each region, there are
equal proportions of agents with the K respective traits. An agent with trait k derives utility
only from consumption of commodity k. After he consumes commodity k, he is able to produce
one and only one unit of commodity k + 1 (mod K) costlessly, and can also store up to one
unit of his production good costlessly. He can neither produce nor store other types of goods.
        In addition to the commodities described above, there are two distinguishable fiat monies
without intrinsic worth, which we call the Home currency and the Foreign currency. Each
currency is indivisible and can be stored costlessly in amounts of up to one unit by any agent,
provided that the agent does not carry his production good or the other currency. This implies
that, at any date, the inventory of each agent consists of one unit of the Home currency, one
unit of the Foreign currency, or one unit of his production good, but does not include more
than one of these objects in total at any one time.
        For some β ∈ (0, 1), in each period n, a Home agent is matched to a Home agent with
probability r, and is matched to a Foreign agent with probability β(1 − r). The probability
with which he is not matched is thus (1 − β)(1 − r). Similarly, a Foreign agent is matched to
a Home agent with probability βr, is matched to a Foreign agent with probability (1 − r), and
is unmatched with probability (1 − β)r.
        The type space S is the set of ordered tuples of the form (a, b, c), where a ∈ {H, F },
b ∈ {1, . . . , K}, and c ∈ {g, h, f }. Here, H represents Home, F represents Foreign, g represents
good, h represents Home currency, and f represents Foreign currency.

                                                                37
       An agent chooses a trading strategy to maximize his expected discounted utility, taking
as given the strategies of other agents and the distribution of inventories. Matsuyama, Kiyotaki
and Matsui (1993) focused on pure strategies that depend only on an agent’s nationality and
the objects that he and his counterparty have as inventories. Thus, the Home agent’s (pure)
trading strategy can be described simply as
                           (
                      H     1 if he agrees to trade object a for object b
                     τab =
                            0 otherwise,

where a and b are in {g, h, f }. The Foreign agent’s trading strategy can similarly be described
as                               (
                          F       1 if he agrees to trade object a for object b
                         τab   =
                                  0 otherwise.
              H = 0 means that a Home agent does not agree to trade his production good
For example, τgf
                                 F = 1 means that a Foreign agent agrees to trade the Home
for the Foreign currency, while τhg
currency for his consumption good.
       We can apply our model of dynamic directed random matching with immediate break-up
in Section 4 to give a mathematical foundation for the matching model in Matsuyama, Kiyotaki
and Matsui (1993) by choosing suitable time-independent parameters (b, q, ν) governing random
mutation, random matching, and match-induced type changing. To this end, we take mutation
probabilities
                                   b(a1 ,b1 ,c1 )(a2 ,b2 ,c2 ) = δa1 (a2 )δb1 (b2 )δc1 (c2 ).

For a given cross-sectional agent type distribution p ∈ ∆, the directed matching probabilities
are                                                           (
                                                               p(a2 ,b2 ,c2 )       if a1 = a2
                            q(a1 ,b1 ,c1 )(a2 ,b2 ,c2 ) (p) =
                                                               β · p(a2 ,b2 ,c2 )         6 a2 .
                                                                                    if a1 =
       Because the nationalities and consumption traits of agents do not change, a matched
agent cannot change to a type with a different nationality or trait. Thus, for the type changing
probability ν of an agent with nationality a1 and trait b1 , search is directed to the three
counterparty types (a1 , b1 , g), (a1 , b1 , f ) and (a1 , b1 , h).
       Suppose that agent i is of type (a1 , b1 , g) and is matched with agent j, who has type
(a2 , b2 , c2 ). The probability that agent i changes type to (a1 , b1 , h) is ν(a1 ,b1 ,g)(a2 ,b2 ,c2 ) ({(a1 , b1 , h)}).
The good carried by an agent of type (a1 , b1 , g) must be b1 + 1. For b2 6= b1 + 1 (mod K), the
good that agent i carries is not the consumption good of agent j, which means that there is no
trade, so the probability ν(a1 ,b1 ,g)(a2 ,b2 ,c2 ) ({(a1 , b1 , h)}) is 0. When c2 6= h, agent i cannot get
the Home currency from j, so ν(a1 ,b1 ,g)(a2 ,b2 ,c2 ) ({(a1 , b1 , h)}) is also 0. When b2 = b1 + 1 and
c2 = h, ν(a1 ,b1 ,g)(a2 ,b2 ,c2 ) ({(a1 , b1 , h)}) is the probability that agent i trades with an agent with


                                                              38
                               a1    a2
the type of agent j, which is τgh · τhg . We therefore have
                                                          (
                                                            a1    a2
                                                           τgh · τhg        if b2 ≡ b1 + 1 (mod K) and c2 = h
          ν(a1 ,b1 ,g)(a2 ,b2 ,c2 ) ({(a1 , b1 , h)}) =
                                                            0               otherwise.

The following type-change probabilities can be obtained by similar arguments:
                                                     (
                                                        a1
                                                      τgf  · τfag2 if b2 ≡ b1 + 1 (mod K) and c2 = f ,
      ν(a1 ,b1 ,g)(a2 ,b2 ,c2 ) ({(a1 , b1 , f )}) =
                                                      0            otherwise

ν(a1 ,b1 ,g)(a2 ,b2 ,c2 ) ({(a1 , b1 , g)}) = 1−ν(a1 ,b1 ,g)(a2 ,b2 ,c2 ) ({(a1 , b1 , h)})−ν(a1 ,b1 ,g)(a2 ,b2 ,c2 ) ({(a1 , b1 , f )})
                                                             (
                                                                  a1     a2
                                                               τhg    · τgh   if b2 ≡ b1 − 1 (mod K) and c2 = g
           ν(a1 ,b1 ,h)(a2 ,b2 ,c2 ) ({(a1 , b1 , g)}) =
                                                               0              otherwise
                                                                              (
                                                                                   a1
                                                                                 τhf  · τfah2 c2 = f
                               ν(a1 ,b1 ,h)(a2 ,b2 ,c2 ) ({(a1 , b1 , f )}) =
                                                                                 0            otherwise
ν(a1 ,b1 ,h)(a2 ,b2 ,c2 ) ({(a1 , b1 , h)}) = 1−ν(a1 ,b1 ,h)(a2 ,b2 ,c2 ) ({(a1 , b1 , g)})−ν(a1 ,b1 ,h)(a2 ,b2 ,c2 ) ({(a1 , b1 , f )})
                                                               (
                                                                 τfag1 · τgf
                                                                          a2
                                                                               if b2 ≡ b1 − 1 (mod K) and c2 = g
           ν(a1 ,b1 ,f )(a2 ,b2 ,c2 ) ({(a1 , b1 , g)}) =
                                                                 0             otherwise
                                                                               (
                                                                                  τfah1 · τhf
                                                                                           a2
                                                                                              if c2 = h
                                ν(a1 ,b1 ,f )(a2 ,b2 ,c2 ) ({(a1 , b1 , h)}) =
                                                                                  0           otherwise
ν(a1 ,b1 ,f )(a2 ,b2 ,c2 ) ({(a1 , b1 , f )}) = 1−ν(a1 ,b1 ,f )(a2 ,b2 ,c2 ) ({(a1 , b1 , h)})−ν(a1 ,b1 ,f )(a2 ,b2 ,c2 ) ({(a1 , b1 , g)}) .



B.3      Matching in labor markets with multi-period employment episodes

This example is taken from Andolfatto (1996), whose Section 1 considers a discrete-time labor-
market-search model. The agents are workers and firms. Each firm has a single job position.
Section 2 of Andolfatto (1996) works with stationary distributions. We can use the model
of dynamic directed random matching with enduring partnership developed in Appendix A
to capture the search process leading to Equation (1) of Andolfatto (1996) in the stationary
setting.
         The agent type space is S = {E, U, A, V, D}. Here, E and U represent, respectively,
employed workers and unemployed workers while A, V and D represent active, vacant and
dormant jobs respectively. Dormant job positions are neither matched with a worker nor
immediately open. The proportion of agents that are workers is w > 0.
         At the beginning of each period, each vacant firm may mutate to a dormant job and
each dormant job may mutate to a vacant job. Let p̌U J and p̌V J be the respective proportions
of unemployed workers and vacant firms after the mutation step. In the stationary setting,

                                                                  39
the quantity M (p̌V J , e · p̌U J ) of new job matches in a given period is governed by a continuous
aggregate matching function M : [0, 1] × R+ → [0, 1] that incorporates37 the search effort e
applied by each worker seeking employment with M (p̌V J , e · p̌U J ) ≤ min{p̌V J , p̌U J }. Job-worker
pairs that have existed for at least one period are assumed to break up with probability θ̄ in
each period. Newly formed pairs cannot break up in the current period. While a job-worker
pair maintain their partnership, their current types (A, E) do not change. On the other hand,
if they break up, the job becomes vacant and the worker becomes unemployed.
       Equation (1) in Andolfatto (1996) in the stationary setting is

                                 E ∗ = (1 − θ̄)E ∗ + M (V ∗ , e · (w − E ∗ )),                              (27)

where E ∗ and V ∗ are the respective fractions of employed workers and vacant jobs in the
particular case.38
       Viewed in terms of our model in Appendix A, the corresponding time-independent pa-
rameters are given as follows. Vacant firms could mutate to dormant, and vice versa. Workers
and active firms do not mutate. For any k and l in S, let
                              1−w−E ∗ −V ∗
                            
                             1−w−E ∗
                                           if k = V or D and l = D
                                  V ∗
                       bkl = 1−w−E ∗        if k = V or D and l = V
                            
                              δk (l)        otherwise.
                            

Matching occurs only between unemployed workers and vacant jobs. The matching probabilities
are defined as follows. For any k and l in S,          define
                               M (p̌ , e·p̌ )
                              
                              
                              
                                     VJ
                                      p̌U J
                                               UJ
                                                       if (k, l) = (U, V ) and p̌U J > 0
                    qkl (p̌) = M (p̌Vp̌J , e·p̌U J )   if (k, l) = (V, U ) and p̌U J > 0
                              
                                       VJ
                               0                       otherwise.
                              

Next, we consider the step of type changing with break-up. For any k, l, r, s ∈ S, we have
                                (
                                 θ̄ if (k, l) = (E, A) or (A, E)
                         θkl =                                                             (28)
                                 0 otherwise

                                      
                                      δE (r)δA (s)
                                                           if k = U and l = V
                          σkl (r, s) = δA (r)δE (s)         if k = V and l = U                              (29)
                                      
                                       δk (r)δl (s)         otherwise
                                      

  37
     The mass of workers is assumed to be one in Andolfatto (1996). Since the matching function in Andolfatto
(1996) is assumed to have constant returns to scale, one can re-scale the total worker-firm population to be one,
with a proportion w of agents being workers.
  38
     See (P60 ) on page 120 of Andolfatto (1996) for the steady state equation with the Cobb-Douglas matching
function.


                                                       40
                                        
                                        δU (r)
                                                      if k = E and l = A
                               ςkl (r) = δV (r)        if k = A and l = E                                     (30)
                                        
                                         δk (r)        otherwise.
                                        

Equation (28) means that an employed worker has probability θ̄ of losing her job. When two
agents are newly matched in the current period, the worker-firm types change from (U, V ) to
(E, A). For those paired agents who were matched in a previous period, their types do not
change while they stay together. Finally, the worker-firm pair of types from (E, A) to (U, V )
when they break up. Equations (29) and (30) express these ideas.
        Taking the equilibrium search effort e as given, Theorem 4 and Proposition 4 imply that
any stationary type distribution satisfies

                                                p̂∗EA = Γ(p̂∗ )EA .                                           (31)

We take a stationary type distribution p̂∗ corresponding to the given fractions of employed
workers and vacant jobs E ∗ and V ∗ as in Equation (27), which means that p̂∗EA = E ∗ and
p̂∗V J = V ∗ . By the formulas above the statement of Theorem 4, we obtain that

                                    ΓEA (p̂∗ ) = p̃EA (1 − θ̄) + p̃U J qU V (p̃)

                                      p̃U J = p̂∗U J = w − p̂∗EA = w − E ∗

                p̃V J = p̂∗V J bV V + p̂∗DJ bDV = p̂∗V J bV V + (1 − w − p̂∗EA − p̂∗V J )bDV = V ∗ .

Substituting the above terms into Equation (31), we derive

                            E ∗ = p̂∗EA = (1 − θ̄)E ∗ + M (V ∗ , e · (w − E ∗ )) .

Thus the stationary distribution of employed workers and vacant jobs considered in Andolfatto
(1996) can be derived from our model of dynamic directed random matching with enduring
partnership with appropriate parameters.

C      Proofs of Theorem 4 and Proposition 4

Before proving Theorem 4, we need a few lemmas.
        First, we state the following general version of the exact law of large numbers in Sun
(2006) as a lemma here for the convenience of the reader.39
  39
     Part (2) of the lemma is part of Theorem 2.8 in Sun (2006). That theorem actually shows that the statement
in Part (2) here is equivalent to the condition of essential pairwise independence. While Parts (1), (3) and (4) of
the lemma are special cases of Part (2), they are stated respectively in Corollary 2.9, Theorem 2.12 and Corollary
2.10 of Sun (2006).




                                                        41
Lemma 1 Let f be a measurable process from a Fubini extension (I × Ω, I  F, λ  P ) to a
complete separable metric space X.

   1. For P -almost all ω ∈ Ω, the sample distribution λfω−1 of the sample function fω is the
         same as the distribution (λ  P )f −1 of the process.40

   2. For any A ∈ I with λ(A) > 0, let f A be the restriction of f to A × Ω, λA and λA  P
         the probability measures rescaled from the restrictions λ and λ  P to {D ∈ I : D ⊆ A}
         and {C ∈ I  F : C ⊆ A × Ω} respectively. Then for P -almost all ω ∈ Ω, the sample
         distribution λA (f A )−1                         A
                               ω of the sample function (f )ω is the same as the distribution of
         (λA  P )(f A )−1 of the process f A .

   3. If there is a distribution µ on X such that for λ-almost all i ∈ I, the random variable
         fi has distribution µ, then the sample function fω (or (f A )ω ) also has distribution µ for
         P -almost all ω ∈ Ω.

   4. If X is the real line R and f is integrable on (I × Ω, I  F, λ  P ), then for P -almost all
              R           R
      ω ∈ Ω, I fω dλ = I×Ω f dλ  P .

          By viewing a discrete-time stochastic process taking values in X as a random variable
taking values in X ∞ , Lemma 1 implies the following exact law of large numbers for a continuum
of discrete-time stochastic processes, which is formally stated in Theorem 2.16 in Sun (2006).

Corollary 1 Let f be a mapping from I × Ω × N to a complete separable metric space X such
that for each n ≥ 0, f n = f (·, ·, n) is an I F-measurable process. Then, for λ-almost all i ∈ I,
{fin }∞                                                                                 n ∞
      n=0 is a discrete-time stochastic process. Assume that the stochastic processes {fi }n=0 , i ∈
I are essentially pairwise independent, i.e., for λ-almost all i ∈ I, λ-almost all j ∈ I, the
random vectors (fi0 , . . . , fin ) and (fj0 , . . . , fjn ) are independent for all n ≥ 0. Then, for P -almost
all ω ∈ Ω, the empirical process fω = {fωn }∞
                                            n=0 has the same finite-dimensional distributions
as that of f = {f n }∞            0            n         0           n
                     n=0 , i.e. (fω , . . . , fω ) and (f , . . . , f ) have the same distribution for any
n ≥ 0.

          To prove that the agents’ extended type processes are essentially pairwise independent
in Lemma 3 below, we need the following elementary lemma, which is Lemma 5 in Duffie and
Sun (2012).

  40
       Here, (λ  P )f −1 is the distribution ν on X such that ν(B) = (λ  P )(f −1 (B)) for any Borel set B in X;
λfω−1   is defined similarly.




                                                         42
Lemma 2 Let φm be a random variable from (Ω, F, P ) to a finite space Am , for m = 1, 2, 3, 4.
If the random variables φ3 and φ4 are independent, and if, for all a1 ∈ A1 and a2 ∈ A2 ,

                        P (φ1 = a1 , φ2 = a2 | φ3 , φ4 ) = P (φ1 = a1 | φ3 )P (φ2 = a2 | φ4 ),                                            (32)

then the two pairs of random variables (φ1 , φ3 ) and (φ2 , φ4 ) are independent.

         The following lemma is useful for applying the exact law of large numbers for discrete
time processes in Theorem 2.16 of Sun (2006) (see Corollary 1) to our setting.

Lemma 3 Assume that the dynamical system D is Markov conditionally independent. Then,
the discrete time processes {βin }∞
                                  n=0 , i ∈ I, are essentially pairwise independent. In addition,
for each fixed n ≥ 1, the random variables β̄ n , i ∈ I (β̄¯n , i ∈ I) are also essentially pairwise
                                                                      i               i
independent.

Proof. Let E be the set of all (i, j) ∈ I × I such that Equations (23), (24) and (25) hold for
all n ≥ 1. Then, by grouping countably many null sets together, we obtain that for λ-almost
all i ∈ I, λ-almost all j ∈ I, (i, j) ∈ E, i.e., for λ-almost all i ∈ I, λ(Ei ) = λ({j ∈ I : (i, j) ∈
E}) = 1.
      We can use induction to prove that for any fixed (i, j) ∈ E, if (βi0 , . . . , βin ) and (βj0 , . . . , βjn )
are independent for n ≥ 0, then so are the pairs β̄in and β̄jn , β̄¯in and β̄¯jn for n ≥ 1. The case
of n = 0 is simply the assumption of initial independence in Subsection A.2. Suppose that it
is true for the case n − 1. That is, if (βi0 , . . . , βin−1 ) and (βj0 , . . . , βjn−1 ) are independent, then
so are the pairs β̄ n−1 and β̄ n−1 , β̄¯n−1 and β̄¯n−1 . Then, the Markov conditional independence
                           i              j         i         j
condition and Lemma 2 imply that (βi , . . . , βin−1 , β̄in ) and (βj0 , . . . , βjn−1 , β̄jn ) are independent,
                                                       0

so are the pairs (βi0 , . . . , βin−1 , β̄in , β̄¯in ) and (βj0 , . . . , βjn−1 , β̄jn , β̄¯jn ), and (βi0 , . . . , βin−1 , β̄in , β̄¯in , βin )
and (βj0 , . . . , βjn−1 , β̄jn , β̄¯jn , βjn ). Hence, the random vectors (βi0 , . . . , βin ) and (βj0 , . . . , βjn ) are
independent for all n ≥ 0, which means that {βin }∞                                         n ∞
                                                                         n=0 and {βj }n=0 are independent. It is also
clear that if, for each n ≥ 1, the random variables β̄in and β̄jn are independent, then so are β̄¯in
and β̄¯jn . The desired result follows.

         The following lemma shows how to compute the expected cross-sectional extended type
distributions E(p̂n ) and E(p̌n ).

Lemma 4 The following hold:

    1. For each n ≥ 1, E(p̂n ) = Γn (E(p̂n−1 )).

    2. For each n ≥ 1, the expected cross-sectional extended type distribution p̃n = E(p̌n ) imme-
       diately after random mutation at time n, satisfies E(p̌nkl ) = k1 ,l1 ∈S E(p̂kn−1   )bnk1 k bnl1 l and
                                                                     P
                                                                                      1 l1

       E(p̌nkJ ) = l∈S E(p̂n−1  n
                  P
                           lJ )blk .


                                                                      43
Proof. Fix any k, l ∈ S. Equations (13) and (14) imply respectively that for any k1 , l1 ∈ S,

         P β̄in = (k, J) | βin−1 = (k1 , l1 ) = 0, and P β̄in = (k, l) | βin−1 = (k1 , J) = 0.
                                                                                        
                                                                                                                                    (33)

The Fubini property will be used extensively in the computations below. We shall illustrate its
usage in Equation (34). It then follows from the Fubini property and Equations (13) and (33)
that
                            Z                                                              Z
              p̃nkl =             λ i ∈ I : β̄ωn (i) = (k, l) dP (ω) =                          P β̄in = (k, l) dλ(i)
                                                                                                              

                            ZΩ X                                                            I

                                                     β̄in       (k, l), βin−1
                                                                                              
                    =                        P              =                     = (k1 , l1 ) dλ(i)
                                I k ,l ∈S
                                   1 1
                            Z         X
                                             P β̄in = (k, l) | βin−1 = (k1 , l1 ) P βin−1 = (k1 , l1 ) dλ(i)
                                                                                                     
                    =
                                I k ,l ∈S
                                   1 1
                                X
                    =                    E(p̂n−1     n     n
                                             k1 l1 )bk1 k bl1 l .                                                                   (34)
                            k1 ,l1 ∈S

By Equations (14) and (33), we obtain that
               Z                         Z X
       n              n
                                             P β̄in = (k, J), βin−1 = (k1 , J) dλ(i)
                                                                             
     p̃kJ =       P β̄i = (k, J) dλ(i) =
                        I                                                I k ∈S
                                                                            1
                     Z X
                                      P β̄in = (k, J) | βin−1 = (k1 , J) P βin−1 = (k1 , J) dλ(i)
                                                                                          
                =
                        I k ∈S
                         1
                        XZ
                                      bnk1 k P βin−1 = (k1 , J) dλ(i)
                                                               
                =
                     k1 ∈S        I
                        X
                =                E(p̂n−1    n
                                     k1 J )bk1 k .                                                                                  (35)
                     k1 ∈S

        By Lemma 3, β̄ n is essentially pairwise independent process. The exact law of large
numbers in Lemma 1 implies that p̌n (ω) = E(p̌n ) = p̃n for P -almost all ω ∈ Ω. Combining
with Equations (17) and (18), we can obtain that for any l ∈ S,

       P (ḡ¯in = l | ᾱin = k, ḡin = J) = qkl
                                             n
                                                (p̃n ) , and P (ḡ¯in = J | ᾱin = k, ḡin = J) = ηkn (p̃n ) .                      (36)

It follows from Equations (20) and (21) that
                   Z                         Z                                                                            
                                                                                         P βin = (k, l), β̄¯in = (k1 , l1 ) dλ(i)
                                                                                  X
            n            n
        E(p̂kl ) =   P (βi = (k, l)) dλ(i) =
                             I                                               I k ,l ∈S
                                                                                1 1
                            Z                                                                    
                                            P βin = (k, l) | β̄¯in = (k1 , l1 ) P β̄¯in = (k1 , l1 ) dλ(i)
                                   X
                    =
                             I k ,l ∈S
                                1 1
                            Z                                                             
                                           (1 − θkn1 l1 )σkn1 l1 (k, l)P β̄¯in = (k1 , l1 ) dλ(i)
                                   X
                    =
                             I k ,l ∈S
                                1 1
                                                                         Z                       
                                                                              P β̄¯in = (k1 , l1 ) dλ(i).
                                X
                    =                   (1 − θkn1 l1 )σkn1 l1 (k, l)                                                                (37)
                            k1 ,l1 ∈S                                     I


                                                                             44
Equations (16) and (36) imply that
    Z                         Z X                                    
       P β̄¯in = (k, l) dλ(i) =     P β̄¯in = (k, l) | β̄in = (k1 , l1 ) P β̄in = (k1 , l1 ) dλ(i)
                                                                                            
        I                                               I k ,l ∈S
                                                           1 1
                    Z X                                           
                                 P β̄¯in = (k, l) | β̄in = (k1 , J) P β̄in = (k1 , J) dλ(i)
                                                                                     
                +
                      I k ∈S
                         1
            Z                               
             P β̄¯in = (k, l) | β̄in = (k, l) P β̄in = (k, l) dλ(i)
                                                             
      =
            I Z
                                               
             + P β̄¯in = (k, l) | β̄in = (k, J) P β̄in = (k, J) dλ(i)
                                                                  
                      I
      = p̃nkl + qkl
                 n
                    (p̃n ) p̃nkJ .                                                                                                      (38)

By substituting Equation (38) into Equation (37), we can express E(p̂nkl ) in terms of E(p̌n ) as
               X                                          X
   E(p̂nkl ) =    p̃nk1 l1 (1 − θkn1 l1 )σkn1 l1 (k, l) +   p̃nk1 J qkn1 l1 (p̃n ) (1 − θkn1 l1 )σkn1 l1 (k, l). (39)
                     k1 ,l1 ∈S                                                  k1 ,l1 ∈S

Similarly, Equations (20) and (22) imply the second and third identities while Equations (36)
and (38) imply the last identity in the following equation:
                 Z
      E(p̂kJ ) = P (βin = (k, J)) dλ(i)
          n

        Z         I                              Z X
                                                                                         
                 n         ¯n
      = P βi = (k, J), β̄i = (k, J) dλ(i) +               P βin = (k, J), β̄¯in = (k1 , l1 ) dλ(i)
                I                                                                   I k ,l ∈S
                                                                                       1 1
            Z                              Z                                                                  
                    P β̄¯in = (k, J) dλ(i) +                              θkn1 l1 ςkn1 l1 (k)P β̄¯in = (k1 , l1 ) dλ(i)
                                                                  X
       =
                I                                             I k ,l ∈S
                                                                 1 1
            Z                                    
               P β̄¯in = (k, J) | β̄in = (k, J) P β̄in = (k, J) dλ(i)
                                                                     
       =
             I                           Z                   
                      θkn1 l1 ςkn1 l1 (k) P β̄¯in = (k1 , l1 ) dλ(i)
                X
            +
                k1 ,l1 ∈S                          I
                                     X                                            X
       = p̃nkJ ηkn (p̃n ) +                    p̃nk1 l1 θkn1 l1 ςkn1 l1 (k) +               p̃nk1 J qkn1 l1 (p̃n )θkn1 l1 ςkn1 l1 (k)   (40)
                                   k1 ,l1 ∈S                                    k1 ,l1 ∈S

By combining Equations (34), (35),(39) and (40), we obtain that E(p̂n ) = Γn (E(p̂n−1 )).

       The following lemma shows the Markov property of the agents’ extended type processes.

Lemma 5 Suppose the dynamical system D is Markov conditional independent. Then, for
λ-almost all i ∈ I, the extended type process for agent i, {βin }∞
                                                                 n=0 , is a Markov chain with
transition matrix z n at time n − 1.

Proof. Fix n ≥ 1; by summing over all the (k2 , l2 ) ∈ Ŝ in Equation (23), we obtain that for
λ-almost all i ∈ I,
                                                         n−1
                             P β̄in = (k1 , l1 ) | (βit )t=0   = P β̄in = (k1 , l1 ) | βin−1 .
                                                                                           
                                                                                                                                        (41)

                                                                            45
By grouping countably many null sets together, we know that for λ-almost all i ∈ I, Equation
(41) holds for all n ≥ 1.
      Similarly, Equations (24) and (25) imply that for λ-almost all i ∈ I,
                                                                                                                          
                          P β̄¯in = (k1 , l1 ) | β̄in , (βit )t=0
                                                              n−1
                                                                    =P                           β̄¯in = (k1 , l1 ) | β̄in
                                                                                                                          
                          P βin = (k1 , l1 ) | β̄¯in , (βit )t=0
                                                              n−1
                                                                    =P                           βin = (k1 , l1 ) | β̄¯in

hold for all n ≥ 1. A simple computation shows that for λ-almost all i ∈ I,

                     P (βin = (k1 , l1 ) | βi0 , . . . , βin−1 ) = P (βin = (k1 , l1 ) | βin−1 )

for all a1 ∈ S, r1 ∈ S ∪ {J} and n ≥ 1. Hence, for λ-almost all i ∈ I, agent i’s extended type
process {βin }∞
              n=0 is a Markov chain.
      By combining Equations (34), (35) and (39), we can obtain that
                                           X
                  E(p̂nkl ) =                          bnk0 k1 qkn1 l1 (p̃n )(1 − θkn1 l1 )σkn1 l1 (k, l)E(p̂n−1
                                                                                                             k0 J )
                                       k1 ,l1 ,k0 ∈S
                                                 X
                                       +                       bnk0 k1 bnl0 l1 (1 − θkn1 l1 )σkn1 l1 (k, l)E(p̂n−1
                                                                                                               k0 l0 ).
                                           k1 ,l1 ,k0 ,l0 ∈S

                                    n                  n
Since the transition probabilities z(k 0 l0 )(kl) and z(k 0 J)(kl) from time n − 1 to time n are the

respective coefficients of E(p̂n−1             n−1                   0 0
                               k0 l0 ) and E(p̂k0 J ) for any k, l, k , l ∈ S, we can obtain that
                                                   X
                         n
                        z(k 0 l0 )(kl) =                       bnk0 k1 bnl0 l1 (1 − θkn1 l1 )σkn1 l1 (k, l)
                                                 k1 ,l1 ∈S
                                                   X
                         n
                        z(k 0 J)(kl)       =                   bnk0 k1 qkn1 l1 (p̃n )(1 − θkn1 l1 )σkn1 l1 (k, l),
                                                 k1 ,l1 ∈S

which follow the corresponding formulas in Equation (26). Similarly, by combining Equations
(34), (35) and (40), we can obtain that
                           X                                                  X
          E(p̂nkJ ) =             bnk0 k ηkn (p̃n )E(p̂kn−1
                                                         0J ) +                              bnk0 k1 bnl0 l1 θkn1 l1 ςkn1 l1 (k)E(p̂kn−1
                                                                                                                                      0 l0 )

                          k0 ∈S                                          k1 ,l1 ,k0 ,l0 ∈S
                                  X
                          +                    bnk0 k1 qkn1 l1 (p̃n )θkn1 l1 ςkn1 l1 (k)E(p̂kn−1
                                                                                              0 J ).

                              k1 ,l1 ,k0 ∈S

                                    n                  n
Since the transition probabilities z(k 0 l0 )(kJ) and z(k 0 J)(kJ) from time n − 1 to time n are the

respective coefficients of E(p̂n−1             n−1                0 0
                               k0 l0 ) and E(p̂k0 J ) for any k, k , l ∈ S, we can obtain that
                                                X
                    n
                   z(k 0 l0 )(kJ) =                      bnk0 k1 bnl0 l1 θkn1 l1 ςkn1 l1 (k)
                                             k1 ,l1 ∈S
                                                                        X
                    n
                   z(k 0 J)(kJ)        = bnk0 k ηkn (p̃n ) +                     bnk0 k1 qkn1 l1 (p̃n )θkn1 l1 ςkn1 l1 (k),
                                                                     k1 ,l1 ∈S


                                                                       46
   which follow the corresponding formulas in Equation (26).

           Now, for each n ≥ 1, we view each β n as a random variable on I × Ω. Thus {β n }∞
                                                                                           n=0 is
   a discrete-time stochastic process.

   Lemma 6 Assume that the dynamical system D is Markov conditionally independent. Then,
   {β n }∞                                                  n
         n=0 is also a Markov chain with transition matrix z at time n − 1.


   Proof. We can compute the transition matrix of {β n }∞
                                                        n=0 at time n − 1 by using Lemma 5
   and the Fubini property. Fix any k1 , k2 ∈ S and any l1 , l2 ∈ S ∪ {J}. We have

          (λ  P )(β n = (k2 , l2 ), β n−1 = (k1 , l1 ))
                          Z
                      =       P (βin = (k2 , l2 ) | β n−1 = (k1 , l1 ))P (βin−1 = (k1 , l1 )) dλ(i)
                          ZI
                                n
                      =       z(k  1 l1 )(k2 l2 )
                                                  P (β n−1 = (k1 , l1 )) dλ(i)
                                 I
                             n
                          = z(k1 l1 )(k2 l2 )
                                              · (λ  P )(β n−1 = (k1 , l1 )),                                                   (42)

   which implies that (λ  P )(β n = (k2 , l2 ) | β n−1 = (k1 , l1 )) = z(k
                                                                         n
                                                                           1 l1 )(k2 l2 )
                                                                                          .
           Next, for any n ≥ 1, and for any (a0 , . . . , an−2 ) ∈ (S × (S ∪ {J}))n−1 , we have

(λ  P ) (β 0 , . . . , β n−2 ) = (a0 , . . . , an−2 ), β n−1 = (k1 , l1 ), β n = (k2 , l2 )
                                                                                            
         Z
            P (βi0 , . . . , βin−2 ) = (a0 , . . . , an−2 ), βin−1 = (k1 , l1 ), βin = (k2 , l2 ) dλ(i)
                                                                                                 
     =
         ZI
            P βin = (k2 , l2 ) | βin−1 = (k1 , l1 ) P (βi0 , . . . , βin−2 ) = (a0 , . . . , an−2 ), βin−1 = (k1 , l1 ) dλ(i)
                                                                                                                      
     =
             I
         n
      = z(k 1 l1 )(k2 l2 )
                           · (λ  P )((β 0 , . . . , β n−2 ) = (a0 , . . . , an−2 ), β n−1 = (k1 , l1 )),                              (43)

   which implies that

         (λ  P )(β n = (k2 , l2 ) | (β 0 , . . . , β n−2 ) = (a0 , . . . , an−2 ), β n−1 = (k1 , l1 )) = z(k
                                                                                                           n
                                                                                                             1 l1 )(k2 l2 )
                                                                                                                            .

   Hence the discrete-time process {β n }∞
                                         n=0 is indeed a Markov chain with transition matrix z
                                                                                               n

   at time n − 1.

   Proof of Theorem 4: Properties (1), (2), and (3) of the theorem are shown in Lemmas 4, 5,
   and 3 respectively.
           By the exact law of large numbers for discrete time processes in Corollary 1, we know
   that for P -almost all ω ∈ Ω, (βω0 , . . . , βωn ) and (β 0 , . . . , β n ) (viewed as random vectors) have
   the same distribution for all n ≥ 1. Since, as noted in Lemma 6, {β n }∞
                                                                          n=0 is a Markov chain
   with transition matrix z n at time n − 1, so is {βωn }∞
                                                         n=0 for P -almost all ω ∈ Ω. Thus property
   (4) is shown.

                                                                 47
      Since the processes β̄ n and β n are essentially pairwise independent as shown in Lemma
3, the exact law of large numbers in Lemma 1 implies that at time period n, for P -almost
all ω ∈ Ω, the realized cross-sectional extended type distribution after the random mutation,
p̌n (ω) = λ(β̄ωn )−1 is the expected cross-sectional extended type distribution E(p̌n ), and the
realized cross-sectional extended type distribution at the end of period n, p̂n (ω) = λ(βωn )−1 is
the expected cross-sectional extended type distribution E(p̂n ). Thus, property (5) is shown.
                                     ˆ such that P β 0 −1 = p̈0 holds for λ-almost every i ∈ I.
      Assume that there exists p̈0 ∈ ∆
                                                       
                                                           i
The exact law of large numbers in Lemma 1 implies that p̈0 = E(p̂0 ). For λ-almost all i ∈ I,
since the transition matrix of {βin }∞        n ∞                         n ∞
                                     n=1 is {z }n=1 , the Markov chains {βi }n=0 induce the same
distribution on Ŝ ∞ as ξ. For P -almost all ω ∈ Ω, the Markov chains {βωn }∞
                                                                            n=0 induce the same
distribution on Ŝ ∞ as ξ. Thus, property (6) is shown.

Proof of Proposition 4: Given that the parameters (b, q, θ, σ, ς) are time independent, the
                ˆ to ∆
mapping Γn from ∆    ˆ in Subsection A.3 is time independent, and will simply be denoted by
Γ. By the continuity assumption in the sentence above Equation (9), p̂kJ qkl      n ( p̂ ) is continuous

        ˆ for any k, l ∈ S. For any k1 , l1 ∈ S, since p̃k J = P
in p̂ ∈ ∆                                                                 n                           ˆ
                                                          1     r∈S p̂rJ brk1 is continuous in p̂ ∈ ∆,
we can also obtain that p̃ q n (p̃) is continuous in p̂ ∈ ∆.     ˆ Therefore, Γ is a continuous
                             k1 J k1 l1
              ˆ to itself. By Brower’s Fixed Point Theorem, Γ has a fixed point p̂∗ . In this
function from ∆
case, E(p̂n ) = p̂∗ , z n = z 1 for all n ≥ 1. Hence the Markov chains {βin }∞
                                                                             n=0 for λ-almost all
i ∈ I, {β n }∞       n ∞
             n=0 , {βω }n=0 for P -almost all ω ∈ Ω are time-homogeneous.
      If the initial extended type process β 0 is i.i.d., then the extended type distribution of
                               −1
agent i at time n = 0 is P βi0     = p̂∗ for λ-almost every i ∈ I. By (6) of Theorem 4, for any
n ≥ 1, βin induce the same distribution on Ŝ for λ-almost all i ∈ I. Therefore, for any n ≥ 1,
P (βin )−1 = p̂∗ for λ-almost all i ∈ I.

D    A Brief Introduction to Nonstandard Analysis

In order to summarize background knowledge for the proofs of Theorems 1 and 5, this section
presents basic nonstandard analysis by adopting some material from Loeb and Wolff (2015) and
several other related results. First, a simple construction of the nonstandard number system
that extends the usual ordered field of real numbers is given in Subsection D.1. A more general
framework of nonstandard analysis is then presented in Subsection D.2. The key constructions
of Loeb measure spaces and Loeb transition probabilities are introduced in Subsections D.3 and
D.4 respectively. The crucial relevant result is the so-called Fubini property for Loeb product
and transition probabilities. In the final part of this section, we discuss some motivation for




                                                  48
using hyperfinite agent spaces in settings like those for random matching.41

D.1    Non-standard number system

First, we extend the ordered field of real numbers R to an ordered field ∗ R that contains
infinitesimals. To this end, we introduce the concept of a free ultrafilter.

Definition 4 A free ultrafilter on the set N of positive integers is a collection U ⊆ P(N) =
{A : A ⊆ N} such that

   1. ∅ ∈
        / U.

   2. A ∈ U and B ∈ U =⇒ A ∩ B ∈ U.

   3. A ⊆ N and A ∈
                  / U =⇒ N\A ∈ U.

   4. A is a finite subset of N =⇒ N\A ∈ U.

       Fix a free ultrafilter U. One can define a set function ι on the power set P(N) of N such
that ι(A) = 1 if A ∈ U, and ι(A) = 0 if A ∈
                                          / U. It is easy check that ι is a finitely additive
probability measure on P(N). If a property holds on some set A ∈ U, then the property holds
with ι-probability one; we can simply say that the property holds almost everywhere, expressed
for brevity as “a.e.”.
       Two sequences hri i and hsi i of real numbers are said to be equivalent if ri = si a.e.,
which means {i ∈ N : ri = si } ∈ U. We write [hri i] for the equivalence class containing the
sequence hri i, and we use ∗ R to denote the collection of such equivalence classes. The set ∗ R is
called the set of nonstandard real numbers, or the “hyperreal” numbers. Such a construction
using an ultrafilter is called an ultrapower construction.42 We note that the set R of real
numbers is embedded in the set of nonstandard real numbers ∗ R via the map c → [hci], where
hci is the constant sequence with term c ∈ R. We write ∗ c for [hci], but later drop the star for
convenience. In contrast to hyperreal numbers in ∗ R, the numbers in R are also called standard
real numbers.
       The summation and multiplication operations +,  and the absolute value function | · |
together with the “less than” order relation < for ∗ R are defined as follows.

Definition 5 Given real sequences hri i and hsi i, we set
  41
     For a comprehensive treatment of nonstandard analysis, see Chapters 1, 2, 3 and 6 in Loeb and Wolff (2015).
For the details on Loeb transition probabilities and the associated Fubini property, see Section 5 of Duffie and
Sun (2007).
  42
     Though the set ∗ R of nonstandard real numbers depends on the underlying ultrafilter, the particular choice
of such an ultrafilter is not an issue. When we consider applications of nonstandard analysis, what we use
are some general properties of nonstandard models, such as the Transfer Principle in Proposition 7 and the
Countable Saturation Principle in Proposition 8 below.


                                                      49
   1. [hri i] + [hsi i] = [hri + si i].

   2. [hri i]  [hsi i] = [hri  si i].

   3. |[hri i]| = [h|ri |i].

   4. [hri i] < [hsi i] if ri < si a.e.

       It is easy to check that the operations + and  , as well as | · | and the ordering <, are
independent of the choices of the representing sequences. The structure (∗ R, +, · , <) forms an
ordered field that extends the ordered field (R, +, · , <).
       For any r ∈ ∗ R, r is infinite (or unlimited) if |r| > n for every standard positive integer
                                                                                                           1
n ∈ N; r is finite (or limited) if |r| < n for some n ∈ N; and r is infinitesimal if |r| <                 n   for
every n ∈ N. Recall that for r = [hri i] ∈            ∗R   and c ∈ R, |r| < c (|r| > c) means that |ri | < c
(|ri | > c) holds a.e.
       For x, y ∈ ∗ R, we say that x and y are infinitesimally close or infinitely close if x − y is
infinitesimal and in that case we write x ' y. The equivalence class for ' containing x is called
the monad of x, written as monad(x). That is, monad(x) = {y ∈ ∗ R : y ' x}.
       If ρ ∈ ∗ R is finite, then the unique real number c with ρ ' c is called the standard part
of ρ. We write c = st(ρ) or c = ◦ ρ.
       Let ∗ N = {[hri i] : ri ∈ N a.e.} ⊆ ∗ R be the set of hyperfinite integers, and ∗ N∞ the set
of unlimited hyperfinite integers. We have the following proposition.

Proposition 6 For any unlimited hyperfinite integer r = [hri i] ∈ ∗ N∞ , the set {r0 ∈ ∗ N : r0 ≤
r}, which is also denoted by {1, 2, . . . , r}, has the cardinality of the continuum.

Proof: Let R∞ be the set of sequences of standard real numbers. Since R∞ has the cardinality
of the continuum, the cardinality of ∗ N is therefore at most the cardinality of the continuum,
which implies that the cardinality of A = {1, 2, . . . , r} is also at most the cardinality of the
continuum.
       Let B = { 1r , 2r , . . . , rr }. It is clear that A and B have the same cardinality. For any
standard real number c ∈ [0, 1], let r0 = bc  rc = [hhi i] ∈ ∗ R, where hi is the integer part of the
standard real number c  ri . From now on, we may drop the multiplication symbol  when there
                                                                                                   r0
is no confusion. It is clear that r0 ∈ ∗ N and cr − 1 < r0 ≤ cr, which implies that                r    ∈ B and
           r0                                                   r0
c − 1r <   r    ≤ c. Note that    1
                                  r   is infinitesimal, and st( r ) = c. Therefore, st( · ) is a surjection from
B to [0, 1], which implies that the cardinality of B is at least the cardinality of the continuum.
Hence, the cardinality of A is also at least the cardinality of the continuum. Combining with
the conclusion of the above paragraph, we know that A has the cardinality of the continuum.



                                                           50
D.2     General framework of nonstandard analysis

To develop the general framework of nonstandard analysis, we need to work with the concept
of superstructure. Fix a set X containing R. Let V0 (X) = X, and for each positive integer
n ∈ N, let Vn (X) = Vn−1 (X) ∪ P(Vn−1 (X)), where P (Vn−1 (X)) is the power set of Vn−1 (X).
The superstructure over X is the set V (X) = ∪∞
                                              n=0 Vn (X). Entities in X are said to be of rank
0, and for n ≥ 1, entities in Vn (X)\Vn−1 (X) are said to be of rank n.
       For a, b ∈ Vn (X), one can define an ordered pair (a, b) as the set {{a}, {a, b}}, which is
an element in Vn+2 (X). With the definition of ordered pairs, one can then define the Cartesian
product of two sets in V (X), as well as relations and functions in V (X). For k ≥ 3, one can
define ordered k-tuples (a1 , a2 , . . . , ak ) as {(1, a1 ), (2, a2 ), . . . , (k, ak )}. The k-tuple versions of
Cartesian products, relations and functions in V (X) can be similarly defined. In fact, the
superstructure can be used to cover basically all of the relevant mathematical structures that
are useful for applications.
       We now describe the construction of formal statements, or “formulas,” in a formal lan-
guage LX about the superstructure V (X). Given X, the language LX for the superstructure
V (X) over X has the following symbols:

   1. Connectives: q, ∨, ∧, →, ↔.

   2. Quantifiers: ∀, ∃.

   3. Parentheses: [ ], ( ), h i.

   4. Constant Symbols: At least one name for each entity in V (X).

   5. Variable Symbols: A fixed collection of symbols representing variables.

   6. Equality Symbol: Denotes equality for elements of X, and set equality otherwise.

   7. Set membership: ∈.

       The above symbols serve as the “alphabet” of the language LX . A fixed set of variable
symbols together with other symbols in LX will lead to a well-defined collection of formal
syntactical statements.

Definition 6 A formula of LX is built up inductively with the following rules:

 (a) If x1 , · · · , xn , x, and y are either constants or variables, then the following are called
      atomic formulas: x ∈ y, x = y; (x1 , · · · , xn ) ∈ y; (x1 , · · · , xn ) = y; ((x1 , · · · , xn ) , x) ∈ y;
      ((x1 , · · · , xn ) , x) = y.


                                                       51
 (b) If Φ and Θ are formulas, so are (¬Φ), (Φ ∧ Θ), (Φ → Θ), (Φ ∨ Θ), and (Φ ↔ Θ).

 (c) If x is a variable symbol and y is either a variable symbol or a constant symbol and Φ is
         a formula, then (∀x ∈ y)Φ and (∃x ∈ y)Φ are formulas.

          The logical connectives q, ∨, ∧, →, ↔ have the usual meanings in terms of the satisfia-
bility of formulas connected by them. For example, (¬Φ) means that Φ is not satisfied while
∨, ∧ mean “or”, “and” respectively. For the formulas (∀x ∈ y)Φ and (∃x ∈ y)Φ, the scope of
the quantifies ∀, ∃ is Φ. One can define the scope of a quantifier within a formula inductively.

Definition 7 A variable x is free in a formula Φ if it is not within the scope of any quantifier
for x. A closed formula in LX is a formula without free variables.

          Fix a free ultrafilter U. Given hai i and hbi i, both in the space X ∞ of sequences in X, are
said to be equivalent if ai = bi a.e. For any c ∈ X, let ∗ c = [hc, c, . . .i] be the equivalence class
of sequences in X ∞ that contains the constant sequence hc, c, . . .i. For any sequence {Ai }∞
                                                                                             i=1 of
sets in Vn (X)\X for some n ≥ 1, define the set [hAi i] = {[hxi i] : xi ∈ Ai a.e.}. For A ∈ V (X),
let ∗ A = [hA, A, A, . . .i]. In particular, ∗ X is the set of equivalent classes of sequences in X ∞ .

Definition 8 If Φ is a formula in LX , the ∗-transform of Φ, denoted ∗ Φ, is the formula in
L∗ X that is obtained by replacing each constant c in Φ with ∗ c.

          The following result is a basic tool in nonstandard analysis.43

Proposition 7 (Transfer Principle) If Φ is a closed formula in LX that is true for V (X),
then ∗ Φ is true for V (∗ X).

          All entities in V (X) and entities in V (∗ X) of the form ∗ b, for some b ∈ V (X), are called
standard. An entity a in V (∗ X) is called internal if for some set b ∈ V (X), a ∈ ∗ b. All other
entities in V (∗ X) are called external. For any internal set A in V (∗ X), one can always find
a sequence {Ai }∞
                i=1 of sets in Vn (X)\X for some n ≥ 1 such that A is the set of equivalence
classes {[hai i] : ai ∈ Ai a.e.}. If any kind of internal operations are applied to internal sets,
one still obtains internal sets; see, for example, Theorem 2.8.4 in Loeb and Wolff (2015). In
particular, if A and B are internal, then so are A ∪ B, A ∩ B, A\B, and A × B. An internal
function is a function whose graph is internal.
          For B ∈ V (X)\X, let PF (B) denote the finite subsets of B. An element A ∈ ∗ PF (B)
will be called a hyperfinite set. In particular, A is the set of equivalence classes {[hbi i] : bi ∈
  43
       For a detailed proof, readers are referred to Sections 2.2-2.5 of Loeb and Wolff (2015).




                                                          52
Bi a.e.} for some sequence hBi i of finite subsets of B. The internal cardinality of A is simply
the hyperinteger [h|Bi |i], where |Bi | is the cardinality of the finite set Bi .
      The following is an important uniformity principle that transforms a local property
expressed by finite intersections to a global property described by the intersection of all the
sets in the sequence. The proof is taken from page 199 of Khan and Sun (1997).

Proposition 8 (Countable Saturation Principle) For a sequence of nonempty internal
sets, A1 ⊇ A2 ⊇ · · · ⊇ An ⊇ . . . , we have ∩∞
                                              n∈N An 6= ∅.


Proof. For any n ∈ N, since An is internal, there exists Bn ∈ V (X) \ X such that An ∈ ∗ Bn =
[hBn , Bn , . . .i]. Then there exists a sequence {Ani } of sets such that Ani ∈ Bn for any i ∈ N
and An = [hAn1 , An2 , . . .i]. Let In = {i ≥ n : A1i ⊇ · · · ⊇ Ani 6= ∅}. Then for all n ∈ N, In ∈ U,
In ⊇ In+1 , and ∩n∈N In = ∅. This implies that for any i ∈ I1 , n(i) = max{n ∈ N : i ∈ In } is well
defined. For i ∈ I1 , since i ∈ In(i) , we know that An(i)i 6= ∅. Pick bi from An(i)i , and note that
i ∈ In implies that n(i) ≥ n, and hence bi ∈ An(i)i ⊆ Ani . Thus {i ∈ I1 : bi ∈ Ani } ⊇ In ∈ U.
By defining bi to be some point in A1 if i is not in I1 , we obtain that [hbi i] ∈ An . Since n is
arbitrary, the proof is finished.

D.3    Construction of hyperfinite Loeb spaces

Fix any unlimited hyperfinite integer M . Let Λ = {1, 2, . . . , M }, and C be the internal power
set of all the internal subsets of Λ. Let w : Λ → ∗ R+ be an internal function such that
P
   i∈Λ w(i) = 1.
       Define an internal finitely-additive measure from C to ∗ [0, 1] such that µ(A) = i∈A w(i)
                                                                                       P

                                                                                                   1
for any A ∈ C. Then (Λ, C, µ) is called a hyperfinite internal probability space. If w(i) ≡        M
for all i ∈ Λ, then (Λ, C, µ) is called a hyperfinite counting probability space.
      We let st(µ) be the function from C into R+ defined by st(µ)(A) = st(µ(A)) for any A ∈ C.
It is clear that st(µ) is a finitely-additive measure on the algebra C. The important point is
that st(µ) is a countably-additive measure on the algebra C. To see this, consider a sequence
A1 ⊇ A2 ⊇ . . . of internal sets in C such that ∩n∈N An = ∅. The countable saturation principle
implies the existence of m ∈ N such that Am = ∅. It is thus clear that limn→∞ st(µ)(An ) = 0.
By the well-known Caratheodory’s extension theorem (see, for example, Loeb (2016, p. 181)),
st(µ) can be extended to a measure µL on the σ-algebra σ(C) that is generated by C. By
including all µL -null subsets, we obtain a standard complete probability space (Λ, Lµ (C), µL ),
which is called a Loeb measure space.




                                                  53
D.4     Transition probabilities

Let (I, I0 , λ0 ) be a hyperfinite internal probability space for which I0 is the internal power set
on some hyperfinite set I. Let Ω be a hyperfinite internal set with F0 its internal power set.
Let P0 be an internal function from I to the space of hyperfinite internal probability measures
on (Ω, F0 ), which is called an internal transition probability. For i ∈ I, denote the hyperfinite
internal probability measure P0 (i) on (Ω, F0 ) by P0i .
       It is clear that the Cartesian product I ×Ω is a hyperfinite set. Let I0 ⊗F0 be the internal
power set on I × Ω. Define a hyperfinite internal probability measure τ0 on (I × Ω, I0 ⊗ F0 )
by letting τ0 ({(i, ω)}) = λ0 ({i})P0i ({ω}) for (i, ω) ∈ I × Ω. The measure τ0 will be called
the product transition probability of the measure λ0 and the transition probability P0 . Let
(I, I, λ), (Ω, Fi , Pi ), and (I × Ω, I  F, τ ) be the Loeb spaces corresponding respectively to
(I, I0 , λ0 ), (Ω, F0 , P0i ), and (I × Ω, I0 ⊗ F0 , τ0 ). The collection {Pi : i ∈ I} of Loeb measures
will be called a Loeb transition probability, and denoted by P . The measure τ will be called
the Loeb product transition probability of the measure λ and the Loeb transition probability
P . We shall also denote τ0 by λ0 ⊗ P0 and τ by λ  P .
       The following result presents a generalized Fubini theorem for a Loeb transition proba-
bility, which is proved in Section 5 of Duffie and Sun (2007).44

Proposition 9 Let f be a real-valued integrable function on (I × Ω, σ(I0 ⊗ F0 ), τ ). Then,
(1) fi = f (i, ·) is σ(F0 )-measurable for each i ∈ I and integrable on (Ω, σ(F0 ), Pi ) for λ-
                         R                                                 R R
almost all i ∈ I; (2) Ω fi (ω) dPi (ω) is integrable on (I, σ(I0 ), λ); (3) I Ω fi (ω) dPi (ω) dλ(i) =
R
 I×Ω f (i, ω) dτ (i, ω).


       If P0i does not depend on i, then τ = λ  P is called the Loeb product measure. The
corresponding measure space (I ×Ω, I F, λP ) is called the Loeb product space. In this case,
the symmetric position of the two probability spaces respectively on I and Ω implies that the
properties as stated in Proposition 9 also hold when the iterated integral is taken in different
order. It is clear that I  F contains the usual product σ-algebra σ(I0 ) ⊗ σ(F0 ). Thus, the
Loeb product space (I × Ω, I  F, λ  P ) is a Fubini extension.45 Such a result in the special
case was shown by Keisler (1977); see also Loeb and Wolff (2015, p. 214).
  44
     For simplicity, we only state the result in terms of the σ-algebra σ(I0 ⊗ F0 ). One can also re-state the result
to the case when the underlying measure space is the completion of (I × Ω, σ(I0 ⊗ F0 ), τ ).
  45
     Assume that both λ and P are atomless. Proposition 8.4.5 in Loeb and Wolff (2015, Chapter 8) (by the
third author of this paper) indicates that I  F is a strict extension of σ(I0 ) ⊗ σ(F0 ). As noted in Proposition
8.4.1 of the same chapter, one can also construct I  F-measurable processes which are essentially pairwise
independent with any given variety of distributions.




                                                         54
D.5     Why hyperfinite agent spaces work

As noted in Subsection D.2, a hyperfinite set can be viewed as an equivalence class of a sequence
of finite sets. The transfer principle also indicates that hyperfinite sets preserve properties of
finite sets. Thus, an idealized model based on a hyperfinite agent space captures the asymptotic
nature of the large finite phenomenon being modeled.46 Khan and Sun (1997) call this property
“asymptotic implementability.”
        A typical hyperfinite set is of the form {1, 2, . . . , r} for some unlimited hyperinteger r.
It is equivalent to work with T = { 1r , 2r , . . . , rr }. Since the standard parts of elements in T are
the real numbers in [0, 1], a limit model based on T may be reduced to a model based on [0, 1],
provided that the relevant mappings on T have the essential continuity property in the sense
that for almost all t ∈ T , those points infinitely close to t will have infinitely close values in
the relevant target spaces.47 In the case of independent random matching, as considered in
this paper, the random types across the agent space are always discontinuous because of the
cross-agent independence assumption. Hence, it is not possible to study independent random
matching via the classical Lebesgue unit interval.


E      Proofs of the Existence Results

The main existence results in this paper are Theorems 1 and 5, which are proved in Subsections
E.1 and E.2 respectively. Subsection E.3 presents the proofs of Propositions 2 and 5.

E.1     Proof of Theorem 1

In the proof of Theorem 2.6 in Duffie and Sun (2007) for the existence of independent static
random partial matching, after one chooses the unmatched agents randomly, the measure on the
space of all the matchings on the set of matched agents is the hyperfinite counting probability,
which treats the matched agents symmetrically regardless of their types. In this paper, since
the matching probabilities depend on the types of both the agents and their partners, such a
symmetric treatment of agents is not possible. The idea underlying the proof of Theorem 1
  46
      Consider a simple example that transfers the classical law of large numbers to the hyperfinite setting. A
hyperfinite sequence {Xi }n  i=1 (n ∈ N∞ ) of internal random variables from an internal probability space (Ω, F0 , P0 )
    ∗
to
Qn    [−c, c] with a positive standard real number c is said to be ∗-independent if P0 (X1 ≤ a1 , . . . , Xn ≤ an ) =
                                                          n
   i=1 P0 (Xi ≤ ai ) holds for any internal sequence {ai }i=1 of hyperreal numbers. Let (I, I0 , λ0 ) be the hyperfinite
counting probability space on {1, . . . , n}. Suppose that Xi , 1 ≤i ≤ n are ∗-independentwith a common mean
                                                                                                        2
m and variance σ 2 . The Chebyshev Inequality says that P0 | X1 +···+X  n
                                                                             n          1
                                                                               − m| ≥ n1/3    ≤ nσ1/3 . Note that
               2
   1
        and nσ1/3 are infinitesimals. Thus, for P -almost all ω ∈ Ω, I Xω (i)dλ0 = X1 (ω)+···+Xn (ω)
                                                                    R
                                                                                                     ' m, that is,
Rn1/3
    ◦             ◦
                                                                                           n

  I
      Xω (i)dλ = m, where λ and P are the corresponding Loeb measures. A similar equality holds for any set in
I0 in place of I.
    47
       See Loeb and Wolff (2015, p. 190) on the characterization of Lebesgue measurability on a hyperfinite Loeb
counting probability space.


                                                          55
is as follows. The set of type-k agents to be matched with type-l agents (denoted by Akl ) is
chosen randomly according to the matching probabilities. One needs to make sure that (1) for
k < l, Akl and Alk must have exactly the same internal cardinality so that the agents between
them can be matched, and (2) the internal cardinality of Akk must be an even hyperinteger
so that agents in Akk can be matched to each other. The key point is to guarantee that the
construction leads an independent directed random matching with given parameters.
      In dynamic directed random matching with enduring partnership as considered in this
paper, only the unmatched agents will conduct directed random searches for counterparties
while those existing paired agents will not participate in the search process. Thus, in order to
handle the matching step for the inductive definition of dynamic directed random matching,
Lemma 7 below allows the existence of both pre-matched agents and unmatched agents so that
pre-matched agents remain matched to the same partners and unmatched agents may search
for counterparties. In contrast, Theorem 2.6 in Duffie and Sun (2007) considers only the case
in which all the agents are unmatched.
      Let I = {1, . . . , M̂ } be a hyperfinite set with M̂ an unlimited hyperfinite integer in ∗ N∞ ,
I0 the internal power set on I, and λ0 the hyperfinite counting probability measure on I0 with
λ0 (A) = |A|/|I| for any A ∈ I0 , where |A| is the internal cardinality of |A|. The corresponding
Loeb counting probability space (I, I, λ) is our space of agents. Following Definition 3, an
internal partial matching ψ from I to I is an internal mapping from I to I such that ψ(ψ(i)) = i
for any i ∈ I. When ψ(i) 6= i (ψ(i) = i), agent i is matched with agent ψ(i) (agent i is not
matched). When ψ(i) 6= i for each i ∈ I, ψ is said to be an internal full matching on I. For a
given hyperfinite internal probability space (Ω, F0 , P0 ), an internal random (partial) matching
π is an internal mapping from I × Ω to I such that πω is an internal partial matching for each
ω ∈ Ω.
      The following lemma will be used to prove both Theorems 1 and 5.

Lemma 7 As above, let (I, I0 , λ0 ) be the hyperfinite counting probability space with its Loeb
space (I, I, λ). Then, there exists a hyperfinite internal set Ω with its internal power set F0 such
that for any initial internal type function α0 from I to S and initial internal partial matching
π 0 from I to I with                     (
                                  0       α0 (π 0 (i))     if π 0 (i) 6= i
                                 g (i) =
                                          J                if π 0 (i) = i,
                                                            −1
and internal extended type distribution ρ̂ = λ0    α0 , g 0     , and for any internal matching proba-
bility function q from S × S to ∗ R+ with r∈S qkr ≤ 1 and ρ̂kJ qkl ' ρ̂lJ qlk (i.e., ρ̂kJ qkl − ρ̂lJ qlk
                                           P

is an infinitesimal) for any k, l ∈ S, there exists an internal random matching π from I × Ω to
I and an internal probability measure P0 on (Ω, F0 ) with the following properties.


                                                  56
  (i) Let H = {i : π 0 (i) 6= i}. Then P0 {ω ∈ Ω : πω (i) = π 0 (i) for any i ∈ H} = 1.
                                                                                  


 (ii) Let g be the internal mapping from I × Ω to S ∪ {J}, defined by
                                         (
                                          α0 (π(i, ω)) if π(i, ω) 6= i
                               g(i, ω) =
                                          J            if π(i, ω) = i,

      for any (i, ω) ∈ I × Ω. Then, for any k, l ∈ S, P0 (gi = l) ' qkl for λ-almost every agent
      i ∈ I satisfying α0 (i) = k and π 0 (i) = i.

(iii) Denote the corresponding Loeb probability spaces of the internal probability spaces (Ω, F0 , P0 )
      and (I × Ω, I0 ⊗ F0 , λ0 ⊗ P0 ) respectively by (Ω, F, P ) and (I × Ω, I  F, λ  P ). The
      mapping g is an essentially pairwise independent process from (I × Ω, I  F, λ  P ) to
      S ∪ {J}.

To reflect their dependence on (α0 , π 0 , q), π and P0 are also denoted π(α0 ,π0 ,q) and P(α0 ,π0 ,q) .

Proof. The proof consists of four steps. The first step is to allow the initially unmatched type-k
agents to randomly choose the types of their partners according to the matching probabilities.
However, for a sample realization, the set of type-k agents to be matched with type-l agents
may not have the same internal cardinality as the set of type-l agents to be matched with type-k
agents. Such sets are modified in the second step so that a matching for those agents becomes
possible. The third step is to randomly match the agents in the divided groups accordingly.
The random matching as constructed is shown to be an independent directed random matching
with the given parameters in the final step.
                                        P
Step 1: For each k ∈ S, let ηk = 1 −      r∈S qkr   (the no-matching probability for a type-k agent),
and Ik = {i ∈ I : α0 (i) = k, π 0 (i) = i} (the set of type-k agents who are initially unmatched).
For each agent i ∈ Ik , define a probability ζi on S ∪ {J} such that ζi (l) = qkl for l ∈ S and
ζi (J) = ηk . For each agent i ∈ I such that π 0 (i) 6= i, define a probability ζi on S ∪{J} such that
ζi (l) = δJ (l) for l ∈ S ∪ {J}, where δJ (l) is 1 if l = J and zero otherwise. Let Ω0 = (S ∪ {J})I
be the internal set of all the internal functions from I to S ∪ {J}, and µ0 the internal product
probability measure Πi∈I ζi on (Ω0 , A0 ), where A0 is the internal power set of Ω0 . For each
fixed sample realization ω0 ∈ Ω0 , k, l ∈ S, the agents in the set Āωkl0 = {i ∈ Ik : ω0 (i) = l} are
intended to be matched with agents in the set Āωlk0 .

Step 2: The point is that one may not be able to produce an internal partial matching so that
agents in Āωkl0 are matched with agents in Āωlk0 , since Āωkl0 and Āωlk0 may not have the same




                                                     57
internal cardinality when k 6= l, and Āωkk0 may not have even internal cardinality to allow an
internal full matching on Āωkk0 . Thus, we need to modify those sets. For k, l ∈ S with k 6= l, let

              ω0
             Ckl = {Akl : Akl ⊆ Āωkl0 , Akl is internal and |Akl | = min{|Āωkl0 |, |Āωlk0 |}}.

                  ω0                           ω0
It is clear that Ckl 6= ∅. For any k ∈ S, let Ckk be the set of all those sets in the form Āωkk0 \{i}
          ω0
for i ∈ Ākk                            ω0
             if |Āωkk0 | is odd, and Ckk    the set with one element Āωkk0 if |Āωkk0 | is even. Denote
                                ω0
                                   by C ω0 . Define an internal probability measure µω0 on C ω0 with
                      Q
the product space k,l∈S Ckl
                                                                1
its internal power set by letting48 µω0 (A) =                 |C ω0 |   for A ∈ C ω0 . The purpose of introducing
the space C ω0 and the internal probability measure µω0 is to randomly remove some agents
from the sets Āωkl0 to obtain the modified sets Aωkl0 with the desired properties for an internal
matching. Let

                  Ω1 = {(Akl )k,l∈S : Akl ⊆ I and Akl is internal, where k, l ∈ S}.

The probability measure µω0 can be trivially extended to the common sample space Ω1 with
its internal power set by letting µω0 (A) = 0 for A ∈ Ω1 \ C ω0 .
       Given the hyperfinite internal probability space (Ω0 , A0 , µ0 ) and internal transition prob-
ability µω0 , ω0 ∈ Ω0 , we can define internal probability measure µ1 on Ω0 × Ω1 with its internal
power set by letting µ1 (ω0 , A) = µ0 (ω0 ) × µω0 (A) for any ω0 ∈ Ω0 and A ∈ Ω1 .

Step 3: For any fixed ω0 ∈ Ω0 and Aω0 = (Akl )k,l∈S ∈ C ω0 , we consider internal partial
matchings on I that match agents from Akl to Alk . We only need to consider those sets Akl
which are nonempty. Let Bkω0 = Ik \( l∈S Aωkl0 ), which is the set of initially unmatched agents
                                    S

who remain unmatched. Let B̄kω0 denote the set {i ∈ Ik : ω0 (i) = J}; then it is clear that
                                                                              ω0 ,Aω0
Bkω0 = B̄kω0 ∪ l∈S Āωkl0 \ Aωkl0 . Let B ω0 = ∪K    ω0
              S                  
                                                k=1 Bk . For each k ∈ S, let Ωkk      be the
                                                                                      ω0
internal set of all the internal full matchings on Aωkk0 . Let µωkk0 ,A                     be the internal counting
                                 ,Aω0                                                ,Aω0
probability measure on Ωωkk0                                            ω0
                                        . For k, l ∈ S with k < l, let Ωkl                  be the internal set of all
                                                                      ω0
the internal bijections from     Aωkl0   to   Aωlk0 .   Let   µωkl0 ,A     be the internal counting probability on
                                                                                                                   ω0
Aωkl0 . Let Ω2 be the internal set of all the internal partial matchings from I to I. Define Ωω2 0 ,A
to be the set of φ ∈ Ω2 , with

  (i) the restriction φ|H = π 0 |H , where H is the set {i : π 0 (i) 6= i} of initially matched agents.

 (ii) {i ∈ Ik : φ(i) = i} = Bkω0 for each k ∈ S.
                                          ω0
 (iii) the restriction φ|Aω0 ∈ Ωωkk0 ,A        for k ∈ S.
                            kk

  48
     We shall also use an element d of a set D to represent the singleton set {d}. Here µω0 (A) actually means
µω0 ({A}).



                                                              58
                                                     ω0
 (iv) for k, l ∈ S with k < l, φ|Aω0 ∈ Ωωkl0 ,A .
                                       kl


Property (i) means that initially matched agents remain matched with the same partners. The
rest is clear.
                                                                ω0
       Define an internal probability measure µω2 0 ,A                on Ω2 such that
                      ω0
  (i) for φ ∈ Ωω2 0 ,A ,
                                         ω0                                      ω0
                                   µω2 0 ,A (φ) =                        µωkl0 ,A (φ|Aω0 ).
                                                            Y
                                                                                          kl
                                                                ω
                                                    1≤k≤l≤K,Akl0 6=∅

                 ω0           ω0
        / Ωω2 0 ,A , µω2 0 ,A (φ) = 0.
 (ii) φ ∈
                                                      ω0                                                   ω0
The purpose of introducing the space Ωω2 0 ,A              and the internal probability measure µω2 0 ,A         is
                                                                                                            ω0
to match the agents in Akl to the agents Alk randomly. The probability measure                      µω2 0 ,A     is
trivially extended to the common sample space Ω2 .
       Define an internal probability measure P0 on Ω = Ω0 × Ω1 × Ω2 with the internal power
set F0 by letting
                                           (
                                            µ1 (ω0 , A) × µω2 0 ,A (ω2 )              if A ∈ C ω0
                      P0 ((ω0 , A, ω2 )) =
                                            0                                         otherwise.

For (i, ω) ∈ I × Ω, let π(i, (ω0 , A, ω2 )) = ω2 (i) and
                                          (
                                            α0 (π(i, ω))             if π(i, ω) 6= i
                              g(i, ω) =
                                            J                        if π(i, ω) = i.

Denote the corresponding Loeb probability spaces of the internal probability spaces (Ω, F0 , P0 )
and (I × Ω, I0 ⊗ F0 , λ0 ⊗ P0 ) respectively by (Ω, F, P ) and (I × Ω, I  F, λ  P ). Since π is an
internal function from I × Ω to I, it is I  F-measurable.
                                n                                                   o
      Denote the internal set (ω0 , A, ω2 ) ∈ Ω : ω0 ∈ Ω0 , A ∈ C ω0 , ω2 ∈ Ωω2 0 ,A by Ω̂. By the
                                         
construction of P0 , it is clear that P0 Ω̂ = 1. By its construction, it is clear that π is an
internal random matching and satisfies part (i) of the lemma.

Step 4: It remains to prove parts (ii) and (iii) of the lemma. Define an internal process f from
I × Ω to S ∪ {J} such that for any (i, ω) ∈ I × Ω,
                                        (
                                         ω0 (i)                      if π 0 (i) = i
                             f (i, ω) =
                                         α0 (π 0 (i))                if π 0 (i) 6= i.

It is clear that if α0 (i) = k and π 0 (i) = i, then

                           P (fi = l) ' P0 (fi = l) = µ0 (ω0 (i) = l) = ζi (l) = qkl ,


                                                           59
which means49 that P (fi = l) = ◦ qkl . Similarly, we have P (fi = J) = ◦ ηk . It is also obvious
that for i 6= j in I, fi and fj are independent random variables on the sample space (Ω, F, P ).
The exact law of large number as in Lemma 1 implies that for P -almost all ω = (ω0 , A, ω2 ) ∈ Ω,
λ({α0 (i) = k, π 0 (i) = i, ω0 (i) = l}) = ◦ ρ̂kJ · ◦ qkl holds for any k, l ∈ S, and λ({α0 (i) = k, π 0 (i) =
i, ω0 (i) = J}) = ◦ ρ̂kJ · ◦ ηk , which means that
                            |Āωkl0 |                             |Āωlk0 |         |B̄kω0 |
                                        ' ρ̂kJ qkl ' ρ̂lJ qlk '               and        (44)  ' ρ̂kJ ηk .
                              M̂                                    M̂                M̂
                                                                                  
Let Ω̃ be the set of ω = (ω0 , A, ω2 ) ∈ Ω such that Equation (44) holds. Then, P Ω̃ = 1, and
               
hence P Ω̂ ∩ Ω̃ = 1.
                                                                                                             ω0
          Fix any ω = (ω0 , A, ω2 ) ∈ Ω̂ ∩ Ω̃; then A = Aω0 for some Aω0 ∈ C ω0 and ω2 ∈ Ωω2 0 ,A .
For any k 6= l ∈ S, we have
         |Aωkl0 |
                         ω0
                         |Ākl | |Āωlk0 |                  |Āω0 |     |Aω0 | |Āω0 |
                                           
                  = min         ,            ' ρ̂kJ qkl ' kl and kk ' kk ' ρ̂kJ qkk ,          (45)
           M̂              M̂      M̂                         M̂         M̂      M̂
which also implies that
                                          |Bkω0 |              |B̄ ω0 |
                                                  ' ρ̂kJ ηk ' k .
                                            M̂                   M̂
For any i ∈ Ik , i ∈ Aωkl0 if and only if π(ω0 , Aω0 , ω2 ) = ω2 (i) ∈ Aωlk0 ; and i ∈ Bkω0 if and
only if π(ω0 , Aω0 , ω2 ) = ω2 (i) = J. Hence, for the fixed ω = (ω0 , Aω0 , ω2 ), and for any
k, l ∈ S, we can obtain that if i ∈ Aωkl0 ⊆ Āωkl0 , f (i, ω) = ω0 (i) = l = α0 (ω2 (i)) = g(i, ω); if
i ∈ B̄kω0 ⊆ Bkω0 , f (i, ω) = ω0 (i) = J = α0 (ω2 (i)) = g(i, ω). For any i ∈ I\(∪k∈S Ik ) which
means π 0 (i) 6= i, we can obtain that f (i, ω) = α0 (π 0 (i)) = α0 (π(i, ω)) = g(i, ω). It is clear that
the set {i ∈ I : f (i, ω) 6= g(i, ω)} is a subset of l∈S Āωkl0 \ Aωkl0 , which has λ-measure zero by
                                                    S                  

(45).
                                   
          By the fact that P Ω̂ ∩ Ω̃ = 1, we know that for P -almost all ω ∈ Ω,

                                             λ (i ∈ I : f (i, ω) = g(i, ω)) = 1.

Since the Loeb product space (I × Ω, I  F, λ  P ) is a Fubini extension as discussed in
Subsection D.4, the Fubini property implies that for λ-almost all i ∈ I, g(i, ω) is equal to
f (i, ω) for P -almost all ω ∈ Ω. Hence g satisfies part (ii) of the lemma. Let I˜ be an I-
measurable set with λ(I) ˜ = 1 such that for any i ∈ I,
                                                     ˜ gi (ω) = fi (ω) for P -almost all ω ∈ Ω.
Therefore, by the construction of f , we know that the collection of random variables {gi }i∈I˜ is
mutually independent in the sense that any finitely many random variables from that collection
are mutually independent. This also implies part (iii) of the lemma.

Proof of Theorem 1: We follow Lemma 7. Let α0 be an internal type function from I to
S such that50 λ0 {α0 (i) = k} ' pk for any k ∈ S. Let π 0 (i) = i for any i ∈ I. Given that
                             

  49
       Recall that for a bounded hyperreal number x ∈ ∗ R, ◦ x is its standard part.
  50
       For any given p ∈ ∆, the atomless property of λ0 implies the existence of such an α0 .


                                                              60
                                                                     P
matching probability function q from S × S to R+ with                  r∈S qkr   ≤ 1 and pk qkl = pl qlk for
all k, l ∈ S, the condition ρ̂kJ qkl ' ρ̂lJ qlk in the statement of Lemma 7 is obviously satisfied.
It is clear that the random matching π and the probability measure P constructed in Lemma
7 satisfies all the conditions in Theorem 1. Let α be α0 . Then α and π, which are defined
on a Fubini extension (I × Ω, I  F, λ  P ), are a type function and an independent directed
random matching with respective parameters p and q. For each ω ∈ Ω, since πω is an internal
bijection on I and λ0 is the hyperfinite counting probability measure on I0 , it is obvious that
πω is measure-preserving from the Loeb space (I, I, λ) to itself.

E.2    Proof of Theorem 5

What we need to do is to construct sequences of internal transition probabilities, internal
type functions, and internal random matchings. Since we need to consider random mutation,
random matching and random type changing with break-up at each time period, three internal
measurable spaces with internal transition probabilities will be constructed at each time period.
After the construction, we need to check the satisfiability of Markov conditional independence
for each step.
       In contrast to the settings in Duffie and Sun (2007), the matched agents considered in
this paper may form an enduring partnership after the matching step. Matched agents will
not participate in the search process until they break up. During their partnership, a matched
agent and her partner may change their types with correlation. Therefore, one needs to keep
track of those matched agents and their partners at each step, which means to work with
extended-type processes that incorporate the types of the agents and their partners in this
paper rather than simply the type processes of the agents in Duffie and Sun (2007). This
brings substantial difficulties in the construction of the dynamical system and the proof of the
property of Markov conditional independence for the extended-type processes.
       Let T0 be the hyperfinite discrete time line {n}M
                                                       n=0 and (I, I0 , λ0 ) be the agent space,
where I = {1, . . . , M̂ }, I0 is the internal power set on I, λ0 is the internal counting probability
measure on I0 , M and M̂ are unlimited hyperfinite numbers in ∗ N∞ . We transfer the sequences
of numbers bn , θn , σ n , ς n , n ∈ N to the nonstandard universe to obtain bn , θn , σ n , ς n , n ∈ ∗ N.
The transfer of the sequence of functions q n , n ∈ N to the nonstandard universe is denoted
by ∗ q n , n ∈ ∗ N. Then, for any k, l ∈ S, ∗ qkl                                       ˆ to ∗ [0, 1]. Let
                                                       n is an internal function from ∗ ∆

  n (ρ̂) = (∗ q n )(ρ̂) and η̂ n = 1 −         n                                 ∗ˆ
                                       P
q̂kl           kl             k          l∈S q̂kl (ρ̂) for any k, l ∈ S and ρ̂ ∈ ∆. Note that an object
with an upper left star means the transfer of a standard object to the nonstandard universe.
       We shall first consider the case of an initial condition Π0 that is deterministic. Let
                                                           |Akl |
{Akl }(k,l)∈Ŝ be an internal partition of I such that              ' p̈kl for any k ∈ S and l ∈ S ∪ {J},
                                                            M̂




                                                    61
and |Akl | = |Alk | and |Akk | are even for any k, l ∈ S. Let α0 be an internal function from
(I, I0 , λ0 ) to S such that α0 (i) = k if i ∈ l∈S∪{J} Akl . Let π 0 be an internal partial matching
                                              S

from I to I such that π 0 (i) = i on k∈S AkJ , and the restriction π 0 |Akl is an internal bijection
                                       S

from Akl to Alk for any k, l ∈ S. Let
                                                   (
                                                    α0 (π 0 (i))          if π 0 (i) 6= i
                                         g 0 (i) =
                                                    J                     if π 0 (i) = i.

It is clear that λ0 ({i : α0 (i) = k, g 0 (i) = l}) ' p̈0kl for any k ∈ S and l ∈ S ∪ {J}.
        Suppose that the dynamical system D has been constructed up to time period n − 1 ∈
∗ N.   That is, {(Ωm , Fm , Qm )}3n−3      l   l n−1
                                 m=1 and {α , π }l=0 have been constructed, where each Ωm is a
hyperfinite internal set with its internal power set Fm , Qm an internal transition probability
from Ωm−1 to (Ωm , Fm ), αl an internal type function from I × Ω3l−1 to the type space S, and
π l an internal random matching51 from I × Ω3l to I. Here, Ωm = m                      m
                                                                   Q
                                                                     j=1 Ωj , and {ωj }j=1 will
also be denoted by ω m when there is no confusion. Denote the internal product transition
probability Q1 ⊗ Q2 ⊗ · · · ⊗ Qm by Qm , and ⊗m
                                              j=1 Fj by F
                                                          m (which is simply the internal power

set on Ωm ). Then, Qm is the internal product of the internal transition probability Qm with
the internal probability measure Qm−1 .
        We shall now consider the constructions for time n. We first work with the random
mutation step. Let Ω3n−2 = S I (the space of all internal functions from I to S) with its
internal power set F3n−2 . For each i ∈ I, ω 3n−3 ∈ Ω3n−3 , if αn−1 (i, ω 3n−3 ) = k, define a
                              3n−3                                3n−3
probability measure γiω               on S by letting γiω(l) = bnkl for each l ∈ S. Define an internal
                      ω 3n−3 on (S I , F                                                       ω 3n−3 . Let
                                                                                        Q
probability measure Q3n−2                3n−2 ) to be the internal product measure        i∈I γi
                                                                                                   
ᾱn : I × 3n−2
          Q                                    n i, ω 3n−2 = ω
                                                                                 n : I ×
                                                                                          Q3n−2
            m=1 Ω m   →   S be such    that ᾱ                  3n−2  (i). Let ḡ           m=1   Ω m    →
S ∪ {J} be such that
                                       (
                                        ᾱn (π n−1 (i, ω 3n−3 ), ω 3n−2 )            if π n−1 (i, ω 3n−3 ) 6= i
                ḡ n i, ω   3n−2
                                 
                                     =
                                        J                                            if π n−1 (i, ω 3n−3 ) = i.
                                           −1
Let ρ̌nω3n−2 = λ0 ᾱωn 3n−2 , ḡωn3n−2            be the internal cross-sectional extended type distribution
after random mutation.
        Next, we consider the step of directed random matching. Let (Ω3n−1 , F3n−1 ) = (Ω̄, F̄),
where (Ω̄, F̄) is the measurable space constructed in the proof of Lemma 7. For any given
ω 3n−2 ∈ Ω3n−2 , the type function is ᾱωn 3n−2 ( · ) while the partial matching function is πωn−1
                                                                                                3n−3 ( · ).
                                                                              3n−2
We can construct an internal probability measure Qω3n−1 = Pᾱn                                            and a
                                                                                                      ,π n−1      n n
                                                                                             ω 3n−2     ω 3n−3 ,q̂ (ρ̌ 3n−2 )
                                                                                                                    ω
                                                                                                   Q3n−1 
                                                                                            n
directed random matching πᾱn                   ,π n−1      n n
                                                    3n−3 ,q̂ (ρ̌ 3n−2   ) by Lemma 7. Let π̄ : I ×  m=1 Ωm → I
                                       ω 3n−2     ω           ω

  51
    To handle the deterministic case at the initial step with l = 0 (3l − 1 = −1 and 3l = 0), one can let Ω0 = Ω−1
be a singleton set.


                                                                  62
be such that
                               π̄ n i, ω 3n−1 = πᾱn
                                              
                                                              ,π n−1 ,q̂ n (ρ̌n 3n−2 ) (i, ω3n−1 ),
                                                        ω 3n−2 ω 3n−3         ω
                                          (
                                            ᾱn (π̄ n (i, ω 3n−1 ), ω 3n−2 ) if π̄ n (i, ω 3n−1 ) 6= i
                     ḡ¯n (i, ω 3n−1 ) =
                                            J                                     if π̄ n (i, ω 3n−1 ) = i.
        Now, we consider the final step of random type changing with break-up for matched
agents. Let Ω3n = (S ×{0, 1})I with its internal power set F3n , where 0 represents “unmatched”
                                              1 , ω2 ) ∈ Ω
and 1 represents “paired”; each point ω3n = (ω3n   3n     3n is an internal function from I to

S × {0, 1}. Define a new type function αn : (I × Ω3n ) → S by letting αn (i, ω 3n ) = ω3n
                                                                                       1 (i). Fix

ω 3n−1 ∈ Ω3n−1 . For each i ∈ I, (1) if π̄ n (i, ω 3n−1 ) = i (i is not paired after the matching step at
                   3n−1
time n), let τiω  be the probability measure on the type space S ×{0, 1} that gives probability
one to the type ᾱn (i, ω 3n−2 ), 0 and zero for the rest; (2) if π̄ n (i, ω 3n−1 ) 6= i (i is paired after
                                   

the matching step at time n), ᾱn (i, ω 3n−2 ) = k, π̄ n (i, ω 3n−1 ) = j and ᾱn (j, ω 3n−2 ) = l, define
                                    3n−1                                                                     3n−1
a probability measure τijω                  on (S × {0, 1}) × (S × {0, 1}) such that τijω                           ((k 0 , 1), (l0 , 1)) =
      n )σ n (k 0 , l0 ) and τ ω   3n−1
(1 − θkl  kl                  ij
                                                                   n ς n (k 0 )ς n (l0 ) for k 0 , l0 ∈ S, and zero for the rest.
                                          ((k 0 , 0), (l0 , 0)) = θkl kl        lk
Let Anω3n−1 = {(i, j) ∈ I × I : i < j, π̄ n (i, ω 3n−1 ) = j} and Bωn3n−1 = {i ∈ I : π̄ n (i, ω 3n−1 ) = i}.
                                                                    3n−1
Define an internal probability measure Qω3n                                on (S × {0, 1})I to be the internal product
measure
                                                             3n−1                              3n−1
                                              Y                             Y
                                                       τiω          ⊗                   τijω          .
                                           i∈B n3n−1                    (i,j)∈An 3n−1
                                               ω                               ω

Let
                     (
                      J                         if π̄ n (i, ω 3n−1 ) = J or ω3n
                                                                             2 (i) = 0 or ω 2 (π̄ n (i, ω 3n−1 )) = 0
                                                                                           3n
    π n (i, ω 3n ) =
                      π̄ n (i, ω 3n−1 )         otherwise.

and                                    (
                            n         3nαn (π n (i, ω 3n ), ω 3n )                  if π n (i, ω 3n ) 6= i
                           g (i, ω ) =
                                        J                                           if π n (i, ω 3n ) = i.
It is clear that π n is a random matching and Equation (19) holds.
        Keep repeating the construction. We can then construct a hyperfinite sequence of in-
ternal transition probabilities {(Ωm , Fm , Qm )}3M
                                                 m=1 and a hyperfinite sequence of internal type
functions and internal random matchings {(αn , π n )}M
                                                     n=0 .
        Let (I ×Ω3M , I0 ⊗F 3M , λ0 ⊗Q3M ) be the internal product probability space of (I, I0 , λ0 )
and (Ω3M , F 3M , Q3M ). Denote the Loeb spaces of (Ω3M , F 3M , Q3M ) and the internal product
(I × Ω3M , I0 ⊗ F 3M , λ0 ⊗ Q3M ) by (Ω3M , F, P ) and (I × Ω3M , I  F, λ  P ) respectively. For
simplicity, let Ω3M be denoted by Ω, Q3M be denoted by P0 . As discussed in Subsection D.4,
the Loeb product space (I × Ω, I  F, λ  P ) is a Fubini extension.



                                                                      63
          In the following, we will often work with functions or sets that are measurable in
(Ωm , F m , Qm )       or its Loeb space for some m ≤ 3M , which may be viewed as functions or
sets based on (Ω3M , F 3M , Q3M ) or its Loeb space by allowing for dummy components for the
tail part. We can thus continue to use P to denote the Loeb measure generated by Qm for
convenience. Since all the type functions, random matchings and the partners’ type functions
are internal in the relevant hyperfinite settings, they are all I  F-measurable when viewed as
functions on I × Ω.
          For n = 0, the initial independence condition in the definition of Markov conditional
independence in Subsection A.2 is trivially satisfied. Suppose that the Markov conditional
independence are satisfied up to period n − 1 ∈ N. It remains to check the Markov conditional
independence for each step of random mutation, random matching, and match-induced type
changes with break-up in period n.
          For the mutation step in period n, fix any (a1 , r1 ), (a2 , r2 ) and (k1t , l1t ), (k2t , l2t ), t =
1, . . . , n − 1 in Ŝ. For any agents i and j with i 6= j, we can obtain that

P β̄in = (a1 , r1 ), β̄jn = (a2 , r2 ), βit = (k1t , l1t ), βjt = (k2t , l2t ), t = 1, . . . , n − 1
                                                                                                     
           Z
                         ω 3n−3
                                 β̄ n (i, ω 3n−2 ) = (a1 , r1 ), β̄ n (j, ω 3n−2 ) = (a2 , r2 ) dQ3n−3 (ω 3n−3 )
                                                                                                    
        '              Q3n−2
                  3n−3
                 Dij
               Z
                          ω 3n−3
                                 β̄ n (i, ω 3n−2 ) = (a1 , r1 ), β̄ n (j, ω 3n−2 ) = (a2 , r2 ) dQ3n−3 (ω 3n−3 )
                                                                                               
           =             Q3n−2
                 D3n−3
                  ij
                   Z
                             ω 3n−3
                                    β̄ n (i, ω 3n−2 ) = (a1 , r1 ), β̄ n (j, ω 3n−2 ) = (a2 , r2 ) dQ3n−3 (ω 3n−3 ),
                                                                                                  
               +     3n−3
                            Q3n−2
                    Dij

where

                3n−3
               Dij   = {ω 3n−3 : β t (i, ω 3t ) = (k1t , l1t ), β t (j, ω 3t ) = (k2t , l2t ), t = 1, . . . , n − 1}


D3n−3
 ij   = {ω 3n−3 : π n−1 (i, ω 3n−3 ) 6= j, β t (i, ω 3t ) = (k1t , l1t ), β t (j, ω 3t ) = (k2t , l2t ), t = 1, . . . , n − 1}


   3n−3
Dij       = {ω 3n−3 : π n−1 (i, ω 3n−3 ) = j, β t (i, ω 3t ) = (k1t , l1t ), β t (j, ω 3t ) = (k2t , l2t ), t = 1, . . . , n − 1}.

                                                    3n−3       3n−3
Fix any agent i ∈ I. It is clear that Dij                  ∩ Dij 0     = ∅ for different j and j 0 . Then there are at
                                                             3n−3                                                      3n−3
most countably many j ∈ I such that P (Dij                          ) > 0. Let Fi3n−3 = {j ∈ I : j 6= i, P (Dij               )=
0}; then    λ(Fi3n−3 )    = 1. Fix any j ∈        Fi3n−3 .   The probability for agents i and j to be partners is
zero at the end of period n − 1. When agents i and j are not partners, their random extended




                                                               64
                                                                                         3n−3
 types will be independent by the construction of Qω3n−2 . Hence, we can obtain that

P β̄in = (a1 , r1 ), β̄jn = (a2 , r2 ), βit = (k1t , l1t ), βjt = (k2t , l2t ), t = 1, . . . , n − 1
                                                                                                     
         Z
                      ω 3n−3
                             β̄ n (i, ω 3n−2 ) = (a1 , r1 ), β̄ n (j, ω 3n−2 ) = (a2 , r2 ) dQ3n−3 (ω 3n−3 )
                                                                                                 
       '           Q3n−2
                D3n−3
                 ij
              Z
                            ω 3n−3
                                                                  3n−3
                                   β̄ n (i, ω 3n−2 ) = (a1 , r1 ) Qω3n−2 β̄ n (j, ω 3n−2 ) = (a2 , r2 ) dQ3n−3 (ω 3n−3 )
                                                                                                       
          =                Q3n−2
                D3n−3
                 ij
              Z
          =                Bk3n−2l
                                                 3n−2
                              n−1 n−1 (a1 , r1 )B n−1 n−1 (a2 , r2 )dQ
                                                 k   l
                                                                      3n−3 3n−3
                                                                          (ω    )
                D3n−3
                 ij
                                1        1                2          2

                  3n−3
          '   P (Dij   )Bk3n−2                3n−2
                           n−1 n−1 (a1 , r1 )B n−1 n−1 (a2 , r2 ),
                              l               k   l
                                     1       1                   2       2


 where                                                            
                                                                    n n                   if l, s ∈ S
                                                                  bkr bls
                                                                  
                                                     3n−2
                                                    Bkl   (r, s) = bnkr                   if l = s = J
                                                                  
                                                                   0                      otherwise.
                                                                  

 Thus, for λ-almost all agent j ∈ I,

                  P β̄in = (a1 , r1 ), β̄jn = (a2 , r2 ) | βit = (k1t , l1t ), βjt = (k2t , l2t ), t = 1, . . . , n − 1
                                                                                                                             

                              = Bk3n−2l
                                                      3n−2
                                   n−1 n−1 (a1 , r1 )B n−1 n−1 (a2 , r2 ).
                                                      k   l
                                                                                                                                       (46)
                                     1       1                   2           2


 Note that for any i ∈ I,
                                                                         Z
                                                                                     3n−3
P (β̄in       (a1 , r1 ), βin−1              (k1n−1 , l1n−1 ))                    Qω3n−2 β̄ n (i, ω 3n−2 ) = (a1 , r1 ) dQ3n−3 (ω 3n−3 )
                                                                                                                       
          =                              =                       '
                                                                         Ei3n−3
              Z
          =                   Bk3n−2
                                 n−1 n−1 (a1 , r1 )dQ
                                    l
                                                      3n−3 3n−3
                                                          (ω    ) ' P (Ei3n−3 )Bk3n−2
                                                                                  n−1 n−1 (a1 , r1 ),
                                                                                     l
                  Ei3n−3         1       1                                                           1       1



 where Ei3n−3 = {ω 3n−3 : β n−1 (i, ω 3n−3 ) = (k1n−1 , l1n−1 )}. Then, we have

                               P (β̄in = (a1 , r1 ) | βin−1 = (k1n−1 , l1n−1 )) = Bk3n−2
                                                                                     n−1 n−1 (a1 , r1 ).
                                                                                        l
                                                                                                                                       (47)
                                                                                                 1       1


 Hence, Equations (13) and (14) in the definition of dynamical system are satisfied. By Equation
 (46), we can obtain for each i ∈ I, and for λ-almost all j ∈ I,

      P β̄in = (a1 , r1 ), β̄jn = (a2 , r2 ) | βit = (k1t , l1t ), βjt = (k2t , l2t ), t = 1, . . . , n − 1
                                                                                                           
                                                                                                           
             = P β̄in = (a1 , r1 ) | βin−1 = (k1n−1 , l1n−1 ) P β̄jn = (a2 , r2 ) | βjn−1 = (k2n−1 , l2n−1 ) . (48)

 Hence, Equation (23) in the definition of Markov conditional independence is satisfied.
            For the random matching step in period n, fix any (a1 , r1 ), (a2 , r2 ) in S × S and any
                                                                                                                      3n−3
 (k1t , l1t ), (k2t , l2t )   in Ŝ for t = 1, . . . , n − 1. Fix any ω 3n−2 ∈ Ω3n−2 . Let Aω                                    = {i ∈ I :
 πωn−1
    3n−3 (i) 6= i}.            By Lemma 1 (i), we know that
          3n−2
                                                                                              3n−3
                                                                                                    
    Qω3n−1          ω3n−1 ∈ Ω3n−1 : π̄ n i, (ω 3n−2 , ω3n−1 ) = π n−1 i, ω 3n−3 for any i ∈ Aω
                                                                              
                                                                                                      = 1,

                                                                                  65
which implies that Equation (15) holds.
        Lemma 2 and Equation (48) imply that the extended type process β̄ n is essentially
pairwise independent. It follows from the exact law of large numbers in Lemma 1 that for
P -almost all ω 3n−2 ∈ Ω3n−2 ,
                                                                    −1
              ρ̌n (ω 3n−2 ) ' p̌n (ω 3n−2 ) = λ β̄ωn3n−2                   = E p̌n ω 3n−2             = p̃n ' E (ρ̌n ) .
                                                                                                 
                                                                                                                           (49)

Then Equation (17) is equivalent to

                                         P (ḡ¯in = l | ᾱin = k, ḡin = J) = qkl
                                                                               n
                                                                                  (p̃n ) .

Since paired agents do not match in this step, their extended types will not change. Thus, to
verify Equation (24), we only need to prove, for the event

           An = {β̄in = (a1 , J), β̄jn = (a2 , J), βit = (k1t , l1t ), βjt = (k2t , l2t ), t = 1, . . . , n − 1},

that
                                                           
              P β̄¯in = (a1 , r1 ), β̄¯jn = (a2 , r2 ) | An
                                                                                                 
                     = P β̄¯in = (a1 , r1 ) | β̄in = (a1 , J) P β̄¯jn = (a2 , r2 ) | β̄jn = (a2 , J) .

Fix any k ∈ S. If p̃nkJ =               P (β̄in = (k, J))dλ(i) = 0, then P (β̄in = (k, J)) = 0 for λ-almost all
                                R
                                    I
agent i ∈ I, which means that Equation (24) automatically holds. It follows from the continuity
requirement above Equation (49) that

                                              ρ̌na1 J q̂an1 r1 (ρ̌n ) ' p̃na1 J qan1 r1 (p̃n )

for P -almost all ω 3n−2 ∈ Ω3n−2 . Suppose p̃na1 J > 0 and p̃na2 J > 0. Hence, we can obtain that
for P -almost all ω 3n−2 ∈ Ω3n−2 , q̂an1 r1 (ρ̌n ) ' qan1 r1 (p̃n ) and q̂an2 r2 (ρ̌n ) ' qan2 r2 (p̃n ).




                                                                    66
        We can now derive
             Z Z   
                  P β̄¯in = (a1 , r1 ), β̄¯jn = (a2 , r2 ), β̄in = (a1 , J), β̄jn = (a2 , J),
              I I
                                                                                      
                         βit = (k1t , l1t ), βjt = (k2t , l2t ), t = 1, . . . , n − 1
                                                       3n−2
                 −qan1 r1 (p̃n ) qan2 r2 (p̃n ) P (Dij       ) dλ(j)dλ(i)
                 Z Z Z                3n−2
                                          ω
                                                   ḡ¯n (i, ω 3n−1 ) = r1 , ḡ¯n (j, ω 3n−1 ) = r2
                                                                                                   
               '                       Q3n−1
                              3n−2
                    I   I    Dij
                                                               
                 −q̂an1 r1 ρ̌n (ω 3n−2 ) q̂an2 r2 ρ̌n (ω 3n−2 ) dQ3n−2 (ω 3n−2 ) dλ0 (j)dλ0 (i)
                                        
                 Z Z Z
                                                         3n−2
                                 1D3n−2 (ω 3n−2 ) Qω3n−1 ḡ¯n (i, ω 3n−1 ) = r1 , ḡ¯n (j, ω 3n−1 ) = r2
                                                                                                         
               ≤
                                       ij
                    I   I   Ω3n−2

                 −q̂an1 r1 ρ̌n (ω 3n−2 ) q̂an2 r2 ρ̌n (ω 3n−2 )   dQ3n−2 (ω 3n−2 ) dλ0 (j) dλ0 (i)
                                                               
                 Z        Z Z
                                                         3n−2
                                 1D3n−2 (ω 3n−2 ) Qω3n−1 ḡ¯n (i, ω 3n−1 ) = r1 , ḡ¯n (j, ω 3n−1 ) = r2
                                                                                                         
               =
                                       ij
                    Ω3n−2    I   I

                  −q̂an1 r1 ρ̌n (ω 3n−2 ) q̂an2 r2 ρ̌n (ω 3n−2 )          dλ0 (j) dλ0 (i) dQ3n−2 (ω 3n−2 ),
                                                                
                                                                                                                                   (50)

where

     D3n−2 = {(ω 3n−2 , i, j) :             β̄ n (i, ω 3n−2 ) = (a1 , J), β̄ n (j, ω 3n−2 ) = (a2 , J),
                                            β t (i, ω 3t ) = (k1t , l1t ), β t (j, ω 3t ) = (k2t , l2t ), t = 1, . . . , n − 1},

 3n−2
Dij   is the (i, j)-section of D3n−2 , and 1D3n−2 is the indicator function of the set 1D3n−2 in
                                                             ij                                                              ij
Ω3n−2 . By Lemma 7 (iii), it is clear that for λ-almost all i ∈ I, for λ-almost all j ∈ I, and for
              3n−2
any ω 3n−2 ∈ Dij   , we have
         3n−2
      Qω3n−1 ḡ¯n (i, ω 3n−1 ) = r1 , ḡ¯n (j, ω 3n−1 ) = r2 ' q̂an1 r1 ρ̌n (ω 3n−2 ) q̂an2 r2 ρ̌n (ω 3n−2 ) .
                                                                                                          


Hence, the last term of Equation (50) is equal to an infinitesimal. Therefore, the first term of
Equation (50) is equal to zero, which implies that for λ-almost all i ∈ I
                                                                    
                        P β̄¯in = (a1 , r1 ), β̄¯jn = (a2 , r2 ) | An = qan1 r1 (p̃n ) qan2 r2 (p̃n ) ,                            (51)

for λ-almost all j ∈ I.
        For i ∈ I, let Ei3n−2 = {ω 3n−2 : β̄ n (i, ω 3n−2 ) = (a1 , J)}. We can obtain that for λ-almost
all i ∈ I, and for any ω 3n−2 ∈ Ei3n−2 ,
                                        Z
         ¯n                 n
      P β̄i = (a1 , r1 ), β̄i = (a1 , J) '
                                                                     3n−2
                                                                  Qω3n−1 ḡ¯n (i, ω 3n−1 ) = r1 dQ3n−2 (ω 3n−2 ),
                                                                                               
                                                        Ei3n−2

       3n−2
and Qω3n−1 ḡ¯n (i, ω 3n−1 ) = r1 ' q̂an1 r1 ρ̌n (ω 3n−2 ) . Hence, we can obtain that for λ-almost all
                                                         



                                                                  67
i ∈ I,
                                                           
                       P β̄¯in = (a1 , r1 ), β̄in = (a1 , J)
                       Z
                                ω 3n−2 ¯n
                                          ḡ (i, ω 3n−1 ) = r1 dQ3n−2 (ω 3n−2 )
                                                              
                     '         Q3n−1
                            Ei3n−2
                            Z
                                     q̂an1 r1 ρ̌n (ω 3n−2 ) dQ3n−2 (ω 3n−2 ) ' P (Ei3n−2 )qan1 r1 (p̃n ) .
                                                           
                     '
                            Ei3n−2

Therefore, we have for λ-almost all i ∈ I,
                                                                       
                                  P β̄¯in = (a1 , r1 ) | β̄in = (a1 , J) = qan1 r1 (p̃n ) .                                       (52)

Since p̌n (ω 3n−2 ) ' p̃n for P -almost all ω 3n−2 ∈ Ω3n−2 , Equation (52) implies Equation (17).
Combining Equations (51) and (52) together, we have
                                             
P β̄¯in = (a1 , r1 ), β̄¯jn = (a2 , r2 ) | An   = qan1 r1 (p̃n ) qan2 r2 (p̃n )
                                                                                                                            
                                                = P β̄¯in = (a1 , r1 ) | β̄in = (a1 , J) P β̄¯jn = (a2 , r2 ) | β̄jn = (a2 , J) .

Hence, Equation (24) in the definition of Markov conditional independence is satisfied.
         For the step of type changing with break-up in period n, fix any (a1 , r1 ), (a2 , r2 ), (x1 , y1 ), (x2 , y2 ),
and (k1t , l1t ), (k2t , l2t ), t = 1, . . . , n − 1 in Ŝ. For any agents i and j with i 6= j, we can obtain
that
               
             P βin = (a1 , r1 ), βjn = (a2 , r2 ), β̄¯in = (x1 , y1 ), β̄¯jn = (x2 , y2 ),
                                                                             
                 βit = (k1t , l1t ), βjt = (k2t , l2t ), t = 1, . . . , n − 1
               Z
                         ω 3n−1
                                     β n (i, ω 3n ) = (a1 , r1 ), β n (j, ω 3n ) = (a2 , r2 ) dQ3n−1 (ω 3n−1 )
                                                                                             
             '         Q3n
                   3n−1
                  Dij
                 Z
                             ω  3n−1
                                       β n (i, ω 3n ) = (a1 , r1 ), β n (j, ω 3n ) = (a2 , r2 ) dQ3n−1 (ω 3n−1 )
                                                                                               
             =              Q3n
                  D3n−1
                   ij
                     Z
                                  3n−1
                              Qω3n        β n (i, ω 3n ) = (a1 , r1 ), β n (j, ω 3n ) = (a2 , r2 ) dQ3n−1 (ω 3n−1 ),
                                                                                                  
                 +       3n−1
                      Dij

where

           3n−1
          Dij   = {ω 3n−1 :              β̄¯in = (x1 , y1 ), β̄¯jn = (x2 , y2 ),
                                         β t (i, ω 3t ) = (k1t , l1t ), β t (j, ω 3t ) = (k2t , l2t ), t = 1, . . . , n − 1},


D3n−1
 ij   = {ω 3n−1 : π̄ n (i, ω 3n−1 ) 6= j,              β̄¯in = (x1 , y1 ), β̄¯jn = (x2 , y2 ),
                                                       β t (i, ω 3t ) = (k1t , l1t ), β t (j, ω 3t ) = (k2t , l2t ), t = 1, . . . , n − 1},


                                                                 68
  3n−1
Dij      = {ω 3n−1 : π̄ n (i, ω 3n−1 ) = j,              β̄¯in = (x1 , y1 ), β̄¯jn = (x2 , y2 ),
                                                         β t (i, ω 3t ) = (k1t , l1t ), β t (j, ω 3t ) = (k2t , l2t ), t = 1, . . . , n − 1}.

                                                                    3n−1         3n−1
         Fix any agent i ∈ I. It is clear that Dij                          ∩ Dij 0      = ∅ for different j and j 0 . Then
                                                                              3n−1
there are at most countably many j ∈ I such                         that P (Dij ) > 0.              Let Fi3n−1 = {j ∈ I : j 6=
         3n−1
i, P (Dij        ) = 0}; then λ(Fi3n−1 ) = 1. Next,                 fix any j ∈ Fi3n−1 .           The probability for agents
i and j to be partners is zero at the matching step in period n. When agents i and j are
                                                                                                                                   3n−1
not partners, their random extended types will be independent by the construction of Qω3n                                                 .
Hence, we can obtain that
    
  P βin = (a1 , r1 ), βjn = (a2 , r2 ), β̄¯in = (x1 , y1 ), β̄¯jn = (x2 , y2 ),
                                                                        
           βit = (k1t , l1t ), βjt = (k2t , l2t ), t = 1, . . . , n − 1
        Z
                    ω 3n−1
                               β n (i, ω 3n ) = (a1 , r1 ), β n (j, ω 3n ) = (a2 , r2 ) dQ3n−1 (ω 3n−1 )
                                                                                       
      '          Q3n
                 D3n−1
                  ij
             Z
                          ω  3n−1                                3n−1 n
                                     β n (i, ω 3n ) = (a1 , r1 ) Qω3n  β (j, ω 3n ) = (a2 , r2 ) dQ3n−1 (ω 3n−1 )
                                                                                                
         =               Q3n
                 D3n−1
                  ij
             Z
         =               Bx3n1 y1 (a1 , r1 )Bx3n2 y2 (a2 , r2 ) dQ3n−1 (ω 3n−1 )
                 D3n−1
                  ij
               3n−1
         ' P (Dij   )Bx3n1 y1 (a1 , r1 )Bx3n2 y2 (a2 , r2 ),

where                                      
                                                    n   n
                                           (1 − θkl )σkl (r, s)
                                                                              if l, s ∈ S
                                3n           n ς n (r)
                               Bkl (r, s) = θkl kl                             if l ∈ S and s = J
                                           
                                            δk (r)δJ (s)                       if l = J.
                                           

Therefore, for any i ∈ I, and for λ-almost all j ∈ I,
                      
                    P βin = (a1 , r1 ), βjn = (a2 , r2 ) | β̄¯in = (x1 , y1 ), β̄¯jn = (x2 , y2 ),                                  (53)
                                                                                     
                         βit = (k1t , l1t ), βjt = (k2t , l2t ), t = 1, . . . , n − 1
                             = Bx3n1 y1 (a1 , r1 )Bx3n2 y2 (a2 , r2 ).                                                              (54)

Note that for any agent i ∈ I,
                                                       Z
      P (βin = (a1 , r1 ), β̄¯in = (x1 , y1 )) '
                                                                     3n−1
                                                                  Qω3n       β n (i, ω 3n ) = (a1 , r1 ) dQ3n−1 (ω 3n−1 )
                                                                                                        
                                                         Ei3n−1
                 Z
             =             Bx3n1 y1 (a1 , r1 ) dQ3n−3 (ω 3n−3 ) ' P (Ej3n−1 )Bx3n1 y1 (a1 , r1 ),
                  Ei3n−1


where Ei3n−1 = {ω 3n−1 : β̄¯n (i, ω 3n−1 ) = (x1 , y1 )}. This implies that

                                    P (βin = (a1 , r1 ) | β̄¯in = (x1 , y1 )) = Bx3n1 y1 (a1 , r1 ).

                                                                   69
Hence, Equations (20), (21) and (22) in the definition of the dynamical system D are satisfied.
By Equation (53), we can obtain for each i ∈ I, and for λ-almost all j ∈ I,
                
              P βin = (a1 , r1 ), βjn = (a2 , r2 ) | β̄¯in = (x1 , y1 ), β̄¯jn = (x2 , y2 ),
                                                                                
                   βit = (k1t , l1t ), βjt = (k2t , l2t ), t = 1, . . . , n − 1
                                                                                            
              = P βin = (a1 , r1 ) | β̄¯in = (x1 , y1 ) P βjn = (a2 , r2 ) | β̄¯jn = (x2 , y2 ) .

Hence, Equation (25) in the definition of Markov conditional independence is satisfied.
       In summary, we have shown the validity of Equations (13) to (22), and (23) to (25).
Hence D is a dynamical system with the Markov conditional independence property, where the
initial condition Π0 is deterministic. Note that, for each n ∈ N and ω ∈ Ω, since πωn and π̄ωn
are internal bijections on I, it is obvious that πωn and π̄ωn are measure-preserving from the Loeb
space (I, I, λ) to itself.
       Finally, we consider the case that the initial extended type process β 0 is i.i.d. across
agents. We shall use the construction for the case of deterministic initial condition. We choose
n = −1 to be the initial period so that we can have some flexibility in choosing the parameters
in period 0. Assume that at n = −1, all agents have type 1, and no agents are matched.
Namely, the initial type function is α−1 ≡ 1 while the initial matching is π −1 ≡ i.
     Denote r∈S∪{J} p̈0kr by p̈0k . For the parameters in period 0, let
              P

                                                           (
                                                            p̈0r              if k = 1
                                                  b0kr   =
                                                            δk (r)                 6 1,
                                                                              if k =
                             
                                                  p̈0kl p̂kJ p̈0kl p̂lJ
                                                                         
                                  1
                 0
                             
                                 p̂kJ   min           p̈0k
                                                            , p̈0             if p̂kJ 6= 0, p̈0k 6= 0 and p̈0l 6= 0
                qkl (p̂) =                                         l
                             0                                               otherwise,
 0 (k 0 , l0 ) = δ (k 0 )δ (l0 ), ς 0 (k 0 ) = δ (k 0 ), and θ 0 = 0 for any k, k 0 , l, l0 ∈ S. Following the
σkl               k       l        kl           k             kl
construction for the case of deterministic initial condition, there exists a Fubini extension
(I × Ω, I  F, λ  P ) on which is defined a dynamical system D = (Πn )∞
                                                                       n=−1 that is Markov
conditionally independent with the parameters (bn , q n , σ n , θn )∞
                                                                    n=0 .
       By Lemma 4, p̃0kl = δJ (l)p̈0k . It follows from part (2) of Theorem 4 that,

                                                     0 p̈0kl
                                                    z(1J)(kl) = p̈0k
                                                             = p̈0kl ,
                                                        p̈0k
                                              X                    X
                              0                  0
                             z(1J)(kJ)    =1−   z(1J)(kl)  =1−         p̈0kl = p̈0kJ .
                                                          l∈S                         l∈S

Therefore, for λ-almost all i ∈ I,

          P (βi0 = (k, l)) = P (βi0 = (k, l) | βi−1 = (1, J))P (βi−1 = (1, J)) = z(1J)(kl)
                                                                                  0
                                                                                           = p̈0kl

                                                                      70
for any k ∈ S, l ∈ S ∪ {J}. Part (3) of Theorem 4 implies the essential pairwise independence
of β 0 . Thus, we can simply start the dynamical system D from time zero instead of time −1
so that we can have an i.i.d. initial extended type process β 0 .

E.3       Proofs of Propositions 2 and 5

In this subsection, the unit interval [0, 1] will have a different notation in a different context.
Recall that (L, L, χ) is the Lebesgue unit interval, where χ is the Lebesgue measure defined on
the Lebesgue σ-algebra L. We shall prove Proposition 5 first. The proof of Proposition 2 then
follows easily.
          Note that the agent space used in the proof of Theorem 5 is a hyperfinite Loeb counting
probability space. As shown in Proposition 6, a hyperfinite index set of agents has the external
cardinality of the continuum. The purpose of Proposition 5 is to show that one can find some
extension of the Lebesgue unit interval as the agent space so that the associated version of
Theorem 5 still holds.
          Fix a Fubini extension (Iˆ × Ω, Î  F, λ̂  P ) as constructed in the proof of Theorem 5.
Following Appendix A of Sun and Zhang (2009) and Appendix B in Duffie and Sun (2012), we
can state the following lemma.52

Lemma 8 There exists a Fubini extension (I × Ω, I  F, λ  P ) such that:

 (1) The agent space (I, I, λ) is an extension of the Lebesgue unit interval (L, L, χ).

 (2) There exists a surjective mapping ϕ from I to Iˆ such that ϕ−1 (î) has the cardinality of
     the continuum for any î ∈ Iˆ and ϕ is measure preserving, in the sense that for any A ∈ Î,
         ϕ−1 (A) is measurable in I with λ[ϕ−1 (A)] = λ̂(A).

 (3) Let Φ be the mapping (ϕ, IdΩ ) from I × Ω to Iˆ × Ω, that is, Φ(i, ω) = (ϕ, IdΩ )(i, ω) =
         (ϕ(i), ω) for any (i, ω) ∈ I × Ω. Then Φ is measure preserving from (I × Ω, I  F, λ  P )
         to (Iˆ × Ω, Î  F, λ̂  P ) in the sense that for any V ∈ Î  F, Φ−1 (V ) is measurable in
         I  F with (λ  P )[Φ−1 (V )] = (λ̂  P )(V ).

          Denote the MCI dynamical system with parameters (b, q, σ, ς, θ) and a deterministic
initial condition, as constructed in proof of Theorem 5 by D̂. For that dynamical system, we
add a hat to the relevant type processes, matching functions, and partners’ type processes. We
shall follow the proof of Theorem 4 in Duffie and Sun (2012).

Proof of Proposition 5: Based on the dynamical system D̂ on the Fubini extension (Iˆ ×
  52
       Parts (2) and (3) of Lemma 8 are taken from Lemma 11 in Duffie and Sun (2012).



                                                      71
Ω, Î  F, λ̂  P ), we shall now define, inductively, a new dynamical system D on the Fubini
extension (I × Ω, I  F, λ  P ).
                                                      0                                                   0
      For any î, î0 ∈ Iˆ with î 6= î0 , let Θî,î be a bijection from ϕ−1 (î) to ϕ−1 (î0 ), and Θî ,î be
                                 0
the inverse mapping of Θî,î . This is possible since both ϕ−1 (î) and ϕ−1 (î0 ) have cardinality of
the continuum.
       Let α0 be the mapping α̂0 (ϕ) from I to S,

                                                          if π̂ 0 (ϕ(i)) = ϕ(i)
                                 
                                             i
                       π 0 (i) =    ϕ(i), π̂ 0 (ϕ(i))
                                   Θ                  (i) if π̂ 0 (ϕ(i)) 6= ϕ(i),

and g 0 (i) = α0 π 0 (i) = ĝ 0 (ϕ(i)). By the measure preserving property of ϕ in Lemma 8, we
                        

know that β 0 = (α0 , g 0 ) is I-measurable type function with distribution p̂0 on S × (S ∪ {J}).
       For each time period n ≥ 1, let ᾱn and αn be the respective mappings ᾱ
                                                                             ˆ n (Φ) and α̂n (Φ)
from I × Ω to S. Define mappings π̄ n , and π n from I × Ω to I such that for each (i, ω) ∈ I × Ω,

                                                             ˆωn (ϕ(i)) = ϕ(i)
                                  
                        n                    i            if π̄
                      π̄ (i, ω) =          ˆ n
                                    Θϕ(i), π̄ω (ϕ(i)) (i) if π̄
                                                             ˆωn (ϕ(i)) 6= ϕ(i)

                                                                      if π̂ωn (ϕ(i)) = ϕ(i)
                                        
                           n                             i
                          π (i, ω) =                   n (ϕ(i))
                                            Θϕ(i),   π̂ω          (i) if π̂ωn (ϕ(i)) 6= ϕ(i).

When πωn (ϕ(i)) 6= ϕ(i), πωn defines a full matching on ϕ−1 (Ĥωn ), where Ĥωn = Iˆ−{i : π̂ω (i)n = i},
which implies that πωn (i) 6= i. Hence, π n is a well-defined mapping from I × Ω to I. For the
same reason, π̄ n is well defined.
                                         ˆ n and α̂n are measurable mappings from (Iˆ × Ω, Î 
       Since Φ is measure-preserving and ᾱ
F, λ̂  P ) to S. By the definitions of ᾱn and αn , it is obvious that for each i ∈ I,

                                         ᾱin = ᾱ n
                                                ˆ ϕ(i) and αin = α̂ϕ(i)
                                                                   n
                                                                        .                                   (55)

       Next, we consider the property of π̄ n and π n . Fix any ω ∈ Ω. Let Hωn = I − {i : πin = i};
then Hωn = ϕ−1 (Ĥωn ). Pick any i ∈ Hωn and denote πωn (i) by j. Then, ϕ(i) ∈ Ĥωn . The
                                                     n                                n
definition of π n implies that j = Θϕ(i), π̂ω (ϕ(i)) (i). Since Θϕ(i), π̂ω (ϕ(i)) is a bijection between
Cϕ(i) and Cπ̂ωn (ϕ(i)) , it follows that ϕ(j) = ϕ(πωn (i)) = π̂ωn (ϕ(i)) by the definition of ϕ. Thus,
j = Θϕ(i), ϕ(j) (i). Since the inverse of Θϕ(i), ϕ(j) is Θϕ(j), ϕ(i) , we know that Θϕ(j), ϕ(i) (j) = i.
By the full matching property of π̂ωn , ϕ(j) 6= ϕ(i), ϕ(j) ∈ Ĥωn and π̂ωn (ϕ(j)) = ϕ(i). Hence, we
have j 6= i, and
                                                  n
                               πωn (j) = Θϕ(j), π̂ω (ϕ(j)) (j) = Θϕ(j), ϕ(i) (j) = i.

This means that the composition of πωn with itself on Hωn is the identity mapping on Hωn , which
also implies that πωn is a bijection on Hωn . Therefore πωn is a full matching on Hωn = I − {i :
πin = i}.


                                                             72
      We define g n : I × Ω → S ∪ {J} by
                                      (
                            n          αn (π n (i, ω))                     if π n (i, ω) 6= i
                           g (i, ω) =
                                       J                                   if π n (i, ω) = i.

As noted in the above paragraph, for any fixed ω ∈ Ω, ϕ(πωn (i)) = π̂ωn (ϕ(i)) for i ∈ Hωn . When
 / Hωn , we have ϕ(i) ∈
i∈                    / Ĥωn , and πωn (i) = i, π̂ωn (ϕ(i)) = ϕ(i). Therefore, ϕ(πωn (i)) = π̂ωn (ϕ(i))
for any i ∈ I. Then, for any (i, ω) such that πin 6= i,

         g n (i, ω) = α̂n (ϕ(π n (i, ω)), ω) = α̂n (π̂ n (ϕ(i), ω), ω) = ĝ n (ϕ(i), ω) = ĝ n (Φ)(i, ω).

For any (i, ω) such that πin = i,

                                 g n (i, ω) = J = ĝ n (ϕ(i), ω) = ĝ n (Φ)(i, ω).

We can prove that ḡ n (i, ω) = ḡˆn (Φ)(i, ω) and ḡ¯n (i, ω) = ḡ¯ˆn (Φ)(i, ω) in the same way. Hence,
the measure-preserving property of Φ implies that g n is I  F-measurable. The previous three
identities on the partners’ processes also mean that for any i ∈ I,

                        gin ( · ) = ĝϕ(i)
                                      n
                                           ( · ), ḡin ( · ) = ḡˆϕ(i)
                                                                  n
                                                                       ( · ), ḡ¯in ( · ) = ḡˆ¯ϕ(i)
                                                                                                n
                                                                                                     ( · ).

Since ᾱn = ᾱ
            ˆ n (Φ) and ḡ n = ḡˆn (Φ), Equation (13) implies that for λ-almost all i ∈ I,

P (ᾱin = k2 , ḡin = l2 | αin−1 = k1 , gin−1 = l1 ) = P (ᾱ n
                                                          ˆ ϕ(i)           n
                                                                 = k2 , ḡˆϕ(i)          n−1
                                                                                = l2 | α̂ϕ(i)          n−1
                                                                                              = k1 , ĝϕ(i) = l1 )
                                                             = bnk1 k2 bnl1 l2 .

Similarly, we can obtain that for λ-almost all i ∈ I,

                       P (ᾱin = k2 , ḡin = r | αin−1 = k1 , gin−1 = J) = bnk1 k2 δJ (r),



                               P (ḡ¯in = l | ᾱin = k, ḡin = J, p̌n ) = qkl
                                                                           n n
                                                                              (p̌ (ω)),


                         P (αin = l1 , gin = r | ᾱin = k1 , ḡ¯in = J) = δk1 (l1 ) δJ (r),


                        P (αin = l1 , gin = J | ᾱin = k1 , ḡ¯in = k2 ) = θkn1 k2 ςkn1 k2 (l1 ),


                  P (αin = l1 , gin = l2 | ᾱin = k1 , ḡ¯in = k2 ) = (1 − θkn1 k2 )σkn1 k2 (l1 , l2 ).

Therefore, D is a dynamical system with random mutation, directed random matching and
type changing with break-up and with the parameters (p0 , b, q, σ, ς, θ).

                                                               73
      It remains to check the Markov conditional independence for D. Since the dynamical
system D̂ is Markov conditionally independent, for each n ≥ 1, there is a set Iˆ0 ∈ Î with
λ̂(Iˆ0 ) = 1, and for each î ∈ Iˆ0 , there exists a set Êî ∈ Î with λ̂(Êî ) = 1, with Equations (23)
to (25) being satisfied for any î ∈ Iˆ0 and any ĵ ∈ Êî . Let I 0 = ϕ−1 (Iˆ0 ). For any i ∈ I 0 , let
Ei = ϕ−1 (Ê       Since ϕ is measure-preserving, λ(I 0 ) = λ(Ei ) = 1. Fix any i ∈ I 0 , and any
                 ϕ(i) ).
j ∈ Ei . Denote ϕ(i) by î and ϕ(j) by ĵ. Then, it is obvious that î ∈ Iˆ0 and ĵ ∈ Êî . Therefore
Equations (23) to (25) are satisfied for any i0 ∈ I 0 and any j 0 ∈ Ei0 . Therefore the dynamical
system D is Markov conditionally independent.
          Now, we check the measure-preserving property as stated in Footnote 34.53 Let I 0  F =
{B 0 ⊆ I × Ω : B 0 = Φ−1 (B) for some B ∈ Î  F}. By Lemma 8, I 0 and I 0  F are sub
σ-algebras of I and I  F respectively. Note that αn , ᾱn , g n , ḡ n and ḡ n are still measurable on
(I ×Ω, I 0 F, λP ), then the dynamical system D is also Markov conditionally independent on
(I × Ω, I 0  F, λ  P ). Note that, for each n ∈ N and ω ∈ Ω, π̂ωn and π̄  ˆωn are measure-preserving
from the Loeb space (I,   ˆ Î, λ̂) to itself. Therefore, for any A ∈ I 0 ,

               λ((πωn )−1 (A)) = λ̂(φ((πωn )−1 (A))) = λ̂((π̂ωn )−1 (φ(A))) = λ̂(φ(A)) = λ(A),

which implies that π n is measure preserving. We can prove that π̄ n is measure preserving on
(I × Ω, I 0  F, λ  P ) in the same way.
          By using exactly the same proof as that given at the end of the proof of Theorem 5, we
can have an i.i.d. (instead of deterministic) initial extended type process β 0 in the statement
of this proposition.

Proof of Proposition 2: In the proof of Proposition 5, take the initial extended type distri-
bution p̂0kl = pk δJ (l). Assume that there is no genuine random mutation. Then, it is clear that
p̃0kl = pk δJ (l) for any k ∈ S. Consider the random matching π 1 in period one.
        Fix an agent i with α0 (i) = k. We have P (ᾱi1 = k) = 1, P ḡ¯i1 = l = qkl and
                                                                                

P ḡ¯i1 = J = ηk . Similarly, Equation (24) implies that the process ḡ¯1 is essentially pair-
           

wise independent. By taking the type function α to be α0 , the matching function π to be π̄ 1 ,
and the associated process g to be ḡ¯1 , the proposition holds.




  53
       The measure-preserving property as stated in Footnotes 16 and 19 can be checked by using the same idea.


                                                       74
