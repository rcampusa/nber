                               NBER WORKING PAPER SERIES




                                   INFERENCE ON WINNERS

                                          Isaiah Andrews
                                          Toru Kitagawa
                                         Adam McCloskey

                                       Working Paper 25456
                               http://www.nber.org/papers/w25456


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                     January 2019




We thank Tim Armstrong, Stéphane Bonhomme, Raj Chetty, Gregory Cox, Áureo de Paula, Nathaniel
Hendren, Patrick Kline, Hannes Leeb, Anna Mikusheva, Magne Mogstad, José Luis Montiel Olea,
Mikkel Plagborg-Møller, Jack Porter, Adam Rosen, Frank Schoerfheide, Jesse Shapiro, and participants
at numerous seminars and conferences for helpful comments. We also thank Raj Chetty and Nathaniel
Hendren for extremely generous assistance on the application using data from Chetty et al. (2018),
and thank Jeff Rowley, Peter Ruhm, and Nicolaj Thor for outstanding research assistance. Andrews
gratefully acknowledges financial support from the NSF under grant number 1654234. Kitagawa gratefully
acknowledges financial support from the ESRC through the ESRC Centre for Microdata Methods
and Practice (CeMMAP) (grant number RES-589-28-0001) and the European Research Council (Starting
grant No. 715940). Initial version posted May 10, 2018. The views expressed herein are those of the
authors and do not necessarily reflect the views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2019 by Isaiah Andrews, Toru Kitagawa, and Adam McCloskey. All rights reserved. Short sections
of text, not to exceed two paragraphs, may be quoted without explicit permission provided that full
credit, including © notice, is given to the source.
Inference on Winners
Isaiah Andrews, Toru Kitagawa, and Adam McCloskey
NBER Working Paper No. 25456
January 2019, Revised in September 2020
JEL No. C12,C13

                                         ABSTRACT

Many empirical questions concern target parameters selected through optimization. For example,
researchers may be interested in the effectiveness of the best policy found in a randomized trial,
or the best-performing investment strategy based on historical data. Such settings give rise to a
winner’s curse, where conventional estimates are biased and conventional confidence intervals
are unreliable. This paper develops optimal confidence intervals and median-unbiased estimators
that are valid conditional on the target selected and so overcome this winner’s curse. If one
requires validity only on average over targets that might have been selected, we develop hybrid
procedures that combine conditional and projection confidence intervals to offer further
performance gains relative to existing alternatives.

Isaiah Andrews                                  Adam McCloskey
Department of Economics                         Department of Economics
Harvard University                              University of Colorado at Boulder
Littauer M18                                    256 UCB
Cambridge, MA 02138                             Boulder, CO 80309
and NBER                                        adam.mccloskey@colorado.edu
iandrews@fas.harvard.edu

Toru Kitagawa
Department of Economics
University College London
Gower Street
London WC1E 6BT
United Kingdom
and Institute For Fiscal Studies - IFS
t.kitagawa@ucl.ac.uk




A data appendix is available at http://www.nber.org/data-appendix/w25456
1       Introduction
A wide range of empirical questions involve inference on target parameters selected through
optimization over a finite collection of candidates. In a randomized trial considering
multiple treatments, for instance, one might want to learn about the true average effect
of the treatment that performed best in the experiment. In finance, one might want to
learn about the expected return of the trading strategy that performed best in a backtest.
     Estimators that do not account for data-driven selection of the target parameter can be
badly biased, and conventional t-test-based confidence intervals may severely under-cover.
To illustrate the problem, consider inference on the true average effect of the treatment
that performed best in a randomized trial.1 Since it ignores the data-driven selection of
the treatment of interest, the conventional estimate for this average effect will be biased
upwards. Similarly, the conventional confidence interval will under-cover, particularly when
the number of treatments considered is large. This gives rise to a form of winner’s curse,
where follow-up trials will be systematically disappointing relative to what we would expect
based on conventional estimates and confidence intervals. This form of winner’s curse has
previously been discussed in contexts including genome-wide association studies (e.g. Zhong
and Prentice, 2009; Ferguson et al., 2013) and online A/B tests (Lee and Shen, 2018).
     This paper develops estimators and confidence intervals that eliminate the winner’s
curse. There are two distinct perspectives from which to consider bias and coverage. The
first requires validity conditional on the target selected, for example on the identity of the
best-performing treatment, while the second is unconditional and requires validity on av-
erage over possible target parameters. Conditional validity is more demanding but may be
desirable in some settings, for example when one wants to ensure validity conditional on the
recommendation made to a policy maker. Both perspectives differ from inference on the ef-
fectiveness of the “true” best treatment, as in e.g. Chernozhukov et al. (2013) and Rai (2018),
in that we consider inference on the effectiveness of the (observed) best-performing treatment
    1
     Such a scenario seems to be empirically relevant, as a number of recently published randomized
trials in economics either were designed with the intent of recommending a policy or represent a direct
collaboration with a policy maker. For example, Khan et al. (2016) assess how incentives for property
tax collectors affect tax revenues in Pakistan, Banerjee et al. (2018) evaluate the efficacy of providing
information cards to potential recipients of Indonesia’s Raskin programme, and Duflo et al. (2018)
collaborate with the Gujarat Pollution Control Board (an Indian regulator tasked with monitoring
industrial emissions in the state) to evaluate how more frequent but randomized inspection of plants
performs relative to discretionary inspection. Baird et al. (2016) find that deworming Kenyan children
has substantial beneficial effects on their health and labor market outcomes into adulthood, and
Björkman Nyqvist and Jayachandran (2017) find that providing parenting classes to Ugandan mothers
has a greater impact on child outcomes than targeting these classes at fathers.


                                                   2
in the sample rather than the (unobserved) best-performing treatment in the population.2
    For conditional inference, we derive optimal median-unbiased estimators and equal-
tailed confidence intervals. We further show that in cases where the winner’s curse does
not arise (for instance because one treatment considered is vastly better than the others)
our conditional procedures coincide with conventional ones. Hence, our corrections do not
sacrifice efficiency in such cases.
    An alternative approach to conditional inference is sample splitting. In settings with
independent observations, choosing the target parameter using one subset of the data and
constructing estimates and confidence intervals using the remaining subset ensures unbiased-
ness of estimates and validity of conventional confidence intervals conditional on the target
parameter. The split-sample target parameter is necessarily more variable than the full-data
target, however. Moreover, since only a subset of the data is used for inference, split-sample
procedures are inefficient within the class of procedures with the same target. In the supple-
ment to this paper we build on our conditional inference results to develop computationally
tractable confidence intervals and estimators that dominate conventional sample-splitting.
    We next turn to unconditional inference. One approach to constructing valid uncondi-
tional confidence intervals is projection, applied in various settings by e.g. Romano and Wolf
(2005), Berk et al. (2013), and Kitagawa and Tetenov (2018a). To obtain a projection confi-
dence interval, we form a simultaneous confidence band for all potential targets and take the
implied set of values for the target of interest. The resulting confidence intervals have correct
unconditional coverage but, unlike our conditional intervals, are wider than conventional
confidence intervals even when the latter are valid. On the other hand, we find in simulation
that projection intervals outperform conditional intervals in cases where there is substantial
randomness in the target parameter, e.g. when there is not a clear best treatment.
    Since neither conditional nor projection intervals perform well in all cases, we introduce
hybrid estimators and confidence intervals that combine conditioning and projection. These
maintain most of the good performance of our conditional approach in cases for which the
winner’s curse does not arise, but also control the performance in cases where conditional
procedures underperform, e.g. by limiting the maximum length of hybrid intervals relative
to projection intervals.
    We derive our main results in the context of a finite-sample normal model with an
unknown mean vector and a known covariance matrix. This model can be viewed as
   2
    See Dawid (1994) for an early discussion of this distinction, and an argument in favor of inference
on the best-performing treatment in the sample.



                                                  3
an asymptotic approximation to many different non-normal finite-sample settings. To
formalize this connection, we show that feasible versions of our procedures, based on
non-normal data and plugging in estimated variances, are uniformly asymptotically valid
over a large class of data-generating processes.
    We illustrate our results with two applications. The first uses data from Karlan and List
(2007) to conduct inference on the effect of the best-performing treatment in an experiment
studying the impact of matching incentives on charitable giving. Simulations calibrated
to these data show that conventional estimates ignoring selection are substantially upward
biased, while our corrections reduce bias and increase coverage. Applied to the original
Karlan and List (2007) data, our corrections suggest substantially less optimism about the
effect of the best-performing treatment than conventional approaches, with point estimates
below the lower bound of the conventional confidence intervals.
    For our second application, we consider the problem of targeting neighborhoods based
on estimated economic mobility. In cooperation with the Seattle and King County public
housing authorities, Bergman et al. (2020) conduct an experiment encouraging housing
voucher recipients to move to high-opportunity neighborhoods, which are selected based on
census-tract level estimates of economic mobility from Chetty et al. (2018). We consider an
analogous exercise in the 50 largest commuting zones in the US, selecting top tracts based
on estimated economic mobility and examining conventional and corrected inference on the
average mobility in selected tracts. Calibrating simulations to the Chetty et al. (2018) data,
we find that conventional approaches suffer from severe bias in many commuting zones.
These biases are reduced, but not eliminated, by the empirical Bayes corrections used by
Chetty et al. (2018) and many others in the applied literature. Intuitively, commonly-
applied empirical Bayes approaches correspond to a normal prior on unit-level causal effects
conditional on covariates. Bayesian arguments (discussed in Appendix E) imply that these
methods correct the winner’s curse when the normal prior matches the distribution of true
effects, but not in general otherwise. Turning to the original Chetty et al. (2018) data,
our corrected estimates imply lower mobility, and higher uncertainty, for selected tracts
than conventional approaches, but nonetheless strongly indicate gains from moving to
selected commuting zones. Our confidence intervals likewise suggest substantially higher
uncertainty than empirical Bayes credible sets, though we do not find a clear ordering
between our bias-corrected point estimates of economic mobility and the empirical Bayes
point estimates of Chetty et al. (2018).
    It is important to emphasize that our goal is to evaluate the effectiveness of a recom-


                                              4
mended policy or treatment, taking the rule for selecting a recommendation as given, rather
than to improve the rule. Our procedures thus play a role similar to that of ex-post policy
evaluations, with the difference that we can produce estimates without waiting for a policy
to be implemented. Like ex-post evaluations, these estimates may be useful for a variety of
purposes, including understanding the true effectiveness of a selected policy and forecasting
the effects of future implementations. Our results are also useful in settings where ex-post
evaluation is possible, since comparison of our estimates with ex-post results can shed light
on whether differences between observed performance and ex-ante conventional estimates
can be explained solely by the winner’s curse.
Related Literature This paper is related to the literature on tests of superior predictive
 performance (e.g. White (2000); Hansen (2005); Romano and Wolf (2005)). That literature
 studies the problem of testing whether some strategy or policy beats a benchmark, while
we consider the complementary question of inference on the effectiveness of the estimated
“best” policy. Our conditional inference results combine naturally with the results of this
 literature, allowing one to condition inference on e.g. rejecting the null hypothesis that
 no policy outperforms a benchmark.
     Our results build upon, and contribute to, the rapidly growing literature on selective
 inference. Fithian et al. (2017) describe a general approach to construct optimal conditional
 confidence sets in wide range of settings, while a rapidly growing literature including e.g.
 Harris et al. (2016); Lee et al. (2016); Tian and Taylor (2018), and our own follow-up work
 in Andrews et al. (2020b), works out the details of this approach for a range of settings.
 Likewise, our analysis of conditional confidence intervals examines the implications of the
 conditional approach in our setting. Our results are also related to the growing literature
 on unconditional post-selection inference, including Berk et al. (2013); Bachoc et al. (2020);
 Kuchibhotla et al. (2020). This literature considers analogs of our projection confidence
 intervals for inference following model selection (see also Laber and Murphy, 2011).
     Beyond the new setting considered, we make three main theoretical contributions
 relative to the selective and post-selection inference literatures. First, when one only
 requires unconditional validity, we introduce the class of hybrid inference and estimation
 procedures. We find that hybrid procedures offer large gains in unconditional performance
 relative both to conditional procedures and to existing unconditional alternatives.3 Second,
 for settings where conditional inference is desired, we observe that the same structure used
   3
    A related hybridization, combining conditional and unconditional inference, is used in Andrews et al.
(2019) to improve power for tests of parameters identified by moment inequalities.


                                                   5
in the literature to develop optimal conditional confidence intervals also allows construction
of optimal quantile unbiased estimators, using results from Pfanzagl (1994) on optimal esti-
mation in exponential families.4 Third, our uniform asymptotic results have no counterpart
in the existing literature on conditional inference. In particular, Tibshirani et al. (2018)
and Andrews et al. (2020b) establish uniform asymptotic validity for conditional confidence
intervals based on similar ideas to ours, but only under particular local sequences. We do
not impose such restrictions, allowing us to cover a large class of data generating processes.
    Finally, there is a distinct but complementary literature that studies inference on ranks
based on some measure of interest. For example, this literature allows one to form valid
confidence intervals for the best performing unit, rather than for the performance of the
unit selected as best by the data. Conventional inference procedures for these problems fail
for similar reasons that give rise to a winner’s curse. Recent work by Mogstad et al. (2020)
overcomes this inference failure and studies, among other settings, inference on ranks in
neighborhood targeting, as in our second application.
    In the next section, we begin by introducing the problem we consider and the techniques
we propose in the context of a stylized example. Section 3 introduces the normal model,
develops our conditional procedures, and briefly discusses sample splitting. Section 4
introduces projection confidence intervals and our hybrid procedures. Section 5 translates
our normal model results to uniform asymptotic results. Finally, Sections 6 and 7 discuss
applications to data from Karlan and List (2007) and Bergman et al. (2020), respectively.
The supplement to this paper contains proofs of our theoretical results and additional
theoretical, numerical and empirical results.

2       A Stylized Example
We begin by illustrating the problem we consider, along with the solutions we propose, in
a stylized example. Suppose we have data from a randomized trial of a binary treatment
(e.g. participation in a job training program), where individuals i∈{1,...,n} were randomly
assigned to treatment (Di =1) or control (Di =0), with n2 individuals in each group. We
are interested in an outcome Yi (e.g. a dummy for employment in the next year), and
compute the treatment and control means,

                                            n           n
                                                                   !
                                         2 X         2 X
                        (Xn∗(1),Xn∗(0))=       DiYi,       (1−Di)Yi .
                                         n i=1       n i=1
    4
     Eliasz (2004) previously used results from Pfanzagl (1994) to study quantile-unbiased estimation
in a different setting, targeting coefficients on highly persistent regressors.

                                                 6
If trial particpants are a random sample from some population, then for Yi,1 and Yi,0 equal
to the potential outcomes for i under treatment and control, respectively, (Xn∗(1),Xn∗(0))
unbiasedly estimate the average potential outcomes (µ∗(1),µ∗(0))=(E[Yi,1],E[Yi,0]) in the
population.
     For policymakers and researchers interested in maximizing the average outcome, it
is natural to focus on the treatment that performed best in the experiment. Formally,
let Θ={0,1} denote the set of policies (just control and treatment in this example) and
define θ̂n = argmax θ∈Θ Xn∗(θ) as the policy yielding the highest average outcome in the
experiment. While Xn∗(θ) unbiasedly estimates µ∗(θ) for fixed policies θ, Xn∗(θ̂n) system-
atically over-estimates µ∗(θ̂n), since we are more likely to select a given policy when the
experiment over-estimates its effectiveness. Likewise, confidence intervals for µ∗(θ̂n) that
ignore estimation of θ may cover µ∗(θ̂n) less often than we intend. Hence, if a policymaker
deploys the treatment θ̂n, or a researcher examines it in a follow-up experiment, the results
will be systematically disappointing relative to the original trial. This is a form of winner’s
curse: estimation error leads us to over-predict the benefits of our chosen policy and to
misstate our uncertainty about its effectiveness.
     To simplify the analysis and develop corrected inference procedures, we turn to asymp-
                                  √                            √
totic approximations. For Xn = nXn∗ and (µn(1),µn(0))= n(µ∗(1),µ∗(0)), provided the
potential outcomes (Yi,0,Yi,1) have finite variance,
                                             !                             !!
                           Xn(0)−µn(0)                      Σ(0)  0
                                                 ⇒N 0,                         ,                     (1)
                           Xn(1)−µn(1)                       0   Σ(1)

where the asymptotic variance Σ can be consistently estimated while the scaled average
outcomes µn cannot be. Motivated by (1), let us abstract from approximation error and
assume that we observe
                            !               !                 !!
                       X(0)            µ(0)       Σ(0)     0
                              ∼N              ,
                       X(1)            µ(1)         0    Σ(1)

for Σ(0) and Σ(1) known, and that θ̂ =argmax θ∈ΘX(θ) with Θ={0,1}.5
    As discussed above, X(θ̂) is biased upwards as an estimator of µ(θ̂). This bias arises
both conditional on θ̂ and unconditionally. To see this note that θ̂ = 1 if X(1) > X(0),
   5
     Finite-sample results in this normal model correspond to asymptotic results for cases where the
difference in outcomes E[Yi,1] − E[Yi,0] is of order √1n , so the optimal policy θ∗ = argmax θ∈Θ µ∗(θ) is
weakly identified. We defer an in-depth discussion of asymptotics to Section 5.


                                                   7
where ties occur with probability zero. Conditional on θ̂ = 1 and X(0) = x(0), however,
X(1) follows a normal distribution truncated below at x(0). Since this holds for all x(0),
X(1) has positive median bias conditional on θ̂ =1:6
                                     n                 o 1
                                 P rµ X(θ̂)≥µ(θ̂)|θ̂ =1 > for all µ.
                                                         2

Since the same argument holds for θ̂ =0, θ̂ is also biased upwards unconditionally:
                                        n           o 1
                                    P rµ X(θ̂)≥µ(θ̂) > for all µ.
                                                      2

For similar reasons, conventional t-statistic-based confidence intervals likewise do not have
correct coverage.
    To illustrate these issues, Figure 1 plots the coverage of conventional confidence intervals,
as well as the median bias of conventional estimates, in an example with Σ(1)=Σ(0)=1.
For comparison we also consider cases with ten and fifty policies (e.g. additional treatments)
|Θ| = 10 and |Θ| = 50, where we again set Σ(θ) = 1 for all θ and for ease of reporting
assume that all the policies other than the first (policy θ1) are equally effective, with
average outcome µ(θ−1). The first panel of Figure 1 shows that while the conventional
confidence interval has reasonable coverage when there are only two policies, its coverage
can fall substantially when |Θ|=10 or |Θ|=50. The second panel shows that the median
bias of the conventional estimator µ̂=X(θ̂), measured as the deviation of the exceedance
probability P rµ{X(θ̂)≥µ(θ̂)} from 12 , can be quite large. The third panel shows that the
same is true when we measure bias as the median of X(θ̂)−µ(θ̂). In all cases we find that
performance is worse when we consider a larger number of policies, as is natural since a
larger number of policies allows more scope for selection.
    Our results correct these biases. Returning to the case with Θ={0,1} for simplicity, let
FT N (x(1);µ(1),x(0)) denote the truncated normal distribution function for X(1), truncated
below at x(0), when the true mean is µ(1). This function is strictly decreasing in µ(1),
and for µ̂α the solution to FT N (X(1);µ̂α,X(0))=1−α, Proposition 2 below shows that
                                      n                o
                                  P rµ µ̂α ≥µ(θ̂)|θ̂ =1 =α for all µ.

Hence, µ̂α is α-quantile unbiased for µ(θ̂) conditional on θ̂ =1, and the analogous statement
   6
       It also has positive mean bias, but we focus on median bias for consistency with our later results.



                                                      8
                       1.0        Unconditional coverage probability of Conventional 95% CIs
coverage probability

                       0.8




                                                                               |Θ| = 2
                       0.6




                                                                               |Θ| = 10
                       0.4




                                                                               |Θ| = 50



                              0            2                     4                  6            8

                                                          µ(θ1) − µ(θ−1)



                                        Unconditional median bias, Pr(X(θ^)>µ(θ^)) − 1/2
                       0.5
                       0.3
probability

                       0.1
                       −0.1




                              0            2                     4                  6            8

                                                          µ(θ1) − µ(θ−1)



                                         Unconditional median bias, Med(X(θ^) − µ(θ^))
                       2.5
                       1.5
    Median bias

                       0.5
                       −0.5




                              0            2                     4                  6            8

                                                          µ(θ1) − µ(θ−1)



              Figure 1: Performance of conventional procedures in examples with 2, 10, and 50 policies.


                                                             9
holds conditional on θ̂ =0. Indeed, Proposition 2 shows that µ̂α is the optimal α-quantile
unbiased estimator conditional on θ̂.
   Using this result, we can eliminate the biases discussed above. The estimator µ̂1/2 is me-
                                                                          
dian unbiased and the equal-tailed confidence interval CSET = µ̂α/2,µ̂1−α/2 has conditional
coverage 1−α, where we say that a confidence interval CS has conditional coverage 1−α if
                        n                o
                     P r µ(θ̂)∈CS|θ̂ = θ̃ ≥1−α for θ̃ ∈Θ and all µ.                      (2)

By the law of iterated expectations, CSET also has unconditional coverage 1−α:
                                n        o
                            P rµ µ(θ̂)∈CS ≥1−α for all µ.                                (3)

Unconditional coverage is easier to attain, so relaxing the coverage requirement from (2)
to (3) may allow tighter confidence intervals in some cases. Conditional and unconditional
coverage requirements address different questions, however, and which is more appropriate
depends on the problem at hand. For instance, if a researcher recommends the policy θ̂
to a policymaker, it may also be natural to report a confidence interval that is valid condi-
tional on the recommendation, which is precisely the conditional coverage requirement (2).
Conditional coverage ensures that if one considers repeated instances in which researchers
recommend a particular course of action (e.g. departure from the status quo), reported
confidence intervals will in fact cover the true effects a fraction 1−α of the time. On the
other hand, if we only want to ensure that our confidence intervals cover the true value
with probability at least 1−α on average across the distribution of recommendations, it
suffices to impose the unconditional coverage requirement (3).
    We are unaware of alternatives in the literature that ensure conditional coverage (2).
For unconditional coverage (3), however, one can form an unconditional confidence interval
by projecting a simultaneous confidence set for µ. In particular, let cα denote the 1−α
quantile of maxj |ξj | for ξ =(ξ1,ξ2)0 ∼N(0,I2) a two-dimensional standard normal random
vector. If we define CSP as
                                      q               q     
                        CSP = Y (θ̂)−cα Σ(θ̂),Y (θ̂)+cα Σ(θ̂) ,


this set has correct unconditional coverage (3).
    Figure 2 plots the median (unconditional) length of 95% confidence intervals CSET and
CSP , along with the conventional confidence interval, again in cases with |Θ|∈{2,10,50}.

                                             10
We focus on median length, rather than mean length, because the results for Kivaranovic
and Leeb (2020) imply that CSET has infinite expected length. As Figure 2 illustrates, the
median length of CSET is shorter than the (nonrandom) length of CSP in all cases when
|µ(θ1)−µ(θ−1)| exceeds four, and converges to the length of the conventional interval as
|µ(θ1)−µ(θ−1)| grows larger. When |µ(θ1)−µ(θ−1)| is small, on the other hand, CSET can
be substantially wider than CSP . Both features become more pronounced as we increase
the number of policies considered, and are still more pronounced for higher quantiles of the
length distribution. To illustrate, Figure 3 plots the 95th percentile of the distribution of
length in the case with |Θ|=50 policies, while results for other quantiles and specifications
are reported in Section F of the supplement.                   
     Figure 4 plots the median absolute error Medµ |µ̂−µ(θ̂)| for different estimators µ̂,
and shows that the median-unbiased estimator likewise exhibits larger median absolute
error than the conventional estimator X(θ̂) when |µ(θ1)−µ(θ−1)| is small. This feature is
again more pronounced as we increase the number of policies considered, or if we consider
higher quantiles as in Section F of the supplement.
     Recall that µ̂ 1 and the endpoints of CSET are optimal quantile unbiased estimators.
                    2
So long as we impose median unbiasedness and correct conditional coverage, there is hence
little scope to improve conditional performance. If we instead focus on unconditional bias
and coverage, by contrast, improved performance is possible.
     To improve performance, we consider hybrid inference, which combines the conditional
and unconditional approaches. Hybrid inference first computes a level β < α projection
interval CSPβ , and then considers conditional inference given θ̂ and µ(θ̂) ∈ CSPβ . In the
case with Θ={0,1}, for instance, if θ̂ =1 and the true mean is µ(1) then the conditional
distribution of X(1) given θ̂ =1, X(0)=x(0), and µ(1)∈CSPβ is a N(µ(1),Σ(1)) distribution
truncated to the interval
                      h   n            p    o        p    i
                       max x(0),µ(1)−cβ Σ(1) ,µ(1)+cβ Σ(1) .

For the corresponding distribution function FTHN (x(1);µ(1),x(0)), the hybrid estimator
µ̂H          H          H
  α solves FT N (X(1);µ̂α ,X(0))=1−α. Arguments analogous to those in the conditional
                    H
case
n imply that µ̂oα is α-quantile  n unbiased conditional
                                             o          on the (potentially false) event
                  β                        β
  θ̂ =1,µ(θ̂)∈CSP . Since P rµ µ(θ̂)∈CSP ≥ 1 − β one can further show that the




                                             11
                4.0 4.5 5.0 5.5 6.0              (a) 2 policies


                                                             Conditional ET       Conventional
median length




                                                             Projection           Hybrid with ET




                                      0   2              4                    6                    8

                                                  µ(θ1) − µ(θ2)



                                                (b) 10 policies
                10
median length

                8
                6
                4




                                      0   2              4                    6                    8

                                                  µ(θ1) − µ(θ−1)



                                                (c) 50 policies
                14
                12
median length

                10
                8
                6
                4




                                      0   2              4                    6                    8

                                                  µ(θ1) − µ(θ−1)



      Figure 2: Median length of confidence intervals for µ(θ̂) in cases with 2, 10, and 50 policies.


                                                    12
                                                                    Conditional ET

                  14
                                                                    Projection
                                                                    Conventional
                                                                    Hybrid with ET
                  12
                  10
         length

                  8
                  6
                  4




                       0              2                  4           6               8

                                                   µ(θ1) − µ(θ−1)



 Figure 3: 95th percentile of length of confidence intervals for µ(θ̂) in case with 50 policies.


unconditional quantile bias of µ̂H
                                 α is bounded, in the sense that

                                    n          o
                                P rµ µ̂H
                                       α ≥µ(θ̂)  −α ≤β·max{α,1−α}.

We again form level 1−α equal-tailed confidence intervals based on these estimates, where
to account for the dependence on the
                                     projection interval we adjust the quantile considered
           H
and take CSET = µ̂Hα−β ,µ̂H
                          1−         α−β      . See Section 4.2 for details on this adjustment. By
                           2(1−β)   2(1−β)

construction, hybrid intervals are never longer than the level 1−β projection interval CSPβ .
    Due to their dependence on the projection interval, hybrid intervals do not in general
have correct conditional coverage (2). By relaxing the conditional coverage requirement,
however, we obtain major improvements in unconditional performance, as illustrated in
Figure 2. In particular, we see that in the cases with 10 and 50 policies, the hybrid
confidence intervals have shorter median length than the unconditional interval CSP for
all parameter values considered. The gains relative to conditional confidence intervals are
large for many parameter values, and are still more pronounced for higher quantiles of the
length distribution, as in Figure 3 and Section F of the supplement. In Figure 4 we report
results for the hybrid estimator µ̂H1 , and again find substantial performance improvements.
                                          2



                                                    13
                                                              (a) 2 policies


                                                                                                 ^=µ
                                                                                Median unbiased: µ ^ 1/2
Median absolute errors

                         1.0




                                                                                         ^ = X(θ^)
                                                                                Maximum: µ
                         0.8




                                                                                Hybrid
                         0.6
                         0.4




                               0                 2                     4                        6          8

                                                                µ(θ1) − µ(θ2)



                                                              (b) 10 policies
Median absolute errors

                         1.4
                         1.0
                         0.6




                               0                 2                     4                        6          8

                                                                µ(θ1) − µ(θ−1)



                                                              (c) 50 policies
                         2.5
Median absolute errors

                         2.0
                         1.5
                         1.0
                         0.5




                               0                 2                     4                        6          8

                                                                µ(θ1) − µ(θ−1)



                     Figure 4: Median absolute error of estimators of µ(θ̂) in cases with 2, 10, and 50 policies.


                                                                  14
                               1.0
                               0.8
        coverage probability




                                                                             Conditional ET
                               0.6



                                                                             Projection
                                                                             Conventional
                                                                             Hybrid with ET
                               0.4
                               0.2
                               0.0




                                          −4         −2             0         2               4

                                                             µ(θ1) − µ(θ2)



                               Figure 5: Coverage conditional on θ̂ =θ1 in case with two policies.


    The improved unconditional performance of the hybrid confidence intervals is achieved
by requiring only unconditional, rather than conditional, coverage. To illustrate, Figure
5 plots the conditional coverage given θ̂ =θ1 in the case with two policies. As expected,
the conditional interval has correct conditional coverage, while coverage distortions appear
for the hybrid and projection intervals when µ(θ1)µ(θ2). In this case θ̂ =θ2 with high
probability but the data will nonetheless sometimes realize θ̂ = θ1. Conditional on this
event, X(θ1) will be far away from µ(θ1) with high probability, so projection and hybrid
confidence intervals under-cover.

3    Conditional Inference
This section introduces our general setting, which extends the stylized example of the
previous section in several directions, and develops conditional inference procedures. We
then discuss sample splitting as an inefficient conditional inference method and briefly
discuss the construction of dominating procedures. Finally, we show that our conditional
procedures converge to conventional ones when the latter are valid.




                                                               15
3.1         Setting
Suppose we observe a collection of normal random vectors (X(θ),Y (θ))0 ∈ R2 for θ ∈ Θ
                                                                       0
where Θ is a finite set. For Θ = θ1,...,θ|Θ| , let X = X(θ1),...,X θ|Θ|     and Y =
                  0
 Y (θ1),...,Y θ|Θ| . Then
                                                        !
                                                    X
                                                            ∼N(µ,Σ)                                     (4)
                                                    Y
for                                     "           !#                          !
                                            X(θ)                       µX (θ)
                                    E                    =µ(θ)=                   ,
                                            Y (θ)                      µY (θ)
                                                            !                       !              !!
                               ΣX (θ,θ̃) ΣXY (θ,θ̃)                       X(θ)            X(θ̃)
                 Σ(θ,θ̃)=                                       =Cov                  ,             .
                               ΣY X (θ,θ̃) ΣY (θ,θ̃)                      Y (θ)           Y (θ̃)
We assume that Σ is known, while µ is unknown and unrestricted unless noted otherwise.
For brevity of notation, we abbreviate Σ(θ,θ) to Σ(θ). We assume throughout that
ΣY (θ)>0 for all θ ∈Θ, since the inference problem we study is trivial when ΣY (θ)=0. As
discussed in Section 5 below, this model arises naturally as an asymptotic approximation.
     We are interested in inference on µY (θ̂), where θ̂ is determined based on X. We define
θ̂ through the level maximization,7

                                                θ̂ =argmax X(θ).                                        (5)
                                                         θ∈Θ


In a follow-up paper, Andrews et al. (2020b), we develop results on inference when θ̂
instead maximizes kX(θ)k and X(θ) may be vector-valued.
    We are interested in constructing estimates and confidence intervals for µY (θ̂) that are
valid either conditional on the value of θ̂ or unconditionally. In many cases, as in Section 2
above, we are interested in the mean of the same variable that drives selection, so X =Y and
µX =µY . In other settings, however, we may select on one variable but want to do inference
on the mean of another. Continuing with the example discussed in Section 2, for instance,
we might select θ̂ based on outcomes for all individuals, but want to conduct inference
on average outcomes for some subgroup defined based on covariates. In this case, Y (θ)
corresponds to the estimated average outcome for the group of interest under treatment θ.
      7
          For simplicity of notation we assume θ̂ is unique almost surely unless noted otherwise.




                                                          16
3.2      Conditional Inference
We first consider conditional inference, seeking estimates of µY (θ̂) which are quantile
unbiased conditional on θ̂:
                              n                    o
                          P rµ µ̂α ≥µY (θ̂)|θ̂ = θ̃ =α for all θ̃ ∈Θ and all µ.               (6)

   Since θ̂ is a function
                      n of X,owe can re-write the conditioning event in terms of the
sample space of X as X : θ̂ = θ̃ =X (θ̃).8 Thus, for conditional inference we are interested
in the distribution of (X,Y ) conditional on X ∈X (θ̃). Our results below imply that the
elements of Y other than Y (θ̃) do not help in constructing a quantile-unbiased estimate
or confidence interval for µY (θ̂) conditional on X ∈X (θ̃). Hence, we limit attention to the
conditional distribution of (X,Y (θ̃)) given X ∈X (θ̃).
    Since (X,Y (θ̃)) is jointly normal unconditionally, it has a multivariate truncated normal
distribution conditional on X ∈X (θ̃). Correlation between X and Y (θ̃) implies that the
conditional distribution of Y (θ̃) depends on both the parameter of interest µY (θ̂) and
µX . To eliminate dependence on the nuisance parameter µX , we condition on a sufficient
statistic. Without truncation and for any fixed µY (θ̃), a minimal sufficient statistic for µX is
                                                             
                                   Zθ̃ =X − ΣXY (·,θ̃)/ΣY (θ̃) Y (θ̃),                        (7)

where we use ΣXY (·,θ̃) to denote Cov(X,Y (θ̃)). Zθ̃ corresponds to the part of X that
is (unconditionally) orthogonal to Y (θ̃) which, since (X,Y (θ̃)) are jointly normal, means
that Zθ̃ and Y (θ̃) are independent. Truncation breaks this independence,nbut Zθ̃ remains
                                                                                       o
minimal sufficient for µX . The conditional distribution of Y (θ̂) given θ̂ = θ̃,Zθ̃ =z is
truncated normal:
                              Y (θ̂)|θ̂ = θ̃,Zθ̃ =z ∼ξ|ξ ∈Y(θ̃,z),                      (8)
                           
where ξ ∼N µY (θ̃),ΣY (θ̃) is normally distributed and

                                     n                                o
                             Y(θ̃,z)= y :z+ ΣXY (·,θ̃)/ΣY (θ̃) y ∈X (θ̃)                      (9)

is the set of values for Y (θ̃) such that the implied X falls in X (θ̃) given Zθ̃ =z. Thus, con-
ditional on θ̂ = θ̃, and Zθ̃ =z, Y (θ̂) follows a one-dimensional truncated normal distribution
   8
       If θ̂ is not unique we change the conditioning event from θ̂ = θ̃ to θ̃ ∈argmaxX(θ).



                                                      17
with truncation set Y(θ̃,z).
   The following result, based on Lemma 5.1 of Lee et al. (2016), characterizes Y(θ̃,z).

Proposition 1
Let ΣXY (θ̃)=Cov(X(θ̃),Y (θ̃)). Define
                                                                                   
                                                            ΣY (θ̃) Zθ̃ (θ)−Zθ̃ (θ̃)
                    L(θ̃,Zθ̃ )=           max                                           ,
                                  θ∈Θ:ΣXY (θ̃)>ΣXY (θ̃,θ)    ΣXY (θ̃)−ΣXY (θ̃,θ)
                                                                                   
                                                            ΣY (θ̃) Zθ̃ (θ)−Zθ̃ (θ̃)
                    U(θ̃,Zθ̃ )=            min                                          ,
                                  θ∈Θ:ΣXY (θ̃)<ΣXY (θ̃,θ)    ΣXY (θ̃)−ΣXY (θ̃,θ)
and                                                                            
                       V(θ̃,Zθ̃ )=            min             − Zθ̃ (θ)−Zθ̃ (θ̃) .
                                     θ∈Θ:ΣXY (θ̃)=ΣXY (θ̃,θ)
                           h               i
If V(θ̃,z)≥0, then Y(θ̃,z)= L(θ̃,z),U(θ̃,z) . If V(θ̃,z)<0, then Y(θ̃,z)=∅.

   Thus, Y(θ̃,z) is an interval bounded above and belown by functionso of z. While we must
have V(θ̃,z)≥0 for this interval to be non-empty, P rµ V(θ̂,Zθ̂ )<0 =0 for all µ so this
constraint holds almost surely when we consider the value θ̂ observed in the data. Hence,
in applications we can safely ignore this constraint and calculate only L(θ̂,Zθ̂ ) and U(θ̂,Zθ̂ ).
    Using this result, it is straightforward to construct quantile-unbiased estimators for
µY (θ̂). Let FT N (y;µY (θ̃),θ̃,z) denote the distribution function for the truncated normal
distribution (8). This function is strictly decreasing in µY (θ̃). Define µ̂α as the unique
solution to FT N (Y (θ̂);µ̂α,θ̃,Zθ̃ )=1−α. Proposition 2 below shows that µ̂α is conditionally
α-quantile-unbiased in the sense of (6), so µ̂ 1 is median-unbiased while the equal-tailed
                                                2
interval CSET = µ̂α/2,µ̂1−α/2 has conditional coverage 1−α
                       n                     o
                    P r µY (θ̂)∈CSET |θ̂ = θ̃ ≥1−α for θ̃ ∈Θ and all µ.                      (10)

Moreover results in Pfanzagl (1979) and Pfanzagl (1994) on optimal estimation for expo-
nential families imply that µ̂α is optimal in the class of quantile-unbiased estimators.
   To establish optimality, we add the following assumption:

Assumption 1
If Σ = Cov((X 0,Y 0)0) has full rank, then the parameter space for µ is R2|Θ|. Otherwise,


                                                    18
                                                           n      1
                                                                               o
there exists some µ∗ such that the parameter space for µ is µ∗ +Σ 2 v :v ∈R2|Θ| , where
  1
Σ 2 is the symmetric square root of Σ.

This assumption requires that the parameter space for µ be sufficiently rich. When Σ is
degenerate (for example when X = Y , as in Section 2), this assumption further implies
that (X,Y ) have the same support for all values of µ. This rules out cases in which a pair
of parameter values µ1, µ2 can be perfectly distinguished based on the data. Under this
assumption, µ̂α is an optimal quantile-unbiased estimator.

Proposition 2
For α∈(0,1), µ̂α is conditionally α-quantile-unbiased in the sense of (6). If Assumption 1
holds, then µ̂α is the uniformly most concentrated α-quantile-unbiased estimator, in that for
                                                                                             
any other conditionally α-quantile-unbiased estimator µ̂∗α and any loss function L d,µY (θ̃)
that attains its minimum at d=µY (θ̃) and is quasiconvex in d for all µY (θ̃),
                      h                    i   h                      i
                    Eµ L µ̂α,µY (θ̃) |θ̂ = θ̃ ≤Eµ L µ̂∗α,µY (θ̃) |θ̂ = θ̃

for all µ and all θ̃ ∈Θ.

Proposition 2 shows that µ̂α is optimal in the strong sense that it has lower risk (expected
loss) than any other quantile-unbiased estimator for a large class of loss functions.
Other Selection Events We have discussed inference conditional on θ̂ = θ̃, but the
same approach applies, and is optimal, for more general conditioning events. For instance,
in the context of Section 2 a researcher might deliver a recommendation to a policymaker
only when a statistical test indicates that the best-performing treatment outperforms some
benchmark (see Tetenov, 2012). In this case, it is natural to also condition inference on
the result of this test. Analogously, one may wish to conduct inference on the performance
of an estimated best trading strategy or forecasting rule after finding a rejection when
testing for superior predictive ability according to methods of e.g. White (2000), Hansen
(2005) or Romano and Wolf (2005). Section A of the supplement discusses the conditional
approach in this more general case and derives the additional conditioning event in the
context of the example just described.
Uniformly Most Accurate Unbiased Confidence Intervals In addition to equal-
tailed confidence intervals, classical results on testing in exponential families discussed in
Fithian et al. (2017) also permit the construction of uniformly most accurate unbiased

                                              19
confidence intervals. A level 1−α confidence set is unbiased if its probability of covering
a false parameter value is bounded above by 1−α, and uniformly most accurate unbiased
confidence intervals minimize the probability of covering all incorrect parameter values
over the class of unbiased confidence sets. Details of how to construct these confidence
intervals are deferred to Section A of the supplement for brevity.
3.3    Comparison to Sample Splitting
An alternative remedy for winner’s curse bias is to split the sample. If we have iid observa-
tions and select θ̂1 based on the first half of the data, conventional estimates and confidence
intervals for µY (θ̂1) that use only the second half of the data will be conditionally valid
given θ̂1. Hence, it is natural to ask how our conditioning approach compares to this
conventional sample splitting approach.
    Asymptotically, even sample splits yield a pair of independent and identically distributed
normal draws (X 1,Y 1) and (X 2,Y 2), both of which follow (4), albeit with a different
scaling for (µ,Σ) than in the full-sample case.9 Sample splitting procedures calculate θ̂1 as
in (5), replacing X by X 1. Inference on µY (θ̂1) is then conducted using Y 2. In particular,
the conventional 95% sample-splitting confidence interval for µY (θ̂1),
                                     q                     q        
                           2 1               1   2 1               1
                          Y (θ̂ )−1.96 ΣY (θ̂ ),Y (θ̂ )+1.96 ΣY (θ̂ ) ,


has correct (conditional) coverage and Y 2(θ̂1) is median-unbiased for µY (θ̂1).
    Conventional sample splitting resolves the winner’s curse, but comes at a cost. First,
  1
θ̂ is based on less data than in the full-sample case, which is unappealing since a policy
recommendation estimated with a smaller sample size leads to a lower expected welfare
(see, e.g., Theorems 2.1 and 2.2 in Kitagawa and Tetenov (2018b)). Moreover, even after
conditioning on θ̂1, the full-sample average 12 (X 1,Y 1)+ 12 (X 2,Y 2) remains minimal sufficient
for µ. Hence, using only Y 2 for inference sacrifices information.
    Fithian et al. (2017) formalize this point and show that conventional sample splitting
tests (and thus confidence intervals) are inefficient.10 Motivated by this result, in Section
C of the supplement we derive optimal estimators and confidence intervals that are
   9
      Section C of the supplement considers cases with general sample splits and describes the scaling
for (µ,Σ). Intuitively, the scope for improvement over conventional split-sample inference is increasing
in the fraction of the data used to construct X1.
   10
      Corollary 1 of Fithian et al. (2017) applied in our setting shows that for any sample splitting test
based on Y 2, there exists a test that uses the full data and has weakly higher power against all alternatives
and strictly higher power against some alternatives.


                                                     20
valid conditional on θ̂1. These optimal split-sample procedures involve distributions that
are difficult to compute, however, so we also propose computationally straightforward
alternatives. These alternatives dominate conventional split-sample methods, but are in
turn dominated by the (intractable) optimal split-sample procedures.
                              n     o
3.4 Behavior When P rµ θ̂ = θ̃ is Large
As discussed in Section 2, if we ignore selection and compute the conventional (or “naive”)
estimator µ̂N =Y (θ̂) and the conventional confidence interval
                                     q                     q       
                   CSN = Y (θ̂)−cα/2,N ΣY (θ̂),Y (θ̂)+cα/2,N ΣY (θ̂) ,

where cα,N is the 1−α-quantile of the standard normal distribution, µ̂N is biased
                                                                                n     and
                                                                                      o CSN
has incorrect coverage conditional on θ̂ = θ̃. These biases are mild when P rµ θ̂ = θ̃ is close
to one, however, sincenin thiso case the conditional distribution is close to the unconditional
one. Intuitively, P rµ θ̂ = θ̃ is close to one for some θ̃ when µX (θ) has a well-separated
maximum. Our procedures converge to conventional ones in this case.

Proposition 3                                     n       o
Consider any sequence of values µm such that P rµm θ̂ = θ̃ →1. Then under µm we have
CSET →p CSN and µ̂ 1 →p Y (θ̃) both conditional on θ̂ = θ̃ and unconditionally, where for
                      2
confidence intervals →p denotes convergence in probability of the endpoints.

    This result provides an additional argument for using our procedures: they remain
valid when conventional procedures fail, but coincide with conventional procedures when
the latter are valid. On the other hand, as we saw in Section 2, there are cases where our
conditional procedures have poor unconditional performance.

4    Unconditional Inference
Rather than requiring validity conditional on θ̂, one might instead require coverage only
on average, yielding the unconditional coverage requirement
                                n          o
                             P r µY (θ̂)∈CS ≥1−α for all µ.                               (11)

All confidence intervals with correct conditional coverage in the sense of (10) also have
correct unconditional coverage provided θ̂ is unique with probability one.



                                              21
Proposition 4
Suppose that θ̂ is unique with probability one for all µ. Then any confidence interval CS
with correct conditional coverage (10) also has correct unconditional coverage (11).

Uniqueness of θ̂ implies that the conditioning events X (θ̃) partition the support of X with
measure zero overlap. The result then follows from the law of iterated expectations.
    A sufficient condition for almost sure uniqueness of θ̂ is that ΣX has full rank. A weaker
sufficient condition is given in the next lemma. Cox (2018) gives sufficient conditions for
uniqueness of a global optimum in a much wider class of problems.

Lemma 1                                                                                    
Suppose that for all θ, θ̃ ∈Θ such that θ =
                                          6 θ̃, either V ar X(θ)|X(θ̃) =6 0 or V ar X(θ̃)|X(θ) =6
0. Then θ̂ is unique with probability one for all µ.

    While the conditional confidence intervals derived in the last section are unconditionally
valid, unconditional coverage is less demanding than conditional coverage. Hence, if we
are only concerned with unconditional coverage, relaxing the coverage requirement may
allow us to obtain shorter confidence intervals in some settings.
    This section explores the benefits of such a relaxation. We begin by introducing un-
conditional confidence intervals based on projections of simultaneous confidence bands for
µ. We then introduce hybrid estimators and confidence intervals that combine projection
intervals with conditioning arguments.
4.1   Projection Confidence Intervals
One approach to obtain an unconditional confidence interval for µY (θ̂) is to start with
a joint confidence interval for µ and project on the dimension corresponding to θ̂. This
approach was used by Romano and Wolf (2005) in the context of multiple testing, and by
Kitagawa and Tetenov (2018a) for inference on an estimated optimal policy. This approach
has also been used in a large and growing statistics literature on post-selection inference
including e.g. Berk et al. (2013), Bachoc et al. (2020) and Kuchibhotla et al. (2020).
    To formally describe the projection approach, let cα denote the 1 − α quantile of
            p
maxθ |ξ(θ)|/ ΣY (θ) for ξ ∼N(0,ΣY ). If we define
                       n                     p                   o
                  CSµ = µY :|Y (θ)−µY (θ)|≤cα ΣY (θ) for all θ ∈Θ ,




                                             22
then CSµ is a level 1−α confidence set for µY . If we then define
     n                                           o          q                 q       
CSP = µ̃Y (θ̂):∃µ∈CSµ such that µY (θ̂)= µ̃Y (θ̂) = Y (θ̂)−cα ΣY (θ̂),Y (θ̂)+cα ΣY (θ̂)


as the projection of CSµ on the parameter space for µY (θ̂), then since µY ∈CSµ implies
µY (θ̂) ∈ CSP , CSP satisfies the unconditional coverage requirement (11). As noted in
Section 2, however, CSP does not generally have correct conditional coverage.
    The width of the confidence interval CSP depends on the variance ΣY (θ̂) but does not
otherwise depend on the data. To account for the randomness of θ̂, the critical value cα
is typically larger than the conventional two-sided normal critical value. Hence, CSP will
be conservative in cases where θ̂ takes a given value θ̃ with high probability. To improve
performance in such cases, we propose a hybrid inference approach.
4.2     Hybrid Inference
As shown in Section 2, the conditional and projection approaches each have good un-
conditional performance in some cases, but neither is fully satisfactory. Hybrid inference
combines the approaches to obtain good performance over a wide range of parameter values.
    To construct hybrid estimators, we condition both on θ̂ = θ̃ and on the event that
µY (θ̂) lies in the level 1−β projection confidence interval CSPβ for 0≤β <α. Hence, the
conditioning event becomes
                                                 q                  q       
                H
              Y (θ̃,µY (θ̃),z)=Y(θ̃,z)∩ µY (θ̃)−cβ ΣY (θ̃),µY (θ̃)+cβ ΣY (θ̃) .


Let FTHN (y;µY (θ̃), θ̃,z) denote the conditional distribution function of Y (θ̃), and define
µ̂H           H             H                                       H
  α to solve FT N (Y (θ̂);µ̂α ,θ̂,Zθ̃ )=1−α. The hybrid estimator µ̂α is α-quantile unbiased
conditional on µ(θ̂)∈CSPβ .

Proposition 5
                                          β
For α ∈ (0,1), µ̂H                 H                                                    H
                 α is unique and µ̂α ∈ CSP . If θ̂ is unique almost surely for all µ, µ̂α is
α-quantile unbiased conditional on µY (θ̂)∈CSPβ :
                            n                         o
                               H                    β
                        P rµ µ̂α ≥µY (θ̂)|µY (θ̂)∈CSP =α for all µ.

      Proposition 5 implies several notable properties for the hybrid estimator. First, since



                                              23
    n            o
P rµ µY (θ̂)∈CSPβ ≥1−β by construction, one can show that

                                 n            o
                                    H
                             P rµ µ̂α ≥µY (θ̂) −α ≤β·max{α,1−α} for all µ.

This implies that the absolute median bias of µ̂H1 (measured as the deviation of the ex-
                                                             2

ceedance probability from 1/2) is bounded above by β/2. On the other hand, since µ̂H1 ∈CSPβ
                         q                                                          2

          H                                                   H
we have µ̂ 1 −Y (θ̂) ≤cβ ΣY (θ̃), so the difference between µ̂ 1 and the conventional esti-
               2                                                            2

mator Y (θ̂) is bounded above by half the width of CSPβ . As β varies, the hybrid estimator in-
terpolates between the median-unbiased estimator µ̂ 1 and the conventional estimator Y (θ̂).
                                                       2
    As with the quantile-unbiased estimator µ̂α, we can form confidence intervals based
on hybrid estimators. In particular, the set [µ̂H        H
                                                 α/2 ,µ̂1−α/2 ] has coverage 1−α conditional
on µY (θ̂) ∈ CSPβ . This is not fully satisfactory, however, as P rµ{µY (θ̂) ∈ CSPβ } < 1.
Hence, to ensure
                  correct coverage,
                                     we define the level 1 − α hybrid confidence interval
     H
as CSET = µ̂Hα−β ,µ̂H
                    1−           α−β     . With this adjustment, hybrid confidence intervals have
                    2(1−β)      2(1−β)

coverage at least 1−α both conditional on µY (θ̂)∈CSPβ and unconditionally.

Proposition 6
                                                                                       H
Provided θ̂ is unique with probability one for all µ, the hybrid confidence interval CSET
has coverage 1−α
              1−β
                  conditional on µY (θ̂)∈CSPβ :

                             n                          o 1−α
                                        H
                         P rµ µY (θ̂)∈CSET |µY (θ̂)∈CSPβ =     for all µ.
                                                           1−β
                                                                        1−α
Moreover, the unconditional coverage is between 1−α and                 1−β
                                                                            ≤1−α+β:

                        n             o              n             o 1−α
                                   H                            H
                inf P rµ µY (θ̂)∈CSET   ≥1−α, supP rµ µY (θ̂)∈CSET  ≤     .
                 µ                             µ                      1−β

    Hybrid confidence intervals strike a balance between the conditional and projection
approaches. The maximal length of hybrid confidence intervals is bounded above by
the length of CSPβ . For small β, hybrid confidence intervals will be close to conditional
confidence intervals, and thus to conventional confidence intervals, when θ̂ = θ̃ with high
probability. However, for β >0, hybrid
                                   n      confidence
                                           o         intervals do not fully converge to conven-
tional confidence intervals as P rµ θ̂ = θ̃ →1.11 Nevertheless, our simulations in Section 2
  11
       Indeed, one can directly choose β to yield a given maximal power loss for the hybrid tests relative to


                                                      24
find similar performance for the hybrid and conditional approaches in well-separated cases.
    While hybrid confidence intervals combine the conditional and projection approaches,
they can yield overall performance more appealing than either. In Section 2 we found
that hybrid confidence intervals had a shorter median length for many parameter values
than did either the conditional or projection approaches used in isolation. Our simulation
results below provide further evidence of outperformance in realistic settings.
Comparison to Bonferroni Adjustment It is worth contrasting our hybrid approach
with Bonferroni corrections as in e.g. Romano et al. (2014) and McCloskey (2017). A
simple Bonferroni approach for our setting intersects a level 1−β projection confidence
interval CSPβ with a level 1 − α + β conditional interval that conditions only on θ̂ = θ̃.
Bonferroni intervals differ from our hybrid approach in two respects. First, they use a
level 1−α+β conditional confidence interval, while the hybrid approach uses a level 1−α1−β
conditional interval, where 1−α
                             1−β
                                 ≤1−α+β.     Second, the  conditional interval used by the
                                                         β
Bonferroni approach does not condition on µY (θ̃)∈CSP , while that used by the hybrid
approach does. Consequently, hybrid confidence intervals exclude the endpoints of CSPβ
almost surely, while the same is not true of Bonferroni intervals.

5    Large-Sample Performance
Our results so far have assumed that (X,Y ) are jointly normal with known variance Σ.
While exact normality is rare in practice, researchers frequently use estimators that are
asymptotically normal with consistently estimable asymptotic variance. Our results for
the finite-sample normal model translate to asymptotic results in this case.
     Specifically, suppose that for sample size n we construct a vector of statistics Xn, that
θ̂n =argmax θ∈ΘXn(θ), and that we are interested in the mean of Yn(θ̂n). In the treatment
                                                                                           √
choice example discussed in Section 2, for instance, θ indexes treatments, Xn(θ) is n
times the sample average outcome under treatment θ, and Yn(θ) = Xn(θ). We suppose
that (Xn,Yn) are jointly asymptotically normal once recentered by vectors (µX,n,µY,n),
                                                     !
                                        Xn −µX,n
                                                         ⇒N(0,Σ).
                                        Yn −µY,n
                                                   √
In the treatment choice example µX,n(θ) = µY,n(θ) = nE[Yi,θ ] is the average potential
conditional tests in the well-separated case. Such a choice of β will depend on Σ, however. For simplicity
we instead use β =α/10 in our simulations. Romano et al. (2014) and McCloskey (2017) find this choice
to perform well in two different settings when using a Bonferroni correction.


                                                   25
                                           √
outcome under treatment θ, scaled by n. We further assume that we have a consistent
estimator Σb for the asymptotic variance Σ. In the treatment choice example we can take
b to be the matrix with the sample variance of the outcome in each the treatment group
Σ
along the diagonal and zeros elsewhere.
     More broadly, (Xn,Yn) can be any vectors of asymptotically normal estimators, and
we can calculate Σ  b as we would for inference on (µX,n,µY,n), including corrections for
clustering, serial correlation, and the like in the usual way.12 Feasible inference based on
our approach simply substitutes (Xn,Yn) and Σ     b in place of (X,Y ) and Σ in all expressions.
Feasible finite-sample estimators and confidence intervals are denoted as their counterparts
in Sections 3–4, with the addition of an n subscript.
     We show that this plug-in approach yields asymptotically valid inference on µY,n(θ̂n).
This result is trivial when the sequence of vectors µX,n has a well-separated maximizer
θ∗ = argmax θ∈Θ µX,n(θ) with µX,n(θ∗)  maxθ∈Θ\θ∗ µX,n(θ) for large n, since in this case
θ̂n =θ∗ with high probability, and the selection problem vanishes. In the example of Section
2, for instance, if we fix a data-generating process with E[Yi,1]>E[Yi,0] and take n→∞,
then P r{θ̂n =1}→1.
     Based on this argument, it could be tempting to conclude that inference ignoring the
winner’s curse may be approximately valid so long as there is not an exact tie for the
treatment yielding the highest average outcome. In finite samples, however, near-ties yield
very similar behavior to exact ties. Moreover, no matter how large the sample size, we can
find near-ties sufficiently close that inference ignoring selection remains unreliable. Hence,
what matters for inference is neither whether there are exact ties, nor the sample size as
such (beyond the minimum needed to justify the normal approximation), but instead how
close the best-performing treatments are to each other relative to the degree of sampling
uncertainty. Depending on the data generating process, selection issues can thus remain
important no matter how large the sample. To obtain reliable large-sample approximations,
we thus seek uniform asymptotic results, which for sufficiently large samples guarantee
performance over a wide class of data generating processes.
     We suppose that the sample of size n is drawn from some (unknown) distribution
P ∈Pn, where we require that the class of data generating processes Pn satisfy uniform
versions of the conditions discussed above. We first impose that (Xn,Yn) are uniformly
  12
                                                     √
     In particular, while we have scaled (Xn,Yn) by n for expositional purposes, dropping this scaling
yields inference on the correspondingly scaled version of µY,n(θ̂n). Hence, one can use estimators and
variances with the natural scale in a given setting.



                                                 26
asymptotically normal under P ∈ Pn, where the centering vectors (µX,n,µY,n) and the
limiting variance Σ may depend on P .
Assumption 2
For the class of Lipschitz functions that are bounded in absolute value by one and have
Lipschitz constant bounded by one, BL1, there exist sequences of functions µX,n(P ) and
µY,n(P ) and a function Σ(P ) such that for ξP ∼N(0,Σ(P )),
                                   "                     !#
                                          Xn −µX,n(P )
                 lim sup sup EP f                             −E[f(ξP )] =0.
                n→∞P ∈Pn f∈BL1            Yn −µY,n(P )
Uniform convergence in bounded Lipschitz metric is one formalization for uniform conver-
gence in distribution. When Xn and Yn are scaled sample averages based on independent
data, as in Section 2, Assumption 2 will follow from moment bounds, while for dependent
data it will follow from moment and dependence bounds.
   We next assume that the asymptotic variance is uniformly consistently estimable.
Assumption 3
The estimator Σ
              b n is uniformly consistent in the sense that for all ε>0
                                          n                o
                           lim sup P rP       b n −Σ(P ) >ε =0.
                                              Σ
                          n→∞P ∈Pn


Provided we use a variance estimator appropriate to the setting (e.g. the sample variance
for iid data, long-run variance estimators for time series, and so on) Assumption 3 will
follow from the same sorts of sufficient conditions as Assumption 2.
     Finally, we restrict the asymptotic variance.
Assumption 4
There exists a finite λ̄>0 such that

                1/λ̄≤ΣX (θ;P ),ΣY (θ;P )≤ λ̄, for all θ ∈Θ and all P ∈Pn,

     1/λ̄≤ΣX (θ;P )−ΣX (θ,θ̃;P )2/ΣX (θ̃;P ) for all θ,θ̃ ∈Θ with θ =
                                                                    6 θ̃ and all P ∈Pn.
The upper bounds on ΣX (θ;P ) and ΣY (θ;P ) ensure that the random variables ξP in
Assumption 2 are stochastically bounded, while the lower bounds ensure that each entry
(Xn,Yn) has a nonzero asymptotic variance. The second condition ensures that no two
elements of Xn are perfectly correlated asymptotically, and hence, by Lemma 1, guarantees
that θ̂n is unique with probability tending to one.

                                              27
High-Dimensional Settings Our asymptotic analysis considers settings where |Θ|,
and hence the dimension of Xn and Yn, are fixed as n→∞. One might also be interested
in settings where |Θ| grows with n, but this will raise complications for both the normal
approximation and estimation of the asymptotic variance. Such an extension is interesting,
but beyond the scope of this paper.
Variance Estimation Practically, even for fixed |Θ| one might still worry about the
difficulty of estimating Σ in finite samples, since this matrix has |Θ|(|Θ|+1)/2 entries. For-
tunately, in many cases Σ has additional structure which renders variance estimation more
tractable than in the fully general case. Suppose, for instance, that we want to conduct in-
ference on the best-performing treatment from a randomized trial, as in Section 2 above and
Section 6 below. In this case, provided trial participants are drawn independently, elements
of Xn(θ) corresponding to distinct treatments are uncorrelated and Σ is diagonal. In other
cases, such as Section 7 below, |Θ| may be large, but the elements of Xn are formed by tak-
ing combinations of a much lower-dimensional set of random variables. In this case, ΣX can
be written as a known linear transformation of a much lower-dimensional variance matrix.
5.1    Uniform Asymptotic Validity
In the finite-sample normal model, we study both conditional and unconditional properties
of our
   n methods.
            o     We would like to do the same in our asymptotic analysis, but may have
P r θ̂n = θ̃ →0 for some θ̃, in which case conditioning on θ̂n = θ̃ is problematic. To address
this, we multiply conditional statements by the probability of the conditioning event.
    Asymptotic uniformity results for conditional inference procedures were established
by Tibshirani et al. (2018) and Andrews et al. (2020b) for settings where the target
parameter is chosen in other ways. Their results, however, limit attention to classes of data
generating processes with asymptotically bounded means (µX,n,µY,n). This rules out e.g.
the conventional pointwise asymptotic case that fixes P and takes n→∞. We do not require
such boundedness. Moreover, the results of Tibshirani et al. (2018) do not cover quantile-
unbiased estimation, and also do not cover hybrid procedures, which are new to the literature.
13
   Our proofs are based on subsequencing arguments as in D. Andrews et al. (2020a),
though due to the differences in our setting (our interest in conditional inference, and the
fact that our target is random from an unconditional perspective) we cannot directly apply
their results. We first establish the asymptotic validity of our quantile-unbiased estimators.

  13
    In a follow-up paper, Andrews et al. (2020b), we apply the conditional and hybrid approaches
developed here to settings where θ̂ =argmaxkX(θ)k.


                                              28
Proposition 7
Under Assumptions 2-4, for µ̂α,n the α-quantile unbiased estimator,
                           n                         o       n        o
               lim sup P rP µ̂α,n ≥µY,n θ̂n;P |θ̂n = θ̃ −α P rP θ̂n = θ̃ =0,                  (12)
              n→∞P ∈Pn


for all θ̃ ∈Θ, and                   n                o
                         lim sup P rP µ̂α,n ≥µY,n θ̂n;P −α =0.                                (13)
                         n→∞P ∈Pn

This immediately implies asymptotic validity of equal-tailed confidence intervals.
Corollary 1
Under Assumptions 2-4, for CSET,n the level 1−α equal-tailed confidence interval
                      n                         o           n        o
          lim sup P rP µY,n θ̂n;P ∈CSET,n|θ̂n = θ̃ −(1−α) P rP θ̂n = θ̃ =0,
          n→∞P ∈Pn


for all θ̃ ∈Θ, and
                                 n                o
                     lim sup P rP µY,n θ̂n;P ∈CSET,n −(1−α) =0.
                     n→∞P ∈Pn


    We can likewise establish uniform asymptotic validity of projection confidence intervals.
Proposition 8
Under Assumptions 2-4, for CSP,n the level 1−α projection confidence interval,
                                       n               o
                        liminf inf P rP µY,n θ̂n;P ∈CSP,n ≥1−α.                               (14)
                         n→∞ P ∈Pn


                                                                            
   To state results for hybrid estimators and confidence intervals, let CnH θ̃;P =
 n                        o
                         β
1 θ̂n = θ̃,µY,n θ̂n;P ∈CSP,n be an indicator for the hybrid conditioning event that
θ̂n is equal to θ̃ and the parameter of interest µY (θ̃) falls in the level β projection confidence
              β
interval CSP,n    . We can establish quantile unbiasedness of hybrid estimators given this
event, along with bounded unconditional bias.
Proposition 9
                                                                                     β
Under Assumptions 2-4, for µ̂H
                             α,n the α-quantile unbiased hybrid estimator based on CSP,n ,

                  n                      o        n  o
      lim sup P rP µ̂α,n ≥µY,n θ̂n;P |Cn θ̃;P =1 −α EP CnH θ̃;P =0,
                     H                 H
                                                                                              (15)
      n→∞P ∈Pn


                                                29
 for all θ̃ ∈Θ, and
                                 n                   o
                  limsup sup P rP µ̂H
                                    α,n ≥µY,n  θ̂n ;P    −α ≤max{α,1−α}β.                                 (16)
                   n→∞ P ∈Pn


Validity of hybrid estimators again implies validity of hybrid confidence intervals.

Corollary 2
                             H
Under Assumptions 2-4, for CSET,n the level 1−α equal-tailed hybrid confidence interval
           β
based on CSP,n,

                 n                      o 1−α       n  o
                                H     H
     lim sup P rP µY,n θ̂n;P ∈CSET,n|Cn θ̃;P =1 −     EP CnH θ̃;P =0, (17)
    n→∞P ∈Pn                                      1−β

for all θ̃ ∈Θ,                           n                 o
                                                        H
                          liminf inf P rP µY,n θ̂n;P ∈CSET,n   ≥1−α,                                      (18)
                           n→∞ P ∈Pn

and                               n                 o 1−α
                                                 H
                   limsup sup P rP µY,n θ̂n;P ∈CSET,n  ≤     ≤1−α+β.                                      (19)
                     n→∞ P ∈Pn                           1−β

Hence, our procedures are uniformly asymptotically valid, unlike conventional inference.14

6        Application: Charitable Giving
Karlan and List (2007) partner with a political charity to conduct a field experiment
examining the effectiveness of matching incentives at increasing charitable giving. In
matched donations, a lead donor pledges to ‘match’ any donations made by other donors
up to some threshold, effectively lowering the price of political activism for other donors.
    Karlan and List (2007) use a factorial design. Potential donors, who were previous
donors to the charity, were mailed a four page letter asking for a donation. The contents
of the letter were randomized, with one third of the sample assigned to a control group
that received a standard letter with no match. The remaining two thirds received a letter
with the line “now is the time to give!” and details for a match. Treated individuals were
randomly assigned with equal probability to one of 36 separate treatment arms. Treatment
arms are characterized by a match ratio, a match size and an ask amount, for which further
    14
     The bootstrap also fails to deliver uniform validity, as it implicitly tries to estimate the difference be-
tween the “winning” policy and the others, which cannot be done with sufficient precision. We are unaware
of results for subsampling, m-out-of-n bootstrap, or other resampling-based approaches for this setting.




                                                      30
Index Treatment Description
0       0            Control group with no matched donations
Match ratio
1       1:1          An additional dollar up to the match limit
2       2:1          Two additional dollars up to the match limit
3       3:1          Three additional dollars up to the match limit
Match size
1       $25,000      Up to $25,000 is pledged
2       $50,000      Up to $50,000 is pledged
3       $100,000     Up to $100,000 is pledged
4       Unstated     The pledged amount is not stated
Ask amount
1       Same         The individual is asked to give as much as their largest past donation
2       25% more     The individual is asked to give 25% more than their largest past
                     donation
3       50% more     The individual is asked to give 50% more than their largest past
                     donation


Table 1: Treatment arms for Karlan and List (2007). Individuals were assigned to the control
group or to the treatment group, in the ratio 1:2. Treated individuals were randomly assigned
a match ratio, a match size and an ask amount with equal probability. There are 36 possible
combinations, plus the control group. The leftmost column specifies a reference index used
throughout this section for convenience.


details are given in Table 1. The outcome of interest is the average dollar amount that
individuals donated to the charity in the month following the solicitation.
    In total, 50,083 individuals were contacted, of which 16,687 were randomly assigned to
the control group, while 33,396 were randomly assigned to one of the 36 treatment arms.
The (unconditional) average donation was $0.81 in the control group and $0.92 in the
treatment group. Conditional on giving, these figures were $45.54 and $44.35, respectively.
The discrepancy reflects the low response rate; only 1,034 of 50,083 individuals donated.
    Table 2 reports average revenue from the four best-performing treatment arms, along
with standard errors and conventional confidence intervals. Taken at face value, the
results for the best-performing arm suggest that a similarly-situated nonprofit considering
a campaign that promises a dollar-for-dollar match up to $100,000 in donations and asks


                                             31
                      Table 2: Letter content and its effect on donations

               Treatment Average donation Standard error                95% CI
               (1,3,2)                 1.52                0.35        [0.83,2.20]
               (2,1,3)                 1.51                0.46        [0.61,2.41]
               (2,1,1)                 1.42                0.39        [0.66,2.19]
               (3,1,3)                 1.40                0.36        [0.70,2.11]

Note: n=50,083. The average donations for the four best treatment arms according to the data.
Treatments are indexed by the indicators for (Match ratio, Match size, Ask amount) defined
in Table 1. The reported 95% confidence intervals do not account for selection.

                                              Winner
                   (1,3,2)   (1,4,2)     (2,1,1) (2,1,3)     (2,2,2)   (3,1,3)
                  16.00% 11.39% 13.04% 18.92% 10.81% 10.04%


Table 3: The percent of simulation replications where each treatment is estimated to perform
best in simulations calibrated to Karlan and List (2007). Treatments are indexed by the
indicators for (Match ratio, Match size, Ask amount) defined in Table 1. 31 of the 37 treatments
are best in at least one replication; the six most frequently represented treatments are reported.


individuals to donate 25% more than their largest past donation could expect to raise
$1.52 per potential donor, on average, with a confidence interval of $0.83 to $2.20. This
estimate and confidence interval are clearly subject to winner’s curse bias, however: we are
picking the best-performing arm out of 37 in the experiment, which will bias our estimates
and confidence intervals upward.
Simulation Results To investigate the extent of winner’s curse bias and the finite-
sample performance of our corrections, we calibrate simulations to this application. We
simulate datasets by resampling observations with replacement from the Karlan and List
(2007) data (i.e. by drawing nonparametric bootstrap samples). In each simulated sample
we re-estimate the effectiveness of each treatment arm, pick the best-performing arm, and
study the performance of estimates and confidence intervals, treating the estimates for the
original Karlan and List (2007) data as the true values. Since the underlying data here
are non-normal and we re-estimate the variance in each simulation draw, these results also
speak to the finite-sample performance of the normal approximation. We report results
based on 10,000 simulation draws.
    There is substantial variability in the “winning” arm: 31 of the 37 treatments won in at

                                               32
                                                             Estimate
                                          Conventional Median unbiased Hybrid
             Median bias                       0.61                 -0.18            -0.18
             Probability bias                  0.50                 -0.07            -0.07
             Median absolute error             0.61                  0.65             0.64


Table 4: Performance measures for alternative estimators in simulations calibrated to Karlan
and List (2007). Probability bias is P r{µ̂>µ(θ̂)}− 12 .


least one simulation draw and six treatment arms won in at least 10% of simulated samples.
Table 3 lists the treatments that “won” in at least 10% of simulated samples. The variability
of the winning arm suggests that there is high potential for a winner’s curse in this setting.
    Table 4 examines the performance of naive, median unbiased, and hybrid estimates,
reporting (unconditional) median bias, probability bias (P r{µ̂>µ(θ̂)}− 12 ), and median
absolute error. Naive estimates suffer from substantial bias in this setting: they have
median bias $0.61, and over-estimate the revenue generated by the selected arm 100%
of the time, up to rounding. The median unbiased and hybrid estimators substantially
improve both measures of bias, though given the finite-sample setting they do not eliminate
it completely and are both somewhat downward biased.15 All three estimators perform
similarly in terms of median absolute error.
    Table 5 reports results for confidence intervals. Specifically, we consider naive, pro-
jection, conditional, and hybrid confidence intervals with nominal coverage 95%. Naive
confidence intervals slightly undercover, with coverage 92%. Projection confidence intervals
  15
     This is a particularly challenging setting for the normal approximation, as the outcomes distribution
is highly skewed due to the large number of zeros. In particular, there are on average only 20 nonzero
outcomes per non-control treatment (out of approximately 930 observations in each treatment group).


                                           Coverage Median length
                             Naive CS         0.92             1.88
                             CSP              1.00             3.08
                             CS               0.97             5.91
                             CS H             0.97             2.56


Table 5: Coverage probability and median length in simulations calibrated to Karlan and List
(2007).


                                                   33
                       Treatment (1,3,2) Estimates Equal-tailed CI
                       Naive                       1.52         [0.83,2.20]
                       Projection                    –          [0.40,2.63]
                       Conditional                -7.49        [-47.66,1.42]
                       Hybrid                      0.20         [0.19,1.47]


Table 6: Naive and bias-corrected estimates and confidence intervals for best-performing
treatment in Karlan and List (2007) data.


overcover, with coverage 100%. Conditional confidence intervals slightly over-cover, with
coverage 97%, but have median length more than three times that of naive confidence
intervals. Hybrid confidence intervals again have coverage 97%, but are substantially
shorter than conditional confidence intervals, with median length around 35% more than
that for naive intervals. The hybrid confidence intervals are also shorter than projection
confidence intervals by around 20%. The good performance of the hybrid approach is
encouraging, and in line with the results in Section 2.
Empirical results Returning to the Karlan and List (2007) data, Table 6 reports
corrected estimates and confidence intervals for the best-performing treatment in the
experiment. We repeat the naive estimate and confidence interval for comparison. The
median unbiased estimate makes an aggressive downwards correction to the naive estimate,
suggesting negative revenue from the winning arm.16 The conditional confidence interval
is extremely wide, ranging from -$47.66 to $1.42. The hybrid estimate also shifts the
conventional estimate downwards, but much less so. Moreover, the hybrid confidence
interval is no wider than the naive interval, and excludes both zero and the naive estimate.
These results suggest that future fundraising campaigns deploying the winning strategy
in the experiment are likely to raise some revenue, but substantially less than would be
expected based on the naive estimates.
  16
     Since revenue does not account for the cost of the fund-raising campaign, it is impossible for the
solicitation to raise a negative amount. This discrepancy reflects the fact that the median unbiased
estimator was developed for the unconstrained parameter space R, whereas the natural parameter space
here is R+. A simple fix is to censor the estimator to max{µ̂,0}. If µ̂ is median unbiased and the true
value is greater than zero, the trimmed estimator is also median unbiased.




                                                  34
7    Application: Neighborhood Effects
We next discuss simulation and empirical results based on Chetty et al. (2018) and Bergman
et al. (2020). Earlier work, including Chetty and Hendren (2016a) and Chetty and Hendren
(2016b) argues that the neighborhood in which a child grows up has a long-term causal
impact on income in adulthood, and, moreover, that these impacts are closely related to
the adult income of children who spend their entire childhood in a given neighborhood.
    Motivated by these findings, Bergman et al. (2020) partnered with the public housing
authorities in Seattle and King County in Washington State in an experiment helping
housing voucher recipients move to a set of higher-opportunity target neighborhoods.
Bergman et al. (2020) choose target neighborhoods based on the Chetty et al. (2018) “Op-
portunity Atlas.” This atlas compiles census-tract level estimates of economic mobility for
communities across the United States. Bergman et al. (2020) define target neighborhoods
by selecting approximately the top third of tracts in Seattle and King County based on
estimated economic mobility.17 They then make “relatively minor” adjustments to the set
of target tracts based on other criteria (Bergman et al., 2020, Appendix A).
    A central question in this setting is whether families moving to the target tracts will in
fact experience the positive outcomes predicted based on the Opportunity Atlas estimates
and the hypothesis of neighborhood effects. Once long-term outcomes for the experimental
sample are available, one can begin address this question by comparing outcomes for children
in treated families to the Opportunity Atlas estimates used to select the target tracts in
the first place. Such a comparison is complicated by the winner’s curse, however: the Atlas
estimates were already used to select the target tracts, so the naive estimate for the causal
effect of the selected tracts will be systematically biased upwards. It is therefore useful to
examine the extent of the winner’s curse, and the impact of our corrections, in this setting.
    Motivated by related issues, Chetty et al. (2018) and Bergman et al. (2020) do not
focus on naive estimates, but instead adopt a shrinkage or empirical Bayes approach. Their
estimates correspond to Bayesian posterior means under a prior that takes tract-level
economic mobility to be normally distributed conditional on a vector of observable tract
characteristics, and then estimates mean and variance hyperparameters from the data. If
one takes this prior seriously and abstracts from estimation of the hyperparameters (for
instance because the number of tracts is large and we plug in consistent estimates), the
posterior median for average economic mobility over selected tracts will be median-unbiased
  17
     They measure economic mobility in terms of the average household income rank in adulthood for
children growing up at the 25th percentile of the income distribution. See Chetty et al. (2018) for details.


                                                    35
under the prior, and Bayesian credible sets will have correct coverage, again under the
prior. See Section E in the supplement for further discussion. The efficacy of Bayesian
approaches for correcting selection issues hinges crucially on correct specification of the
prior, however, whereas our results ensure correct coverage and controlled median bias for
all possible distributions of economic mobility across tracts. Throughout this section, we
include empirical Bayes procedures in our analysis as a point of comparison.18
Simulation Results To examine the extent of winner’s curse bias and the performance
of different corrections, we calibrate simulations to the Opportunity Atlas data. For each of
the 50 largest commuting zones (CZs) in the United States we treat the (un-shrunk) tract-
level Opportunity Atlas estimates as the true values. We then simulate estimates by adding
normal noise with standard deviation equal to the Opportunity Atlas standard error.19
    We select the top third of tracts in each commuting zone based on these simulated
estimates.20 To cast this into our setting, let T be the set of tracts in a given CZ and
Θ the set of selections from T containing one third of tracts, Θ = {θ ⊂T :|θ|=b|T |/3c}.
For µ̂t the estimated effect of tract t, define X(θ) as the average estimate over tracts
                   1
                      P
in θ, X(θ) = |θ|         t∈θ µ̂t . Target tracts are selected as θ̂ = argmax θ∈Θ X(θ), and the
naive estimate for the average effect over selected tracts is X(θ̂). Correspondingly, for
µt the neighborhood effect for tract t, let µX (θ) be the average neighborhood effect over
                            1
                               P
tracts in θ, µX (θ) = |θ|         t∈θ µt . We are interested in the difference between the average
quality of selected tracts and the average over all tracts in the same commuting zone,
µY (θ̂) = |1θ̂| t∈θ̂ µt − |T1 | t∈T µt, and so define Y (θ) = |θ|1              1             21
               P               P                                   P                P
                                                                     t∈θ µ̂t − |T |  t∈T µ̂t .   We study
  18
      Armstrong et al. (2020) propose an approach to robustify empirical Bayes confidence intervals to the
choice of priors. Applied in the present setting, this approach would ensure correct coverage for tract-level
economic mobility on average across all tracts in a given commuting zone. This approach does not focus
on high-ranking tracts, however, and so does not (and is not intended to) address the winner’s curse.
Hence, we do not report results for this approach.
   19
      We base our estimates in this setting on the public Opportunity Atlas estimates and standard errors
since we do not have access to the underlying microdata. We also do not have access to the correlation
structure of the estimate across tracts. Such correlations arise from individuals who move across tracts, and
there are few movers between most pairs of tracts, so we expect that these omitted correlations are small.
   20
      We select the target tracts based on the un-shrunk estimates, rather than shrunk estimates as in
Bergman et al. (2020). We do this because we find that selecting based on un-shrunk estimates yields
slightly higher average quality for selected tracts than selecting on shrunk estimates, and because selection
based on shrunk estimates introduces nonlinearity (due to estimation of the hyperparameters) which
complicates conditional and hybrid inference.
   21
      Under the neighborhood effects model, µY (θ̂) corresponds to the average effect of moving a household
from a randomly selected tract in the CZ to a randomly selected target tract. This need not correspond
to the average treatment effect from the experiment in Bergman et al. (2020), since treatment and control
households are not in general uniformly distributed across these sets of tracts. Indeed, some treated


                                                    36
the performance of naive estimates and confidence intervals, empirical Bayes estimates
and credible sets, and our corrected estimates and confidence intervals.
    Figure 6 reports results based on ten thousand simulation draws. Panel (a) plots the av-
erage
  h truei upward mobility for selected tract less the average over all tracts in the same CZ,
E µY (θ̂) , across the 50 CZs considered. Selected tracts are better than average across all
50 CZs, though the precise degree of improvement varies. Panel (b) shows median bias for
the estimators we consider, where the quantity of interest is again the difference between av-
erage upward mobility for selected tracts, less the average over all tracts in the same CZ. As
expected the naive estimator is biased upwards, while sign of the bias for empirical Bayes dif-
fers across CZs. The conditional estimator is median unbiased up to simulation error, while
the hybrid estimator is very close to median unbiased. Panel (c) plots the median absolute
estimation error across the four estimators. The naive estimator has the largest median abso-
lute estimation error in most CZs, while the empirical Bayes typically has the smallest. The
conditional and hybrid estimators are in the middle, with quite similar median absolute esti-
mation errors for this application. Finally, panels (d) and (e) plot the coverage and median
length of confidence intervals. We see that the naive confidence interval severely under-
covers, with coverage close to zero in all 50 CZs. The coverage of empirical Bayes intervals
differs widely across CZs, ranging from less than 1% to over 90%.22 Conditional confidence
intervals have coverage equal to 95% up to simulation error in all CZs, while the hybrid inter-
vals have coverage very close to 95%, and below 96%, in all cases. Turning to median length,
we see that hybrid intervals are longer than empirical Bayes and naive confidence intervals,
but are considerably shorter than conditional intervals in many cases. For clarity of presenta-
tion we do not show results for projection intervals, but their average coverage across the 50
CZs is equal to 100% up to simulation error, while their average length is approximately 1.39.
Empirical Results Figure 7 plots results for the Opportunity Atlas data. As in the
simulations we select the top third of census tracts in each CZ based on the naive estimates
and then report naive, empirical Bayes, and hybrid estimates and intervals for the average
upward mobility across selected tracts, less the average over the commuting zone. For
households settle in non-target tracts, and some control households settle in target tracts. Given realized
location choices for treatment and control households, one could re-define µY (θ̂) accordingly. We do not
pursue this extension, however, as data on location choice under treatment only exists for the Seattle
CZ, where Bergman et al. (2020) conducted their experiment.
   22
      If one selects target tracts based on the empirical Bayes, rather than naive, estimates, this reduces
the bias of EB estimates, with the average median bias across the 50 CZs falling from approximately
-0.053 to approximately -0.029. Empirical Bayes credible sets continue to under-cover, however, with
average coverage rising from approximately 50% to approximately 59%.


                                                    37
Figure 6: Simulation results from calibration to Chetty et al. (2018) Opportunity Atlas. Panel
(a) shows the distribution of average improvement in economic mobility in selected tracts, relative
to within-CZ average, across 50 largest CZs. An effect size of 1 corresponds to a 1 percentage
point increase in predicted household income in adulthood for a child spending their entire
childhood in a given tract. Panel (b) shows the median bias of different estimators across the 50
CZs. Panel (c) plots the median absolute error across the same CZs. Panel (d) shows coverage
of confidence intervals across the 50 largest CZs, while panel (e) plots their median length.


visibility, we defer results for conditional and projection intervals to Figure 8 of Section
E of the supplement. From these results, we see that both the empirical Bayes and hybrid
adjustments shift the naive estimates and intervals downward. There is not a clear pattern
to the shifts in the estimates: in some cases the empirical Bayes estimate is below the
hybrid, while in other cases the order is reversed. As expected given our simulation results,
the (coverage-maintaining) hybrid intervals are wider than the (under-covering) empirical
Bayes intervals. Hybrid intervals nonetheless exclude zero in all CZs, suggesting that, under


                                                38
the hypothesis of neighborhood effects, there is real scope for selecting better neighborhoods
based on the Opportunity Atlas, albeit less than the naive estimates suggest.23
    The results for conditional and projection procedures in Figure 8 of Section E of the
supplement are qualitatively similar, but the projection intervals are generally quite wide,
and the width of the conditional intervals is extremely variable across CZs. Specifically,
while conditional intervals are quite similar to hybrid intervals in some CZs, they are much
longer in others.24 Projection intervals exclude zero in all cases, again suggesting that
selected tracts are better than average in each commuting zone. Conditional intervals lie
above zero in 20 of the 50 commuting zones, but include zero in the other 30. Hence, if
we are satisfied with unconditional coverage we find strong evidence that selected tracts
are better than average, while if we demand conditional coverage results are more mixed,
and depend on which commuting zone we consider.

8        Conclusion
This paper considers a form of the winner’s curse that arises when we select a target
parameter for inference based on optimization. We propose confidence intervals and
quantile unbiased estimators for the target parameter that are optimal conditional on its
selection. We hence recommend our conditional inference procedures when it is appropriate
to remove uncertainty about the choice of target parameters from inferential statements.
These conditionally valid procedures are also unconditionally valid, but we find that they
sometimes have unappealing (unconditional) performance relative to existing alternatives.
If one is satisfied with correct unconditional coverage and (in the case of estimation) a
small, controlled degree of bias, we propose hybrid procedures which combine conditioning
    23
      It is useful to compare our results with those of Mogstad et al. (2020), who study the problem of
inference on ranks and consider the Opportunity Atlas data for Seattle as an example. They show that
if one forms simultaneous confidence sets for individual tracts, one can say very little about which tracts
are best. Hence, we can say little about the effect of moving an individual from an arbitrary non-target
tract to arbitrary target tract, and can likewise say little about the average treatment effect of shifting
households from one group of tracts to the other if we allow arbitrary location choices within each group
of tracts. We consider a complementary exercise, inference on the average quality of selected sets of
tracts, corresponding to an average treatment effect under uniformly distributed location choices. For this
problem, we find strong evidence that selected tracts are, as a group, better than average. These exercises
answer different questions, and the more positive results obtained in our case reflect that it is statistically
easier to distinguish average mobility across groups of selected tracts than it is to rank individual tracts.
   24
      Interestingly, the hybrid interval for Seattle, the site of Bergman et al. (2020)’s experiment, is very
short, and substantially shifted downwards relative to the naive and empirical Bayes intervals. This
reflects that fact that the “next best” tracts in this case are very close to some of the included tracts. This
leads to very strong downward correction by the conditional approach, and a hybrid interval concentrated
near the lower bound of the projection interval CSPβ .


                                                     39
                    0                1                        2
          Chicago
        New York
     Philadelphia
        Cleveland
         Baltimore
         St. Louis
           Newark
             Dallas
            Atlanta
            Detroit
   Washington DC
     New Orleans
        Milwaukee
        Columbus
      Kansas City
            Raleigh
             Austin
    Port St. Lucie
        Bridgeport
        Cincinnati
      Indianapolis
    San Francisco
         Charlotte
           Phoenix
            Boston
          Houston
            Buffalo
         Nashville
    Salt Lake City
        Pittsburgh
            Denver
      Minneapolis
    Grand Rapids
            Dayton
     Los Angeles
             Miami
         San Jose
           Orlando
       Fort Worth
        Las Vegas
            Tampa
      Sacramento
        San Diego
     San Antonio
     Jacksonville
            Seattle
            Fresno
       Providence                                                             Naive
          Portland                                                            EB
      Manchester                                                              Hybrid




Figure 7: Estimates and confidence intervals for average economic mobility for selected census
tracts based on Chetty et al. (2018) Opportunity Atlas, relative to the within-CZ average. CZs
are ordered by the magnitude of the naive estimate.


with projection confidence intervals.
    Our results suggest a range of opportunities for future work. First, rather than con-
sidering inference on µY (θ̂), under suitable assumptions one could build on our results to
forecast Y (θ̂). Alternatively, while conditional and projection confidence intervals have
antecedents in the literature on inference after model selection, including in Berk et al.


                                             40
(2013) and Fithian et al. (2017), there is no analog of our hybrid approach in this literature.
Our positive simulation results for the hybrid approach suggest that this approach might
yield appealing performance in a range of post-selection-inference settings. Even if a fully
conditional approach is desired in the post-selection problem, as in Fithian et al. (2017),
one could consider the analog of our optimal median-unbiased estimates that condition on
the selected model. Finally, the problem of estimating the value of a dynamic treatment
rule (c.f. Chakraborty and Murphy, 2014; Han, 2020) is closely related to our setting, so
it seems likely that our results could prove to be useful there as well.

References
Andrews, D., Cheng, X., and Guggenberger, P. (2020a). Generic results for establishing
  the asymptotic size of confidence sets and tests. Journal of Econometrics, Forthcoming.

Andrews, I., Kitagawa, T., and McCloskey, A. (2020b). Inference after estimation of
  breaks. Journal of Econometrics, Forthcoming.

Andrews, I., Roth, J., and Pakes, A. (2019). Inference for linear conditional moment
  inequalities. Unpublished Manuscript.

Armstrong, T., Kolesar, M., and Plagborg-Moller, M. (2020). Robust empirical bayes
  confidence intervals. Unpublished Manuscript.

Bachoc, F., Preinerstorfer, D., and Steinberger, L. (2020). Uniformly valid confidence
  intervals post-model-selection. Annals of Statistics, 48(1):440–463.

Baird, S., Hicks, J. H., Kremer, M., and Miguel, E. (2016). Worms at work: Long-
  run impacts of a child health investment. The Quarterly Journal of Economics,
  131(4):1637–1680.

Banerjee, A., Hanna, R., Kyle, J., Olken, B. A., and Sumarto, S. (2018). Tangible
  information and citizen empowerment: Identification cards and food subsidy programs
  in Indonesia. Journal of Political Economy, 126(2):451–491.

Bergman, P., Chetty, R., DeLuca, S., Hendren, N., Katz, L. F., and Palmer, C. (2020).
  Creating moves to opportunity: Experimental evidence on barriers to neighborhood
  choice. Unpublished Manuscript.




                                              41
Berk, R., Brown, L., Buja, A., Zhang, K., and Zhao, L. (2013). Valid post-selection
  inference. Annals of Statistics, 41(2):802–831.

Björkman Nyqvist, M. and Jayachandran, S. (2017). Mothers care more, but fathers
  decide: Educating parents about child health in Uganda. American Economic Review,
  107(5):496–500.

Chakraborty, B. and Murphy, S. (2014). Dynamic treatment regimes. Annual Review
 of Statistics and Its Applications, 1:447–64.

Chernozhukov, V., Lee, S., and Rosen, A. (2013). Intersection bounds: Estimation and
  inference. Econometrica, 81(2):667–737.

Chetty, R., Friedman, J., Hendren, N., Jones, M. R., and Porter, S. R. (2018). The oppor-
  tunity atlas: Mapping the childhood roots of social mobility. Unpublished Manuscript.

Chetty, R. and Hendren, N. (2016a). The impacts of neighborhoods on intergenerational mo-
  bility i: Childhood exposure effects. Quarterly Journal of Economics, 133(3):1107–1162.

Chetty, R. and Hendren, N. (2016b). The impacts of neighborhoods on intergenerational
  mobility ii: County-level estimates. Quarterly Journal of Economics, 133(3):1163–1228.

Cox, G. (2018). Almost sure uniqueness of a global minimum without convexity. Annals
  of Statistics, 48(1):584–606.

Dawid, A. P. (1994). Selection paradoxes in bayesian inference. Institute of Mathematical
  Statistics Lecture Notes - Monograph Series, 24:211–220.

Duflo, E., Greenstone, M., Pande, R., and Ryan, N. (2018). The value of regulatory discre-
  tion: Estimates from environmental inspections in India. Econometrica, 86(6):2123–2160.

Eliasz, P. (2004). Optimal median unbiased estimation of coefficients on highly persistent
  regressors. Unpublished Manuscript.

Ferguson, J. P., Cho, J. H., Yang, C., and Zhao, H. (2013). Empirical bayes correction
  for the winner’s curse in genetic association studies. Genetic Epidemiology, 37(1):60–68.

Fithian, W., Sun, D., and Taylor, J. (2017). Optimal inference after model selection. arXiv.

Han, S. (2020). Identification in nonparametric models for dynamic treatment effects.
 Journal of Econometrics, Forthcoming.

                                            42
Hansen, P. R. (2005). A test for superior predictive ability. Journal of Business and
 Economic Statistics, 23(4):365–380.

Harris, X. T., Panigrahi, S., Markovic, J., Bi, N., and Taylor, J. (2016). Selective sampling
  after solving a convex problem. arXiv.

Karlan, D. and List, J. A. (2007). Does price matter in charitable giving? evidence from
  a large-scale natural field experiment. American Economic Review, 97(5):1774–1793.

Khan, A. Q., Khwaja, A. I., and Olken, B. A. (2016). Tax farming redux: Experimental
 evidence on performance pay for tax collectors. The Quarterly Journal of Economics,
 131(1):219–271.

Kitagawa, T. and Tetenov, A. (2018a). Supplement to “who should be treated? empirical
  welfare maximization methods for treatment choice”. Econometrica.

Kitagawa, T. and Tetenov, A. (2018b). Who should be treated? empirical welfare
  maximization methods for treatment choice. Econometrica, 86(2):591–616.

Kivaranovic, D. and Leeb, H. (2020). On the length of post-model-selection confidence
  intervals conditional on polyhedral constraints. Journal of the American Statistical
  Association, Forthcoming.

Kuchibhotla, A. K., Brown, L. D., Buja, A., Cai, J., George, E. I., and Zhao, L. (2020). Valid
  post-selection inference in model-free linear regression. Annals of Statistics, Forthcoming.

Laber, E. and Murphy, S. (2011). Adaptive confidence intervals for the test error in
  classification. Journal of the American Statistical Association, 106(495):904–913.

Lee, J. D., Sun, D. L., Sun, Y., and Taylor, J. E. (2016). Exact post-selection inference,
  with application to the LASSO. Annals of Statistics, 44(3):907–927.

Lee, M. R. and Shen, M. (2018). Winner’s curse: Bias estimation for total effects of
  features in online controlled experiments. In KDD.

Lehmann, E. and Scheffé, H. (1955). Completeness, similar regions, and unbiased
  estimation: Part ii. Sankhyā: The Indian Journal of Statistics, 15(3):219–236.

Lehmann, E. L. and Romano, J. P. (2005). Testing Statistical Hypotheses. Springer, third
  edition.

                                             43
McCloskey, A. (2017). Bonferroni-based size-correction for nonstandard testing problems.
 Journal of Econometrics, 200(1):17–35.

Mogstad, M., Romano, J. P., Shaikh, A., and Wilhelm, D. (2020). Inference for ranks
 with applications to mobility across neighborhoods and academic achievement across
 countries. Unpublished Manuscript.

Pfanzagl, J. (1979). On optimal median unbiased estimators in the presence of nuisance
  parameters. Annals of Statistics, 7(1):187–193.

Pfanzagl, J. (1994). Parametric Statistical Theory. De Gruyter.

Rai, Y. (2018). Statistical inference for treatment assignment policies. Unpublished
  Manuscript.

Romano, J. P., Shaikh, A., and Wolf, M. (2014). A practical two-step method for testing
  moment inequalities. Econometrica, 82(5):1979–2002.

Romano, J. P. and Wolf, M. (2005). Stepwise multiple testing as formalized data snooping.
  Econometrica, 73(4):1237–1282.

Tetenov, A. (2012). Statistical treatment choice based on asymmetric minimax regret
  criteria. Journal of Econometrics, 166(1):157–165.

Tian, X. and Taylor, J. (2018). Selective inference with a randomized response. Annals
  of Statistics, 46(2):679–710.

Tibshirani, R. J., Rinaldo, A., Tibshirani, R., and Wasserman, L. (2018). Uniform
  asymptotic inference and the bootstrap after model selection. Annals of Statistics,
  46(3):1255–1287.

van der Vaart, A. W. (1998). Asymptotic Statistics. Cambridge University Press.

White, H. (2000). A reality check for data snooping. Econometrica, 68(5):1097–1126.

Zhong, H. and Prentice, R. (2009). Correcting “winner’s curse” in odds ratios from
  genomewide association findings for major complex human diseases. Genetic
  Epidemiology, 34(1):78–91.




                                           44
                              Supplement to the paper

                            Inference on Winners

        Isaiah Andrews               Toru Kitagawa                 Adam McCloskey

                                     September 3, 2020


    This supplement contains proofs and additional results for the paper “Inference on
Winners.” Section A generalizes the conditional inference results discussed in the main text,
introducing additional conditioning variables and unbiased confidence intervals. Section B
proves our results for the finite-sample normal model. Section C constructs procedures that
dominate conventional sample splitting as discussed in Section 3.3 of the paper. Section
D proves the uniform asymptotic results discussed in the main text. Section E provides
additional results and discussion to complement the application in Section 7 of the main
text. Finally, Section F reports additional simulation results for the stylized example of
Section 2 of the paper.

A     Conditional Inference
This section extends the conditional inference results developed in Section 3 of the main
text in two directions, first allowing dependence on additional conditioning variables, and
then introducing uniformly most accurate unbiased confidence intervals.
A.1    Additional Conditioning Events
Suppose that in addition to conditioning on {θ̂ = θ̃}, we also want to condition on an
additional event {γ̂ = γ̃}, for γ̂ =γ(X) some function of X. We thus seek estimators that
are quantile-unbiased conditional on (θ̂,γ̂),
                 n                            o
             P rµ µ̂α ≥µY (θ̂)|θ̂ = θ̃,γ̂ = γ̃ =α for all θ̃ ∈Θ, γ̃ ∈Γ, and all µ,      (20)

and confidence sets with correct conditional coverage
               n                          o
           P rµ µY (θ̂)∈CS|θ̂ = θ̃,γ̂ = γ̃ ≥1−α for all θ̃ ∈Θ, γ̃ ∈Γ, and all µ.



                                              45
   Asnin the main text, o we re-write the conditioning event in terms of the sample space of
X as X : θ̂ = θ̃,γ̂ = γ̃ =X (θ̃,γ̃), and study the conditional distribution of (X,Y (θ̃)) given
X ∈X (θ̃,γ̃). For Zθ̃ as defined in (7), let
                                n                                   o
                     Y(θ̃,γ̃,z)= y :z+ ΣXY (·,θ̃)/ΣY (θ̃) y ∈X (θ̃,γ̃) .

Conditional on θ̂ = θ̃, γ̂ = γ̃, and Zθ̃ = z, Y (θ̂) again follows a one-dimensional normal
distribution N(µY (θ̃),ΣY (θ̃)) truncated to Y(θ̃,γ̃,z).
    To characterize Y(θ̃,γ̃,z), note that for for X (θ̃) as derived in the main text, we can write
X (θ̃,γ̃)=X (θ̃)∩Xγ (γ̃). Likewise, for Yγ (γ̃,z) defined analogously to (9), Y(θ̃,γ̃,z)=Y(θ̃,z)∩
Yγ (γ̃,z). The form of Xγ (γ̃) and Yγ (γ̃,z) depends on the conditioning variables γ̂ considered.
    To construct quantile-unbiased estimators, let FT N (y;µY (θ̃),θ̃,γ̃,z) denote the distri-
bution function for a N(µY (θ̃,ΣY (θ̃))) distribution truncated to Y(θ̃,γ̃,z). This function
is strictly decreasing in µY (θ̃), so define µ̂α as the unique solution to

                                  FT N (Y (θ̂);µ̂α,θ̃,γ̃,Zθ̃ )=1−α.                          (21)

Proposition 10
Let µ̂α solve (21). µ̂α is conditionally α-quantile-unbiased in the sense of (20). If Assump-
tion 1 holds, then µ̂α is the uniformly most concentrated α-quantile-unbiased estimator in
                                                                     ∗
that
   for any  other conditionally α-quantile-unbiased estimator µ̂α and any loss function
L d,µY (θ̃) that attains its minimum at d=µY (θ̃) and is quasiconvex in d for all µY (θ̃),

                 h                            i   h                              i
               Eµ L µ̂α,µY (θ̃) |θ̂ = θ̃,γ̂ = γ̃ ≤Eµ L µ̂∗α,µY (θ̃) |θ̂ = θ̃,γ̂ = γ̃

for all µ and all θ̃ ∈Θ, γ̃ ∈Γ.

Proposition 10 shows that optimality of µ̂α extends to general conditioning variables γ̂.
Hence, µ̂ 1 is an optimal median-unbiased estimator, while CSET =[µ̂ α2 ,µ̂1− 1 ] is an optimal
          2                                                                   2
equal-tailed confidence interval.
A.1.1    Additional Conditioining Example: Outperforming a Benchmark
Here, we derive the truncation set Yγ (γ̃,z) corresponding to the additional selection
event discussed as an example in Section 3 of the main text. We would like to con-
dition on rejection of the null hypothesis that no treatment outperforms a benchmark,
H0 :maxθ∈ΘµX (θ)≤µX (0). For X(0) the observed outcome for the under the benchmark

                                                 46
treatment, a non-studentized test of the null hypothesis rejects when the difference between
the empirical outcome at the best treatment and the benchmark n     exceeds a criticalovalue
threshold: X(θ̂)−X(0)≥c for some c>0. In this case, γ̂ =γ(X)=1 X(θ̂)−X(0)≥c and
we wish to condition inference on rejection of H0 for which γ̂ =1.25 In order to implement
our procedures in this context, we need a tractable expression for Yγ (1,z).
    Note that we can write
                                (                                               )
           n                o                     ΣXY (θ̃)−ΣXY (θ̃,0)
            X(θ̃)−X(0)≥c = Zθ̃ (θ̃)−Zθ̃ (0)+                           Y (θ̃)≥c .
                                                          ΣY (θ̃)

Rearranging, we see that
                                                             
                                   ΣY (θ̃)(c−Zθ̃ (θ̃)+Zθ̃ (0))
                             y :y ≥ Σ (θ̃)−Σ (θ̃,0)              if ΣXY (θ̃)−ΣXY (θ̃,0)>0
                          
                          
                          
                                      XY         XY
                          
                                                             
                                   ΣY (θ̃)(c−Zθ̃ (θ̃)+Zθ̃ (0))
                          
                          
                           y :y ≤ ΣXY (θ̃)−Σ                    if ΣXY (θ̃)−ΣXY (θ̃,0)<0
                          
                          
                          
                                                 XY (θ̃,0)
                          
                          
             Yγ (1,Zθ̃ )=                                          if ΣXY (θ̃)−ΣXY (θ̃,0)=0 .
                          
                          R
                          
                          
                                                                     and Zθ̃ (θ̃)−Zθ̃ (0)≥c
                          
                          
                          
                          
                                                                  if ΣXY (θ̃)−ΣXY (θ̃,0)=0
                           ∅
                          
                          
                          
                          
                                                                     and Zθ̃ (θ̃)−Zθ̃ (0)<c.
                                                                          h                      i
Thus, if for example ΣXY (θ̃)>ΣXY (θ̃,0), then Y(θ̃,1,z)=Y(θ̃,z)∩Yγ (1,z)= L∗(θ̃,Zθ̃ ),U(θ̃,Zθ̃ ) ,
where U(θ̃,Zθ̃ ) is the upper bound defined in Proposition 1 while
                                                                           
                                                  ΣY (θ̃) c−Zθ̃ (θ̃)+Zθ̃ (0) 
                       L∗(θ̃,Zθ̃ )=max L(θ̃,Zθ̃ ),                             ,
                                                     ΣXY (θ̃)−ΣXY (θ̃,0) 

for L(θ̃,Zθ̃ ) defined as in Proposition 1. Hence, when ΣXY (θ̃)−ΣXY (θ̃,0)>0, conditoning
on γ̂ =1 simply modifies the lower bound L(θ̃,Zθ̃ ). Likewise, when ΣXY (θ̃)−ΣXY (θ̃,0)<0
or ΣXY (θ̃)−ΣXY (θ̃,0)=0, conditioning on γ̂ =1 modifies U(θ̃,Zθ̃ ) and V(θ̃,Zθ̃ ), respectively.
    As this example illustrates, it is straightforward to incorporate additional conditioning
variables γ̂ in the level maximization problems we study here provided one can characterize
the set Yγ (γ̃,z). While such characterizations are easy to obtain in many cases, they depend
on the conditioning variable considered and must be derived on a case-by-case basis.
  25
       We could equally well consider studentized tests.


                                                     47
A.2     Unbiased Confidence Intervals
Rather than considering equal-tailed intervals, we can alternatively consider unbiased
confidence intervals. Following Lehmann and Romano (2005), we say that a level 1−α
two-sided confidence interval CS is unbiased if its probability of covering any given false
parameter value is bounded above by 1−α. Likewise, a one sided lower (upper) confidence
interval is unbiased if its probability of covering a false parameter value above (below) the
true value is bounded above by 1−α. Using the duality between tests and confidence inter-
vals, a level 1−α confidence interval CS is unbiased if and only if φ(µY,0)=1{µY,0 ∈CS}     /
                                                                    26
is an unbiased test for the corresponding family of hypotheses. The results of Lehmann
and
n Scheffé o     (1955) applied in our setting imply that optimal unbiased
                                                                         n tests conditional on o
  θ̂ = θ̃,γ̂ = γ̃ are the same as optimal unbiased tests conditional on θ̂ = θ̃,γ̂ = γ̃,Zθ̃ =zθ̃ .
These optimal tests take a simple form.
     Define a size α test of the two-sided hypothesis H0 :µY (θ̃)=µY,0 as
                                            n                            o
                              φT S,α(µY,0)=1 Y (θ̃)6∈ [cl(Zθ̃ ),cu(Zθ̃ )]

where cl(z), cu(z) solve

               P r{ζ ∈[cl(z),cu(z)]}=1−α, E[ζ1{ζ ∈[cl(z),cu(z)]}]=(1−α)E[ζ]

for ζ that follows a truncated normal distribution
                                                                 
                             ζ ∼ξ|ξ ∈Y(θ̃,γ̃,z), ξ ∼N µY,0,ΣY (θ̃) .

Likewise, define a size α test of the one-sided hypothesis H0 :µY (θ̃)≥µY,0 as
                                         n                            o
                           φOS−,α(µY,0)=1 FT N (Y (θ̃);µY,0,θ̃,γ̃,z)≤α

and a test of H0 :µY (θ̃)≤µY,0 as
                                      n                              o
                        φOS+,α(µY,0)=1 FT N (Y (θ̃);µY,0,θ̃,γ̃,z)≥1−α .

Proposition 11
  26
     That is, H0 :µY (θ̃)=µY,0 for a two-sided confidence interval, H0 :µY (θ̃)≥µY,0 for a lower confidence
interval and H0 :µY (θ̃)≤µY,0 for an upper confidence interval.



                                                    48
If Assumption 1 holds, φT S,α, φOS−,α, and φOS+,α are uniformly most powerful unbiased
size α tests of their respective null hypotheses conditional on θ̂ = θ̃ and γ̂ = γ̃.

    To form uniformly most accurate unbiased confidence intervals we collect the values
not rejected by these tests. The two-sided uniformly most accurate unbiased confidence
interval is CSU = {µY,0 :φT S,α(µY,0)=0}. CSU is unbiased and has conditional coverage
1−α by construction. Likewise, we can form lower and upper one-sided uniformly most
accurate unbiased confidence intervals as CSU,− = {µY,0 :φOS−,α(µY,0)=0} = (−∞,µ̂1−α],
and CSU,+ ={µY,0 :φOS+,α(µY,0)=0}=[µ̂α,∞), respectively. Hence, we can view CSET as
the intersection of level 1− α2 uniformly most accurate unbiased upper and lower confidence
intervals. Unfortunately, no such simplification is generally available for CSU , though
Lemma 5.5.1 of Lehmann and Romano (2005) guarantees that this set is an interval.
                                 n          o
A.3 Behavior When P rµ θ̂ = θ̃,γ̂ = γ̃ is Large
In Proposition 3 of the main text, we showed that our median-unbiased    n estimators
                                                                                 o    and
equal-tailed confidence intervals converge to conventional ones when P rµ θ̂ = θ̃ →1. The
same result holds for general conditioning events and unbiased confidence intervals.

Lemma 2                                                         n                          o
Consider any sequence of values µY,m and zθ̃,m such that P rµY,m θ̂ = θ̃,γ̂ = γ̃|Zθ̃ =zθ̃,m →1.
                                 n                        o
Then under µY,m, conditional on θ̂ = θ̃,γ̂ = γ̃,Zθ̃ =zθ̃,m we have CSU →p CSN , CSET →p
CSN , and µ̂ 1 →p Y (θ̃).
             2


Proposition 12                                    n               o
Consider any sequence of values µm such that P rµm θ̂ = θ̃,γ̂ = γ̃ → 1. Then under µm,
                                                                          n               o
we have CSU →p CSN , CSET →p CSN , and µ̂ 1 →p Y (θ̃) both conditional on θ̂ = θ̃,γ̂ = γ̃
                                           2
and unconditionally.

B     Proofs
We first prove the results stated in Section A, and then build on these to prove the results
for the finite-sample normal model discussed in the main text.
B.1    Proofs for Results in Section A
Proof of Proposition 10 For ease of reference, let us abbreviate (Y (θ̃),µY (θ̃),Zθ̃ ) by
(Ỹ , µ̃Y , Z̃). Let Y (−θ̃) collect the elements of Y other than Y (θ̃) and define µY (−θ)


                                              49
analagously. Let
                                                      !!                    !!+            !
                                                 Ỹ                    Ỹ            Ỹ
              Y ∗ =Y (−θ̃)−Cov Y (−θ̃),                     V ar                           ,
                                                 X                     X             X
                                                      !!                    !!+            !
                                                 Ỹ                    Ỹ            µ̃Y
             µ∗Y =µY (−θ̃)−Cov Y (−θ̃),                     V ar                               ,
                                                 X                     X             µX
                                
and µ̃Z = µX − ΣXY (·,θ̃)/ΣY (θ̃) µY . Here we use A+ to denote the Moore-Penrose
pseudoinverse of a matrix A. Note that (Z̃,Ỹ ,Y ∗) is a one-to-one transformation of (X,Y ),
and thus that observing (Z̃,Ỹ ,Y ∗) is equivalent to observing (X,Y ). Likewise, (µ̃Z ,µ̃Y ,µ∗Y )
is a one-to-one linear transformation of (µX ,µY ), and if the set of possible values for the
latter contains an open set, that for the former does as well (relative to the appropriate
linear subspace).
    Note, next, that since (Z̃,Ỹ ,Y ∗) is a linear transformation of (X,Y ), (Z̃,Ỹ ,Y ∗) is jointly
normal (with a potentially degenerate distribution). Note next that (Z̃,Ỹ ,Y ∗) are mutually
uncorrelated, and thus independent. That Z̃ and Ỹ are uncorrelated is straightforward
                           ∗
to verify. To show
                that Y is likewise uncorrelated with the other elements, note that we
can write Cov Y ∗,(Ỹ ,X 0)0 as

                           !!                          !!                     !!+                       !!
                      Ỹ                          Ỹ                     Ỹ                        Ỹ
   Cov Y (−θ̃),                 −Cov Y (−θ̃),                V ar                   V ar                 .
                      X                           X                      X                         X
                                                  
For V ΛV 0 an eigendecomposition of V ar (Ỹ ,X 0)0 (so V V 0 =I), note that we can write

                                         !!+                     !!
                                    Ỹ                      Ỹ
                           V ar                V ar                   =V DV 0
                                    X                       X

for D a diagonal matrix with ones in the entries corresponding to the nonzero entries of
Λ andzeros everywhere  else. For any column v of V corresponding to a zero entry of D,
              0
v0V ar Ỹ ,X 0    v =0, so the Cauchy-Schwarz inequality implies that

                                                            !!
                                                     Ỹ
                                  Cov Y −θ̃ ,                    v =0.
                                                       X


                                                 50
Thus,
                            !!                                                  !!                        !!
                   Ỹ                0
                                                                         Ỹ        0
                                                                                                   Ỹ
Cov Y −θ̃ ,                        V DV =Cov Y −θ̃ ,                              V V =Cov Y −θ̃ ,           ,
                     X                                                     X                         X
                                  0
so Y ∗ is uncorrelated with Ỹ ,X 0 .
   Using independence, the joint density of (Z̃,Ỹ ,Y ∗) absent truncation is given by

                                      fN,Z̃ (z̃;µ̃Z )fN,Ỹ (ỹ;µ̃Y )fN,Y ∗ (ỹ∗;µ∗Y )

for fN normal densities with respect to potentially degenerate base measures:
                                                                                          
                                      ˜                − 21        1         0 +
                    fN,Z̃ (z̃;µ̃Z )= det(2πΣZ̃ )              exp − (z̃−µ̃Z ) ΣZ̃ (z̃−µ̃Z )
                                                                   2

                                                                            (ỹ−µ̃Y )2
                                                                                      
                                                               − 12
                                  fN,Ỹ (ỹ;µ̃Y )=(2πΣỸ )            exp −
                                                                               2ΣỸ
                                                                                            
                            ∗            ˜          −1               1 ∗ ∗ 0 + ∗ ∗
                fN,Y ∗ (y       ;µ∗Y )= det(2πΣY ∗ ) 2 exp          − (y −µ̃Y ) ΣY ∗ (y −µY ) ,
                                                                     2
       ˜
where det(A) denotes the pseudodeterminant of a matrix A, ΣZ̃ =V ar(Z̃), ΣỸ =ΣY (θ̃),
                 ∗
and ΣY ∗ =V ar(Y
             n ).        o
   The event X ∈X (θ̃,γ̃) depends only on (Z̃,Ỹ ) since it can be expressed as

                                       (                              !              )
                                                 ΣXY (·,θ̃)
                                            Z̃ +            Ỹ            ∈X (θ̃,γ̃) ,
                                                  ΣY (θ̃)

so conditional on this event Y ∗ remains
                                  n      independent
                                              o      of (Z̃,Ỹ ). In particular, we can write
the joint density conditional on X ∈X (θ̃,γ̃) as

         n                                 o
        1 z̃+ΣXY (·,θ̃)ΣY (θ̃)−1ỹ ∈X (θ̃,γ̃)
                        n            o         fN,Z̃ (z̃;µ̃Z )fN,Ỹ (ỹ;µ̃Y )fN,Y ∗ (ỹ∗;µ∗Y ).           (22)
             P rµ̃Z ,µ̃Y X ∈X (θ̃,γ̃)

The density (22) has the same structure as (5.5.14) of Pfanzagl (1994), and satisfies proper-
ties (5.5.1)-(5.5.3) of Pfanzagl (1994) as well. Part 1 of the proposition then follows immedi-


                                                              51
ately from Theorem 5.5.9 of Pfanzagl (1994). Part 2 of the proposition follows by using Theo-
rem 5.5.9 of Pfanzagl (1994) to verify the conditions of Theorem 5.5.15 of Pfanzagl (1994). 
Proof of Proposition 11 In the proof of Proposition 10, we showed that the joint density
of (Z̃,Ỹ ,Y ∗) (defined in that proof) has the exponential family structure assumed in equation
4.10 of Lehmann and Romano (2005). Moreover, Assumption 1 implies that the parameter
space for (µX ,µY ) is convex and is not contained in any proper linear subspace. Thus, the
parameter space for (µ̃Z ,µ̃Y ,µ∗Y ) inherits the same property, and satisfies the conditions
of Theorem 4.4.1 of Lehmann and Romano (2005). The result follows immediately. 
Proof of Lemma 2 Recall that conditional on Zθ̃ = zθ̃ , θ̂ = θ̃ and γ̂ = γ̃ if and only if
Y (θ̃)∈Y(θ̃,γ̃,zθ̃ ). Hence, the assumption of the lemma implies that
                                 n                               o
                          P rµY,m Y (θ̃)∈Y(θ̃,γ̃,Zθ̃ )|Zθ̃ =zθ̃,m →1.

    Note, next, that both the conventional and conditional confidence intervals are equiv-
ariant under shifts, in the sense that the conditional confidence interval for µY (θ̃) based
on observing Y (θ̃) conditional on Y (θ̃) ∈ Y(θ̃, γ̃,Zθ̃ ) is equal to the conditional confi-
dence interval for µY (θ̃) based on observing Y (θ̃)−µ∗Y (θ̃) conditional on Y (θ̃)−µ∗Y (θ̃) ∈
Y(θ̃,γ̃,Zθ̃ )−µ∗Y (θ̃) for any constant µ∗Y (θ̃). Hence, rather
                                                            n than considering ao sequence of
values µY,m, we can fix some µ∗Y and note that P rµ∗Y Y (θ̃)∈Ym      ∗
                                                                       |Zθ̃ =zθ̃,m → 1, where
  ∗
Ym  =Y(θ̃,γ̃,Zθ̃ )−µY,m(θ̃)+µ∗Y (θ̃). Confidence intervals for µY,m(θ̃) in the original problem
are equal to those for µ∗Y (θ̃) in the new problem, shifted by µY,m(θ̃)−µ∗Y (θ̃). Hence, to prove
the result it suffices to prove the equivalence of conditional and conventional confidence
intervals in the problem with µY fixed (and likewise for estimators).
    To prove the result, we make use of the following lemma, which is proved be-
low. First, we must introduce the following notation. Let (cl,ET (µY,0,Y),cu,ET (µY,0,Y))
denote
                   values for an equal-tailed test of H0 : µY (θ̃) = µY,0 for Y (θ̃) ∼
          the critical
N µY (θ̃),ΣY (θ̃) conditional on Y (θ̃) ∈ Y. That is, (cl,ET (µY,0,Y),cu,ET (µY,0,Y)) solve
FT N (cl,ET (µY,0,Y);µY,0,Y)= α2 and FT N (cu,ET (µY,0,Y);µY,0,Y)=1−
                                                                  
                                                                          α
                                                                          2
                                                                            , where
                                                                                FT N (·;µY,0,Y)
is the distribution function for the normal distribution N µY,0,ΣY (θ̃) truncated to Y.
Similarly, let (cl,U (µY,0,Y),cu,U (µY,0,Y)) denote the critical values for the corresponding un-
biased test. That is, (cl,U (µY,0,Y),cu,U (µY,0,Y)) solve P r{ζ ∈[cl,U (µY,0,Y),cu,U (µY,0,Y)]}=
1 − αand E [ζ1{ζ     ∈[cl,U (µY,0,Y),cu,U (µY,0,Y)]}] = (1−α) E [ζ] for ζ ∼ ξ|ξ ∈ Y where
ξ ∼N µY,0,ΣY (θ̃) .



                                               52
Lemma 3                                          
Suppose that we observe Y (θ̃) ∼ N µY (θ̃),ΣY (θ̃) conditional on Y (θ̃) falling in a
                             
set Y. If we hold ΣY (θ̃),µY,0 fixed and consider a sequence of sets Ym such that
   n         o
P r Y (θ̃)∈Ym →1, we have that for

                               n                                          o
                   φET (µY,0)=1 Y (θ̃)6∈ [cl,ET (µY,0,Ym),cu,ET (µY,0,Ym)]                (23)

and                               n                                      o
                     φU (µY,0)=1 Y (θ̃)6∈ [cl,U (µY,0,Ym),cu,U (µY,0,Ym)] ,               (24)
                                                        q                   q       
       (cl,ET (µY,0,Ym),cu,ET (µY,0,Ym))→ µY,0 −c α2 ,N ΣY (θ̃),µY,0 +c α2 ,N ΣY (θ̃)

and
                                                     q                     q       
        (cl,U (µY,0,Ym),cu,U (µY,0,Ym))→ µY,0 −c α2 ,N ΣY (θ̃),µY,0 +c α2 ,N ΣY (θ̃) .


    To complete the proof, first note that CSET and CSU are formed by inverting (families
of) equal-tailed and unbiased tests, respectively. Let CSm denote a generic conditional
confidence interval formed by inverting a family of tests
                                n                                    o
                                                    ∗            ∗
                      φm(µY,0)=1 Y (θ̃)6∈ [cl(µY,0,Ym ),cu(µY,0,Ym )] .

Hence, we want to show that
                                   h                              i
                             CSm →p Y (θ̃)−c α2 ,N ,Y (θ̃)+c α2 ,N ,                      (25)

as m→∞, for CSm formed by inverting either (23) or (24).
    We note that CSm is a finite interval for all m, which holds trivially for the equal-tailed
confidence interval CSET , and holds for CU by Lemma 5.5.1 of Lehmann and Romano
(2005). For each value µY,0 our Lemma 3 implies that
                                  n                                 o
                      φm(µY,0)→p 1 Y θ̃ ∈
                                        / µY,0 −c α2 ,N ,µY,0 +c α2 ,N

for φm equal to either (23) or (24). This convergence in probability holds jointly for all
finite collections of values µY,0, however, which implies (25). The same argument works


                                               53
for the median unbiased estimator µ̂ 1 , which can also be viewed as the upper endpoint
                                     2
of a one-sided 50% confidence interval. 
Proof of Proposition
            n              o12 We prove this result for then unconditional
                                                                         o case, noting that
since P rµm θ̂ = θ̃,γ̂ = γ̃ →1, the result conditional on θ̂ = θ̃,γ̂ = γ̃ follows immediately.
                       n           o                     n                    o
    Note that P rµm θ̂ = θ̃,γ̂ = γ̃ → 1 implies P rµY,m θ̂ = θ̃,γ̂ = γ̃|Zθ̃ =z →p 1 for all z.
                             n                  o
Hence, for g(µY ,z)=P rµY θ̂ = θ̃,γ̂ = γ̃|Zθ̃ =z , we see that g(µY,m,z)→p 1 for all z. Note,
next, that for d the Euclidian distance between the endpoints, if we define hε(µY ,z) =
P rµY {d(CSU ,CSN )>ε|Zθ̃ =z}, Lemma 2 implies that for any sequence (µY,m,zm) such that
g(µY,m,zm)→1, hε(µY,m,zm)→0. Hence, if we define G(δ)={(µY ,z):g(µY ,z)>1−δ} and
H(ε)={(µY ,z):hε(µY ,z)<ε}, for all ε>0 there exists δ(ε)>0 such that G(δ(ε))⊆H(ε).
    Hence, since our argument above implies that for all δ >0, P rµm {(µY,m,z)∈G(δ)}→1
for all z, we see that for all ε > 0, P rµm {(µY,m,z)∈H(ε)} → 1 for all z as well, which
suffices to prove the desired claim for confidence intervals. The same argument likewise
implies the result for our median unbiased estimator. 
Proof of Lemma 3 Note that we can assume without        loss of generality that µY,0 =0 and
                                                     q
ΣY (θ̃) = 1 since we can define Y ∗(θ̃) = Y (θ̃)−µY,0 / ΣY (θ̃) and consider the problem
of testing that the mean of Y ∗(θ̃) is zero (transforming the set Ym accordingly). After
deriving critical values (c∗l ,c∗u) in thisqtransformed problem, we can recover critical values
for our original problem as (cl,cu)= ΣY (θ̃)(c∗l ,c∗u)+µY,0. Hence, for the remainder of the
proof we assume that µY,0 =0 and ΣY (θ̃)=1.
    Equal-Tailed Test We consider first the equal-tailed test. Note that this test rejects
if and only if Y (θ̃)6∈ [cl,ET (Y),cu,ET (Y)], where we suppress the dependence of the critical
values on µY,0 = 0 for simplicity, and (cl,ET (Y),cu,ET (Y)) solve FT N (cl,ET (Y),Y) = α2 and
FT N (cu,ET (Y),Y) = 1 − α2 , for FT N (·,Y) the distribution function of a standard normal
random variable truncated to Y. Recall that we can write the density corresponding to
FT N (y,Y) as P1{y∈Y}
                r{ξ∈Y} N
                        f (y) where fN is the standard normal density and P r{ξ ∈Y} is the
                                                                               Ry
                                                                                    1{ỹ∈Y}fN (ỹ)dỹ
probability that ξ ∈Y for ξ ∼N(0,1). Hence, we can write FT N (y,Y)= −∞ P r{ξ∈Y}         .
   Note next that for all y we can write FT N (y,Ym) = a (y)+FN (y), where FN is the
                                                     Ry m
                                                        1{ỹ∈Ym }fN (ỹ)dỹ
standard normal distribution function and am (y) = −∞ P r{ξ∈Ym}             − FN (y). Recall,
however, that P r{ξ ∈Ym}→1 and
              Z   y                                  Z   y
                      1{ỹ ∈Ym}fN (ỹ)dỹ−FN (y) =           [1{ỹ ∈Ym}−1]fN (ỹ)dỹ
                −∞                                   −∞


                                              54
                       Z   y
                   =           1{ỹ ∈
                                    6 Ym}fN (ỹ)dỹ ≤P r{ξ ∈
                                                           6 Ym}→0
                       −∞


for all y, so am(y)→0 for all y. Theorem 2.11 in van der Vaart (1998) then implies that
am(y)→0 uniformly in y as well.
    Note next that FT N (cl,ET (Ym),Ym) = am (cl,ET (Ym)) + FN (cl,ET (Ym)) = α2 implies
cl,ET (Ym)=FN−1 α2 −am(cl,ET (Ym)) , and thus that cl,ET (Ym)→FN−1 α2 . Using the same
                                                                      

argument, we can show that cu,ET (Ym)→FN−1 1− α2 , as desired.
                                                    

     Unbiased Test We next consider the unbiased test. Recall that critical values
cl,U (Y) , cu,U (Y) for the unbiased test solve P r {ζ ∈[cl,U (Y),cu,U (Y)]} = 1 − α and
E[ζ1{ζ ∈[cl,U (Y),cu,U (Y)]}]=(1−α)E[ζ] for ζ ∼ξ|ξ ∈Y where ξ ∼N(0,1).
     Note that for ζm the truncated normal random variable corresponding to Ym, we can
write P r{ζm ∈[cl,cu]}=am(cl,cu)+(FN (cu)−FN (cl)) with

                   am(cl,cu)=(FN (cl)−P r{ζm ≤cl})−(FN (cu)−P r{ζm ≤cu}).

As in the argument for equal-tailed tests above, we see that both FN (cu)−P r{ζm ≤cu}
and FN (cl)−P r{ζm ≤cl} converge to zero pointwise, and thus uniformly in cu and cl by
Theorem 2.11 in van der Vaart (1998). Hence, am(cl,cu)→0 uniformly in (cl,cu).
   Note, next, that we can write E[ζm1{ζm ∈[cl,cu]}]=[ξ1{ξ ∈[cl,cu]}]+bm(cl,cu) for
                                                                     cu                 
                                                                             1{y ∈Ym}
                                                                 Z
   bm(cl,cu)=E[ζm1{ζm ∈[cl,cu]}]−[ξ1{ξ ∈[cl,cu]}]=                                     −1 yfN (y)dy.
                                                                 cl         P r{ξ ∈Ym}
                               R cu
Note, however, that             cl
                                      (1{y ∈Ym}−1)yfN (y)dy ≤E[|ξ|1{ξ ∈
                                                                      6 Ym}]. Hence, since

          cu                 
                  1{y ∈Ym}
      Z
                            −1 yfN (y)dy
       cl        P r{ξ ∈Ym}
              cu                                          cu       
                                             1{y ∈Ym}
          Z                                           Z
      ≤     (1{y ∈Ym}−1)yfN (y)dy +                    −1{y ∈Ym} yfN (y)dy
         cl                            cl   P r{ξ ∈Ym}
                                         Z cu
                                1
      ≤E[|ξ|1{ξ ∈
                6 Ym}]+               −1        1{y ∈Ym}|y|fN (y)dy
                           P r{ξ ∈Ym}       cl
                                     
        p                    1
      ≤ P (ξ ∈6 Ym)+               −1 E[|ξ|]
                        P r{ξ ∈Ym}

by the Cauchy-Schwartz Inequality, where the right hand side tends to zero and doesn’t
depend on (cl,cu), bm(cl,cu) converges to zero uniformly in (cl,cu).


                                                          55
     Next, let us define (cl,m,cu,m) as the solutions to P r {ζm ∈[cl,cu]} = 1 − α and
E [ζm1{ζm ∈[cl,cu]}] = (1−α)E [ζm]. From our results above, we can re-write the prob-
lem solved by (cl,m,cu,m) as FN (cu) − FN (cl) = 1 − α − am (cl,cu), E [ξ1{ξ ∈[cl,cu]}] =
(1−α)E [ζm] − bm (cl,cu). Letting ām = supcl,cu |am(cl,cu)|, and b̄m = supcl,cu |bm(cl,cu)| we
thus see that (cl,m,cu,m) solves FN (cu) − FN (cl) = 1 − α − a∗m and E [ξ1{ξ ∈[cl,cu]}] =
(1−α)E[ζm] − b∗m for some a∗m ∈ [−ām,ām], b∗m ∈ −b̄m,b̄m . We will next show that for
                                                             

any sequence of values (a∗m,b∗m) such that a∗m ∈ [−ām,ām] and b∗m ∈ −b̄m,b̄m for all m,
                                                                                  

the implied solutions cl,m(a∗m,b∗m), cu,m(a∗m,b∗m) converge to FN−1 α2 and FN−1 1− α2 . This
                                                                                       

follows from the next lemma, which is proved below.

Lemma 4
Suppose that cl,m and cu,m solve P r{ξ ∈[cl,cu]}=1−α+am and E[ξ1{ξ ∈[cl,cu]}]=dm for
                                              
am, dm →0. Then (cl,m,cu,m)→ −c α2 ,N ,c α2 ,N .

    Using this lemma, since E[ζm]→0 as m→∞ we see that for any sequence of values
(a∗m,b∗m)
        → 0, (cl,m(a∗m,b∗m),cu,m(a∗m,b∗m)) → −c α2 ,N ,c α2 ,N . However, since ām, b̄m → 0 we
                                                              

know that the values a∗m and b∗m corresponding to the true cl,m, cu,m must converge to
                                          
zero. Hence (cl,m,cu,m)→ −c α2 ,N ,c α2 ,N as we wanted to show. 
Proof of Lemma 4 Note that the critical values solve
                                                                     !
                                    FN (cu)−FN (cl)−(1−α)−am
                   f(am,dm,c)=            R cu                           =0.
                                           cl
                                               yfN (y)dy−dm
                                       ∂
                                                               Rc
We can simplify this expression, since ∂y fN (y) = −yfN (y), so clu yfN (y)dy = fN (cl) −
fN (cu).
    We thus must solve the system of equations g(c)−vm =0, for
                                              !                           !
                            FN (cu)−FN (cl)                 am +(1−α)
                   g(c)=                        , vm =                     .
                            fN (cl)−fN (cu)                     dm

Note that for vm =(1−α,0)0 this system is solved by c= −c α2 ,N ,c α2 ,N . Further,
                                                                        

                                                                !
                            ∂            −fN (cl)    fN (cu)
                               g(c)=                             ,
                            ∂c           −clfN (cl) cufN (cu)




                                              56
                                       
which evaluated at c= −c α2 ,N ,c α2 ,N is equal to
                                                                 !
                                  −fN c α2 ,N         fN c α2 ,N
                                                                    
                               c α2 ,N fN c α2 ,N c α2 ,N fN c α2 ,N

and has full rank for all α∈(0,1). Thus, by the implicit function theorem there exists an
open neighborhood V of v∞ = (1−α,0) such that g(c)−v = 0 has a unique solution c(v)
for v ∈ V and c(v) is continuously differentiable. Hence,
                                                       ! if we consider any sequence of
                                              −c α2 ,N
values vm →(1−α,0), we see that c(vm)→                  , again as we wanted to show. 
                                               c α2 ,N

B.2    Proofs for Results in Main Text
Proof of Proposition 1 Let us assume without loss of generality that θ̃ =θ1. Note that
the conditioning event {maxθ∈ΘX(θ)=X(θ1)} is equivalent to {MX ≥0}, where
                                                                 
                                      1 −1 0 0 ... 0
                                      1 0 −1 0 ... 0
                                                                 
                                                                 
                             M ≡     .. .. .. .. .. ..           
                                       . .   . . .    .
                                                                 
                                                                 
                                      1 0   0 0 ... −1
                                                                        h                i
is a (|Θ|−1)×|Θ| matrix and the inequality is taken element-wise. Let A= − M 0(|Θ|−1)×|Θ| ,
where 0(|Θ|−1)×|Θ| denotes the (|Θ|−1)×|Θ| matrix of zeros. Let W = (X 0,Y 0)0 and note
that we can re-write the event of interest as {W :AW ≤0} and that we are interested
in inference on η0µ for η the 2|Θ|×1 vector with one in the (|Θ|+1)st entry and zeros
everywhere else. Define
                                    Z̃θ̃∗ =W −cY (θ̃),

for c=Cov(W,Y (θ̃))/ΣY (θ̃), noting that the definition of Zθ̃ in (7) corresponds to extracting
the elements of Z̃θ̃∗ corresponding to X. By Lemma 5.1 of Lee et al. (2016),
                          n                                               o
               {W :AW ≤0}= W :L(θ̃,Z̃θ̃∗)≤Y (θ̃)≤U(θ̃,Z̃θ̃∗),V(θ̃,Z̃θ̃∗)≥0 ,

where for (v)j the jth element of a vector v,

                                                      −(Az)j
                                  L(θ̃,z)= max
                                            j:(Ac)j <0 (Ac)j




                                               57
                                                                 −(Az)j
                                        U(θ̃,z)= min
                                                       j:(Ac)j >0 (Ac)j



                                        V(θ̃,z)= min −(Az)j .
                                                       j:(Ac)j =0

Note, however, that                          
                                        AZ̃θ̃∗ =Zθ̃ (θj )−Zθ̃ (θ1)
                                                   j

and
                                              ΣXY (θ1,θ1)−ΣXY (θ1,θj )
                                (Ac)j =−                               .
                                                      ΣY (θ1)
Hence, we can re-write

                             −(AZ̃θ̃∗)j ΣY (θ1)(Zθ̃ (θj )−Zθ̃ (θ1))
                                       =                            ,
                              (Ac)j      ΣXY (θ1,θ1)−ΣXY (θ1,θj )

                                                              ΣY (θ1)(Zθ̃ (θj )−Zθ̃ (θ1))
                 L(θ̃,Z̃θ̃∗)=              max                                            ,
                                j:ΣXY (θ1 ,θ1 )>ΣXY (θ1 ,θj ) ΣXY (θ1 ,θ1 )−ΣXY (θ1 ,θj )

                                                              ΣY (θ1)(Zθ̃ (θj )−Zθ̃ (θ1))
                 U(θ̃,Z̃θ̃∗)=               min                                           ,
                                j:ΣXY (θ1 ,θ1 )<ΣXY (θ1 ,θj ) ΣXY (θ1 ,θ1 )−ΣXY (θ1 ,θj )

and
                      V(θ̃,Z̃θ̃∗)=              min                 −(Zθ̃ (θj )−Zθ̃ (θ1)).
                                     j:ΣXY (θ1 ,θ1 )=ΣXY (θ1 ,θj )

Note, however, that these are functions of Zθ̃ , as expected. The result follows. 
Proof of Proposition 2 Follows as a special case of Proposition 10. 
Proof of Proposition 3 Follows as a special case of Proposition 12. 
Proof of Proposition 4 Provided θ̂ is unique with probability one, we can write
                    n        o X    n       o    n                o
                P rµ µ(θ̂)∈CS = P rµ θ̂ = θ̃ P rµ µ(θ̃)∈CS|θ̂ = θ̃ .
                                            θ̃∈Θ

        P         n        o
Since   θ̃∈Θ P rµ  θ̂ = θ̃   =1, the result of the proposition follows immediately. 

Proof of Lemma 1 The assumption of the lemma implies that X(θ̃) − X(θ) has a
non-degenerate normal distribution for all µ. Since Θ is finite, almost-sure uniqueness of
θ̂ follows immediately. 


                                                         58
Proof of Proposition 5 We first establish uniqueness of µ̂H            α . To do so, it suffices to show
        H
that FT N (Y (θ̃);µY (θ̃),θ̃,Zθ̃ ) is strictly decreasing in µY (θ̃). Note first that this holds for the
truncated normal assuming truncation that does not depend on µY (θ̃) by Lemma A.1 of Lee
et al. (2016). When we instead consider FTHN (Y (θ̃);µY (θ̃),θ̃,Zθ̃ ), we impose truncation to
                                         q                  q       
                        Y (θ̃)∈ µY (θ̃)−cβ ΣY (θ̃),µY (θ̃)+cβ ΣY (θ̃) .


Since this interval shifts upwards as we increase µY (θ̃), FTHN (Y (θ̂); µY (θ̃), θ̃, Zθ̃ ) is a-
fortiori decreasing in µY (θ̃). Uniqueness of µ̂H     α for α ∈ (0,1) follows. Note, next, that
  H
                                                   6 CSPβ from which we immediately see that
FT N (Y (θ̃);µY (θ̃), θ̃,Zθ̃ ) ∈ {0,1} for µY (θ̃) ∈
          β
µ̂H
  α ∈CSP .
    Finally, note that for µY (θ̃)othe true value, FTHN (Y (θ̂);µY (θ̃),θ̃,Zθ̃ )∼U[0,1] conditional
    n
on θ̂ = θ̃,Zθ̂ =zθ̃ ,µY (θ̃)∈CSPβ . Since FTHN (Y (θ̂);µY (θ̃),θ̃,Zθ̃ ) is decreasing in µY (θ̃),

                              n                                            o
                                                                         β
                          P rµ µ̂H
                                 α ≥µY (θ̃)|θ̂ = θ̃,Zθ̂ =zθ̃ ,µY (θ̃)∈CSP

                 n                                                                   o
         =P rµ FTHN (Y (θ̂);µY (θ̃),θ̃,γ̃,Zθ̃ )≥1−α|θ̂ = θ̃,Zθ̂ =zθ̃ ,µY (θ̃)∈CSPβ =α,
                                                        n                                 o
                                                                                        β
and thus µ̂H
           α  is α-quantile-unbiased     conditional on    θ̂ = θ̃,Zθ̂ =zθ̃ ,µY (θ̃)∈CSP . We can
drop the conditioning on Zθ̃ by the law of iterated expectations, and α-quantile unbiasedness
conditional on µY (θ̃)∈CSPβ follows by the same argument as in the proof of Proposition 4.
Proof of Proposition 6 The first part of the proposition follows immediately from
Proposition 5. For the second part of the proposition, note that
                             n             o      n            o
                                        H
                         P rµ µY (θ̂)∈CSET   =P rµ µY (θ̂)∈CSPβ ×

            X    n                    o   n                                  o
                                    β                H                     β
             P rµ θ̂ = θ̃|µY (θ̂)∈CSP P rµ µY (θ̃)∈CSET |θ̂ = θ̃,µY (θ̃)∈CSP
            θ̃∈Θ
                            n             o 1−α        1−α
                       =P rµ µY (θ̂)∈CSPβ       ≥(1−β)     =1−α,
                                            1−β        1−β
where the second equality follows from the first part of
                                                      n the proposition.
                                                                    o The upper bound
                                                                  β
follows by the same argument and the fact that P rµ µY (θ̂)∈CSP ≤1. 




                                                  59
C      Alternatives to Conventional Sample Splitting
In Section 3.3 of the main text, we discuss the relationship of our conditional approach to
conventional sample splitting methods and note that the results of Fithian et al. (2017) im-
ply that traditional sample splitting methods are dominated in our setting. Here, we derive
optimal split-sample confidence intervals and estimators as well as easy-to-implement con-
fidence intervals and estimators that dominate their conventional split-sample counterparts
in the asymptotic version of the split-sample problem.
The Split-Sample Limit Experiment Let τ denote the fraction of the full sample
used to compute the estimated maximum and let (Xn1,Yn1) and (Xn2,Yn2) correspond to the
first and second portions of the data, with

                                     (Xn1,Yn1)=τ −1/2(X[τ·n],Y[τ·n]),
                                                 √
                       (Xn2,Yn2)=(1−τ)−1 (Xn,Yn)− τ(X[τ·n]+1,Y[τ·n]+1)
                                                                      

and [a] denoting the nearest integer to a ∈ R. Finally, let θ̂n1 = argmaxθ∈Θ Xn1(θ) or θ̂n1 =
argmaxθ∈ΘkXn1(θ)k, as in Andrews et al. (2020b), denote the estimated maximum from the
first part of the sample. In large samples, (Xn1,Yn1), (Xn2,Yn2) and θ̂n1 behave according to27
                                                   !
                                              X1
                                                       ∼N(µ,Σ),
                                              Y1
                                                  !
                                            X2
                                                      ∼N µ,c−1Σ
                                                               
                                            Y2
and
                                          θ̂1 =argmaxθ∈ΘX 1(θ)

or
                                        θ̂1 =argmaxθ∈Θ X 1(θ) ,

where c=(1−τ)/τ and (X 1,Y 1) is independent of (X 2,Y 2). This is the generalization of the
asymptotic problem discussed in Section 3.3 of the main text to arbitrary sample splits.28
  27
     The quantity Σ in the exposition of this section corresponds to the quantity Σ in the main text,
multiplied by τ −1.
  28
     For simplicity of exposition, in this section we suppress the
                                                                  possibility of using additional conditioning
variables γ̂n =γ Xn1 with asymptotic counterpart γ̂ =γ X 1 .



                                                      60
    Traditional sample splitting methods base inference on Y 2(θ̂1). Since Y 2 is independent
of X 1, and thus of θ̂1, this ensures the (conditional) median-unbiasedness of conventional
split-sample estimates Y 2(θ̂1) and the (conditional) validity of conventional split-sample
confidence intervals
                                q                          q                   
                         2 1                          2  1
            CSSS = Y (θ̂ )− c−1ΣY (θ̂1)cα/2,N ,Y (θ̂ )+ c−1ΣY (θ̂1)cα/2,N

but does not make full use of the information in the data. To derive optimal procedures
in the sample splitting framework,
                             n      owe first derive a sufficient statistic for the unknown
                               1
parameter µ conditional on θ̂ = θ̃ and then apply classical exponential family results
as in Section 3 of the main text.
Optimal Estimators and Confidence Sets The joint (unconditional) density of
(X 1,Y 1,X 2,Y 2) is proportional to
                   !      !0               !     !                            !        !0                !     !
        1      X1                      X1                 c             X2                             X2
  exp−                 −µ      Σ−1             −µ exp−                            −µ        Σ−1              −µ .
        2      Y1                      Y1                 2             Y2                             Y2

                             n       o
                                1
The conditional density given θ̂ = θ̃ is thus propotional to

             n        o                                         !0                                 !
           1 X 1 ∈X 1 θ̃
                                                           !                                    !
                                                       1                                    1
                                  1                  X                                    X
              n         o exp−                              −µ Σ−1                                −µ ×
          P rµ X 1 ∈X 1 θ̃        2                  Y1                                   Y1

                                               !     !0                        !         !
                                            2                               2
                                   c    X                              X
                        exp−                       −µ Σ−1                           −µ 
                                   2    Y2                             Y2

with X 1(θ̃)={X 1 : θ̂ = θ̃}, which we can re-write as
                                                                  !                       !!0         
                                                               1                      2
                                                           X                         X
             g1 X 1,Y 1 g2 X 2,Y 2 h(µ)exp                                                      Σ−1µ
                                 
                                                                       +c
                                                           Y1                        Y2

for                                                                            !0                   !
                                                                        1                        1
                                 n         o      1               X                           X
             g1 X 1,Y    1
                               =1 X 1 ∈X 1 θ̃ exp−                                  Σ−1
                           
                                                                                                      ,
                                                    2               Y1                          Y1




                                                     61
                                                             !0                  !
                                                          2                   2
                                          c           X                   X
                        g2 X 2,Y 2 =exp−                          Σ−1
                                  
                                                                                   ,
                                          2           Y2                  Y2

and
                                                                     
                                            1            1+c 0 −1
                       h(µ)=      n         o exp −         µΣ µ .
                             P rµ X 1 ∈X 1 θ̃              2
                                                             !             !           !!
                                                          ∗              1           2
                                                        X              X           X
     This exponential family structure shows that         ∗
                                                                =        1
                                                                             +c             is
                                                        Y              Y           Y2
sufficient for µ. Hence, for any function of (X 1,Y 1,X 2,Y 2), there exists a (potentially
randomized) function of (X ∗,Y ∗) with the same distribution for all µ. Thus, to study
questions of optimality it is without loss to limit attention to confidence intervals and
estimators that depend only on (X ∗,Y ∗).
     Now that we have derived a sufficient statistic (X ∗,Y ∗) for µ, we turn to the question
of
n how o   to construct optimal estimators and confidence intervals for µY (θ̃) conditional on
  θ̂1 = θ̃ . Note that the unconditional density of (X ∗,Y ∗) is proportional to

                                       !            !0                       !          !
                                    ∗                                     ∗
                   1               X                                     X
            exp−                           −(1+c)µ Σ−1                           −(1+c)µ .
                  2+2c             Y∗                                    Y∗

                              n        o
The density of (X ∗,Y ∗) given θ̂1 = θ̃ is thus proportional to

   n                   o                                               !0                           !
P r X 1 ∈X 1 θ̃ |X ∗,Y ∗
                                                          !                                  !
                                                      ∗                                  ∗
                              1                  X                                      X
       n         o exp−                                    −(1+c)µ Σ−1                        −(1+c)µ ,
   P rµ X 1 ∈X 1 θ̃          2+2c                Y∗                                     Y∗

 where we have used sufficiency to drop dependence of the numerator on µ.
    This joint distribution has the same exponential family structure used to derive the
optimal estimators and confidence intervals in the main text (see the proofs of Propositions
10 and 11). Hence, the same arguments deliver optimal procedures for the split-sample
setting. Specifically, for
                              !                      !           !        !
                         X∗                     X∗                         
               Zθ̃∗ =             − Cov                   ,Y ∗ θ̃ /ΣY ∗ θ̃ Y ∗ θ̃ ,
                         Y∗                     Y∗



                                                 62
where ΣY ∗ denotes the variance of Y ∗, we can re-write
                    !             !!        !
               X1            X2        −1
                                                 
                                               ∗
                                                                 
                                                                      ∗ +
                                                                                   
    exp                 +c           Σ µ =exp Y θ̃ µY ∗ θ̃ /ΣY ∗ θ̃ +Zθ̃ ΣZ ∗ µZ ∗
               Y1            Y2

for ΣZ ∗ the variance of Z ∗, A+ the Moore-Penrose pseudoinverse of a matrix A, and
                                                !      !         !
                                         X∗       ∗
                                                           
                                                             ∗
                                                                       
            µZ ∗ =(1+c)µ− Cov                   ,Y θ̃ /V ar Y θ̃  µY θ̃ .
                                                                    ∗
                                         Y∗

This
n expression
         o          shows that when we are interested in inference on µY (θ̃) conditional on
  θ̂ = θ̃ , µZ ∗ is the nuisance parameter, and Zθ̃∗ is minimal sufficient for this parameter
    1

relative to observing (X 1,Y 1,X 2,Y 2).
                   ∗
     If we let FSS   (Y ∗(θ̃); µY ∗ (θ̃), θ̃, z∗) denote the conditional distribution function of
Y ∗|Z ∗ = z∗, θ̂1 = θ̃, then the same arguments used to prove Proposition 10 show that
the optimal α quantile-unbiased estimator µ̂∗SS,α in the sample splitting problem solves

                              ∗
                             FSS (Y ∗(θ̂1);(1+c)µ̂∗SS,α,θ̃,Zθ̃∗)=1−α.

Likewise, the same arguments used to prove Proposition 11 show that the optimal two-sided
unbiased test rejects H0 :µY (θ̃)=µY,0 when

                                   Y ∗(θ̃)6∈ cl Zθ̃∗ ,cu Zθ̃∗ ,
                                                           


where cl(z), cu(z) solve

             P r{ζ ∈[cl(z),cu(z)]}=1−α, E[ζ1{ζ ∈[cl(z),cu(z)]}]=(1−α)E[ζ]

                                   ∗
with ζ distributed according to FSS  (·;(1+c)µY,0,θ̃,z). These optimal procedures condition
     ∗                1   1
on Zθ̃ rather than (X ,Y ) and so, unlike conventional sample splitting, continue to treat
(X 1,Y 1) as random for inference.
Feasible Dominating Estimators and Confidence Sets To implement the optimal
split-sample proecdures, we need to evaluate (or at least be able to draw from) the condi-
                     ∗
tional distribution FSS (·;(1+c)µY,0,θ̃,z). Unfortunately, however, it is not computationally
straightforward to do so since Y ∗|Z ∗ = z∗,θ̂1 = θ̃ is distributed as a normal random vari-
able truncated to a dependent random set. We thus introduce side constraints to derive

                                                63
procedures that, although they are not fully optimal in the unconstrained problem, are
computationally straightforward to implement and dominate conventional sample splitting
procedures. These computationally feasible procedures are optimal within the class of
split-sample procedures that condition on {θ̂1 = θ̃} and the realizations of
                                                          
                               Zθ̃i =X i −    ΣXY ·,θ̃ /ΣY θ̃ Y i θ̃

for i = 1,2, where (Zθ̃1,Zθ̃2) is a sufficient statistic for the nuisance parameter µX . Since
Y 2(θ̂1)|{θ̂1 = θ̃,(Zθ̃1,Zθ̃2)=(z1,z1)}∼Y 2(θ̃), the conventional split-sample estimator Y 2(θ̂1)
and confidence interval CSSS fall within the class of split-sample conditional procedures
that condition on {θ̂1 = θ̃} and (Zθ̃1,Zθ̃2). These conventional procedures are therefore
dominated by the optimal procedures within this class, which we now describe.
    Standard exponential family arguments show that (Zθ̃1,Zθ̃2) is sufficient for the nuisance
parameter µX and, conditional on {θ̂1 = θ̃} and (Zθ̃1,Zθ̃2), optimal estimation and inference
is based upon the conditional distribution of Y ∗(θ̃). Note that since Y 2(θ̃) is independent
of (Zθ̃1,Zθ̃2) and both θ̂1 and Y 2(θ̃) are independent of Zθ̃2,

            Y ∗(θ̃)|{θ̂1 = θ̃,(Zθ̃1,Zθ̃2)=(z1,z2)}∼Y 1(θ̃)|{θ̂1 = θ̃,Zθ̃1 =z1}+cY 2(θ̃).

Thus, the feasible dominating split-sample procedures rely upon the computation of the
distribution function of Y 1(θ̃)|{θ̂1 = θ̃,Zθ̃1 =z1}+cY 2(θ̃). We now describe a fast method
for computing this object.
    In analogy with full sample inference, let
                             n                          o
                     1     1   1 1                    1  1
                   Y (θ̃,z )= y :z + ΣXY ·,θ̃ /ΣY θ̃ y ∈X (θ̃)

so that conditional on {θ̂1 = θ̃} and Zθ̃1 = z1, Y 1(θ̃) follows a one-dimensional trun-
cated normal distribution with truncation set Y 1(θ̃,z1). Note that in both the level
and norm maximization contexts, Y 1(θ̃,z1) can be expressed as a finite union of disjoint
intervals: Y 1(θ̃,z1) = K         1       1                                 1              1
                       S
                        k=1 [`k (z ),uk (z )], where the dependence of `k (z ) and uk (z ) for
k =1,...,K on θ̃ is suppressed for notational simplicity. Note that Y 1(θ̃)|{θ̂1 = θ̃,Zθ̃1 =z1}
is distributed as ξ1|ξ1 ∈ Y 1(θ̃,z1), where ξ1 ∼ N(µY (θ̃),ΣY (θ̃)). The density function of




                                                   64
Y 1(θ̃)|{θ̂1 = θ̃,Zθ̃1 =z1} is thus
                                          q       
                           PK   1                          1   1      1
                       k=1 fN (y −µY (θ̃))/ ΣY (θ̃) 1(`k (z )≤y ≤uk (z ))
f 1(y1)= q                               q                            q       
                 PK
          ΣY (θ̃) k=1 FN (uk (z1)−µY (θ̃))/ ΣY (θ̃) −FN (`k (z1)−µY (θ̃))/ ΣY (θ̃)

                                                                     q        
and cY 2(θ̃) has density function f 2(y2)=c−1/2ΣY (θ̃)−1/2fN (y2 −cµ)/ cΣY (θ̃) . There-
fore, since Y 1(θ̃)|{θ̂1 = θ̃,Zθ̃1 = z1} and cY 2(θ̃) are independent, the density function of
Y ∗(θ̃)|{θ̂1 = θ̃,Zθ̃1 =z1} is equal to
                                     q                         q        
         PK R uk (z1)                               ∗
          k=1 `k (z1 ) fN (t−µY (θ̃))/ ΣY (θ̃) fN (y −t−cµY (θ̃))/ cΣY (θ̃) dt

  √
                                      q                            q       
           PK
   cΣY (θ̃) k=1 FN (uk (z1)−µY (θ̃))/ ΣY (θ̃) −FN (`k (z1)−µY (θ̃))/ ΣY (θ̃)

with corresponding distribution function

                                       A
                                      FSS (y∗;µY (θ̃),θ̃,z1)
                                    q                         q        
          PK R uk (z1)                             ∗
         k=1 `k (z1 ) fN (t−µY (θ̃))/ ΣY (θ̃) FN (y −t−cµY (θ̃))/ cΣY (θ̃) dt
 =q                                  q                           q        
          PK                1                             1
   ΣY (θ̃) k=1 FN (uk (z )−µY (θ̃))/ ΣY (θ̃) −FN (`k (z )−µY (θ̃))/ ΣY (θ̃)
                                 q                               
                  ∗     1                       1
                                                  SK       1      1
           E FN (y −ξ −cµY (θ̃))/ cΣY (θ̃) 1 ξ ∈ k=1[`k (z ),uk (z )]
      =                         q                            q        ,
        PK            1                              1
         k=1 FN (uk (z )−µY (θ̃))/ ΣY (θ̃) −FN (`k (z )−µY (θ̃))/ ΣY (θ̃)


where the expectation is taken with respect to ξ1 ∼N(µY (θ̃),ΣY (θ̃)). This latter expression
      A
for FSS (y∗;µY (θ̃),θ̃,z1) is very easy to compute by generating normal random variables in
standard software packages. This makes the computation of optimal estimators, tests and
confidence intervals within the class discussed here computationally straightforward.
    Similarly to the optimal case above, the same arguments used to prove Proposition 2
show that the optimal α quantile-unbiased estimator µ̂A  SS,α in the sample splitting problem




                                               65
that conditions on {θ̂1 = θ̃} and the realizations of Zθ̃1 and Zθ̃2 solves

                                 A
                                FSS (Y ∗(θ̂1);µ̂A         1
                                                SS,α ,θ̃,Zθ̃ )=1−α.


                                                                               A
Therefore, our (equal-tailed) alternative split-sample confidence interval is CSS =[µ̂A         A
                                                                                      SS,α/2 ,µ̂SS,1−α/2 ].
Likewise, the same arguments used to prove Proposition 11 show that the optimal two-sided
unbiased test rejects H0 :µY (θ̃)=µY,0 when

                                    Y ∗(θ̃)6∈ cl Zθ̃1 ,cu Zθ̃1 ,
                                                            


where cl(z), cu(z) solve

              P r{ζ ∈[cl(z),cu(z)]}=1−α, E[ζ1{ζ ∈[cl(z),cu(z)]}]=(1−α)E[ζ]

                                   A
with ζ distributed according to FSS  (·;µY,0,θ̃,z). These dominating procedures condition
      1               1   1
on Zθ̃ rather than (X ,Y ), and so unlike conventional sample splitting continue to treat
(X 1,Y 1) as random for inference.

D     Uniformity Proofs
In this section, we prove the uniform asymptotic results stated in the main text. One
could use arguments along the same lines as those below to derive results for additional
conditioning variables γ̂n, but since such arguments would be case-specific, we do not
pursue such an extension here.
D.1    Auxiliary Lemmas
This section collects lemmas that we will use to prove our uniformity results.

Lemma 5
Under Assumption 4, for any sequence
                              n     of confidence
                                                  ointervals CSn, any sequence of sets
Cn(P ) indexed by P , Cn(P )=1 Xn,Yn,Σb n ∈Cn(P ) , and any constant α, to show that

                        n                      o
         limsup sup P rP µY,n θ̂n;P ∈CSn|Cn(P )=1 −α P rP {Cn(P )=1}=0
          n→∞ P ∈Pn


it suffices to show that for all subsequences {ns}⊆{n}, {Pns }∈P ∞ =×∞
                                                                     n=1 Pn with:




                                                66
   1. Σ(Pns )→Σ∗ ∈S for
             n                                                                  o
          S = Σ:1/λ̄≤(ΣX (θ),ΣY (θ))≤ λ̄,1/λ̄≤ΣX (θ;P )−ΣX (θ,θ̃;P )2/ΣX (θ̃;P ) ,      (26)


   2. P rPns {Cns (Pns )=1}→p∗ ∈(0,1], and

   3. µX,ns (Pns )−maxθ µX,ns (θ;Pns )→µ∗X ∈M∗X for
                                    n                       o
                                               |Θ|
                            M∗X =    µX ∈[−∞,0] :maxµX (θ)=0 ,
                                                        θ


we have                       n                                 o
                    lim P rPns µY,ns θ̂ns ;Pns ∈CSns |Cns (Pns )=1 =α.                  (27)
                   s→∞

Lemma 6                                                           n                   o
For collections of sets Cn,1 (P ),...,Cn,J (P ), and Cn,j (P ) = 1 Xn,Yn,Σb n ∈Cn,j (P ) , if
limn→∞supP ∈Pn P rP {Cn,j (P )=1,Cn,j0 (P )=1}=0 for all j =    6 j 0 and
                  n                         o
      lim sup P rP µY,n θ̂n;P ∈CSn|Cn,j (P )=1 −(1−α) P rP {Cn,j (P )=1}=0
      n→∞P ∈Pn


for all j, then
                    n             o                  X
     liminf inf P rP µY,n θ̂n;P ∈CSn ≥(1−α)·liminf inf  P rP {Cn,j (P )=1},
      n→∞ P ∈Pn                                       n→∞ P ∈Pn
                                                                  j

                     n             o                X
      limsup sup P rP µY,n θ̂n;P ∈CSn ≤1−α·liminf inf  P rP {Cn,j (P )=1}.
       n→∞ P ∈Pn                                     n→∞ P ∈Pn
                                                                  j

   To state the next lemma, define
                                                                  
                                                     ΣY θ̃ Z(θ)−Z θ̃
                   L θ̃,Z,Σ =           max                                         (28)
                                θ∈Θ:ΣXY (θ̃)>ΣXY (θ̃,θ) Σ
                                                         XY θ̃ −ΣXY θ̃,θ

                                                                  
                                                    ΣY θ̃ Z(θ)−Z θ̃
                  U θ̃,Z,Σ =           min                        ,                 (29)
                               θ∈Θ:ΣXY (θ̃)<ΣXY (θ̃,θ) Σ    θ̃ −Σ
                                                        XY       XY θ̃,θ

where we define a maximum over the empty set as −∞ and a minimum over the empty



                                             67
set as +∞. For                     !                                     !
                             Xn∗            Xn −maxθ µX,n(θ;P )
                                       =                                   ,
                             Yn∗               Yn −µY,n(P )
                                  
we next show that using Xn∗,Yn∗,Σ
                                b n in our calculations yields the same bounds L and
                    
U as using Xn,Yn,Σn , up to additive shifts
                  b

Lemma 7               
For L θ̃,Z,Σ and U θ̃,Z,Σ as defined in (28) and (29), and

                                                              
                          b XY,n ·,θ̃
                          Σ                            b XY,n ·,θ̃
                                                         Σ             
                                             ∗
              Zθ̃,n =Xn −         Yn θ̃ , Zθ̃,n =Xn∗ −         Yn∗ θ̃ ,
                           Σ
                           b Y,n θ̃                       Σ
                                                          b Y,n θ̃

we have
                                                                    
       ∗ b                                       ∗ b
 L θ̃,Zθ̃,n,Σn =L θ̃,Zθ̃,n,Σn −µY,n θ̃;P , U θ̃,Zθ̃,n,Σn =U θ̃,Zθ̃,n,Σn −µY,n θ̃;P .
                           b                                         b

    For brevity, going forward we use the shorthand notation
                                                             
       L θ̃,Zθ̃,n,Σ
                  b n ,U θ̃,Z ,Σ  b n ,L θ̃,Z ∗ ,Σb n ,U θ̃,Z ∗ ,Σb n =(Ln,Un,L∗ ,U ∗).
                             θ̃,n            θ̃,n            θ̃,n              n n


Lemma 8
Under Assumptions 2 and  3, for any{ns} and {Pns } satisfying
                                                                conditions (1)-(3)
                                                                                   of Lemma
                      ∗                 ∗   ∗    ∗ b                ∗ ∗ ∗ ∗
5 and any θ̃ with µX θ̃ > −∞, Yns ,Lns ,Uns ,Σns ,θ̂ns →d Y ,L ,U ,Σ ,θ̂ , where the
                                                                            0   0 0
objects on the right hand side are calculated based on (Y ∗,X ∗,Σ∗) for X ∗ ,Y ∗ ∼N(µ∗,Σ∗)
with µ∗ =(µ∗0    0 0
             X ,0 ) .

Lemma 9
For FN again the standard normal distribution function, the function
                                                                              
                                                Y (θ)∧U−µ
                                       FN           √           −FN     √L−µ
                                                     ΣY (θ)             ΣY (θ)
          FT N (Y (θ);µ,ΣY (θ),L,U)=                                       1(Y (θ)≥L)   (30)
                                           FN       √U−µ        −FN   √L−µ
                                                     ΣY (θ)             ΣY (θ)


is continuous in (Y (θ),µ,ΣY (θ),L,U) on the set

        (Y (θ),µ,ΣY (θ))∈R3,L∈R∪{−∞},U ∈R∪{∞}:ΣY (θ)>0,L<Y (θ)<U .
       


                                                    68
D.2    Proofs for Auxiliary Lemmas
Proof of Lemma 5 To prove that
                         n                      o
          limsup sup P rP µY,n θ̂n;P ∈CSn|Cn(P )=1 −α P rP {Cn(P )=1}=0
          n→∞ P ∈Pn


it suffices to show that
                            n                      o 
           liminf inf    P rP µY,n θ̂n;P ∈CSn|Cn(P )=1 −α P rP {Cn(P )=1}≥0               (31)
            n→∞ P ∈Pn


and                      n                      o 
           limsup sup P rP µY,n θ̂n;P ∈CSn|Cn(P )=1 −α P rP {Cn(P )=1}≤0.                 (32)
           n→∞ P ∈Pn

We prove that to show (31), it suffices to show that for all {ns}, {Pns } satisfying conditions
(1)-(3) of the lemma,
                                 n                                 o
                    liminf P rPns µY,ns θ̂ns ;Pns ∈CSns |Cns (Pns )=1 ≥α.                 (33)
                    s→∞


An argument along the same lines implies that to prove (32) it suffices to show that
                              n                                 o
                  limsupP rPns µY,ns θ̂ns ;Pns ∈CSns |Cns (Pns )=1 ≤α.                    (34)
                    s→∞


Note, however, that (33) and (34) together are equivalent to (27).
   Towards contradiction, suppose that (31) fails, so
                     n                      o 
       liminf inf P rP µY,n θ̂n;P ∈CSn|Cn(P )=1 −α P rP {Cn(P )=1}<−ε,
        n→∞ P ∈Pn


for some ε > 0 but that (33) holds for all sequences satisfying conditions (1)-(3) of the
lemma. Then there exists an increasing sequence of sample sizes nq and some sequence

  Pnq with Pnq ∈Pnq for all q such that
               n                             o                
   limsup P rPnq µY,nq θ̂nq ;Pnq ∈CSnq |Cnq Pnq =1 −α P rPnq Cnq Pnq =1 <−ε.              (35)
    q→∞


We want to show that there exists a further subsequence {ns}⊆{nq } satisfying (1)-(3) in
the statement of the lemma, and so establish a contradiction.
   Note that since the set S defined in (26) is compact (e.g. in the Frobenius norm),
                                      
and Assumption 4 implies that Σ Pnq ∈ S for all q, there exists a further subsequence


                                              69
{nr }⊆{nq } such that
                                       lim Σ(Pnr )→Σ∗
                                      r→∞

for some Σ∗ ∈S.
    Note, next, that P rPnr {Cnr (Pnr )=1} ∈ [0,1] for all r, and so converges along a sub-
sequence {nt}⊆{nr }. However, (35) implies that P rPnr {Cnr (Pnr )=1}≥ αε for all r, and
thus that P rPnt {Cnt (Pnt )=1}→p∗ ∈ αε ,1 .
                                          

    Finally, let us define µ∗X,n (P ) = µX,n (P )−maxθ µX,n (θ;P ), and note that µ∗X,n (P ) ≤ 0
by construction. Since µ∗X,n(P ) is finite-dimensional and maxθ µ∗X,n(P ;θ)=0, there exists
some θ ∈Θ such that µ∗X,n(P ;θ) is equal to zero infinitely often. Let {nu}⊆{nt} extract
the corresponding sequence of sample sizes. The set [−∞,0]|Θ| is compact under the metric
d(µX ,µ̃X )=kFN (µX )−FN (µ̃X )k for FN (·) the standard normal cdf applied elementwise,
and k·k the Euclidean norm. Hence, there exists a further subsequence {ns}⊆{nu} along
which µ∗X,ns (Pns ) converges to a limit in this metric. Note, however, that this means that
µ∗X,ns (Pns ) converges to a limit µ∗ ∈M∗ in the usual metric.
    Hence, we have shown that there exists a subsequence {ns}⊆{nq } that satisfies (1)-(3).
By supposition, (33) must hold along this subsequence. Thus,
                 n                                 o 
     liminf P rPns µY,ns θ̂ns ;Pns ∈CSns |Cns (Pns )=1 −α P rP {Cns (Pns )=1}≥0,
      n→∞


which contradicts (35). Hence, we have established a contradiction and so proved that (33)
for all subsequences satisfying conditions (1)-(3) of the lemma implies (31). An argument
along the same lines shows that (34) along all subsequences satisfying conditions (1)-(3)
of the lemma implies (32). 
Proof of Lemma 6 Define Cn,J+1(P )=1{Cn,j (P )=0 for all j ∈{1,...,J}}. Note that
                                     n               o
                                 P rP µY,n θ̂n;P ∈CSn
                     n                             o
          = J+1
           P
            j=1 P rP  µY,n  θ̂n ;P   ∈CS  |C
                                         n n,j (P )=1 P rP {Cn,j (P )=1}+o(1)

where the o(1) term is negligible uniformly over P ∈Pn as n→∞. Hence,
                            n             o
                        P rP µY,n θ̂n;P ∈CSn −(1−α)
     PJ+1    n                         o      
    = j=1 P rP µY,n θ̂n;P ∈CSn|Cn,j (P )=1 −(1−α) P rP {Cn,j (P )=1}+o(1)




                                              70
and                                      n             o
                          liminf inf P rP µY,n θ̂n;P ∈CSn −(1−α)
                           n→∞ P ∈Pn

               J+1 
               X        n                         o      
   =liminf inf      P rP µY,n θ̂n;P ∈CSn|Cn,j (P )=1 −(1−α) P rP {Cn,j (P )=1}
       n→∞ P ∈Pn
                   j=1
                 n                          o      
  =liminf inf P rP µY,n θ̂n;P ∈CSn|Cn,J+1(P )=1 −(1−α) P rP {Cn,J+1(P )=1}
      n→∞ P ∈Pn

                            ≥−(1−α)limsup sup P rP {Cn,J+1(P )=1}
                                       n→∞ P ∈Pn

                                              J
                                                                  !
                                              X
                         =−(1−α) 1−liminf inf   P rP {Cn,j (P )=1}
                                       n→∞ P ∈Pn
                                                   j=1

which immediately implies that

                     n             o                 J
                                                       X
      liminf inf P rP µY,n θ̂n;P ∈CSn ≥(1−α)liminf inf   P rP {Cn,j (P )=1}.
       n→∞ P ∈Pn                                         n→∞ P ∈Pn
                                                                     j=1


   Likewise,                             n             o
                          limsup sup P rP µY,n θ̂n;P ∈CSn −(1−α)
                           n→∞ P ∈Pn

              J+1 
              X        n                         o      
  =limsup sup      P rP µY,n θ̂n;P ∈CSn|Cn,j (P )=1 −(1−α) P rP {Cn,j (P )=1}
       n→∞ P ∈Pn
                   j=1
                 n                          o      
  =limsup sup P rP µY,n θ̂n;P ∈CSn|Cn,J+1(P )=1 −(1−α) P rP {Cn,J+1(P )=1}
      n→∞ P ∈Pn

                                                       J
                                                                           !
                                                       X
      ≤α·limsup sup P rP {Cn,J+1(P )=1}=α 1−liminf inf   P rP {Cn,j (P )=1} .
           n→∞ P ∈Pn                                     n→∞ P ∈Pn
                                                                     j=1

This immediately implies that

                     n             o                J
                                                      X
      limsup sup P rP µY,n θ̂n;P ∈CSn ≤1−α·liminf inf   P rP {Cn,j (P )=1},
       n→∞ P ∈Pn                                         n→∞ P ∈Pn
                                                                     j=1


as we wanted to show. 




                                              71
Proof of Lemma 7 Note that
                                                                 
                                                          µY,n θ̃;P
                      ∗
                     Zθ̃,n =Zθ̃,n −maxµX,n(θ;P )+ Σ
                                                  b XY,n ·,θ̃       ,
                                    θ
                                                              ΣY,n θ̃
                                                              b

so
                                                                             
                                                              µY,n θ̃;P
       ∗         ∗                             b XY,n θ,θ̃ − Σ
      Zθ̃,n (θ)−Zθ̃,n  θ̃ =Zθ̃,n(θ)−Zθ̃,n θ̃ + Σ             b XY,n θ̃          .
                                                                         Σ
                                                                         b Y,n θ̃

The result follows immediately. 
Proof of Lemma 8 By Assumption 2
                                                    !
                                Xns −µX,ns (Pns )
                                                        →d N(0,Σ∗).
                                Yns −µY,ns (Pns )

Hence, by Slutsky’s lemma
                     !                                  !             !
              Xn∗s           Xns −maxθ µX,ns (θ;Pns )            X∗
                         =                                  →d            ∼N(µ∗,Σ∗).
              Yn∗s               Yns −µY,ns (Pns )               Y∗
                                      n o
                                                           b ns →p Σ∗ by Assumption
     We begin by considering one θ ∈Θ\ θ̃ at a time. Since Σ
                  
3, if Σ∗XY θ̃ −Σ∗XY θ̃,θ =6 0 then

                                                     
                         ∗        ∗         ∗       ∗      ∗
              ΣY,ns θ̃ Zθ̃,n (θ)−Zθ̃,n θ̃
              b                            ΣY θ̃ Zθ̃ (θ)−Zθ̃ θ̃
                         s       s  →d                ,
                                              ∗         ∗
                ΣXY,ns θ̃ − ΣXY,ns θ̃,θ
                b            b              ΣXY θ̃ −ΣXY θ̃,θ

where the terms on the right hand side are based on (X ∗,Y ∗,Σ∗). The limit is finite if
µ∗X (θ)>−∞, while otherwise µ∗X (θ)=−∞ and
                                                
            Σ∗Y        ∗      ∗
                 θ̃ Zθ̃ (θ)−Zθ̃ θ̃   −∞ if Σ∗ θ̃ −Σ∗ θ̃,θ >0
                                             XY      XY
                             =                      .
               ∗           ∗                 ∗       ∗
              ΣXY θ̃ −ΣXY θ̃,θ       +∞ if ΣXY θ̃ −ΣXY θ̃,θ <0




                                              72
                                                            q
    If instead   Σ∗XY         ∗                       ∗
                         θ̃ −ΣXY θ̃,θ =0, then since ΣX (θ̃,θ) < Σ∗X (θ̃)Σ∗X (θ),

                                                             
                                   Zθ̃∗(θ)−Zθ̃∗ θ̃ =X ∗(θ)−X ∗ θ̃

is normally distributed with non-zero variance. Hence, in this case
                                                          
                                b Y,ns θ̃ Z ∗ (θ)−Z ∗ θ̃
                                Σ           n ,θ̃    ns ,θ̃
                                           s                 →∞.                             (36)
                                   ΣXY,ns θ̃ − ΣXY,ns θ̃,θ
                                   b              b

    Let us define              n                        o
                           Θ∗ θ̃ = θ ∈Θ\ θ̃ :Σ∗XY θ̃ −Σ∗XY θ̃,θ =
                                                                6 0 .

The argument above implies that
                                                                               
                                                                   ∗         ∗
                                                       ΣY,ns θ̃ Zθ̃,n (θ)−Zθ̃,n θ̃
                                                       b
                                  max                             s        s 
                             b XY,n (θ̃)>Σ
                   θ∈Θ∗ (θ̃):Σ     s
                                         b XY,n (θ̃,θ)
                                               s         b XY,ns θ̃ − Σ
                                                         Σ            b XY,ns θ̃,θ

                                                                          
                                                         Σ∗Y θ̃ Zθ̃∗(θ)−Zθ̃∗ θ̃
                        →d L∗ =          max                             ,
                                θ∈Θ:Σ∗XY (θ̃)>Σ∗XY (θ̃,θ) Σ∗    θ̃ −Σ  ∗
                                                            XY         XY θ̃,θ

and                                                                              
                                                      b Y,ns θ̃ Z ∗ (θ)−Z ∗ θ̃
                                                      Σ            θ̃,n      θ̃,ns
                                min                               s         
                   θ∈Θ (θ̃):ΣXY,ns (θ̃)<ΣXY,ns (θ̃,θ)
                      ∗     b           b
                                                         ΣXY,ns θ̃ − ΣXY,ns θ̃,θ
                                                         b              b
                                                                            
                                                          Σ∗Y θ̃ Zθ̃∗(θ)−Zθ̃∗ θ̃
                     →d U ∗ =             min                               .
                               θ∈Θ:Σ∗XY (θ̃)<Σ∗XY (θ̃,θ) Σ∗      θ̃   −Σ  ∗
                                                             XY           XY θ̃,θ

By (36), the same convergence holds when we minimize and maximize over Θ rather than
Θ∗(θ̃). Hence,    L∗ns ,Un∗s →d (L∗,U ∗). Moreover, θ̂ns is almost everywhere continuous in Xn∗s ,
                               

so Yn∗s ,Σb ns ,θ̂ns →d Y ∗,Σ∗,θ̂ by the continuous mapping theorem, and this convergence
holds jointly with that for L∗ns ,Un∗s . Hence, we have established the desired convergence. 
                                          

Proof of Lemma 9 Continuity for ΣY (θ) > 0,L < Y (θ) < U with all elements finite
is immediate from the functional form. Moreover, for fixed (Y (θ),µ,ΣY (θ)) ∈ R3 with




                                                   73
ΣY (θ)>0 and L<Y (θ)<U,
                                                                                                  
                     Y (θ)∧U−µ
            FN       √             −FN         √L−µ      FN √          Y (θ)−µ
                                                                      −FN                     √L−µ
                        ΣY (θ)                  ΣY (θ)       ΣY (θ)                            ΣY (θ)
      lim                                 1(Y (θ)≥L)=                    
      U→∞
            FN √U−µ                −FN √L−µ
                                                         FN √ ∞
                                                                      −FN √ L−µ
                        ΣY (θ)                  ΣY (θ)                   ΣY (θ)                 ΣY (θ)

                                                                                                  
                     Y (θ)∧U−µ
            FN        √              −FN       √L−µ        FN √         Y (θ)−µ
                                                                        −FN                   √−∞
                          ΣY (θ)                ΣY (θ)         ΣY (θ)                          ΣY (θ)
      lim                                   1(Y (θ)≥L)=                   
    L→−∞
            FN √U−µ                  −FN √L−µ
                                                           FN √U−µ
                                                                        −FN √ −∞
                          ΣY (θ)                ΣY (θ)                   ΣY (θ)                 ΣY (θ)

and
                                                                                                       
                          Y (θ)∧U−µ
                 FN        √             −FN        √L−µ       FN √         Y (θ)−µ
                                                                            −FN                    √−∞
                             ΣY (θ)                  ΣY (θ)        ΣY (θ)                           ΣY (θ)
      lim                                       1(Y (θ)≥L)=                   .
(L,U)→(−∞,∞)
                 FN √U−µ                 −FN √L−µ
                                                               FN √ ∞
                                                                            −FN √ −∞
                             ΣY (θ)                  ΣY (θ)                  ΣY (θ)                  ΣY (θ)


Hence, we obtain the desired result. 
D.3    Proofs for Uniformity Results
Proof of Proposition 7 Note that
                                                             
                                 µ̂α,n ≥µY,n θ̂n;P ⇐⇒ µY,n θ̂n;P ∈CSU,−,n

for CSU,−,n = (−∞, µ̂α,n]. Hence, by Lemma 5, to prove that (12) holds it suffices to
show thatnfor allo{ns} and {Pns } such that conditions (1)-(3) of the lemma hold with
Cn(P )=1 θ̂n = θ̃ , we have

                                      n                                   o
                            lim P rPns µ̂Y,ns θ̂ns ;Pns ∈CSU,−,ns |θ̂ns = θ̃ =α.                             (37)
                            s→∞


     To this end,recall
                      that
                          for   FT N(Y (θ);µ,Σ
                                                  Y (θ),L,U)
                                                          as defined in    (30), the estimator
µ̂α,n solves FT N Yn θ̂n ;µ,Σb Y,n θ̂n ,L θ̂n,Z ,Σ
                                                θ̂n ,n n ,U θ̂n ,Zθ̂n ,n ,Σn
                                                      b                   b     =1−α. This cdf is
                                                                                               
strictly decreasing in µ as argued in the proof of Proposition 5, and is increasing in Yn θ̂ .
                         
Hence, µ̂α,n ≥µY,n θ̂n;P if and only if

                                                                     
       FT N Yn θ̂n ;µY,n θ̂n;P ,Σ
                                b Y,n θ̂n ,L θ̂n,Z ,Σ    b n ,U θ̂n,Z ,Σ    b n ≥1−α.
                                                  θ̂n ,n             θ̂n ,n



                                                              74
   Note, next, that by Lemma 7 and the form of the function FT N ,
                                                                        
          FT N Yn θ̂n ;µY,n θ̂n;P ,Σ
                                   b Y,n θ̂n ,L θ̂n,Z ,Σ
                                                     θ̂n ,n
                                                            b n ,U θ̂n,Z ,Σ
                                                                        θ̂n ,n
                                                                               bn

                                                                
                        ∗                      ∗                ∗
              =FT N Yn θ̂n ;0,ΣY,n θ̂n ,L θ̂n,Zθ̂n,n,Σn ,U θ̂n,Zθ̂n,n,Σn ,
                                b                    b                b
                   
so µ̂α,n ≥µY,n θ̂n;P if and only if

                                                                  
           FT N Yn∗ θ̂n ;0,Σ
                           b Y,n θ̂n ,L θ̂n,Z ∗ ,Σ
                                             θ̂n ,n
                                                    b n ,U θ̂n,Z ∗ ,Σ
                                                                θ̂n ,n
                                                                       b n ≥1−α.

                                                     
Lemma 8 shows that Yn∗ θ̂ns ,Σ  b Y,ns θ̂ns ,L∗ ,U ∗ ,θ̂ns converges in distribution as s→∞,
                                              ns ns
so since FT N is continuous by Lemma 9 while argmaxθ X ∗(θ) is almost surely unique and
continuous for X ∗ as in Lemma 8, the continuous mapping theorem implies that
                                                    n           o
                           ∗                      ∗    ∗
                    FT N Yns θ̂ns ;0,ΣY,ns θ̂ns ,Lns ,Uns ,1 θ̂ns = θ̃
                                     b
                                                  n        o
                     →d FT N Y ∗ θ̂ ;0,Σ∗Y θ̂ ,L∗,U ∗ ,1 θ̂ = θ̃ .

   Since we can write
                    n                                                 o
              P rPns FT N Yn∗s θ̂ns ;0,Σ
                                       b Y,ns θ̂ns ,L∗n ,Un∗ ≥1−α|θ̂ns = θ̃
                                                       s    s


                h n                                    o n          oi
                          ∗                       ∗    ∗
            EPns 1 FT N Yns θ̂ns ;0,ΣY,ns θ̂ns ,Lns ,Uns ≥1−α 1 θ̂ns = θ̃
                                    b
          =                            h n          oi                       ,
                                   EPns 1 θ̂ns = θ̃

and by construction (see also Proposition 10 in the main text),
                                                  
                       FT N Y ∗ θ̂ ;0,Σ∗Y θ̂ ,L∗,U ∗,θ̂ |θ̂ = θ̃ ∼U[0,1],

       n       o
and P r θ̂ = θ̃ =p∗ >0, we thus have that

                    n                                                o
              P rPns FT N Yn∗s θ̂ns ;0,Σ
                                       b Y,ns θ̂ns ,L∗ ,U ∗ ≥1−α|θ̂ns = θ̃
                                                     ns ns

                     n                                    o
                 →P r FT N Y ∗ θ̂ ;0,Σ∗Y θ̂ ,L∗,U ∗ ≥1−α|θ̂ = θ̃ =α,

which verifies (37).


                                              75
                                          ∈ Θ, and Assumptions
     Since this argument holds for all θ̃ n           o        2 and 4 imply that for all
θ,θ̃ ∈Θ with θ =6 θ̃, limn→∞supP ∈Pn P rP Xn(θ)=Xn θ̃ =0, Lemma 6 implies (13). 
                                                                    
Proof of Corollary 1 By construction, CSET,n = µ̂α/2,n,µ̂1−α/2,n , and µ̂1−α/2,n > µ̂α/2,n
for all α<1. Hence,               n                          o
                              P rP µY,n θ̂n;P ∈CSET,n|θ̂n = θ̃
                n                           o      n                          o
         =P rP µY,n θ̂n;P ≤ µ̂1−α/2,n|θ̂n = θ̃ −P rP µY,n θ̂n;P ≤ µ̂α/2,n|θ̂n = θ̃ ,

so the result is immediate from Proposition 7 and Lemma 6. 
Proof of Proposition 8 By the same argument as in the proof of Lemma 5, to show
that (14) holds it suffices ton showthat forall {ns}, o
                                                        {Pns } satisfying conditions (1)-(3) of
Lemma 5, liminf n→∞P rPns µY,ns θ̂ns ;Pns ∈CSP,ns ≥1−α.
   To this end, note that
                                                       r               r  
                                         ∗
 µY,ns θ̂ns ;Pns ∈CSP,ns if and only if Yns θ̂ns ∈ −cα ΣY,ns
                                                        b      ΣY θ̂ns ,cα ΣY,ns
                                                               b           b       Σ
                                                                                   b Y θ̂ns

                                            p
for cα(ΣY ) the 1−α quantile of maxθ |ξ(θ)|/ ΣY (θ) where ξ ∼N(0,ΣY ). Next, note that
                                                                                 p
cα (ΣY ) is continuous in Σ on S as defined in (26). 
                                                     Hence, for all θ, c
                                                                        α (ΣY )  ΣY (θ) is
                                                                                  
continuous as well. Assumptions 2 and 3 imply that Yn∗s ,Σ
                                                         b ns ,θ̂ns →d Y ∗,Σ∗,θ̂ , which
by the continuous mapping theorem implies
                                             !                    r  !
                               r                
             Yn∗s θ̂ns ,cα Σ
                           b Y,ns   Σ
                                    b Y θ̂ns   →d Y ∗ θ̂ ,cα(Σ∗Y ) Σ∗Y θ̂ .

                                 r   
                        ∗        ∗
Hence, since P r       Y θ̂ −cα(ΣY ) Σ∗Y θ̂ =0 =0,

                                       (       "       r            r  #)
          n                     o         
                                         ∗          ∗              ∗
    P rPns µY,ns θ̂ns ;Pns ∈CSP,ns →P r Y θ̂ ∈ −cα(ΣY ) ΣY θ̂ ,cα(ΣY ) Σ∗Y θ̂
                                                         ∗                                  (38)


 where the right hand side is at least 1−α by construction. 
                                                                                          
Proof of Proposition 9 Note that µ̂H        α,n ≥ µY,n  θ̂n ;P   if and only if µY,n  θ̂n ;P   ∈
    H          H
CSU,−,n for CSU,−,n  =(−∞,µ̂H α,n ]. Hence, by Lemma 5, to prove that (15) holds it suffices
to show that for all {ns} and {Pns } such that conditions (1)-(3) of the lemma hold with




                                              76
        n                          o
                                 β
Cn(P )=1 θ̂n = θ̃,µY,n θ̂n;Pn ∈CSP,n  , we have

                     n                                                       o
                                          H                                  β
           lim P rPns µ̂Y,ns θ̂ns ;Pns ∈CSU,−,n|θ̂ns = θ̃,µY,ns θ̂ns ;Pns ∈CSP,ns =α.
          s→∞


      Recall that for FT N (Y (θ);µ,ΣY (θ),L,U) defined as in (30), µ̂H
                                                                      α,n solves

                                                        
                                               H         H
                     FT N Yn θ̂n ;µ,ΣY,n θ̂n ,Ln µ,θ̂n ,Un µ,θ̂n =1−α,
                                    b

for                           (                                       )
                                                        r  
                 LH
                  n µ,θ̂n =max L θ̂n ,Zθ̂n ,n ,Σn ,µ−cα ΣY,n
                                               b         b     Σ
                                                               b Y θ̂n ,

                                  (                                       )
                                                            r  
                 UnH    µ,θ̂n =min U θ̂n,Zθ̂n,n,Σ
                                                b n ,µ+cα Σb Y,n   Σ
                                                                   b Y θ̂n .

                                                                           
                                                     b Y,n θ̂n ,LH µ,θ̂n ,U H µ,θ̂n is
The proof of Proposition 5 shows that FT N Yn θ̂n ;µ,Σ           n         n
strictly decreasing in µ, so for a given value µY,0,
                                                                    
       µ̂H
         α,n ≥µY,0
                                                     H            H
                     ⇐⇒ FT N Yn θ̂n ;µY,0,ΣY,n θ̂n ,Ln µY,0,θ̂n ,Un µY,0,θ̂n ≥1−α.
                                          b

      As in the proof of Proposition 7
                                                                                
                                  b Y,n θ̂n ,LH
        FT N Yn θ̂n ;µY,n θ̂n;Pn ,Σ             n  µY,n   θ̂n ;Pn  ,θ̂n ,U H µ
                                                                               Y,n  θ̂n ;Pn  ,θ̂n
                                                                  n 
                       =FT N Yn∗ θ̂n ;0,Σ  b Y,n θ̂n ,LH∗ θ̂n ,U H∗ θ̂n ,
                                                        n             n


where                                (                                        )
                                                                r  
                      LH∗
                       n     θ̂n =max L θ̂n,Zθ̂∗      ,Σ
                                                       b ,−cα Σ
                                                    ,n n
                                                               b Y,n   Σ
                                                                       b Y θ̂n ,
                                                n

                                    (                                            )
                                                                  r  
                       UnH∗ θ̂n =min U θ̂n,Zθ̂∗           ,Σ
                                                           b ,cα Σ
                                                        ,n n
                                                                 b Y,n   Σ
                                                                         b Y θ̂n
                                                    n

                                                                      
so µ̂H                                         ∗                   H∗ θ̂ ,U H∗ θ̂
     α,n ≥µY,n θ̂n ;P if and only if FT N Yn θ̂n ;0,ΣY,n θ̂n ,Ln        n        n   ≥1−α.
                                                        b
                                                                           n
                                                                                 
    Lemma 8 implies that Yn∗s ,Σ   b Y,ns ,LH∗ θ̃ ,U H∗ θ̃ ,θ̂ns →d Y ∗,Σ∗ ,LH∗ θ̃ ,U H∗ θ̃ ,θ̂ ,
                                            ns       ns                   Y
                                                          
where L  H∗  θ̃ and U  H∗                      H∗           H∗
                           θ̃ are equal to Ln θ̃ and Un θ̃ after replacing (Xn,Yn,Σ    b n) with




                                                    77
(X,Y,Σ∗). Then by the continuous mapping theorem and (38),
                                                 n                                   o
                                                                                              β
         FT N Yn∗s θ̂ns ;0,Σ
                           b Y,ns θ̂n ,LH∗
                                     S     ns  θ̃ ,U H∗ θ̃
                                                    ns        ,1 θ̂ns = θ̃,µ     θ̂  ;P
                                                                            Y,ns ns ns    ∈CSP,ns
                                                                 r             r  
          ∗        ∗
 →d FT N Y θ̂ ;0,ΣY θ̂ ,L   H∗   θ̃ ,U H∗                  ∗                 ∗      ∗       ∗
                                           θ̃ ,1 θ̂ = θ̃,Y θ̂ ∈ −cα(ΣY ) ΣY θ̂ ,cα(ΣY ) Σ∗Y θ̂          .

Hence, by the same argument as in the proof of Proposition 7,
                        n                                                   o
                                            H                                β
              lim P rPns µY,ns θ̂ns ;Pns ∈CSU,−,n |θ̂
                                                 s ns
                                                      = θ̃,µ     θ̂ ;P
                                                            Y,ns ns ns   ∈CSP,ns =α,
              s→∞

as we aimed to show.
                                   f H =(µ̂H ,∞),
      To prove (16), note that for CS U,+,n α,n


                                                                      fH
                                                               
                            µ̂H
                              α,n ≥µY,n  θ̂n ;P   ⇐⇒ µY,n  θ̂n ;P   ∈
                                                                    6 CS U,+,n


and thus that the argument above proves that

                                       f H |C H θ̃;P −(1−α) P rP C H θ̃;P =0
                         n                       o          n  o
             lim sup P rP µY,n θ̂n;P ∈ CS U,+,n n                 n
         n→∞P ∈Pn

        
for CnH θ̃;P as in the statement of the proposition. Since

           n                                o       n                   o
                                         β                               β
      X
       P rP θ̂ns = θ̃,µY,ns θ̂ns ;Pns ∈CSP,ns
                                                =P rP  µ     θ̂ ;P
                                                        Y,ns ns ns   ∈CSP,ns +o(1),      (39)
      θ̃∈Θ

and Proposition 8 shows that
                                        n                      o
                                                            β
                         liminf inf P rP µY,ns θ̂ns ;Pns ∈CSP,ns
                                                                   ≥1−β,
                          s→∞ P ∈Pns


Lemma 6 together with (15) implies that
                               n                  o
                liminf inf P rP µ̂H
                                  α,n <µY,n θ̂n ;P    ≥(1−α)(1−β)=(1−α)−β(1−α)
                n→∞ P ∈Pn


and                                n                  o
                    limsup sup P rP µ̂H
                                      α,n <µY,n θ̂n ;P    ≤1−α(1−β)=(1−α)+βα
                    n→∞ P ∈Pn

from which the second result of the proposition follows immediately. 




                                                   78
                                                                                                            
Proof of Corollary 2              Note that by construction            H
                                                                     CSET,n   = µ̂Hα−β     ,µ̂H α−β              , where
                                                                                  2(1−β)
                                                                                         ,n 1− 2(1−β) ,n
                                          α−β
µ̂Hα−β       < µ̂H  α−β        provided   1−β   <1. Hence,
 2(1−β)
        ,n      1− 2(1−β) ,n

                                        n                      o
                                                       H
                                    P rP µY,n θ̂n;P ∈CSET,n |CnH θ̃,P

                                                                                                   
    =P rP µY,n θ̂n;P ≤ µ̂H
                         1−           α−β       |C H
                                              ,n n
                                                        θ̃,P −P rP µY,n θ̂n;P < µ̂Hα−β              |C H
                                                                                                  ,n n
                                                                                                            θ̃,P ,
                                     2(1−β)                                              2(1−β)


so Proposition 9 immediately implies (17).
    Equation (39) in the proof of Proposition 9 together with Lemma 6 implies that
                                     n                 o 1−α
                                                    H
                      liminf inf P rP µY,n θ̂n;P ∈CSET,n  ≥     (1−β)=1−α
                       n→∞ P ∈Pn                            1−β
so (18) holds. We could likewise get an upper bound on coverage using Lemma 6, but obtain a
sharper bound by proving the result directly. Specifically, note that
                                                             
                                               H                    β
                                µY,n θ̂n;Pn ∈CSET,n ⇒µY,n θ̂n;Pn ∈CSP,n .

Hence,
                                   n                 o
                                                    H
                              P rP µY,n θ̂n;P ∈CSET,n
               n                                  o n                   o
                              H |µ̂                β                       β
          =P rP µY,n θ̂n;P ∈CSET,n  Y,n θ̂n ;Pn ∈CSP,n P r µY,n θ̂n ;Pn ∈CSP,n .


By the first part of the proposition, this implies that
                     n                 o 1−α               n                 o
                                    H                                        β
      limsup sup P rP µY,n θ̂n;P ∈CSET,n  ≤    limsup sup P r µY,n θ̂n;Pn ∈CSP,n
        n→∞ P ∈Pn                           1−β n→∞ P ∈Pn

                                                             1−α
                                                         ≤       ,
                                                             1−β
so (19) holds as well. 

E        Additional Results for Neighborhoods Application
E.1      Empirical Bayes and Winner’s Curse
Chetty et al. (2018) focus on what they term forecast-unbiased estimates, which correspond to
posterior means from a correlated random effects model which treats tract quality as normally
and homoskedastically distributed conditional on a set of observable tract characteristics, with a
mean that changes linearly in the tract characteristics. Specifically, for Wt the characteristics of



                                                             79
tract t, and µt tract quality, these estimates correspond to posterior means under the prior π
that takes µt independent across tracts, with

                                       µt|Wt ∼N Wt0β,ω2 .
                                                       
                                                                                                   (40)

They then plug in estimates of ω and β.
    If we take the model (40) seriously and abstract from estimation of ω and β (for instance
because the number of tracts is large and we plug in consistent estimates), Bayesian posterior
means solve the winner’s curse problem under the prior. Specifically, note that the posterior
mean for µt given the vector of estimates µ̂ is simply the mean given µ̂t, Eπ [µt|µ̂] = Eπ [µt|µ̂t].
The law of iterated expectations implies, however, that Eπ [µt|µ̂] is unbiased for µt conditional
on µ̂, so for any event E such that P rπ {µ̂∈E}>0,

                                    Eπ [µt −Eπ [µt|µ̂t]|µ̂∈E]=0.

Likewise, since we model µ̂t as normally distributed conditional on µt, the posterior mean is also
the posterior median, so
                                                             1
                                  P rπ {Eπ [µt|µ̂t]>µt|µ̂∈E}= ,
                                                             2
and Eπ [µt|µ̂t] is median-unbiased under the prior conditional on the event E. Note, however,
that selection of a particular set of target tracts can be written as an event E, so this argument
implies that Bayesian posterior means are immune to the winner’s curse under the prior. This
depends crucially on the prior, however, since if we calculate the outer probability with respect
                                              6 π, we typically have
to some other distribution of effect sizes π̃ =

                                                               1
                                  P rπ̃ {Eπ [µt|µ̂t]>µt|µ̂∈E}6= .
                                                               2

E.2    Additional Figure for Movers Application
Figure 8 plots naive, conditional, and projection intervals for the Opportunity Atlas application
described in the main text.

F     Additional Simulation Results for Stylized Example
In the stylized example discussed in Section 2 of the main text, we focus on the median length
of confidence intervals and the median absolute error of estimators. In this section, we report
results for other quantiles, in particular that τ-th quantiles for τ ∈{0.05,0.25,0.5,0.75,0.95}.
    Figures 9 and 10 show the unconditional quantiles of the length of the 95% confidence
intervals CSU and CSET , for cases with |Θ|=2, 10, and 50 policies. In each case and for each



                                                 80
                    0                 1                          2
          Chicago
        New York
     Philadelphia
        Cleveland
         Baltimore
         St. Louis
           Newark
             Dallas
            Atlanta
            Detroit
   Washington DC
     New Orleans
        Milwaukee
        Columbus
      Kansas City
            Raleigh
             Austin
    Port St. Lucie
        Bridgeport
        Cincinnati
      Indianapolis
    San Francisco
         Charlotte
           Phoenix
            Boston
          Houston
            Buffalo
         Nashville
    Salt Lake City
        Pittsburgh
            Denver
      Minneapolis
    Grand Rapids
            Dayton
     Los Angeles
             Miami
         San Jose
           Orlando
       Fort Worth
        Las Vegas
            Tampa
      Sacramento
        San Diego
     San Antonio
     Jacksonville
            Seattle
            Fresno
       Providence                                                              Naive
          Portland                                                             Conditional
      Manchester                                                               Projection




Figure 8: Estimates and confidence intervals for average economic mobility for selected census
tracts based on Chetty et al. (2018) Opportunity Atlas, relative to the within-CZ average.


τ ∈ {0.05,0.25,0.5,0.75,0.95}, the τ-th quantile is monotonically decreasing in µ(θ1) − µ(θ−1).
Noting the different scales of the y-axes, we see that the upper quantiles grow as the number
of policies increase, particularly for small µ(θ1)−µ(θ−1).
    Figures 11 and 12 show the unconditional quantiles of the length of 95% hybrid confidence
intervals CSUH and CSET
                     H with β =0.005. Compared with Figures 9 and 10, the upper quantiles

are much smaller, especially for small µ(θ1)−µ(θ−1). This substantial reduction in length directly


                                               81
comes from the construction of the hybrid confidence intervals, which ensures that CSUH and
  H are contained in CS β . For the case of |Θ|=50, even the 95% quantiles of the length of
CSET                   P
CSUH and CSET
           H are shorter than the length of CS uniformly over the range of µ(θ )−µ(θ )
                                              P                               1     −1
values we consider.
    Figures 13, 14, and 15 examine the performance of point estimators for µ(θ̂). They plot the
unconditional quantiles of the absolute error of the conventional estimator, the median unbiased
estimator, and the hybrid estimator, respectively. In spite of the severe median bias shown in
Figure 1 in the main text, the distribution of the conventional estimator is relatively concentrated
compared to that of the median unbiased estimator. In particular, the upper quantiles of the
absolute errors of µ̂1/2 are very large for small µ(θ1)−µ(θ−1) (similar to the quantile plots of
the length of CSU and CSET shown in Figures 9 and 10).
    At the cost of a small median bias, the hybrid estimator substantially reduces the absolute
errors (Figure 15). The 95% quantile of the absolute errors of the hybrid estimator is overall
similar to the 95% quantile of the absolute errors of the conventional estimator with a notable
exception of the case of 2 policies. In contrast, for |Θ| = 10 and 50, and for quantiles other than
95%, the hybrid estimator outperforms the conventional estimator over a wide range of values
for µ(θ1)−µ(θ−1). These numerical results show that the hybrid estimator successfully reduces
bias without greatly inflating the variability of the estimator.




                                                82
         70                                   (a) 2 policies



                                                                95%−quantile   25%−quantile
         50




                                                                75%−quantile   5%−quantile
length




                                                                median
         30
         10




               0                 2                    4                  6              8

                                               µ(θ1) − µ(θ2)



                                             (b) 10 policies
         150
         100
length

         50
         0




               0                 2                    4                  6              8

                                               µ(θ1) − µ(θ−1)



                                             (c) 50 policies
         200
         150
length

         100
         50
         0




               0                 2                    4                  6              8

                                               µ(θ1) − µ(θ−1)



         Figure 9: Quantiles of the length of 95% conditionally UMAU confidences sets CSU .


                                                 83
         40                                 (a) 2 policies


                                                               95%−quantile       25%−quantile

                                                               75%−quantile       5%−quantile
         30
length




                                                               median
         20
         10




               0               2                     4                        6                  8

                                              µ(θ1) − µ(θ2)



                                            (b) 10 policies
         80
length

         60
         40
         20
         0




               0               2                     4                        6                  8

                                              µ(θ1) − µ(θ−1)



                                            (c) 50 policies
         150
         100
length

         50
         0




               0               2                     4                        6                  8

                                              µ(θ1) − µ(θ−1)



   Figure 10: Quantiles of the length of 95% conditionally equal-tailed confidences sets CSET .


                                                84
         8                                  (a) 2 policies


                                          95%−quantile        median           5%−quantile
         7




                                          75%−quantile        25%−quantile     Projection
         6
length




                                                                               Conventional
         5
         4
         3




             0                 2                    4                  6                8

                                              µ(θ1) − µ(θ2)



                                           (b) 10 policies
         8
         7
         6
length

         5
         4
         3




             0                 2                    4                  6                8

                                             µ(θ1) − µ(θ−1)



                                           (c) 50 policies
         8
         7
         6
length

         5
         4
         3




             0                 2                    4                  6                8

                                             µ(θ1) − µ(θ−1)



   Figure 11: Quantiles of the length of 95% hybrid confidence intervals CSUH , with β =0.005.


                                               85
         7                                (a) 2 policies

                                        95%−quantile        median          5%−quantile
         6




                                        75%−quantile        25%−quantile    Projection
length




                                                                            Conventional
         5
         4
         3




             0               2                    4                  6               8

                                            µ(θ1) − µ(θ2)



                                         (b) 10 policies
         7
         6
length

         5
         4
         3




             0               2                    4                  6               8

                                           µ(θ1) − µ(θ−1)



                                         (c) 50 policies
         7
         6
length

         5
         4
         3




             0               2                    4                  6               8

                                           µ(θ1) − µ(θ−1)


                                                                         H , with β =0.005.
 Figure 12: Quantiles of the length of 95% hybrid confidence intervals CSET


                                             86
                                            (a) 2 policies


                                                              95%−quantile       25%−quantile
                 4
absolute error




                                                              75%−quantile       5%−quantile
                 3




                                                              median
                 2
                 1
                 0




                     0        2                     4                        6                  8

                                             µ(θ1) − µ(θ2)



                                           (b) 10 policies
                 4
absolute error

                 3
                 2
                 1
                 0




                     0        2                     4                        6                  8

                                             µ(θ1) − µ(θ−1)



                                           (c) 50 policies
                 4
absolute error

                 3
                 2
                 1
                 0




                     0        2                     4                        6                  8

                                             µ(θ1) − µ(θ−1)



Figure 13: Quantiles of the absolute error of the conventional estimator (i.e. of |X(θ̂)−µ(θ̂)|).


                                               87
                                          (a) 2 policies
                 8



                                                            95%−quantile       25%−quantile
                 6
absolute error




                                                            75%−quantile       5%−quantile
                 4




                                                            median
                 2
                 0




                                  0   2           4                        6                  8

                                            µ(θ1) − µ(θ2)



                                          (b) 10 policies
                 20
                 15
absolute error

                 10
                 5
                 0




                                  0   2           4                        6                  8

                                           µ(θ1) − µ(θ−1)



                                          (c) 50 policies
                 10 15 20 25 30
absolute error

                 5
                 0




                                  0   2           4                        6                  8

                                           µ(θ1) − µ(θ−1)



Figure 14: Quantiles of the absolute error of the conditionally optimal median unbiased
estimator (i.e. of |µ̂1/2 −µ(θ̂)|).


                                             88
                                           (a) 2 policies


                                                              95%−quantile       25%−quantile
                 4
absolute error




                                                              75%−quantile       5%−quantile
                 3




                                                              median
                 2
                 1
                 0




                     0        2                     4                        6                  8

                                             µ(θ1) − µ(θ2)



                                           (b) 10 policies
                 4
absolute error

                 3
                 2
                 1
                 0




                     0        2                     4                        6                  8

                                             µ(θ1) − µ(θ−1)



                                           (c) 50 policies
                 4
absolute error

                 3
                 2
                 1
                 0




                     0        2                     4                        6                  8

                                             µ(θ1) − µ(θ−1)



Figure 15: Quantiles of the absolute error of the hybrid estimator (i.e. of |µ̂H
                                                                               1/2 −µ(θ̂)|) with
β =0.005.

                                               89
