                                NBER WORKING PAPER SERIES




                        THINKING AHEAD: THE DECISION PROBLEM

                                           Patrick Bolton
                                       Antoine Faure-Grimaud

                                        Working Paper 11867
                                http://www.nber.org/papers/w11867


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                    December 2005




We are grateful to Markus Brunnermeier, Luis Garicano, Oliver Hart, Margaret Meyer, John Moore,
Nobuhiro Kiyotaki, Jean Tirole and Wei Xiong for helpful comments as well as the participants at several
seminars for discussions and comments. The views expressed herein are those of the author(s) and do not
necessarily reflect the views of the National Bureau of Economic Research.

©2005 by Patrick Bolton and Antoine Faure-Grimaud. All rights reserved. Short sections of text, not to
exceed two paragraphs, may be quoted without explicit permission provided that full credit, including ©
notice, is given to the source.
Thinking Ahead: The Decision Problem
Patrick Bolton and Antoine Faure-Grimaud
NBER Working Paper No. 11867
December 2005
JEL No. D81, D84, C61

                                            ABSTRACT

We propose a model of bounded rationality based on time-costs of deliberating current and future
decisions. We model an individual decision maker’s thinking process as a thought-experiment that
takes time and let the decision maker “think ahead” about future decision problems in yet unrealized
states of nature. By formulating an intertemporal, state-contingent, planning problem, which may

involve costly deliberation in every state of nature, and by letting the decision-maker deliberate

ahead of the realization of a state, we attempt to capture the basic idea that individuals generally do

not think through a complete action-plan. Instead, individuals prioritize their thinking and leave

deliberations on less important decisions to the time or event when they arise.

Patrick Bolton
Columbia Business School
804 Uris Hall
Columbia University
New York NY 10027
and NBER
pb2208@columbia.edu

Antoine Faure-Grimaud
London School of Economics
a.faure_grimaud@lse.ac.uk
1     Introduction
This paper proposes a simple and tractable model of bounded rationality based on time-
costs of deliberating current and future decisions. We introduce a deliberation technology
based on the classical two-armed bandit problem (Gittins and Jones, 1974 and Rothschild,
1974) and model an individual decision maker’s thinking process as a thought-experiment
that takes time.
    The basic situation we model is that of a boundedly rational decision-maker who thinks
through a simple decision, such as which of two actions to take, by weighing in her mind
the costs and bene…ts associated with each possible action through thought-experiments
which take time. Eventually, following enough “thought-experimentation” the decision-
maker (DM) becomes su¢ ciently con…dent about which action is best and takes a decision.
    Although our model is built on the powerful multi-armed bandit framework, we depart
from the classical bandit problem in a fundamental way by introducing the notion of ‘thinking
ahead’about future decision problems in yet unrealized states of nature. By formulating an
intertemporal, state-contingent, planning problem, which may involve costly deliberation in
every state of nature, and by letting the decision-maker deliberate ahead of the realization of
a state, we attempt to capture the basic idea that individuals generally do not think through
a complete action-plan. Instead, individuals prioritize their thinking and …rst think through
the decisions that seem most important to them. They also generally leave deliberations on
less important decisions to the time or event when they arise.
    Such behavior is understandable if one has in mind deliberation costs but, as Rubinstein
(1998) has noted, it is irreconcilable with the textbook model of the rational DM with no
thinking costs:
    “ In situations in which the decision maker anticipates obtaining information before taking
an action, one can distinguish between two timings of decision making: 1. Ex ante decision
making. A decision is made before the information is revealed, and it is contingent on the
content of the information to be received. 2. Ex post decision making. The decision maker
waits until the information is received and then makes a decision. In standard decision
problems, with fully rational decision makers, this distinction does not make any di¤erence.”
[Rubinstein 1998, page 52]
                                               1
       There are at least three motivations for a model with costly deliberation such as ours.
First, there is the obvious reason that the behavior captured by such a model is more
descriptive of how individuals make decisions in reality1 . Second, as we shall explain, such
a model can provide new foundations for two popular behavioral hypotheses: “satis…cing”
behavior (Simon, 1955 and Radner, 1975) and decision-making under time pressure that
takes the form of “putting out …res” (Radner and Rothschild, 1975). The main motivation
of the current paper is to show how a model of decision-making with costly deliberation can
explain both “satis…cing”behavior and a prioritization of decision problems akin to “putting
out …res”. Third, a model with costly deliberation can also provide a tractable framework to
analyze long-term incomplete contracting between boundedly rational agents. We analyze
this contracting problem in our companion paper on “satis…cing contracts” (Bolton and
Faure-Grimaud, 2005).
       Indeed, our initial objective was mainly to formulate a tractable framework of contracting
between boundedly rational agents in response to Oliver Hart’s observation that: “In reality,
a great deal of contractual incompleteness is undoubtedly linked to the inability of parties not
only to contract very carefully about the future, but also to think very carefully about the utility
consequences of their actions. It would therefore be highly desirable to relax the assumption
that parties are unboundedly rational.” [Hart, 1995, p. 81] However, having formulated our
deliberation technology for boundedly rational agents we found that the decision problem is
of su¢ cient independent interest to be discussed in a separate paper.
       In our model the decision-maker starts with some prior estimate of the payo¤ associated
with each possible action choice in every state of nature. She can either take her prior as
her best guess of her …nal payo¤ and determine her optimal action-plan associated with that
prior, or she can ‘think’further and run an experiment on one action. This experiment will
allow her to update her estimate of the payo¤ associated with that action and possibly to
improve her action choice.
       At each point in time, DM, thus, faces the basic problem whether to explore further the
payo¤ associated with a particular action, search further other parts of her optimization
problem, or make a decision based on what she has learnt so far. Since ‘thinking ahead’
   1
    Recent research in behavioral economics suggests the distinction between costly conscious deliberation
and a¤ective impulses, and that actual decisions are the outcome of complex interactions between a¤ective
and deliberative processes (see e.g. Metcalfe and Mischel, 1999, Bernheim and Rangel, 2002, Benabou and
Pycia, 2002, and Loewenstein and O’Donoghue, 2004). A limitation of our model is that it entirely leaves
out a¤ective impulses. However, we think that this is not a critical limitation when our model is applied to
business decisions and sophisiticated long-term contracting situations.

                                                     2
takes time, DM will generally decide to leave some future decisions that she is only likely to
face in rare or distant states of nature to be thought through later.
   As is well understood, thinking ahead and resolving future decisions allows a DM to
make better current decisions only when current actions are partially or completely irre-
versible. Our decision problem, thus, involves irreversible actions, and thereby introduces a
bias towards deliberation on future actions ahead of the realization of future states of nature.
   To understand the underlying logic of our decision problem it is helpful to draw a parallel
with the problem of irreversible investment under uncertainty involving a ‘real option’(see
Henry, 1974 and Dixit and Pyndick, 1996). In this problem the rational DM may choose to
delay investment, even if it is known to generate an expected positive net present value, in
an e¤ort to maximize the value of the ‘real option’of avoiding making investments that ex
post turn out to have a negative net present value. If one interprets information acquisition
through delayed investment as a form of time-consuming deliberation on future actions, one
is able to infer from the ‘real options’literature that a boundedly rational DM with time-
costs of deliberation facing an irreversible action choice problem will behave like an investor
facing a real option problem.
   That is, the boundedly rational DM will postpone taking a current irreversible action
until she is su¢ ciently con…dent that this action yields the highest payo¤. The interesting
observation, however, from our perspective is not so much that the boundedly rational DM
will delay taking an action, but that she will eventually decide to act even if she has not
fully resolved her entire future action-plan.
   As helpful as the parallel with real options is in understanding the basic logic of our
problem, it is not a perfect analogy. Also, to emphasize the di¤erences with the real options
problem we specialize our general framework to a decision problem with no real option value
at all. That is, in our problem there is no doubt that a particular current action (investment)
is clearly preferable. Nevertheless, in this problem a boundedly rational DM will generally
choose to delay investment and think ahead about future decisions in some if not all future
states of nature.
   What is the reason for this delay, if no option value is present? The answer is that, by
thinking ahead about future decisions the boundedly rational DM can reduce the time-lag
between the realization of a state of nature and the date when DM takes a decision in that
state. By reducing this time-lag the boundedly rational DM is able to reduce the overall
expected lag between the time she makes a costly investment decision and the time when

                                                3
she recoups the returns from her investment. Because DM discounts future payo¤s, reducing
this time-lag raises her payo¤. In other words, our framework models the general idea that
the bene…t of ‘thinking ahead’is to be able to react more promptly to new events, but the cost
is delayed current decisions. This is the main novel mechanism we study in this paper.
   How does this framework provide a new foundation for satis…cing behavior? In general
it is optimal for the boundedly rational DM to engage in what we refer to as ‘step-by-step’
thinking. This involves singling out a subset of future decision problems and thinking these
through …rst. If the thought-experiments on these problems reveal that the payo¤ from
investing is appreciably higher than DM initially thought then DM will decide that she is
satis…ed with what she found and will choose to invest without engaging into further thinking
or optimization on other future decisions she has not yet thought about. If, on the other
hand, the thought-experiments reveal that the payo¤ from investing is no higher and possibly
lower than initially thought then DM will continue thinking about other decision problems
in yet unexplored future states of nature.
   In other words, the boundedly rational DM will generally refrain from fully determining
the optimal future action-plan and will settle on an incomplete plan which provides a satis-
factory expected payo¤. Note that, in our framework the satis…cing threshold is determined
endogenously, as the solution of an optimal stopping problem. Thus, our framework can
address a basic criticism that has been voiced against the original satis…cing hypothesis,
namely that the satis…cing threshold is imposed exogenously.
   In what way does the boundedly rational DM behave as if she were ‘putting out …res’?
We show that quite generally the boundedly rational DM prioritizes her thinking by …rst
choosing to think about the most important and urgent problems. It is in this sense that she
behaves as if she were putting out …res. The original formulation of this behavioral hypothesis
by Radner and Rothschild considered only very extreme situations, where DM had no choice
but to put out …res. Our framework highlights that the general idea underlying the notion of
putting out …res, that a boundedly rational DM prioritizes her thinking by focusing …rst on
the most important problems or those most likely to arise, extends far beyond the extreme
high stress situation considered by Radner and Rothschild.
   Several other major insights emerge from our analysis. First, when the number of future
decisions to think about is large so that the complexity of the overall planning problem is
overwhelming then it is best to act immediately simply by guessing which action is best,
and to postpone all thinking to the time when decision problems arise. This result is quite

                                              4
intuitive and provides one answer to the well known ‘how-to-decide-how-to-decide’paradox
one faces when one introduces deliberation costs into a rational decision-making problem (see
Lipman, 1995)2 . In the presence of deliberation costs DM faces a larger decision problem, as
she has to decide how to economize on deliberation costs. Presumably this larger decision
problem itself requires costly deliberation, which ought to be economized, etc. We suggest
here that one way of resolving this paradox is to have DM simply act on a best guess without
any deliberation when the problem becomes overwhelming.
      In contrast, when the number of future decision problems (or states of nature) is more
manageable then step-by-step thinking is generally optimal. For an even lower number of
future problems complete planning is optimal (in particular, when there is only one state
of nature, and therefore only one future problem to think about, then thinking ahead and
complete planning is always optimal).
      There is obviously a large and rapidly growing literature on bounded rationality and
some of the ideas we have touched on have been explored by others. The literature on
bounded rationality that is most closely related to ours is the one on decision procedures
and costly deliberation (see Payne, Bettman, and Johnson, 1993, and Conlisk, 1996) and
within this sub-literature our work is closest in spirit to Conlisk (1988). The main di¤erence
with Conlisk (1988) is that we formulate a di¤erent deliberation technology, with thinking
modeled as thought experimentation and thinking costs taking the form of time thinking
costs.
      Another closely related paper is Gabaix and Laibson (2002). In this paper, a boundedly
rational DM has to choose between a number of paths that provide di¤erent utility ‡ows at a
…nite sequence of steps. Paths di¤er in transition probabilities from one step to another. DM
can at some cost explore each step and evaluate the resulting utility ‡ow before choosing one
of these paths. Gabaix and Laibson assume that in doing so DM follows a simple heuristic
that ignores the option value of exploring one step or another. In their model DM chooses
which step to explore presuming that it would be the last time she will explore a step.
DM stops exploring when the information value of all the unexplored steps is less than the
exploration cost. Therefore there are two main di¤erences between their approach and ours:
1) we introduce an exploration cost as the only source of bounded rationality while they also
assume that DM follows a sub-optimal heuristic (in their model the actual choice may not
converge to the fully rational solution when the deliberation cost goes to zero); 2) unlike
  2
      A similar answer to this paradox is proposed by MacLeod (2004).

                                                    5
in our setup, Gabaix and Laibson do not consider the choice over thinking ahead or on the
spot, as their DM has to make only one decision. Thus, in their model DM cannot explore
a few steps …rst down a given path and possibly re-explore some other steps depending on
what she has learned.
    The literature on satis…cing behavior and aspiration levels is also closely related. Indeed
some research on satis…cing behavior that builds on Simon (1955) has also attempted to
tackle the question of the endogenous aspiration level but in a framework where costly
deliberation by DM is not explicitly modeled (see Gilbert and Mosteller, 1966, Bruss, 2000,
and Beckenkamp, 2004).
    The remainder of our paper is organized as follows. Section 2 presents a simpli…ed
version of our model and characterizes DM’s optimal thinking and action plan in this setting.
Section 3 derives a number of key comparative statics results for this simple model. Section
4 analyzes the general model with an arbitrary …nite number of states. Section 5 concludes
by summarizing our main …ndings and pointing to new directions of research. Finally two
appendices contain the proofs of our main results.




2         A Simple Model of “Bandit” Rationality
The general dynamic decision problem we have in mind is a possibly in…nite horizon (discrete
time) problem involving an initial decision on which of n actions to take, as well as future
decision problems that may be contingent on the initial action choice ai and on a realized
state of nature         j.   The initial action may be taken at any time t                0 and when an action is
chosen at time t a state of nature is realized at some later time. When some action ai has
been chosen DM receives an immediate payo¤ !(ai ). In addition, following the realization
of the state of nature           j   DM chooses another action aijk and obtains another payo¤ (ai ,
aijk ;    j ).   Future payo¤s are discounted and the discount factor is given by                 < 1. Thus, the
present discounted payo¤ when DM chooses action ai in period t and action aijk in period
         t is given by:
                                            t              t+
                                                !(ai ) +            (ai ; aijk ;   j ).


    Although DM knows the true payo¤ !(ai ) she does not know the true payo¤ (ai ; aijk ;                     j ).

She starts out with a prior belief over those payo¤s. Before taking any action, DM can learn
more about the true payo¤ associated with that or any other action by engaging in thought

                                                                6
experimentation. We model this thought experimentation in an exactly analogous way as in
the multi-armed bandit literature. That is, in any given period t DM can ‘think’about an
action aijk in state   j   and obtain a signal which is correlated with the true payo¤. Upon
obtaining this signal, DM revises her belief on the future payo¤ associated with the action
aijk .
    Thus, at t = 0 DM’ s decision problem is to decide whether to pick an action ai right
away or whether to think ahead about one of the future decision problems. DM faces this
same problem in subsequent periods, with possibly updated beliefs from earlier thought
experimentation, as long as she has not picked an action ai .
    When she has chosen an action ai some time elapses until a state of nature is realized.
Upon realization of a state       j,   DM’ s decision problem is again to decide whether to pick
an action aijk right away or whether to think about one of the actions she may take at that
point. Should DM decide to think about the payo¤ associated with an action then she faces
again this same decision problem in subsequent periods, with updated beliefs.
    This general framework is clearly restrictive in some respects: we only allow for two
rounds of action choice, the action sets are …nite, the state-space is …nite and learning through
thought-experimentation can only be done for one action at a time. Yet, the framework is
su¢ ciently general and versatile to be able to capture many dynamic decision problems
boundedly rational DMs are likely to face in reality.
    In this paper we specialize the framework described above to a problem of irreversible
investment under uncertainty. In addition, we shall only allow DM to choose between two
initial actions, which we label as invest and don’t invest. Also, in the remainder of this
section we will further specialize the model to the case where there are at most two states of
nature and only two actions in each state. We describe this simpler model in greater detail
below, while section 6 will consider the generalization to N                  2 states.

2.1      The model with two states of nature
In its simplest form our model has the following basic structure. It involves an initial
investment decision with a set-up cost I > 0. If DM chooses to invest at date t, then at date
t + 1 the project ends up in one of two states:               2 f 1;   2 g.   We denote by   the ex ante
probability that state     1   occurs. When state       i   is realized, investment returns are obtained
only after DM has chosen one of two possible actions: a risky or a safe action. The return of
the risky action, R; is unknown and may take two possible values, R 2 fR; Rg. The return

                                                    7
of the safe action is known and equal to S 2 (R; R), so that there is some prior uncertainty
about which is the e¢ cient action to take in state          i.   We denote by      i   the prior probability
that the risky action is e¢ cient in state        i.   To begin with we shall assume that           i   =   so
that both states have the same payo¤ structure, but that payo¤s are independently drawn
across the two states.
     As we have described above, DM may think about which decision to take in each state,
which in our simple model is equivalent to experimenting (in DM’s head) with the risky
action. We formulate the simplest possible thought-experimentation problem and assume
that when DM experiments with the risky action in any given period, there is a probability
     that she …nds out the true payo¤ associated with the risky action, and a probability
(1      ) that she learns nothing. In that case she must continue to think, or experiment, in
subsequent periods until she gets lucky if she wants to …nd out the true payo¤ of the risky
action3 .
     A critical departure from the standard multi-armed bandit setup is that DM can choose
to think through what should be done in some state                i   before or after the realization of the
state. She can think about what to do in one of these states or in both before acting. Of
course, if she acts …rst, invests and only starts thinking after the realization of a state of
nature, then there is no need to think through what to do in the state that is not realized.
     Without much loss of generality we assume that at any date t DM can either think or
act, but not do both at the same time. More precisely, we assume that each date t contains
two subperiods: an early subperiod when DM has the choice between acting or thinking.
If she thinks and her thinking is successful, she can in a later subperiod either pursue her
thinking or make a decision on what action to take. Otherwise, time moves on to date t + 1.
     We shall make the following assumptions on the payo¤ structure:
     Assumption: A1 : R + (1              )R > S
                     A2 : S      I>0
     Assumption A1 implies that the risky action is the best action for DM if she decides
not to do any thinking. This is not a critical assumption for our main qualitative results.
Assumption A2 , on the other hand, is more important. It ensures that the investment
project has a positive net present value (NPV) when the safe action is always chosen. Under
    3
      Our model is related to Conlisk’s approach to optimization costs. In Conlisk (1988) a decision maker
learns about an optimal decision x by drawing a sequence of random variables x   ~. This process is costly as
it takes time to collect more draws, but the longer the person thinks, the better is the estimate of x . Our
model can be viewed as a special case of Conlisk’s, where the decision space is binary and where the draws
are either fully informative or not informative at all.
                                                       8
this assumption, any deliberation on future decisions, or knowledge of the payo¤ of the
risky action, does not a¤ect the initial decision whether to invest: there is always a way of
generating a positive NPV, so that there is no ex-ante real option value to be maximized.
As we explained earlier this assumption is imposed to highlight the di¤erence of our simple
model with the classical real options model.


2.2    Solving the Decision Problem
DM can decide to invest right away or to think …rst about what decision to take in future
states of nature. If she were to invest right away then she would not be prepared to act
immediately upon the realization of a state of nature        i,   unless she decides to act only
on the basis of her incomplete prior knowledge. But, if she prefers to …rst think through
what to do in state   i   she would delay the time when she would reap the returns from her
investment. Thus, an important bene…t of identifying the e¢ cient decision in state      i   ahead
of time (before investing) is that DM will be able to act immediately following the realization
of a state of nature and thus reduce the time gap between when she makes an investment
outlay and when she reaps the returns from her investment. On the other hand, thinking
before investing may not be that valuable and may unnecessarily delay investment. This is
the basic deliberation tradeo¤ DM faces and that we now explore.
   A lower bound for DM’s payo¤ is the expected return obtained if DM acts immediately
and does not think at all (we refer to this as the no thinking strategy). Under assumption
A1 DM then always selects the risky action and obtains an expected payo¤:

                                 V; =     I+     R + (1     )R :

   Consider now the payo¤ DM could obtain by engaging in some thought experimentation.
A …rst thinking strategy for DM is to invest right away at date 0, …nd out which state of
nature prevails at date 1, and see whether some thinking is worthwhile once the state is
realized.
   Note that under our learning technology (where following each thought experiment, DM
either learns the true payo¤ for sure or learns nothing) if it is optimal to undertake one
experiment, then it is optimal to continue experimenting until the true payo¤ is found.
Indeed, suppose that when state      i   is realized DM prefers to experiment. Then, since the
decision problem at that point is stationary, she will also prefer to continue experimenting
should she gain no additional knowledge from previous rounds of experimentation. Our
                                           9
experimentation problem has been deliberately set up to obtain this particularly simple
optimal stopping solution.
   If, following successful experimentation DM learns that the true payo¤ of the risky action
is R then she will optimally choose the safe action given that R < S. If, on the other hand,
she learns that the true payo¤ is R then she chooses the risky action. Thus, the expected
present discounted payo¤ from thinking in state                    i    is given by

                            R + (1         )S            + (1       )         R + (1             )S
                        |      {z               }          |                  {z                      }

                  present discounted payo¤                   present discounted payo¤
                    from learning the true                     from learning the true
                   payo¤ in the …rst round                   payo¤ in the second round
                      of experimentation
                                                                  X
                                                                  1
                               2 2                                                   t
                  +(1       )           R + (1           )S +           (1      )t         R + (1         )S :
                                                                  t=3


   Or, letting ^ =      1 (1    )
                                     ;the expected present discounted payo¤ from thinking in state                i

can be written as:
                                               ^        R + (1         )S .

   Therefore once DM learns that the true state is                       i   she will prefer to think before acting
if and only if:

                                 R + (1         )R           ^     R + (1            )S
                                                         ,
                                                    ^        ^L          R + (1           )R
                                                                                             .
                                                                         R + (1           )S
   This condition essentially imposes a lower bound on DM’s thinking ability for her to be
willing to engage in some thought experimentation. If DM were a very slow thinker ( is
very small) then it obviously makes no sense to waste a huge amount of time thinking. Thus,
for su¢ ciently high values of , DM will choose to think on the spot if she has not done any
thinking prior to the realization of the state of nature. In that case, when DM chooses to
…rst invests and then think on the spot she gets an ex ante payo¤ VL equal to:

                                        VL =        I+ ^         R + (1        )S

Notice that under this strategy DM has to solve only one decision problem: the one she faces
once the state of nature is realized.
                                                             10
   To compare the payo¤s of the two deliberation strategies considered so far it is convenient
to introduce the following notation: Let

                                                    R + (1                  )S

and
                                                    R + (1                 )R:

   It is then immediate to see that the strategy of no thinking dominates the strategy of
thinking on the spot if and only if:

                                         VL     V; , ^            ^L =             :

   Consider next a third strategy available to DM, which is to think ahead about one or both
                                                                   1
states of nature. Suppose to begin with that                  =    2
                                                                           and that DM, being indi¤erent between
which state to think about …rst, starts with state                    1.   Again, if it is optimal to begin thinking
about state         1   and DM does not gain any new knowledge from the …rst thought-experiment
then it is optimal to continue thinking until the true payo¤ of the risky action in state                                1   is
found.
   Suppose that DM has learned the true payo¤s in state                                1,   under what conditions should
she continue thinking about the other state before investing instead of investing right away
and gambling on the realization of state            1?    If she decides to think about state                  2   she will
again continue to think until she has found the true payo¤s in that state. If she learns that
the return on the risky action in state         2   is R, her continuation payo¤ is

                                                                  1         R
                                         Vr =       I+                 +      ;
                                                               2            2

where        1   2 fS; Rg is DM’s payo¤ in state         1.   Similarly, if she …nds that the best action in
state    2   is the safe action, her continuation payo¤ is:

                                                                  1         S
                                         Vs =       I+                 +      :
                                                               2            2
Therefore, DM’ s expected continuation payo¤ from thinking ahead about state                                       2,   given
that she has already thought through her decision in state                             1    is:

                                                                      1
                                      VE1 = ^       I+                     +            :
                                                                  2            2

                                                         11
     If instead of thinking ahead about state                         2   DM immediately invests once she learns the
true payo¤ in state          1   her continuation payo¤ is:
                                                                                                !
                                                                     1   maxf ; ^ g
                                           VL1 =     I+                +                             :
                                                                    2        2

Thus, continuing to think about state                       2   before investing, rather than investing right away
is optimal if:
                                                                                   h                               i
                                                     ^ )(       1                      ^
                 1
                       VE1           VL1   =    (1                    I) +                      maxf ; ^ g             0.
                                                                2              2
     From this equation it is easy to characterize the solution of DM’s continuation decision
problem once she knows her true payo¤ in state                                1.   We state it in the following lemma:

     Lemma 1: Suppose that DM is thinking ahead and has already solved her decision prob-
lem in state     1,   then it is better to invest right away and possibly think on the spot in state
 2   rather than continue thinking ahead about state        I 0:                   2   if   1
                                                                                            2
                                                              1
If, on the other hand, 21 I < 0; then there exists a cut-o¤ ^ E such that thinking ahead
                                              1
about state 2 is preferred if and only if ^ ^ E :

     Proof. see the appendix.
     As one might expect, the decision to continue thinking ahead about state                                               2   depends on
what DM has learned before. The higher is                                 1   the less keen DM is to continue thinking.
When DM …nds a good outcome in state                            1   she wants to reap the rewards from her discovery
by accelerating investment. By thinking further about state                                      2   she delays investment and if
she ends up in state             1   anyway her thinking will be wasted. The opportunity cost of these
delays is captured by                 2
                                       1
                                               I ; the expected payo¤ in state                           1.   Note, in particular, that
a thinking strategy such that DM stops thinking on a bad outcome where she learns that
 1   = S, but continues thinking on a good outcome, where                                   1   = R, is necessarily sub-optimal.
This simple observation, we believe, highlights a basic mechanism behind satis…cing behavior.
Why do boundedly rational DMs settle with good but not necessarily optimal outcomes?
Because they want to bring forward the time when they get the good reward, or as the saying
goes, because the best is the enemy of the good.
     Having characterized this key intermediate step we are now in a position to determine
when DM should start to think ahead at all and when DM should defer until later all of her
thinking. Depending on the value of                    1    several cases have to be considered.

                                                                     12
                                 S
   First, suppose that I         2
                                   :   In that case, DM will not think about state         2   ahead of time,
irrespective of the outcome of her optimization in state                  1.   DM will then think about at
most one state. If she thinks ahead about one state her payo¤ is:

    VE = ^           I + R + maxf ; ^ g + (1                         )   I + S + maxf ; ^ g              =
                        2   2                                               2   2


                                       ^    I+          +       maxf ; ^ g
                                                  2         2
If she decides to invest without thinking her payo¤ is:

                                           VL =       I + maxf ; ^ g

   Comparing the two payo¤s VE and VL we immediately observe that:

                                 S
   Lemma 2: When I              thinking ahead is always dominated and DM chooses:
                                 2
                                   ;
   - no thinking if and only if ^ ^ L ; or
   - thinking on the spot if and only if ^ ^ L .

   Proof. see the appendix.
   Lemma 2 establishes another important observation. If DM knows that she will stop
thinking ahead irrespective of the outcome of her current thinking, then it is not worth
thinking ahead about her current problem. In other words, it is only worth thinking ahead
about some state       i   if DM may want to continue thinking ahead about other states with
positive probability; in particular, when the outcome of her current thinking is bad. In our
simple model it is quite intuitive that DM would not want to do any thinking ahead if she
                                                                          S
can guarantee a high net return, which is the case when                   2
                                                                               I.

   Second, suppose that I              R
                                       2
                                           and that ^           ^ L . In that case, DM wants to think about
state   2   ahead of time, irrespective of the outcome of her optimization in state               1.   In other
words, if DM does any thinking ahead, she will want to work out a complete plan of action
before investing. Thus, if she thinks ahead her payo¤ is:
                                                    2
                                              VE = ^ [              I]

while if she does not, she can expect to get:

                                              VL =        I+ ^
                                                         13
It is easy to check that in this case, VL < VE .

      Third, suppose that I          R
                                     2
                                          but ^            ^ L . In this case DM either thinks ahead and works
out a complete plan of action before investing, or DM prefers not to do any thinking ever
(thinking on the spot is always dominated by no thinking, since ^      ^ L ). If she does not
                                                                    2
think at all she gets V; and if she thinks ahead her payo¤ is VE = ^ [        I]. Comparing
V; and VE , we …nd:

      Lemma 3: When ^            ^ L thinking on the spot is always dominated and when I                                  R
                                                                                                                          2
DM chooses:
                                         q
      - no thinking if ^      ^E =            I
                                               I
                                                 ;   or
      - thinking ahead (complete planning) if ^                    ^E :

      Proof. see the appendix.
      In this situation DM is a slow thinker but the opportunity cost of thinking ahead is also
low as investment costs are high. Thinking on the spot is dominated because once investment
costs are sunk DM wants to reap the returns from investment as quickly as possible. On the
other hand, as long as investment costs have not been incurred, DM is less concerned about
getting a low net expected return quickly.

                                R                    S
      Fourth, suppose that      2
                                     > I >           2
                                                       .    In this intermediate case, DM wants to continue
thinking ahead about state           2   only if the outcome of her thinking on state                 1    is bad. Thus,
if DM thinks ahead, then with probability                      she learns that the risky action has a payo¤ R
in state   1   and stops thinking further about state                 2.      Later, of course, if state    2   is realized,
DM may decide to think on the spot about what to do. With probability 1                                     instead, she
learns that the return of the risky decision in state                     1   is S and decides to continue thinking
about state      2   before investing. Therefore, in this situation, her payo¤ if she thinks ahead
is:
            VE = ^      I + R + maxf ; ^ g + (1             )^     I+ S+
                             2      2                                  2     2
      Having determined all the relevant payo¤s we are in a position to derive the conditions
under which DM prefers to think ahead. To limit the number of cases, and consistent with
our focus on the choice between thinking ahead or deferring thinking, we narrow down our
analysis to values of ^ greater than ^ L , so that no thinking is always dominated. We provide
an analysis of the alternative situation in the Appendix.

      Assumption:           A3 : ^
                                                              14
    Proposition 1: Under assumptions A1 ; A2 ;and A3 the solution to DM’s decision problem
is as follows: DM prefers to think on the spot if and only if:
                                            h          i
                                         ^          R
                                                     2
                                     I                   :
                                           1+^       ^
Otherwise DM prefers to think ahead, either adopting:
- a step-by-step strategy where she …rst thinks about state                   1    and continues her thinking
about state     2      if she …nds that the payo¤ in state       1   is S and if
                                             h         i
                                           ^         R
                                                       2               R
                                                             I           :
                                           1+^         ^               2
If she uncovers that the payo¤ on state            1   is R, she stops thinking ahead beyond state          1

and if state       2   is realized resumes her thinking then.
- a complete planning strategy where she thinks ahead about both states before investing if
R
2
      I.
    Proof. see the appendix.
    The following …gure is helpful to summarize Proposition 1.
        I
               6
               -

                                            Complete Planning

           R
           2   -
                                      Step by Step Planning




                                           Thinking on the Spot
           S
           2
               -



                                                                                      |        -
               ^L                                                                     1
                                     Figure 1
    The …gure maps out the three di¤erent regions: i) complete planning, where DM thinks
everything through ahead of time; ii) thinking on the spot, where DM defers all the thinking
                                             15
to when the state of nature is realized, iii) step-by-step planning, where DM thinks …rst about
state    1   before investing and maybe about state     2   as well. Recall that in this region, we
could see DM thinking both before investing and after: for example, she thinks about what
to do in state      1,   learns some good news about that state and invests, but unfortunately
state    2   is realized. She is then led to think again about what to do in that state before
making a decision.



3       Comparative Statics in the simple model
Having characterized decision-making in the presence of positive deliberation costs in a
simple symmetric example, we now explore how DM’s behavior changes with the underlying
parameters of our problem, while continuing to retain our simple two-state structure.

3.1      Quick thinkers tend to ‘think on their feet’
In our model quick thinkers have a higher . We show in this subsection that a DM with a
higher       is more likely to choose to think on the spot. To see this, consider a set of decision
problems di¤ering only in the size of I, with I distributed on the interval (0; I] according
                                                                    R
to some cumulative distribution function F (:), with                2
                                                                        < I < S. Then the following
proposition holds:
    Proposition 2: The probability mass
                                                   h        i
                                               ^        R
                                                        2
                                          F(                    )
                                               1+^      ^
is strictly increasing in . In other words, the set of problems for which DM is thinking on
the spot is increasing in .
    Proof. Immediate corollary of Proposition 1.
    With high investment costs DM wants to plan ahead, whether she is a slow or fast thinker
(as long as      ^ L ). As we have already explained, the reason is that this allows her to
better align investment costs and monetary returns in time. When I is high the NPV of
the investment is smaller and the opportunity cost of thinking ahead is low. In addition,
the bene…t of aligning cost and bene…ts goes up. Therefore both slow and fast thinkers then
prefer to plan ahead.
    In contrast, for intermediate values of investment costs, fast thinkers do more thinking
on the spot than slow thinkers, who engage in step-by-step thinking. The reason is that, for
                                            16
fast thinkers the time gap between investment and the realization of returns is smaller and
therefore matters less than for slow thinkers. As a result, they are more likely to prefer to
bring forward the positive NPV by investing right away. Slow thinkers, on the other hand,
are prepared to accept some delay in getting the positive NPV, in return for a higher NPV
achieved by reducing the time lag between investment and recoupment.
     Interestingly, as Figure 1 reveals, DM does not necessarily engage in complete planning
as     approaches 1. As long as               < 1, what determines whether DM engages in complete
                                                                   R
planning is whether I is greater or smaller than                   2
                                                                     . Thus even for arbitrarily close to 1,
                                                                 R
DM will not engage in complete planning if I <                   2
                                                                   . It is only when there are no deliberation
costs at all, so that       = 1, that complete planning is always a rational behavior.

3.2     Think …rst about the most likely state
In the symmetric example we have studied DM faces maximum uncertainty about which
state will occur, as each state is equally likely. We now explore how DM’s decisions change
when ; the probability that state                  1   is realized is larger than 12 , while keeping the other
characteristics of the model unchanged.
     It is immediate to check that the payo¤ of the thinking on the spot strategy is unchanged
when     > 12 . Similarly, the payo¤ associated with the complete planning strategy remains
the same. On the other hand, the payo¤ associated with step-by-step thinking is a¤ected in
an important way when state              1   is more likely to arise.
                 1
     When    >   2
                     it is straightforward to see that the best step-by-step thinking strategy for
DM is to think …rst about state              1,   the most likely state. The costs of thinking ahead about
one state only are the same whichever state DM thinks about, but the expected bene…t in
terms of DM’s ability to act quickly following the realization of a state is higher for state               1,

as this is the most likely state. Note also that DM is more likely to think ahead about one
state only when        > 12 , as the marginal expected payo¤ of thinking ahead about the other
state is reduced.
     The continuation payo¤ obtained by thinking ahead about state                    2,   once DM knows that
the payo¤ in state      1   is   1   is given by:


                                     VE1 ( ) = ^ ( I + (          1   + (1   ) ))

Compare this to the continuation payo¤ obtained by stopping to think ahead at that point


                                                            17
and investing:
                                   VL1 =      I+       1   + (1           )^       ;
and observe that the di¤erence in continuation payo¤s is given by:
                                  1
                                      = VL1    VE1 ( ) = (1        ^ )(        1   I):
                                                                                                   1
      Thus, an increase in , the likelihood of state          1,   results in an increase in           . As a result,
there are now more values of I for which DM stops to think ahead once she knows the payo¤
in state    1.   This has two implications:
      First, DM is less likely to work out a complete action-plan before investing, so that the
complete planning region in Figure 1 is now smaller.
      Second, as the payo¤ of step-by-step thinking increases with , this strategy becomes
more attractive relative to not thinking ahead at all.
      Consequently we have:

      Proposition 3: A reduction in the uncertainty about states of nature reduces the attrac-
tiveness of complete planning and thinking on the spot and favors a step-by-step approach.
      Proof. Obvious.
      As    approaches 1, it is clear that thinking ahead about only state               1   is the best strategy:
it allows DM to reduce the time between when she incurs investment costs and when she
recoups her investment. In addition, thinking ahead is very unlikely to create unnecessary
delays.
                                        1
      Vice-versa, as     approaches     2
                                            and uncertainty about the state of nature increases, DM
can respond by either working out a complete plan to deal with the greater uncertainty, or
she can adopt a wait-and-see approach and defer all her thinking until after the uncertainty
is resolved.
      One may wonder whether this …nding extends to changes in other forms of uncertainty.
For instance, would DM change her behavior following a change in her prior belief                          i   in state
 i   that the risky action is e¢ cient? It turns out that such a change in beliefs not only a¤ects
the perceived riskiness of the risky decision, but also changes the average payo¤ that DM
can expect in state      i.   We will show below that this change has several implications.


3.3        Solve easier problems …rst
So far we have interpreted the parameter              as a measure of DM’s thinking ability. But
can also be seen as a measure of the di¢ culty of the problem to be solved, with a higher
                                           18
  denoting an easier problem. If we take that interpretation, we can let                     vary with the
problem at hand and ask whether DM thinks …rst about harder or easier problems. Thus,
suppose that the decision problem in state               1     is easier than the one in state   2.   That is,
suppose that      1   >   2.   Holding everything else constant, we now explore how variations in
the complexity of decision problems a¤ects DM’s payo¤ and behavior. It is immediate that
the payo¤ associated with the complete planning strategy is:

                                              VE = ^ 1 ^ 2 [         I]

and the payo¤ of thinking on the spot is:

                                           VL =     I + (^2 + ^1)         .
                                                                      2
   Comparing these two payo¤s we can illustrate a …rst e¤ect of variation in problem com-
plexity. Take ^ 1 = ^ + " and ^ 2 = ^ ", where " > 04 . Note, …rst, that the payo¤ under
the thinking on the spot strategy is the same whether " = 0, as we have assumed before, or
                                                                2
" > 0. On the other hand, the payo¤ under complete planning, ( ^ "2 ) [     I], is lower the
higher is ". Thus, increasing the variance in problem-complexity across states, while keep-
ing average complexity constant, does not a¤ect the payo¤ under the thinking-on-the-spot
strategy as DM then only incurs average thinking costs. In contrast, when DM attempts
to work out a complete action-plan, thinking costs compound. This is why DM is worse o¤
under a complete planning strategy when the variance in problem-complexity increases.
       How do di¤erences in problem-complexity across states a¤ect DM’s step-by-step thinking
strategy? Intuitively, it seems to make sense to have a crack at the easier problem …rst. This
intuition is, indeed, borne out in the formal analysis, but the reason why it makes sense
to start …rst with the easier problem is somewhat subtle. Under the step-by-step thinking
strategy, whenever DM ends up thinking about both states of nature, she compounds think-
ing costs in the same way as under complete planning. Under this scenario, whether she
starts with state     1   or   2   is irrelevant.
       Hence, the choice of which state to think about …rst only matters in the event when:
i) the outcome of thinking is good news (that is, if DM learns that the true payo¤ is R),
so that DM engages in only partial planning before investment, and; ii) when the state of
nature which DM has thought about is realized. Under this latter scenario DM is better
   4
    We assume that " is small enough , so that even in state 2 DM will continue to think before acting if
she does not know which decision is e¢ cient. More precisely, we take " to be small enough that assumption
A3 remains valid.
                                                        19
o¤ thinking …rst about the easier problem since she then gets to realize the net return from
investment sooner. Formally, the payo¤s under the two alternative step-by-step strategies
are given by:


             VE1 = ^ 1       I + R + ^2             + (1   )^2       I+ S+
                                2   2                                  2   2


             VE2 = ^ 2       I + R + ^1             + (1   )^1       I+ S+
                                2   2                                  2   2
   Therefore, as    I + 2 R > 0, it is best to think …rst about the simple problem with the
higher ^ i . We summarize our discussion in the proposition below:

   Proposition 4: Let ^ 1 = ^ + " and ^ 2 = ^          ", with " > 0 but small.
   Then, when DM chooses to think ahead, it is (weakly) optimal to think …rst about the
easier problem, and the payo¤ associated with:
- the complete planning strategy is decreasing in the di¤erence in problem-complexity across
states;
- the thinking on the spot strategy is unchanged,
- the step-by-step strategy, where DM thinks …rst about the simple problem (in state        1 ),

rises for " small enough.
   Proof. See the appendix.
   Thus, an important insight emerging from our analysis is that partial planning is more
likely to take place in environments where problem-complexity varies across states. Moreover,
the simpler problems are solved …rst, while the harder ones are deferred to later in the hope
that they won’t have to be solved. These conclusions are quite intuitive and reassuring.

3.4       Think …rst about the highest payo¤ state
If thinking …rst about more likely and easier problems makes sense, is it also desirable to
think …rst about problems with a higher expected payo¤? In our simple model expected
returns may di¤er across states if either the probability of high returns on the risky action
is di¤erent, or if returns themselves di¤er. We explore each variation in turn and …nd:
   Proposition 5: Suppose that either       1   =     + "; and   2   =    " or that S1 = S + ",
S2 = S     "; while R1 = R + " and R2 = R       ", for " > 0 but small.

      Whenever some thinking ahead takes place, it is best to start thinking about the high
      payo¤ state (   1 ).
                                                20
              The payo¤ associated with the complete planning or the thinking on the spot strategies
              is una¤ected by changes in ", however

              The payo¤ associated with the step-by-step strategy, where DM thinks …rst about the
              high return state, is increasing in ".

         Proof. See the appendix.

         This is again a reassuring and quite intuitive result. As DM seeks to get to the point
where she will make a decision as quickly as possible it makes sense for her to …rst bring
forward in time the highest expected payo¤s, which are most likely to tip her decision whether
to invest or continue thinking ahead.

3.5            Think about the most urgent problems …rst
We have argued that Radner and Rothschild’s (1975) notion of putting out …res can be
understood more generally as the idea that a boundedly rational DM prioritizes her thinking
by focusing …rst on the most important problems or those most likely to arise. In this
subsection we want to highlight a third aspect of this general idea: thinking …rst about the
most urgent problems.
         We model a problem as more or less urgent by varying the time lag between when DM
decides to invest and when DM can make the decision on which action to take in a given
state of nature. That is, we still assume that the state of nature is realized one period after
DM invests, but now we only allow DM to act in either state                            i   at the earliest after a time-lag
of        i        0 after the realization of the state. Speci…cally, we consider an example here where
     1   = 0 and         2   > 0, so that the problem in state         1   is in some sense more “urgent”than the
problem in state               2.

         Given this timing structure, we can show that it is optimal for DM to think ahead about
the decision problem in state                 2   only if it is also optimal to think ahead about the problem
in state            1:

         Proposition 6: Suppose that DM can act following the realization of state                               i   at the
earliest after a time-lag of              i       0. Then if   1   <       2   it is never optimal to think ahead about
state          2   before state     1.

         Proof. See the appendix.
         Intuitively, there are two e¤ects which favor thinking about the most urgent problem
…rst: 1) should DM end up not facing this problem after all, she will then have time on
                                         21
her hands to think about the less urgent problem; 2) as more urgent problems tend to arise
sooner they have a higher present discounted payo¤. As we saw above, it is always optimal
for DM to …rst think about the highest payo¤ states.
     Consider, …nally, the e¤ect of discounting on DM’s planning strategy. As can be seen
immediately from Figure 1, as DM discounts the future more (that is, when          is lower) she
will, somewhat counter-intuitively, be led to do more planning other things equal. Indeed,
she may either switch from a step-by-step policy to complete planning, or from thinking on
the spot to a step-by-step planning policy. The only caveat is that at some point, when       is
su¢ ciently low, it does not pay to do any thinking on the spot or even any thinking ahead.

3.6     Exploiting Statistical Regularities
Suppose that DM has prior knowledge that payo¤s of the risky action may be correlated
across states of nature. Speci…cally, suppose that with prior probability 1=2 the payo¤ on
the risky action is R in any given state    i   . In addition, suppose that the probability that
Ri = R conditional on Rj = R is given by " > 1=2, so that payo¤s on the risky action
are positively correlated. How does this prior statistical regularity a¤ect DM’ s thinking
strategy?
     Note …rst that the expected ex-ante payo¤s of thinking on the spot or complete planning
are independent of the value of ". Similarly the continuation payo¤s remain the same as
long as: i) following the discovery that Ri = R we have

                              "R + (1     ")R < ^ ["R + (1        ")S];

and,
     ii) following the discovery that Ri =R we have

                                     S < ^ ["S + (1      ")R]:

     Indeed, when these conditions hold DM’s interim problem is still whether to do more
thinking ahead or to postpone thinking until after the realization of the state, and the costs
and bene…ts involved in this decision are una¤ected by ".
     On the other hand, for high degrees of correlation the opposite inequalities hold:

                              "R + (1     ")R     ^ ["R + (1      ")S];

or
                                                   ^S    R
                                 "   "1                               ;
                                           R      R     ^ (R     S)
                                                 22
and
                                    S    ^ ["S + (1    ")R];

or
                                                 ^R    S
                                     "    "2                :
                                                ^ (R   S)
     In this case DM would not want to think on the spot about Rj following the discovery of
Ri . Therefore, DM would also not want to do any thinking ahead about state        j   . Thus, for
high degrees of correlation in payo¤s DM would never want to pursue a complete planning
strategy. As for the choice between thinking on the spot and thinking ahead about one state,
it is determined as follows:
     Proposition 7: For "      maxf"1 ; "2 ; "3 g thinking ahead about one state only is optimal,
and for "3 > "     maxf"1 ; "2 g thinking on the spot is optimal, where

                                               4I(1    ^)
                                     "3 = 1                 :
                                               ^ (R    R)
     Proof. Follows immediately by comparing the expected payo¤s of the two strategies.
     Thus, quite generally, whenever DM is aware of existing statistical regularities she will
engage in more step-by-step thinking.
     Interestingly, therefore, with correlated payo¤s DM is more easily satis…ed with partial
optimization. In other words, there is more satis…cing when payo¤s are correlated. Note
that the mechanism here is through future satis…cing behavior feeding back into current
satis…cing. However, the ‡ip side of this behavior is that DM will end up making more
mistakes. Thus, we have the paradoxical situation where although DM engages in more
planning she ends up making more mistakes.
     Notice also that DM is more likely here to be satis…ed with step-by-step thinking the
lower is . In other words, slower thinkers are more likely to cut corners and therefore to
make mistakes.
     In sum, greater correlation in payo¤s brings about a quicker response time by DM, but
this comes at the expense of more decision errors. DM’s behavior in this environment, thus,
gives the appearance of overcon…dence.


4      The Model with N states
One may wonder to what extent our insights into optimal dynamic decision and deliberation
strategies derived in our tractable two state example extend to more general settings. We
                                             23
attempt a limited exploration into this question in this section by partially analyzing the
more general decision problem with a …nite, arbitrary number, of states of nature N      2.
   We begin by looking at the case of N equiprobable states, each with the same structure:
a safe action with known payo¤ S, and a risky action with unknown payo¤ R 2 fR; Rg.
As before, we assume that payo¤s of the risky action are identically and independently
distributed across states and take DM’s prior belief to be Pr(R = R) = . We also continue
to assume that assumption A3 holds.
   As there is a …nite number of states to explore, the optimal deliberation policy cannot
be stationary. As and when DM discovers the solution to future decision problems in some
states of nature by thinking ahead, she is more likely to stop thinking further, given that
the remaining number of unexplored states diminishes. We also know from our previous
analysis that she may be more or less willing to continue her thought experimentation before
investing, depending on whether she learns ‘good’or ‘bad’news about her payo¤ in states she
is thinking about. Therefore there is no hope in identifying a simple optimal stopping policy
where, for instance, DM explores m out of N states and then stops. It is also unlikely that
DM’s optimal policy would take the simple form where DM would stop thinking further once
she discovers that she can obtain a minimum average return in the states she has successfully
explored. To identify DM’s optimal policy we therefore proceed in steps and characterize
basic properties the optimal policy must satisfy.
   One property we have identi…ed in our two state example is that if DM is sure to stop
thinking and to invest after learning her payo¤ about the current state she thinks about,
then she prefers not to think ahead about the current state. The next lemma establishes
that this property holds in the more general model:

   Lemma 4: If DM prefers to stop thinking ahead and to invest, irrespective of what she
learns about state   i,   then she also prefers not to think ahead about state   i.

   Proof. Suppose that DM has already deliberated about m out of N states, and found
that zm of these states have a payo¤ for the risky action of R. If she deliberates about the
(m + 1)th state and knows that she will then invest no matter what, she can expect to get:

                b     I+         zm R + (m    zm ) S +   + (N    (m + 1)) b
                             N

   Thus, suppose by contradiction that DM decides to deliberate on this (m + 1)th state.
This is in her best interest if the payo¤ above is larger than what she could get by investing

                                                 24
right away :
                                 I+       zm R + (m      zm ) S + (N        m) b
                                      N
Therefore, it must be the case that

                    C1 :     I+        zm R + (m      zm ) S + (N         (m + 1)) b       <0
                                  N
Now, if DM were sure to stop deliberating after the (m + 1)th state, she must be better o¤
stopping further deliberations even when she learns bad news about the m + 1th state. For
that to be true, it must be the case that stopping even following bad news is better than
continuing exploring just one more state (m + 2), before investing, or that:

                        I+        zm R + (m     zm ) S + S + (N           (m + 1)) b
                             N

                b       I+        zm R + (m     zm ) S + S +        + (N       (m + 2)) b
                             N
or:
                     I+           zm R + (m     zm ) S + S + (N           (m + 2)) b        0
                             N
or
              I+           zm R + (m      zm ) S + (N        (m + 1)) b                b    S   0
                    N                                                           N
which implies that condition C1 is violated, , as b                 S > 0.

      One implication of this simple lemma is that if DM wants to do some thinking ahead,
then with positive probability she may also want to work out a complete plan of action (e.g.
in the event that she only learns bad news from her deliberations).
      Recall that the reason why it may be in DM’s interest to do some thinking ahead is not
to improve her current decision. In this respect, thinking ahead adds no value, as investment
is guaranteed to yield a positive net present value. Rather, the reason why DM gains by
thinking ahead is that she may be able to respond faster to the realization of a new state of
nature. So the intuition behind lemma 4 is that if the gain from a quick response in state
(m + 1) does not justify the cost of delaying investment, and this even after learning that
the expected net present value of the investment will be lower, then it cannot possibly be
the case that this gain exceeds deliberation costs prior to learning the bad news.
      This intuition immediately suggests that the following property of the optimal delibera-
tion policy must be satis…ed.

                                                        25
      Lemma 5: It is never optimal to stop thinking ahead on learning bad news and to
continue thinking ahead on learning good news.
      Proof. see Appendix B.

      From these two simple observations we are able to infer that:

      Theorem: For N small enough, DM adopts a step-by-step strategy whereby she thinks
ahead about some states, continues to do so upon learning bad news and invests only once
she has accumulated enough good news.
      Proof. The two previous lemmata imply that DM never stops exploring upon receiving
bad news. Indeed, if she did then she would not continue either when receiving good news.
This in turn would imply that it would be best not to think ahead about that particular
state. Therefore, when following a step-by-step strategy DM only stops on good news about
the last state she explores.
      Turning to the second part of the theorem, to see why it is best to do some thinking ahead
when N is su¢ ciently small, it su¢ ces to note that thinking ahead is obviously bene…cial if
N = 1. Also, the strategy of investing right away delivers a lower payo¤ than …rst thinking
ahead about exactly one state (which, itself is a dominated strategy), if:

                              I+ b           b      I+             + (N   1)b
                          |    {z }                        N         N
                                             |                     {z            }
                      invest immediately
                                                 think ahead about one state
or,
                                                       b
                                        ,N
                                                   b           I

      As stated, the theorem establishes that DM would want to do some thinking ahead if
the number of possible states N is small enough. We also know from the analysis of the
example with two states that even for N = 2, DM may prefer thinking ahead step-by-step
rather than determine a complete action plan. However, what we cannot conclude from the
theorem or our previous analysis is that DM would de…nitely not want to do any thinking
ahead when N is su¢ ciently large. We establish this result in the next proposition.

                                                                           S
      Proposition 8: Consider N equiprobable states. If N                 S I
                                                                                the optimal thinking
strategy is to invest right away and to think on the spot.
                                                 26
                                             S
   Proof. Notice …rst that if N             S I
                                                ;   DM will never think about the last state. That
                    S
is, when N         S I
                         then m < N . To see this, denote by QN                    1   the average payo¤ that DM
expects from the N         1 other states. We then have:

               b    I+         +       (N   1)QN    1             I+       b       +       (N   1)QN   1
                          N        N                                   N               N

                                       , N ( QN     1        I)       QN       1

                                                         QN 1                                       S
as QN   1   2 (S; R) this is equivalent to N            QN 1 I
                                                               ;      which is true if N           S I
                                                                                                       :   Now from
lemma 4 we know that if DM stops thinking ahead at the penultimate state irrespective
of what she learns then she also prefers not to think ahead about the penultimate state,
irrespective of what she learned before. By backward induction, it then follows that DM
prefers no to do any thinking ahead.

   Proposition 8 is quite intuitive. There is little to be gained by working out a highly
incomplete action-plan and if any reasonable plan is complex and will take a long time to
work out then the only reasonable course of action is to just hope for the best and not do
any planning at all.
   Another extreme situation where the best course of action is just to think on the spot
is when the state of nature is highly transitory. Thus, for example, in the extreme case
where there is a new iid draw of payo¤s on the risky action every period there is no point
in thinking ahead, as the knowledge obtained by DM on a particular state will already be
obsolete by the time the state is realized.
   In reality, people sometimes prefer to be faced with complex life situations where rational
forethought makes little di¤erence and the only course of action is to essentially put their
fate in God’s hands so to speak. In such situations they are absolved of all responsibility for
their fate and that makes them better o¤. In contrast, here our DM is always (weakly) worse
o¤ facing more uncertainty than less (as measured by the number of states N ). Indeed, if
                                                               S
one were to reduce the number of states below                 S I
                                                                  ,    DM would want to do some thinking
ahead and be better o¤ as a result.
   Just as some key properties of the optimal thinking strategy with time-deliberation costs
derived in the two-state special case extend to an arbitrary number N of equiprobable states,
some important comparative statics results also extend to this general setting. We report
below two general results we have been able to prove on the optimal order in which DM
should think ahead about future states.
                                                        27
    The …rst result establishes that quite generally it is optimal for DM to think ahead …rst
about the most likely states:

    Proposition 9: Consider N identical states with probabilities of realization        1   >   2   >
::: >   N.   The optimal thinking strategy is to think ahead about the most likely states …rst.
    Proof. see Appendix B.

    The second result establishes that it is also optimal for DM to think ahead …rst about
the highest payo¤ states:

    Proposition 10: Consider N equiprobable states ranked in order of decreasing expected
payo¤, Ri = R + "i ; Si = S + "i , with "1 > "2 > ::: > "N . Assume, in addition, that the "i
are small enough that assumption A3 holds in all N states: for all i; ^ x+"
                                                                          y+"i
                                                                              i
                                                                                . The optimal
thinking strategy is then to think ahead about the highest payo¤ states …rst.
    Proof. see Appendix B.
    The other comparative statics results we have derived in the two-state special case may
also generalize, but we have not yet formally proved these generalizations.


5       Conclusion
The notion of bounded rationality has been with us for a long time now and few economists
would dispute that the model of rational decision-making in most microeconomics textbooks
is a poor description of how agents actually make decisions in reality. It is especially poor at
describing decision making in dynamic decision problems, as normal human beings are not
able to solve complex dynamic programming problems.
    However, despite the descriptive limitations of the rational model, it continues to be the
reference model in economics to describe individual behavior. Sometimes, justi…cations are
o¤ered for sticking with the rational model, such as evolutionary selection, unbiased errors
which average out in the aggregate, or convergence to the rational choice through simple
learning algorithms in stable environments. As pertinent as these justi…cations may be,
they do not always apply in reality and often they are simply invoked as convenient excuses
for staying within a well-understood paradigm that is easy to manipulate. Indeed, a basic
di¢ culty in moving beyond the rational choice framework is that there are several alternative
approaches one could take. Moreover, while bounded rationality models may fare better as
descriptive models of decision-making in reality, they are also signi…cantly more complex and
                                                 28
unwieldy. That is a basic reason why these models have not been incorporated more readily
into the mainstream.
   This is why our primary concern in formulating our approach has been tractability.
Although we have probably oversimpli…ed the dynamic decision problems agents are likely
to face in reality we nevertheless capture a basic aspect of most people’s behavior when
they face complex intertemporal decision problems: they think ahead only about the most
important or salient aspects of the problem in their mind and leave the determination of less
important aspects to a later time. By modeling the basic dynamic decision problem as one
of optimization in the presence of time-deliberation costs, we have been able to characterize
simple optimal incomplete planning strategies that resemble satis…cing behavior. Thus, for
example, we have been able to show that it is optimal for a decision-maker to think ahead
…rst about the most likely future decisions she will face.
   We have also been able to highlight a basic tradeo¤ that is likely to be present in most
dynamic decision problems: the bene…t of thinking ahead is that a decision-maker will be
able to react more quickly to new events or challenges, but the cost is that she delays her
current decision while she thinks through the most important future implications.
   We have simpli…ed our framework so much that our analysis may be seen to be only
applicable to situations where information a¤ects the timing of investment but not the
decision whether to invest. In particular, a natural question is whether our analysis extends
to situations where the investment under consideration could have a negative NPV? We
think that all our analysis extends straightforwardly to situations where DM is uncertain to
begin with whether the investment opportunity she faces has a positive NPV or not. Indeed,
if under DM’s prior beliefs the project has a negative NPV then there is no opportunity
cost in thinking ahead. DM then …nds it optimal to think before investing, and she will
continue thinking until she has learned all the relevant information or up to the point when
her revised beliefs are such that the project has a positive NPV. Should her beliefs evolve in
that direction then we are back to the situation we have analyzed so far.
   Perhaps a more important limitation of our framework may be that we exclude situations
where DM has other irreversible options available besides investing in the project under
consideration. For instance she may face a decision of selecting among several alternative
irreversible investment projects. Adding this possibility to our framework can indeed lead to
changes in the optimal thinking strategies we have characterized. In particular, for this more
complex problem it may be optimal for DM to stop thinking ahead on learning bad news

                                              29
under a step-by-step thinking strategy, an outcome we could not obtain in our simple setup.
We leave the characterization of optimal thinking strategies in this more general problem for
future research.




                                             30
   References
   Beckenkamp, Martin (2004) “Is there an optimization in bounded rationality? The ratio
of aspiration levels”, Max Planck Institute for Research on Collective Goods working paper
#2004/15
   Benabou, Roland and Marek Pycia (2002) “Dynamic Inconsistency and Self-Control: A
Planner-Doer Interpretation”, Economics Letters, 77, 419-424
   Bernheim, Douglas and Antonio Rangel (2002), “Addiction and Cue-Conditioned Cog-
nitive Processes”, NBER working paper #9329
   Berry, D. A. and D. Fristedt (1985), Bandit Problems: Sequential Allocation of Experi-
ments, London, Chapman Hall.
   Bolton, Patrick and Antoine Faure-Grimaud (2004), “Thinking Ahead: Part II, Satis…c-
ing Contracts”, mimeo.
   Bruss, T. (2000), “Mathematische Unterhaltungen: Der Ungewissheit ein Schnippchen
schlagen”, Spektrum der Wissenschaft, 6, 106
   Conlisk, John. “Costly Optimizers Versus Cheap Imitators” Journal of Economic Be-
havior and Organization, 1980, 275-293.
   Conlisk, John. “Optimization Cost” Journal of Economic Behavior and Organization,
1988, 213-228.
   Conlisk, John. “Why bounded rationality?”Journal of Economic Literature, June 1996,
34 (2), pp.669-700.
   Dixit, A.K. and Pindyck, (1994) Investment under Uncertainty, Princeton University
Press
   Gabaix, Xavier and David Laibson, “A boundedly rational decision algorithm” AER
Papers and Proceedings, May 200.
   Gabaix, Xavier and David Laibson, “Bounded rationality and Directed Cognition”mimeo,
Harvard University, 2002.
   Gabaix, Xavier, David Laibson and Guillermo Moloche, “The allocation of Attention:
Theory and Evidence”mimeo Harvard University, August 2003.
   Gilbert, J. and F. Mosteller, (1966) “Recognizing the Maximum of a Sequence”, Journal
of the American Statistical Association 61, 35-73
   Gittins, J.C. and D. M. Jones (1974), “A Dynamic Allocation Index for the Sequential
Design of Experiments.”, in Progress in Statistics, Gani, Sarkadi and Vince (eds.), New York,
North Holland
                                             31
   Gittins, J.C. (1979) “Bandit processes and dynamic allocation indices”, Journal of the
Royal Statistical Society B, 41, 2, p.148-77.
   Henry, C. (1974a) “Option Values in the Economics of Irreplaceable Assets.”, Review of
Economic Studies, 41, 89-104
   Henry, C. (1974b) “Investment Decisions under Uncertainty: The Irreversibility E¤ect”,
American Economic Review, 64, 1006–1012.
   Jehiel, Philippe. “Limited horizon forecast in repeated alternate games,” Journal of
Economic Theory, 1995, 67: 497-519.
   Kahneman, Daniel and Tversky, Amos. “Judgment Under Uncertainty: Heuristics and
Biases”, Science, 1974.
   Loewenstein, George and Ted O’Donoghue, “Animal Spirits: A¤ective and Deliberative
Processes in Economic Behavior”, (2004), Carnegie Mellon Discussion Paper
   Macleod, W. Bentley. (2004) “Towards a Quantum Theory of Learning”mimeo, Columbia.
   Metcalfe, Janet and Walter Mischel (1999), “A Hot/Cool-System Analysis of Delay of
Grati…cation: Dynamics of Willpower”, Psychological Review, 106, 3-19
   Payne, John W., James R. Bettman, and Eric J. Johnson, The Adaptive Decision Maker,
1993, Cambridge: Cambridge University Press.
   Radner, R. (1975) “Satis…cing”, Journal of Mathematical Economics, 2 (2), 253-62.
   Radner, R. and M. Rothschild (1975), “On the Allocation of E¤ort”, Journal of Economic
Theory, 10 (3), 358-76.
   Rothschild, M. (1974) “A Two-Armed Bandit Theory of Market Pricing,” Journal of
Economic Theory, 9, 185-202.
   Rubinstein, Ariel. (1998), Modeling Bounded Rationality. MIT Press: Cambridge.
   Simon, Herbert. “A behavioral model of rational choice,” Quarterly Journal of Eco-
nomics, 1955, 69 (1).
   Whittle, P. (1980) “Multi-armed Bandits and the Gittins Index”, Journal of the Royal
Statistical Society, 42, 143-149
   Whittle, P. (1982), Optimization over time: Dynamic Programming and Stochastic Con-
trol, John Wiley, New York
   Winston, G. (1989) “Imperfectly Rational Choice: Rationality as the Result of a Costly
Activity”, Journal of Economic Behavior and organization, 12 (1), 67-86.




                                                32
                            APPENDIX A: The 2-state model

   Proof of Lemma 1:
   It is immediate from the previous equation that thinking on the spot dominates if
                                                             1
                                                                       I           0:
                                                             2

Suppose now that 21 I < 0:
   - If ^  ^ L ; then maxf ; ^ g = ^ and                                       = (1             ^ )(
                                                                                                         2
                                                                                                             1
                                                                                                                     I) < 0 so that thinking
ahead dominates.
   - If ^ ^ L ; then maxf ; ^ g = and
                                                                                            h            i
                                                      ^ )(       1                                  ^
                                     = (1                                  I) +
                                                                 2                      2
therefore,
                                                                       0,
                                             1                         ^                        1
                            I+                   +                                 I+               +
                                         2            2                                         2        2

                                                             ,
                                         ^           ^E              I + 2(         1+ )
                                                                     I + 2(         1+  )

   Proof of Lemma 2:
   The di¤erence = VE               VL is always negative as:
                                                                                            h                               i
                  = ^       I+                   +       maxf ; ^ g                             I + maxf ; ^ g
                                     2               2
                        h                 i                                                         h                      i
                  =         I + maxf ; ^ g (1                                      ^) + ^                        maxf ; ^ g
                                                                                                2
   Case 1: ^     ^L

                                    =            [ I+             ] (1         ^) + ^ [                          ]
                                                                                     2
                                    = I                   +^           I+
                                                                     + (                                         )
                                                                        2
as the term in bracket is positive,                  is at most equal to:

                                I            +               I+                + (                      ) =
                                                                                2

                            1                                                  2
                                (I                           I +                   + (                  ) )=
                                                                                    2


                                                                 (I                )<0
                                                                           2
                                                                      33
and therefore thinking on the spot dominates thinking ahead. As ^                      ^ L ; no thinking
dominates thinking on the spot.
   Case 2: ^ ^ L
                               h        i              h      i
                         =       I+ ^     (1 ^ ) + ^        ^
                                                     2

                              = (1       ^) I              ^               <0
                                                       2

and therefore thinking on the spot dominates thinking ahead. As ^                   ^ L ; thinking on the
spot also dominates no thinking.

   Proof of Lemma 3:
   As in the proof of Lemma 2, there are two cases to consider:
   Case 1: ^ ^ L
   In that case, no thinking dominates thinking on the spot. Whether thinking ahead is
best depends on the sign of
                                      2
                                  =^ [        I] + I
and this is positive for
                                                       (               ^2 )
                                           I
                                                           1           ^2
The right hand side of this inequality is decreasing in ^ . The inequality cannot hold for
^ = 0, but it holds for ^ =   as then this inequality becomes:
                                                           2
                                           (               2           )
                                     I                     2               =
                                                1              2
                                                                                +

             R
but as I     2
               ,   then I >   2
                                  and we note that

                                                       >
                                               2                           +
                                                       ,

                                                       +           >2
which is true since > . Therefore there exists a new threshold ^ E < ^ L such that thinking
ahead dominates no thinking if and only if ^ exceeds that threshold.
    Case 2: ^ ^ L
    In that case, no thinking is dominated by thinking on the spot. Whether thinking ahead
is best depends on the sign of
                                           2
                                         =^ [                      I] + I       ^

and this is positive for
                                                                   ^
                                                   I
                                                            1+^
                                                           34
The right hand side of this inequality is increasing in ^ . It thus su¢ ces to note that the
inequality holds for ^ = 1. Indeed, we then have:

                                                    I>
                                                                2
which is true under our assumptions.

    Proof of Proposition 1:
                                                                                       S
    First, lemmata 2 and 3 tell us that thinking ahead is dominated if I               2
                                                                                          and that
                                             R
thinking on the spot is dominated if I       2
                                               .
    Case 1: suppose …rst as we did in the text that ^          ^ L . Then surely for I       R
                                                                                             2
                                                                                               , the
best strategy is to think ahead as it dominates thinking on the spot, which itself dominates
no thinking in that case. When I is so high DM thinks about both states (from lemma 1)
                                        S
before investing. Conversely for I      2
                                          , deferring all thinking is best as it dominates both
thinking ahead and no thinking.
    We are left with the intermediate case where 2S I         R
                                                              2
                                                                . In this case the best alternative
to planning ahead is to follow the strategy of thinking on the spot. Indeed, we have:

                        VE    VL


                  = ^        I + R+ ^                       + (1           )^        I+ S+
                                2   2                                                  2   2
                         h        i
                           I+ ^



                     = ^         I+ R + ^                           + (1        )^    I+ S
                                    2   2                                               2
                             h        i
                               I+ ^



       = ^ ^       I + R + (1                  ^)          I+ R + ^                  + (1   )^   I+ S
                      2                                      2   2                                 2
           h          i
                I+ ^



                             = ^ (1            ^)          I + R +^(                   I)
                                                              2
                                 h                     i
                                           I+ ^


                    =         ^2       ^       I ^
                                                   2
                                                           1 + ^ (1         ^)         I+ R
                                                                                         2

                        = ^        1       ^        I 1+^              ^         I+ R       ;
                                                                                   2
                                                           35
     and so we have        0 if and only if:

                            ^       I 1+^               ^       I+ R               0;
                                                                  2
or                                                  h               i
                                                ^               R
                                                                2
                                          I                             :
                                                 1+^            ^
Note that the right hand side is increasing and concave in ^ . Moreover, we have:
                                              h        i
                                                     R
                                     S               2
                                        <
                                     2     1 + (1      )

and,                                                h                  i
                                                                   R
                                              R                    2
                                                >
                                              2         2

    Case 2: suppose now that ^        ^ L . Then the best alternative to planning ahead is to
follow the strategy of no thinking. From the previous lemmata, we can verify that for I < 2S ;
the best strategy is to invest without any thinking taking place. For intermediate values,
where 2S < I < 2R , we have:

                VE = ^          I + R+              + (1            )^           I+ S+
                                   2   2                                           2   2
and so

                     = ^       I + R+                   + (1           )^        I+ S+
                                  2   2                                            2   2
                           [ I+ ]

     Rearranging terms as before, we obtain that                       0 if and only if:
                                h         i
                             ^          R
                                                                      ^ )
                                        2                       (
                         I                  +                                              (1)
                               1+^      ^     (1               ^ )(1 + ^ (1         ))

   which may or may not be feasible given that we need both I     2
                                                                    R and ^ ^ L to hold.
                                                       2
Inequality (1) in particular cannot be satis…ed if R     +
                                                           , which is therefore a su¢ cient
condition for no thinking to dominate thinking ahead. Otherwise thinking ahead may be
best.
   Similarly, when 2R < I, step-by-step thinking is dominated by complete planning.
Whether DM starts investigating any state before investing depends on the sign of
                                        2
                                      =^ [               I]    (            I)

     and   > 0 if:                    h            i
                                          ^2    ^ L + (1           ^ 2 )I > 0
                                                     36
                                   ,I>                  (^L     ^2)
                                                   2
                                           (1     ^ )

which may or may not be true. Notice that it is more likely to be satis…ed for ^ close enough
to ^ L , as the right hand side of this last inequality is decreasing in ^ . A su¢ cient condition
for thinking ahead to dominate in this parameter region is

                                             R
                                               >
                                             2
or R > 2 .


   Proof of Proposition 4:
   We …rst establish under what conditions the payo¤ of the step-by-step strategy is in-
creasing in ". Given that DM begins by exploring the easy state, she expects:

      M0 = ( ^ + ")        I + R + (^        ")         + (1      )( ^    ")       I+ S+
                              2   2                                                  2   2
and so
                  @M0
                      =         I+ R         2"          + (1         )   I+ S
                   @"             2                2                        2
The value of this derivative is positive when " goes to zero as I              2
                                                                                   R in the region where
step-by-step planning is optimal.

    Proof of Proposition 5:
    Consider …rst the case where 1 = + "; 2 =           ", with " > 0. In that case, the payo¤
of the thinking on the spot, or complete planning strategies are unchanged. In contrast,
the payo¤ of the step-by-step strategy is a¤ected by this average-belief-preserving spread as
follows.
    Whichever state i DM thinks about …rst, DM will want to stop thinking further about
the other state if she discovers that i = 2R , whenever I 2 [ 2S ; 2R ]. It is then best for her
to start thinking about state 1 , the state with the higher prior belief 1 . The reason is
that she is then more likely to …nd that 1 = 2R and if state 1 arises, DM will be able to
realize high returns relatively quickly. Note, therefore, that as more prior probability mass
is shifted to the high return on the risky action in state 1 , the step-by-step strategy also
becomes relatively more attractive.
    Second, suppose that returns themselves di¤er across the two states, and that returns in
state 1 are higher than in state 2 : S1 = S + " while S2 = S "; and R1 = R + " while
R2 = R ". Again, we take " to be small enough that assumption A3 remains valid.
    This redistribution of returns across states leaves DM’ s expected payo¤ una¤ected as
long as I < (S2 ") or I > (R+")
                              2
                                  . Indeed, in that case she chooses to either defer all of her
thinking until the uncertainty about the state is resolved, or to work out a complete plan
                                                                                             +
before investing. In both cases, her expected payo¤ only depends on average returns 1 2 2
and is therefore una¤ected by changes in ".
    But if (S+")
             2
                 < I < (R2 ") , DM will engage in step-by-step thinking and will stop thinking
ahead if she learns that the risky decision is e¢ cient in the state she thinks through …rst.
                                              37
Once again, it is then best for her to think about the high payo¤ state …rst. The basic logic
is the same as before: by thinking …rst about the high payo¤ state DM is able to bring
forward in time the moment when she realizes the highest return.
     Does this mean that when DM chooses to think ahead, she is always (weakly) better
o¤ thinking …rst about the high payo¤ state? The answer to this question is yes. There is
one particular situation where conceivably this prescription might not hold. That is when
 (S ")
   2
        < I < (S+")
                  2
                      . In this case DM either thinks about the low return state …rst, or she
defers all of her thinking to when the state of nature is realized. However, in the later case,
her payo¤ is una¤ected by changes in ", while in the former it is decreasing in ". What is
more, for " = 0 thinking on the spot dominates step-by-step thinking. Therefore, the latter
strategy is then dominated by thinking on the spot for all " > 0. Hence, it remains true that
whenever thinking ahead pays, it is best to think …rst about the problem with the highest
return.
     Finally, for (R2 ") < I < (R+")
                                   2
                                      , DM either thinks …rst about the high return state or
works out a complete plan. In the latter case, her payo¤ is una¤ected by ". If she decides
to think …rst about the high return state, her payo¤ goes up with " so that eventually this
strategy dominates complete planning. This completes the proof.

   Proof of Proposition 6:
   Without loss of generality set 1 = 0. Suppose by contradiction that DM thinks ahead
about state 2 …rst under a step-by-step thinking approach. Note that for this form of
step-by-step thinking to be optimal we must have
                                        (1+     2)                  (1+   2)

                                                     S<I<                      R.
                                            2                         2
   Her payo¤ under this strategy is
               "                       !                                                                          !#
                          (1+ 2 )                                                        (1+   2)

        V2 = ^      I+            R+ ^   + (1                             )^        I+              S+
                            2       2                                                     2               2

On the other hand if DM thinks ahead …rst about state 1 her payo¤ is
            "                                                               !
                             (1+ 2 ) h                                    i
   V1 = ^          I + R+              (1 (1         ) 2 ) + (1      ) 2^
                      2        2
                        (                (1+ 2 )
                                                 h                                                                i )#
                            I + 2S + 2             (1 (1     ) 2 ) + (1      )                            2   ^    ;
           +(1    ) max                                      (1+ 2 )
                                                                                                                         :
                                             ^( I + S +               )
                                                                      2             2

   Thus,
                    "                                    !                                                        !#
                                        (1+     2)                                                  (1+   2)

        V1      ^       I + R+                       ^       + (1         )^        I+ S+
                           2                2                                         2              2

and
            "                                   !                                                     !#
                             (1+   2)                                                    (1+   2)
        ^           I + R+              ^            + (1        )^       I+ S+                                   V2 :
                       2       2                                            2              2

                                                            38
                                      APPENDIX B: The N -states model.


    The derivation of the results of section 4 is made easier by formalizing DM’s problem as
follows. Denote by:

         i   2 fR; Sg, the maximum payo¤ that DM can achieve in state                            i,

      hk , a history of payo¤ observations of length k; that is, hk 2 fR; Sgk is a sequence of
      payo¤s uncovered in states 1 ::: k . For instance, h2 = (S; S) means that the optimal
      decision in the …rst two states, 1 and 2 , has been found to be the safe action with
      payo¤ of S in both states.

       (hk ), the probability that DM decides to explore state k + 1 when she has already
      explored k states and has learned history hk . That is, with probability (1    (hk ))
      she explores no more than k states and invests right away. Given these de…nitions we
      have: (hk ) = 0 ) (hk+1 ) = 0.

         k,   the set of all possible continuation-histories following k :                  k    = fR; SgN    k
                                                                                                                  .


   Using these notations, we can represent the expected payo¤ of DM who decides to explore
a …rst state before investing as:
               h (1+ (h )+ (h ) (h )+:::)
  M0 = E 0 ^
                       1     1    2
                                          ( I + [ 1 1 + (h1 ) 2 2 + (h1 ) (h2 ) 3 3 + :::]
                                                             i
           + ^ [ 2 2 (1     (h1 )) + 3 3 (1    (h2 )) + :::]

Or,
                        "                          "                                                      #!#
                                (1+'1 )                          X1
                                                                 N             X1
                                                                               N
         M0 = E             ^                 I+             +            +^
                    0                                  1 1            1             +1   +1 (1        (h ))           ;
                                                                 =1            =1

where:
                                          !
                 X1 Y
                 N
  1. '1 =                         (ht )       is the expected number of subsequent states that DM explores
                  =1 t=1
    ahead given that DM decides to explore a …rst state, and
                             !
                      Y
  2. 1 =      +1   +1   (ht ) is the payo¤ in state + 1 times the likelihood of state
                                     t=1
        + 1, multiplied by the probability of exploring state ahead under the exploration-
      plan f (ht )g, given that DM decides to explore a …rst state.




                                                                 39
   For illustration purposes, this expression in the case where N = 2 becomes:
                     h 1+ (h )                                                   i
           M0 = E 0 ^             I + ( 1 1 + (h1 ) 2 2 ) + ^ 2 2 (1
                            1
                                                                          (h1 ))
                                                                                    S         R
and in the equiprobable symmetric state case, with parameter values satisfying      2
                                                                                        <I<   2
                                                                                                ;
we have (S) = 1 and (R) = 0. Thus, we obtain:
                                          !
                               R      ^                 2        S
               M0 = ^      I+     +          + (1    )^    I+      +         .
                               2       2                         2     2

    Similarly, we can write the expected continuation value of exploring one more state when
already m 1 states have been explored as:
                          "                 "m 1                       #
                              (1+'m )         X
                    E       ^          I+           (z R + (1 z )S) +
                        m 1
                                                       =1

                        "                                                    #!#
                                        X1
                                        N             X1
                                                      N

                             m m    +        m   +^         +1   +1 (1   (h ))
                                        =m            =m

where again:
                                !
               X1
               N    Y
  1. 'm =                   (ht )   is the expected number of subsequent states that DM explores
               =m t=m
    ahead, given that DM decides to explore at least m 1 states,
                               !
                      Y
  2. m =       +1  +1     (ht ) is the payo¤ in state + 1 times the likelihood of state
                             t=m
       + 1, multiplied by the probability of exploring state ahead under the exploration-
     plan f (ht )g, given that DM decides to explore at least m 1 states.
     And,

  3. z is an indicator variable that takes the value 1 if and only if DM …nds out that the
     return on the risky action in state   is R.

   We can now proceed to prove the following results.

   Lemma 5: It is never optimal to stop thinking ahead on learning bad news and to
continue thinking ahead on learning good news.

    Proof of Lemma 5:
    (We prove this lemma without assuming that states are equiprobable, nor that their
average payo¤ is identical). Suppose, by contradiction, that there exists an exploration
strategy and history of length m 1 such that: (hm 2 ; Rm 1 ) = 1 and (hm 2 ; Sm 1 ) = 0.
For this exploration strategy to be optimal it must be the case that:


                                                      40
    a) there exists a continuation strategy pro…le following good news in state m 1,
(~ (hm 2 ; Rm 1 ; m ); ~ (hm 2 ; Rm 1 ; m ; m+1 ); :::~ (hm 2 ; Rm 1 ; m ; ::: N )), such that:
                      "                                                !
                            'm
                          1+~              X2
                                           m
              E         ^          I+            (z R + (1 z )S ) +
                  m 1
                                                           =1
                                                                                                                             !!#
                                                           X1
                                                           N                       X1
                                                                                   N

                  m 1 Rm 1            +          m m   +             ~m + ^                 +1        +1 (1     e(h ))
                                                            =m                     =m
                                                                          !
                          X2
                          m                                                                                   X1
                                                                                                              N
               I+                          (z R + (1       z )S )             +     m 1 Rm           1+
                                                                                                        ^                   +1   +1
                              =1                                                                                =m

   b) and, for any continuation strategy pro…le following bad news in state m 1,
( (hm 2 ; Sm 1 ; m ); (hm 2 ; Sm 1 ; m ; m+1 ); ::: (hm 2 ; Sm 1 ; m ; ::: N )); we have:
                     "                                                !
                       1+'m
                                        mX2
             E m 1 ^            I+              (z R + (1 z )S ) +
                                                           =1
                                                                                                                             !!#
                                                           X1
                                                           N                       X1
                                                                                   N

                  m 1 Sm 1            +          m m   +             m    +^                +1        +1 (1         (h ))
                                                           =m                      =m
                                                                          !
                          X2
                          m                                                                                   X1
                                                                                                              N
          <    I+                          (z R + (1        z )S )             +    m 1 Sm           1+
                                                                                                        ^               +1       +1
                              =1                                                                                =m

    We claim this yields a contradiction: consider instead the reverse strategy where upon
learning that state m 1 yields Sm 1 , DM decides to follow the continuation strategy pro-
…le that she would have followed if she had learned instead that state m 1 yields Rm 1 ,
(~ (hm 2 ; Rm 1 ; m ); ~ (hm 2 ; Rm 1 ; m ; m+1 ); :::~ (hm 2 ; Rm 1 ; m ; ::: N )).

   This gives her:
                              "                                                                           !
                                        'm
                                      1+~                   X2
                                                            m
        M        E                ^              I+                       (z R + (1           z )S )            +
                        m 1
                                                                =1
                                                                                                                                 !!#
                                                                X1
                                                                N                   X1
                                                                                    N

                        m 1 Sm 1           +        m m    +          ~m + ^                     +1     +1 (1       e(h ))
                                                                =m                   =m

            1+~'m
   Adding ^                                  ^ 1+~'m
                    m 1 Sm 1 and subtracting                                         m 1 Rm 1           on both sides of condition
a) we observe that this is larger than:
                                                                                    !
                                           X2
                                           m
                          I+                        (z R + (1             z )S )        +        m 1 Rm 1 +
                                            =1


                        X1
                        N                                                 h                                         i
                    ^                                                         ^ (1+~'m ) Sm
                                      +1     +1 +     m    1E    m 1                             1      Rm      1       =
                         =m
                                                                     41
                                                                        !
                         X2
                         m                                                                         X1
                                                                                                   N
                 I+               (z R + (1                  z )S )         +                 ^
                                                                                  m 1 Sm   1+           +1    +1
                           =1                                                                      =m
                                  h                      i
             +                              ^ (1+~'m )
                  m   1E    m 1       1                      Rm     1       Sm    1


we observe that the last term is positive, and so we conclude that M is larger than the
right hand side of b): there exists a continuation strategy that is preferred to the strategy
of investing upon learning bad news in state m 1 .

    Proofs of Propositions 8 and 9:
    We establish these propositions by showing that the optimal policy is to think ahead
about the state with the highest expected payo¤ …rst. To simplify notations we suppose
that states are ranked by decreasing expected payo¤, such that 1 1         2 2   ::::   N N.
    Suppose by contradiction that the optimal exploration plan f (ht )g is such that the
order in which states are explored is not by decreasing expected payo¤. Then there exists
at least one pair of states ( j ; k ), adjacent in the order in which they are explored, such
that the …rst of these two states to be explored, say state j , has a lower expected payo¤:
  j j < k k.
    We now show that DM will then be better o¤ interchanging the order in which these two
states are explored. That is, DM is better o¤ with exploration plan f k_j (ht )g than with
plan f (ht )g.
    There are four di¤erent types of histories ht under the optimal exploration plan f (ht )g
to consider:

  1. under ht neither of the states ( j ;                k)   is explored ahead,

  2. both states ( j ;     k)   are explored ahead,

  3. exactly one of the states ( j ) is explored ahead,

  4. DM invests after exploring                 j   upon …nding Rj and continues exploring               k   upon …nding
     Rj .

    Observe that in the …rst two cases the expected payo¤s under respectively f k_j (ht )g and
f (ht )g are the same. In the third case, if DM inverts the order of j and k and otherwise
leaves her exploration plan unchanged, she will be strictly better o¤. Indeed, under f (ht )g
her expected payo¤ just before exploring state j and having explored all states i , i 2 E,
is:
                       "                                                #
                               X                              X
                     ^   I+           i+        + ^       +^              ;
                                                    i         j j               k k          i i
                                          i2E                                         i2U

where U denotes the subset of states that remain unexplored. Under f k_j (ht )g her expected
payo¤ just before exploring state k and having explored all states i , i 2 E, is:
                     "                                                #
                              X                             X
                   ^    I+         i i+     k  + ^ j +^           i     .
                                                                k                 j            i
                                          i2E                                         i2U


                                                               42
Thus, the incremental payo¤ obtained from inverting the order of these two states is
                                                           ^ (1   ^ )(
                                                                            k k            j j)   > 0.

   In the fourth case the strategy of proof is very similar. DM invests after exploring j
upon …nding R and continues exploring k upon …nding R. We show that if DM instead
explored state k …rst and stuck to exactly the same policy after that exploration, she will
be made better o¤.
   In a …rst step, suppose that when starting with state j DM invests when …nding R while
she explores state k only upon …nding R, and then stops exploring: exploration will either
stop at state j or will cover states j and k but no more. Starting with state j provides
DM with the payo¤:

                                         "                                                                                                #
                                                           X                                                         X
              Mj = ^                 j           I+               i i   +         j Rj     + ^      k k         +^                i i         +
                                                           i2E                                                       i2U
                                                       "                                                                                            #
                                 2                                 X                                                           X
                                ^ (1              j)        I+                  i i    +     j Sj   +           k k   +^                      i i
                                                                    i2E                                                        i2U

where U denotes the subset of states other than j and k that remain unexplored. Under
f k_j (ht )g where DM starts with state k and at most explores another state j she can
expect to get:

                                         "                                                                                                #
                                                           X                                                         X
              Mk = ^                 k           I+               i i   +           k Rk   + ^      j j         +^                i i         +
                                                           i2E                                                        i2U
                                                       "                                                                                            #
                                 2                                 X                                                           X
                                ^ (1              k)        I+                   i i   +     k Sk   +           j j   +^                      i i
                                                                    i2E                                                        i2U

Thus, when        k   =     j   the incremental payo¤ obtained from inverting the order of these two
states is
                                                       ^ (1       ^ )(
                                                                                k k        j j)     > 0.
                                              1
If now,   k   >       j   but        i   =    N
                                                ; Ri       = R and Si = S for all i, then the previous expressions
become:

                                                 "                                                                                        #
                                                                  X                                                        X
                          Mj = ^             j         I+                   i   +        R+         ^
                                                                                                            k   +^                    i       +
                                                              N   i2E
                                                                                     N         N                     N      i2U
                                                  "                                                                                   #
                           2                                      X                                                    X
                          ^ (1               j)        I+                   i    +        S+                +^
                                                                                                        k                         i
                                                              N   i2E
                                                                                      N        N                 N       i2U

and:

                                                                                43
                                      "                                                                                                       #
                                                            X                                                               X
                   Mk = ^         k            I+                       i    +        R+            ^
                                                                                                            j   +^                        i       +
                                                       N    i2E
                                                                                  N             N                     N     i2U
                                       "                                                                                                  #
                       2                                    X                                                             X
                   ^ (1           k)           I+                        i   +         S+                   +^
                                                                                                        j                             i
                                                       N    i2E
                                                                                  N             N                     N    i2U

we then have Mk        Mj =
                                                                "                                                               #
                                                                                      X                             X
                           ^ (1       ^ )(     k           j)        I+                         i   +^                      i
                                                                                  N    i2E
                                                                                                            N       i2U

                                  2
                                                                                                h                 i
                             +^            (   j           k ) + (vk                vj ) ^          R           ^S =
                                      N                                                N
                                                              "                                                                 #
                                                                                      X                             X
                           ^ (1       ^ )(     k           j)        I+                         i   +^                      i
                                                                                  N    i2E
                                                                                                            N       i2U


                                               +(      k        j)
                                                                     ^          (1      ^ )R > 0
                                                                            N


    We have thus shown that if DM starts exploring state j and explores at most one other
state k (upon …nding that R =R), she is better o¤ inverting the order of j and k .
    In a second step, consider now the possibility that DM explores j , stops exploring further
if she …nds that the return in that state is Rj ; continues exploring if Sj ; and then either stops
exploring upon …nding that state k returns Rk , or explores exactly one more state if she
…nds that state k returns Sk . This strategy yields an expected payo¤ of:
                      "                                                    #
                               X                                X
         Mj = ^ j        I+           i+      Rj + ^
                                                   i         +^     j         +              k k                           i i
                                       i2E                                                                      i2U
                                       "                                                                                                              #
                   2                                       X                                                                 X
                 ^ (1         j) k             I+                               +                                     +^
                                                                        i i             j Sj    +           k Rk                              i i         +
                                                           i2E                                                                  i2U
                                                       "                                                                                                        #
                   3                                                    X                                                                     X
                 ^ (1         j )(1            k)           I+                        i i   +       j Sj        +         k Sk   +^                       i i
                                                                         i2E                                                                  i2U

   If now DM started instead with state k and otherwise stuck with the same strategy (i.e.
invests as soon as she gets one piece of good news and continues exploring for a maximum




                                                                             44
of 3 states otherwise), she would get:
                     "                                                                                                               #
                              X                                                                                X
        Mk = ^ k         I+        i i+                             k Rk        + ^           j j     +^                       i i       +
                                         i2E                                                                   i2U
                                         "                                                                                                           #
                   2                                        X                                                                      X
                  ^ (1      k) j               I+                               +                                          +^
                                                                       i i                   k Sk   +        j Rj                              i i       +
                                                             i2E                                                                   i2U
                                                       "                                                                                                         #
                   3                                                       X                                                                 X
                  ^ (1      j )(1               k)           I+                      i i      +         j Sj       +          k Sk   +^                   i i
                                                                           i2E                                                               i2U

    Observe that the di¤erence in payo¤s does not depend on what happens if DM goes on
exploring more than states k and j . This therefore establishes that if Mk       Mj for the
strategies under consideration then this inequality also holds for any continuation strategy
that involves exploring more than states k and j .
    We have:
    - when j = k ; Mj Mk equals:
                               h                                i
                           ^      R  +  ^           R     ^
                                 j j       k k     k k       j j +


                                 ^ 2 (1                )           j Sj    +        k Rk              k Sk                 j Rj

or rearranging,
                                              h
                                     ^              j Rj     +^        k k               k Rk
                                                                                                        ^      j j         +
                                                                                                                               i
                                     (1             )^       j Sj +            k Rk               k Sk                 j Rj
                   h                                                                                                                                     i
           =^          j Rj +
                              ^
                                         k    Rk             k Rk
                                                                            ^
                                                                                 j       Rj + (1                   )^          k Rk            j Rj

                                         =^              ( j Rj             k Rk )(1
                                                                                                    ^)             0
if either j < k or Rj < Rk .
                                             1
     - If now, k > j but i =                 N
                                               ; Ri        = R and Si = S for all i, then the previous expressions
become:
                                     "                                                                                                     #
                                                            X                                                              X
  Mj    Mk = ^ (       j        k)           I+                        i   +         R+               ^        +^                              +
                                                        N    i2E
                                                                                 N                N                        i2U
                                                                                                                                   N
                                                                           "                                                                                         #
                   2                                                                                X                                                    X
                  ^ [(1          j) k               (1        k) j ]             I+                            i   +           S+          R+^
                                                                                              N     i2E
                                                                                                                           N           N                  i2U
                                                                                                                                                                 N
Simplifying,
                                                    "                                                                                                    #
                                                                           X                                                           X
        Mj     Mk = ^ (vj                    vk )          I+                        i   +          R+                 ^       +^                            +
                                                                   N       i2E
                                                                                              N                N                       i2U
                                                                                                                                               N
                                                       "                                                                                              #
                            2                                               X                                                        X
                           ^ [       k            j]        I+                           i   +        S+                R+^
                                                                    N       i2E
                                                                                                 N                 N                 i2U
                                                                                                                                             N
                                                                           45
Or,
                                       "                                                                             #
                                                            X                                           X
 Mj    Mk = ^ (       j           k)           I+                   i   +        R+           ^    +^                    +
                                                    N       i2E
                                                                             N            N             i2U
                                                                                                                N
                                           "                                                                                                 #
                 2                                          X                                                                      X
                ^ [       k           j]         I+                     i   +        S+        ^         ^      +        R+^
                                                       N    i2E
                                                                                 N         N        N               N              i2U
                                                                                                                                         N

And,
                                                            "                                                                        #
                                                                                 X                                       X
      Mj   Mk = ^ (           j            k )(1
                                                       ^)         I+                   i   +       R+       ^       +^                   +
                                                                            N    i2E
                                                                                               N        N                i2U
                                                                                                                               N

                      ^2(         k         j)         S            ^
                                                   N            N

                      0

    Therefore, DM is better o¤ starting thinking ahead about state k . This establishes that
when DM’s strategy involves investing when she …nds one piece of good news and continuing
exploring ahead otherwise, she is better o¤ thinking ahead …rst about state k . As we have
also established that the same is true when DM’s strategy involves investing no matter what
she …nds in exploring states j or k . The same is true if DM explores both states no matter
what she …nds. We can therefore conclude that DM is always better o¤ exploring …rst state
 k.




                                                                            46
