              LINEAR REGRESSION DIAGNOSTICS

                     Roy E. Welsch
                           and
                         Edwin Kuh
          Massachusetts Institute of Technology
                           and
               NBER Computer Research Center

                 Working Paper No. 173




                   PRELIMINARY DRAFT

                 Revised   25 March 1977




We are   indebted to the National Science   Foundation for supportingthis
research under   Grant   SOC76-]j311 to the NEER Computer Research Center
.                                         ABSTRACT


         This paper attempts to provide the user of linear multiple regression
    with a battery of diagnostic tools to deteniuine which, if any, data points
    have high leverage or influence on the estiiiation process and how these
    possibly discrepant data points differ from the patterns set by the rrajori-ty
    of the data. The point of view taken is that when diagnostics indicate the
    presence of ariomolous data, the choice is open as to whether these data are
    in fact    unusual and helpful, or possibly harmful and thus in need of modifica-

    tions or deletion.

         The methodology developed depends on differences, derivatives, and

    decompositions of basic regression statistics. There is also a discussion of

    how these techniques can be used with robust and ridge estimators. An example

    is given showing the use of diagnostic methods in the estimation of a cross-

    couriUy    savings rate model.


                                     ACKNOWLEDGE!EJ'JT5


         The authors would like to     acknowledge helpful conversations with David
    Hoaglin,   Frank Hampel, Richard   Hill,   David   Andrews, Jim   Franc, Cohn Mallows,
    Doug Martin, and Fred Schweppe.
S
                                    TABLE OF CONTENTS

                                                                                   Page
     1. INTRODUCTION                                                           .         1
         1.1 General Goals                                                           1
         1.2   Regression Diagnostics arid   Model Input
               Perturbations                                                         3
         1.3 Modeling Research Aims and Diagnostics .                                6
              1.3.1 Leverage and Disparate Data                                      6
               1.3.2 Collinearity                                                    7
              1.3.3 Regression Parameter Variability in Time                         8
         1.14 Notation                                                               9

     2. LEVERAGE POINTS AND DISPARATE DATA       .                                  10
         2.1 Introduction                                                  •        10
         2.2 Residual Diagnostics                                          • 14
         2.3 The Hat Matrix                                                •       18
        2.14 Row Deletion Diagnostics                                      • 23
        2.5 Regression Statistics                                          •       26
        2.6 Influence and Variance Decomposition                           •       28
        2.7 More Than One Row at a Time                                    •       31
        2.8 Interface with Robust and Ridge Regression                     • 33
        2.9 An Example: An Inter-Country Life Cycle Savings
               Function                                                            36
               2.9.1 Residuals                                                     38
               2.9.2 Leverage and Diagonal Hat    Matrix Entries                   38
               2.9.3 Coefficient Perturbation                      •   .   . 39
               2.9.4  Variation in Coefficient Standard Errors     •   .   . 39
               2.9.5 Change in Fit                                 •   .   . 40
               2.9.6 A Provisional Sununary                        •   •   . 41
               2.9.7 One Further Step                              •   .   . 42
       2.10    Final Conunents                                                     143

    REFERENCES                                                     •   .           414

    Appendix 1. BASIC DIFFERENCE FORMULAS                          •   .   . A1.1
    Appendix 2. DIFFERENTIATION FORMULAS                           •   .   . A2.1
    Appendix 3. 'IHEOREMS ON THE HAT MATRIX                        •   .   . A3.1
    Appendix 4. EiIBITS FOR SECTION 2.9                            •   •   . A4.1
.    1. INTRODUCTION

         1.1 General Goals

            Economists and   other model   builders have responded willingly to major

    opportunities that have appeared in the past two decades -          a rapidly growing
    deirand for policy    guidance and forecasts                    business, and
                                                    from government and
    the    purely intellectual goal of advancing the state of knowledge through
    model development. The fundamental enabling condition has           been   the ability
    to   produce more intricate models at decreasing unit cost because of advances

    in computer technology. A large econometric         model twenty   years ago had
    twenty   equations:   today a large model has a thousand equations. It is not

    only   larger mode's, but also larger data sets and more sophisticated
    functional   forms and estimators      that have burgeoned.
           The transition from slider'ule and desk calculator to the large scale

    digital computer has happened with startling speed. The benefits have, in

    our opinion, been notable and at times exciting we know a great deal more

    about the economy and can provide more intelligent guidance as a direct

    result of increased computational power. At the same time, there are

    hidden costs of current approaches to quantitative economic research via

    computer which ought to be recognized.

           One major cost is that, today, the     researcher is   a great deal further

    away from data than he was, perforce, in the heyday of the desk calculator.

    If there are a great many equations to estimate or thousands of observations

    for a few equations, there is a natural tendency to use the computer for what

    it does well: process data. A tape arrives and after a frustrating day or

    two is accessible by a computer program (often a regression package, plain or
fancy). Then estimation
satisfactory conclusion is obtained.
                                            —2—




                             and hypothesis testing get underway until some

                                                It is not misguided   nostalgia to

point out that it was more likely, with the more labor intensive technology
                                                                                     .
of the past, for the researcher to uncover peculiarities in the data.

Nor    do we counsel a return to the golden past. What concerns
us is that the      Tsomethingt   which has been lost in modern practice is
valuable and   is   not recoverable from standard regression statistics.
Our first   major objective is to suggest procedures that exploit computer
brawn in new ways that will permit us to get closer to the character
of the data and its relation to hypothesized and estimated models.

       There is the related       issue   of reliability. Our ability to crunch

large   quantities of numbers at low cost makes it feasible to iterate
many   times with a given body of data until the estimated model meets

widely accepted performance criteria in terms of statistical measures

such as t statistics, Durhin-Watson statistics and multiple correlations,

along with theoretically approved coefficient signs and magnitudes.

The iterative process is not what the statistical theory employed was

originally all about, so that it behooves us to consider alternative

ways of assessing reliability, which is a second major objective of this

paper.

       Another aspect of reliability is associated with questions of distance

from the data that were mentioned at the outset. Specifically, the

closer one is to the data, the more likely it is that oddities in the

data will he uncovered or failure of the model arid data to conform with

each other will be discernible, so that reliability can be increased

when the   researcher has   more     intimate   contact with the data. At the same
.    time,
                                            —3—




             this posses a dilejmia, since the researcher may     then   be excessively
     prone to devise theories from data.       This temptation, often referred to as
     data   mining, should be restrained. One sort of insurance against        data
    mining    is to be a strict Baysian arid   thus   be guided by sensible rules for
    combining prior and    posterior   information. Alteniatively the model
    should be tested - repeatedly      if possible - on bodies of data unavailable
    at the time. Being a strict Baysian is not always practical nor is it

    deemed to be universally desirable. As a general rule then, the most

    practical safeguard lies with replication using previously unavailable data.


      1.2 Pegression Diagnostics arid Model Input Perturbations


            This paper presents a different approach to the analysis of linear

    regression. While we will sometimes use classical procedures, the

    principal novelty is greater emphasis on new diagnostic techniques.

    These procedures sometimes lack rigorous theoretical support,

    hut possess a decided advantage in that they will serve as yet unmet

    needs of applied research. A significant aspect of our approach is

    the development of a comprehensive set of diagnostics.

         An important underlying concept is      that of       regression model
                                                           perturbing

    inputs   and examining the model output response. We view model inputs broadly
    to include data,                be estimated), error models and estimation
                       parameters (to

    assumptions, functional forr and a data ordering in time or space or
    over other characteristics. Outputs include fitted values of the
    dependent   variable, estimated parameter values, residuals and functions

    of these (R2, standard errors, autocorrelations, etc.).

    We plan to   develop various types of input perturbations that will       reveal
    where model outputs are unusually sensitive. Perturbations can take the
form of differentiation or differencing, deletion (of data), or a
                                                                                     .
change in estimation or error     model   assumptions.

       The   first approach to perturbation is "differentiation" (in a
broad sense) of output processes     with respect to input processes, in

order   to find a rate of change. This will provide a first order measure
of   how output is influenced by input; differences would he substituted

for derivatives in discrete cases. If the rate of change is large, it

can be a sign of potential trouble. Generally, one would like to have

small input perturbations lead     to small output deformations. We would
also use this idea to see how big a perturbation can be before everything
breaks down. Of course, a "good' model is generally responsive to anticipated
changes in input.
      For example, one could 'differentiate" the model with respect to its
parameters to ascertain output sensitivity to small changes in the
parameters. (We     could,   for example, evaluate this parameter sensitivity
function at the estimated parameter values.) This might indicate some
of   the more critical parameters in   the model that deserve further
analysis.
      A second procedure is to perturb the input data by deleting or

altering one data point and observe changes       in the   outputs. More generai]y

we can remove random groups of data points or, for time series, sequences

of data points. This is one way to search for parameter instability

over time. By deleting individual data points or collections of points

one can observe whether or not subsets of the data exert unusual influence

on the outputs. In particular,      it is   possible to establish if a minority

of the data behave differently from the majority of the data. The concept
                                               —5—

S
    of discrepant behavior by a minority of the data is basic to the diagnostic
    view elaborated in this paper.
           The   third approach will be to examine output sensitivity to changes
    in   the   error   model.   Instead of using least squares, estimators such as
    least   absolute residuals would be applied which impute less influence

    to large residuals. A more promising alternative for diagnostic purposes

    is the Huher type error model [1]. Varying a parameter in the Huher           model
    provides a. way to examine sensitivity to charges in the error assumptions.

    This area is related to recent research in robust statistics[17].

           Another aspect of changed error assumptions is specific to time

    series. Practicing econometricians are well aware that parameter estimates

    change when the sample period is altered. While this might only

    reflect expected sampling fluctuations, the possibility exists that the

    population parameters are truly variable and should be modeled as a random

    process. It is also possible that the population parameters are stable

    hut mispecification causes sample estimates to behave as if they were a

    random process. In either case explicit estimation methods for random

    parameters based on the Kaimnan filter might reveal parameter instability

    of interest from a diagnostic point of view.

           Thile classical statistical methods in most social       science   contexts treat

    the   sample as a given and     then   derive tests about model adequacy, we take the

    more   eclec-tric position that diagnostics might reveal weaknesses in the data,
    the model or both. Several diagnostic procedures, for example, are designed to
    reveal unusual rows or       outliers in the data matrix   which by assumption
S   has no formal       distribution properties.   If a suspect data row has been

    located, the investigator faces several choices. One common practice is
                                          —6—

                                                                                      .
to introduce a di..mnny variable, especially when subsequent examination
reveals that an "unusual" situation could have generated that data row.
Alternatively the model      may   be respecified in a more   complex way. Of
course the suspicious row might simply be deleted or modified if found
to be in error • In surrnary, the diagnostic approach leaves open the
question of whether the model, the data or both should be modified.
In some instances described later on, one might discover a discrepant
row and decide to retain it, while at the same time having acquired
a more complete understanding of the statistical est5ites relative
to the data.

     1.3 Modeling   Research Aims and   Diagnostics

       We   reiterate here several principal objectives that diagnostics can
serve, from the modeler's perspective, in obtaining a clearer understanding
of regression beyond those obtainable from standard procedures. Some of
these are of recent origin or are relatively neglected and ought to be
more heavily emphasized. The three main modeling goals are detection
of disparate data seents, collinearity, and temporally unstable regression
parameters. It will become clear as this paper proceeds that overlaps
exist among detection procedures,

      1.3.1   Leverage and   Disparate Data

       The first goal is the detection of data points that have disproportionate
weight,     either because error distributions are    poorly behaved or   because

the explanatory variables have (multivariate) outliers. In either case

regression    statistics, coefficients in particular,    may be   heavily dependent

on   subsets of the data. (This draft is principally concerned with these
                                              —7—
S
    aspects of diagnosis; the other topics are of equal importance. At this

    stage   of our research we are coming to a better understanding of the
    scope of regression diagnostics and        we   shall rely heavily on the work
    of others in describing these other        methods.)


          1.3.2 Collinearity

          While   exact linear dependencies are rare among explanatory variables
    apart from incorrect problem formulation, the occurance of near dependencies
    arises (all too)     frequently in practice. While some collinearity can be

    moderated by appropriate rescaling, in many         instances    ill-conditioning

    remains. There are two separate issues, diagnosis and             treatment.   Since,

    our   main purpose is   diagnosis, we are not presently concerned with what

    to   do about it, except to note that the       more   collinear the data, the
    more prior information needs to be incorporated.
          Collinearity diagnosis is experimental too, but the most satisfactory
    treabnent   we   know of has   been   proposed by David Beisley [2],    who builds

    on earlier work of Silvey [3].       By exploiting a technique of numerical
    analysts   called the singular    value decomposition, it is possible to
    obtain an   index of ill-conditioning and relate this to a decomposition
    of the estimated coefficient variances. This relation enables the
    investigator to locate which columns of the explanatory variable matrix,
    associated with the index of collinearity, contribute strongly to each
    coefficient variance. By thus joining Silvey's decomposition of the
    covariance matrix to numerical measures of ill-conditioning, economists
    now have an experimental diagncstic tool that          enables   an assessment of which
    columns of the data matrix are prime sources of degradation in estimated
    coefficient variances.
                                          —8—




   1.3.3    Regression Parameter    Variability in Time
                                                                                              S
    A  third major goal is the detection of systematic parameter variation
in time. Many statistical models assume that there exist constant but
unobservable   parameters to be estimated. In practice, econometricians

often find this assumption invalid, Suspicions that there are more than

one set of population parameters can be aroused for a large number of

reasons: the occurance of an external shock that might be expected to

modify behavior significantly (a war, hyperinflation, price-wage controls,

etc.) is one possibility. Another is that a poorly specified relation might

exclude important variables which change abruptly. There is always the

possibility that aggregation weights [] may       change   over time and   thereby
introduce variability in macro     parameters   even when micro   parameters   are stahl e.


An argument has been made by Lucas [23] that anticipated changes in government

policy   will cause modifications in underlying behavior. Finally the parameters

may follow a random process and thus be inherently variable. When discrete

changes in parameters are suspected, and the sub-divisions of data where this

occurs is identifiable from outside information, the analysis of covariance in

the form discussed in Gregory Chow [5] or Franklin Fisher [6] is an appropriate

diagnostic that has been   frequently applied. When the break point         of points have

to be estimated, maximum likelihood estimators proposed by Quandt and Goldfeld

 .7][8] are available.

        An alternative diagnostic   procedure has   recently been suggested by

 Brown, Durhin and Fvans [9]. They have designed two test statistics with

 a time series orientation.     From a regression formed by cumulatively adding
 new observations to an initial     subset   of the data, one-step ahead
 predictions are   generated.   Both the associated cumulated recursive lebidual s
                                          —9—




and their stuns of squares have well-behaved distributions on the null
hypothesis of parameter constancy.


 1.      Notation

      We use the following notation:



        Population Pegression                                  Estimated Regression
                                                               YX+r
      Y : nxl column     vector   for dependent variable       same

      X : nxp matrix of explanatory variables                  same

           pxl   cohutu vector of regression coefficients        : estte of
           nxl column error vector                             r : residual vector


                                                           I
      Additional notation

                 row   of   X matrix                           same

            error   variance                               I
                                                               s2 estinated error variance

                                                           s              estimated with
                                                           i   row   of   data matrix   and
                                                               Y vector deleted.



Other   notation    is either obvious or will be introduced in a specific

context not so obviously tied to the generic regression model.
                                                                                                 I
                                           — 10 —




2.    LEVERAGE POINTS AND DISPARATh DATA


     2,1 Introduction

       At this stage in the development of diagnostic regression procedures,

we   turn   to   analysis of the structure of the X matrix through     perturbation of     its
rows. In the usual case, the X's are assumed to           be a matrix of fixed numbers
and the matrix to have full column rank. Otherwise, statistical theory

suggests we ought to have little interest in the X matrix, except when

exper:imental design considerations enter. In actual practice, researchers

pay   a great deal of attention to explanatory variables, especially in          initial

investigatory      stages.   Even when data are experimentally generated,

peculiarities in the data can impact subsequent analysis, but when data

are non-experimental, the possibilities for unusual data to influence

estimation is typically greater,

       To be more precise, one is often concerned that subsets of the data,

i.e., one or more rows of the X matrix and associated Y's might have a

disproportionate influence on the estimated parameters or predictions.

If, for example, the task at hand is estimating the mean arid standard

deviation    of a univariate distribution,      exploration of the   data will
often   reveal outliers, skewness or multimodal distributions. Any one of

these   might cast suspicion on the data or the appropriateness of the
mean and    standard   deviation as measures of location and variability.
The original model may also be      questioned and transformations     of the
original    data consistent with an alternative model may be suggested, for

instance. In the more        complicated   multiple regression context, it is corrTnon

practice to look at the univariate distribution of each column of X as well

as Y, to see if any oddities (outliers or           gaps) strike the eye. Scatter
                                      — 11   -


diagrams   are also examined. While there are clear benefits from sorting
out peculiar observations in this way, diagnostics of this type cannot
detect multivariate discrepant observations. That wea)-iess is what we
hope to remedy.
     The benefits from isolating sub-sets of the data that might disproportion-
ately impact the estimated parameters are clear, but the sources of
discrepancy are   diverse.   First, there is the inevitable occurance of
improperly recorded data, either at the source or in transcription to
computer readable form, Second, observational errors are often inherent
in the data. While more appropriate estimation procedures than least squares
ought to be used, the diagnostics we propose below may reveal the unsuspected

existance or severity of observational errors. Third, outlying data points

may contain valuable information that will improve estimation efficiency.

We all seek the "crucial experiment", which may provide indispensible

information and its counterpart can be incorporated in non-experimental

data. Even in this situation, however, it is     constructive to isolate
extreme   points that indicate how much the parameter estimates lean on these

desirable data. Fourth, patterns may emerge from the data that lead to

a reconsideration and alteration of the initial model in lieu of suppressing

or modifying the anomolous data.

     Before describing multivaria-te diagnostics, a brief two dimensional

graphic preview will indicate what sort of interesting situations might

be subject to detection. We begin by an examination of Figure 1,    which
portrays the ideal null case of uniformly distributed and, to avoid statistical

connotations, what might be called evenly distributed X. If the variance of

X is small, estimates of will be unreliable, hut in these circumstances
                                                                                  .
                                     — 12   —




standard   test statistics contain the necessary information,
     In Figure 2, the point o is anomolous, but since it occurs near the
mean of X, no adverse leverage effects are inflicted on the slope estimate

although the intercept will be affected. The source of this discrepant

observation might be in X, Y or c, If the latter, it could be indicative

of heteroscedas-ticity or thick-tailed error distributions; clearly more

such points are needed to analyze those problems further, but isolating

the single point is constructive.

     Figure 3 illustrates an instance of leverage where a gap arises

between the main body of data and the outlier, While it constitutes a

disproportionate amount of weight in the determination of ,     it might
be that benign third source of leverage mentioned above which supplies

crucially useful information. Figure        is a more troublesome configuration

that can arise in practice. In this situation the estted regression

slope is almost wholly determined by the extreme point. In its absence,

the slope might be almost any-thing. Unless the extreme point is a crucial

and valid piece of evidence (which of course depends on the research

context), the researcher is likely to be highly suspicious of the estimate.

Given the gap and configuration of the main body of data, the estiiite

surely has less than n-2 degrees of freedom; in fact it might appear that

there are effectively two data points altogether, not n.

     Finally, the leverage displayed in Figure 5 is a potential source of

concern since o and/or • will heavily influence      but differently than the

remaining data. Here is a case where deletion of data, perhaps less

drastic downweighting, or model rformu1 ation is clearly    indicated.
                                        — 13   —




                     Plots for Alternative Configurations of 1)ta




yi
                                                                      0




                           x.1                                            x        x1
          Figure 1                                                  Figure 2


y-.                       0                           yi                       0

      /


                           x.1                                                     x.1
      Figure 3                                                      Figure 4

y.
                     0




                            x.1
      Figure    5
                                       — lL


                                                                                     .
                                              —




  2.2   Residual Diaiostics

     Traditionally the examination of functions of the residuals,

     y. -   y., and especially   large residuals, has   been   used to provide

indications of suspect data that     in turn may unduly affect     regression
results.    It is best to have a scalar covariance matrix, so that
detection of heteroscedasticity or autocorrelation (and later on, eliminating
them) is desirable.
     Approximate normality is another desirable property in terms of estimation
efficiency and the ability to test hypotheses. Harmful departures from nonility
include pronounced skewness, multiple modes arid thick-tailed error distributions.
Even moderate departures from normality can noticeably impair estimation
efficiency. At the same time, large outliers in error space will often be
associated with modest—sized residuals in least squares estimates since the
squared error criterion heavily weights extreme values.
     It will often be difficult in practice to distinguish between
heteroscedasticity and thick—tailed error distributions; to observe the
former, a number of dependent variable values must be associated with
(at least) several given configurations of explanatory variables. Otherwise,
a few large residual outliers could have been generated by a thick-tailed
error distribution or franents from a heteroscedastic distribution.
     Relevant diagnostics have three aspects, two of which examine the
residuals and the third involving a change in error distribution assumptions.
The first is simply a frequency distribution of the residuals. If there
is evident visual skewness, multiple rrdes or a heavy tailed distribution,
the graph will prove infonmative. It is interesting to note that econaiiists
often look at time plots of residuals but seldczn at their frequency distrikution.
                                                -15-




        The second is the normal probability plot, which displays the cumula-

tive normal distribution as a straight line whose slope measures the standard

deviation    arid whose intercept reflects the mean. Thus departures from

normality of the cumulative residual plot will show up in noticeable departures

from a straight line. Outliers will appear inunediately at either end of              the

cumulative distribution.

        Finally, Denby and    Mallows    [17] and   Welsch   [18] have suggested plotting
the estimated coefficients and          residuals   as the error
                                                               or, equivalently,
                                                                   density

as the loss function (negative logarithm of the density) is changed. One
family of loss functions       has    been suggested by Huber [1];

                       It?                  ItIc
             p(t)      S
                                _c2         ItI>c


which goes from least-squares (cx) to least absolute residuals (c0). This

approach is attractive because of its relation to robust estimation [1], but

requires considerable computation.

        For diagnostic use the residuals can be modified in ways that will

enhance our ability to detect problem data. We first               note that the
do not have equal variances because if we let H X(XTX)1XT, then

                    E[(YY)(YY)T] E[(IH)YYT(I_H)T]


                     (I-H) E(YYT)(I_H)         a2(I-H)


since (I-H)2        I—H and (I-H)X          0. (See Theil [10] and Hoaglin and Welsch [13]

for a more detailed discussion.) Thus

                                        2
                        var   (r.)          (ib)                                   (2.2.1)



where    h1 is the       diagonal element of H.
                                                -16-




        Consequently a number of authors 111] have suggested that instead
                                                                                                 .
 of   studying r., we should use the standardized residuals


                                       r./s                                            (2.2.2)


where S2      is    the estimated error variance.

        For diagnostic purposes we might want to go further             and ask
about the size of the residual corresponding to y when data point i has
been omitted from the fit, since this corresponds to a simple
pertutation of the data. That is, we base the fit on the remaining
n—i data points and then predict the value for y1. This residual is

                                     yl -   x
and   has been studied in a different context by Allen [12].              Similarly

s2.
 (1)is the estimated error variance for the "not i" fit, and the
                                              _______________________
standard deviation of                 is estimated by 5(i)/i + x1(X1)X(1))x
We now define the studentized residual:




                    r
                                -
                               (i)
                                     I+X            -
                                            i (1) (1)
                                                        -l
                                                             i
                                                                    .                 (2.2.4)




Since the numerator and denominator in (2.2.4) are independent,


r1
      has a     t   distribution       with n-p--l degreees of freedom. Thus

we can   readily     assess the significance of any single       studentized   residual.

(Of   course,       r   arid
                                 r will not     be independent.) Perhaps even more

useful for our purposes is the fact that



                                                                                      (2.2.5)
                                     r1/(s(1)v'1_h1)
                                                   -17—




 and
                                   2                             1
                           (fl—P—l)S()
                                               (n—p)s —
                                                         2
                                                              I—F;:
                                                                                           226

 These results are proved        easily     by   using the matrix     identities   in Appendix 1.

        Tberefore we think that a good           way to examine residuals is
to   look at the studentized residuals, both because they have equal

variances and because they are             easily      related to the t-distribution.

However this does not tell the whole story, since some of the most

influential data points can have relatively small studentized residuals

(and very small r1).

       To illustrate with the simplest case, regression through                the origin, we
have
                          r.1         x.
                                                 x.1
                                                         1
                                                                                         (2.2.7)




                            - (i)           xr/Z,1 ?                                     (2.2.8)
                                                  J


where   (1) denotes an estimate obtained by removing the                    row
(data point)     from   the computation. Thus the residuals are             related   to the
change in the least-square estimate caused by deleting one row. But each contains
different information since large values of J -        can be associated
with small fr.          arid vice   versa. Therefore we are lead to consider row
deletion as an important        diagnostic       tool, to be treated on at least an
equal footing with the analysis of residuals.
       For   multivariate linear regression (2.2.8) becomes


                            -
                                (i)        (XTX)_l xTr./(l_h.)                           (2.2.9)
                                          — 18   —




                                                                                                 S
where      the h. are the diagonal elements of H, the least-squares
proj ect ion     matrix defined   earlier. We will call this the "hat"       matrix    since
                                  1
                         HY       Y                                                   (2.2.10)



Clearly     the hat matrix plays a crucial role not only in the studentized

residuals but also in row deletion and other diagnostic tools. We now develop some

inportant resUlts (based on the discussion in Hoaglin and Welsch [13]) relating to

this    matrix.
     2.3 The Hat Matrix

        Geometrically    Y is the projection of Y onto the p-dimensional
subspace of n-space spanned by the columns of X. The element h.. of H
                                                                            1]
has    a direct interpretation as the amount         of   leverage or influence exerted
            by y. Thus a look at the hat matrix            can reveal sensitive points
in the      X space, points at which the value of y has a large impact
on    the fit.
       The influence of the response value y on the fit is most directly

reflected in its leverage on the corresponding fitted value              y, and
this is precisely the infonnation contained in h1, the corresponding

diagonal     element of the hat matrix. When there are two or fewer explaiiatory

variables scatter plots will quickly reveal any x-outliers, arid it              is
not hard to verify that they have relatively large h values. When

p >   2,   scatter plots may not reveal "multivariate outliers," which are

separated in p-space from the hulk of the x—points but do not appear as

outliers in a plot of any single explanatory variable or pair of them
                                                    — 19     -


    yet will be revealed by an examination of H . Looking at the diagonal
    elements of H is not absolutely conclusive but provides a basic starting
    point. Even if there were no hidden multivariate outliers, computing
    and examining        H (especially the h) is usually less trouble than

    looking      at all possible scatter plots.

            As   a projection matrix, H is syiiunetric and idempotent (H2                        H).
    Thus we can write


                               '
                              h..        2
                                      E h..         h.ii +         h?.                                 (2.3.1)

    and it    is clear   that 0      h..
                                       11 1. These limits are                  useful    in

    understanding      and iterpreting
                                             hj(Eh1),        but they do riot yet tell us

    when h1 is "large". It is easy to show, however, that                          the   eigenvalues
    of a proj ection matrix are           either   0 or 1    and that the      ninber    of   non-zero

    elgenvalues is equal to the rank           of   the matrix.          In   this case rank (H)

    rank (X)        p and hence trace H p, that              is,
                               n
                               E h.                                                                 (2.3.2)
                              1=1


    The average size of a diagonal element, then, is p/n. If we were designing

    an   experiment a desirable goal would be to have all the data points be about
    equally      influential or all h1 nearly        equal. Since the X data is given
    to us   and we cannot     design our experiment to             keep the h1    equal,      we will follow [13]
    and   say that   h1 is   a leverage point       if h1    >   2pm.     We shall see later that

    leverage     points can he     both   harmful   and helpful.
S
        The quantity 2pm      has
                                                       —20—




                                      worked well in practice and
 theoretical justification for its use. When the explanatory variables are
                                                                          there   is some
                                                                                                        .
 multivariate Gaussian it is possible to canpute the exact distribution of

 certain     functions of the h. Let )( denote the nx(p_l) matrix obtained by
 centering the explanatory variables.                    Now


                          Y-Hy_Vy                                                           (2.3.3)

and   thus the diagonal elements of the centered hat matrix are

                                  h. —            .
                                                                                            (2.3.1-i)

Let          denote X with the i
                                      th row          removed and X(.) denote the centered
      X(i)
version of X
                (1)
                     i.e. means based on
                      ,                                all but the 1th observation have been

subtracted     out. Finally note that
                              —     n-l        —
                          x—x       —h-—   (xj—x(i))                                        (2.3.5)

and

                                       —
                                           (x(.)—x1).                                       (2.3.6)


Using (Al.l ) and (2.3.5)

                          h.1 l+y
              n-i2        —                -          --1           T
where y       (—)  (x.—x(.))(X(.)          X())             (x1—x(.))


Again   using (Al.l ) and (2.3.6)

                          -   (fll)
                                n
                                            cY.



                                           1÷(n—l)
                                                2
                                              n
                                                       —21-



    where


                            a
                                                                  -l       —        T
                                     (x1-x(1)) (X(.)      X(.))        (X..-X(.))

    The distribution of (n-2)a is well known                  since    it is the Mahalanobis

 distance between observation i and the mean of the remaining observations
    [19,   p.   '480].   Thus


                                     n (p-i)
                                                   F              .                         (2.3.7)
                                    (n-U(n-p)       p—l,n-p

 Reversing the above algebraic manipulations we obtain

                                               a
                            1         n
                                          L-
and

                             -       (n—i)cx + 1
                           hi-(1)
Solving for a gives


                                     n (h.-1/n
                           anl \. i-h.
and    from (2.3.7)

                           h.-l/n
                            1
                            1-h1           n
                                                   a EL
                                                      n-p
                                                              F
                                                                  p-i,n-p                  (2.3.8)

           For nDderate p and moderate n the              95% point     for F is near 2. Therefore,
a   cut-off point        would be

                                     2(p—l)+ n E
                                >
                          h1           n+p-2                                               (2.3.9)

which is approximated by 2pm.
       From   equation
                                           —22—




                         (2.3.1) we can see that        whenever h 0 or 1,
                                                                                                 .
we have    h..    0 for all j     i. These two extreme cases can be           interpreted
as   follows. If h 0, then y. must            be   fixed at zero   -   it is not affected
by y or by any     other        A point   with     x.     0 when the model is a

straight  line through the origin provides a simple example.
      When
           h1    1, we have ;       - the model always fits this              data
                                    y1
value exactly. This is equivalent to saying that, in some coordinate

system,    one parameter is determined completely          by y or, in effect, dedicated
to   one data point. The following theorems are proved in appendix 3.

Theorem:       If h1 1, there exists a nonsingular transformation, T
such that the least-squares estimates of a               T
                                                 have the following
properties: a1 y and            {a}2     do not depend on y1.




Theorem:          If X is nonsingular,         then



                  det(XT. X . )
                       Ci) (i)            i
                                    (1-h.) det(XTX)           .                       (2.3.10)


Clearly when h 1 the new matrix X(1) formed by deleting a row is                     singular

and we cannot obtain the usual least-squares estimates. This is                extreme

leverage and does not often occur in          practice.




                                                                                                 .
                                                   —23—




       To complete our    discussion         of the hat     matrix we   give a few simple

examples. For the sample mean all elements of H are                         1/n   .   Here

p = 1   and each h = p/n, the perfectly balanced case.
       For a straight line through the origin

                          h..       x.x.I     E                                                   (2.3.11)

                  n
and clearly E h1 p 1.
                 i=l

       Simple    linear regression is slightly more complicated but a few

steps of algebra give


                                             (x.   - )(x. -
                                =    +
                                                                                                  (2.3.12)
                                               E (Xk -
                                             k  1

       n
and         h.    2. We can see from (2.3.12) how x-values far fran                    x   will
      i=l
lead   to large values of h. It is this idea                 in the    multivariate        case
that we attempt to capture by looking               at elements of the hat matrix.


  2.' Row Deletion Diagnostics

       We now    return to the basic formula

                           -             (XX) x           r1/(l-h1).                              (2..l)



Since the variability     of                                   T
                                    is measured by s((X X))             ,   a more useful measure
ofchangeis
                                                    ci) )
                        DFBFrAS..        =
                                                      / -1
                                                  s.vxTx)
                                                                                                  (2..2)
                                                   (i)        jj
where   we have replaced s by   S() in order to make the denominator stochastically
independent of the numerator in the Gaussian case. To provide a
surrmary of the relative coefficient changes we suggest
                                                                                         .
                    NDFBTS1 \/                      jl DFBrAS .            (2..3)
The term        has been incorporated to make NDFBAS more comparable across
data sets which may   have   different values of p and n. This normalizing
value   was chosen because   when X is an orthogonal matrix (but not necessarily




                                                                                         .
orthonormal)
                                      x..
                                       1-j
                                              r.1
                   DFBETAS.. z

                                     tl x2. /1-h.
and

                                             h.
                                          1
                       DFBFTAS.
                                 ij
                                                      r.1
                                        1-h1

Since the average value of h1          p/n, a rough average value for h1/(l-h1)

is p/(n-p). Clearly (2.L.3) could be modified to reflect the fact that

some coefficients may be more important than others to the model builder

(e.g., including only the main        estimates         of interest).

      Another obvious row deletion diagnostic is the change in fit




If we
                   DFFIT.     x. (- (.))


        scale this by dividing by s(1)Awe have

                    _
                    1h1
                    ______
                    Il_hi
                               r.1
                                                  =
                                                       1-h1   r1   •       (2   .




                                                                           (2.4.5)
                                                                                    .)

                                                                                         .
                                                 —25—




 For across data set nonializatjon
                                             we will multiply by v'P/p to obtain



                                                             2
                                 1
                                          p 1-h1
                                         /('2E)( h.)                                   (2.4.6)

A measure similar        to   this has   been   suggested by Cook [14].
        Clearly DFFITS arid             in an orthogonal coordinate system.
                                NDFBETAS agree

When orthogonali-ty does not hold these two measures provide somewhat different
information. Since we tend to emphasize coefficients,our preference is for
NDFBETAS.
                                                                                  -
        Deciding   when a difference like J        (-            J or   other diagnostic

statistic is large will depend, in part, on how this information is being

used. For example, large changes in coefficients that are not of particular

interest might not overly upset the model builder while a change in an

important coefficient may cause considerable concern even though the change

is    small   relative   to traditional estimation error.
       We have used two       approaches   to measure the size of changes caused by
row deletion.  The first, called external comparison, generally uses measures
associated with the quantity whose changes are being studied. For example,
the standard error of a particular coefficient                   would be used with
(-
       The
        second method, called internal comparison, treats each set of
diagnostic values (e.g.,    -             as a single data series
and   then finds, for example, the standard deviation of this series as

a measure of relative size. As we have noted, all of
                                                                    the diagnostic measures
we have discussed so far               functions of r./v'l-h.
                                                            1arid
                                 are                                    in   view of our   discussion
                                                     1
of   studentized residuals, it is natural to divide this                          to achieve   a
                                                                        by 5(i)
reasonable scaling before making plots,             etc.
                                                        —26—


                                                                                                     .
     Once 5(.) has been used, the temptation arises to try to perform formal
                                                                         *
statistical tests because we 1iow the distribution of r.                            In our opinion

this is not a very promising procedure because it puts too much emphasis

on residuals (although looking at studentized residuals is better than

using the raw residuals). We prefer to use external or internal

comparison     to make   decisions about which              data points    deserve further

attention     except, of course, when we        specifically at the
                                                        are looking

studentized residuals as we did earlier. Using any Gaussian distributional
theory      depends on the appropriateness of the Gaussian error distribution -

d tcJ:o     we will return to       atei
  2. 5 ReessionStatiics

       Most   users of statistics realize that                 estiirtes   like         should
have   some   measure of variability           associated      with them.     It   is less
often realized that regression statistics like t,                            and    F    should
also   he   thought of as having a variability associated with them.
       One way to assess this variability is to examine the effects of row

deletion on these regression statistics. We have focused on three:



              ATSTAT.           1          -        /I
                            s.e.() s.e.((.)).



              AFSTAT        F(afl              0)   -
                                                                      0)
                                                        F(.)(ali


                                                                                                     •
                                        —27—




Again W(:   should   ask when a difference is large enough   to merit   attention.

For ex-ten-ial comparison we would compare to the standard deviation of
t, F, or R2:
                 Statistic                     Standard Deviation


                      t                              n-p     1/2
                                                    n-p-2



                      F                           (2(fl_p)2fl__\1/2
                                                  p(n_p_2)(n_p_L)J

                                                               2        1/2
                      R2                           —   (p—i)
                                                   (p+n-2)2(p+n-1)


However, we tend to view internal comparison as more appropriate for

regression statistics.

       Studying the changes in regression statistics is a good second order

diagnostic tool because if a row appears to he overly influential on

other grounds, an     examination of the regression statistics will show
if   the conclusions of hypothesis testing would be affected.

      There is, of course, room for misuse of this procedure. Data points

could he removed solely on the basis of their ability (when removed) to

increase F2 or some other measure. While this danger exists we feel

that it is often offset by the ability to study changes in regression statistics

caused by row deletion. Again we want to emphasize that changes in

regression statistics should not be used as aprinary diagnostic tool.
                                            —28—




  2.6
                                                                                         S
         Influence and Variance Decomposition

       We now would like    to consider perturbing      our assumptions   in a new
way.    Consider the   standard regression model (1.4) but with var(c)
replaced by a2/w1 for just the i data point. In words, we are

perturhing the homoscedasticity assumption for this one data point.

       In appendix 2 we show that


                                   TX)
                                     -iT
                                       x.r.
                   ___          CX
                                        11
                    w.
                       1        (l-(l-w.)h.)

and it follows that


                       W.              T   1T
                                      CX X) x.r                                (2.6.2)
                               1




                   ____
                   w.1
                                      (X
                                        T
                                           X) x.r. ii
                                                   2
                                                            Ci)
                                                          1-h.
                                                                               (2.6.3)
                            w.0
                             1          (1—h.)
                                              1              1



Equation   (2.6.2) tells us about infinitesimal changes in caused by small

changes in w1 about the value 1 and similarly for (2.6.3). From the mean value

theorem we know that




                                                                               (2.6.4)



where 5 is between 0 and 1. Any one of (2.6.2), (2.6.3) or (2.6.4) can be used

for diagnostic purposes. We have chosen to emphasize                  because of its

intuitive appeal and the fact that it is a compromise between (2.6.2) and (2.6.3).
                                                   —29—



Formula (2.6.2) can     also   be considered as a function which represents the

influence   of the 1th data point and can be linked to the                           theory of robust

estimation [15] and the jackknife [16].
       If we let

                    s2                                                                          (2.6.5)

and
                               1

                    w                                                                           (2.6.6)
                                      .


                                          •l

then   in appendix 2 we show that


                    .2__
                           r sL
                           I
                                     m 11
                                   (XWX)
                    w.1 L w1                                      -
                                                               w.-1
                                                                1


                    r.2
                    —      (X X)
                                T1   —         s
                                                   2
                                                           Cx x)
                                                                     1    T    T
                                                                                     .          (2.6.7)
                                                                         x.x1(X X)




Since   we would like to remove scale we define


                                    r                      -   [xTx' 4x1(xTx)_h]jj              (2.6.8)
                    DBVARS1                            2
                                     (n-p)s                                CX X)•
                                                                                JJ


as the scaled infinitesimal change in the variance of j. As a suniiiaxy measure
over   all of the coefficients we use

                                          p
                    NDBVARS.
                         1
                             a •z
                                   pjl                 BVARS.. .
                                                                1]
                                                                                                (2.6.9)

where the n/p term is used to improve comparability across data sets.
                                                   —30--


                                                                                                            .
        If we used row deletion instead of derivatives, our basic measure
would   be

                                         2 T' 2
                                         s (X X).. — s (i)
                                                 jj
                                                        . CX
                                                            T x )..
                                                             (i) Ci)       .       .
                                                                                       -l
                       DFBVARS..                                                                 (2.6.10)
                                                        2 (XTX)T
                                                                     JJ



with suuniary measure
                                         (ri p,
                       NDFBVARS.
                                 1
                                                  jlE      I
                                                               DFBVARS..
                                                                          13
                                                                               .                 (2.6.11)

       The. measures   so far   discussed        in this section include both the explanatory

variables and    the   response. If we wish to examine the X-matrix only, the second,

part   of   (2.6.8) provides a good way to do this. We notice that

                        r   T                T     -l                T -1
                        E (X X) x.x.(X x)                        (x X)
                       j 1

and   define
                                         r        -l                 -i1

                       BETAVRD..
                                         L xTx)     xx. (XTX)
                                                        11            J 33
                                13
                                                 (XTX).

with    suinaxy measure

                                             p
                       NBErAVRD.     =       E    BETAVPD..
                                 1                              13


These measures provide a way to decompose the cross products matrix with respect
to the individual observations.
       Again it is useful to look at the                orthogonal X case. When             orthogonality
holds
                                                   —31—




                                                   2
                                                  x..
                                                   11 —
                        BETAVRD..
                                 1]
                                            jj
 arid

                       NBFTAVRD.
                                    1       h.1
 Since h has a strong intuitive appeal it may be a better suJrniry value even
 when orthogonality does not hold. We have chosen not to multiply NBFTAVRD
 by n/p (the average value for hi), so it is not useful across data sets.

         If we examine the formula for DFBVARS we see that             this    quality could
 be positive or negative. As we might expect, in some cases downweighting a

 da.ta point can    inprove   our   estimate       of the variance of a coefficient. (Down—

 weighting     corresponds to placing a minus sign in            front of DFBVARS.) One of
 the    best ways to examine the tradeoffs of DFBETAS arid DFBVARS (or                BFTAVRD)

 is to make a scatter plot. A high leverage point with small values of DFBETAS
may be a "good" observation because it is helping to reduce the variance of
certain coefficients. The setting aside of all high leverage points is
generally not an efficient procedure because it fails to take account of the

response data.


2.7 More Than One Row at a Tfrie

        It   is natural to ask   if there might be groups of leverage points              that
we   are failing to diagnose because we are only looking at one                 row   at a time.
There    are easily   constructed examples where          this   can happen.
        One approach is to proceed sequentially - remove              the   "worst" leverage
point    (based perhaps on both NDFBLTAS and NBETAVRD), reexamine the diagnostic
measures arid remove the      next      "worst" observation, etc.      This does not fully
                                                  —32—




cope with   the   problem of groups of leverage points                 and just   as stepwise
regression   can be troublesome, so can sequential row deletion.

     A straightforward induction argument shows that

                            -   h. .(k           . .k
                      6..
                       ij        ij 1    ,k ,.
                                           2           t
                                                           )




                                det (I-H).
                                               1, k
                                                det
                                                               l'2'"t
                                                        (I-H)k k        1




where H is the hat matrix for all of the data, h. (k1,. . .                   ,kt)   denotes the


hat matrix for a regression with rows k1,.. .kt removed and the subscripts on I-H

denote a suiatrix formed by taking those rows and columns of I-H.

     Even though all of these differences are based on H, multiple row deletion

will involve large amounts of computation. It is instructive to note that




                      1 h (k) --
                                         (l-h1)(l-h,)
                                                               -
                                                                   h
                                                      lhk

                                         (1-h1)
                                                           l -(l_(l)]

                                         (1—h1)
                                                           [1 —cor (rj,rk)]



The term cor (rI,rk) also appears when more rows are deleted and, in place of

looking at all possible subsets of rows, an examination of the correlation matrix

of the residuals for large correlations has provided useful clues to groups of

leverage points. This requires )iowing the off-diagonal values of H and -therefore

increases computational cost and perhaps storage requirements.
                                             —33—



2.8 Interface with Robust and        Ridge   Regression

       It is natural to ask how     the   above diagnostics could or should be

used with some of the newer estimation methods like robust arid          ridge regression.
The   first question is whether we should do diagnostics or robust         or ridge

first.    There is no clear answer, but some sort of iterative procedure is
probably called for.
     However, it is possible to perform regression diagnostics after using
either a robust procedure or a ridge procedure. In the robust case we can

make use   of weights

                             p'                                               (2.8.1)

where P is the robust     loss    function, R are the robust estimates of and
SR is a robust estimate of the scale of the residuals, y1-x1                (A canpiete

discussion of weights is contained in [20].) We now imdify the data by forming

a diagonal matrix of weights, W, and using AiY,           Aix. This   revised data is

then the input to regression diagnostics. If the robust estimation procedure

has been   allowed to converge
                     A          T
                              (XWX)       XWY

will   be close to       and our procedures will accurately reflect what would happen
to       locally. Of course they do not reflect what would happen if a data point
were deleted and then robust estimation applied.
       The ridge estimator [21] is given by

                                 (XTXtkI)     xTy   •                          (2.8.2)
                     RD

There are many   generalizations but        most will fit into the following frame-

work. We assume that k has been           chosen by sane means such as   those listed
                                                            —34—




in    [21]. Then we form


                        x
                                       rxlI, Y
                                       I
                                                              A    :j
                         A
                                       Iv•i                         LP
                                       L        P<PJ



where        is a pxl vector of zeros (prior values tines v                        in   more general cases).

So we now have "new" data XA and
                                                    A with         nxp rows. Clearly

                                                         T
                                  -
                                  -        (T
                                            XA XA       XA A •                                 (2.8.3)

                                                                                 When we delete a
We now perform regression diagnostics using XA and
                                                                            A
row   with   index n+j >     n,    it is equivalent to saying we do not want                 to "shrink"
that parameter estimate            zero (or its prior). In the Bayesian context
                                   toward

dropping     such a row is like setting the prior precision of to zero.
Plots of DFBETAS would then show the effects of such a process by looking
at those DFBFTAS values for index greater than n.
       We can do sane diagnostics to decide if a ridge estimator is warTanted.
If we differentiate (2.8.2) with respect to k, then

                        3RD                 T          -1
                         3k                (X X +kI)                                           (2.8.4)
and
                        RD                      T      -1
                         3k                   (X X)                                            (2.8.5)
                                   o


Thus (2.8.5) provides infoniation about infinitesimal charges                            about kO.
If   xTx   were diagonal then              (2.8.5) has       ccmponents j/X where A. are the
elgenvalues.       So        large         and/or A small would lead to a large value of
the   derivative. Since the ridge estimator depends heavily on the scaling

of   the   explanatory   variables,             so does (2.8. Li.) and     we recorimend scaling before
using this     diagnostic measure.
                                    —35—




     When diagnostics have been canpleted a few observations ny be suspect.
The rows can then be set aside and a new robust or ridge estimate caiiputed.
Diagnostics can -then be applied again. There are obvious limits of time and
money but we think that to passes through this process will often be r'th-
while.
                                            —36—




  2.9 An Example: An Inter-Country Life Cycle Savings Function

  Arlie Sterling of NIT has made available to us. data he has
collected on fifty countries in order to undertake a cross-sectional
study of the life cycle saving hypothesis. The savings ratio
(aggregate personal saving divided by disposal incane) is explained
by per   capita disposable income, the percentage rate of change in per

capita disposable income and     two population    variables: per cent       less
that   15 years old and per cent over 75 years old. The data are averaged

over the decade 1960-1970 to remove the business cycle or other short-term

fluctuations.

       According to the life cycle hypothesis, savings rates should be

negatively affected if non-members of the labor force constitute a large

part of the population. Income is not expected to be important since

age distribution and the rate of income growth constitute the core of

life cycle savings behavior. The regression equation and variable

definitions are then:



             cOEF.1 +                   +                   + COEF.4* INC.
       SR1              COEP.2*POP151       COEF.3*P0P751


                   +
                        COEF.5INGRO1                                                (2.9.1)




                                                                                              .
                                              —37—



        SR1              the average aggregate personal savings rate in
                         country i from 1960-1970
        POP15.       =   the average % of the population under 15 years
                         of age from 1960-1970
                     =   the average % of the population over 75
        P0P751                                                            years
                         of age   from 1960-1970

        INC1             the average level of real per capita disposable
                         income in country I from 1960-1970 measured in
                         U.S. dollars
        INCROi           the average % growth rate     of INC.   from
                         1960—1970.


    A full list of countries, together with their numerical designation,
appears in Exhibit 1, and the data are in Exhibit 2. It is evident that
a wide geographic area and span of economic developnent                 are included. It is
also    plausible to suppose that the quality         of   the underlying   data    is

highly    variable. With these obvious        caveats, the LS estimates of (2.9.1)
are    shown in Exhibit 3.        To coliment briefly, the R2 is not uncharacteristically

low    for   cross-sections, the population variables have correct negative signs -

COEF 3 has     a   small t statistic but COEF 2 does not - income           is    statistically
insignificant, while income growth     reflected in COEF 5 is significant                  at
the    5 per cent level arid has a positive influence on the savings rate
as it should. Broadly speaking, these results are                consistent with the
life   cycle hypothesis.
       The remainder of this section will be a guided tour through some
of the diagnostics       discussed    previously. The computations were performed

using   SENSSYS (acronym for sensitivity system), a ThOLL experimental                   subsystem
for regression diagnostics. Orthogonal             decompositions are used        in the

least-squares      regression computations and this makes it possible to             get all
of the diagnostic measures in addition to the usual LS results in less than
twice the computer time for the LS results alone.
                                               —38—




David Jones and Steve Peters of the NBER Computer Research Center                    have

programmed SENSSYS.      Both have actively participated in analytical and
empirical    aspects of the research.
       Only a   selection of plots and diagnostics          will be shown for two       reasons.

One is that to provide the full battery of plots would be excessively tedious;

however, the missing plots and tables are readily                obtainable. The other
reason   is that we found these diagnostics to be among the ircre instructive

from examination     of this and       several other problems.


     2.9.1   Residuals
    The first plot, Exhibit      '+,    is   a normal probability plot.       Departure from
a   fitted line (which represents a particular Gaussian distribution with mean

equal to the intercept and standard deviation equal to the slope) is not sub-

stantial in the main body of the data for these studentized residuals, but

Zambia (46) is an extreme residual which departs fran the line. Different

information, an index plot of the r1, appears in Exhibit 5 which reveals not

only Zambia, but possibly Chile (7) as well to be an outlier; each exceeds

2.5 tines the standard error.


     2.9.2   Leverage   and Diagonal     Hat    Matrix Entries

     Exhibit 6   plots the h which, as diagnonals           of    the hat matrix, are indicative
of leverage points.       Most   of the h are         small, but   two   stand out   sharply:    Libya

(49) and the United States (44). Two others, Japan (23) arid Ireland (21) exceed

the 2pm         .20 criterion (which happens to be equal to the 95% significance level

based on the F distribution), but just barely. Deciding whether or not leverage

is potentially detrinental       depends on what        happens elsewhere in the diagnostic

analysis, although it      should be recalled that it is values near unity                  that pose

the most severe problems, which has             not   happened   here.
                                                        —39—




       2.9.3 Coefficient              Perturbation
      An overview        of        the effects   of individual row deletion          (see Exhibit   7)
is    based on    (2   Li..   3)   NDFBETAS, the square root of the scaled sum of the squared
differences between the full data set and row deleted coefficients. The measure
used is scaled approximately as the t distribution so that values greater than 2
are a potential source of concern. T countries that also showed up as possible
high leverage candidates, Libya (49) and Japan (23), also seem to have a heavy
influence   on the coefficients while Ireland                   (21), a marginal high leverage candidate,
is also a marginal                 candidate   for   influencing coefficient behavior. Individual plots

of DFBEI'AS      (2 . L1. 2) follow next, fran which the following table has been constncted
based on an examination of Exhibits                         8-11.

                 Noticeably Large Effects on                        from Row   Deletion

       Population <15                  Population >75                Income          Incne   Growth

       Japan   (23)                    Ireland       (21)                            Libya (49)
                                       Japan    (23)                                 Japan (23)


      The countries that              stand    out in   the     individual coefficients are perhaps,

not   surprisingly, the two that appeared in the overall measure. Ireland, in
addition, appears once. Except on the incane variable, the comparatively large
values are just about one LS standard error for each particular coefficient.

      2.9.4 Variation in Coefficient Standard E'rors
      Exhibit 12 is a surruliary measure of coefficient standard error variations
as   a consequence of row deletions, designated as NDFBVARS in (2.6.9). Since

these standard errors involve both error variance and elements from (XTX) 1,
                                                 0—



 large values indicate simultaneous or individual extremes in residuals or

 multivariate outliers in the X matrix. These quite numerous candidates

 include:
                   Index          Country
                    7              Chile

                   21              Ireland

                   23              Japan

                  37               Southern Rhodesia

                                   United   States

                                   Zambia

                  L9               Libya

     Of   these   seven countries, six       appeared      previously, while the only new

 candidate   is Southern Rhodesia. Libya had both high leverage and large
 coefficient changes, Ireland and            Japan   had   noticeable   coefficient changes,
 while Chile      and Zambia possess large residuals. Thus this particular

 diagnostic may        have   some use as a comprehensive measure.

     Plots   for percent changes in the individual coefficient standard
errors   are shown in Exhibits 13-16. Large individual changes (here taken to

be in excess of 25%) appear for the United States with a 7% change for

the income variable, while the deletion of Libya increases the standard error

for the same variable by nearly 85%.

           2.9.5 Change in Fit
          The standardized change in fit, DFFITS              (2..6), with a row deleted,
while sjmi1ar in algebraic structure to coefficient change, conveys somewhat
different infonition of general interest with specific applications in a tine
series context. DFFITS can be viewed in some theoretical cases as having a
t distribution so that extremes of concern show up for values in excess of 2.                  S
In Exhibit 17 three countries that surfaced previously reappear:
Japan (23), Zambia (6) and Libya (149). When coefficient changes
                                                   —41—



 alone are considered as shown in Exhibit 7, Zambia did not appear, while

 Ireland     (21)    did. Thus somewhat        different   information is contained in each.

       2.9.6    A Provisional Sunmary
       It   is now desinable to bring together the information that has              been
 assembled     thus far,     to    see what   it   all adds up to. One useful   sunnai-y plot
is shown in Exhibit 18, which plots the sunuiary measure of -        NDFBETAS
against the corresponding hat matrix diagonal,
                                                h.
    The first point which emerges is that Japan (23) and Libya (49) have
both high leverage and a significant influence on the estimated parameters.
This is reason enough to view them as serious problems. (After
                                                                the analysis
had reached this point, we
                           were informed by Arlie Sterling that a data error
had been discovered for Japan.                When corrected, he   tells us that   the revised
data   is more similar        to   the majority of countries.      These   diagnostics have
thus "proven        their   worth" in bad     data detection   in a modest way. Second,
Ireland is an in-between      with moderately large leverage and a somewhat
                                    case,

disproportionate impact on the coefficient estimates.
    Third, the United         States    has   high leverage   combined with only meager
differential effect on the estijmted coefficients. Thus leverage in this
instance can be viewed as neutral or beneficial. It is important to note
that not all leverage points cause large changes in .
    Exhibit     19   plots the suninary       of coefficient change, NDFBETAS
                                                                              against
the studen-tized residuals arid        drives home the point that large
                                        visually

residuals do not necessarily coincide with large changes in coefficients; all of
the large changes in coefficients are associated with standardized residuals
less than 2. Thus residual analysis alone is not a sufficient diagnostic tool.
      Another summary plot, that of change in coefficient standard error,
NDFBVARS against leverage as measured by h in Exhibit 20 indicates the
close anticipated association between leverage and estimated parameter
variability. This is clearly shown by the diagonal line composed of (21) Ireland,

(23) Japan, (1-i4) United States and (9)        Libya. But residuals also can      have
a   large arid separate influence, as evidenced by the low leverage, high
standard error changes for (7)       Chile and (46) Zambia.                                H



      A final summary plot, Exhibit 21 of NDFBFI'AS against NDFBVARS, is revealing
in that all of the points noted outside the cutoff points            (3,2)   have been
spotted      in the previous diagnostics as worth another look for one reason or

another. Thus about 15% of the observations have been flagged, not an

excessive fraction for many data sets.


      2.9.7 One Further Step

      Since Libya (L9)    is   clearly an extreme   and probably   deleterious influence

on the original regression, a reasonable next step is to eliminate it to find

out whether its presence has       masked other problems or not. Exhibit 22 plots
the h when Libya (9) has          been   excluded in the data set. There is only one

noticeable difference since Ireland (21), Japan (23) and the United States (t4)

remain   high leverage points. Southeri Rhodesia (37) now appears as a

marginally significant leverage point, whereas it had previously been just

below the cutoff. The only really new fact is that Jamaica ('+7) now appears

as a prominent leverage point.

      Jamaica    has furthermore now become a source of parameter influence which

is perhaps most effectively observed in the recalculation of scaled parameter

changes, NDFBETAS, in Exhibit 23 which reveals Jamaica as the single largest

    source   of overall coefficient variation.
                                           _L1.3_




This illustrates the proposition that perverse extreme points can mask the

impact of     still other perverse points. Yet the original analysis did

contain   most of     the pertinent   information   about exceptional data behavior.

The   correlation matrix of     the residuals discussed in Section 2.7 provided

a clue, since the squared correlation between (47) and (49) was .173,
the hightest value. It is nevertheless a prudent step to reanalyze the               data

with   suspect points removed, to ascertain whether one or more extreme or
suspect data points have obscured or dominated others.

      2.10    Final Comments

      The question naturally    arises as    to whether the approach we have taken

in detection of outliers is more effective than simply examining each
individual column of the data to        look for detached observations. We believe

the answer is yes. Detached outliers           did appear in   column 5 (INGRO) of the

X matrix     for   Libya (49) and Jamaica (47), but     not   elsewhere. Libya, of

course, was "the villain of the piece" in the prior analysis. But leverage

points for minerous other countries were revealed by row deletion diagnostics,

while Jamaica, as matters turned out, was not a particularly troublesome data

point. In addition we discussed how various leverage points affected our

output -     coefficients,   fit, or both. So we conclude at this early stage of

our investigation, that these new procedures have merit in uncovering discrepant
data that is not possible with a high degree of confidence by just looking at

the raw data.
References
 [1]   Huber, P.J., "Robust Regression: Asyrnptotics, Conjectures and Monte Carlo,"
          Annals of Statistics, 1 (1973), pp. 799—821.
 [2] Beisley, David     A.,
                          "Multicoflinearity: Diagnosing its Presence and Assessing
          the Potential Damage it Causes Least-Squares Estimation," Working Paper 154,
          National Bureau of Economic Research, Computer Research Center for Economics
          and Management Science, October 1976, Cambridge, Mass.

 [3]   Silvey,    S.D., "Multicollinearity andImprecise Estimation," Journal         of the
          Royal   Statistical Society, Series B, Vol. 31, 1969, pp. 539—552.

 [4] Theil, H., Linear Aggregation of Economic Relations, North Holland, Amsterdam,
          1954.

 [5] Chow, Gregory C., "Tests of Equality Between Sets of Coefficients in Two Linear
         Regressions," Econometrica, Vol. 28, 1960, pp. 591—605.

 [6] Fisher, Franklin M., "Tests of Equality Between Sets of Coefficients in Two
         Linear Regressions: An Expository Note," Econometrica, Vol. 38, 1970,
         pp. 361—366.

 [7] Goldfeld,     Stephen M. and       Richard   E. Quandt, "The Estination of Structural
          Shifts by Switching Regression," Annals of Economic and Social Measurement,
             2, No. 4, 1973, pp. 475—485.
          Vol.

 [8] Quandt, Richard E., "A New Approach to Estimating Switching Regressions,"
        Journal of the American Statistical Association, Vol. 67, 1972, pp. 306-310.
 [9]   Brown, R.L., J. Durbin and J.M. Evans, "Techniques for Testing the Constancy
          of Regression Relationships," Journal of the Royal Statistical Society,
          Series B, Vol. 37, No. 2, 1975, pp. 149—163.

[10] Theil, H., Principles of Econometrics, John Wiley and Sons, New York, 1971,
         pp. 193—195.

[11] Anscombe, F.J. and Tukey, J.W., "The Examination and Analysis of Residuals,"
         Technometrics, 5 (1963), pp. 141—160.

[12] Allen, David M., "The Relationship Between Variable Selection and Data
        Augmentation and a Method for Prediction," Technanetrics, 16 (1974),            pp.
          125—127.

[13]   Hoaglin,   D.C. and    Welsch,   R.E., "The Hat   Matrix in Regression and Anova,"
          Memorandnii N5- 3'41, Department of       Statistics, Harvard University, December 1976.
[14]   Cook, R.D., "Detection of Influential Observations in Linear Regression,"
          Technometrics, 19 (1977), pp. 15—18.

[15] Mallows, C. L., "On Some Topics in Robustness," Paper delivered at the Eastern
         Regional INS meeting, University of Rochester, 1973.
                                                        Ai.i

Appendix 1. BASIC DIFFERENCE FORMULAS



       The fundamental difference formulas are known as the Sherman-
Morrison-Woodbury Theorem           [19, p. 29].



                                T       -1               T -1 + (XTx4x.(xTx
                         (X .  x . )
                            (i) Ci)
                                                       (X X)                                                        (A1.l)
                                                                        1—h1
                                                                                                           -l
                            T -1
                         (X X)      =        T             -1
                                                                -     (i i) )xTx.(XTX
                                                                    (XT)X(.    i (j) (i))
                                                                                                                    (Al.2)
                                        (X(1)X(1))                             T
                                                                         1—x1(X(1)X(1))                x
From   this comes




and   since
                        i)
                        A   A
                                          T
                                         (Xx)  T
                                              xr1
                                                 1—h.
                                                    1
                                                       1
                                                                                                                    (A1.3)




                        (n-p-i) S1)
                                                 t
                                                                -
                                                                    t   8(j)
                                                                                 2




we get

                                                                                     2         2
                                                                    h .r.                    r.
                                -
                        (n- p 1) S(2)
                                   .               E       (r +       tin.               -    1
                                                                    1-h.
                                                 t=l                    1
                                                                                             (i-h1)2

                                                                 2r         n                    r2     n              2
                                                                                                                      r.
                                                  (n—p)s2+
                                                                  1
                                                                ____         Z                  _______ Z h2
                                                                                                _______
                                                                                               (1_h1)2 t1 ti
                                                                                                                -   _________

                                                                                                                     (1-h1)2

                                                          2             i
                                             z
                                                   (n-p) S —
                                                                      1—h1

by using   the   fact   that H annihilates the vector                    of residuals
                                Ai.2




Finally we   obtain

                      2 T -l —            2    T         —l
               (n—p) s (X X)   (n—p—i)   S(1) (X()X())



                           (X1)X(1))1 - (n-p)s2
                                                   (ix)   l4x(xTx)'
                      i_




                                                                      •H




                                                                      .
                                                          A2.l


 Appendix 2. DIFFERENTIATION FORMULAS



         Let

                                      1


                                              1


                             w=                     1     1
 and


                                      (xwx)i xTwy.                                            (A2.2)

From    (Al.l)   we obtain

                                                                         T              -1
                                  -1               T —1
                         (xTWx)                   (X X)       + —(l—w1)(X X) x1x1(XX)
                                                                                             (A2.3)
                                                                       l—(l.-w1)h1


and    then

                             3        T —1                                     -1
                                 (X WX)                 _(XTX)
                                                                 1xx   (xTx)
                                                                                             (A2.L)
                                                          (1-(1-w1)h.)2

        Some algebraic manipulation using (A2. 2) and (A2.
                                                           3) gives
                         A        -       T       TXP•
                                                     (l_w)
                                                     11llw.)h.                               (A2.5)
                                                           13.
        A
where       and r1 are   the least-squares estimates obtained when w11. Thus



                     ___
                     ____ -- (XTX)
                                                        X1 r.1
                                                                                             (A2.6)
                                                    (1—(1-w.)h.)2
                                                 A2.2



or equivalently (again        using A2.3)                                                                       •
                       = (XTWX)4 (y-x1                                                                (A2.7)

       It is also useful to look at the squared residual error



                       SSRW        tl w
                                    n                          2
                                                                                                      (A2.8)


Using (A2.7) we have


                       SSR                n                        A              —l    T
                                     -2
                                              w (y_x                         X.        x1(y1x1 w1)

                                                                   A     2
                                                 +
                                                      (y-x1

                                                              fl                  A              -l
                                          (y.-x1
                                            1             1                                  T
                                     -2
                                                              tl                  ) vçx(X WX)            )c


                                                 +
                                                      (y1—x1   )2                                     (A2.9)


For the data v'W Y     and v X
                                              -l
                                iW X(XTWX)           xT       and


                       HR=O.
This   implies that    the   sum   in (A2.9)     is zero       so      that

                                                                             2

                                          1 1.)                          r
                       aSSRW.                    A        2
                        W.              (y.-x.1                                                       (A2.lO)




because   of (A2.3).
                                        A2.3



Putting   (A2.Li) and   (A2.lO)    together gives



                   [SSR      (XTWx)_1]


                                                         —l             —1
                    1
               ____________         T  i —           xTx xTX1(XTX)
                                (X WX)     SSR..1                            (A2.ll)
                              2                                     2
               (l_(l_w)h1)                            (l-(l—w1)h)



When       1 this is equivalent to


                   T -l
               r (X X) —          (n—p)s
                                        2      T 1T T -l
                                            (X X) x1x(X X)                   (A2.l2)
                                          A3.1



Appendix 3. THEOREMS ON THE HAT MATRIX


       In   this appendix we for!ially show   that   when h11 (we can take 1=1
without loss of generality), there exists a nonsingular transfonrtion T,
such   that
                    (T)1 y and                   do not depend on y1. This implies
                                     a2,... ,a
that, in the transformed coordinate system, the parameter a1 has been dedicated

to observation 1.

       When h1l we have for the coordinate vector                     ,   0)'

                     He1

since h1       0,        Let P be any pXp nonsingular matrix whose first column


is (XTX)XTe1. Then


                                 a
                     XP=I
                                 A


where a is lx(p-1) arid 0 is (p—l)xl. Now let



                     QIL        -a




with   I denoting the   (p-1)x(p-1) identity matrix. The transfonition we seek
is   given by T PQ, which is nonsingular because both P and Q have inverses.

Clearly

                           10
                     CT
                           OA
                                             A3.2


and the least-squares estimate of the parameter a T1 will have the first
residual, y1-a1, equal to zero since a2,. .. ,& cannot affect this residual.
This also   implies that   &2. . . ,&     will not depend on y1.


     To prove the second theorem in Section 2.3



                     det(X(.)TX(.))               (1—h1) det (xTx)


we need first to show that


                     det   (I_uvT)        i_vTu


where u and   v   are column vectors. Let          Q   be an orthononnal matrix   such that

                     Qu                                                                       (A3.l)
                              !lulle1


where e1 is the first standard           basis   vector. Then


                     det(I_uvT)          det Q[I_uvT] QT


                       det [I-I lul Ie1vTQT              i -           I lull



which is just i_vTu because of (A3 .1). Now

                     det   x . Tx
                            (i)   (i).    = det
                                                        ii
                                                   [(I-xTx. (XTX)_l)   xTx]

and letting u = x1 and V x(X X) completes the proof since x(X                         X) x±h.

(We are indebted to David Gay for simplifying our original proof.)




                                                                                                       S
                                     A4.l



Appendix 4. DiIBITS FOR SECTION 2.9


Exhibit No.                          Title

    1               Assignments of Row Indices to Countries

    2               Data

    3               Ordinary Least Squares Regression Results

    4               Normal Probability Plot of Studentized Residuals

    5               Studentized Residuals

    6               Diagonal Elements of the Hat Matrix.
    7               NBFBFI'AS: Square Roots of the Sum of Squares of the
                    Scaled Differences of LS Full Data and Row Removed
                    Coefficients (DFBETAS)
    8-   11         DFBFTAS (for individual coefficients)
    12              Summary of Relative Changes in   Coefficient   Standard
                    Errors: NDFBVARS
    13 -   16       Individual Relative Change in Coefficient Standard
                    Errors: DFBVARS
    17              Scaled Change   in   Fit

    18              Scatter Plot of NDFBEI'AS versus Diagonal Elements of
                    the Hat Matrix
    19              Scatter Plot of NDFBL'TAS versus Studentized Residuals
    20              Scatter Plot of NDFBVARS versus Diagonal Elements of
                    the Hat Matrix
    21              Scatter Plot of NDFBETAS versus NDFBVARS
    22              Diagonals of Hat Matrix with Observation 49 Removed
    23              NDFBETAS with Observation 49 Removed
                             EXHIBIT 1

 POSITION   LABEL
 1          AUSTRALIA
 2          AUSTRIA
 3          BELGIUM
4           BOLIVIA
5           BRAZIL
6           CANADA
 7          CHILE
S           CHINA(TAIWAN)
9           COLOMBIA
10          COSTA RICA
ii          t'ENMARK
12          ECUADOR
13          FINLAND
14          FRANCE
15          GERMANY F.R.
16          GREECE
17          (3IJATEMALA
18          HONDURAS
19          ICELAND
20          INDIA
2:L         IRELAND
22          IrAL.y
23          JAPAN
24          KOREA
25          LUXEMBOURG
26          MALTA
27          NORWAY
28          NETHERLANDS
29          NEW ZEALAND
30          N:ECARAGUA
31          PANAMA
32          FARAGUAY
33          PERU
34          PHILLIPINES
:35
36          SOUTH AFRICA
37          SOUTH RHODESIA
38          SPAIN
39          SWEDEN
40          SWITZERLAND
41          TURKEY
42          TUNISIA
43          UNITED KINGDOM
44          UNITED STATES
45          VENEZUELA
46          ZAMBIA
47          JAMAICA
48          URUGUAY
49          LIBYA
So          MALAYSIA
                             EXHIBIT 2
                  V           COL 2      COL 3    COL 4•
IUSTRALIA        11.43         29.35       2.87   2329.68
IUSTRIA          12.07         23,32       4.41   1507.99
)ELGIUM          13.17         23 • 8      4,43   2108.47
WLIVIA            S •   75     41.89       1.67    189.13
)RAZIL           12.88         42.19       0.83    728.47
ANADA             8 •   79     31,72       2.85   2982.88
:HILE             0.6          39,74       1.34    662.86
 HINA (TAIWAN)   11.9          44,75       0.67    289,52
OLOMBIA           4.98         46.64       1.06    276,65
:OSTA RICA       10.78         47.64       1.14    471.24
JENMARK          16.85         24.42       3,93   2496.53
 CUADOR           3,59         46.31       1.19    287,77
 INLAND          11 • 24       27.84       2.37   1681.25
RANCE            12.64         25.06       4,7    2213.82
3ERMANY F.R,     12.55         23.31       3,35   2457.12
REECE            10.67         25.62       3.1     870.85
UATEMALA          3.01         46.05       0.87    289.71
IONDURAS          7.7          47.32       0.58    232.44
:CELAND           1 •   27     34.03       3.08   1900.1
:NDIA             9.           41.31       0.96     88,94
:RELAND          11.34         31.16       4.19   1139,95
:TALY            14.28         24,52       3,48   1390.
JAPAN            21.1          27.01       1.91   1257.2
:OREA             3,98         41.74       0.91   207.
.UXEMBOURG       10.35         21.8        3,73   2449.3
JALTA            15.48         32.54       2.47    601.05
IOR WAY          10.25         25.95       3,67   2231,03
IETHERLANDS      14.65         24.71       3.25   1740.7
JEW ZEALAND      10.67         32.61       3.17   1487.52
JICARAOUA         7,3          45.04       1.21    325.54
'ANAMA            4,44         43.56       1.2     568.56
'ARAGUAY          2.02         41.18       1.05    220.56
'ERU             12.7          44.19       1.28    400.06
'HILLIPINES      12.78         46.26       1.12    152.01
'ORTUGAL         12.49         28,96       2.85    579.51
SOUTH AFRICA     11.14         31,94       2.28    651.11
IOUTH RHODESIA   13.3          31.92       1.52    250.96
PAIN             11.77         27,74       2.87    768.79
WEDEN             6.86         21.44       4,54   3299,49
tWITZERLAND      14.13         23.49       3,73   2630.96
URKEY             5.13         43.42       1.08    389.66
•UNISIA           2.81         46.12       1.21    249.87
JNITED KINGDOM    7.81         23.27       4.46   1813.93
INITED STATES     7.56         29.81       3,43   4001.89
'ENEZUELA         9.22         46.4        0.9     813.39
:AMBIA           18.56         45.25       0.56    138.33
IAMAICA           7,72         41.12       1.73    380.47
IRUGUAY           9.24         28.13       2.72    766.54
.IBYA             8.89         43.69       2,07    123.
ALAYSIA           4.71         47.2        0.66    242.6
                                              S



                                EXHIBIT   1


    POSITION   LABEL
    I          AUSTRALIA
    2          AUSTRIA
    3          BELGIUM
    4          BOLIVIA
    5          BRAZIL
    6          CANADA
    7          CHILE
    S          CHINACTAIWAN)
    9          COLOMBIA
    10         COSTA RICA
    11         DENMARK
    12         ECUADOR
    13         FINLAND
    14         FRANCE
    15         GERMANY F.R.
    16         GREECE
    17         GUATEMALA
    18         HONDURAS
    19         ICELAND
    20         INDIA
    21         IRELAND
    2?         ITALY
    23         JAPAN
    24
S   25
               KOREA
               LUXEMBOURG
    26         MALTA
    27         NORWAY
    28         NETHERLANDS
    29         NEW ZEALAND
    30         NICARAGUA
    31         PANAMA
    32         PARAGUAY
    33         PERU
    34         PHILLIPINES
    35         PORTUGAL
    36         SOUTH AFRICA
    37         SOUTH RHODESIA
    38         SPAIN
    39         SWEDEN
    40         SWITZERLAND
    41         TURKEY
    42         TUNISIA
    43         UNITED KINGDOM
    44         UNITED STATES
    45         VENEZUELA
    46         ZAMBIA
    47         JAMAICA
    48         URUGUAY
               LIBYA
S   50         MALAYSIA
U




                             EXHIBIT 2 CONTINUED
                     COL 5
    AUSTRALIA         2.87
    AUSTRIA           3.93
    BELGIUM           3.82
    BOLIVIA           0.22
    BRAZIL            4.56
    CANADA            2.43
    CHILE             2.67
    CHINACTAIWAN)     6.51
    COLOMBIA          3.08
    COSTA RICA        2,8
    DENMARK           3.99
    ECUADOR           2.19
    FINLAND           4.32
    FRANCE            4.52
    GERMANY F.R.      3.44
    GREECE            6.28
    GUATEMALA         1.48
    HONDURAS          3.19
    ICELAND           1.12
    INDIA             1.54
    IRELAND           2.99
    ITALY             3.54
    JAPAN             8.21
    KOREA             5.81
    LUXEMBOURG        1.57
    MALTA             8.12
    NORWAY            3.62
    NflHERLANDS       7.66
    NEW ZEALAND       1.76
    NICARAGUA         2.48
    PANAMA            3.61
    PARAGUAY          1.03
    PERU              0.67
    PHILLIPINES       2.
    PORTUGAL          7.48
    SOUTH AFRICA      2.19
    SOUTH RHODESIA    2.
    SPAIN             4.35
    SWEDEN            3.01
    SWITZERLAND       2.7
    TURKEY            2.96
    TUNISIA           1.13
    UNITED KINGDOM    2.01
    UNITED STATES     2.45
    VENEZUELA         0.53
    ZAMBIA            5.14
    JAMAICA          10.23
    URUGUAY           1.88
    LIBYA            16.71
    MALAYSIA          5.08
                                               EXHIBIT 3


NOB=50                t'IOUAR=S       CONDITION OF SCALED    =34.8683
'—.i1       i   J.
                     .7
                            SER=3. 80267       RSQ=. 338456      OWO)=1 .9234
F.. 4. 45 )=5 .                    TRACE OF XTXINU=3.82582




                 NE          EST COEF          910   ERR       1—STAT
C'JEF. 1    flTEREFT          28.5661           7.35452         3.88415
COEF. 2     POP15             —0.461193         0.144642       —3.18851
COEF. 3     P0P75             —1.6915           1.8836         —1.561
COEF   4    INC               —0. 00033?        0.000931       —8.361829
CJEF .5     INGRO                 0.409694      0.196197        2. 88818




.                                                    .                          S
                         EXHIBIT 2 CONTINUED
                 COL5
AUSTRALIA         2.87
AUSTRIA           3.93
BELGIUM           3.82
BOLIVIA           0.22
BRAZIL            4.56
CANADA            2.43
CHILE             2.67
CHINACTAIWAN)     6.51
COLOMBIA          3.08
COSTA RICA        2.8
DENMARK           3.99
ECUADOR           2,19
FINLAND           4.32
FRANCE            4.52
GERMANY F.R.      3.44
GREECE            6.28
GUATEMALA         1.48
HONDURAS          3.19
ICELAND           1.12
INDIA             1.54
IRELAND           2.99
ITALY             3,54
JAPAN             8,21
KOREA             5.81
LUXEMBOURG        1.57
MALTA             8.12
NORWAY            3.62
NETHERLANDS       7,66
NEW ZEALAND       1.76
NICARAGUA         2.48
PANAMA            3,61
PARAGUAY          1.03
PERU              0.67
PHILLIPINES       2.
PORTUGAL          7,48
SOUTH AFRICA      2.19
SOUTH RHODESIA    2.
SPAIN             4.35
SWEDEN            3.01
SWITZERLAND       2.7
TURKEY            2.96
TUNISIA           1.13
UNITED KINGDOM    2.01
UNITED STATES     2.45
VENEZUELA         0.53
ZAMBIA            5.14
JAMAICA          10.23
URUGUAY           1.88
LIBYA            16.71
MALAYSIA          5.08
.                                          .
                                      DBIT i•
                                                                           .
                         NORMAL PROBABILITY PLOT OF RSTIJDENT
      3.
(—I
L
U
E
       1.
      —1
      -3.                                                                  2.5
                                                           INLJERSE_HORMAL..
            ROBUST EQUATION   jS;   Y1.O8O6 -O.O18218
p-b




I-b
fl




             '-4
N
I-b
             z0

             -r
             0
             -4
             0
             -TI


             w
             -4
             C
             0
             in
             —I




        0)


  ci'
.                                          .                            S
                                           EXHIBIT 14

                           INOE   PLOT OF SELECTED COLUMNS OF DFBUARS



 e. 15




—a.a




—0.25                 11              21                31      41      51



    TIME BOIJNDS:    I TO 50
    DATA NAMES      C3(P75
                      EiIBIT   7


                INDEX PLOT OF      1DFBET


4.00


300


2.00


10O



       1   11    21                         41   51




.                     .                          S
.                                         .                              .
                                          EXHIBIT 8


                            INDE> PLOT OF ;ELECTEO COLUMNS OF OFBETAS



030




—3.20




-070 1                11             21                         41      51


    TIME BOUNDS   1    TO   50
    DtTA NAIIES: C2P0P15)
                                          E1IBIT   9


                           INDEX PLOT OF SELECTED COLUMNS OF DFBETAS



 0.50



 e_ 10




—0.70
                      11            21                 31     41       51


    TIME BOUNDS:    1 TO   5
    DATA NAMES     C3 (P0P75




.                                        .                              S
.                                      .                             .
                                       EHIBIT 10

                        INDEX PLOT OF SELECTED COLIJMN3 OF OFEETAS



0.16




—0.12




—0.26
        1




    TI ME BOUNDS   I   TO 50
    DATA flAMES: C4(niC)
                                                     EXHIBIT 1J

                                      INDE< PLOT OF SELECTED COLUMNS OF OFOETAS



 3.42



    -   a.   .

-€1.60




_1.1t31                         11             21                 31     41       51



             TIME   BOIJNDS;   1 TO   50
         DATA NAMES: CS         (INGRO)




.                                                   .                             .
                 S                        .
                 EXHIBIT 12

           INDE< PLOT OF HOF'JARS




5.0




      11    21                31    41   51
                                          HIBIT   13

                           INDEX PLOT OF SELECTED COLUMNS OF UJFUARS



 0.20




 0.80




—
                      11            21                         41      51



    TIME BOUNDS   I   TO   50
    DATA HAMES C2 (p0p15)




.                                        .                              S
.                                          .                            S
                                           EXHIBIT 14

                           INOE   PLOT OF SELECTED COLUMNS OF DFBUARS



 e. 15




—a.a




—0.25                 11              21                31      41      51



    TIME BOIJNDS:    I TO 50
    DATA NAMES      C3(P75
                                              DUiIBIT 15

                          INOE   PLOT OF SELEC:TEO COLUMHS OF OFBL)RS



 -1s


—'...4



—2.26




—0.42                11                                        41       51
         1                           21                31



    TIME BOUNDS   1 TO     50
    OITA NAMES: C4   (C)


.                                         .                             .
                                       .                              S
                                        EUBIT   16


                         INOE< PLOT OF SELECTED COLUMNS OF OFBUARS


 a




—.'-



—0.60




—1.00              11             21                 31      41      51



     TIME BOuNDS: I TO   50
     DATA NAMES: C5(GRO)
                  U,   .

      (J)
      I-




                       .
      LL
C—,




H
      0
x -jLI
      w
      0
      •-4




            ".4
                       S
:.                                                      .                                       .
                                                        EiIBIT   18


                                           SCATTER PLOT OF HFBETAS US H
     4.oe
                                                   23
 F
 B




     2.ø13

                      a




     1.00                         a.
                                                                      44
                                       a



                  •la •
              •   •   •       a
                      • aa.
     8 . 08                                                                                 0.688
      0.080           0.100                8.280                           0.400   0.500
                                                                                           H.
                                                     IBIT 19
                       SCATTER PLOT OF HOFETAS tJ5 RSTtJOEF4T
     4.00
                                                                                                  23
I-


I



                       I
                                   •                      44
                                         .            I
                                                                                              I
                                                 I    I                        I I.•
                                   • •
                                                          I
                                             •                     •       •
                                                                           a
                                                              •        I
     0.0
      —3.IiØ   —2.Oe       -1.00                                  0.00                 1.00            2,00    3.00
                                                                                                        RSTUDEHT




.                                                         S
:•:•
                                                     EXHIBIT 20

                                            SCATTER PLOT OF NOFBUARS US H




       10.0



                   46                                             44
                                        :37
              .7

                            9           9



                                9
               •
                   :... a
                        :
                   .•               S




       0.0Ø          0.100                  0.200                      0.400   0.500       0.600
                                                                                       H
                                                      EiIBIT   21

                                          SCATTER PLOT OF NOFBETAS US HDFBURS
        4J30
    N
    0
    F
    B
        3.00
    H



                                               21
        2.
                                                      .46

        i.o          .. ..           .               44
                        e
                 •. .       .•
                •           .   S



        0.
                                    2.0     4.0      6.0                  10.0   12.0        14.8
             0'0.0                                                  8.0
                                                                                 NDFBUARS.




.                                                    S                                       .
                   .                       .
                     DffBIT 22
                   INDEX PLOT OF H



34O




c   be


3.e2O                            31   41   51
         11   21
                     EXHIBIT 23

               INOE> PLOT OF t1OFBET$



4.0




a—.




          11    21                31    41   51
      1




S                    .                        .
