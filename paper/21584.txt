                               NBER WORKING PAPER SERIES




           PRINCIPAL COMPONENT ANALYSIS OF HIGH FREQUENCY DATA

                                        Yacine Aït-Sahalia
                                          Dacheng Xiu

                                       Working Paper 21584
                               http://www.nber.org/papers/w21584


                     NATIONAL BUREAU OF ECONOMIC RESEARCH
                              1050 Massachusetts Avenue
                                Cambridge, MA 02138
                                   September 2015




We thank Hristo Sendov for valuable discussions on eigenvalues and spectral functions. In addition,
we benefited much from comments by Torben Andersen, Tim Bollerslev, Oleg Bondarenko, Marine
Carrasco, Gary Chamberlain, Kirill Evdokimov, Jianqing Fan, Christian Hansen, Jerry Hausman, Jean
Jacod, Ilze Kalnina, Jia Li, Yingying Li, Oliver Linton, Nour Meddahi, Per Mykland, Ulrich Müller,
Andrew Patton, Eric Renault, Jeffrey Russell, Neil Shephard, George Tauchen, Viktor Todorov, Ruey
Tsay, and Xinghua Zheng, as well as seminar and conference participants at Brown University, CEMFI,
Duke University, Harvard University, MIT, Northwestern University, Peking University, Princeton
University, Singapore Management University, University of Amsterdam, University of Chicago,
University of Illinois at Chicago, University of Tokyo, the 2015 North American Winter Meeting of
the Econometric Society, the CEME Young Econometricians Workshop at Cornell, the NBER-NSF
Time Series Conference in Vienna, the 10th International Symposium on Econometric Theory and
Applications, the 7th Annual SoFiE Conference, the 2015 Financial Econometrics Conference in Toulouse
and the 6th French Econometrics Conference. The views expressed herein are those of the authors
and do not necessarily reflect the views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2015 by Yacine Aït-Sahalia and Dacheng Xiu. All rights reserved. Short sections of text, not to
exceed two paragraphs, may be quoted without explicit permission provided that full credit, including
© notice, is given to the source.
Principal Component Analysis of High Frequency Data
Yacine Aït-Sahalia and Dacheng Xiu
NBER Working Paper No. 21584
September 2015
JEL No. C22,C55,C58,G01

                                            ABSTRACT

We develop the necessary methodology to conduct principal component analysis at high frequency.
We construct estimators of realized eigenvalues, eigenvectors, and principal components and provide
the asymptotic distribution of these estimators. Empirically, we study the high frequency covariance
structure of the constituents of the S&P 100 Index using as little as one week of high frequency data
at a time. The explanatory power of the high frequency principal components varies over time. During
the recent financial crisis, the first principal component becomes increasingly dominant, explaining
up to 60% of the variation on its own, while the second principal component drives the common variation
of financial sector stocks.


Yacine Aït-Sahalia
Department of Economics
Bendheim Center for Finance
Princeton University
Princeton, NJ 08540
and NBER
yacine@princeton.edu

Dacheng Xiu
Booth School of Business
University of Chicago
5807 South Woodlaswn Avenue
Chicago, IL 60637
dachxiu@chicagobooth.edu
            Principal Component Analysis of High Frequency Data∗
                               Yacine Aı̈t-Sahalia†                          Dacheng Xiu‡
                              Department of Economics                   Booth School of Business
                           Princeton University and NBER                  University of Chicago

                                        This Version: September 16, 2015


                                                          Abstract
           We develop the necessary methodology to conduct principal component analysis at high frequency.
       We construct estimators of realized eigenvalues, eigenvectors, and principal components and provide the
       asymptotic distribution of these estimators. Empirically, we study the high frequency covariance structure
       of the constituents of the S&P 100 Index using as little as one week of high frequency data at a time.
       The explanatory power of the high frequency principal components varies over time. During the recent
       financial crisis, the first principal component becomes increasingly dominant, explaining up to 60% of the
       variation on its own, while the second principal component drives the common variation of financial sector
       stocks.

         Keywords: Itô Semimartingale, High Frequency, Spectral Function, Eigenvalue, Eigenvector, Principal
       Components, Three Factor Model.

           JEL Codes: C13, C14, C55, C58.



1      Introduction
Principal component analysis (PCA) is one of the most popular techniques in multivariate statistics, providing
a window into any latent common structure in a large dataset. The central idea of PCA is to identify a small
number of common or principal components which effectively summarize a large part of the variation of the
data, and serve to reduce the dimensionality of the problem and achieve parsimony.
   Classical PCA originated in Pearson (1901) and Hotelling (1933) and is widely used in macroeconomics
and finance among many other fields. One example is Litterman and Scheinkman (1991), who document a
    ∗ Wethank Hristo Sendov for valuable discussions on eigenvalues and spectral functions. In addition, we benefited much
from comments by Torben Andersen, Tim Bollerslev, Oleg Bondarenko, Marine Carrasco, Gary Chamberlain, Kirill Evdokimov,
Jianqing Fan, Christian Hansen, Jerry Hausman, Jean Jacod, Ilze Kalnina, Jia Li, Yingying Li, Oliver Linton, Nour Meddahi,
Per Mykland, Ulrich Müller, Andrew Patton, Eric Renault, Jeffrey Russell, Neil Shephard, George Tauchen, Viktor Todorov,
Ruey Tsay, and Xinghua Zheng, as well as seminar and conference participants at Brown University, CEMFI, Duke University,
Harvard University, MIT, Northwestern University, Peking University, Princeton University, Singapore Management University,
University of Amsterdam, University of Chicago, University of Illinois at Chicago, University of Tokyo, the 2015 North American
Winter Meeting of the Econometric Society, the CEME Young Econometricians Workshop at Cornell, the NBER-NSF Time
Series Conference in Vienna, the 10th International Symposium on Econometric Theory and Applications, the 7th Annual SoFiE
Conference, the 2015 Financial Econometrics Conference in Toulouse and the 6th French Econometrics Conference.
   † Address: 26 Prospect Avenue, Princeton, NJ 08540, USA. E-mail address: yacine@princeton.edu.
   ‡ Address: 5807 S Woodlawn Avenue, Chicago, IL 60637, USA. E-mail address: dacheng.xiu@chicagobooth.edu. Xiu gratefully

acknowledges financial support from the IBM Faculty Scholar Fund at the University of Chicago Booth School of Business.


                                                              1
three-factor structure of the term structure of yields using PCA. PCA has also been applied to analyze the
dimension of volatility dynamics, e.g., Egloff, Leippold, and Wu (2010), suggesting a two-factor model of
volatility, capturing the long and short-term fluctuations of the volatility term structure. Another example
is the development of economic activity and inflation indices by Stock and Watson (1999) using PCA and a
factor model. A sentiment measure, reflecting the optimistic or pessimistic view of investors, was created by
Baker and Wurgler (2006) using the first principal component of a number of sentiment proxies, while a policy
uncertainty index was created by Baker, Bloom, and Davis (2013).
    The estimation of the eigenvalues of the sample covariance matrix is the key step towards PCA. Classical
asymptotic results, starting with Anderson (1958) and Anderson (1963), show that eigenvalues of the sample
covariance matrix are consistent and asymptotically normal estimators of the population eigenvalues, at least
when the data follow a multivariate normal distribution. Even under normality, the asymptotic distribution
becomes rather involved when repeated eigenvalues are present. Waternaux (1976) showed that a similar
central limit result holds for simple eigenvalues as long as the distribution of the data has finite fourth moment,
while Tyler (1981) obtains the asymptotic distribution of eigenvectors under more general assumptions; see
also discussions in the books by Jolliffe (2002) and Jackson (2003).
    The classical PCA approach to statistical inference of eigenvalues suffers from three main drawbacks. First
is the curse of dimensionality. For instance, it is well known that even the largest eigenvalue is no longer
consistently estimated when the cross-sectional dimension grows at the same rate as the sample size along
the time domain. Second, the asymptotic theory is essentially dependent on frequency domain analysis under
stationarity (and often additional) assumptions, see, e.g., Brillinger (2001). Third, the principal components
are linear combinations of the data, which fail to capture potentially nonlinear patterns therein.
    These three drawbacks create difficulties when PCA is employed on asset returns data. A typical portfolio
may consist of dozens of stocks. For instance, a portfolio with 30 stocks has 465 parameters in their covariance
matrix, if no additional structure is imposed, while one with 100 stocks contain 5,050 parameters. As a
result, years of time series data are required for estimation, raising issues of survivorship bias, potential non-
stationarity and parameter constancy. Moreover, asset returns are known to exhibit time-varying volatility
and heavy tails, leading to deviations from the assumptions required by the classical PCA asymptotic theory.
In addition, prices of derivatives, e.g., options, are often nonlinear functions of the underlying asset’s price
and volatility. Ruling out nonlinear combinations disconnects their principal components from the underlying
factors that drive derivative returns.
    These issues motivate the development in this paper of the tools necessary to conduct PCA for continuous-
time stochastic processes using high frequency data and high frequency or “in-fill” asymptotics, whereby the
number of observations grows within a fixed time window. This approach not only adds the important method
of PCA to the high frequency toolkit, but it addresses the three drawbacks mentioned above. First, the large
amount of intraday time series data vastly improve the time series vs. cross-sectional dimensionality trade-off.
And these are not redundant observations for the problem at hand. Unlike for expected returns, it is well
established theoretically that increasing the sampling frequency improves variance and covariance estimation,
at least until market microstructure concerns start biting. Yet market microstructure noise is not a serious
concern at the one minute sampling frequency we will employ empirically, for liquid stocks which typically
trade on infra-second time scales. As a result of the large increase in the time series dimension, it is plausible
to expect high frequency asymptotic analysis with the cross-sectional dimension fixed to serve as accurate
approximations.1 In fact, we will show both in simulations and empirically that PCA works quite well at high
   1 At low frequency, in the case of large dimensional data, Geman (1980) and Bai, Silverstein, and Yin (1988) investigated

the strong law of the largest eigenvalue, when the cross-sectional dimension of the data grows at the same rate as the sample
size. Johnstone (2001) further analyzed the distribution of the largest eigenvalue using random matrix theory. These analyses



                                                             2
frequency for a portfolio of 100 stocks using as little as one week of one-minute returns data. Second, the
high frequency asymptotic framework enables nonparametric analysis of general stochastic processes, thereby
allowing strong dependence, non-stationarity, and heteroscedasticity due to stochastic volatility and jumps,
and freeing the analysis from the strong parametric assumptions that are required in a low frequency setting. In
effect, the existence of population moments becomes irrelevant and PCA becomes applicable at high frequency
for general Itô semimartingales. Third, our principal components are built upon instantaneous (or locally)
linear combinations of the stochastic processes, which thereby capture general nonlinear relationships by virtue
of Itô’s lemma.
    Within a continuous-time framework, we first develop the concept of “realized” or high-frequency PCA for
data sampled from a stochastic process within a fixed time window. Realized PCA depends on realizations
of eigenvalues, which are stochastic processes, evolving over time. Our implementation of realized PCA is
designed to extend classical PCA from its low frequency setting into the high frequency one as seamlessly
as possible. Therefore, we start by estimating realized eigenvalues from the realized covariance matrix. One
technical challenge we must overcome is the fact that when an eigenvalue is simple, it is a smooth function
of the instantaneous covariance matrix. On the other hand, when it comes to a repeated eigenvalue, this
function is not differentiable, which hinders statistical inference, as the asymptotic theory requires at least
second-order differentiability. To tackle the issue of non-smoothness of repeated eigenvalues, we propose
an estimator constructed by averaging all repeated eigenvalues. Using the theory of spectral functions, we
show how to obtain differentiability of the proposed estimators, and the required derivatives. We therefore
construct an estimator of realized spectral functions, and develop the high frequency asymptotic theory for
this estimator. Estimation of eigenvalues, principal components and eigenvectors then arise as special cases of
this estimator, by selecting a specific spectral function. Aggregating local estimates results in an asymptotic
bias, due to the nonlinearity of the spectral functions. The bias is fortunately of higher order, which enables us
to construct a bias-corrected estimator, and show that the latter is consistent and achieves stable convergence
in law to a mixed normal distribution.
    As far as asymptotic theory is concerned, our estimators are constructed by aggregating functions of in-
stantaneous covariance estimates. Other examples of this general type arise in other high frequency contexts.
Jacod and Rosenbaum (2013) analyze such estimators with an application of integrated quarticity estimation.
Li and Xiu (2014) develop inference theory based on the generalized method of moments to investigate struc-
tural economic models which include the instantaneous volatility as an explanatory variable. Kalnina and
Xiu (2013) discuss estimation of the leverage effect measured by the integrated correlation with an additional
volatility instrument. Li, Todorov, and Tauchen (2013) discuss inference theory for volatility functional de-
pendencies. An alternative inference theory in Mykland and Zhang (2009) is designed for a class of estimators
based on an aggregation of local estimates using a finite number of blocks.
    Also related to the problem we study is the rank inference developed in Jacod, Lejay, and Talay (2008)
confirmed that the largest eigenvalue is no longer consistently estimated. Moreover, Johnstone and Lu (2009) prove that, for a
single factor model, the estimated eigenvector corresponding to the largest eigenvalue is also inconsistent unless the sample size
grows at a rate faster than the one at which the cross-sectional dimension increases. To resolve the inconsistency of the PCA
in this setting, different estimators of the covariance matrix have been proposed, using banding (Bickel and Levina (2008b)),
tapering (Cai and Zhou (2012)), and thresholding (Bickel and Levina (2008a)) methods, or imposing additional assumptions such
as sparsity. However, the sparsity requirement of the covariance matrix does not hold empirically or a large cross-section of stock
returns, see, e.g., Fan, Furger, and Xiu (2015). Alternatively, there is a new literature on methods that “sparsify” the PCA
directly, i.e., imposing the sparsity on eigenvectors, see, e.g., Jolliffe, Trendafilov, and Uddin (2003), Zou, Hastie, and Tibshirani
(2006), d’Aspremont, Ghaoui, Jordan, and Lanckriet (2007), Johnstone and Lu (2009), and Amini and Wainwright (2009). None
of these methods are currently applicable to dependent data nor are central limit theorems available for these estimators in a
large dimensional setting.




                                                                  3
and Jacod and Podolskij (2013), where the cross-sectional dimension is also fixed and the processes follow
continuous Itô semimartingales. Allowing for an increasing dimension but with an additional sparsity structure,
Wang and Zou (2010) consider the estimation of integrated covariance matrix with high-dimensional and high
frequency data in the presence of measurement errors. Tao, Wang, and Zhou (2013) investigate the minimax
rate of convergence for covariance matrix estimation in the same setting; see also Tao, Wang, and Chen (2013)
and Tao, Wang, Yao, and Zou (2011) for related work. Zheng and Li (2011) establish a Marcenko-Pastur type
theorem forstudy the spectral distribution of the integrated covariance matrix for a special class of diffusion
processes. In a similar setting, Heinrich and Podolskij (2014) study the spectral distribution of empirical
covariation matrices of Brownian integrals.
    Since it is about PCA, this paper shares all the natural pros and cons of PCA, especially relative to factor
models. There is a large low frequency literature in economics and finance on factor analysis. While factor
analysis in the classical setting of low frequency and fixed dimension is a very useful tool, as in Ross (1976) for
instance, Chamberlain and Rothschild (1983) point out that when the dimension grows with the sample size,
PCA may be preferred due to its simplicity, and the fact that it may be employed under weaker assumptions,
such as a factor structure that is only approximate. In fact, PCA has been used extensively at low frequency
for analyzing factor models, to determine the number of factors, e.g. Bai and Ng (2002), or to construct and
use factors for forecasting, e.g. Stock and Watson (2002), with the asymptotic theory developed by Connor
and Korajczyk (1988), Stock and Watson (1998), and Bai (2003). The above factor models are static, as
opposed to the dynamic factor models discussed in Forni, Hallin, Lippi, and Reichlin (2000), Forni and Lippi
(2001), and Forni, Hallin, Lippi, and Reichlin (2004), in which discrete-time lagged values of the unobserved
factors may also affect the observed dependent variables. By contrast, this paper develops the theory for
high frequency PCA in a continuous-time setting. Our model consists of Itô semimartingales, and is fully
nonparametric without any assumptions on the existence of a factor structure. That said, similar techniques
as those developed here can in principle be developed in the future to estimate continuous-time models with
a factor structure but this is a distinct problem from PCA (just as it is in a low frequency setting).
    The paper is organized as follows. Section 2 sets up the notation and the model. Section 3 provides the
estimators and the main asymptotic theory. Section 4 reports the results of Monte Carlo simulations. We
then implement the method to analyze by PCA the covariance structure of the S&P 100 stocks in Section
5, where we ask whether at high frequency cross-sectional patterns in stock returns are compatible with the
well-established low frequency evidence of a low-dimensional common factor structure (e.g., Fama and French
(1993)). We find that, excluding jump variation, three Brownian factors explain between 50 and 60% of
continuous variation of the stock returns, that their explanatory power varies over time and that during the
recent financial crisis, the first principal component becomes increasingly dominant, explaining up to over
60% of the variation on its own, capturing market-wide, systemic, risk. Despite the differences in methods,
time periods, and length of observation, these empirical findings at high frequency are surprisingly consistent
with the well-established low frequency Fama-French results of three common factors. Of course, the design
limitation of PCA is that it does lend itself easily to an identification of what the principal components are
in terms of economic factors. Nevertheless, we find evidence that the first principal component shares time
series characteristics with the overall market return, and, using biplots, we find that at the height of the crisis
the second principal component drives the common variation of financial sector stocks. Section 6 concludes.
Proofs are in the appendix.




                                                        4
2     The Setup
2.1    Standard Notations and Definitions
In what follows, Rd denotes the d-dimensional Euclidean space, and its subset R+             contains non-negative real
                                                                                         Pdd
vectors. Let ek be the unit vector in R+      d , with  the kth  entry equal  to  1, 1 =       k
                                                                                          k=1 e , and I be the identity
matrix. Md denotes the Euclidean space of all d × d real-valued symmetric matrices. M+                 d is a subset of
                                                                           ++
Md , which includes all positive-semidefinite matrices. We use Md to denote the set of all positive-definite
matrices. The Euclidean space Md is equipped with an inner product hA, Bi = Tr(AB). We use k·k to denote
the Euclidean norm for vectors or matrices, and the superscript “+” to denote the Moore-Penrose inverse of
a real-matrix, see e.g., Magnus and Neudecker (1999).
    All vectors are column vectors. The transpose of any matrix A is denoted by A| . The ith row of A is written
as Ai,· , and the jth column of A is A·,j . Aij denotes the (i, j)th entry of A. The operator diag : Md → Rd
is defined as diag(A) = (A11 , A22 , . . . , Add )| . In addition, we define its inverse operator Diag, which maps a
vector x to a diagonal matrix.
    Let f be a function from R+  d to R. The gradient of f is written as ∂f , and its Hessian matrix is denoted
as ∂ f . The derivative of a matrix function F : M+
     2
                                                           d → R is denoted by ∂F ∈ Md , with each element written
as ∂ij F , for 1 ≤ i, j ≤ d. Note that the derivative is defined in the usual sense, so that for any A ∈ M+          d,
∂ij A = Jij , where Jij is a single-entry matrix with the (i, j)th entry equal to 1. The Hessian matrix of F is
written as ∂ 2 F , with each entry referred to as ∂jk,lm 2
                                                             F , for 1 ≤ j, k, l, m ≤ d. We use ∂ k to denote kth order
derivatives and δ i,j to denote the Kronecker’s delta function giving 1 if i = j or 0 otherwise. A function f is
Lipchitz if there exists a constant K such that |f (x + h) − f (x)| ≤ K khk . In the proof, K is a generic constant
which may vary from line to line. A function with kth continuous derivatives is denoted a C k function.
                                                                                                         u.c.p.
    Data are sampled discretely every ∆n units of time. All limits are taken as ∆n → 0. “ =⇒ ” denotes
                                                     p                                                   L−s
uniformly on compacts in probability, and “−→” denotes convergence in probability. We use “−→” to denote
stable convergence in law. We write an  bn if for some c ≥ 1, bn /c ≤ an ≤ cbn for all n. Finally, [·, ·] denotes
the quadratic covariation between Itô semimartingales, and [·, ·]c the continuous part thereof.


2.2    Eigenvalues and Eigenvectors
We now collect some preliminary results about eigenvalues and eigenvectors. For any vector x ∈ R+d , x̄ denotes
the vector with the same entries as x, ordered in a non-increasing order. We use R̄+
                                                                                   d to denote  the   subset of
  +
Rd containing vectors x satisfying x1 ≥ x2 ≥ . . . ≥ xd ≥ 0.
    By convention, for any x ∈ R̄+
                                 d , we can write:

                   x1 = . . . = xg1 > xg1 +1 = . . . = xg2 > . . . xgr−1 > xgr−1 +1 = . . . = xgr ≥ 0,               (1)

where gr = d, and r is the number of distinct element. {g1 , g2 , . . . , gr } depends on x. We then define a
corresponding partition of indices as Ij = {gj−1 + 1, gj−1 + 2, . . . , gj }, for j = 1, 2, . . . , r.
    For any A ∈ M+                                             |
                    d , λ(A) = (λ1 (A), λ2 (A), . . . , λd (A)) is the vector of its eigenvalues in a non-increasing
order. This notation also permits us to consider λ as a mapping from M+                        +
                                                                                     d to R̄d . An important result
establishing the continuity of λ is, see, e.g., Tao (2012):

Lemma 1. λ : M+     +
              d → R̄d is Lipchitz.

   Associated with any eigenvalue λg of A ∈ M+     d , we denote its eigenvector as γ g , which satisfies Aγ g = λg γ g ,
and γ |g γ g = 1. The eigenvector, apart from its sign, is uniquely defined when λg is simple. In such a case,
without loss of generality we require the first non-zero element of the eigenvector to be positive. In the presence


                                                           5
of repeated eigenvalues, the eigenvector is determined up to an orthogonal transformation. In any case, we
can choose eigenvectors such that for any g 6= h, γ |g γ h = 0, see, e.g., Anderson (1958).
    When λg is a simple root, we regard γ g as another vector-valued function of A. It turns out in this case,
both λg (·) and γ g (·) are infinitely smooth at A, see, e.g., Magnus and Neudecker (1999).

Lemma 2. Suppose λg is a simple root of A ∈ M+              +     +           +
                                             d , then λg : Md → R̄ and γ g : Md → Rd are C
                                                                                           ∞
                                                                                             at A.
Moreover, we have

                        ∂jk λg (A) = γ gj (A)γ gk (A),         and     ∂jk γ g (A) = (λg I − A)+
                                                                                               ·,j γ gk (A),

where γ gk is the kth entry of γ g . If in addition all the eigenvalues of A are simple, with (γ 1 , γ 2 , . . . , γ d ) being
the corresponding eigenvectors, then

                        2
                                           X          1                                            
                       ∂jk,lm γ gh = −                        γ γ γ γ γ − γ pl γ pm γ ph γ pj γ gk
                                                  (λg − λp )2 gl gm ph pj gk
                                           p6=g
                                           XX                    1
                                       +                                    γ γ γ γ γ
                                                        (λg − λp )(λp − λq ) ql pm qh pj gk
                                           p6=g q6=p
                                           XX                    1
                                       +                                    γ γ γ γ γ
                                                        (λg − λp )(λp − λq ) ql pm qj ph gk
                                           p6=g q6=p
                                           XX                    1
                                       +                                    γ γ γ γ γ .
                                                        (λg − λp )(λg − λq ) qk ql gm ph pj
                                           p6=g q6=g

   In general, while the eigenvalue is always a continuous function, the eigenvector associated with a repeated
root is not necessarily continuous. In what follows, we only consider estimation of an eigenvector when it is
associated with a simple eigenvalue.


2.3     Dynamics of the Variable
The process we analyze is a general d-dimensional Itô semimartingale, defined on a filtered probability space
(Ω, F, (Ft )t≥0 , P) with the following Grigelionis representation:
                                 Z t         Z t
                     Xt = X0 +       bs ds +     σ s dWs + (δ1kδk≤1 ) ∗ (µ − ν)t + (δ1{kδk>1} ) ∗ µt ,     (2)
                                   0                0

where W is a d-dimensional Brownian motion, µ is a Poisson random measure on R+ ×Rd with the compensator
ν(dt, dx) = dt ⊗ ν̄(dx), and ν̄ is a σ-finite measure. More details on high frequency models and asymptotics
can be found in the book Aı̈t-Sahalia and Jacod (2014).
    The volatility process σ s is càdlàg, cs = (σσ | )s ∈ M+d , for any 0 ≤ s ≤ t. We denote d non-negative
eigenvalues of cs by λ1,s ≥ λ2,s ≥ . . . ≥ λd,s , summarized in a vector λs . As is discussed above, we sometimes
write λs = λ(cs ), regarding λ(·) as a function of cs . By Lemma 1, λ(·) is a continuous function so that λ(cs )
is càdlàg.


2.4     Principal Component Analysis at High Frequency
Similar to the classical PCA, the PCA procedure in this setting consists in searching repeatedly for instanta-
neous linear combinations of X, namely principal components, which maximize certain measure of variation,
while being orthogonal to the principal components already constructed, at any time between 0 and t. In
contrast to the classical setting, where one maximizes the variance of the combination (see, e.g., Anderson
(1958)), the criterion here is the continuous part of the quadratic variation.

                                                                   6
Lemma 3. Suppose that X is a d-dimensional vector-valued process described in (2). Then there exists a
sequence of {λg,s , γ g,s }1≤g≤d,0≤s≤t , such that

                              cs γ g,s = λg,s γ g,s ,   γ |g,s γ g,s = 1,   and   γ |h,s cs γ g,s = 0,

where λ1,s ≥ λ2,s ≥ . . . ≥ λd,s ≥ 0. Moreover, for any càdlàg and vector-valued adapted process γ s , such that
γ |s γ s = 1, and γ |s cs γ h,s = 0, 1 ≤ h ≤ g − 1,
                               Z u           Z u            Z u         c
                                                    |              |
                                   λg,s ds ≥      γ s− dXs ,     γ s− dXs , for any 0 ≤ u ≤ t.
                          0                   0               0

    When λg,s is a simple root of cs between 0 and t, then γ g,s is an adapted càdlàg process, due to the
                                                                                                    Rt
continuity of γ g (·) by Lemma 2, so that we can construct its corresponding principal component 0 γ |g,s− dXs .
                                                                                             Rt
    Given Lemma 3, we can define our parameters of interest: the realized eigenvalue, i.e. 0 λs ds; the realized
                                                                 Rt |
principal components associated with some simple root λg , i.e. 0 γ g,s− dXs . It may also be worth investigating
                                                                                            Rt
the average loading on the principal component, i.e. the realized eigenvector, which is 0 γ g,s ds. Since the
definitions of these quantities all rely on integrals, we call them integrated eigenvalues, integrated principal
components, and integrated eigenvectors.
                                                                       Rt
    The above analysis naturally leads us to consider inference for 0 λ(cs )ds, for which the differentiability
of λ(·) is critical: we start with the convergence of an estimator ĉs to cs , from which we need in a delta
method sense to infer the convergence of the integrated eigenvalues estimator. A simple eigenvalue is C ∞ -
differentiable, but for repeated eigenvalues this is not necessarily the case. For this reason, in order to fully
address the estimation problem, we need to introduce spectral functions.


2.5    Spectral Functions
A real-valued function F defined on a subset of M+     d is called a spectral function (see, e.g., Friedland (1981)) if
for any orthogonal matrix O in Md and X in M+                          |                               +
                                                     d , F (X) = F (O XO). We describe sets in Rd and functions
from R+       +
       d to R as symmetric if they are invariant under coordinate permutations. That is, for any symmetric
function f with an open symmetric domain in R+     d , we have f (x) = f (P x) for any permutation matrix P ∈ Md .
Associated with any spectral function F , we define a function f on R+        d , so that f (x) = F (Diag(x)). Since
permutation matrices are orthogonal, f is symmetric, and f ◦ λ = F , where ◦ denotes function composition.
    We will need to differentiate the spectral function F . We introduce a matrix function associated with the
corresponding symmetric function f of F , which, for any x ∈ R̄+      d in the form of (1), is given by
                              
                               0
                                                                    if p = q;
                  Afp,q (x) =      2
                                 ∂pp          2
                                     f (x) − ∂qq f (x)               if p 6= q, and p, q ∈ Il ;
                              
                                 (∂p f (x) − ∂q f (x))/(xp − xq ) otherwise.
                              

for some l = {1, 2, . . . , r}.
    The following lemma collects some known and useful results regarding the continuous differentiability and
convexity of spectral functions, which we will use below:

Lemma 4. The symmetric function f is twice continuously differentiable at a point λ(A) ∈ R̄+     d if and only
if the spectral function F = f ◦ λ is twice continuously differentiable at the point A ∈ M+
                                                                                          d . The gradient and
the Hessian matrix are given below:
                                    d
                                    X
                ∂jk (f ◦ λ)(A) =          Opj ∂p f (λ(A))Opk ,
                                    p=1


                                                                  7
                                  d
                                  X                                             d
                                                                                X
             2                             2
            ∂jk,lm (f ◦ λ)(A) =           ∂pq f (λ(A))Opl Opm Oqj Oqk +                 Afpq (λ(A))Opl Opj Oqk Oqm ,
                                  p,q=1                                         p,q=1


where O is any orthogonal matrix that satisfies A = O| Diag (λ(A)) O. More generally, f is a C k function at
λ(A) if and only if F is C k at A, for any k = 0, 1, . . . , ∞. In addition, f is a convex function if and only if
F is convex.

   Next, we note that both simple and repeated eigenvalues can be viewed as special cases of spectral functions.

Example 1 (A Simple Eigenvalue). Suppose the kth eigenvalue of A ∈ M+
                                                                    d is simple, that is, λk−1 (A) >
λk (A) > λk+1 (A). Define for any x ∈ R+
                                       d ,

                                      f (x) = the kth largest entry in x = x̄k .

Apparently, f is a symmetric function, and it is C ∞ at any point y ∈ R̄+d , with yk−1 > yk > yk+1 . Indeed,
∂f (y) = e , and ∂ f (y) = 0 for l ≥ 2. By Lemma 4, λk (A) = (f ◦ λ)(A) is C ∞ at A.
          k       l


Example 2 (Non-Simple Eigenvalues). Suppose the eigenvalues of A ∈ M+
                                                                    d satisfy:


                              λgl−1 (A) > λgl−1 +1 (A) ≥ . . . ≥ λgl (A) > λgl+1 (A),

for some 1 ≤ gl−1 < gl ≤ d. By convention, when gl = d, the last “ >” above is not used. Consider the
following function f , evaluated at x ∈ R+
                                         d , which is given by

                                                                      gl
                                                           1          X
                                             f (x) =                            x̄j .
                                                       gl − gl−1    j=gl−1 +1


It is easy to verify that f is symmetric, and C ∞ at any point y ∈ R̄+d that satisfies ygl−1 > ygl−1 +1 ≥ . . . ≥
                                     1
                                         Pgl         k       l
ygl > ygl+1 . Moreover, ∂f (y) = gl −gl−1 k=gl−1 +1 e , and ∂ f (y) = 0, for any l ≥ 2. As a result of Lemma
4, the corresponding spectral function,
                                                                            gl
                                                           1                X
                                  F (A) = (f ◦ λ)(A) =                                  λj (A),
                                                       gl − gl−1         j=gl−1 +1

is C ∞ at A. In the special case where

                              λgl−1 (A) > λgl−1 +1 (A) = . . . = λgl (A) > λgl+1 (A),

i.e., there is a repeated eigenvalue, and F (A) = λgl−1 +1 (A) = . . . = λgl (A). By contrast, λj (A) is not
differentiable, for any gl−1 + 1 ≤ j ≤ gl .

Example 3 (Trace and Determinant). For any x ∈ R+
                                                d , define

                                                 d
                                                 X                                d
                                                                                  Y
                                      f1 (x) =         xj ,   and    f2 (x) =           xj .
                                                 j=1                              j=1


Both f1 and f2 are symmetric and C ∞ . Therefore, for any A ∈ M+
                                                               d , Tr(A) = (f1 ◦ λ)(A) and det(A) =
                 ∞
(f2 ◦ λ)(A) are C at A.

   The previous examples make it clear that the objects of interest, eigenvalues, are special cases of spectral
functions, whether they are simple or repeated. (We are also able to estimate the trace and determinant “for


                                                              8
free”.) Lemma 4 links the differentiability of a spectral function F, which is needed for statistical inference,
to that of its associated symmetric function f . The key advantage is that differentiability of f is easier to
establish.
    Before turning to inference, we prove a useful result that characterizes the topology of the set of matrices
with special eigenvalue structures. The domain of spectral functions we consider will be confined to this set
in which these spectral functions are smooth.

Lemma 5. For any 1 ≤ g1 < g2 < . . . < gr ≤ d, the set

               M(g1 , g2 , . . . , gr ) = A ∈ M++
                                         
                                               d  | λgl (A) > λgl +1 (A), for any l = 1, 2, . . . , r − 1              (3)

is dense and open in M++     d . In particular, the set of positive-definite matrices with distinct eigenvalues, i.e.,
M(1, 2, . . . , d), is dense and open in M++d .

   We conclude this section with some additional notations. First, we introduce a symmetric open subset of
 +
R̄ /{0}, the image of M(g1 , g2 , . . . , gr ) under λ(·):

                  D(g1 , g2 , . . . , gr ) = x ∈ R+
                                            
                                                  d /{0} | x̄gl > x̄gl +1 , for any l = 1, 2, . . . , r − 1 .          (4)

We also introduce a subset of M(g1 , g2 , . . . , gr ), in which our spot covariance matrix ct will take values.
                                 n
     M∗ (g1 , g2 , . . . , gr ) = A ∈ M++
                                       d  |
                                                                                                                o
      λ1 (A) = . . . = λg1 (A) > λg1 +1 (A) = . . . = λg2 (A) > . . . λgr−1 (A) > λgr−1 +1 (A) = . . . = λgr (A) .

Finally, we introduce the following set, which becomes relevant if we are only interested in a simple eigenvalue
λg :

                  M(g) = A ∈ M++
                        
                              d  | λg−1 (A) > λg (A) > λg+1 (A) , and D(g) = λ(M(g)).

By convention, we ignore the first (resp. second) inequality in the definition of M(g) when g = 1 (resp. g = d).


3      Estimators and Asymptotic Theory
3.1      Assumptions
We start with the standard assumption on the process X:2

Assumption 1. The drift terms bt is progressively measurable and locally bounded. The spot covariance matrix
ct = (σσ | )t is an Itô semimartingale. Moreover, for any γ ∈ [0, 1), there is a sequence of stopping times (τ n )
                                                             R           γ
increasing to ∞, and a deterministic function δ̄ n such that Rd δ̄ n (x) ν̄ (dx) < ∞ and that kδ(ω, t, x)k ∧ 1 ≤
δ̄ n (x), for all (ω, t, x) with t ≤ τ n (ω).
                                                                                                      Rt
      In view of the previous examples, our main theory is tailored to the statistical inference on 0 F (cs )ds.
Thanks to Lemma 4, we can make assumptions directly on f instead of F , which are much easier to verify.

Assumption 2. Suppose F is a vector-valued spectral function, and f is the corresponding vector-valued
                                                                                                     ζ
symmetric function such that F = f ◦ λ. f is a continuous function, and satisfies kf (x)k ≤ K(1 + kxk ), for
some ζ > 0.
    2 Using
          the notation on Page 583 of Jacod and Protter (2011), Assumption 1 states that the process X satisfies Assumption
(H-1) and that ct satisfies Assumption (H-2).


                                                              9
   In all the examples of Section 2.5, this assumption holds. By Lemma 1 and Assumption 2, F (cs ) is càdlàg,
                 Rt
and the integral 0 F (cs )ds is well-defined.
   The above assumptions are sufficient to ensure the desired consistency of estimators we will propose.
Additional assumptions are required to establish their asymptotic distribution.

Assumption 3. There exists some open and convex set C, such that its closure C¯ ⊂ M(g1 , g2 , . . . , gr ), where
1 ≤ g1 < g2 < . . . < gr ≤ d, and that for any 0 ≤ s ≤ t, cs ∈ C ∩ M∗ (g1 , g2 , . . . , gr ). Moreover, f is C 3 on
D(g1 , g2 , . . . , gr ).

    Assumption 3, in particular, cs ∈ M∗ (g1 , g2 , . . . , gr ), guarantees that different groups of eigenvalues do not
cross over within [0, t]. This condition is mainly used to deliver the joint central limit theorem for spectral
functions that depend on all eigenvalues, although it may not be necessary for some special cases, such as
det(·) and Tr(·), which are smooth everywhere. The convexity condition on C is in principle not difficult to
satisfy, given that M(g1 , g2 , . . . , gr ) can be embedded into some Euclidean space of real vectors. This condition
is imposed to ensure that the domain of the spectral function can be restricted to certain neighborhood of
{cs }0≤s≤t , in which the function is smooth and the mean-value theorem can be applied. This assumption is
not needed if ct is continuous.
    It is also worth mentioning that all eigenvalues of ct being distinct is a special case, which is perhaps
the most relevant scenario in practice, as is clear from Lemma 5. Even in this scenario, Assumption 3 is
required (with g1 , g2 , . . . , gr being chosen as 1, 2, . . . , d), because the eigenvalue function λ(·) is not everywhere
differentiable.
    Assumption 3 resembles the spacial localization assumption in Li, Todorov, and Tauchen (2014) and Li
and Xiu (2014), which is different from the polynomial growth conditions proposed by Jacod and Rosenbaum
(2013). The growth conditions are not satisfied in our setting, when the difference between two groups of
repeated eigenvalues approaches zero.
    If we are only interested in the spectral function that depends on one simple eigenvalue, e.g., the largest
                                            Rt
and simple integrated eigenvalue, 0 λ1 (cs )ds, then it is only necessary to ensure that λ1 (cs ) > λ2 (cs ), for
0 ≤ s ≤ t, regardless of whether the remaining eigenvalues are simple or not. We thereby introduce the
following assumption for this scenario, which is much weaker than Assumption 3.

Assumption 4. There exists some open and convex set C, such that C¯ ⊂ M(g), for some g = 1, 2, . . . , d, and
that for any 0 ≤ s ≤ t, cs ∈ C. Moreover, f is C 3 on D(g).


3.2    Realized Spectral Functions
We now turn to the construction of the estimators. To estimate the integrated spectral function, we start with
estimation of the spot covariance matrix. Suppose we have equidistant observations on X over the interval
[0, t], separated by a time interval ∆n . We form non-overlapping blocks of length kn ∆n . At each ikn ∆n , we
estimate cikn ∆n by
                                          kn
                                     1 X                           |
                                              ∆nikn +j X ∆nikn +j X 1{k∆n
                                                        
                         cikn ∆n =                                                      ,                   (5)
                                                                        ikn +j X k≤un }
                         b
                                   kn ∆n j=1

where un = α∆$            n
                n , and ∆l X = Xl∆n − X(l−1)∆n . Choices of α and $ are standard in the literature (see, e.g.,
Aı̈t-Sahalia and Jacod (2014)) and are discussed below when implemented in simulations.
    We then estimate eigenvalues of b  cikn ∆n by solving for the roots of |b
                                                                            cikn ∆n − λI| = 0. Using the notation
in Lemma 1, we have λ(b  cikn ∆n ) = λ
                                     bik ∆ . Almost surely, the eigenvalues stacked in λ
                                        n n
                                                                                          bik ∆ are distinct (see,
                                                                                             n n




                                                            10
e.g., Okamoto (1973)) so that we have λ  b1,ik ∆ > λ
                                              n n
                                                   b2,ik ∆ > . . . > λ
                                                        n n
                                                                     bd,ik ∆ . Our proposed estimator of the
                                                                          n n

integrated spectral function is then given by3
                                                                              [t/(kn ∆n )]                  
                                                                                     X
                                             V (∆n , X; F ) = kn ∆n                                f λbik ∆ .
                                                                                                         n n                                        (6)
                                                                                     i=0

   We start by establishing consistency of this estimator:
                                                                                     ζ−1) 1
Theorem 1. Suppose Assumptions 1 and 2 hold. Also, either ζ ≤ 1, or ζ > 1 with $ ∈ [ 2ζ−γ , 2 ) holds. Then
the estimator (6) is consistent. As kn → ∞ and kn ∆n → 0,
                                                                                     Z       t
                                                                            u.c.p.
                                                      V (∆n , X; F ) =⇒                          F (cs )ds.                                         (7)
                                                                                         0

    Next, obtaining a central limit theorem for the estimator is more involved, as there is a second-order
asymptotic bias associated with the estimator (6), a complication that is also encountered in other situations
such as the quarticity estimator in Jacod and Rosenbaum (2013), the GMM estimator in Li and Xiu (2014),
or the leverage effect estimator by Kalnina and Xiu (2013). The bias here is characterized as follows:

Proposition 1. Suppose Assumptions 1, 2, and 3 hold. In addition, kn  ∆−ς          $
                                                                        n and un  ∆n for some
      γ 1               1−ς 1
ς ∈ ( 2 , 2 ) and $ ∈ [ 2−γ , 2 ). As ∆n → 0, we have
                                   Z    t                             d            Z       t
                                                        p 1             X
                                                                                                  2
          kn V (∆n , X; F ) −                F (cs )ds −→                                        ∂jk,lm F (cs ) (cjl,s ckm,s + cjm,s ckl,s ) ds.    (8)
                                     0                    2                              0
                                                                     j,k,l,m=1

   The characterization of the bias in (8) suggests a bias-corrected estimator as follows:
                                                           [t/(kn ∆n )] n
                                                              X
                      Ve (∆n , X; F ) = kn ∆n                           F (b
                                                                           cikn ∆n )                                                                (9)
                                                               i=0
                                  d
                           1      X
                                               2
                                                                                                                  o
                      −                       ∂jk,lm F (b
                                                        cikn ∆n ) (b
                                                                   cjl,ikn ∆n b
                                                                              ckm,ikn ∆n + b
                                                                                           cjm,ikn ∆n b
                                                                                                      ckl,ikn ∆n ) .
                          2kn
                                j,k,l,m=1

   We then derive the asymptotic distribution of the bias-corrected estimator:
                                                                                                 γ 1
Theorem 2. Suppose Assumptions 1, 2, and 3 hold. In addition, kn  ∆−ς          $
                                                                    n and un  ∆n for some ς ∈ ( 2 , 2 )
          1−ς 1
and $ ∈ [ 2−r , 2 ). As ∆n → 0, we have
                                                                              Z     t            
                                              1                                                     L−s
                                         √                Ve (∆n , X; F ) −              F (cs )ds −→ Wt ,                                         (10)
                                              ∆n                                 0

where W is a continuous process defined on an extension of the original probability space, which conditionally
on F, is continuous centered Gaussian martingale with its covariance given by
                                              Z   t       d
                                                          X
                   E(Wp,t Wq,t |F) =                             ∂jk Fp (cs )∂lm Fq (cs ) (cjl,s ckm,s + cjm,s ckl,s ) ds.                         (11)
                                                0 j,k,l,m=1


Remark 1. As previously noted, in the case when the spectral function F only depends on a simple eigenvalue
λg , the same result holds under the weaker Assumption 4 instead of 3.
  3 We  prefer this estimator to the alternative one using overlapping windows, because the overlapping implementation runs
much slower. In a finite sample, both estimators have a decent performance.


                                                                            11
   A feasible implementation of this distribution requires an estimator of the asymptotic variance, which we
construct as follows:

Proposition 2. The asymptotic variance of Ve (∆n , X; F ) can be estimated consistently by:
                    [t/(kn ∆n )]     d
                       X             X
            kn ∆n                              ∂jk Fp (b
                                                       cikn ∆n )∂lm Fq (b
                                                                        cikn ∆n ) (b
                                                                                   cjl,ikn ∆n b
                                                                                              ckm,ikn ∆n + b
                                                                                                           cjm,ikn ∆n b
                                                                                                                      ckl,ikn ∆n )
                       i=0         j,k,l,m=1
            Z   t   d
        p           X
       −→                    ∂jk Fp (cs )∂lm Fq (cs ) (cjl,s ckm,s + cjm,s ckl,s ) ds.                                               (12)
             0 j,k,l,m=1



3.3    Realized Eigenvalues
We next specialize the previous theorem to obtain a central limit theorem for the realized eigenvalue estimators.
With the structure of eigenvalues in mind, we use a particular vector-valued spectral function F λ , tailor-made
to deliver the asymptotic theory we need for realized eigenvalues:
                                                                                                  |
                                  g1                   g2                                gr
                              1  X              1     X                         1       X
                  F λ (·) =         λj (·),                λj (·), . . . ,                  λj (·) .       (13)
                              g1 j=1         g2 − g1 j=g +1                 gr − gr−1 j=g +1
                                                                       1                                     r−1



Apparently, if a group contains only one λj , then the corresponding entry of F λ is equal to this single
eigenvalue; if within certain group, all eigenvalues are identical, then the corresponding entry of F λ yields the
common eigenvalue of the group.
                                                              γ 1
Corollary 1. Suppose kn  ∆−ς                $                                 1−ς 1
                              n and un  ∆n for some ς ∈ ( 2 , 2 ) and $ ∈ [ 2−r , 2 ).
(i) Under Assumption 1, the estimator of integrated eigenvalue vector given by
                                                                                [t/(kn ∆n )]
                                                                                        X
                                                V (∆n , X; λ) = kn ∆n                          λ(b
                                                                                                 cikn ∆n )                           (14)
                                                                                        i=0

is consistent.
(ii) If Assumption 3 further holds, the estimator corresponding to the pth entry of F λ given by (9) can be
written explicitly as:
                                      [t/(kn ∆n )]      gp                                                                          
                         k n ∆n          X              X
                                                                     bh,ik ∆ − 1 Tr (λ
                                                                                                                  
 Ve (∆n , X; Fpλ ) =                                                 λ    n n             n n
                                                                                                 cikn ∆n )+ b
                                                                                     bh,ik ∆ I − b          cikn ∆n λbh,ik ∆
                                                                                                                          n n
                                                                                                                                         .
                       gp − gp−1          i=0
                                                                               kn
                                                     h=gp−1 +1

                                                                                                   |
The joint central limit theorem for Ve (∆n , X; F λ ) = Ve (∆n , X; F1λ ), . . . , Ve (∆n , X; Frλ ) is given by
                                                                           Z       t              
                                             1                                                       L−s
                                         √            Ve (∆n , X; F λ ) −               F λ (cs )ds −→ Wtλ ,                         (15)
                                             ∆n                                 0

where W λ is a continuous process defined on an extension of the original probability space, which conditionally
on F, is continuous centered Gaussian martingale with a diagonal covariance matrix given by
                                  2 Rt 2                                                       
                                    g1 0 λg1 ,s ds
                                                      2
                                                         R t 2
                                                   g2 −g1 0 λg2 ,s ds
                                                                                               
                   λ   λ |
                                                                                               
              E(Wt (Wt ) |F) =  
                                                                      ..
                                                                                                ,          (16)
                                                                         .
                                                                                                
                                                                                               
                                                                               2
                                                                                    Rt 2
                                                                           gr −gr−1 0 λgr ,s ds


                                                                           12
where F λ (cs ) = (λg1 ,s , λg2 ,s , . . . , λgr ,s )| .
(iii) Under Assumptions 1 and 4, our estimator with respect to the gth simple eigenvalue λg (·), is given by

                                    [t/(kn ∆n )]                                                                         
                                                     bg,ik ∆ − 1 Tr (λ
                                        X                                                        
                                                                     bg,ik ∆ − b        +
         Ve (∆n , X; λg ) = kn ∆n                    λ    n n             n n
                                                                               cik  ∆
                                                                                   n n
                                                                                       )  c
                                                                                          bik  ∆
                                                                                              n n
                                                                                                    λ
                                                                                                    bg,ik ∆
                                                                                                         n n
                                                                                                                              ,   (17)
                                         i=0
                                                               kn

which satisfies:                                                     Z   t              
                                        1                                                  L−s λ
                                    √            Ve (∆n , X; λg ) −           F λ (cs )ds −→ Wt g ,                               (18)
                                        ∆n                            0

where W λg is a continuous process defined on an extension of the original probability space, which conditionally
                                                                        Rt
on F, is a continuous centered Gaussian martingale with its variance 0 λ2g,s ds.

Remark 2. Corollary 1 is the analogue in our context to the classical results on asymptotic theory for PCA
by Anderson (1963), who showed that
                                        √               d
                                              b − λ) → N 0, 2Diag λ2 , λ2 , . . . , λ2
                                                                                             
                                            n(λ                    1    2            d            .                               (19)

where λb and λ are the vectors of eigenvalues of the sample and population covariance matrices and λ is simple.
Note the similarity between (19) and (16) in the special case where the eigenvalues are simple (in which case
gi = i) and are constant instead of being stochastic. The classical setting requires the strong assumption that
the covariance matrix follows a Wishart Distribution. By contrast, our results are fully nonparametric, relying
instead on high frequency asymptotics.

Remark 3. If there are eigenvalues of cs that are equal to 0 for any 0 ≤ s ≤ t, then our estimator is super-
consistent. This is an interesting case as the rank of the covariance matrix is determined by the number of
non-zero eigenvalues, see, e.g., the rank inference in Jacod, Lejay, and Talay (2008) and Jacod and Podolskij
(2013).


3.4    Realized Principal Components
As we have remarked before, when eigenvalues are simple, the corresponding eigenvectors are uniquely deter-
mined. Therefore, we can estimate the instantaneous eigenvectors together with eigenvalues, which eventually
leads us to construct the corresponding principal component:

Proposition 3. Suppose Assumptions 1 and 4 hold. In addition, γ g,s is a vector-valued function that corre-
sponds to the eigenvector of cs with respect to a simple root λg,s . Suppose kn  ∆−ς          $
                                                                                   n and un  ∆n for some
ς ∈ ( γ2 , 12 ) and $ ∈ [ 2−r
                          1−ς 1
                              , 2 ). Then we have as ∆n → 0

                      [t/(kn ∆n )]−1                                                             Z    t
                           X                                                             u.c.p
                                        b|g,(i−1)kn ∆n (X(i+1)kn ∆n − Xikn ∆n ) =⇒
                                        γ                                                                 γ |g,s− dXs .
                           i=1                                                                    0


3.5    Realized Eigenvectors
So far we have constructed the principal components and estimated integrated eigenvalues. It remains to figure
out the loading of each entry of X on each principal component, i.e., the eigenvector. As the eigenvector is
stochastic, we estimate the integrated eigenvector. Apart from its sign, an eigenvector is uniquely identified if
its associated eigenvalue is simple. We determine the sign hence identify the eigenvector, by requiring a priori
certain entry of the eigenvector to be positive, e.g., the first non-zero entry.


                                                                  13
Corollary 2. Suppose Assumptions 1 and 4 hold. In addition, γ g,s is a vector-valued function that corresponds
to the eigenvector of cs with respect to a simple root λg,s , for each s ∈ [0, t]. Then we have as ∆n → 0
                                                                                                      
                   [t/(kn ∆n )]                                                              Z t
       1              X                       1   X        λ
                                                            bg,ik ∆ λbp,ik ∆                               L−s
     √      k n ∆n              γbg,ikn ∆n +                    n n      n n
                                                                                γ         −     γ g,s ds −→ Wtγ ,
                                                                      bp,ik ∆ )2 g,ikn ∆n
                                                                                b
       ∆n              i=0
                                              2k n      (λbg,ik ∆ − λ                         0
                                                   p6=g        n n         n n



where W γ is a continuous process defined on an extension of the original probability space, which conditionally
on F, is a continuous centered Gaussian martingale with its covariance matrix given by
                                              Z t
                          E(Wtγ (Wtγ )| |F) =     λg,s (λg,s I − cs )+ cs (λg,s I − cs )+ ds.
                                                              0


3.6    “PCA” on the Integrated Covariance Matrix
One may wonder why we chose to estimate the integrated eigenvalues of the spot covariance matrix rather
than the eigenvalues of the integrated covariance matrix. Because eigenvalues are complicated functionals of
the covariance matrix, the two quantities are not much related, and it turns out that the latter procedure is
less informative than the former. For instance, the rank of the instantaneous covariance matrix is determined
by the number of non-zero instantaneous eigenvalues. The eigenstructure of the integrated covariance matrix
is rather opaque due to the aggregation of instantaneous covariances. Nevertheless, it is possible to construct
a “PCA” procedure on the integrated covariance matrix, with the following results:
                                                                              Rt
Proposition 4. Suppose the eigenvalues of the integrated covariance matrix 0 cs ds are given by

         λ1 = . . . = λg1 > λg1 +1 = . . . = λg2 > . . . λgr−1 > λgr−1 +1 = . . . = λgr > 0,              for 1 ≤ r ≤ d.

Then, we have, for $ ∈ [1/(4 − 2γ), 1/2),
                                                                                   
                           [t/∆n ]                                        Z t      
                1  λ X                                                                L−s
              √      F            (∆ni X)(∆ni X)| 1{k∆n X k≤α∆$ }  − F λ      cs ds  −→ W̄tλ ,
                ∆n           i=0
                                                      i       n
                                                                            0


where W̄ λ is a continuous process defined on an extension of the original probability space, which conditionally
on F, is a continuous centered Gaussian martingale with its covariance matrix given by
                                d
                                X                   Z   t         Z t                                       Z t      
            λ     λ
        E(W̄i,t W̄j,t |F) =               ∂uv Fiλ            cs ds       (cuk,s cvl,s + cul,s cvk,s )ds ∂kl Fjλ      cs ds ,
                              u,v,k,l=1              0                 0                                         0
                                              gi
                                  1           X
             ∂uv Fiλ (A) =                               Oku Okv .
                              gi − gi−1
                                           k=gi−1 +1


where O is any orthogonal matrix that satisfies A = O| Diag (λ(A)) O.

   Similarly, we have:

Proposition 5. Suppose $ ∈ [1/(4−2γ), 1/2). For the eigenvector γ g corresponding to some simple eigenvalue
     Rt
λg of 0 cs ds,
                                                                                 
                         [t/∆n ]                                        Z t      
                 1   X                                                              L−s
               √     γg         (∆ni X)(∆ni X)| 1{k∆n X k≤α∆$ }  − γ g      cs ds  −→ W̄tγ ,
                 ∆n        i=0
                                                    i       n
                                                                          0




                                                                      14
where W̄ γ is a continuous process defined on an extension of the original probability space, which conditionally
on F, is a continuous centered Gaussian martingale with its covariance matrix given by
                                  d
                                  X                    Z     t         Z t                                        Z t      
          γ      γ |
      E(W̄i,t (W̄j,t ) |F) =                ∂uv γ gi              cs ds       (cuk,s cvl,s + cul,s cvk,s )ds ∂kl γ gj      cs ds ,
                                u,v,k,l=1                 0                   0                                        0

                                                 +
               ∂kl γ gi (A) = (λg (A)I −       A)ik    γ gl (A).

   The corresponding principal component is then defined as γ |g (Xt − X0 ). However, this eigenvalue and the
principal component do not satisfy the fundamental relationship between the eigenvalue and the variance of
the principal component:         Z t      
                                                                               c
                                      cs ds 6= γ |g (Xt − X0 ), γ |g (Xt − X0 ) ,
                                              
                             λg                                                                          (20)
                                              0

which is a desired property of any sensible PCA procedure. This fact alone makes this second procedure much
less useful than the one we proposed above, based on integrated eigenvalues of the spot covariance. Another
feature that distinguishes this second “PCA” procedure with both the classical one and our realized PCA is
                                                           Rt
that the asymptotic covariance matrix of eigenvalues of 0 cs ds is no longer diagonal.
    An analogy of the relationship between the two PCA procedures is that between integrated quarticity
  Rt 4                                            Rt
t 0 σ s ds and the squared integrated volatility ( 0 σ 2s ds)2 . If the covariance matrix is constant, the two
procedures are equivalent but not in general.


4    Simulation Evidence
We now investigate the small sample performance of the estimators in conditions that approximate the em-
pirical setting of PCA for large stock portfolios. In order to generate data with a common factor structure,
we simulate a factor model in which a vector of log-stock prices X follows the continuous-time dynamics

                                                            dXt = β t dFt + dZt ,                                                    (21)

where F is a collection of unknown factors, β is the matrix of factor loadings, and Z is a vector of idiosyncratic
components, which is orthogonal to F . This model is a special case of the general semimartingale model (2).
   By construction, the continuous part of the quadratic variation has the following local structure:

                                            [dX, dX]ct = β t [dF, dF ]ct β |t + [dZ, dZ]ct .

When the idiosyncratic components have smaller magnitude, the dominating eigenvalues of X are close to
the non-zero eigenvalues of β t [dF, dF ]ct β |t , by Weyl’s inequality. As a result, we should be able to detect the
number of common factors in F from observations on X. That said, our goal here is to conduct nonparametric
PCA rather than to estimate a parametric factor model, for which we need to resort to a large panel of X,
whose dimension increases with the sample size, see, e.g., Bai and Ng (2002), and so (21) is only employed
as the data-generating process on which PCA is employed without any knowledge of the underlying factor
structure.
    Specifically, we simulate
                    r
                    X
                                                                                       F                                 Z
          dXi,t =         β ij,t dFj,t + dZi,t ,       dFj,t = µj dt + σ j,t dWj,t + dJj,t ,       dZi,t = γ t dBi,t + dJi,t ,
                    j=1




                                                                         15
where i = 1, 2, . . . , d, and j = 1, 2, . . . , r. In the simulations, one of the F s plays the role of the market
factor, so that its associated βs are positive. The correlation matrix of dW is denoted as ρF . We allow for
time-varying σ j,t , γ t , and β ij,t , which evolve according to the following system of equations:
                                                           fj,t + dJ σ2 , dγ 2 = κ(θ − γ 2 )dt + ηγ t dB̄t ,
                dσ 2j,t = κj (θj − σ 2j,t )dt + η j σ j,t dW         j,t     t            t
                          
                          κ̃ (θ̃ − β )dt + ξ̃ β dB      p
                              j ij         ij,t        j     ij,t
                                                                  eij,t if the jth factor is the “market”,
               dβ ij,t =                                                                                     ,
                          κ̃j (θ̃ij − β )dt + ξ̃ dB       eij,t         otherwise.
                                           ij,t        j


where the correlation between dWj,· and dW      fj,· is ρj , {J F }1≤j≤r and {J Z }1≤i≤d are driven by two Poisson
                                                               j                 i
                                F         Z
Processes with arrival rates λ and λ , respectively. Their jump sizes follow double exponential distributions
                                                                    σ2
with means denoted by µF        F    Z        Z                                               F
                           + , µ− , µ+ , and µ− , respectively. {Jj }1≤j≤r co-jumps with J , and their jump sizes
                                                                 σ2
follow exponential distributions with the mean equal to µ . All the model parameters are given in Table 1.
They are chosen to be realistic given the empirical characteristics of the cross-section of stock returns and
their volatilities.
    Anticipating our empirical application to the S&P 100 Index constituents, we simulate intraday returns
of d = 100 stocks at various frequencies and with horizons T spanning 1 week to 1 month. When r = 3, the
model implies 3 distinct eigenvalues reflecting the local factor structure of the simulated data. The remaining
population eigenvalues are identical, due to idiosyncratic variations. Throughout, we fix kn to be the closest
                           −1/2 p                                                                           p
divisors of [t/∆n ] to θ∆n         log(d), with θ = 0.5 and d is the dimension of X. Our choice of log(d)
is motivated from the literature on high-dimensional covariance matrix estimation, although our asymptotic
design does not take into account an increasing dimensionality. Other choices of θ, e.g., 0.05 - 0.5, or functions
            √
of d, e.g., d log d, deliver the same results. To truncate off jumps from spot covariance estimates, we adopt
                                                                  Rt                                       Rt
the usual procedure in the literature, i.e., choosing ui,n = 3( 0 cii,s ds/t)0.5 ∆0.47
                                                                                   n , for 1 ≤ i ≤ d, where 0 cii,s ds
can be estimated by, for instance, bipower variations.
    We then apply the realized PCA procedure. We first examine the curse of dimensionality – how increasing
number of stocks affects the estimation – and how the sampling frequency affects the estimation. In light
of Corollary 1, we estimate 3 simple integrated eigenvalues as well as the average of the remaining identical
                          Rt
eigenvalues, denoted as 0 λis ds, i = 1, 2, 3, and 4. We report the mean and standard errors of the estimates
as well as the root-mean-square errors of the standardized estimates with d = 5, 10, 15, 20, 30, 50, and 100
stocks, respectively, using returns sampled every ∆n = 5 seconds, 1 minute and 5 minutes over one week and
one month horizons. The results, as shown from Tables 2 - 5, suggest that, as expected, the estimation is more
difficult as the dimensionality increases, but the large amount of high frequency data and in-fill asymptotic
techniques deliver very satisfactory finite sample approximations. The eigenvalues are accurately recovered.
The repeated eigenvalues are estimated with smaller biases and standard errors, due to the extra averaging
taken at the estimation stage. In Tables 6 - 7, we provide estimates for the first eigenvectors. Similar to the
estimation for eigenvalues, the estimates are very accurate, even with 100 stocks.
    To further verify the accuracy of the asymptotic distribution in small samples as the sample size increases,
we provide in Figure 1 histograms of the standard estimates of integrated eigenvalues using 30 stocks with
5-second returns. Finally, we examine the finite sample accuracy of the integrated simple eigenvalues, in the
more challenging scenario with 100 stocks sampled every minute, which matches the setup of the empirical
analysis we will conduct below. To verify that the detection of 3 eigenvalues is not an artifact, we vary the
number of common factors in the data generating process from r = 2 to 4, by removing the third factor or
adding another factor that shares the same parameters as the third factor. The histograms are provided in
Figure 2. The results collectively show that the finite sample performance of the method is quite good even
for a 100-stock portfolio with one-minute returns and one-week horizon.


                                                             16
5     High-Frequency Principal Components in the S&P 100 Stocks
Given the encouraging Monte Carlo results, we now conduct PCA on intraday returns of S&P 100 Index
(OEX) constituents. We collect stock prices over the 2003 - 2012 period from the Trade and Quote (TAQ)
database of the New York Stock Exchange (NYSE).4 Due to entry and exit from the index, there are in total
158 tickers over this 10-year period. We conduct PCA on a weekly basis. One of the key advantages of the
large amount of high frequency data is that we are effectively able to create a ten-year long time series of
eigenvalues and principal components at the weekly frequency by collating the results obtained over each week
in the sample.
    After removing those weeks during which the index constituents are switching, we are left with 482 weeks.
To clean the data, for each ticker on each trading day, we only keep the intraday prices from the single
exchange which has the largest number of transaction records. We provide in Figure 3 the quantiles of the
number of transactions between 9:35 a.m. EST and 3:55 p.m. EST, across 100 stocks each day. These stocks
have excellent liquidity over the sampling period. We thereby employ 1-minute subsamples for the most liquid
90 stocks in the index (for simplicity, we still refer to this 90-stock subsample as the S&P 100) in order to
address any potential concerns regarding microstructure noise and asynchronous trading.5 These stocks trade
multiple times per sampling interval and we compute returns using the latest prices recorded within each
sampling interval. Overnight returns are removed so that there is no concern of price changes due to dividend
distributions or stock splits. The 158 time series of cumulative returns are plotted in Figure 4.


5.1     Scree Plot
We report the time series average of the eigenvalue estimates against their order in Figure 5, a plot known in
classical PCA analysis as the “scree plot” (see e.g. Jolliffe (2002)). This plot classically constitutes the main
graphical aid employed to determine the appropriate number of components in empirical applications. The
graph reveals that there are a handful of eigenvalue estimates, three, which are separated from the rest. The
fact that three factors on average explain the cross-sectional variation of stock returns is surprisingly consistent
with the well-established low frequency Fama-French common factor analysis, despite the large differences in
methods, time periods, sample frequencies and length of observation.
    We also plot the estimates of the percentage variation explained by the first three integrated eigenvalues in
Figure 6, along with the average variation explained by the remaining eigenvalues. There are substantial time
variation of the first three eigenvalues, all of which are at peak around the recent financial crisis, indicating
an increased level of comovement, which in this context is the definition of systemic risk. The idiosyncratic
factors become relatively less important and even more dominated by the common factors during the crisis.
    The first eigenvalue accounts for on average 30-40% of the total variations of the 90 constituents, capturing
the extent of the market variation in the sample. The second and third components together account for an
additional 15%-20% of the total variation. Therefore, there exists significant amount of remaining idiosyncratic
variation, beyond what can be explained by a three-common-factor model. The average variation explained
by the remaining 87 components are around 0.4%-0.6%, which corresponds to the idiosyncratic contributions
relative to the first three principal components.
    4 While the constituents of the OEX Index change over time, we keep track of the changes to ensure that our choice of stocks

is always in line with the index constituents.
    5 Estimators that are robust to both noise and asynchronicity are available for integrated covariance estimation in Aı̈t-Sahalia,

Fan, and Xiu (2010), Christensen, Kinnebrock, and Podolskij (2010), Barndorff-Nielsen, Hansen, Lunde, and Shephard (2011),
and Shephard and Xiu (2012). Extending the current method to a noisy and asynchronous setting is theoretically interesting but
not empirically necessary for the present paper; it is left for future work.



                                                                 17
    Next, we compare the cumulative returns of the first three cumulative principal components. The time
series plots are shown in Figure 7. The empirical correlations among the three components are -0.020, 0.005,
and -0.074, respectively, which agrees with the design that the components are orthogonal. The first principal
component accounts for most of the common variation, and it shares the time series features of the overall
market return. This is further reinforced by the fact that the loadings of all the stocks on the first principal
component, although time-varying, remain positive throughout the sample period, which is not the case for
the additional principal components. It is worth pointing out however that these principal components are
not constrained to be portfolio returns, as their weights at each point in time are nowhere constrained to add
up to one. This means that the first principal component cannot be taken directly to be the market portfolio,
or more generally identified with additional Fama-French or additional mimicking portfolios.


5.2    Biplots
Although PCA is by design not suited to identifying the underlying economic factors corresponding to the
principal components, the time series evidence in Figure 7 suggests that the first principal component captures
the features of the overall market return, as already noted. Can we extract more information about the
economic nature of the principal components? For this purpose, we can use our results to produce biplots (see
Gabriel (1971)) and see which assets line up together in the basis of the principal components. A biplot gives
the relative position of the d variables on a plot where two principal components are the axes. The length of a
vector from the origin pointing towards the position of a variable on the biplot tends to reflect the magnitude
of the variation of the variable. Moreover, vectors associated with similar variables tend to point towards the
same direction, hence a biplot can be used for classification of the assets. While it is possible to compute
biplots for the spot eigenvectors, the biplots for the integrated eigenvectors are more stable.
    We report two representative snapshots of the biplots of the first two integrated eigenvectors in Figures
8 (March 7-11, 2005, preceding the crisis) and 9 (March 10-14, 2008, during the crisis, although not at its
most extreme), in order to interpret the second principal component. We identify differently in the figures
the vectors that correspond to financial stocks (identified using the Global Industrial Classification Standard
(GICS) codes). The Federal Reserve made several important announcement during the week of March 10
-14, 2008 to address heightened liquidity pressures of the financial system, including the creation of the Term
Securities Lending Facility, and the approval of a financing arrangement between J.P. Morgan Chase and Bear
Stearns.
    We find that the financial sector is clearly separated from the rest of the stocks during March 10-14, 2008,
in sharp contrast with the biplot for the week of March 7 - 11, 2005, during which no such separation occurs.
This suggests that PCA based on low-frequency data is likely to overlook these patterns. Interestingly, we can
also see from the March 2008 biplot that the Lehman Brothers (LEH) stands out, having the largest loadings
on both components. The corresponding scree plots for these two weeks in Figures 10 and 11 both suggest
the presence of at least three factors, but the eigenvalues in March 10-14, 2008 are much larger in magnitude,
consistently with a higher systematic component to asset returns. For each week, the figures report 95%
confidence intervals for the first three eigenvalues computed based on the distribution given in Corollary 1.


6     Conclusions
This paper develops the tools necessary to implement PCA at high frequency, constructing and estimating
realized eigenvalues, eigenvectors and principal components. This development complements the classical
PCA theory in a number of ways. Compared to its low frequency counterpart, PCA becomes feasible over


                                                      18
short windows of observation (of the order of one week), relatively large dimensions (90 in our application)
and further are free from the need to impose strong parametric assumptions on the distribution of the data,
applying instead to a broad class of semimartingales. The estimators perform well in simulations and reveal
that the joint dynamics of the S&P 100 stocks at high frequency are well explained by a three-factor model,
a result that is broadly consistent with the Fama-French factor model at low frequency, surprisingly so given
the large differences in time scale, sampling and returns horizon.
    This paper represents a necessary first step to bring PCA tools to a high frequency setting. Although not
empirically relevant in the context of the analysis above of a portfolio of highly liquid stocks at the one-minute
frequency, the next steps in the development of the theory will likely include the incorporation of noise-robust
and asynchronicity-robust covariance estimation methods. We hope to pursue these extensions in future work.




                                                       19
References
Aı̈t-Sahalia, Y., J. Fan, and D. Xiu (2010): “High-Frequency Covariance Estimates with Noisy and
  Asynchronous Data,” Journal of the American Statistical Association, 105, 1504–1517.

Aı̈t-Sahalia, Y., and J. Jacod (2014): High Frequency Financial Econometrics. Princeton University Press.

Amini, A. A., and M. J. Wainwright (2009): “High-Dimensional Analysis of Semidefinite Relaxations for
 Sparse Principal Components,” Annals of Statistics, 37(5B), 2877–2921.

Anderson, T. W. (1958): An Introduction to Multivariate Statistical Analysis. Wiley, New York.

         (1963): “Asymptotic Theory for Principal Component Analysis,” Annals of Mathematical Statistics,
  34, 122–148.

Bai, J. (2003): “Inferential Theory for Factor models of Large Dimensions,” Econometrica, 71, 135–171.

Bai, J., and S. Ng (2002): “Determining the Number of Factors in Approximate Factor Models,” Econo-
 metrica, 70, 191–221.

Bai, Z., J. W. Silverstein, and Y. Q. Yin (1988): “A Note on the Largest Eigenvalue of a Large Dimen-
 sional Covariance Matrix,” Journal of Multivariate Analysis, 26, 166–168.

Baker, M., and J. Wurgler (2006): “Investor Sentiment and the Cross-Section of Stock Returns,” The
 Journal of Finance, 61(4), 1645–1680.

Baker, S. R., N. Bloom, and S. J. Davis (2013): “Measuring Economic Policy Uncertainty,” Discussion
 paper, Stanford University and University of Chicago.

Ball, J. M. (1984): “Differentiability Properties of Symmetric and Isotropic Functions,” Duke Mathematical
 Journal, 51, 699–728.

Barndorff-Nielsen, O. E., P. R. Hansen, A. Lunde, and N. Shephard (2011): “Multivariate Realised
 Kernels: Consistent Positive Semi-Definite Estimators of the Covariation of Equity Prices with Noise and
 Non-Synchronous Trading,” Journal of Econometrics, 162, 149–169.

Bickel, P. J., and E. Levina (2008a): “Covariance Regularization by Thresholding,” Annals of Statistics,
  36(6), 2577–2604.

        (2008b): “Regularized Estimation of Large Covariance Matrices,” Annals of Statistics, 36, 199–227.

Brillinger, D. R. (2001): Time Series: Data Analysis and Theory, Classics in Applied Mathematics (Book
 36). SIAM: Society for Industrial and Applied Mathematics.

Cai, T. T., and H. H. Zhou (2012): “Optimal Rates of Convergence for Sparse Covariance Matrix Estima-
 tion,” Annals of Statistics, 40(5), 2389–2420.

Chamberlain, G., and M. Rothschild (1983): “Arbitrage, Factor Structure, and Mean-Variance Analysis
 on Large Asset Markets,” Econometrica, 51, 1281–1304.

Christensen, K., S. Kinnebrock, and M. Podolskij (2010): “Pre-averaging estimators of the ex-post
 covariance matrix in noisy diffusion models with non-synchronous data,” Journal of Econometrics, 159,
 116–133.

                                                   20
Connor, G., and R. Korajczyk (1988): “Risk and Return in an Equilibrium APT: Application of a New
 Test Methodology,” Journal of Financial Economics, 21, 255–289.

d’Aspremont, A., L. E. Ghaoui, M. I. Jordan, and G. R. G. Lanckriet (2007): “A Direct Formulation
  for Sparse PCA Using Semidefinite Programming,” SIAM Review, 49(3), 434–448.

Davis, C. (1957): “All Convex Invariant Functions of Hermitian Matrices,” Archiv der Mathematik, 8(4),
 276–278.

Egloff, D., M. Leippold, and L. Wu (2010): “The term structure of variance swap rates and optimal
 variance swap investments,” Journal of Financial and Quantitative Analysis, 45, 1279–1310.

Fama, E. F., and K. R. French (1993): “Common Risk Factors in the Returns on Stocks and Bonds,”
  Journal of Financial Economics, 33, 3–56.

Fan, J., A. Furger, and D. Xiu (2015): “Incorporating Global Industrial Classification Standard into
  Portfolio Allocation: A Simple Factor-Based Large Covariance Matrix Estimator with High Frequency
  Data,” Journal of Business and Economic Statistics, forthcoming.

Forni, M., M. Hallin, M. Lippi, and L. Reichlin (2000): “The Generalized Dynamic-Factor Model:
 Identification and Estimation,” The Review of Economics and Statistics, 82, 540–554.

        (2004): “The Generalized Dynamic Factor Model: Consistency and Rates,” Journal of Econometrics,
  119(2), 231–255.

Forni, M., and M. Lippi (2001): “The Generalized Dynamic Factor Model: Representation Theory,” Econo-
 metric Theory, 17, 1113–1141.

Friedland, S. (1981): “Convex Spectral Functions,” Linear and Multilinear Algebra, 9, 299–316.

Gabriel, K. (1971): “The biplot graphic display of matrices with application to pricipal component analysis,”
 Biometrika, 58, 453–467.

Geman, S. (1980): “A Limit Theorem for the Norm of Random Matrices,” Annals of Probability, 8, 252–261.

Heinrich, C., and M. Podolskij (2014): “On Spectral Distribution of High Dimensional Covaraition
 Matrices,” Discussion paper, University of Aarhus.

Horn, R. A., and C. R. Johnson (2013): Matrix Analysis. Cambridge University Press, second edn.

Hotelling, H. (1933): “Analysis of a Complex of Statistical Variables into Principal Components,” Journal
 of Educational Psychology, 24, 417–441, 498–520.

Jackson, J. E. (2003): A User’s Guide to Principal Components. Wiley.

Jacod, J., A. Lejay, and D. Talay (2008): “Estimation of the Brownian dimension of a continuous Itô
  process,” Bernoulli, 14, 469–498.

Jacod, J., and M. Podolskij (2013): “A test for the rank of the volatility process: The Random Pertur-
  bation Approach,” Annals of Statistics, 41, 2391–2427.

Jacod, J., and P. Protter (2011): Discretization of Processes. Springer-Verlag.



                                                     21
Jacod, J., and M. Rosenbaum (2013): “Quarticity and Other Functionals of Volatility: Efficient Estima-
  tion,” Annals of Statistics, 41, 1462–1484.

Jacod, J., and A. N. Shiryaev (2003): Limit Theorems for Stochastic Processes. Springer-Verlag, second
  edn.

Johnstone, I. M. (2001): “On the distribution of the largest eigenvalue in principal components analysis,”
  Annals of Statistics, 29, 295–327.

Johnstone, I. M., and A. Y. Lu (2009): “On Consistency and Sparsity for Principal Components Analysis
  in High Dimensions,” Journal of the American Statistical Association, 104(486), 682–693.

Jolliffe, I. T. (2002): Principal Component Analysis. Springer-Verlag.

Jolliffe, I. T., N. T. Trendafilov, and M. Uddin (2003): “A Modified Principal Component Technique
  Based on the LASSO,” Journal of Computational and Graphical Statistics, 12(3), 531–547.

Kalnina, I., and D. Xiu (2013): “Model-Free Leverage Effect Estimators at High Frequency,” Discussion
 paper, Université de Montréal and University of Chicago.

Lewis, A. S. (1996a): “Convex Analysis on the Hermitian Matrices,” SIAM Journal of Optimizaiton, 6(1),
  164–177.

        (1996b): “Derivatives of Spectral Functions,” Mathematics of Operations Research, 21, 576–588.

Lewis, A. S., and H. S. Sendov (2001): “Twice Differentiable Spectral Functions,” SIAM Journal on
  Matrix Analysis and Applications, 23, 368–386.

Li, J., V. Todorov, and G. Tauchen (2013): “Inference Theory on Volatility Functional Dependencies,”
  Discussion paper, Duke University.

         (2014): “Adaptive Estimation of Continuous-Time Regression Models using High-Frequency Data,”
  Discussion paper, Duke University.

Li, J., and D. Xiu (2014): “Generalized Method of Integrated Moments for High-Frequency Data,” Discussion
  paper, Duke University and The University of Chicago.

Litterman, R., and J. Scheinkman (1991): “Common factors affecting bond returns,” Journal of Fixed
  Income, June, 54–61.

Magnus, J. R., and H. Neudecker (1999): Matrix Differential Calculus with Applications in Statistics and
 Economics. Wiley.

Mykland, P. A., and L. Zhang (2009): “Inference for continuous semimartingales observed at high fre-
 quency,” Econometrica, 77, 1403–1445.

Okamoto, M. (1973): “Distinctness of the Eigenvalues of a Quadratic form in a Multivariate Sample,”
 Annals of Statistics, 1, 763–765.

Pearson, K. (1901): “On Lines and Planes of Closest Fit to Systems of Points in Space,” Philosophical
 Magazine, 2, 559–572.




                                                   22
Protter, P. (2004): Stochastic Integration and Differential Equations: A New Approach. Springer-Verlag,
 second edn.

Rockafellar, R. T. (1997): Convex Analysis. Princeton University Press.

Ross, S. A. (1976): “The Arbitrage Theory of Capital Asset Pricing,” Journal of Economic Theory, 13,
 341–360.

Shephard, N., and D. Xiu (2012): “Econometric analysis of multivariate realized QML: Estimation of
  the covariation of equity prices under asynchronous trading,” Discussion paper, University of Oxford and
  University of Chicago.

Silhavý, M. (2000): “Differentiability Properties of Isotropic Functions,” Duke Mathematical Journal, 104,
  367–373.

Stock, J. H., and M. W. Watson (1998): “Diffusion Indexes,” Discussion paper, NBER.

         (1999): “Forecasting Inflation,” Journal of Monetary Economics, 44, 293–335.

        (2002): “Forecasting using Principal Components from a Large Number of Predictors,” Journal of
  American Statistical Association, 97, 1167–1179.

Sylvester, J. (1985): “On the Differentiablity of O(n) Invaraint Functions of Symmetric Matrices,” Duke
  Mathematical Journal, 52.

Tao, M., Y. Wang, and X. Chen (2013): “Fast Convergence Rates in Estimating Large Volatility Matrices
  Using High-Frequency Financial Data,” Econometric Theory, 29(4), 838–856.

Tao, M., Y. Wang, Q. Yao, and J. Zou (2011): “Large Volatility Matrix Inference via Combining Low-
  Frequency and High-Frequency Approaches,” Journal of the American Statistical Association, 106, 1025–
  1040.

Tao, M., Y. Wang, and H. H. Zhou (2013): “Optimal Sparse Volatility Matrix Estimation for High-
  Dimensional Itô Processes with Measurement Errors,” Annals of Statistics, 41(1), 1816–1864.

Tao, T. (2012): Topics in Random Matrix Theory. American Mathematical Society.

Tyler, D. E. (1981): “Asymptotic Inference for Eigenvectors,” Annals of Statistics, 9, 725–736.

Wang, Y., and J. Zou (2010): “Vast volatility matrix estimation for high-frequency financial data,” Annals
 of Statistics, 38, 943–978.

Waternaux, C. M. (1976): “Asymptotic Distribution of the Sample Roots for a Nonnormal Population,”
 Biometrika, 63, 639–645.

Zheng, X., and Y. Li (2011): “On the Estimation of Integrated Covariance Matrices of High Dimensional
  Diffusion Processes,” Annals of Statistics, 39, 3121–3151.

Zou, H., T. Hastie, and R. Tibshirani (2006): “Sparse Principal Component Analysis,” Journal of
  Computational and Graphical Statistics, 15(2), 265–286.




                                                    23
Appendix A                    Mathematical Proofs
Appendix A.1                Proof of Lemma 1
Proof. By the classical Weyl inequalities in e.g., Horn and Johnson (2013) and Tao (2012), we have |λj (A +
) − λj (A)| ≤ K kk, for all j = 1, 2, . . . , d, where A,  ∈ M+
                                                                 d , and K is some constant. This establishes the
Lipchitz property.


Appendix A.2                Proof of Lemma 2
Proof. It is straightforward to show (by the implicit function theorem, see Magnus and Neudecker (1999)
Theorem 8.7) that any simple eigenvalue and its corresponding eigenvector, written as functions of A, λg (A)
and γ g (A), are C ∞ . To calculate their derivatives, note that Aγ g = λg γ g , hence we have (∂jk A)γ g +A(∂jk γ g ) =
(∂jk λg )γ g + λg (∂jk γ g ). Pre-multiplying γ |g on both sides yields ∂jk λg = γ |g (∂jk A)γ g = γ gj γ gk . Rewrite it into
(λg I − A)∂jk γ g = (∂jk A)γ g − (∂jk λg )γ g , which leads to (λg I − A)+ (λg I − A)∂jk γ g = (λg I − A)+ (∂jk A)γ g . As
a result, ∂jk γ g = (λg I − A)+ Jjk γ g .
    In the case when all eigenvalues are simple, by direct calculation we have
                                                                 X         1
                                                    ∂jk γ gh =                 γ γ γ ,
                                                                        λg − λp ph pj gk
                                                                 p6=g


where we use the fact that (γ |1 , γ |2 , . . . , γ |d )| A(γ 1 , γ 2 , . . . , γ d ) = Diag(λ(A)). Further,

               2
                                  X          1                                          X    1                       
              ∂jk,lm γ gh = −                      2
                                                     (∂lm λg − ∂lm λp )γ ph γ pj γ gk +           ∂lm γ ph γ pj γ gk
                                         (λg − λp )                                       λg − λp
                                  p6=g                                                       p6=g
                                  X          1                                                           
                           =−                      2
                                                     γ gl γ gm γ ph γ pj γ gk − γ pl γ pm γ ph γ pj γ gk
                                         (λg − λp )
                                  p6=g
                                  XX                   1
                              +                                   γ γ γ γ γ
                                              (λg − λp )(λp − λq ) ql pm qh pj gk
                                  p6=g q6=p
                                  XX                   1
                              +                                   γ γ γ γ γ
                                              (λg − λp )(λp − λq ) ql pm qj ph gk
                                  p6=g q6=p
                                  XX                   1
                              +                                   γ γ γ γ γ ,
                                              (λg − λp )(λg − λq ) qk ql gm ph pj
                                  p6=g q6=g

which concludes the proof.


Appendix A.3                Proof of Lemma 3
Proof. The proof is by induction. Consider, at first the following optimization problem:
                                 Z u
                            max      γ |s cs γ s ds, s.t. γ |s γ s = 1, 0 ≤ s ≤ u ≤ t.
                                         γs    0

Using a sequence of Lagrange multipliers λs , the problem can be written as solving

                                         cs γ s = λs γ s ,   and γ |s γ s = 1, for any 0 ≤ s ≤ t.

Hence, the original problem is translated into eigenanalysis.



                                                                         24
    Suppose the eigenvalues of cs are ordered as in λ1,s ≥ λ2,s ≥ . . . ≥ λd,s . Note that γ |s cs γ s = λs , so that
λs = λ1,s , and γ s = γ 1,s is one of the corresponding eigenvectors (if λ1,s is not unique), and the maximal
             Rt
variation is 0 λ1,s ds.
    Suppose that we have found γ 1,s , . . . , γ k,s , for 1 ≤ k < d and 0 ≤ s ≤ t, the (k + 1)th principal component
is defined by solving the following problem:
                 Z u
             max      γ |s cs γ s ds, s.t. γ |s γ s = 1, and γ |j,s cs γ s = 0, for 1 ≤ j ≤ k, 0 ≤ u ≤ t.
              γs    0

Using similar technique of Lagrange multipliers, λs , and ν 1,s , . . . , ν k,s , we find
                                                                       k
                                                                       X
                                                  cs γ s = λs γ s +          ν j,s cs γ j,s .
                                                                       j=1


Multiplying on the left γ |l,s , for some 1 ≤ l ≤ k, we can show that ν l,s cs γ l,s = 0. Indeed,

                                                                                 k
                                                                                 X
                         0 = λl,s γ |l,s γ s = γ |l,s cs γ s = γ |l,s λs γ s +         ν j,s γ |l,s cs γ j,s = ν l,s λl,s .
                                                                                 j=1

Therefore, since l is an arbitrary number between 1 and k, we have cs γ s = λs γ s . Hence, λs = λk+1,s ,
γ s = γ k+1,s is one of the eigenvectors associated with the eigenvalue λk+1,s . This establishes the first part of
the theorem.
    For any càdlàg and adapted process γ s ,
                                   Z u             Z u          c Z t
                                        γ |s− dXs ,     γ |s− dXs =     γ |s cs γ s ds.
                                           0                 0                          0

Hence the statement follows from the gth-step optimization problem. Note that the validity of the integrals
above is warranted by the continuity of λ given by Lemma 1.


Appendix A.4             Proof of Lemma 4
Proof. The first statement of the proof follows by immediate calculations from Theorem 1.1 in Lewis (1996b)
and Theorem 3.3 in Lewis and Sendov (2001). The second statement is discussed and proved in, e.g., Ball
(1984), Sylvester (1985), and Silhavý (2000). Finally, the last statement on convexity is proved in Davis (1957)
and Lewis (1996a).


Appendix A.5             Proof of Lemma 5
Proof. Obviously, for any 1 ≤ g1 < g2 < . . . < gr ≤ d, the set defined in (4), D(g1 , g2 , . . . , gr ), is an open set in
R+
                                   P
  d /{0}. Define f (x) = |x̄gr | +    i6=j |x̄gi − x̄gj |, which is a continuous and convex function. It is differentiable
at x if and only if x ∈ D(g1 , g2 , . . . , gr ). Therefore, by Lemma 4, f ◦ λ is convex, and it is differentiable at
A if and only if λ(A) ∈ D(g1 , g2 , . . . , gr ), i.e., A ∈ M(g1 , g2 , . . . , gr ). On the other hand, a convex function
is almost everywhere differentiable, see Rockafellar (1997), which implies that M(g1 , g2 , . . . , gr ) is dense in
M++                                                                                 +
   d . Moreover, M(g1 , g2 , . . . , gr ) is the pre-image of the open set R /{0} under a continuous function h ◦ λ,
               Q
where h(x) = i6=j |x̄gi − x̄gj ||x̄gr |. Therefore, it is open.




                                                                     25
Appendix A.6              Proof of Theorem 1
Proof. Note that
                                                    [t/(kn ∆n )]                  [t/(kn ∆n )]
                                                       X                                X
                  V (∆n , X; F ) = kn ∆n                           f λbik ∆ = kn ∆n
                                                                         n n
                                                                                                (f ◦ λ)(b
                                                                                                        cikn ∆n ).
                                                        i=0                                   i=0

By Assumption 2 and Lemma 4, f ◦ λ is a continuous vector-valued function. Moreover, for c ∈ M+            d,
                            ζ            ζ
kf ◦ λ(c)k ≤ K(1 + kλ(c)k ) ≤ K(1 + kck ). Below we prove this theorem for any spectral function F that is
                        ζ
bounded by K(1 + kck ).
     We start with a function F bounded by K everywhere. We extend the definition of b
                                                                                     c to the entire interval
[0, t] by letting:

                                     cs = b
                                     b    c(i−1)kn ∆n , for               (i − 1)kn ∆n ≤ s < ikn ∆n .

Note that for any t > 0, we have
                         Z t
      E V (∆n , X; F ) −     F (cs )ds
                                0
                                                Z    [t/(kn ∆n )]kn ∆n                                  Z   t
    ≤kn ∆n E F (b
                c[t/(kn ∆n )]kn ∆n ) +                                     E kF (b
                                                                                 cs ) − F (cs )k ds +                       E kF (cs )k ds
                                                 0                                                      [t/(kn ∆n )]kn ∆n
                 Z     [t/(kn ∆n )]kn ∆n
    ≤Kkn ∆n +                              E kF (b
                                                 cs ) − F (cs )k ds.
                   0

                               p
By the fact that b cs − cs −→ 0, it follows that E kF (b cs ) − F (cs )k → 0, which is bounded uniformly in s and n
because F is bounded. Therefore, by the dominated convergence theorem, we obtain the desired convergence.
    Next we show the convergence holds under the polynomial bound on F . Denote ψ to be a C ∞ function on
R+ such that 1[1,∞) (x) ≤ ψ(x) ≤ 1[1/2,∞] (x). Let ψ ε (c) = ψ(kck /ε), and ψ 0ε (c) = 1 − ψ ε (c). Since the function
                                                                                                 p Rt
F · ψ 0ε is continuous and bounded, the above argument implies that V (∆n , X; F · ψ 0ε ) −→ 0 F · ψ 0ε (cs )ds, for
                                                  Rt                     Rt
any fixed ε. When  is large enough, we have 0 F · ψ 0ε (cs )ds = 0 F (cs )ds by localization, since cs is locally
                                                    ζ
bounded. On the other hand, F · ψ ε (c) ≤ K kck 1{kck≥ε} , for ε > 1. So it remains to show that
                                                                                         
                                                 [t/(kn ∆n )]
                                                     X                  ζ
                           lim lim sup E kn ∆n               kb
                                                               cikn ∆n k 1{kbcikn ∆n k>ε}  = 0.
                             ε→∞ n→∞
                                                                    i=0


By (9.4.7) of Jacod and Protter (2011), there exists some sequence an going to 0, such that
                                                                K
                                      ζ
                             cikn ∆n k 1{kbcikn ∆n k>ε} |Fikn ∆n ≤ ζ + Kan ∆n(1−ζ+$(2ζ−γ)) ,
                          E kb
                                                                  ε
which establishes the desired result.


Appendix A.7              Proof of Proposition 1
Proof. We divide the proof into several steps. To start, we need some additional notations. Let X 0 and c0
                                                                                       c0ikn ∆n to denote the
denote the continuous parts of the processes X and c, respectively. Also, we introduce b
estimator constructed similarly as in (5) with X replaced by X 0 and without truncation, namely
                                                                kn
                                                           1 X
                                           c0ikn ∆n =              (∆n    X 0 )(∆nikn +j X 0 )| .
                                                         kn ∆n j=1 ikn +j
                                           b


                                                                          26
             b0
In addition, λ                                                   c0ikn ∆n , and
              ikn ∆n corresponds to the vector of eigenvalues of b

                                                                                [t/(kn ∆n )]     0      
                                                                                   X
                                               V 0 (∆n , X; F ) = kn ∆n                        f λ
                                                                                                 b
                                                                                                   ikn ∆n .
                                                                                   i=0

We also define
                            Z   (i+1)kn ∆n
                  1
     c̄ikn ∆n =                                cs ds,              c0ikn ∆n − cikn ∆n ,
                                                          β nikn = b                               αnl = (∆nl X)(∆nl X)| − cl∆n ∆n ,   and
                kn ∆n        ikn ∆n
                                                                                  1/2
          η ni = E                   sup             |bi∆n +u − bi∆n |2 |Fi∆n                  .
                          i∆n ≤u≤i∆n +kn ∆n

We first collect some known estimates in the next lemma:

Lemma 6. Under the assumptions of Proposition 1, we have
                          
                       q
    E sup kct+u − ct k |Ft ≤ Ks1∧q/2 , kE (ct+s − ct |Ft )k ≤ Ks,                                                                        (A.1)
            0≤u≤s

        E (∆ni X)(∆ni X)| 1{k∆n X k≤un } − (∆ni X 0 )(∆ni X 0 )| ≤ Kan ∆n(2−r)$+1 , for some an → 0,                                     (A.2)
                                i
                              q
        E b          c0ikn ∆n
           cikn ∆n − b            ≤ Kan ∆n(2q−r)$+1−q , for some q ≥ 1, and an → 0,                                                      (A.3)
                                      p
          c0ikn ∆n − c̄ikn ∆n
        E b                               ≤ Kkn−p/2 , for some p ≥ 1,                                                                    (A.4)
                 q
        E (kαni k     |Fi∆n ) ≤      K∆qn ,
                                      for some q ≥ 0,                                                                                    (A.5)
                                                  
        kE (αni |Fi∆n )k ≤ K∆3/2 n     ∆ 1/2
                                         n   + η n
                                                 i ,                                                                                     (A.6)
                                                          
         E αn,jk
              i    αn,lm
                     i   − cjl      km       jm kl      2
                               i∆n ci∆n + ci∆n ci∆n ∆n |Fi∆n   ≤ K∆5/2
                                                                   n ,                                                                   (A.7)
                                                         
         E β nikn |Fikn ∆n ≤ K∆n 1/2 kn ∆1/2            n
                          
                                                  n + ηi ,                                                                               (A.8)
                                                     
                   q
        E β nikn |Fikn ∆n ≤ K kn−q/2 + kn ∆n , for some q ≥ 2,
                             
                                                                                                                                         (A.9)
               [t/∆n ]
                X
        ∆n E             η ni → 0.                                                                                                      (A.10)
                i=1

Proof of Lemma 6. These estimates are given by Lemma A.2 in Li and Xiu (2014), (4.3), (4.8), (4.10), (4.11),
(4.12), (4.18), Lemmas 4.2 and 4.3 of Jacod and Rosenbaum (2013), and Lemma 13.2.6 of Jacod and Protter
(2011).

    Now we return to the proof of Proposition 1.
1) We show that we can restrict the domain of function f to some compact set, where both the estimates
{b
 cikn ∆n }i=0,1,2,...,[t/(kn ∆n )] and the sample path of {cs }s∈[0,t] take values. By (A.4), we have for p ≥ 1,
                                                                                    p
                                                          c0ikn ∆n − c̄ikn ∆n
                                                        E b                             ≤ Kkn−p/2 .

Therefore, by the maximal inequality, we deduce, by picking p > 2/ς − 2,

                                                                                        p
                                  E            sup           c0ikn ∆n − c̄ikn ∆n
                                                             b                              ≤ K∆−1 −p/2−1
                                                                                                n kn      → 0,
                                      0≤i≤[t/(kn ∆n )]

                               c0ikn ∆n − c̄ikn ∆n = op (1). Moreover, by (A.2) we have
therefore, sup0≤i≤[t/(kn ∆n )] b
                                                                   [t/∆n ]−kn
                                                             1        X
 E        sup             cikn ∆n −
                          b               c0ikn ∆n
                                          b             ≤                       E (∆ni X)(∆ni X)| 1{k∆n X k≤un } − (∆ni X 0 )(∆ni X 0 )|
     0≤i≤[t/(kn ∆n )]                                     k n ∆n      i=0
                                                                                                               i




                                                                            27
                                                     ≤Kan ∆n(2−γ)$−1+ς → 0.

As a result, we have as ∆n → 0,
                                                                                                    p
                                                           sup       kb
                                                                      cikn ∆n − c̄ikn ∆n k −→ 0.                                                (A.11)
                                                 0≤i≤[t/(kn ∆n )]


Note that by Assumption 3, for 0 ≤ s ≤ t, cs ∈ C ∩ M∗ (g1 , g2 , . . . , gr ), where C is a convex and open set.
Therefore, {c̄ikn ∆n }i=0,1,2,...,[t/(kn ∆n )] ∈ C by convexity. For n large enough, {b   cikn ∆n }i=0,1,2,...,[t/(kn ∆n )] ∈
                                                                   ¯
C, with probability approaching 1, by (A.11). Since C ⊂ M(g1 , g2 , . . . , gr ), we can restrict the domain
of f to the compact set λ(C)     ¯ ⊂ D(g1 , g2 , . . . , gr ), in which f is C ∞ with bounded derivatives. Moreover,
because λgj (·), 1 ≤ j ≤ r are continuous functions, min1≤j≤r−1 (λgj (·) − λgj+1 (·)) is hence continuous, so that
inf c∈C {min1≤j≤r−1 (λgj (c) − λgj+1 (c))} ≥  > 0. It follows from Lemma 4 and Theorem 3.5 of Silhavý (2000)
that F (·) is C ∞ with bounded derivatives on C.
2) Next, we have
                                                                                [t/(kn ∆n )]
                                                                                      X
                       kV (∆n , X; F ) − V 0 (∆n , X; F )k ≤ kn ∆n                                                 c0ikn ∆n )
                                                                                                    cikn ∆n ) − F (b
                                                                                                 F (b
                                                                                      i=0
                                                                                     [t/(kn ∆n )]
                                                                                        X
                                                                     ≤ Kkn ∆n                                 c0ikn ∆n .
                                                                                                    cikn ∆n − b
                                                                                                    b
                                                                                        i=0

By (A.3), we have
                                                                 c0ikn ∆n           ≤ Kan ∆n(2−γ)$ ,
                                                                                
                                                 E     cikn ∆n − b
                                                       b

where an is some sequence going to 0, as n → ∞, which implies

                                           V (∆n , X; F ) − V 0 (∆n , X; F ) = Op (an ∆n(2−r)$ ).                                               (A.12)

As a result, given the conditions on $, we have

                                               kn (V (∆n , X; F ) − V 0 (∆n , X; F )) = op (1),

hence we can proceed with V 0 in the sequel.
3) Then we show for each 1 ≤ h ≤ d, we have
                                          [t/(kn ∆n )] 
                                             X
         0
  kn V (∆n , X; Fh ) − kn ∆n                               Fh (cikn ∆n )
                                             i=0
                                           d
                                                                                                                                        !
                                 1         X
                                                     2
                                                                                                                                    
                              −                     ∂jk,lm Fh (cikn ∆n ) (cjl,ikn ∆n ckm,ikn ∆n         + cjm,ikn ∆n ckl,ikn ∆n )           = op (1).
                                2kn
                                        j,k,l,m=1

where Fh is the hth entry of the vector-valued function F .
   To prove it, we decompose the left hand side into 4 terms:
                  [t/(kn ∆n )]                                                     d
                     X                                                              X
 n
R1,h =kn2 ∆n                       Fh (cikn ∆n + β nikn ) − Fh (cikn ∆n ) −                 ∂lm Fh (cikn ∆n )β n,lm
                                                                                                               ikn
                     i=0                                                            l,m=1
                   d
             1     X                                          
        −                     2
                             ∂jk,lm Fh (cikn ∆n )β n,lm  n,jk
                                                   ikn β ikn ,                                                                                  (A.13)
             2
                 j,k,l,m=1




                                                                           28
               [t/(kn ∆n )]            d                                                                                              
                     X           1     X                                               1
 n
R2,h =kn2 ∆n                                      2
                                                 ∂jk,lm Fh (cikn ∆n ) β n,lm
                                                                        ikn  β n,jk
                                                                               ikn  −    (c         c
                                                                                           jl,ikn ∆n km,ikn ∆n + c         c
                                                                                                                  jm,ikn ∆n kl,ikn ∆n )  ,
                     i=0
                                 2                                                    kn
                                     j,k,l,m=1
                                                                                                                                                        (A.14)
               [t/(kn ∆n )]      d                              kn
                     X           X                              X
 n
R3,h =kn ∆n                             ∂lm Fh (cikn ∆n )            (clm,(ikn +u)∆n − clm,ikn ∆n ),                                                    (A.15)
                     i=0      l,m=1                             u=1
           [t/(kn ∆n )]    d                               kn
               X           X                               X
 n
R4,h =kn                             ∂lm Fh (cikn ∆n )           αn,lm
                                                                  ikn +u ,                                                                              (A.16)
               i=0         l,m=1                           u=1

                      n
   We first consider R1,h . By (A.9), we have

                                                         [t/(kn ∆n )]                               [t/(kn ∆n )]
                                                              X                     3                   X
                          n
                      E(|R1,h |) ≤ Kkn2 ∆n                              E β nikn        ≤ Kkn2 ∆n                (kn−3/2 + kn ∆n )
                                                              i=0                                       i=0

                                        ≤   Kkn2 ∆n      +   Kkn−1/2     → 0.

       n                                                 n
As to R2,h , we denote the term inside the summation of R2,h as ν nikn . So we have

                                                    [t/(kn ∆n )]
                                                         X
                               n
                                   = kn2 ∆n                         ν nikn − E(ν nikn |Fikn ∆n ) + E(ν nikn |Fikn ∆n ) .
                                                                                                                      
                              R2,h
                                                         i=0

By (A.8), we have
                                                                                              p
                                                              |E(ν nikn |Fikn ∆n )| ≤ K        ∆n .

On the other hand, by (A.9), we can derive
                                                                                   2
                                                     E ν nikn − E(ν nikn |Fikn ∆n ) ≤ Kkn ∆n .

Then Doob’s inequality implies that
                                                                                                        
                                                         [s/(kn ∆n )]
                                                              X
                                                                         ν nikn   − E(ν nikn |Fikn ∆n )  ≤ Kt.
                                                                                                       
                                            E sup
                                                  s≤t         i=0


As a result,
                                                                                                   
                                                 [s/(kn ∆n )]                                                      [t/(kn ∆n )]
                                                    X                                                                 X
          n
              | ≤kn2 ∆n E sup                                   ν nikn − E(ν nikn |Fikn ∆n )  + kn2 ∆n                          E(ν nikn |Fikn ∆n )
                                                                                               
      E |R2,h
                                         s≤t         i=0                                                              i=0
                                            p
                     ≤Kkn2 ∆n          + Kkn ∆n → 0.

                  n
The proof for E(|R3,h |) → 0 is similar. Denote the term inside the summand as ξ nikn . By (A.1) and the
Cauchy-Schwarz inequality, we have

                                     |E(ξ nikn |Fikn ∆n )| ≤ Kkn2 ∆n ,              E |ξ nikn |2 |Fikn ∆n ≤ Kkn3 ∆n .
                                                                                                         


By Doob’s inequality again,
                                                                                                                                        1/2
                                               [t/(kn ∆n )]                                                 [t/(kn ∆n )]
                                                   X                                                           X
                   n
                                                              E E ξ nikn |Fikn ∆n                                          E |ξ nikn |2 
                                                                                                                                    
               E |R3,h | ≤ k n ∆n                                                             + k n ∆n 
                                                   i=0                                                         i=0



                                                                                   29
                              ≤ Kkn2 ∆n → 0.
     n                                                           n
                                                                             √
For R4,h , it can be shown in the proof of Theorem 2 below that R4,h = Op (kn ∆n ) = op (1).

4) Finally, it is sufficient to show that
                                                                                                                                      
         [t/(kn ∆n )] Z (i+1)kn ∆n
                                                                                         !
                                                     Z             (i+1)kn ∆n                         Z   t
             X                                                                                                                         p
    kn                            Fh (cikn ∆n )ds −                            Fh (cs )ds    −                           Fh (cs )ds −→ 0,
             i=0           ikn ∆n                                 ikn ∆n                              [t/(kn ∆n )]kn ∆n

                                                             2
as the similar result holds if we replace Fh (cikn ∆n ) by ∂jk,lm Fh (cikn ∆n ) (cjl,ikn ∆n ckm,ikn ∆n + cjm,ikn ∆n ckl,ikn ∆n ).
Since Fh is bounded, the second term is bounded by Kkn2 ∆n → 0. As to the first term, we notice that
                                       Z (i+1)kn ∆n                   Z (i+1)kn ∆n
                              ζ nikn =              Fh (cikn ∆n )ds −                 Fh (cs )ds
                                                ikn ∆n                              ikn ∆n

is measurable with respect to F(i+1)kn ∆n , and that

                     |E ζ nikn |Fikn ∆n | ≤ K(kn ∆n )2 ,                   E |ζ nikn |2 |Fikn ∆n ≤ K(kn ∆n )3 ,
                                                                                               


so the same steps as in (2) and (3) yield the desired results.


Appendix A.8                 Proof of Theorem 2
Proof. To start, we decompose
                                                         
                                 [t/(kn ∆n )]
     1 e                            X                          1
   √      V (∆n , X; F ) − kn ∆n              F (cikn ∆n ) = √       (R1n + R2n + R3n + R4n + R5n + R6n ) ,                               (A.17)
     ∆n                              i=0
                                                             kn   ∆ n

                                       |
where Rin = Ri,1n    n
                  , Ri,2            n
                         , . . . , Ri,d                                       n
                                           , for i = 1, 2, 3, 4, and 5, with R1,h    n
                                                                                  , R2,h    n
                                                                                         , R3,h      n
                                                                                                and R4,h given by equations
                                      n            n
(A.13) - (A.16). In addition, R5,h and R6,h are given by
                    [t/(kn ∆n )]     d
   n       k n ∆n      X             X
                                                2
  R5,h =                                       ∂jk,lm Fh (cikn ∆n ) (cjl,ikn ∆n ckm,ikn ∆n + cjm,ikn ∆n ckl,ikn ∆n )
              2        i=0         j,k,l,m=1
             2
                       c0ikn ∆n )       c0jl,ikn ∆n b
                                                    c0km,ikn ∆n + b
                                                                  c0jm,ikn ∆n b
                                                                              c0kl,ikn ∆n
                                                                                             
           −∂jk,lm Fh (b                b                                                         .
                    [t/(kn ∆n )]     d
           k n ∆n      X             X
   n                                            2
                                                          c0ikn ∆n ) b
                                                                     c0jl,ikn ∆n b
                                                                                 c0km,ikn ∆n + b
                                                                                               c0jm,ikn ∆n b
                                                                                                           c0kl,ikn ∆n
                                                                                                                            
  R6,h =                                       ∂jk,lm Fh (b
              2        i=0         j,k,l,m=1
             2
                                                                             ckl,ikn ∆n ) + kn (V (∆n , X; F ) − V 0 (∆n , X; F )) .
                                                                                         
           −∂jk,lm Fh (b
                       cikn ∆n ) (b
                                  cjl,ikn ∆n b
                                             ckm,ikn ∆n          +b
                                                                  cjm,ikn ∆n b

We have shown in the proof of Proposition 1 that Rin = Op (kn2 ∆n ), for i = 1, 2, 3. Therefore, these terms do
not contribute to the asymptotic variance of Ve 0 (∆n , X; F ).
                         n
                                          √
   Next, we show that R5,h  is also op (kn ∆n ). By (A.9) and the mean-value theorem, we have
                                          [t/(kn ∆n )]
                                               X                                                                           p
                  n
               E|R5,h | ≤Kkn ∆n                                      c0ikn ∆n ≤ K(kn−1/2 + kn ∆n ) = op (kn
                                                         E cikn ∆n − b                                                         ∆n ).
                                               i=0
          n
   As to R6,h , by (A.12) and the mean-value theorem, we have
                                           [t/(kn ∆n )]
                                                X                                                                         p
                   n
                E|R6,h |     ≤Kkn ∆n                                  c0ikn ∆n = Op (an ∆n(2−r)$ ) = op (kn
                                                            cikn ∆n − b
                                                          E b                                                              ∆n ).
                                                i=0


                                                                       30
            1−ς      1
Hence, $ ≥ 2−γ   > 4−2γ  is sufficient to warrant the desired rate.
   As a result, except for the term that is related to R4n , all the remainder terms on the right-hand side of
(A.17) vanish. We write R4n as
                                 [t/(kn ∆n )]kn      d
                                        X            X
                 R4n = kn                                   ω n,lm
                                                              i    αn,lm
                                                                    i    ,        where     ω n,lm
                                                                                              i    = ∂lm F (c[(i−1)/kn ]kn ∆n ).
                                        i=1        l,m=1


where ω n,lm
         i   is a vector measurable with respect to F(i−1)∆n , and kω ni k ≤ K. To prove the stable convergence
result, we start with
                    [t/(kn ∆n )]kn                                                       [t/(kn ∆n )]kn
          1                X                                                   1           X
                                        ω n,lm           αn,lm
                                                                                                                p
         √ E                              i    E          i    |Fi∆n          ≤√                          K∆3/2           n
                                                                                                            n ( ∆n + E (η i )) → 0,
          ∆n               i=0
                                                                                 ∆n           i=0


where we use (A.6) and (A.10). Moreover, by (A.5), we have
                                                                      
                                [t/(kn ∆n )]kn
                         1          X                4
                                                        
                                                               4
                                                                     
                            E                  kω ni k E kαni k |Fi∆n  ≤ K∆n → 0.
                        ∆2n          i=0

                                                                                     
Also, similar to (4.18) in Jacod and Rosenbaum (2013), we have E αn,lm
                                                                   i    ∆ n
                                                                          i N |F i∆ n   = 0, for any N that is
an arbitrary bounded martingale orthogonal to W , which readily implies
                                                   [t/(kn ∆n )]kn
                                              1           X                                   p
                                         √                          ω n,lm
                                                                      i    E αn,lm
                                                                              i    ∆ni N |Fi∆n −→ 0.
                                              ∆n          i=0

Finally, note that for any 1 ≤ p, q ≤ r, by (A.7),
         [t/(kn ∆n )]kn
     1       X                                         
                          |ω n,jk  n,lm     n,jk n,lm
                             i,p ω i,q | E αi   αi |Fi∆n − (ci∆n ,jl ci∆n ,km + ci∆n ,jm ci∆n ,kl ) ∆2n ≤ K∆1/2
                                                                                                            n ,
    ∆n        i=0

which implies
                                         [t/(kn ∆n )]kn
                                   1           X                                                
                                                           ω n,jk
                                                             i,p  ω n,lm
                                                                    i,q  E   α n,jk n,lm
                                                                               i   α i   |F i∆ n
                                  ∆n           i=0
                                         [t/(kn ∆n )]kn
                                   1           X
                             =                             ω n,jk  n,lm                                          2
                                                             i,p ω i,q (ci∆n ,jl ci∆n ,km + ci∆n ,jm ci∆n ,kl ) ∆n
                                  ∆n           i=0
                                  Z t
                             p
                           −→            ∂jk Fp (cs )∂lm Fq (cs ) (cs,jl cs,km + cs,jm cs,kl ) .
                                    0

Finally, by Theorem IX.7.28 of Jacod and Shiryaev (2003), we establish
                                                                   1     L−s
                                                                   √ R4 −→ Wt ,
                                                                k n ∆n n
where Wt is conditional Gaussian on an extension of the probably space, with a covariance matrix
                                                         d
                                                         X      Z    t
                     E (Wp,t Wq,t |F) =                                  ∂jk Fp (cs )∂lm Fq (cs ) (cs,jl cs,km + cs,jm cs,kl ) .
                                                    j,k,l,m=1    0




                                                                               31
Appendix A.9                   Proof of Proposition 2
Proof. As we have seen from the above proof, we have for any c ∈ C,
                                                                                                                  2
                                        k∂jk Fp (c)∂lm Fq (c) (cjl ckm + cjm ckl )k ≤ K(1 + kck ),

which, combined with the same argument in the proof of Theorem 1, establishes the desired result.


Appendix A.10                    Proof of Corollary 1
Proof. The first statement on consistency follows immediately from Theorem 1, as Assumption 2 holds with
ζ = 1. Next, we prove the central limit result. For any 1 ≤ p ≤ d, we define fpλ as,
                                                                                       gp
                                                                           1           X
                                                          fpλ (x̄)   =                         x̄j ,
                                                                       gp − gp−1   j=gp−1 +1

hence we have
                                                                            gp
                                                                1           X
                                           ∂fpλ (x̄)      =                           ev ,    ∂ 2 fpλ (x̄) = 0,
                                                            gp − gp−1     v=gp−1 +1
      λ       ∞
and f is C         and Lipchitz. By Lemma 4 we can derive
                       d                                                d     gp                                          gp
                       X                                         1     X     X                      1                     X
   ∂jk Fpλ (cs )   =         Ouj ∂u fpλ (λ(cs ))Ouk        =                      Ouj evu Ouk =                                    Ovj Ovk .
                       u=1
                                                             gp − gp−1 u=1 v=g +1               gp − gp−1              v=gp−1 +1
                                                                                      p−1


Therefore, the asymptotic covariance matrix is given by
          Z t X  d
                     ∂jk Fpλ (cs )∂lm Fqλ (cs ) (cjl,s ckm,s + cjm,s ckl,s ) ds
             0 j,k,l,m=1

                                          Z   t       d          gp         gq
                1         1                           X          X          X
          =                                                                           Ovj Ovk Oul Oum (cjl,s ckm,s + cjm,s ckl,s ) ds
            gp − gp−1 gq − gq−1              0 j,k,l,m=1 v=g
                                                             p−1 +1 u=gq−1 +1

                                                  Z   t   d      gp         gq
                       2                                  X      X          X
          =                                                                           Ovl Ovm Oul Oum λ2v,s ds
            (gp − gp−1 )(gq − gq−1 )               0 l,m=1 v=g
                                                               p−1 +1 u=gq−1 +1
                                                  Z   t   gp           gq
                       2                                  X            X
          =                                                                  δ u,v λ2v,s ds
            (gp − gp−1 )(gq − gq−1 )               0 v=g
                                                         p−1 +1 u=gq−1 +1
                         Z t
                2δ p,q
          =                  λ2 ds.                                                                                                     (A.18)
            (gp − gp−1 ) 0 gp ,s
   Next, we calculate the bias-correction term. Recall that the estimator is given by
                                                                                      gp
                                                                           1          X
                                                  Fpλ (b
                                                       cikn ∆n ) =                             λ
                                                                                               bv,ik ∆ ,
                                                                                                    n n
                                                                       gp − gp−1   v=gp−1 +1

where λbv,ik ∆ is the corresponding eigenvalue of the sample covariance matrix b
            n n
                                                                                    cikn ∆n . Although b
                                                                                                       cikn ∆n and
cikn ∆n may have different eigenstructure, it is easy to verify that the functional forms of the second order
derivative of Fpλ evaluated at both points turn out to be the same, so here we only provide the calculations
based on bcikn ∆n . Since almost surely, sample eigenvalues are simple, it implies from Lemma 4 that
                               d
  2
                               X        fλ
 ∂jk,lm Fpλ (b
             cikn ∆n ) =               Auvp (λ(b
                                               cikn ∆n ))O
                                                         bul O
                                                             buj O
                                                                 bvk O
                                                                     bvm
                               u,v=1


                                                                            32
                                                gp          d
                                     1          X           X             ehu − ehv
                             =                                                       Ob O
                                                                                        b Ob O
                                                                                bv,i∆ ul uj vk vm
                                                                                              b
                                 gp − gp−1                            bu,i∆ − λ
                                                                      λ
                                             h=gp−1 +1 u,v=1,u6=v             n            n

                                                 gp        d
                                     1          X          X                      1                                                     
                             =                                                                          O
                                                                                                        bul O
                                                                                                            buj O
                                                                                                                bhk O
                                                                                                                    bhm + O
                                                                                                                          bhl O
                                                                                                                              bhj O
                                                                                                                                  buk O
                                                                                                                                      bum ,
                                 gp − gp−1                           bh,ik ∆ − λ
                                                                     λ         bu,ik ∆
                                             h=gp−1 +1 u=1,u6=h           n n       n n



where Ob is the orthogonal matrix such that Obb cik ∆ Ob | = Diag(λ(b
                                                                    cikn ∆n )). The dependence of O
                                                                                                  b on ikn ∆n
                                                   n n

is omitted for brevity.
    To facilitate the implementation, we consider the matrix λbh,ik ∆ I − bcikn ∆n . Note that
                                                                   n n


            bh,ik ∆ − λ
       Diag(λ                   bh,ik ∆ − λ
                      b1,ik ∆ , λ                           bh,ik ∆ − λ
                                          b2,ik ∆ , . . . , λ         bd,ik ∆ ) = O(
                                                                                  b λbh,ik ∆ I − b        b| ,
                                                                                                 cikn ∆n )O
                 n n       n n       n n       n n               n n       n n            n n



hence we have
                                                                                                                                          !
                             +                             1                                 1                                1
    bh,ik ∆ I − b
    λ           cikn ∆n             b | Diag
                                   =O                               ,                    , . . . , 0, . . .                                    O.
                                                                                                                                               b
         n n
                                                 bh,ik ∆ − λ
                                                 λ         b1,ik ∆ λ  bh,ik ∆ − λ
                                                                                b2,ik ∆                     bh,ik ∆ − λ
                                                                                                            λ         bd,ik ∆
                                                      n n       n n        n n       n n                         n n       n n


As a result, we obtain

                                                           +          d
                                 
                                     bh,ik ∆ I − b
                                                                       X                       1
                                     λ    n n    cikn ∆n         =                                            O
                                                                                                              buk O
                                                                                                                  bum ,
                                                                     u=1,u6=p λh,ikn ∆n − λu,ikn ∆n
                                                            km                b           b

Therefore, we have
                                                gp                                +                                +
 2                                 1            X                                            
∂jk,lm Fpλ (b
            cikn ∆n )        =                           O
                                                         bhk λbh,ik ∆ I − b
                                                                   n n
                                                                          cikn ∆n    O
                                                                                     bhm + O
                                                                                           bhj λbh,ik ∆ I − b
                                                                                                     n n
                                                                                                            cikn ∆n    O
                                                                                                                       bhl .
                               gp − gp−1                                                       jl                                    km
                                             h=gp−1 +1

Now we can calculate the following term, which is used for bias-correction:
               d
               X
                          2
                         ∂jk,lm Fpλ (b
                                     ci∆n ) (b
                                             cjl,i∆n b
                                                     ckm,i∆n + b
                                                               cjm,i∆n b
                                                                       ckl,i∆n )
             j,k,l,m=1
                              gp         d                                +                                 +     
             1                X          X                                            
       =                                         Ohk λh,ikn ∆n I − b
                                                  b   b            cikn ∆n    Ohm + Ohj λh,ikn ∆n I − b
                                                                              b     b    b            cikn ∆n    Ohl
                                                                                                                 b
         gp − gp−1                                                                    jl                                        km
                          h=gp−1 +1 j,k,l,m=1

         · (b
            cjl,ikn ∆n b
                       ckm,ikn ∆n + b
                                    cjm,ikn ∆n b
                                               ckl,ikn ∆n )
                         gp                                        +        
              2          X                    
       =                        λh,ikn ∆n Tr λh,ikn ∆n I − b
                                b               b           cikn ∆n    cikn ∆n .                                                      (A.19)
         gp − gp−1
                                                                       b
                          h=gp−1 +1


The last equality uses the following observation:6

                                                       bh,ik ∆ I − b
                                                      (λ                      b | = 0,
                                                                   cikn ∆n )+ O
                                                            n n                h,·


which concludes the proof of (ii). The proof of (iii) uses the same calculations as above. Note that we can
apply Theorem 2 with only Assumption 4, because the spectral function here only depends on λg .
     6 See   page 160 of Magnus and Neudecker (1999).




                                                                         33
Appendix A.11                      Proof of Proposition 3
Proof. By Lemma 5 and the uniform convergence of b   cikn ∆n − c̄ikn ∆n to 0 established above, we can restrict the
                                                ∞
domain of γ g (·) to the set C, in which it is C with bounded derivatives. By Theorem 21 of Protter (2004),
we have
                                   [t/(kn ∆n )]−1                                                       Z   t
                                          X                                                     u.c.p
                                                     γ |g,ikn ∆n (X(i+1)kn ∆n   − Xikn ∆n ) =⇒                  γ g,s− dXs .
                                          i=1                                                           0

Therefore, it remains to show that
                           [t/(kn ∆n )]−1                                    
                                   X                                                                    u.c.p
                                                  b|g,(i−1)kn ∆n − γ |g,ikn ∆n (X(i+1)kn ∆n − Xikn ∆n ) =⇒ 0.
                                                  γ
                                    i=1

Define a F(i+1)kn ∆n -measurable function:
                                                               
                          ξ ikn = γ b|g,(i−1)kn ∆n − γ |g,ikn ∆n (X(i+1)kn ∆n − Xikn ∆n ).

By standard estimates in (A.1) with c replaced by X, (A.3), and (A.9),
                                                                                                       
                                       γ g,(i−1)kn ∆n − γ g,ikn ∆n ||E (X(i+1)kn ∆n − Xikn ∆n )|Fikn ∆n |
               E|E ξ ikn Fikn ∆n )| =E|b
                                                     c(i−1)kn ∆n − cikn ∆n |(kn ∆n )
                                                 ≤KE|b
                                                                                  q        
                                                 ≤K (kn ∆n )1/2 + an ∆n(2−γ)$ + kn−1 + kn ∆n (kn ∆n )

Moreover, we have by the same estimates above,

                    E |ξ ikn |2 |Fikn ∆n ≤ (kn ∆n + an ∆n(4−γ)$−1 + kn−1 + kn ∆n )kn ∆n .
                                        


Finally, using Doob’s inequality, and measurability of ξ ikn , we obtain
                                         
                    [s/(kn ∆n )]−1
                         X
          E  sup |                ξ ikn |
                0≤s≤t              i=1
                                                                                                           1/2
            [t/(kn ∆n )]−1                                         [t/(kn ∆n )]−1
                X                                                      X
                                                                                    E |ξ ikn |2 |Fikn ∆n 
                                                                                                        
        ≤                      E|E ξ ikn Fikn ∆n )| + 
                 i=1                                                    i=1
                                     q            
                                         −1
        ≤K (kn ∆n )1/2 + an ∆(2−γ)$
                             n      +  k n  + k                     (4−γ)$−1
                                                n n + K(kn ∆n + an ∆n
                                                 ∆                           + kn−1 + kn ∆n )1/2

       →0,

because (4 − γ)$ ≥ 1 under our assumptions on $ and ς, which establishes the proof.


Appendix A.12                      Proof of Corollary 2
Proof. The (p, q) entry of the asymptotic covariance matrix is given by
                           Z   t    d
                                    X
                                                ∂jk γ gp,s ∂lm γ gq,s (cjl,s ckm,s + cjm,s ckl,s ) ds
                           0 j,k,l,m=1

                           Z   t    d
                                    X
                       =                    (λg,s I − cs )+                +
                                                          pj (λg,s I − cs )ql γ gk,s γ gm,s (cjl,s ckm,s + cjm,s ckl,s ) ds
                           0 j,k,l,m=1



                                                                              34
                        Z   t   d
                                X
                                                                                   2
                                     (λg,s I − cs )+                +
                                                                                                    
                    =                              pj (λg,s I − cs )ql λg,s cjl + λg,s γ gl,s γ gj,s ds
                        0 j,l=1
                        Z   t
                                λg,s (λg,s I − cs )+ cs (λg,s I − cs )+
                                                                          
                    =                                                         p,q
                                                                                    ds,
                        0
                                           Pd
where we use (λg,s I − cs )+ γ g,s = 0, and k=1 γ gk,s ckm,s = λg,s γ gm,s . To calculate the asymptotic bias, we
note that the b
              cs has only simple eigenvalues almost surely. Denote λ   bh and γbh as the corresponding eigenvalue
and eigenvector. We omit the dependence on time s to simplify the notations. By Lemma 2, we obtain
       d
       X
                 2
                ∂jk,lm γ
                       bgh (b
                            cjl b
                                ckm + b
                                      cjm b
                                          ckl )
    j,k,l,m=1
                                                                                                                               
                               d                                                  d                          d
         X       λ
                 bp λ
                    bg        X     2
                                                       X        λ
                                                                bp λ
                                                                   bg           X       2
                                                                                                            X     2
  =−2                             γ
                                  bgl γ
                                      bpm γ
                                          bph γbgm +                                 γ
                                                                                      bpl γbgm γbph γ
                                                                                                    bpm +       γ
                                                                                                                bpm γ
                                                                                                                    bgl γ
                                                                                                                        bpl γ
                                                                                                                            bph 
             ( bg − λ
               λ     b p )2                                 ( bg − λ
                                                              λ     b p )2
       p6=g                 l,m=1                      p6=g                   l,m=1                       l,m=1
                                                                                                     
                                             d                                 d
     XX                   λ
                          bp λ
                             bg             X                                 X 2
   +                                            γ
                                                 bql γ
                                                     bpl γ
                                                         bpm γbgm γbqh +            bpm γ
                                                                                    γ     bgl γ
                                                                                              bql γ
                                                                                                  bqh 
                ( bg − λ
                  λ     bp )(λbp − λ
                                   bq )
     p6=g q6=p                            l,m=1                             l,m=1
                                                                                                      
                                             d                             d
     XX                   λ
                          bq λ
                             bg             X 2                           X
   +                                            bql γ
                                                 γ   bgm γbph γbpm +            γ
                                                                                bqm γbgl γbql γ
                                                                                              bpm γbph 
                ( bg − λ
                  λ     bp )(λbp − λ
                                   bq )
     p6=g q6=p                            l,m=1                         l,m=1
                                                                                                      
                                             d                             d
     XX                   λ
                          bp λ
                             bq             X           2
                                                                          X
   +                                            γ
                                                 bpm γbql γ
                                                          bgm γ bph +           γ
                                                                                bqm γbpl γbql γ
                                                                                              bgm γbph 
                (λbg − λbp )(λbg − λ
                                   bq )
       p6=g q6=g                                l,m=1                               l,m=1

       X         λ
                 bp λ
                    bg
  =−                      γ
                          b .
               bg − λ
              (λ     bp )2 gh
       p6=g

Since γ g (·) is a C ∞ function, it is straightforward using the proof of Theorem 2 that the desired CLT holds,
even though γ g (·) is not a spectral function.


Appendix A.13                    Proof of Propositions 4 and 5
Proof. This follows by applying the “delta” method, using Lemma 2 and Theorem 13.2.4 in Jacod and Protter
(2011).




                                                                      35
Appendix B               Figures and Tables

              κj          θj            ηj           ρj           µj           κ̃j             θ̃i,j              ξ̃ j
j=1            3         0.05           0.3         -0.6         0.05           1          U[0.25, 1.75]          0.5
j=2            4         0.04           0.4         -0.4         0.03           2           N (0, 0.52 )         0.6
j=3            5         0.03           0.3        -0.25         0.02           3           N (0, 0.52 )         0.7
                                                                    2
              λF         µF             λZ         µZ            µσ           ρF12             ρF13              ρF23
                          √
                          +/−                        √
                                                     +/−         √
              1/t        4 ∆            2/t        6 ∆             ∆          0.05             0.1               0.15
                                                                                κ                θ                 η
                                                                                4              0.3               0.06


                            Table 1: Parameters in Monte Carlo Simulations

Note: In this table, we report the parameter values used in the simulations. The constant matrix θ̃i,j is generated
randomly from the described distribution, and is fixed throughout all replications. The dimension of Xt is 100, whereas
the dimension of Ft is 3. ∆ is the sampling frequency, and t is the length of the time window. The number of Monte
Carlo replications is 1,000.




                                                           36
                                 1 Week, 5 Seconds                                      1 Week, 1 Minute
 # Stocks             True             Bias               Stdev              True             Bias               Stdev
     5               0.4686          -0.0001              0.0075            0.4703           0.0007              0.0152
    10               0.6489          0.0001               0.0110            0.6552          -0.0014              0.0223
    15               0.8927          -0.0004              0.0149            0.8975          -0.0011              0.0296
    20               1.3044          0.0003               0.0225            1.3148          -0.0018              0.0424
    30               2.1003          -0.0002              0.0356            2.1134          -0.0033              0.0688
    50               2.9863          0.0002               0.0514            3.0104          -0.0054              0.1002
   100               6.6270          -0.0004              0.1141            6.6732          -0.0127              0.2179
                                 1 Week, 5 Minutes                                     1 Month, 5 Minutes
      5              0.4879          0.0107               0.0452            0.5642           0.0006              0.0247
     10              0.6839          0.0084               0.0574            0.7143          -0.0024              0.0258
     15              0.9397          0.0128               0.0724            1.0167          -0.0024              0.0382
     20              1.3765          0.0130               0.1076            1.3882          -0.0041              0.0503
     30              2.2157          0.0210               0.1670            2.2383          -0.0073              0.0806
     50              3.1554          0.0267               0.2410            3.1518          -0.0125              0.1155
    100              7.0000          0.0552               0.5314            6.9632          -0.0270              0.2451


                        Table 2: Simulation Results: First Eigenvalue Estimation

Note: In this table, we report the summary statistics of 1,000 Monte Carlo simulations for estimating the first integrated
eigenvalue. Column “True” corresponds to the average of the true integrated eigenvalue; Column “Bias” corresponds
to the mean of the estimation error; Column “Stdev” is the standard deviation of the estimation error.



                                 1 Week, 5 Seconds                                      1 Week, 1 Minute
 # Stocks             True             Bias               Stdev              True             Bias               Stdev
     5               0.3145          0.00004              0.0051            0.3173           0.0003              0.0110
    10               0.4268          -0.0001              0.0071            0.4289           0.0006              0.0146
    15               0.5531          -0.0003              0.0086            0.5572           0.0004              0.0188
    20               0.6517          -0.0008              0.0103            0.6556          -0.0006              0.0215
    30               0.9186          -0.0015              0.0144            0.9251          -0.0017              0.0305
    50               1.2993          -0.0017              0.0205            1.3080          -0.0026              0.0458
   100               2.3273          -0.0041              0.0361            2.3441          -0.0065              0.0766
                                 1 Week, 5 Minutes                                     1 Month, 5 Minutes
 # Stocks             True             Bias               Stdev              True             Bias               Stdev
     5               0.3255           0.0020              0.0269            0.3639          -0.0003              0.0189
    10               0.4384           0.0028              0.0379            0.4469          -0.0003              0.0186
    15               0.5751           0.0014              0.0427            0.6524          -0.0017              0.0257
    20               0.6758           0.0046              0.0535            0.7779          -0.0021              0.0297
    30               0.9586           0.0017              0.0725            1.1365          -0.0036              0.0438
    50               1.3508          -0.0022              0.1046            1.5630          -0.0064              0.0683
   100               2.4312          -0.0084              0.1787            2.9148          -0.0136              0.1113


                      Table 3: Simulation Results: Second Eigenvalue Estimation

Note: In this table, we report the summary statistics of 1000 Monte Carlo simulations for estimating the second
integrated eigenvalue. Column “True” corresponds to the average of the true integrated eigenvalue; Column “Bias”
corresponds to the mean of the estimation error; Column “Stdev” is the standard deviation of the estimation error.




                                                           37
                               1 Week, 5 Seconds                                  1 Week, 1 Minute
 # Stocks           True             Bias             Stdev             True            Bias             Stdev
     5             0.1338          0.0001             0.0022           0.1345          0.0003            0.0044
    10             0.2132          0.0001             0.0035           0.2149          0.0002            0.0070
    15             0.2941          0.0002             0.0046           0.2954          0.0002            0.0093
    20             0.3212          0.0001             0.0052           0.3242          0.0000            0.0105
    30             0.4825          0.0005             0.0078           0.4853          0.0001            0.0153
    50             0.6943          0.0001             0.0113           0.7016         -0.0005            0.0226
   100             1.3808          0.0004             0.0221           1.3935         -0.0013            0.0438
                               1 Week, 5 Minutes                                 1 Month, 5 Minutes
 # Stocks           True             Bias             Stdev             True            Bias             Stdev
     5             0.1364          0.0041             0.0124           0.1404          0.0006            0.0054
    10             0.2199          0.0033             0.0183           0.2367         -0.0001            0.0091
    15             0.3018          0.0056             0.0264           0.3396          0.0009            0.0131
    20             0.3324          0.0037             0.0294           0.3546          0.0003            0.0132
    30             0.4971          0.0055             0.0409           0.5686          0.0006            0.0211
    50             0.7216          0.0028             0.0577           0.8051         -0.0010            0.0306
   100             1.4302          -0.0016            0.1119           1.6278         -0.0028            0.0612


                     Table 4: Simulation Results: Third Eigenvalue Estimation

Note: In this table, we report the summary statistics of 1,000 Monte Carlo simulations for estimating the third
integrated eigenvalue. Column “True” corresponds to the average of the true integrated eigenvalue; Column “Bias”
corresponds to the mean of the estimation error; Column “Stdev” is the standard deviation of the estimation error.


                               1 Week, 5 Seconds                                  1 Week, 1 Minute
 # Stocks            True           Bias              Stdev             True            Bias             Stdev
     5              0.0596         0.0002             0.0007           0.0597          0.0005            0.0015
    10              0.0596         0.0001             0.0004           0.0597          0.0003            0.0008
    15              0.0596         0.0001             0.0003           0.0597          0.0004            0.0006
    20              0.0596         0.0001             0.0002           0.0597          0.0004            0.0005
    30              0.0596         0.0001             0.0002           0.0597          0.0004            0.0004
    50              0.0596         0.0001             0.0001           0.0597          0.0004            0.0003
   100              0.0596         0.0001             0.0001           0.0597          0.0004            0.0002
                               1 Week, 5 Minutes                                 1 Month, 5 Minutes
 # Stocks            True           Bias              Stdev             True            Bias             Stdev
     5              0.0595         0.0026             0.0041           0.0595          0.0006            0.0017
    10              0.0595         0.0029             0.0029           0.0595          0.0006            0.0009
    15              0.0595         0.0027             0.0024           0.0595          0.0006            0.0007
    20              0.0595         0.0028             0.0021           0.0595          0.0006            0.0006
    30              0.0595         0.0030             0.0020           0.0595          0.0007            0.0005
    50              0.0595         0.0029             0.0018           0.0595          0.0006            0.0004
   100              0.0595         0.0031             0.0016           0.0595          0.0006            0.0003


       Table 5: Simulation Results: Repeated Eigenvalue Estimation, Fourth and Beyond

Note: In this table, we report the summary statistics of 1,000 Monte Carlo simulations for estimating the repeated
integrated eigenvalues. Column “True” corresponds to the average of the true integrated eigenvalue; Column “Bias”
corresponds to the mean of the estimation error; Column “Stdev” is the standard deviation of the estimation error.



                                                       38
                                1 Week, 5 Seconds                                   1 Week, 1 Minute
 # Stocks            True             Bias             Stdev             True             Bias             Stdev
                    0.2457          -0.0002            0.0173           0.2475           0.0074            0.1559
                    0.8987          0.0028             0.0261           0.8963           0.0174            0.2135
     5              0.0566          0.0013             0.0235           0.0536          -0.0018            0.1281
                    0.0633          0.0001             0.0119           0.0639           0.0037            0.0786
                    0.3110          0.0017             0.0202           0.3086           0.0042            0.0645
                    0.0506          0.0004             0.0049           0.0495           0.0029            0.0266
                    0.3444          0.0006             0.0087           0.3452           0.0080            0.0691
                    0.4632          0.0011             0.0129           0.4619           0.0121            0.0966
                    0.1422          0.0008             0.0137           0.1422           0.0094            0.0990
     10             0.4166          0.0004             0.0220           0.4164           0.0045            0.1562
                    0.0864          0.0008             0.0158           0.0863           0.0089            0.1117
                    0.3460          0.0008             0.0088           0.3440           0.0101            0.0600
                    0.1268          0.0005             0.0076           0.1259           0.0066            0.0451
                    0.3409          0.0005             0.0174           0.3415           0.0046            0.1268
                    0.4262          0.0007             0.0112           0.4271           0.0099            0.0942


                         Table 6: Simulation Results: Eigenvector Estimation

Note: In this table, we report the summary statistics of 1,000 Monte Carlo simulations for estimating the integrated
eigenvector associated with the first eigenvalue. Column “True” corresponds to the average of the true integrated
vector; Column “Bias” corresponds to the mean of the estimation error; Column “Stdev” is the standard deviation of
the estimation error. The setup for these simulations is identical to that of Tables 2-5.




                                                        39
                                  1 Week, 5 Seconds                                      1 Week, 1 Minute
 # Stocks            True               Bias               Stdev              True              Bias              Stdev
                    0.2587           −4 × 10−6             0.0026            0.2581           0.00004             0.0090
                    0.1557           3 × 10−7              0.0015            0.1558           -0.0001             0.0054
                    0.2364            -0.00004             0.0015            0.2359          -0.00008             0.0053
                    0.0608           1 × 10−6              0.0028            0.0608           0.00009             0.0096
                    0.1398            -0.00001             0.0019            0.1394           0.00002             0.0069
                    0.2471           −3 × 10−6             0.0017            0.2476          -0.00001             0.0059
                    0.2250            0.00001              0.0014            0.2244            0.0001             0.0043
                    0.2642            0.00002              0.0027            0.2647           0.00008             0.0094
                    0.2004            0.00005              0.0015            0.2006            0.0002             0.0047
                    0.0446            -0.00004             0.0022            0.0447            0.0002             0.0075
                    0.2311            0.00005              0.0023            0.2312           0.00001             0.0080
                    0.2560            0.00002              0.0021            0.2565          -0.00002             0.0075
                    0.1979            -0.00005             0.0013            0.1979           0.00008             0.0043
                    0.2255            -0.00004             0.0019            0.2247           -0.0001             0.0066
                    0.2185           6 × 10−7              0.0014            0.2186          -0.00007             0.0042
     30             0.1262            -0.00006             0.0015            0.1263            0.0002             0.0051
                    0.1916            0.00002              0.0013            0.1917           -0.0001             0.0045
                    0.0661            -0.00004             0.0025            0.0661            0.0002             0.0088
                    0.2118            -0.00004             0.0020            0.2117          -0.00003             0.0070
                    0.0414            0.00006              0.0013            0.0414           0.00008             0.0045
                    0.1007            -0.00004             0.0025            0.1008          -0.00006             0.0086
                    0.0518            -0.00006             0.0017            0.0517          -0.00003             0.0058
                    0.0550            -0.00001             0.0022            0.0548            0.0002             0.0077
                    0.2345            -0.00005             0.0013            0.2349           -0.0002             0.0043
                    0.2003            0.00003              0.0014            0.2006           -0.0001             0.0047
                    0.1276            -0.00003             0.0025            0.1275           0.00008             0.0085
                    0.2813            0.00002              0.0033            0.2813            0.0003             0.0116
                    0.0319            -0.00005             0.0030            0.0319          -0.00001             0.0106
                    0.1417            0.00006              0.0022            0.1414            0.0002             0.0075
                    0.1233            -0.00001             0.0024            0.1235            0.0002             0.0085


                           Table 7: Simulation Results: Eigenvector Estimation

Note: In this table, we report the summary statistics of 1,000 Monte Carlo simulations for estimating the integrated
eigenvectors associated with the first eigenvalues. The column “True” corresponds to the average of the true integrated
vector; “Bias” corresponds to the mean of the estimation error; “Stdev” is the standard deviation of the estimation
error. The setup for these simulations is identical to that of Tables 2-5. To save space, we do not report the eigenvectors
in dimensions larger than 30.




                                                            40
                   0.4                                            0.4



                   0.2                                            0.2



                     0                                             0
                         -4     -2      0       2        4             -4   -2       0        2        4




                   0.4                                            0.4



                   0.2                                            0.2



                     0                                             0
                         -4     -2      0       2        4             -4   -2       0        2        4



                         Figure 1: Finite Sample Distribution of the Standardized Statistics.



Note: In this figure, we report the histograms of the 1,000 simulation results for estimating the first four integrated
eigenvalues using 5-second returns for 30 stocks over one week. The purpose of this figure is to validate the asymptotic
theory, hence the use of a short 5-second sampling interval. The smallest 27 eigenvalues are identical so that the fourth
eigenvalue is repeated. The solid lines plot the standard normal density; the dashed histograms report the distribution
of the estimates before bias correction; the solid histograms report the distribution of the estimates after bias correction
is applied and is to be compared to the asymptotic standard normal. Because the fourth eigenvalue is small, the dashed
histogram on the fourth subplot is out of the x-axis range, to the right.




                                                             41
                  0.4                          0.4                        0.4
                  0.2                          0.2                        0.2
                    0                            0                          0
                        -4   -2   0   2    4         -4   -2    0   2   4     -4    -2   0   2    4

                  0.4                          0.4                        0.4
                  0.2                          0.2                        0.2
                    0                            0                          0
                        -4   -2   0   2    4         -4   -2    0   2   4     -4    -2   0   2    4

                                               0.4                        0.4
                                               0.2                        0.2
                                                 0                          0
                                                     -4   -2    0   2   4     -4    -2   0   2    4

                                                                           0.4
                                                                           0.2
                                                                             0
                                                                               -4   -2   0   2    4



                        Figure 2: Finite Sample Distribution of the Standardized Statistics.



Note: In this figure, we report the histograms of the integrated simple eigenvalues using weekly one-minute returns of
100 stocks, a setting that matches that of our empirical analysis below. Columns 1, 2, and 3 report the results with
r = 2, 3, and 4 common factors in the data generating process, respectively. The number of Monte Carlo replications
is 1,000.




                                                               42
                             Figure 3: The Liquidity of S&P 100 Index Components



Note: In this figure, we provide quartiles of the average time between successive transactions, or sampling frequencies,
for the 100 stocks S&P 100 Index constituents between 2003 and 2012 . We exclude the first and the last 5 minutes
from the 6.5 regular trading hours in the calculation, during which trading intensities are unusually high. The y-axis
reports the corresponding average sampling frequencies in seconds, computed from the high frequency transactions in
the sample. From these transactions, we construct a synchronous one-minute sample using the latest tick recorded
within that minute. The 10 least liquid stocks in the index are included in this figure but excluded from the empirical
analysis that follows.




                                                          43
              Figure 4: Time Series of the Cumulative Returns of S&P 100 Index Components



Note: In this figure, we plot the time series of cumulative daily open-to-close returns of 158 S&P 100 Index Components
from 2003 to 2012. The thick black solid line plots the cumulative daily open-to-close S&P 100 Index returns. All
overnight returns are excluded. Companies that exited the index during the sample period, including bankruptcies,
are represented by a time series truncated at the time of delisting.




                                                          44
                  2.5



                    2



                  1.5



                    1



                  0.5



                    0
                        0     10      20       30       40       50      60       70       80       90


                                             Figure 5: The Scree Graph



Note: In this figure, we report the time series average over the entire sample of the estimated eigenvalues against their
order, or “scree graph”. These integrated eigenvalues are estimated assuming that all of them are simple.




                                                           45
                 0.7
                              1st
                              2nd
                 0.6          3rd
                                4 Average

                 0.5


                 0.4


                 0.3


                 0.2


                 0.1


                   0
                   2003           2005           2007           2009           2011           2013


                                 Figure 6: Time Series of Realized Eigenvalues



Note: In this figure, we plot the time series of the three largest realized eigenvalues, representing the percentage
variations explained by the corresponding principal components, using weekly 1-min returns of 90 most liquid S&P 100
Index constituents from 2003 to 2012. For comparison, we also plot the average of the remaining 87 eigenvalues.




                                                        46
                    3
                               PC1
                               PC2
                    2          PC3

                    1

                    0

                   -1

                   -2

                   -3

                   -4

                   -5
                   2003           2005            2007           2009           2011            2013


                     Figure 7: Time Series of Cumulative Realized Principal Components



Note: In this figure, we compare the cumulative returns of the first three principal components, using weekly 1-minute
returns of the S&P 100 Index constituents from 2003 to 2012.




                                                         47
                                                       Week: 03/07/2005 -- 03/11/2005
                           0.06
                                                                                            Non-Financial
                                                                                            Financial
                           0.04

                           0.02

                              0
             Component 2




                           -0.02

                           -0.04

                           -0.06

                           -0.08

                            -0.1

                           -0.12
                                   0            0.05           0.1          0.15           0.2              0.25
                                                                Component 1


                                       Figure 8: Biplot of the Integrated Eigenvector: Pre-crisis Week



Note: This biplot shows the integrated eigenvector estimated during the week March 7-11, 2005. Each dot and the
vector joining it and the origin represents a firm. The x and y coordinates of a firm denote its average loading on the
first and second principal components, respectively. The dots and lines associated with firms in the financial sector are
colored in red and dashed, while non-financial firms are colored in blue and drawn as solid lines. The figures shows
little difference between the two groups during that week.




                                                                     48
                                                    Week: 03/10/2008 -- 03/14/2008
                           0.15
                                                                                          Non-Financial
                                                                                          Financial

                            0.1



                           0.05
             Component 2




                              0

                                                                        AIG
                           -0.05
                                                                              GS C
                                                                                 BAC
                            -0.1
                                                                                JPM

                           -0.15                                                                     LEH
                                   0        0.05        0.1        0.15         0.2        0.25         0.3
                                                              Component 1


                                       Figure 9: Biplot of the Integrated Eigenvector: Crisis Week



Note: This biplot shows the integrated eigenvector estimated during the week March 10-14, 2008. Each dot and the
vector joining it and the origin represents a firm. The x and y coordinates of a firm denote its average loading on the
first and second principal components, respectively. The dots and lines associated with firms in the financial sector
are colored in red and dashed, with the following singled out: American International Group (AIG), Bank of America
(BOA), Citigroup (C), Goldman Sachs (GS), J.P. Morgan Chase (JPM), and Lehman Brothers (LEH). Non-financial
firms are colored in blue and drawn as solid lines. The figure shows a sharp distinction between the two groups’
dependence on the second principal component during that week.




                                                                   49
                                          Week: 03/07/2005 -- 03/11/2005
                 0.6



                 0.5



                 0.4



                 0.3



                 0.2



                 0.1



                   0
                       0     10      20       30      40       50      60      70       80      90



                       Figure 10: Scree Plot of the Integrated Eigenvalues: Pre-crisis Week



Note: This figure contains a scree plot of the integrated eigenvalues estimated during the week March 7-11, 2005.
The blue solid lines provide 95% confidence interval for the first three eigenvalues computed based on the results of
Corollary 1.




                                                         50
                                            Week: 03/10/2008 -- 03/14/2008
                   8


                   7


                   6


                   5


                   4


                   3


                   2


                   1


                   0
                       0       10      20       30     40      50      60      70       80      90



                           Figure 11: Scree Plot of the Integrated Eigenvalues: Crisis Week



Note: This figure contains a scree plot of the integrated eigenvalues estimated during the week March 10-14, 2008.
The blue solid lines provide 95% confidence interval for the first three eigenvalues computed based on the results of
Corollary 1.




                                                         51
