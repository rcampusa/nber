                                NBER WORKING PAPER SERIES




      ESTIMATORS FOR PERSISTENT AND POSSIBLY NON-STATIONARY DATA
                       WITH CLASSICAL PROPERTIES

                                       Yuriy Gorodnichenko
                                         Anna Mikusheva
                                            Serena Ng

                                        Working Paper 17424
                                http://www.nber.org/papers/w17424


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                    September 2011




Mikusheva acknowledges financial support from the Castle-Krob Career Development Chair. Ng acknowledges
financial support from the National Science Foundation (SES-0962431). The views expressed herein
are those of the authors and do not necessarily reflect the views of the National Bureau of Economic
Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2011 by Yuriy Gorodnichenko, Anna Mikusheva, and Serena Ng. All rights reserved. Short sections
of text, not to exceed two paragraphs, may be quoted without explicit permission provided that full
credit, including © notice, is given to the source.
Estimators for Persistent and Possibly Non-Stationary Data with Classical Properties
Yuriy Gorodnichenko, Anna Mikusheva, and Serena Ng
NBER Working Paper No. 17424
September 2011
JEL No. C22,C32,E27,E37,G17

                                              ABSTRACT

This paper considers a moments based non-linear estimator that is root-T consistent and uniformly
asymptotically normal irrespective of the degree of persistence of the forcing process. These properties
hold for linear autoregressive models, linear predictive regressions, as well as certain non-linear dynamic
models. Asymptotic normality is obtained because the moments are chosen so that the objective function
is uniformly bounded in probability and that a central limit theorem can be applied.

Critical values from the normal distribution can be used irrespective of the treatment of the deterministic
terms. Simulations show that the estimates are precise, and the t-test has good size in the parameter
region where the least squares estimates usually yield distorted inference.


Yuriy Gorodnichenko                                  Serena Ng
Department of Economics                              Department of Economics
549 Evans Hall #3880                                 Columbia University
University of California, Berkeley                   440 W. 118 St.
Berkeley, CA 94720-3880                              International Affairs Building, MC 3308
and NBER                                             New York
ygorodni@econ.berkeley.edu                           NY 10027
                                                     serena.ng@columbia.edu
Anna Mikusheva
MIT
amikushe@mit.edu
1       Introduction


                                                                                        √
This paper considers a quasi-differencing (QD) framework that can yield                     T consistent and uni-
formly asymptotically normal estimators of autoregressions and multiple regressions when the pre-
dictors are persistent and possibly non-stationary. The approach can also be used to estimate
dynamic stochastic general equilibrium (DSGE) models. The critical values are invariant to the
presence of deterministic trends.


Let θ be an unknown parameter vector and let θ0 be its true value. We propose non-linear QD
estimators that can generically be defined as

                                          θbK = argmin g(θ)0 WT g(θ),                                            (1)
                                                   θ∈Θ

where g(θ) is a K ×1 vector of moments, WT is a K ×K positive-definite matrix, and Θ is a bounded
set containing values of θ.1 The basic premise of QD estimation is that for θbK to have classical
properties, g(θ) needs to be uniformly bounded in probability and that a central limit theorem can
be applied. To this end, g(θ) is defined as the difference between the normalized autocovariances of
the variables in the model and the data, all quasi-differenced at a persistence parameter that is to
be estimated jointly with other parameters of the model. The normalization and quasi-differencing
together provide a non-linear transformation of the autocovariances to result in estimators that are
robust to possible non-stationarity in the data.


Achieving asymptotic normality without knowing when the exogenous process has an autoregressive
unit root can be very useful in applied work because the answers to many macroeconomic questions
are sensitive to assumptions about the nature of the trend and to whether the corresponding
regressions are estimated in levels or in first-differences. The price to pay for practical simplicity
                                                         √
and robustness is that the proposed estimators are T consistent rather than super-consistent
when the regressors are truly non-stationary. While other asymptotically normal estimators robust
to non-stationary regressors are available, they apply only to specific linear models. The QD
estimation framework is general and can be used whenever the variables can be quasi-differenced
in the way discussed below.


We establish uniform asymptotic normality of QD-based estimators in many different settings.
    1
    The optimization is performed over an expanded neighborhood of the set of admissible values of θ so that the
parameter of interest is not on the boundary of the support. For the AR(1) model, the admissible values are (−1+δ, 1]
where δ > 0. We optimize over Θ = [C1 , C2 ], where C1 < −1 + δ < 1 < C2 . When the context is clear, Θ will not be
explicitly specified.


                                                         1
Throughout, we use the notion of uniformity given in Mikusheva (2007a), who studied uniform
coverage properties of various inference procedures for the AR(1) model.


                                              (1)
Definition 1 A family of distributions Fθ,T (x) = Pθ,T {ξ1 < x} is asymptotically approximated by
                                              (2)
(converges to) a family of distributions Fθ (x) = Pθ {ξ2 < x} uniformly over θ ∈ Θ if

                                                    (1)        (2)
                                   lim sup sup Fθ,T (x) − Fθ (x) = 0.
                                  T →∞ θ∈Θ x



                    (2)
In our analysis, Fθ (x) is in the family of Gaussian distributions.


The paper is structured as follows. Section 2 provides a rigorous analysis of the AR(1) model.
Section 3 extends the analysis to AR(p) models, while Section 4 studies predictive regressions.
Section 5 considers non-linear estimation of structural parameters. Simulations are presented in
                                                  √
Section 6. The relation of QD estimation to other T -consistent estimators is discussed in Section
7. All proofs are in the Appendix. As a matter of notation, the indicator function I(a) is one if a
is true and zero otherwise. We let W (·) be the standard Brownian motion and use ⇒ to denote
weak convergence.



2   The AR(1) Model



To systematically motivate the idea behind QD estimation, we begin with the simple AR(1) model
with parameter α and whose true value is α0 . For t = 1, . . . , T , the data are generated by

                                        yt = α0 yt−1 + εt , y0 = 0.                              (2)

Hereafter, we let εt be the deviation between the dependent variable and the conditional mean
evaluated at the true parameter value, while et is the deviation evaluated at an arbitrary value of
the parameter vector. The error εt does not need to be iid or Gaussian, but it cannot be conditional
heteroscedastic or heteroscedastic.



Assumption A (εt , Ft ) is a stationary ergodic martingale-difference sequence with conditional
variance E(ε2t |Ft−1 ) = σ 2 = γ0 and E((ε2t − σ 2 )2 |Ft−1 ) = µ4 .




                                                          2
                                                                                                    1   PT
The least squares estimator α
                            bOLS is defined as the solution to g(α) = 0, where g(α) =               T    t=1 et yt−1
is the sample analog of the moment condition

                                             Egt (α0 ) = E[εt yt−1 ] = 0,
                                                          √
with et = (yt − αyt−1 ). When α0 < 1, α
                                      bOLS is                 T consistent and asymptotically normal. While
α
bOLS is super-consistent at     α0   = 1, its distribution is non-standard which makes inference difficult.
In particular, the t-statistic for testing α0 = 1 is non-normal in finite samples and converges to
the so-called Dickey-Fuller distribution. The issue of non-standard inference arises because of two
problems. First, when α0 = 1, the sample moment evaluated at a value α 6= α0 = 1 explodes, and
second, the normalized sample moment evaluated at the true value does not obey a central limit
theorem. Specifically, g(α) = T1 Tt=1 et yt−1 is stochastically unbounded, and T1 Tt=1 εt yt−1 ⇒
                                  P                                              P
   R1
σ 2 0 W (r)dW (r), where W (r) is the standard Brownian motion.


Our starting point is to resolve the second issue by exploiting the autocovariance structure of the
errors. Specifically, for j ≥ 1, it holds that

                                                  E(εt εt−j ) = 0.                                               (3)

Furthermore, for all |α0 | ≤ 1 and εt = yt − α0 yt−1 , the population moment condition has a sample
analog that obeys a central limit theorem:
                                                         
                      √           T
                              1 X
                        T           εt εt−j − E(εt εt−j ) ⇒ ξj ∼ N (0, σ 4 ).                                  (4)
                              T
                                     t=j+1



Obviously, α0 is unknown and εt is not observed. However, we can quasi-difference yt at some
α and then optimize over all possible values of α by matching the sample autocovariances of the
quasi-differenced data2
                                                           T
                                                         1 X
                                              γ
                                              bj (α) =       et et−j
                                                         T
                                                          t=j+1

with those of the model evaluated under the assumption that α is the true value. Precisely, the
model implied moments are

                                      γj (α) = Eα et et−j = I(j = 0)σ 2 ,

where Eα is the expectation taken under the (not necessarily correct) assumption that α is the
true value. In cases when γj (α) is constant, the dependence of γj on α can be suppressed. For
   2
    To be more precise, we should write γ  bj (α, α0 , σ 2 ) because the data are generated under α0 and σ 2 , and we
quasi-difference the data at α. For notational simplicity, the dependence of γ bj on α0 and σ 2 are suppressed.


                                                          3
example, in the AR(1) under consideration, γj (α) = 0 for all j ≥ 1, and we may write γj instead
of γj (α). This is also true of the AR(p) and predictive regressions considered in the sections 3 and
4. However, in more complex models such as the one considered in Section 5, γj often depends on
α in a complicated and analytically intractable way. For clarity, we keep the explicit dependence
of γj on α throughout.


Let g N QD (α) = (g 0,N QD (α), . . . , g K,N QD (α))0 , where

                                                        bj (α) − γj (α),
                                         g j,N QD (α) = γ

be the difference between the model-implied and the sample autocovariance of et . The estimator
is defined as
                                   bK,N QD = argmin g N QD (α)0 WT g N QD (α).
                                   α
                                                    α
Although α
         bK,N QD is a standard GMM estimator, viewing it from the perspective of a covariance
structure estimator helps understand the analysis to follow. In standard covariance structure
estimation where a typical element of g(α) is g j (α) = T1 Tt=1 yt yt−1 − Eα yt yt−1 , each sample
                                                          P

autocovariance is a function of the data yt and hence depends on α0 , but it does not depend on α.
In our g j,N QD (α), both the sample and model moments depend on α. It differs from the standard
                                                                            √
formulation of covariance structure estimation but it is necessary because T ( T1 Tt=1 yt yt−1 −
                                                                                   P

Eα yt yt−1 ) does not obey a central limit theorem at α = α0 = 1.

                                                                                            √
The NQD solves one of the two problems inherent in least squares estimation by making           T g j,N QD (α0 )
asymptotically normal for all |α0 | ≤ 1. However, the estimator still has non-standard properties
        bj (α) is stochastically unbounded when α0 = 1 and α 6= 1. Thus the moment g j,N QD (α)
because γ
explodes at α 6= α0 when α0 is unity or in the neighborhood of one. To resolve this problem,
suppose γ0 is known and define

                               g j,F QD (α) = g j,N QD (α) − g 0,N QD (α)
                                                                           
                                            =    bj (α) − γ
                                                 γ        b0 (α) − γj (α) − γ0 ,                         (5)

                                   bK,F QD = argmin g F QD (α)0 WT g F QD (α).
                                   α
                                                    α

Obviously, g j,F QD   (α0 )   obeys a central limit theorem. More important is that normalizing γ
                                                                                                bj (α) by
γ
b0 (α) and γj (α) by γ0 yield
                                                          T
                                                        1 X             
                                   g j,F QD (α) =           et et−j − et + γ0 .
                                                        T
                                                          t=j+1


                                                           4
                                       bj (α) − γ
As shown in Lemma A-2 of the Appendix, γ        b0 (α) is bounded in probability in the limit even
when α0 = 1 and α 6= α0 . Because g j,F QD (α) is uniformly bounded in probability for all values
over which α is optimized and α0 ∈ (1 − δ, 1], the FQD has very different properties from the NQD
in the local-to-unity framework.


To gain insight about the importance of normalization, we let WT be an identity matrix as it does
not affect uniformity arguments and this simplifying assumption makes it possible to obtain useful
closed form expressions.



Proposition 1 Let yt be generated as in equation (2) with error terms satisfying Assumption A.
Assume that WT is a K × K identity matrix.


                                                                                                PK         2
   i. Let n ∈ {1, 2} be the number of local minima in the optimization problem minα                  k=1 g k,N QD (α).
     In the local-to-unity framework in which α0 = 1 + c/T with c ≤ 0, α
                                                                       bK,N QD is super-consistent
     and                                                                  −ξ                        
                                                                                    I{ξ < 0}
                                T 3/2 (b          α0 )2
                                                                     R1
                     I{n = 2} ·              −
                                       αK,N QD                            J 2 (s)ds
                                                                       R01 c
                                                              ⇒                                     ,            (6)
                                     αK,N QD − α0 )
                       I{n = 1} · T (b                              1+2 0 Jc (s)dW (s)
                                                                                       I{ξ >   0}
                                                                      2 01 Jc2 (s)ds
                                                                        R


     where Jc is an Ornstein-Uhlenbeck process generated by the Brownian motion W that is in-
     dependent of ξ ∼ N (0, 1/K).

  ii. Let γ0 be the true value of σ 2 . For any fixed K > 1, the estimator α
                                                                           bK,F QD is consistent.
     Furthermore, uniformly over −1 + δ < α0 ≤ 1:
                                                                        P                 2
                                                                           K     0 (k−1)        µ4       PK        0 )2(k−1)
     √                 0            2                      2               k=1 (α )             σ4
                                                                                                     +    k=1 (α
            αK,F QD −α ) ⇒
         T (b                N (0, σK,F QD ),    where    σK,F QD   =              P                     2                   .
                                                                                       K     0 2(k−1)
                                                                                       k=1 (α )



The NQD estimator is the basis of the estimators we subsequently investigate. It is consistent
and has a data dependent convergence rate. Since the objective function is a polynomial of the
fourth order, there are multiple solutions. If the realization of data is such that there is a unique
minimum to the optimization problem, the convergence rate is T . If there are two minima, a slower
convergence rate of T 3/4 is obtained. In either case, the distribution of α
                                                                           bK,N QD is not asymptoti-
cally normal because g N QD (α) is not well behaved for all values of α0 . However, the problematic
                                              bj (α) around α0 is asymptotically collinear with the
term that frustrates a quadratic expansion of γ
corresponding term in γ
                      b0 (α) in the local-to-unity framework. Normalizing each γ
                                                                               bj (α) by γ
                                                                                         b0 (α) and

                                                      5
γj (α) by γ0 results in an FQD estimator whose asymptotic distribution is normal uniformly over
α0 ∈ (−1 + δ, 1]. When K > 1, the FQD objective function has only one minimum asymptotically.3


                   bK,F QD are stated assuming the true value of σ 2 is known. The reason why
The properties for α
γ0 = σ 2 is not freely estimated along with α is that doing so would yield multiple solutions. The
objective function is zero not only at the true solution α = α0 , σ 2 = γ0 , but also at α = 1/α0 , σ 2 =
γ0 /(α0 )2 . Without additional information, the FQD cannot uniquely identify α and σ 2 .


In practice, the true value of σ 2 is not known, and the FQD estimator is infeasible. This can be
overcome by finding another moment that can identify σ 2 . Let θ = (α, σ 2 ) and consider
                                                  s2 − σ 2
                                                                      

                                               b0 (α) − (γ1 (α) − s2 ) 
                                     b1 (α) − γ
                                    γ
                         g QD (θ) =                                   .                                          (7)
                                                                       
                                                      ..
                                                      .               
                                                       b0 (α) − (γK (α) − s2 )
                                              bK (α) − γ
                                              γ
              1 0
where s2 =    T y M y,   and M = IT − z(z 0 z)−1 z 0 is the matrix that projects onto the space orthogonal
to z with zt = yt−1 . Observe that the first component of g QD (θ) is s2 −σ 2 or equivalently s2 −γ0 (α),
but not s2 − γ
             b0 (α). Thus, g QD (θ) is not a linear transformation of g N QD (θ). Using s2 − γ
                                                                                             b0 (α) in
the first entry would result in an estimator with the same non-standard properties like α
                                                                                        bN QD .


Let

                         θbK,QD = (b
                                   αK,QD , σ 2
                                           bK,QD ) = argmin g QD (θ)0 WT g QD (θ)
                                                                  θ



Proposition 2 Let WT be a (K + 1) × (K + 1) identity matrix. For any fixed K > 1, the estimator
bK,QD is consistent, and the following convergence holds uniformly over −1 + δ < α0 ≤ 1:
α
                                                                 PK
            √               0            2              2               (α0 )2(k−1)
                  αK,QD − α ) ⇒ N (0, σK,QD ), where σK,QD = P k=1
               T (b                                                                2 .
                                                                   K      0 2(k−1)
                                                                   k=1 (α )



          bK,QD can also be implemented as a two-step estimator in which s2 is first obtained, and
Estimator α
its value would then be used as γ0 in the moment function g F QD (α) = (g 1,F QD (α), . . . , g K,F QD (α))0 .


                           bK,QD takes into account the sampling uncertainty of s2 . The surprising
The asymptotic variance of α
aspect of Proposition 2 is that α
                                bK,QD does not have an inflated variance as is typical of two-step
   3
     When the NQD has two local minima, it does not matter which one is chosen as they are asymptotically symmetric
around the true value. This was why we state our result as T 3/2 (b
                                                                  αK,N QD − α0 )2 rather than T 3/4 (b
                                                                                                     αK,N QD − α0 ). If
                                                                                        0
K = 1, the FQD objective function has 2 minima, only one of which is consistent for α .


                                                          6
                                                          bK,F QD that has a known σ 2 . Pierce (1982)
estimators. Instead, the estimator is more efficient than α
showed in a framework for stationary data that using estimated values of nuisance parameters can
yield statistics with smaller variance than if the nuisance parameters were known. This somewhat
paradoxical result was also reported by Prokhorov and Schmidt (2009) and Han and Kim (2011)
for GMM estimators. Our results suggest that this feature may also arise in the local-to-unity
framework.


The closed form expression for the asymptotic variance of α
                                                          bK,QD in Proposition 2 was obtained
under the assumption that WT is an identity matrix. For an arbitrary positive-definite weighting
matrix, the asymptotic variance of α
                                   bK,QD is the (1,1)-th element of asymptotic variance matrix

                      Avar(θbK,QD ) = (G00 W G0 )−1 G00 W S 0 W G0 (G00 W G0 )−1                  (8)

where W, G0 and S 0 are the probability limits of WT , the derivative of g QD (θ) with respect to θ
evaluated at θ0 , and the asymptotic variance of g QD (θ0 ), respectively. The asymptotic variance
can thus be estimated as though the GMM estimator were developed in the stationary framework
under regularity conditions such as those given in Newey and McFadden (1994). In theory, more
efficient estimates can be obtained if WT is an optimal weighting matrix. However, it has been
documented in Abowd and Card (1989) and Altonji and Segal (1996) that an optimal weighting
matrix may not be desirable for covariance structure estimation for empirically relevant sample
sizes.


The key to the classical properties of α
                                       bK,QD is the ability to exploit the autocovariance properties
of the quasi-differenced data in an appropriate way. Quasi-differencing has a long tradition in
econometrics and underlies GLS estimation, see Phillips and Xiao (1998). Canjels and Watson
(1997) and Phillips and Lee (1996) found that quasi-differencing gives more precise estimates of
trend parameters when the errors are highly persistent. Pesavento and Rossi (2006) suggest that
for such data, quasi-differencing can improve the coverage of impulse response functions. In both
studies, the data are quasi-differenced at α = α which is fixed at the value suggested by the local-
to-unity framework. In contrast, the FQD and QD simultaneously estimate this parameter and
use the normalized autocovariances of the quasi-differenced data for estimation. Notably, both the
FQD and the QD have classical properties that hold even in the presence of deterministic terms.
Consider data generated as

                                          yt = dt + xt ,                                         (9a)
                                          xt = α0 xt−1 + εt .                                    (9b)


                                                  7
                                                          Pr         j
The deterministic terms are captured by dt =                 j=0 ψj t    where r is the order of the deterministic
trend function. In the intercept-only case, dt = ψ0 , and in the linear trend case, dt = ψ0 + ψ1 t.
Once the parameters of the trend function are consistently estimated, QD estimation proceeds by
replacing yt with demeaned or de-trended data, xbt = yt − dbt . Let ebt = x
                                                                          bt − αb
                                                                                xt−k . The sample
autocovariances can be constructed as
                                                           T
                                                         1 X
                                              γ
                                              bk (α) =       ebt ebt−k .
                                                         T
                                                            t=k+1

Demeaning and de-trending do not affect the asymptotic distribution of the QD.4


The practical appeal of QD estimation is that asymptotic normality permits standard inference.
The usual critical values of ±1.96 and ±1.64 can be used for two-tailed tests at the 5 and 10 percent
significance levels, respectively. We will see in simulations that the size of tests and the coverage
of the confidence sets based on the asymptotic normality of α
                                                            bK,QD are stable over the parameter
set α0 ∈ (−1 + δ, 1]. The cost of imposing the stronger assumption of conditional homoscedasticity
seems well justified.


To recapitulate, the proposed QD estimation of the AR(1) model is based on two simple premises:
first, that for all j ≥ 1, E(εt εt−j ) = 0 and its sample analog obeys a central limit theorem, and,
second, that the objective function is uniformly bounded in probability for all values of α and α0 .
The idea can be used whenever the variables can be quasi-differenced to form suitably normalized
moment conditions that satisfy these two properties. The next two sections consider the AR(p)
model and predictive regressions, respectively. We then show that the quasi-differenced variables
can be serially correlated and that the QD framework can be used in non-linear estimations.



3       AR(p) Models



Consider the data generating process
                                                           p−1
                                                           X
                                                 0
                                         yt = α yt−1 +           b0j ∆yt−j + εt .                                   (10)
                                                           j=1

    4
      De-trending does not affect the asymptotic distribution of FQD, but the Jc in the distribution of the NQD
estimator will depend
                  R1     on dt . In the intercept only case, one should use the de-meaned Ornstein-Uhlenbeck
                                                                                                     R1            process
J c (r) = Jc (r) − 0 Jc (s)ds. In the linear trend case, the de-trended process is Jec (r) = Jc (r) − 0 (4 − 6s)Jc (s)ds −
  R1
r 0 (12 − 6s)J(s)ds.



                                                            8
Let β = (α, b1 , . . . , bp−1 ) be a p × 1 parameter vector of interest. The true parameter vector is
denoted β 0 , and the correct lag length is denoted p. Let |λ1 | ≤ |λ2 | . . . ≤ |λp | be defined implicitly
by the identity
                                       p−1
                                       X
                            1 − αL −          bj Lj (1 − L) = (1 − λ1 L) . . . (1 − λp L).
                                        j=1

We restrict the parameter set in such a way that the p−1 smallest roots do not exceed δ in absolute
value for some fixed 0 < δ < 1. If the largest root exceeds δ in absolute value, then it is positive
and not larger than 1.


Definition 2 The parameter set Rδ consists of all β such that the corresponding roots satisfy the
following two conditions: (i) |λp−1 | < δ, (ii) if λp ∈ R, then −δ ≤ λp ≤ 1.5


Define the quasi-differenced series et by
                                                                p−1
                                                                X
                                         et = yt − αyt−1 −            bj ∆yt−j .
                                                                j=1

Obviously, et = εt is white noise when β = β 0 , but et is in general serially correlated. Thus, as in
the AR(1) model, the model-implied autocovariances satisfy

                                 γj (β) = Eβ (et et−j ) = 0,      j≥1             ∀β ∈ Rδ

with γ0 (β) = σ 2 . The sample autocovariances of et are
                                                            T
                                                       1    X
                                              γ
                                              bj (β) =                et et−j .
                                                       T
                                                           t=j+p+1

             1 0
Let s2 =     T y My    where M projects onto the space orthogonal to the one spanned by Xt =
(yt−1 , ∆yt−1 , . . . , ∆yt−p+1 )0 , t = 1, . . . , T . Let γ0 be the true value of σ 2 . Define

                                  βbK,F QD = argmin g F QD (β)0 WT g F QD (β),
                                                    β

where                                                                              
                                   g 1,F QD (β)      b1 (β) − γ
                                                     γ        b0 (β) − (γ1 (β) − γ0 )
                    g F QD (β) =         ..                         ..
                                                =                                   .
                                                                                   
                                           .                          .
                                  g K,F QD (β)     γbK (β) − γb0 (β) − (γK (β) − γ0 )
   5
    The optimization in equation (1) is done over a bounded set that includes a neighborhood of Rδ in order to avoid
the boundary problem.


                                                            9
Define
                               (βbK,QD , σ 2
                                         bK,QD ) = argmin g QD (β, σ 2 )0 WT g QD (β, σ 2 ),
                                                        β,σ 2

where
                                                                    s2 − σ 2
                                                                                              
                                         g 0,QD (β)
                                       g
                                        1,QD (β)   γ          b0 (β) − (γ1 (β) − s2 )
                                                      b1 (β) − γ                             
                      g QD (β, σ 2 ) =               =                                        .
                                                                                               
                                              ..                      ..
                                              .                       .                     
                                                                bK (β) − γ
                                                                γ                            2
                                                                         b0 (β) − (γK (β) − s )
                                           g K,QD (β)




Proposition 3 Let yt be generated as in equation (10) with error terms satisfying Assumption
                                                    P             −1
                                                        K        0
A. Let ak = E[(Xt+k + Xt−k − 2Xt ) εt ] and G =         k=1 a k ak     . For any fixed K > p > 1,
the estimators βbK,QD and βbK,F QD are consistent. Furthermore, when WT is an identity weighting
matrix, the following results hold uniformly over β 0 ∈ Rδ :

        √                                                                               P
                                                                                               K
                                                                                                         P
                                                                                                            K
                                                                                                                     0
  (i)       T (βbK,F QD − β 0 ) ⇒ N (0, ΣK,F QD ), where ΣK,F QD = σ 4 G + µ4 G                k=1 ak       k=1 ak        G;
        √
 (ii)       T (βbK,QD − β 0 ) ⇒ N (0, ΣK,QD ), where ΣK,QD = σ 4 G.



The proof is a generalization of Propositions 1 and 2. A sketch of the arguments is as follows. From
the definition that et (β) = εt + (β 0 − β)0 Xt , we have
                     T                                  T                                               T
                1    X                            1
                                                  0
                                                        X                                        1        X
γ
bj (β) =                      εt εt−j         0
                                        + (β − β)                 Xt εt−j + Xt−j εt + (β 0 − β)0                       0
                                                                                                                   Xt Xt−j (β 0 − β).
                T                                 T                                              T
                    t=j+p+1                           t=j+p+1                                            t=j+p+1

The moment function can be rewritten as

                     g j,F QD (β) = Aj,F QD + (β 0 − β)0 Bj,F QD + (β 0 − β)0 Cj,F QD (β 0 − β).

The thrust of the proof is to show that for each 1 ≤ j ≤ K uniformly over Rδ ,
                                                                        
                            T                            T
                      1    X                           1 X
         Aj,F QD =              εt εt−j − γj (β) −       ε2t − γ0 (β 0 ) = Op (T −1/2 ),                          (11)
                      T                                T
                                 t=j+p+1                               t=p+1
                                 T                                 
                          1      X
              Bj,F QD   =                 Xt εt−j + Xt−j εt − 2Xt εt →p aj ,                                          (12)
                          T
                              t=j+p+1
                                  T
                           1      X
                                               0
                                                   + Xt−j Xt0 − 2Xt Xt0 = Op (1).
                                                                       
              Cj,F QD =                    Xt Xt−j                                                                    (13)
                          2T
                                t=j+p+1



                                                                10
Equations (11)-(13) imply that the function g j,F QD (β) is bounded in probability uniformly for all
β in the optimization set and β0 ∈ Rδ . It also follows from equations (11)-(13) that

                                          ∂g j,F QD
                                                    (βbK,F QD ) →p aj ,
                                             ∂β
and
               √                               √                  √
                   T g k,F QD (βbK,F QD ) =        T Ak,F QD + a0k T (βbK,F QD − β 0 ) + op (1).

The first order condition for the optimization problem implies:

                           √                               K √
                                                           X
                               T (βbK,F QD − β 0 ) = G−1      T Aj,F QD aj + op (1)
                                                           j=1
                            √                  √
In view of equation (4),        T Aj,F QD =        T g j,F QD (β 0 ) ⇒ ξj − ξ0 uniformly over Rδ , part (i) of
the proposition follows. Part (ii) uses a similar argument with one exception: Aj,QD = Aj,F QD +
s2 − σ 2 = T1 Tt=j+1 εt εt−j . As in the AR(1) case, βbK,QD has a smaller variance than βbK,F QD .
              P

Furthermore, one can use other weighting matrices in the estimation. The asymptotic variance of
βbK,QD can be computed from the expression given in equation (8).



4     Predictive Regressions



Consider the predictive regression with scalar predictor xt−1 :

                                               yt = β 0 xt−1 + εyt                                      (14a)
                                              xt = α0 xt−1 + εxt .                                      (14b)



If α0 = 1, then (1; −β 0 ) is a co-integrating vector, and ordinary least squares provide super-
consistent estimates but inference is non-standard. Unfortunately, the finite sample distribution of
βbOLS is not well approximated by the normal distribution if xt is highly persistent. The challenge is
                                                                                    0
how to conduct inference robust to the dynamic properties of xt . Let εt = εyt , εxt be a martingale-
                                                           
                                                   σyy σyx
difference sequence with E(εt ε0t |Ft−1 ) = Ω0 =              . Consider quasi-differencing the data
                                                   σyx σxx
at θ = (β, α) to obtain

                                              eyt = yt − βxt−1 ,
                                              ext = xt − αxt−1 .

                                                        11
                                                                     
                                        yt                             e
Now Yt =    θ0 x   t−1 + εt where Yt =     and θ = (β , α ) . Let et = yt . Then
                                                0    0   0 0
                                        xt                             ext

                                            et = (θ0 − θ)xt−1 + εt .


Let Γj (θ) = Eθ (et e0t−j ) where Eθ is the expectation taken under the assumption that θ is the true
value. The model implies

                                             Γj (θ) = 0,          j 6= 0,
                                             Γ0 (θ) = Ω.

The sample autocovariance at lag j is
                                                       T
                                                     1 X
                                            Γj (θ) =
                                            b            et e0t−j .
                                                     T
                                                          t=j+1
                                  √
                                     b j (θ0 ). Evaluating Γ0 at the true value of Ω and letting
A central limit theorem applies to T Γ
     1 0
S=   T Y MY    , M = IT − z(z 0 z)−1 z 0 , zt = xt−1 , we can define, for j = 1, . . . , K:
                                                                                 
                                               b j (θ) − Γ
                            g j,F QD (θ) = vec Γ         b 0 (θ) − (Γj (θ) − Γ0 ) .

Let g F QD (θ) = (g 1,F QD (θ)0 , . . . , g K,F QD (θ)0 )0 . The FQD estimator is

                                θbK,F QD = arg min g F QD (θ)0 WT g F QD (θ).
                                                      θ

Analogously, let g QD (θ, Ω) = (g 0,QD (θ, Ω)0 , . . . , g K,QD (θ, Ω)0 )0 where
                                                                            
                                            b j (θ) − Γ
                        g j,QD (θ, Ω) = vec Γ         b 0 (θ) − (Γj (θ) − S) ,      j≥1
                        g 0,QD (θ, Ω) = vech(S − Ω).

Define the QD estimator as

                                    b K,QD ) = arg min g QD (θ, Ω)0 WT g QD (θ, Ω).
                          (θbK,QD , Ω
                                                          θ,Ω



Proposition 4 Suppose that the data are generated according to formulas (14a) and (14b). Sup-
pose also that error terms are stationary martingale-difference sequence with E(εt ε0t |Ft−1 ) = Ω0
and finite four moments. Define aj = E[xt−1 (εt−j − εt )]. Then for any fixed K > 1, the estima-
tors θbK,F QD and θbK,QD are consistent. Furthermore, when WT is an identity matrix, the following
asymptotic results hold uniformly over all possible values of β, and uniformly over all possible values
of α ∈ (−1 + δ, 1]:

                                                          12
                               √
  (i) Let Γ0 = Ω0 . Then        T (θbK,F QD − θ0 ) ⇒ N (0, ΣK,F QD ), where
                                          !2 K                                !2 "     K
                                                                                                      #
                                  1         X
                                                   0 0     0          1             0
                                                                                      X
                                                                                               2    0
                ΣK,F QD =     PK                (ak Ω ak )Ω + PK                E (εt     a j ) εt εt ;
                                     0                                   0
                               k=1 ak ak    k=1                     k=1 ak ak         k=1
        √
 (ii)       T (θbK,QD − θ0 ) ⇒ N (0, ΣK,QD ) where
                                                                    !2   K
                                                          1              X
                                       ΣK,QD =    PK                       (a0k Ω0 ak )Ω0 .
                                                             0
                                                        k=1 ak ak        k=1



As in the case of autoregressions, the FQD moments alone cannot globally identify both θ and Ω.
Thus, the properties of θbK,F QD are stated by evaluating Ω at the true value of Ω0 . Proposition 4
shows that θbK,QD has classical properties both in the stationary and the local-to-unity framework
and is more efficient than the estimator θbK,F QD that uses the known Ω. The QD can be implemented
as a sequential estimator in which the covariance matrix is computed for shocks obtained from two
least squares regressions: one by regressing yt on xt to get ebyt , and another autoregression in xt to
obtain ebxt .


Proposition 4 has useful implications for applied work because there does not exist an estimator
that is robust to the persistent properties of the predictors. The approach of Jansson and Moreira
(2006) relies on model-specific conditional critical values and, in any event, their inference procedure
does not yield an estimator per se. In contrast, the QD estimator is simple and robust.


The predictive regression can be generalized to accommodate stationary and pre-determined re-
gressors, zt . Suppose the data generating process is

                      yt = xt−1 β 0 + zt γ 0 + εyt
                                                                                               
                               0                               0                       σyy σyx
                      xt = α xt−1 + εxt ,         (εyt , εxt ) ∼ (0, Ω),        Ω=                .
                                                                                       σxy σxx .

Let θ = (β, α) and as before, Γj = Eθ (εt ε0t−j ) = 0 for all j =
                                                                6 0 with Γ0 = Ω0 . Let γ
                                                                                       bOLS be obtained
from least squares regression of yt on xt−1 and zt , and let Ω  b OLS be the estimated covariance matrix
                                                               √
of the errors. Since zt is stationary, the estimator γ
                                                     bOLS is T consistent and asymptotically normal
uniformly over α ∈ (−1 + δ, 1]. Define the quasi-differenced sequence
                                                                                                   
                                                                                                eyt
                        eyt = yt − xt−1 β − zt γ
                                               bOLS ,     ext = xt − αxt−1 ,         et =             .
                                                                                                ext
                  1   PT         0
Let Γ
    b j (θ) =
                  T    t=j+1 et et−j
                               and define g j,QD (θ, Ω) as in the absence of zt . Using arguments
                                                                 √
analogous to Proposition 4, it can be shown that θbK,QD is still T consistent and asymptotically

                                                          13
normal. Proposition 4 assumes that the regression error εyt is white noise. This is not restrictive
as lags of ∆yt and ∆xt can be added to zt to control for residual serial correlation.



5   Non-Linear Models and Minimum Distance Estimation



So far, the QD framework has been used to estimate linear models, where the model autocovariances
are such that γj (θ) = 0 for all θ assumed to be the true value and j ≥ 1. The analysis also holds if
γj (θ) equals a constant vector other than zero provided that the constant vector is known or can be
computed numerically. For example, if xt is an ARMA(1,1) instead of an AR(1), γj (θ) will depend
on the parameters of the model. Another example is DSGE models which we now consider.


To fix ideas, consider the simple one sector stochastic growth model presented in Uhlig (1999). Let
Qt , Ct , Kt , It be output, consumption, capital stock, and investment, respectively. The problem
facing the central planner is to maximize expected utility Et−1 ∞             −t log C subject to the
                                                                 P
                                                                   t=0 (1 + ρ)        t
                  ψ
constraints Qt = Kt−1 Zt1−ψ = Ct + It and Kt = (1 − δ)Kt−1 + It where ut = log Zt evolves as

                                    ut = αut−1 + εt ,   εt ∼ (0, σ 2 ).

Denote the deviation of a variable from its mean by lower case letters. Let Yt = (y1t , . . . , yN t )0 be
the collection of endogenous variables in the model (such as consumption, output, etc.). As shown
in Uhlig (1999), this simple model has an analytic solution:

                                          kt = vkk kt−1 + vkz ut

where vkk < 1 does not depend on α, but vkz depends on α. For each ynt ∈ Yt ,

                                 (1 − vkk L)(1 − αL)ynt = ut + ϑn ut−1

is an ARMA(2,1) with a moving-average parameter ϑn that is a function of the structural pa-
rameters. Note that all series in Yt have the same autoregressive dynamics as kt . The param-
eters of the linearized solution are β = (vkk , vkz , ϑ1 , . . . , ϑN ). The parameters of the model are
θ = (ψ, α, σ 2 , ρ, δ). Let Θ be a compact set containing possible values of θ.


In analysis of DSGE models, whether the shocks have permanent or transitory effects matter for
how a model is to be linearized. For this reasons, researchers typically need to decide whether to
difference the data ahead of estimation even though it is understood that the assumption affects
the estimates and policy analysis. To date, there does not exist an estimator of DSGE models that

                                                   14
has classical properties for all values of α within the likelihood framework because the likelihood
function is not well defined when the data are non-stationary.6


We propose to estimate the parameters of the model without making a priori assumptions about
the degree of persistence of the shocks. We use the fact that the features of covariance station-
ary processes are completely summarized by their second moments. Conveniently, the software
DYNARE automatically calculates the covariance structure of the data. Even though the analysis
is not likelihood based, priors can still be incorporated using the approach of Chernozhukov and
Hong (2003).7 The key is to construct the moments g(θ) appropriately.


Two variations of the QD framework are considered. The first method proceeds as follows. For given
θ, let et = Yt −αYt−1 with Γj (θ) = E(et e0t ). Define the moment ωj (θ) = (Γj (θ)−Γ0 (θ)) whose sample
analog is ω          b j (θ) − Γ
           bj (θ) = (Γ         b 0 (θ)). Note that since et (θ) can be serially correlated, Γj (θ) need not be
a null matrix as in the applications considered thus far. Let g QD (θ) = (g 1,QD (θ)0 , . . . , g K,QD (θ)0 )0
                       ωj (θ) − ωj (θ)). The QD estimator considered in Gorodnichenko and Ng
where g j,QD (θ) = vec(b
(2010) is defined as8
                                       θbK,QD = argmin g QD (θ)0 g QD (θ).                                       (15)
                                                    θ∈Θ

As written, θbK,QD is an equally weighted estimator. An optimal weighting matrix can be used
subject to constraints imposed by stochastic singularity. In the one-shock stochastic growth model
considered, the autocovariance at lag one of both output and consumption can both be used to
construct an efficient θbK,QD , but additional autocovariances will not add independent information.
In contrast, the use of data for both output and consumption in likelihood estimation would not
even be possible.


Another QD-based estimator can be obtained if we entertain the possibility of a reduced form
model. Consider a finite order AR(p) model:
                                                          p−1
                                                          X
                                               0
                                        yt = a yt−1 +           b0j ∆yt−j + εtp                                  (16)
                                                          j=1

where β 0 = (a0 , b01 , .., b0p−1 ) = β(θ0 ) are the true ‘reduced-form’ parameters that can be computed
analytically or numerically. We also need the following:
   6
      Likelihood estimation is also problematic when there are more variables than shocks, a problem known as stochas-
tic singularity.
    7
      For an example of this implementation, see Coibion and Gorodnichenko (2011).
    8                               ∂g    (θ)
      Identification requires rank j,QD∂θ
                                              = dimθ. Now g j (θ) depends on θ through the parameters in the solution
to expectation equations. Formal identification conditions are given in Komunjer and Ng (2011).


                                                          15
Assumptions B. (Identification) (i) there is a unique θ0 such that β(θ0 ) = β 0 ; (ii) the function
β(θ) is twice continuously differentiable; (iii) B(θ0 ) = ∇β(θ0 ) has full rank k = dim(θ) ≤ p.


                                                                                          Pp−1
The method proceeds as follows. For ynt ∈ Yt , define ent (β) = ynt − aynt−1 −               j=1 bj ynt−j   where
β = (a, b1 , . . . , bp−1 ). Note that Yt is now quasi-differenced using the ‘reduced-form’ parameter
β instead of the structural parameter α as in method 1. Once the data are quasi-differenced,
                                                                                         bj (θ; p) − ωj (θ; p)
estimation proceeds by defining ωj (θ; p) = (Γj (θ; p) − Γ0 (θ; p)) with g j,QD (θ; p) = ω
and sample analog as in (15). Let g QD (θ; p) = (g 1,QD (θ; p)0 , . . . , g K,QD (θ; p)0 )0 . The estimator is

                                   θbK,QD = argmin g QD (θ; p)0 g QD (θ; p).                                 (17)
                                                θ∈Θ

Because β is p-dimensional, this second estimator also depends on the choice of p. Since ent is
not necessarily exactly white noise, Γj (θ) will not be zero. However, its autocovariances can be
computed for any given θ.


We have presented two uses of the QD framework that can yield estimators that are robust to
non-stationary exogenous variables in DSGE models. The data-dependent transformations allow
us to construct moments that are uniformly bounded. Applying the central limit theorem to the
sample moments yields estimators with classical properties.



6   Simulations



We consider the finite sample properties of OLS, FQD with γ0 fixed at the true σ02 , and QD. Even
though the FQD estimator is infeasible in practice, it is a useful benchmark. The simulations are
based on 2,000 replications. We use the standard Newey-West plug-in estimator for the variance
of the moments. As starting values, we use 0.9 times the true values of the parameters. The QD
estimator requires evaluation of the model implied autocovariances Γj (θ). This is straightforward
once a model is cast in a state space. For example, the system
                                                        
                               yt         0 β yt−1        1 0 εyt
                                    =                 +
                               xt         0 α xt−1        0 1 εxt

in quasi-differenced form is
                                                        
                           eyt (θ)       0 0 eyt−1 (θ)     1 0 εyt
                                     =                  +          .
                           ext (θ)       0 0 ext−1 (θ)     0 1 εxt



                                                       16
More generally, every ARMA model has a state-space representation from which a state-space
model for the quasi-differenced data can be expressed as

                                         wt = D0 wt−1 + D1 εt

where wt includes et (θ) (and possibly its lags), and εt is the set of exogenous white noise shocks with
variance Ωε . The variance matrix Ωw (0) = E(wt wt0 ) can be computed by iterating the equation

                                 Ω(i)         (i−1)
                                  w (0) = D0 Ωw     (0)D00 + D1 Ωε D10                             (18)

until convergence. The autocovariance matrices can then be computed as Ωw (j) = D0j Ωw (0). Now
Γj (θ) are submatrices of Ωw (j). If we are only interested in computing the moments wtd ⊂ wt , we
                                                                                (i)       (i−1)
iterate equation (18) until the block that corresponds to wtd converges, i.e. kΩwd (0)−Ωwd (0)k < c.


Data are generated from the AR(2) model:

                            (1 − λ01 L)(1 − λ02 L)yt = εt ,   εt ∼ N (0, 1).

The process can be written as
                                     yt = α0 yt−1 + b0 ∆yt−1 + εt .

The parameter of interest is α0 = λ01 + λ02 − λ01 λ02 with b0 = λ01 λ02 . The OLS estimate of α has a
non-standard distribution when the roots are unity, in which case α0 = 1.


We estimate an AR(2) model when λ02 = 0 (Table 1). Demeaned data are used to compute the
sample autocovariances in the intercept case, and linearly de-trended data are used in the linear
trend case. We report the mean of the QD, FQD, and OLS estimates when T = 200 and 500,
the J test for over-identifying restrictions, along with the finite sample power for one-sided t tests
evaluated at α = α0 − 0.05.


Table 1 shows that all three estimators are precise when α0 ≤ 0.8. The t-statistic for the null
hypothesis that α = α0 for all three estimators has rejection rates close to the nominal size of 0.05
when α0 ≤ 0.8. The picture is, however, very different at larger values of α0 . The FQD has slightly
smaller bias but is much less efficient. While OLS has the largest bias, its root-mean-squared error
(RMSE) is much smaller than the FQD. The QD is neither the most accurate nor the most efficient,
but has RMSE closer to OLS and much smaller than the FQD, in support of Proposition 2.


Efficiency of OLS comes at the cost of size distortion, however. At T = 200, the OLS-based t-
statistic has a rejection rate of 0.473 when α0 = 1 and 0.15 when α0 = 0.95, much larger than

                                                  17
the nominal size of 0.05. Even at T = 500, the rejection rates are 0.462 and 0.127, well above the
nominal rate of 5%. The rejection rates for the FQD and QD are 0.107 and 0.08 when T = 200,
and are 0.066 and 0.076 when T = 500, much closer to the nominal size of 0.05. The QD has
accurate rejection rates that are always around 0.05 for all values of α0 , but it has less power than
OLS. Figure 1 plots the distribution of t-statistics for QD at T =200 and T =500. The normal
approximation to the finite sample distribution is good.


We report results for over-parameterized AR(p) models in Table 2. In particular, we estimate an
AR(3) when the true model is an AR(2). We fix λ02 to 0.2 and set λ01 such that the sum of the
AR(2) parameters is the same as in Table 1. Statistics are reported for α
                                                                        b. The results are similar
to Table 1: the FQD is inefficient; the t statistic associated with it has good size but lower power.
The OLS has large size distortion when α0 is close to one, but the estimates are tightly estimated.
The QD strikes a balance between the two, and the t statistic has very similar size property for all
values of |α0 (1)| ≤ 1 even when T = 200 in the intercept case. In the linear time trend case, the
uniform size property can be achieved with T = 500. Importantly, the QD achieves a more accurate
size when α0 is close to unity without sacrificing too much power outside of the persistent range.
For example, when α0 is 0.5, the power of OLS and QD are quite similar. The reason is that when
                                      √
α0 is far from the unit circle, OLS is T consistent, but so is FQD and QD. Power is also fairly
similar for both the intercept only and the linear trend model.


Table 3 presents results for predictive regressions. Data for the predictive regressions are generated
as in (14a) and (14b). We let β 0 = 1, σyy
                                        0 = 1, σ 0 = 1, σ 0 = 0.5. The results are generally
                                                xx       yx
similar to the AR(p) case. The QD is slightly less efficient than OLS when α0 is close to one but no
less efficient when α0 is far from the unit circle. This loss of efficiency arises from the fact that at
                                                   √
α0 = 1, OLS is T consistent but the QD is only T consistent. The benefit to this small efficiency
loss is robustness. As one can see, the size of the t test for H0 : β = β 0 is fairly constant around
the nominal size of 0.05, but the size of the test based on OLS varies with α0 . Furthermore, as
 0 rises, the size of the t-test based on OLS becomes increasingly distorted while QD continues to
σyx
have sizes close to nominal.


Finally, we consider the stochastic growth model presented in Section 5. The data are generated
as follows. We fix the true value of capital intensity ψ to 0.25 and σ 2 to 1 and consider five values
of α: 1, 0.98, 0.95, 0.9, 0.8. We use data on consumption to estimate ψ, α and σ 2 using the second
method discussed in Section 5. This consists of solving for ψ, α and σ from the six autocovariances
of et (β), where β are the parameters of an AR(3) model. For the sake of comparison, we also use


                                                  18
the Kalman filter to obtain the maximum likelihood estimates. The results are reported in Table
4. Evidently, the QD estimates of ψ are close to the true value of 0.25, and the size of the t-test is
close to the nominal value for all values of α. In contrast, the MLE estimates are biased when α is
close to one, and the t test for the null hypothesis that ψ = 0.25 is severely distorted.


                            √
7       Relation to other       T -Consistent Linear Estimators



As discussed in Section 2, one of the problems with the OLS estimator when a unit root is present
                                                  √
is that the moment condition at the true value T g(α0 ) = √1T Tt=1 yt−1 εt does not satisfy a
                                                                 P

central limit theorem. Although yt−1 is orthogonal to εt , the persistence of yt−1 requires a stronger
normalization and standard distribution theory cannot be used. The thrust of QD estimator is
to use moment conditions that satisfy a central limit theorem uniformly over values of α0 . The
approach can be used to estimate a broad range of models. However, for the simple AR(1) model,
the ideas underlying the QD estimation can be used to construct a two-step linear estimator. We
now show how this can be done and then relate this approximate QD estimator to other known
linear estimators of the AR(1) model with classical properties in the local-to-unity framework.


For the AR(1) model, the QD moment condition (3) replaces yt−1 with εt−j . As seen from equation
(4), the central limit theorem holds whether or not there is a unit root present. But the moment
condition can be understood in an instrumental variable setup because εt−j is uncorrelated with
εt and is hence a valid instrument. The only problem is that εt−j is not observed. But α
                                                                                       bOLS is
consistent for all |α0 | ≤ 1. Thus, let eet−1 = yt − α
                                                     bOLS yt−1 , noting that generated instruments do
not require a correction for the standard errors like generated regressors do. We can now define a
(hybrid differencing) HD estimator using the following moment condition:9
                                                 T
                                               1 X
                              g HD (b
                                    αHD ) =        eet−k (yt − α
                                                               bHD yt−1 ) = 0.
                                               T
                                                 t=k+1

This leads to the estimator
                                          PT                           PT
                                              t=k yt e
                                                     et−k         0        et eet−k
                            α
                            bHD =        PT                   = α + PT t=k            .
                                            t=k yt−1 e
                                                     et−k            t=k y t−1  e
                                                                                e t−k

            bHD as a hybrid estimator because it is based on the covariance between the quasi-
We refer to α
difference of yt and a stationary random variable. Notice that the HD and QD use the same moment
    9
                                                                                              √
    Laroque and Salanie (1997) used two OLS regressions in stationary variables to obtain a    T -consistent estimate
of the co-integrating vector.


                                                         19
condition. What distinguishes the HD from the QD is that the objective function of the HD is now
                                                                                 p
                            bHD follows from the fact that T1 Tt=1 et (α0 )e
                                                             P
linear in α. Consistency of α                                              et−k −→Eεt εt−k = 0. It is
straightforward to show that in the local-to-unity framework,
                                   √
                                          αHD − α0 ) ⇒ 2(1 + Jc (1)2 )−1 N (0, 1).
                                       T (b



Once the HD is understood as an instrumental estimator, other possibilities arise. Instead of eet−1 ,
we can use any stationary series uncorrelated with the error term.10 For example, using ∆yt−1
would by-pass the need for a preliminary least squares estimation. The first differencing (FD)
estimator is:
                                                          PT
                                                                ∆yt−1 yt
                                            α
                                            bF D =       PT t=2          .
                                                          t=2 ∆yt−1 yt−1

The FD is a special case of estimators analyzed in So and Shin (1999). These authors used the sign
of yt−1 as instrument xt to construct
                                                          PT
                                                              xt yt
                                                 bSS = PT t=2
                                                 α                  .
                                                        t=1 xt yt−1



Another estimator with classical properties in the local-to-unity framework is that of Phillips and
Han (2008). The PH estimator, defined as
                                                 PT
                                                   t=2 ∆yt−1 (2∆yt + ∆yt−1 )
                                        α
                                        bP H =         PT                    ,
                                                             (∆y     )2
                                                         t=2     t−1
                             √
has the property that               αP H − α0 ) ⇒ N (0, 2(1 + α0 )) for all α0 ∈ (−1, 1]. As shown in the
                                 T (b
Appendix, the FD estimator is asymptotically equivalent to the PH estimator in the stationary
case when α0 is far from unit circle. That is, for the AR(1) model, α
                                                                    bP H = α
                                                                           bF D + Op (1/T ) under
stationary classical asymptotics. However, these two estimators differ in the local-to-unity setting.
       √                                                       √
While T (b αF D − α0 ) ⇒ 2(1 + W (1)2 )−1 N (0, 1) when α0 = 1, T (bαP H − α0 ) ⇒ N (0, 4). The FD
is thus more efficient at α0 = 1. Simulations presented in Table 5 show that the QD dominates the
FD and PH, but is comparable to the HD. The simulations also support the theoretical predictions
                                                           √
that the FD, HD, and QD are all asymptotically normal and T consistent.


No estimator is perfect and QD estimation has its drawbacks. As mentioned in the introduction,
                                                                            √
the price we pay for asymptotic normality is that α
                                                  bQD converges at a rate of T instead of T when
 10
      As suggested by a referee, E(et (et−j − et−k )) = 0, 1 ≤ j < k is also a valid moment condition.


                                                           20
there is a unit root. Han et al. (2011) aggregate L stationary moment conditions and showed that
                                                                                            √
by suitable choice of L, uniform asymptotic normality can be achieved at a rate faster than T .
Extension of their result outside of the AR(p) model is, however, not straightforward. In contrast,
the QD framework is broadly applicable.



8   Concluding Comments



In this paper, we use a quasi-differencing framework to obtain estimators with classical properties
even when the underlying data are highly persistent. Quasi-differencing can render non-stationary
processes stationary so that classical limit theorems can be applied. However, the QD estimator
  √
is T consistent rather than super-consistent in the local-to-unity framework. In exchange for
this slower convergence is generality, as QD estimation can be used in a broad range of linear and
non-linear models. However, there are several issues that remain to be solved. The first is allowing
J to be data dependent and increase with the sample size. The second is to allow for conditional
heteroscedasticity. Third, simulations suggest that the QD works well even when forcing process
is mildly explosive. Relaxing the assumption that the largest autoregressive root is inside the unit
disk may well be useful for practitioners. These issues are left for future investigation.




                                                  21
References

Abowd, J. M. and Card, D. 1989, On the Covariance Structure of Earnings and Hours Changes,
 Econometrica 57(2), 411–445.
Altonji, J. G. and Segal, L. M. 1996, Small-Sample Bias in GMM Estimation of Covariance Struc-
  tures, Journal of Business and Economic Statistics 14(3), 353–366.
Canjels, E. and Watson, M. W. 1997, Estimating Deterministic Trends in the Presence of Serially
  Correlated Errors, Review of Economics and Statistics 79(2), 184–200.
Chernozhukov, V. and Hong, H. 2003, An MCMC Approach to Classical Estimation, Journal of
 Econometrics 115(2), 293–346.
Coibion, O. and Gorodnichenko, Y. 2011, Strategic Interaction Among Heterogeneous Price Setters
  in Estimated DSGE Model, Review of Economics and Statistics 93(3), 920–940.
Gorodnichenko, Y. and Ng, S. 2010, Estimaton of DSGE Models When the Data are Persistent,
 Journal of Monetary Economics 57(3), 325–340.
Han, C. and Kim, B. 2011, A GMM Interpretation of the Paradox in the Inverse Probability Weight-
 ing Estimation of the Average Treatment Effect on the Treated, Economics Letters 110, 163–165.
Han, C., Phillips, P. C. B. and Sul, D. 2011, Uniform Asymptotic Normality in Stationary and
 Unit Root Autoregression, Econometric Theory, forthcoming.
Jansson, M. and Moreira, M. J. 2006, Optimal Inference In Regession Models with Nearly Integrated
  Regressors, Econometrica 74, 681–714.
Komunjer, I. and Ng, S. 2011, Dynamic Identification of DSGE models, Econometrica, forthcoming.
Laroque, G. and Salanie, B. 1997, Normal Estimators for Cointegrating Relationships, Economics
  Letters 55(2), 185–189.
Mikusheva, A. 2007a, Uniform Inference in Autoregressive Models, Econometrica 75(5), 1411–1452.
Mikusheva, A. 2007b, Uniform Inference in Autoregressive Models, Econometrica 75(5), Supple-
 mentary Material.
Mikusheva, A. 2011, One Dimensional Inference in Autogressive Models with the Potential Presence
 of a Unit Root, Econometrica. forthcoming.
Newey, W. K. and McFadden, D. 1994, Large Sample Estimation and Hypothesis Testing, Handbook
  of Econometrics, Vol. 4,Chapter 36, North Holland.
Pesavento, E. and Rossi, B. 2006, Small–sample Confidence Interevals for Multivariate Impulse
  Response Functions at Long Horizons, Journal of Applied Econometrics 21(8), 1135–1155.
Phillips, P. C. B. and Han, C. 2008, Gaussian Inference in AR(1) Time Series With or Without a
  Unit Root, Econometric Theory 24, 631–650.
Phillips, P. C. B. and Lee, C. C. 1996, Efficiency Gains from Quasi-differencing under Nonstation-
  arity, in P. Robinson and M. Rosenblatt (eds), Athens Conference on Applied Probability and
  Time Series: Volume II Time Series Analysis in Honor of E.J. Hannan.
Phillips, P. C. B. and Solo, V. 1992, Asymptotics for Linear Processes, Annals of Statistics
  20(2), 971–1001.

                                               22
Phillips, P. C. B. and Xiao, Z. 1998, A Primer on Unit Root Testing, Journal of Economic Surveys
  12(5), 423–469.
Pierce, D. A. 1982, The Asymptotic Effect of Substituting Estimators for Parameters in Certain
  Types of Statistics, Annals of Statistics 10(2), 475–478.
Prokhorov, A. and Schmidt, P. 2009, GMM Redundancy Results for General Missing Data Problem,
  Journal of Econometrics 151(1), 47–55.
So, B. S. and Shin, D. W. 1999, Cauchy Estimators for Autoregressive Processes with Applications
  to Unit Root Tests and Confidence Intervals, Econometric Theory 15(2), 165–176.
Uhlig, H. 1999, A Toolkit for Analyzing Nonlinear Dynamic Stochastic Models Easily, in R. Mari-
 mon and A. Scott (eds), Computational Methods for the Study of Dynamic Economies, Oxford
 University Press, pp. 30–61.




                                              23
Appendix A        Proofs


The proofs proceed with the assumption that the weighting matrix WT is an identity matrix.


Proof of Proposition 1 (i): First, consider the problem of matching the j−th autocovariance.
That is, Qj (α) = (bγj (α) − γj (α))2 , and α
                                            bj = arg minα Qj (α). Under the assumption that α is the
true value, γ0 (α) = σ 2 , and γj (α) = 0 for all j > 0. Note that
                  T                         T                                      T
                1 X                  α − α0 X                           (α − α0 )2 X
γ
bj (α)−γj (α) =     εt εt−j −γj (α)−          [εt yt−j−1 + εt−j yt−1 ]+              yt−1 yt−j−1 .
                T                      T                                    T
                   t=j+1                               t=j+1                                  t=j+1

As a result, the NQD objective function is the fourth-order polynomial:
                   (0)                   (1)              (2)              (3)               (4)
       Qj (α) = Qj + (α − α0 )Qj + (α − α0 )2 Qj + (α − α0 )3 Qj + (α − α0 )4 Qj .                    (A.1)

In the local-to-unity framework with α0 = 1 + c/T , the following results hold as T → ∞:
                                                          Z 1
                             1X
                                  εt−j yt−1 ⇒ σ 2 + σ 2        Jc (s)dW (s),                          (A.2)
                             T                              0
                                                    Z 1
                             1X                  2
                                  εt yt−1−j ⇒ σ         Jc (s)dW (s),                                 (A.3)
                             T                       0
                                                    Z 1
                          1 X                    2
                                yt−1 yt−j−1 ⇒ σ         Jc2 (s)ds.                                    (A.4)
                          T2                         0

It follows from equations (4) and (A.2) - (A.4) that
                                                          
                             √         T                          T
                     (1)           1  X                       1 X
              T 1/2 Qj = −2 T             εt εt−j − γj (α)          [εt yt−j−1 + εt−j yt−1 ]
                                  T                           T
                                     t=j+1                      t=j+1
                                          Z 1              
                         ⇒ −2ξj σ 2 + 2σ 2      Jc (s)dW (s) ;
                                                   0

                                                                                 
                            √        T                              T
                      (2)        1  X                          1   X
              T −1/2 Qj = 2 T           εt εt−j − γj (α)  2          yt−1 yt−j−1 
                                 T                           T
                                   t=j+1                          t=j+1
                                                               2
                                    T                                        Z 1
                            1 1 X                                         2
                          +√           [εt yt−j−1 + εt−j yt−1 ]   ⇒ 2ξj σ       Jc2 (s)ds;
                             T T           t=j+1                               0


                                                                                           
                                        T                        T
                     (3)           1   X                     1  X
              T −1 Qj      = −2  2          yt−1 yt−j−1            [εt yt−j−1 + εt−j yt−1 ]
                                  T                          T
                                     t=j+1                     t=j+1
                                   Z 1                     Z 1               
                           ⇒ −2σ 2     Jc2 (s)ds σ 2 + 2σ 2     Jc (s)dW (s) ;
                                     0                          0


                                                        24
                                                       T
                                                                             !2                                   2
                                                                                       Z             1
                                   −2    (4)        1 X
                              T         Qj     =         yt−1 yt−2                   ⇒ σ2                 Jc2 (s)ds     > 0,
                                                    T2                                            0
                                                              1
        √
            T ( T1        εt εt−j − γj ) ⇒ ξj = N (0, σ 4 ). To summarize:
                     P
where
                      (1)                               (2)                              (3)                          (4)
                     Qj     = Op (T −1/2 ),         Qj        = Op (T 1/2 ),          Qj       = Op (T 1 ),       Qj        = Op (T 2 ).   (A.5)
It follows that
                                                          (0)                                   2
                                          Qj (α) − Qj                            1
                                                                    Z
                                                                  ⇒ σ2               Jc2 (s)ds        (α − α0 )4
                                                   T2                        0
                                                             bj is a consistent estimate of α0 .
uniformly over a bounded parameter space for α. As a result, α

To study the large sample properties of α
                                        bj , consider the first order condition:
                             (1)                         (2)                             (3)                           (4)
                                   αj − α0 )Qj + 3(b
                            Qj + 2(b               αj − α0 )2 Qj + 4(b
                                                                     αj − α0 )3 Qj                                           = 0.          (A.6)

This is a cubic equation of the form ax3 + bx2 + cx + d = 0, where x stands for (b
                                                                                 αj − α0 ) with the
obvious correspondence between the coefficients. The cubic equation may have one or three real
roots depending on the sign of the determinant:
                                           ∆ = 18abcd − 4b3 d + b2 c2 − 4ac3 − 27a2 d2 .
Given the orders established in (A.5), it can be shown that
                                                   Z 1         5
                                −7/2            10
                              T      ∆ ⇒ −32σ           Jc (s)ds ξj3 .
                                                         2
                                                                                     0

The sign of the determinant ∆ is asymptotically defined by the sign of ξj . When the sign of ∆
is negative, there is a unique real root to equation (A.6); otherwise, there are three real roots.
However, in the case of three real roots, the middle one corresponds to the local maxima, while the
other two roots are the local minima of (A.1).

The next step is to work out the formulas for the roots and to check their rates of convergence
toward zero. For example, when there is only one real root, the formula is:
                  r                                     r                                 !
          1       3  3
                        9       27 2     1p       2      3  3
                                                                9      27 2   1p       2
  x1 = −      b + b − abc + a d +            −27a ∆ + b − abc + a d −             −27a ∆ .
         3a             2        2       2                      2       2     2

Using the asymptotic orders of the terms in (A.5) and after tedious algebra, we can deduce that
                          αj − α0 ) = Op (1)). Similarly, using the explicit formula for cubic roots and
T x1 = Op (1) (that is T (b
denoting the two non-central roots by x2 and x3 , we can deduce that when ∆ > 0, T 3/4 x2 = Op (1)
and T 3/4 x3 = Op (1) (that is, T 3/4 (b
                                       αj − α0 ) = Op (1) in this case).

                                                                                                   αj −
To find the asymptotic distribution of these roots, we start with the case of one root. Because T (b
α0 ) = Op (1), some terms in equation (A.6) are asymptotically negligible. Thus, asymptotically we
       (1)                (2)
have Qj + 2(b   αj − α0 )Qj = 0. Equivalently,
                                                                  (1)                        R1
                                               0
                                                              Qj                   σ 2 + 2σ 2 0 Jc (s)dw(s)
                                 αj − α ) = −T
                              T (b                                (2)
                                                                        + op (1) ⇒          R1              .
                                                              2Qj                       2σ 2 0 Jc2 (s)ds

                                                                            25
Similarly, for the case of two local maxima T 3/4 (b
                                                   αj − α0 ) = Op (1) and asymptotically 2(b
                                                                                           αj −
 0   (2)           0 3  (4)
α )Qj + 4(b  αj − α ) Qj = 0. Equivalently,
                                                                         (2)
                                     3/2             0 2         3/2
                                                                       Qj                  −ξj
                                 T          αj − α ) = −T
                                           (b                            (4)
                                                                               ⇒         R1          .
                                                                       2Qj         σ2        2
                                                                                          0 Jc (s)ds

The equation has a solution only when ξj < 0. As shown above, this is the condition for the
cubic equation to have three roots. Thus, we proved that equation (6) holds for α
                                                                                bj with K = 1.
                                              bK,N QD = arg minα K
                                                                  P
Analogous arguments hold in the general case α                          Q
                                                                    j=1 j  (α) with ξj replaced
   PK                    4
by j=1 ξj /K ∼ N (0, σ /K). This also shows that in AR(1) case, matching more than one auto-
covariance leads to increase in efficiency. 2

Proposition 1 (ii) and Proposition 2 are special cases of Proposition 3. Observe that
                    g j,F QD (β) = Aj,F QD + (β 0 − β)0 Bj,F QD + (β 0 − β)0 Cj,F QD (β 0 − β),
where Aj,F QD , Bj,F QD and Cj,F QD are defined in equations (11)-(13). The following three lemmas
will be used to prove Proposition 3.


Lemma A-1 (Uniform Law of Large Numbers) Let εt = (εt,1 , εt,2 )0 be martingale-difference
sequence with Ω = E(εt ε0t |Ft−1 ) and finite fourth moments, Ω = (σi,j ) and ηt,i = ∞     i
                                                                                     P
                                                                                      j=0 cj εt−j,i for
i = 1, 2. Uniformly over the set of all sequences cij satisfying ∞     i
                                                                P
                                                                 j=0 |cj | < C for some constant C,

                                                     T
                                                   1X
                                                       ηt,1 ηt,2 →p E[ηt,1 ηt,2 ].
                                                   T
                                                       t=1



Proof of Lemma A-1 Notice first that
                                                                                     ∞
                                      γji1 ,i2
                                                                                     X
                                                 = cov(ηt,i1 , ηt+j,i2 ) = σi1 ,i2         cin+j
                                                                                              1
                                                                                                 cin2
                                                                                     n=0
                              P∞       i1 ,i2
and for any i1 and i2 ,          j=0 |γj      |    < kΩkC 2 . Furthermore,
                                                                                                         ∞
           E[ηt,1 ηt,2 η,t+j,1 ηt+j,2 ] = (γ01,2 )2 + γj1,2 γj2,1 + γj1,1 γj2,2 + E(ε21,t ε22,t )
                                                                                                         X
                                                                                                                c1n c1n+j c2n c2n+j .
                                                                                                         n=0

As a result,
                                                                                                      ∞
                cov(ηt,1 ηt,2 , ηt+j,1 ηt+j,2 ) = γj1,2 γj2,1 + γj1,1 γj2,2 + E(ε21,t ε22,t )
                                                                                                      X
                                                                                                            c1n c1n+j c2n c2n+j
                                                                                                      n=0

and
∞                                                                                  ∞
                                                                                                  !2
X                                                                                  X
      cov((ηt,1 ηt,2 ), (ηt+j,1 ηt+j,2 )) ≤ 2kΩk2 C 4 + E(ε2t,1 ε2t,2 )                  |c1n c2n |     ≤ 2kΩk2 + E(ε2t,1 ε2t,2 ) C 4
                                                                                                                                 

j=1                                                                                n=0

Chebyshev’s inequality implies the statement of the Lemma. 2

                                                                   26
Lemma A-2 The following three statements hold asymptotically uniformly over Rδ and uniformly
over 1 ≤ j ≤ K

       √
 (a)     T (A1,F QD , ..., AK,F QD )0 ⇒ (ξ1 − ξ0 , ..., ξK − ξ0 ), where (ξ0 , ξ1 , ..., ξK )0 is a normally dis-
       tributed random vector with mean zero and diagonal covariance matrix, Eξ02 = µ4 , Eξj2 = σ 4
       for all 1 ≤ j ≤ K;
 (b) Bj,F QD →p aj = E [(Xt+j + Xt−j − 2Xt )εt ];
 (c) Cj,F QD = Op (1).


Proof of Lemma A-2: Part (a) follows from applying the Central Limit Theorem (4) to the
sums of ε2t − σ 2 and εt εt−j for 1 ≤ j ≤ K. For (b) we need to show that the Uniform Law of Large
Numbers holds for
                  T                                                      T −j
                                                                                                                    √
                                                                                                     
             1    X                                               1      X
Bj,F QD    =                   Xt εt−j + Xt−j εt − 2Xt εt       =                    Xt+j + Xt−j   − 2Xt εt + Op (1/ T ),
             T                                                    T
                 t=p+j+1                                               t=p+j+1
                √
where the Op (1/ T ) term appears due to the change of limits of summation by a finite number
of summands. To apply Lemma A-1, we need to show that the process Xt+j + Xt−j − 2Xt has
absolutely summable MA coefficients. From Lemma S8 in the web Appendix of Mikusheva (2007b),
the process Zt = Xt − Xt−1 has absolutely summable MA coefficients uniformly over Rδ . Now
                                                                 j
                                                                 X               j−1
                                                                                 X
                                     Xt+j + Xt−j − 2Xt =               Zt+k −          Zt−k .
                                                                 k=1             k=0

Our process of interest is the sum of a finite number of processes each with summable MA coefficients
and thus its MA coefficients are absolutely summable. Lemma A-1 implies that uniformly over Rδ
                                                                 
                                       p
                            Bj,F QD → E Xt+j + Xt−j − 2Xt εt = aj .



Turning to (c), the object of interest is the p × p matrix:
                                                  T
                                             1    X
                                                               0
                                                                   + Xt−j Xt0 − 2Xt Xt0 ,
                                                                                       
                                Cj,F QD =                  Xt Xt−j                                                (A.7)
                                            2T
                                                 t=j+p+1

where all elements except possibly the top-left element satisfy the uniform Law of Large Numbers,
and thus are of order Op (1). Now the last p−1 elements of Xt are ∆yt−1 , ..., ∆yt−p+1 . From Lemma
S8 in Mikusheva (2007b), they have absolutely summable MA coefficients uniformly over Rδ . Thus,
the elements of the right-bottom (p − 1) × (p − 1) sub-matrix satisfy conditions of Lemma A-1. The
elements in the first row and the first column (except the top-left element) are of the form
        T                                                      T                                       √
  1     X                                                    1 X
                 (yt−1 zt−j + yt−1−j zt − 2yt−1 zt ) =           (yt−1+j + yt−1−j − 2yt−1 ) zt + Op (1/ T ),
 2T                                                         2T
       t=j+p+1                                                   t=j+1



                                                                27
where zt is one of ∆yt−1 , ..., ∆yt−p+1 . From the proof of (b), the series yt−1+j + yt−1−j − 2yt−1 also
has absolutely summable MA coefficients. Thus, the conditions of Lemma A-1 are satisfied.

It remains to consider the top-left element of the matrix Cj,F QD which is given by
                                                            T
                                                    1       X
                                                                                   2
                              (Cj,F QD )11        =                [yt−1 yt−j−1 − yt−1 ].
                                                    T
                                                        t=j+p+1

If the largest (in absolute value) root λp is not real, then by definition of Rδ , it is less than
δ < 1 in absolute value, and the process yt is uniformly stationary. Thus (Cj,F QD )11 satisfies
the conditions of Lemma A-1. Assume now that the largest root λp is a real number. We have
1 − αL − p−1         j
           P
             j=1 bj L (1 − L) = (1 − λp )B(L), where all inverse roots of B(L) are strictly inside the
circle of radius δ.

Let ut = yt − λp yt−1 and thus B(L)ut = εt . Now ut has absolutely summable MA coefficients
uniformly over Rδ .
                    T                 T                      j−1
                  1 X               1 X                      X
                      yt−1 yt−j−1 =     yt−j−1 (λjp yt−j−1 +     λkp ut−k−1 ) =
                  T                 T
                    t=j+1                           t=j+1                          k=0
                                                      T −j        j−1       T
                                                    1 X 2         X       1 X
                                            = λjp          yt−1 +     λkp     yt−j−1 ut−k−1 .
                                                    T                     T
                                                        t=1          k=0         t=j+1

As a result,
                                            T        j−1        T
                                          1X 2       X
                                                          k−1 1
                                                                X
         (Cj,F QD )11 = −(1 −     λjp )       yt−1 +     λp       yt−j−1 ut−k−1 + Op (T −1/2 ),
                                          T                   T
                                            t=1             k=1          t=j+1

again the Op term appears due to change of summation bounds. First, observe that
                                T −j                                 T −j t
                                                  !                                         !
                              1 X                                  1 XX s
                      V ar           yt ut+k          = V ar                λp ut−s ut+k        =
                              T                                    T
                                    t=1                               t=1 s=0
                                  T −j Xt
                             1    X
                         =                   λsp cov(ut+k , ut−s ) < V ar(ut ) < const(δ).
                             T2
                                  t=1 s=0

The variance of ut is uniformly bounded because all roots of this process are uniformly separated
                                 PT −j
from the unit circle. That is, T1 t=1  yt ut+k = Op (1) uniformly over β 0 ∈ Rδ and for all 1 ≤ k ≤
j ≤ K.

Next, consider the term (1−λjp ) T1 Tt=1 yt−1
                                    P       2 . From Theorem 1 in Mikusheva (2011), (1−λ ) 1
                                                                                                   PT      2
                                                                                               p T   t=1 yt−1
                                σ 2 R  1 2                      R 1 2
is uniformly approximated by g(c)     0 Jc (t)dt, where g(c) = E 0 Jc (t)dt, where Jc (t) is the Ornstein-
Uhlenbeck process, and c = T log(|λp |). It follows from Lemma 4(h) and Lemma 10 in Mikusheva
               1
                  R1 2
(2007a) that g(c)  0 Jc (t)dt is uniformly bounded in probability over c. Summing up, Cj,F QD is
asymptotically uniformly Op (1) over Rδ and the proof of Lemma A-2 is complete. 2

                                                              28
Lemma A-3 Under assumptions of Proposition 3 the estimator βbK,F QD is consistent for any
K > p.


Proof of Lemma A-3 Let f (x) = (f1 (x), ..., fp+1 (x)), where fj (x) = x0 Bj,F QD + x0 Cj,F QD x and
Q(x) = K       2
        P
          j=1 fj (x). Any K ≥ p + 1 suffices for consistency of βK,F QD , though additional moments
                                                                  b
may improve efficiency. For any bounded set C in the parameter space, and by Lemma A-2, it
holds that:
                                               K
                                    0
                                              X                2
                           sup Q(β − β) −          g j,F QD (β)    = op (1).
                             β∈C               j=1

Since Q(0) = 0, for consistency of βbK,F QD , it is enough to show that for any ς > 0, there is ε > 0
such that
                                      lim P ( inf Q(x) > ε) = 1,                                (A.8)
                                      T →∞    |x|>ς

where x = β 0 − β. Since β 0 ∈ Rδ and β belongs to bounded neighborhood of Rδ , x is bounded.
There are two cases to consider: |λp | < δ1 < 1, and λp ≥ δ1 .


Case (i) |λp | < δ1 : We will show that for any fixed 0 < δ1 < 1 statement (A.8) holds uniformly
over β 0 ∈ Rδ ∩ {|λp | < δ1 }.

Since K > p, Q(x) ≥ f (x)0 f (x). For any orthonormal transformation A, Q(x) ≥ (Af (x))0 (Af (x)) ≥
(Af (x))21 , where (Af )1 is the first component of vector Af (x). Consider a linear transformation,
the first component of which is
                                                           p−1
                                                           X                             1
                        (Af (x))1 = (fp+1 − α0 fp −              b0j (fp+1−j − fp−j ))
                                                                                         a
                                                           j=1
            p
where a = 1 + (α0 + b01 )2 + (b02 − b01 )2 + ... is a (non-zero) multiplier that normalizes the linear
                                          Pp−1 0 j
transformation. Let A(L) = 1 − α0 L − j=1         bj L (1 − L) be a lag operator. Given the definition
of fj and linearity of the transformation,
                           1            1 0
                                          x (A(L)Bp+1,F QD ) + x0 (A(L)Cp+1,F QD )x .
                                                                                   
             (Af (x))1 =     A(L)fp+1 =                                                         (A.9)
                           a            a
From (b) of Lemma A-2, Bj,F QD →p aj = E[Xt+j εt ]. From (10) and the definition of Xt ,

                       A(L)Xt+p+1 = [εt+p+1 , ∆εt+p+1 , ..., ∆εt+2 ]0 = eet+p+1 .              (A.10)
       et+p+1 εs = 0 for any s ≤ t,
Since Ee
             A(L)Bp+1,F QD →p A(L)E[Xt+j εt ] = E[A(L)Xt+j εt ] = E[e
                                                                    et+p+1 εt ] = 0.
Thus, uniformly over all x in a bounded set,
                                          1 0                  
                            (Af (x))1 =     x (A(L)Cp+1,F QD )x + op (1).                      (A.11)
                                          a

                                                      29
It follows from (13) and (A.10) that
                            T                                           T
                          1 X                                         1 X
                                     eet+p+1 Xt0 + Xt ee0t+p+1 − A(1)     Xt Xt0 + op (1),
                                                              
      A(L)Cp+1,F QD =                                                                           (A.12)
                         2T                                           T
                             t=p+1                                       t=p+1

where op (1) appears from change in the bounds of summation. Since |λp | < δ1 by assump-
                                         1 PT
tion, the process is stationary. Thus 2T      t=p+1 eet+p+1 Xt0 →p 0 uniformly over |λp | < δ1 and
A(1) T1 Tt=p+1 Xt Xt0 is uniformly positive definite. This gives us the needed bound in (A.8) for
       P

processes with |λp | < δ1 .


Case (ii) |λp | ≥ δ1 : To show that for δ1 < 1 close enough to the unity, (A.8) holds uniformly
over β 0 ∈ Rδ ∩ {|λp | ≥ δ1 }, we divide the area |x| > ς from (A.8) into two regions: I1 = {x : |x| >
ς, |x1 | > ς1 } and I2 = {x : |x| > ς, |x1 | ≤ ς1 }, where 0 < ς1 < ς.

Consider x ∈ I1 . We will prove that for any fixed ς1 > 0, one can choose δ1 close enough to the
unity such that uniformly over β 0 ∈ Rδ ∩ {|λp | > δ1 }, an analog of (A.8) holds where the infinimum
is taken over x ∈ I1 .

Applying the arguments and transformation as in (A.9), it can be shown that equations (A.11) and
(A.12) hold. Since A(1) converges to zero as δ1 converges to 1, one can choose δ1 close enough to 1
to make all terms except the (1,1)-th element of A(1) T1 Tt=p+1 Xt Xt0 sufficiently small, and all but
                                                          P

the (1,1)-th element of T1 Tt=p+1 eet+p+1 Xt0 converge in probability to its expected value of zero. In
                          P

consequence, the following holds uniformly over β 0 ∈ Rδ ∩ {|λp | > δ1 } and x ∈ I1 :
                                                                         
                                     T                          T
                                 1  X                        1 X
      x0 A(L)Cp+1,F QD x = x21          εt+p+1 yt−1 − A(1)           2 
                                                                     yt−1   + op (1) + op (1 − δ1 ).
                                 T                          T
                                     t=p+1                       t=p+1



It remains to show that T1 Tt=p+1 (εt+p+1 − A(1)yt−1 ) yt−1 satisfies the uniform Law of Large Num-
                            P
bers, and thus converges uniformly to a non-zero constant. To do so, we use the decomposition as
in Phillips and Solo (1992)) that εt+p+1 − A(1)yt−1 = ut − ut−1 , where ut is a series
                                                                                    √ with absolutely
                                    1                      1
summable MA coefficients. Since T (ut − ut−1 )yt−1 = − T (yt − yt−1 )ut + Op (1/ T ), Lemma A-1
applies, and T1 Tt=p+1 (εt+p+1 − A(1)yt−1 ) yt−1 converges in probability to its expectation. Since
                 P
A(1) PT          2
  T E     t=p+1 yt−1 is uniformly different from zero, this implies that for any fixed ς1 > 0 there
exists δ1 < 1 such that uniformly over |λp | > δ1 an analog of (A.8) holds where the infinimum is
taken over x ∈ I1 .

Consider now x ∈ I2 . One can choose ς1 small enough and δ1 close enough to the unity such that
uniformly over β 0 ∈ Rδ ∩ {|λp | > δ1 }, an analog of (A.8) holds, where the infinimum is taken over
x ∈ I2 . Given that Bj,F QD and Cj,F QD are uniformly bounded,
                             fj (x) = x0−1 Bj,−1 + x0−1 Cj,−1 x−1 + op (ς1 ).
where x−1 = (x2 , ..., xp ) is (p − 1) × 1 sub-vector of x, and Bj,−1 and Cj,−1 are the (p − 1) × 1 and
(p − 1) × (p − 1) sub-matrixes of Bj,F QD and Cj,F QD corresponding to the last p − 1 components
of β.

                                                   30
Let Zt = (∆yt−1 , ..., ∆yt−p+1 ) and Zet = (yt−1 − λp yt−2 , ..., yt−p+1 − λp yt−p )0 be two (p − 1) × 1
uniformly stationary vector-processes. Note that the matrices Bj,−1 and Cj,−1 satisfy equations
analogous to (12) and (13) with Zt in place of Xt . Similarly, B   ej and C  ej are defined as in (12) and
          et in place of Xt . Observe that Zt0 = Z
(13) with Z                                      et0 − (1 − λp )(yt−2 , ..., yt−p ). It is easy to see that

                              fj (x) = x0−1 B
                                            ej + x0−1 C
                                                      ej x−1 + op (1 − δ1 ) + op (ς1 ).

The function fej = x0−1 B ej + x0 C
                                 −1 j x−1 corresponds to that of the uniformly stationary process
                                     e
yt − λp yt−1 with all roots smaller than δ in absolute value. The rest of the proof follows arguments
as in Case i. 2


Proof of Proposition 3 (i): To establish the asymptotic distribution of βbK,F QD , consider first
order condition:
                         K
                        X                       ∂g j,F QD
                           g j,F QD (βbK,F QD )           (βbK,F QD ) = 0.
                                                   ∂β
                                   j=1

From Lemma A-2 and consistency of βbK,F QD ,
                                ∂g j,F QD
                                          (βbK,F QD ) = −Bj,F QD + op (1) →p aj ,
                                   ∂β
and uniformly over Rδ :
               √                         √               √
                 T g j,F QD (βbK,F QD ) = T Aj,F QD + a0j T (βbK,F QD − β 0 ) + op (1).
As a result, the following holds uniformly:
                                                −1                 
              √                         K
                                        X              K
                                                       X
                T (βbK,F QD − β 0 ) ⇒    aj a0j       aj (ξj − ξ0 ) = N (0, ΣK,F QD ),
                                            j=1               j=1

            P               −1                                    P          P         0
                 K       0                                            K            K
where G =        j=1 aj aj         , and ΣK,F QD = σ 4 G + µ4 G       j=1 aj       j=1 aj        G.



Proof of Proposition 3 (ii):             The proof proceeds by treating QD as a two-step estimator. First,
note that
                              T
                            1 X 2     1 X           X               X
                         2
                        s −     εt = − (  Xt εt )0 (   Xt Xt0 )−1 (   Xt εt )
                            T         T t            t              t
                                    t=p+1

                                                                          0             0 −1
                                                                     P            P                   P
Theorem 1 in Mikusheva (2011) shows that the statistic (         t Xt εt ) ( R t Xt Xt ) ( t Xt εt ) is uni-
                                                                                J (t)dw(t)
formly approximated by the distribution (tc +N (0, p−1))2 , where tc = √Rc 2                is a local-to-unity
                                                                                   Jc (t)dt
limit of a t-statistic, and c = T log(|λp |) . Given that tc is uniformly bounded in probability over
all possible values of c ≤ 0, the following holds uniformly over Rδ :
                                             1X 2
                                       s2 =       ε + Op (1/T ).                                          (A.13)
                                            T t t


                                                         31
Since g j,QD (β) = g j,F QD (β) − γ0 + s2 ,

                      g j,QD (β) = Aj,QD + (β 0 − β)0 Bj,QD + (β 0 − β)0 Cj,QD (β 0 − β),

where Aj,QD = Aj,F QD + s2 − σ 2 , Bj,QD = Bj,F QD and Cj,QD = Cj,F√         QD . A result analogous
of Lemma A-2 holds for Aj,QD , Bj,QD and Cj,QD with one correction: T (A1,QD , ..., AK,QD ) ⇒
(ξ1 , ..., ξK ). This gives us consistency and asymptotic normality of βbK,QD with asymptotic covari-
ance matrix ΣK,QD = σ 4 G. 2

The following lemma will be used to prove Proposition 4.


Lemma A-4 Uniformly over all possible values of θ,

       √
 (a)       T (Aj + Ω0 ) ⇒ ξj − ξ0 ;
       √
 (b)       T (Aj + S) ⇒ ξj ;
 (c) Bj,1 →p E[εt (xt−j−1 − xt−1 )] = 0;
 (d) Bj,2 →p E[xt−1 (εt−j − εt )0 ] = aj ;
 (e) Cj = Op (1).


where √1T Tt=j+1 εt ε0t−j ⇒ ξj and ξj is a 2 × 2 matrix with normally distributed components such
           P

that for any non-random vector a the vector ξj a is normally distributed with variance-covariance
matrix Ω0 a0 Ω0 a. We also have √1T Tt=j+1 εt ε0t ⇒ ξ0 where ξ0 is a 2 × 2 matrix with normally
                                      P

distributed components such that for any non-random    vector a, the vector ξ0 a is normally distributed
with variance-covariance matrix E (ε0t a)2 εt ε0t . The variables ξj are independent for any j ≥ 0.
                                                 



Proof of Lemma A-4 Result (a) follows from Central Limit Theorem. To prove (b), note that
             T                           P                2
                                                                  P               P               
          1X 0          1   1           (    s ε xs x s−1 )     (  s ε xs x s−1 )( s ε ys x s−1 )
       S=       εt ε t − P 2
                                                                       ( s εys xs−1 )2
                                    P               P                   P
          T             T s xs−1   ( s εys xs−1 )( s εxs xs−1 )
            t=1
     PT
           εxs xs−1
Now  qs=1
       PT       2
                    ⇒ tc uniformly over α0 ∈ (−1 + δ, 1], and the family tc is uniformly bounded. The
          s=1 xs−1
     PT
       s=1 εys xs−1
sum qP   T        2
                    has a bounded second moment since εys xs−1 is martingale-difference sequence,
         s=1 Exs−1
                                                                           PT
                                                                                  x2
and thus it is uniformly bounded by Chebyshev’s inequality. Lastly, PTs=1 s−1       2  is uniformly sep-
                                                                             s=1 Exs−1
arated from zero, a result that follows from Lemma 4(h) and    √ Lemma 10 in P Mikusheva (2007a).
Summing up, we have S = T1 Tt=1 εt ε0t +Op ( T1 ). As a result, T (Aj +S) = √1T Tt=j+1 εt ε0t−j ⇒ ξj .
                            P


The proof of part (c) follows from Lemma A-1, since we show in the proof of Proposition 3 that
xt−j−1 − xt−1 has absolutely summable MA coefficients uniformly over α.

                                                      32
To prove (d), re-write
           T                       T                        2j               T +j
         1 X                  0  1 X                  0   1 X              1 X
Bj,2   =     xt−1 (εt−j − εt ) =     (xt+j−1 − xt−1 )εt +      xt−1 εt−j −        xt−1 εt−j .
         T                       T                        T                T
           t=j+1                             t=j+1                                t=j+1                  t=T +1


The terms T1 2j       xt−1 εt−j and T1 Tt=T
             P                        P +j
          √     t=j+1                       +1 xt−1 εt−j both have j summands each of which are of
order Op ( T ). This means that for any j ≤ K where K is fixed, the following holds uniformly:
                                             T
                                           1 X                             1
                                 Bj,2 =        (xt+j−1 − xt−1 )ε0t + Op ( √ ).
                                           T                                T
                                              t=j+1

The rest of the proof is the same as for part (c). Part (e) follows from Proposition 3.(c).2


Proof of Proposition 4: Note first that
                               T
                             1 X
           Γj (θ) − Γ0 (θ) =
           b        b            et (θ)(et−j (θ) − et (θ))0
                             T
                                  t=j+1
                                T
                              1 X
                            =     ((θ0 − θ)xt−1 + εt )((θ0 − θ)(xt−j−1 − xt−1 ) + εt−j − εt )0
                              T
                                  t=j+1

                            = Aj + Bj,1 (θ0 − θ)0 + (θ0 − θ)Bj,2 + (θ0 − θ)Cj (θ0 − θ)0 ,

where
                              T                            T
                            1 X                0         1 X
                       Aj =     εt (εt−j − εt ) ; Bj,1 =     εt (xt−j−1 − xt−1 );
                            T                            T
                                   t=j+1                                  t=j+1
                                T                                       T
                            1   X                                   1   X
                   Bj,2 =               xt−1 (εt−j − εt )0 ; Cj =               xt−1 (xt−j−1 − xt−1 ).
                            T                                       T
                                t=j+1                                   t=j+1



Lemma A-4 showed that uniformly over α:

               kΓ         b 0 (θ) + Ω0 k22 = k(θ0 − θ)Bj,2 + Cj (θ0 − θ)(θ0 − θ)0 k22 + op (1),
                b j (θ) − Γ

and
                kΓ         b 0 (θ) + Sk22 = k(θ0 − θ)Bj,2 + Cj (θ0 − θ)(θ0 − θ)0 k22 + op (1).
                 b j (θ) − Γ
We minimize the sum of such functions for j = 1, ..., K. Obviously, the minimized function is non-
negative and one of its minimal value of zero is achieved at θ = θ0 . The question is whether there
are any other minima. For this, there should exist θ such that k(θ0 − θ)Bj,2 + Cj (θ0 − θ)(θ0 − θ)0 k2 is
zero for all j. For a given j, the only non-trivial null of function k(θ0 − θ)Bj,2 + Cj (θ0 − θ)(θ0 − θ)0 k2
implies θj = θ0 + C1j Bj,2 which is asymptotically different for different j. This implies that for
K ≥ 2 no other asymptotic null of the objective function other than θ = θ0 exists, and thus θbK,F QD
and θbK,QD are consistent.

                                                           33
To derive the limit distribution of θbK,F QD , we use the fact that the first order condition must be
satisfied at θ = θbK,F QD . Now the first order condition is

  ∇θ kΓ          b 0 (θ) + Ω0 k2
      b j (θ)) − Γ
                               2
                                                                                                                             0
= − 2 Aj + Σ0 + Bj,1 (θ0 − θ)0 + (θ0 − θ)Bj,2 + Cj (θ0 − θ)(θ0 − θ)0                               0
                                                                                                       + Bj,2 + 2Cj (θ0 − θ)0 .
                                                                                            
                                                                                                  Bj,1

Since we proved that θbK,F QD is uniformly consistent, and given statements (d) and (e),

                                              Bj,2 + 2Cj (θ0 − θbK,F QD )0 →p aj .
Furthermore,
   √                                                                                                
     T Aj + Ω0 + Bj,1 (θ0 − θbK,F QD ) + (θ0 − θbK,F QD )Bj,2 + Cj (θ0 − θbK,F QD )(θ0 − θbK,F QD )0
      √             √
    = T (Aj + Ω0 ) + T (θ0 − θbK,F QD )aj + op (1).
As a result,
                                   √                                             K
                                                               1                 X
                                       T (θbK,F QD − θ0 ) ⇒ PK                        (ξj − ξ0 )aj
                                                                              0
                                                                      j=1 aj aj j=1

uniformly over α. Similarly,
                                        √                                            K
                                                                   1                 X
                                             T (θbK,QD − θ0 ) ⇒ PK                     ξj a j .
                                                                                 0
                                                                         j=1 aj aj j=1

The last two formulas lead to the conclusion of Lemma 4.


Relation between PH and FD:                          Observe that
T
X                      T
                       X                      T
                                              X
               2
       (∆yt−1 ) =            ∆yt−1 yt−1 −           (yt−1 − yt−2 )yt−2
 t=2                   t=2                    t=2
                       XT                     XT                                                  T
                                                                                                  X
                   =         ∆yt−1 yt−1 +           yt−1 (yt−1 − yt−2 ) − yT2 −1 + y02 = 2              ∆yt−1 yt−1 − yT2 −1 + y02 .
                       t=2                    t=2                                                 t=2

Thus if |α0 | < 1 is fixed and T → ∞,
                                     T                 T
                                   1X                1X
                                       (∆yt−1 )2 = 2     ∆yt−1 yt−1 + Op (1/T ).
                                   T                 T
                                       t=2                         t=2

Similarly,
             T
             X                                            T
                                                          X                    T
                                                                               X                     T
                                                                                                     X
                   ∆yt−1 (2∆yt + ∆yt−1 ) =2                     ∆yt−1 yt − 2         ∆yt−1 yt−1 +      (∆yt−1 )2 =
             t=2                                          t=2                  t=2                   t=2
                                                          XT
                                                     =2         ∆yt−1 yt − yT2 −1 + y02
                                                          t=2


                                                                    34
and
                      T                             T
                    1X                            1X
                        ∆yt−1 (2∆yt + ∆yt−1 ) = 2     ∆yt−1 yt + Op (1/T ).
                    T                             T
                      t=2                                 t=2

This leads us to the result that α
                                 bP H = α
                                        bF D + Op   (T −1 )   under stationary asymptotics.




                                               35
                                                                                Table 1. AR(1) model.


                                                     QD                                             FQD                                      OLS
                     T      α      mean     rmse          t-test       J-test     mean      rmse          t-test       J-test   mean     rmse         t-test
                                                     size     power     size                         size     power     size                     size     power
                                    (1)      (2)     (3)         (4)    (5)         (6)     (7)      (8)         (9)    (10)    (11)     (12)    (13)      (14)
                                                                                      Panel A: Intercept model
                    200    1.00    0.975    0.059   0.107    0.533     0.085      0.991    0.122    0.033      0.098   0.078     0.973   0.035   0.473   1.000
                    200    0.98    0.960    0.052   0.102    0.484     0.083      0.971    0.113    0.032      0.085   0.069     0.956   0.035   0.219   1.000
                    200    0.95    0.934    0.050   0.080    0.428     0.101      0.942    0.114    0.028      0.066   0.095     0.929   0.036   0.150   0.969
                    200    0.90    0.883    0.054   0.098    0.417     0.088      0.888    0.117    0.030      0.084   0.083     0.880   0.040   0.111   0.687
                    200    0.80    0.782    0.058   0.091    0.359     0.075      0.794    0.126    0.035      0.081   0.077     0.782   0.050   0.097   0.413
                    200    0.50    0.485    0.069   0.091    0.260     0.061      0.510    0.133    0.044      0.098   0.096     0.487   0.064   0.078   0.246
                    200    0.00    -0.006   0.074   0.068    0.216     0.036      0.030    0.131    0.057      0.128   0.114    -0.007   0.072   0.060   0.210
                    200    -0.50   -0.494   0.066   0.050    0.179     0.029      -0.470   0.101    0.042      0.149   0.105    -0.497   0.061   0.054   0.208
                    500    1.00    0.990    0.030   0.066    0.748     0.069      0.997    0.072    0.023      0.136   0.061     0.989   0.014   0.462   1.000
                    500    0.98    0.974    0.028   0.064    0.681     0.056      0.977    0.073    0.035      0.150   0.054     0.971   0.015   0.138   1.000
                    500    0.95    0.943    0.030   0.076    0.676     0.077      0.946    0.072    0.037      0.154   0.071     0.941   0.019   0.127   1.000
                    500    0.90    0.894    0.031   0.073    0.598     0.068      0.896    0.074    0.029      0.144   0.067     0.893   0.022   0.082   0.930
                    500    0.80    0.793    0.035   0.085    0.545     0.047      0.792    0.080    0.043      0.146   0.052     0.792   0.030   0.082   0.687
                    500    0.50    0.492    0.042   0.077    0.416     0.040      0.499    0.089    0.055      0.137   0.073     0.494   0.041   0.070   0.402
                    500    0.00    -0.004   0.046   0.065    0.335     0.022      0.012    0.078    0.039      0.178   0.070    -0.004   0.046   0.064   0.322
                    500    -0.50   -0.499   0.039   0.048    0.356     0.017      -0.488   0.060    0.048      0.247   0.084    -0.500   0.038   0.048   0.368




36
                                                                                    Panel B: Linear trend model
                    200    1.00    0.964    0.063   0.224    0.641     0.108      0.991    0.112    0.025      0.091   0.101     0.950   0.057   0.775   1.000
                    200    0.98    0.954    0.058   0.164    0.547     0.099      0.967    0.114    0.028      0.081   0.098     0.940   0.050   0.454   1.000
                    200    0.95    0.926    0.056   0.146    0.511     0.090      0.945    0.111    0.022      0.078   0.077     0.916   0.047   0.278   0.986
                    200    0.90    0.876    0.059   0.143    0.481     0.108      0.892    0.119    0.032      0.071   0.105     0.869   0.049   0.180   0.775
                    200    0.80    0.775    0.060   0.121    0.432     0.075      0.788    0.127    0.043      0.081   0.078     0.770   0.054   0.133   0.512
                    200    0.50    0.479    0.068   0.097    0.286     0.040      0.512    0.135    0.051      0.091   0.086     0.482   0.065   0.083   0.259
                    200    0.00    -0.012   0.071   0.070    0.220     0.037      0.029    0.125    0.044      0.114   0.117    -0.012   0.070   0.064   0.215
                    200    -0.50   -0.496   0.066   0.053    0.214     0.037      -0.469   0.104    0.056      0.155   0.128    -0.499   0.063   0.065   0.237
                    500    1.00    0.985    0.033   0.137    0.775     0.072      0.999    0.069    0.020      0.160   0.063     0.980   0.023   0.777   1.000
                    500    0.98    0.972    0.029   0.080    0.717     0.082      0.976    0.071    0.041      0.153   0.081     0.967   0.018   0.245   1.000
                    500    0.95    0.942    0.031   0.086    0.678     0.073      0.950    0.074    0.034      0.161   0.068     0.938   0.021   0.185   1.000
                    500    0.90    0.891    0.032   0.090    0.654     0.061      0.898    0.076    0.041      0.151   0.068     0.889   0.025   0.123   0.957
                    500    0.80    0.792    0.034   0.089    0.567     0.043      0.797    0.078    0.025      0.122   0.054     0.789   0.030   0.090   0.729
                    500    0.50    0.490    0.041   0.082    0.438     0.040      0.502    0.088    0.051      0.127   0.070     0.491   0.040   0.073   0.442
                    500    0.00    -0.004   0.045   0.058    0.329     0.020      0.011    0.078    0.047      0.160   0.081    -0.004   0.044   0.062   0.321
                    500    -0.50   -0.498   0.038   0.045    0.345     0.029      -0.485   0.060    0.028      0.221   0.092    -0.500   0.038   0.046   0.357



     Note: DGP is AR(1) model while AR(2) model is fitted. Three autocovariances are used in QD estimation. T-test and J-test sizes are for 5 percent level. Power of the
     t-test is computed for the null of H0 : α = α0 − 0.05. Additional results are available in the online appendix.
                                                                               Table 2. AR(2) model.


                                                    QD                                             FQD                                      OLS
                     T      α0     mean     rmse         t-test       J-test     mean      rmse          t-test       J-test   mean     rmse         t-test
                                                    size     power     size                         size     power     size                     size     power
                                    (1)      (2)    (3)         (4)    (5)         (6)     (7)      (8)         (9)    (10)    (11)     (12)    (13)      (14)
                                                                                     Panel A: Intercept model
                    200    1.00    0.962    0.101   0.073   0.322     0.050      0.983    0.155    0.057      0.097   0.044     0.968   0.042   0.468   1.000
                    200    0.98    0.965    0.070   0.059   0.269     0.045      0.975    0.149    0.046      0.074   0.039     0.952   0.041   0.247   0.999
                    200    0.95    0.939    0.070   0.062   0.266     0.051      0.947    0.158    0.040      0.073   0.039     0.923   0.044   0.179   0.928
                    200    0.90    0.890    0.072   0.064   0.246     0.038      0.900    0.163    0.051      0.081   0.033     0.877   0.048   0.121   0.590
                    200    0.80    0.787    0.077   0.074   0.215     0.048      0.795    0.184    0.058      0.094   0.047     0.776   0.059   0.089   0.364
                    200    0.50    0.487    0.103   0.072   0.156     0.007      0.502    0.239    0.095      0.131   0.017     0.472   0.086   0.094   0.234
                    200    0.00    -0.004   0.147   0.056   0.094     0.001      0.061    0.316    0.093      0.114   0.038    -0.016   0.111   0.073   0.136
                    200    -0.50   -0.582   0.238   0.079   0.130     0.002      -0.571   0.350    0.182      0.231   0.034    -0.516   0.137   0.059   0.126
                    500    1.00    0.982    0.067   0.045   0.503     0.036      0.987    0.099    0.050      0.121   0.030     0.987   0.017   0.464   1.000
                    500    0.98    0.975    0.037   0.046   0.447     0.031      0.977    0.094    0.041      0.119   0.029     0.969   0.017   0.176   1.000
                    500    0.95    0.946    0.040   0.056   0.431     0.034      0.951    0.093    0.047      0.117   0.030     0.940   0.022   0.122   0.999
                    500    0.90    0.897    0.041   0.051   0.405     0.032      0.895    0.101    0.060      0.138   0.030     0.891   0.025   0.079   0.874
                    500    0.80    0.798    0.045   0.059   0.344     0.025      0.798    0.122    0.077      0.155   0.031     0.793   0.032   0.074   0.546
                    500    0.50    0.497    0.060   0.050   0.219     0.010      0.507    0.153    0.068      0.136   0.023     0.493   0.050   0.058   0.297
                    500    0.00    -0.002   0.085   0.049   0.139     0.000      0.009    0.203    0.099      0.158   0.036    -0.008   0.068   0.059   0.181
                    500    -0.50   -0.541   0.140   0.075   0.154     0.007      -0.576   0.217    0.177      0.251   0.019    -0.505   0.084   0.063   0.151




37
                                                                                   Panel B: Linear trend model
                    200    1.00    0.962    0.088   0.158   0.423     0.045      0.989    0.152    0.055      0.094   0.045     0.940   0.070   0.762   1.000
                    200    0.98    0.949    0.080   0.142   0.395     0.040      0.975    0.153    0.045      0.094   0.039     0.928   0.064   0.493   0.999
                    200    0.95    0.928    0.075   0.110   0.343     0.045      0.940    0.155    0.043      0.085   0.041     0.907   0.058   0.301   0.969
                    200    0.90    0.881    0.077   0.104   0.306     0.053      0.899    0.162    0.041      0.080   0.045     0.862   0.059   0.210   0.699
                    200    0.80    0.775    0.087   0.117   0.300     0.037      0.796    0.186    0.066      0.099   0.037     0.757   0.072   0.158   0.520
                    200    0.50    0.473    0.110   0.102   0.219     0.020      0.514    0.239    0.083      0.115   0.031     0.464   0.090   0.117   0.254
                    200    0.00    -0.024   0.147   0.062   0.120     0.005      0.048    0.306    0.089      0.124   0.035    -0.031   0.114   0.076   0.155
                    200    -0.50   -0.590   0.234   0.082   0.127     0.004      -0.587   0.324    0.163      0.222   0.021    -0.528   0.136   0.074   0.139
                    500    1.00    0.984    0.046   0.107   0.566     0.023      0.997    0.093    0.045      0.145   0.024     0.975   0.029   0.760   1.000
                    500    0.98    0.972    0.039   0.062   0.482     0.028      0.980    0.092    0.049      0.120   0.031     0.962   0.024   0.317   1.000
                    500    0.95    0.942    0.041   0.078   0.484     0.029      0.950    0.087    0.028      0.105   0.030     0.934   0.026   0.184   1.000
                    500    0.90    0.891    0.043   0.084   0.447     0.034      0.897    0.104    0.061      0.137   0.033     0.884   0.030   0.145   0.921
                    500    0.80    0.793    0.046   0.072   0.395     0.027      0.795    0.116    0.063      0.143   0.026     0.785   0.035   0.097   0.649
                    500    0.50    0.492    0.062   0.077   0.273     0.009      0.509    0.155    0.070      0.138   0.019     0.487   0.053   0.079   0.347
                    500    0.00    -0.010   0.088   0.055   0.182     0.001      0.027    0.200    0.079      0.128   0.043    -0.013   0.072   0.072   0.233
                    500    -0.50   -0.548   0.142   0.072   0.174     0.004      -0.590   0.219    0.200      0.281   0.025    -0.510   0.086   0.056   0.156



     Note: DGP is AR(2) model while AR(3) model is fitted. All results are for the sum of autoregressive coefficients. Three autocovariances are used in QD estimation.
     T-test and J-test sizes are for 5 percent level. Power of the t-test is computed for the null of H0 : α = α0 − 0.05.
                                                     Table 3. Predictive regression. (True parameter β is equal to one)


                                                      QD                                          FQD                                     OLS
                      T      α     mean     rmse          t-test       J-test   mean     rmse          t-test       J-test   mean    rmse         t-test
                                                     size     power     size                      size     power     size                    size     power
                                    (1)      (2)     (3)         (4)    (5)      (6)     (7)      (8)         (9)    (10)    (11)    (12)    (13)      (14)
                                                                                   Panel A: Intercept model
                     200   1.00    0.986    0.051   0.054    0.302     0.166    0.999   0.106    0.026      0.087   0.243    0.987   0.022   0.169   0.985
                     200   0.98    0.988    0.049   0.042    0.274     0.181    0.996   0.105    0.027      0.097   0.255    0.989   0.025   0.104   0.937
                     200   0.95    0.992    0.052   0.044    0.258     0.182    1.001   0.108    0.035      0.099   0.304    0.990   0.030   0.097   0.751
                     200   0.90    0.991    0.055   0.057    0.265     0.191    1.004   0.114    0.031      0.092   0.296    0.990   0.036   0.081   0.554
                     200   0.80    0.992    0.057   0.047    0.224     0.195    1.001   0.117    0.033      0.101   0.309    0.994   0.045   0.064   0.343
                     200   0.50    0.989    0.070   0.068    0.230     0.152    1.011   0.130    0.041      0.105   0.311    0.991   0.063   0.054   0.236
                     200   0.00    0.996    0.078   0.061    0.186     0.125    1.028   0.123    0.047      0.124   0.310    0.997   0.071   0.059   0.185
                     200   -0.50   1.004    0.069   0.047    0.176     0.142    1.028   0.102    0.046      0.137   0.327    1.000   0.062   0.052   0.217
                     500   1.00    0.995    0.030   0.034    0.494     0.068    0.999   0.063    0.022      0.147   0.109    0.995   0.009   0.172   1.000
                     500   0.98    0.995    0.030   0.032    0.502     0.089    0.999   0.066    0.031      0.181   0.150    0.995   0.012   0.093   1.000
                     500   0.95    0.995    0.031   0.042    0.466     0.073    0.999   0.071    0.041      0.172   0.157    0.996   0.016   0.082   0.987
                     500   0.90    0.995    0.032   0.045    0.466     0.076    0.996   0.069    0.035      0.173   0.147    0.996   0.021   0.072   0.871
                     500   0.80    0.997    0.034   0.051    0.381     0.083    1.002   0.077    0.039      0.147   0.153    0.997   0.028   0.061   0.631
                     500   0.50    0.996    0.043   0.056    0.359     0.058    1.009   0.085    0.039      0.133   0.170    0.997   0.040   0.060   0.381
                     500   0.00    0.999    0.048   0.064    0.287     0.038    1.017   0.081    0.039      0.170   0.166    0.999   0.045   0.062   0.306
                     500   -0.50   1.000    0.041   0.046    0.323     0.060    1.015   0.061    0.033      0.223   0.167    0.999   0.038   0.060   0.377




38
                                                                                 Panel B: Linear trend model
                     200   1.00    0.978    0.056   0.089    0.374     0.175    0.994   0.108    0.032      0.102   0.270    0.973   0.036   0.310   0.983
                     200   0.98    0.985    0.053   0.064    0.303     0.180    1.004   0.109    0.027      0.097   0.288    0.980   0.033   0.180   0.927
                     200   0.95    0.983    0.055   0.073    0.316     0.191    0.999   0.110    0.037      0.119   0.302    0.981   0.037   0.151   0.793
                     200   0.90    0.984    0.055   0.062    0.292     0.177    1.002   0.115    0.038      0.104   0.299    0.984   0.039   0.109   0.591
                     200   0.80    0.984    0.060   0.075    0.279     0.180    0.998   0.119    0.036      0.099   0.302    0.987   0.047   0.084   0.388
                     200   0.50    0.985    0.072   0.081    0.226     0.141    1.013   0.128    0.040      0.103   0.290    0.989   0.064   0.071   0.250
                     200   0.00    0.996    0.080   0.064    0.189     0.115    1.033   0.131    0.049      0.119   0.307    0.996   0.070   0.063   0.181
                     200   -0.50   1.004    0.069   0.049    0.172     0.136    1.029   0.099    0.044      0.118   0.317    1.003   0.062   0.049   0.204
                     500   1.00    0.992    0.031   0.054    0.543     0.086    0.998   0.068    0.030      0.188   0.139    0.990   0.014   0.278   1.000
                     500   0.98    0.993    0.030   0.035    0.522     0.080    1.001   0.067    0.039      0.178   0.149    0.993   0.014   0.148   1.000
                     500   0.95    0.993    0.031   0.042    0.514     0.074    0.999   0.066    0.033      0.161   0.146    0.993   0.018   0.100   0.983
                     500   0.90    0.995    0.031   0.033    0.479     0.095    1.002   0.071    0.027      0.150   0.154    0.994   0.022   0.080   0.891
                     500   0.80    0.994    0.035   0.045    0.448     0.085    1.009   0.077    0.029      0.121   0.158    0.995   0.027   0.062   0.647
                     500   0.50    0.993    0.042   0.073    0.371     0.061    1.003   0.084    0.041      0.153   0.165    0.994   0.039   0.073   0.408
                     500   0.00    0.996    0.048   0.060    0.310     0.055    1.016   0.081    0.035      0.178   0.169    0.997   0.045   0.066   0.322
                     500   -0.50   1.000    0.042   0.042    0.322     0.057    1.016   0.064    0.034      0.221   0.185    0.999   0.039   0.059   0.395



     Note: DGP is given by equations (14a) and (14b). All results are for the parameter β. In all simulations ρue = 0.5. Two autocovariances are used in QD estimation.
     T-test and J-test sizes are for 5 percent level. Power of the t-test is computed for the null of H0 : β = β0 − 0.05.
                                    Table 4. DSGE model, capital intensity ψ.


                                               QD                                     MLE
                T      ρ    mean     rmse         t-test        J-test   mean    rmse        t-test
                                              size    power      size                    size    power
                             (1)      (2)     (3)       (4)      (5)      (6)     (7)    (8)       (9)

                                         Panel A: Intercept model
               200   1.00   0.255    0.122 0.077 0.245       0.088       0.181   0.117   0.309   0.577
               200   0.98   0.255    0.120 0.098 0.259       0.085       0.220   0.094   0.142   0.409
               200   0.95   0.268    0.133 0.076 0.238       0.095       0.246   0.094   0.076   0.294
               200   0.90   0.284    0.147 0.066 0.201       0.094       0.266   0.114   0.065   0.235
               200   0.80   0.289    0.144 0.046 0.148       0.079       0.285   0.131   0.065   0.184
               500   1.00   0.260    0.072 0.050 0.304       0.049       0.173   0.110   0.517   0.830
               500   0.98   0.261    0.071 0.059 0.322       0.053       0.231   0.058   0.151   0.626
               500   0.95   0.262    0.073 0.066 0.331       0.049       0.245   0.054   0.076   0.522
               500   0.90   0.266    0.082 0.052 0.306       0.043       0.253   0.062   0.053   0.428
               500   0.80   0.281    0.110 0.035 0.197       0.035       0.270   0.096   0.062   0.315

                                        Panel B: Linear trend   model
               200   1.00   0.255    0.123 0.088 0.264          0.093    0.223   0.094   0.153   0.399
               200   0.98   0.267    0.128 0.075 0.222          0.090    0.243   0.091   0.102   0.315
               200   0.95   0.272    0.138 0.085 0.233          0.090    0.257   0.097   0.065   0.249
               200   0.90   0.282    0.145 0.066 0.204          0.083    0.273   0.113   0.057   0.205
               200   0.80   0.298    0.146 0.053 0.130          0.081    0.296   0.135   0.057   0.150
               500   1.00   0.259    0.071 0.050 0.308          0.041    0.209   0.079   0.324   0.730
               500   0.98   0.262    0.072 0.057 0.309          0.040    0.237   0.059   0.139   0.593
               500   0.95   0.263    0.073 0.058 0.315          0.039    0.250   0.056   0.069   0.477
               500   0.90   0.268    0.082 0.042 0.280          0.034    0.259   0.064   0.042   0.387
               500   0.80   0.284    0.108 0.024 0.162          0.028    0.277   0.097   0.053   0.295



Note: The true value of capital intensity is ψ = 0.25. The observed series is consumption. QD uses OLS estimate of
the standard deviation of innovations consumption for QD estimation. M LE corresponds to the maximum likelihood
estimation (Kalman filter) of the structural parameters. Three autocorrelation coefficients (i.e, the fitted model is
AR(3)) and six autocovariances are used in QD estimation. T-test and J-test sizes are for 5 percent level. Power of
the t-test is computed for the null of H0 : ψ = ψ0 − 0.1.




                                                         39
                                                                  Table 5. Linear QD Estimators: AR(1) model.


                                                      HD                                            FD                                         PH
                      T      α      mean     rmse          t-test       J-test   mean      rmse          t-test       J-test   mean     rmse             t-test
                                                      size     power     size                       size     power     size                         size     power
                                     (1)      (2)     (3)         (4)    (5)       (6)     (7)      (8)         (9)    (10)    (11)     (12)        (13)      (14)
                                                                                     Panel A: Intercept model
                     200   1.00     0.975    0.048   0.156    0.662     0.073    0.954    0.154    0.025      0.189   0.072     1.003   0.171       0.092   0.157
                     200   0.98     0.961    0.046   0.132    0.561     0.077    0.958    0.093    0.030      0.145   0.084     0.977   0.180       0.111   0.173
                     200   0.95     0.933    0.045   0.114    0.532     0.068    0.931    0.081    0.035      0.170   0.073     0.952   0.179       0.116   0.173
                     200   0.90     0.883    0.049   0.122    0.481     0.066    0.881    0.081    0.052      0.179   0.075     0.896   0.174       0.108   0.173
                     200   0.80     0.781    0.053   0.099    0.424     0.072    0.779    0.083    0.058      0.190   0.080     0.792   0.170       0.115   0.188
                     200   0.50     0.485    0.066   0.095    0.275     0.079    0.485    0.093    0.060      0.147   0.090     0.515   0.159       0.094   0.163
                     200   0.00     -0.005   0.072   0.068    0.210     0.069    -0.011   0.101    0.076      0.163   0.053     0.013   0.127       0.089   0.176
                     200   -0.50    -0.496   0.062   0.052    0.217     0.074    -0.492   0.086    0.081      0.195   0.021    -0.482   0.087       0.070   0.182
                     500   1.00     0.991    0.026   0.081    0.805     0.058    0.977    0.122    0.016      0.313   0.058     1.001   0.114       0.106   0.210
                     500   0.98     0.974    0.025   0.086    0.772     0.062    0.973    0.048    0.028      0.301   0.064     0.982   0.116       0.114   0.208
                     500   0.95     0.943    0.026   0.088    0.754     0.061    0.943    0.047    0.038      0.314   0.063     0.952   0.115       0.112   0.204
                     500   0.90     0.894    0.028   0.078    0.694     0.065    0.894    0.047    0.041      0.290   0.069     0.902   0.112       0.101   0.203
                     500   0.80     0.794    0.032   0.081    0.613     0.065    0.794    0.050    0.048      0.264   0.069     0.805   0.110       0.101   0.200
                     500   0.50     0.494    0.040   0.074    0.420     0.060    0.492    0.057    0.053      0.230   0.067     0.502   0.099       0.104   0.221
                     500   0.00     -0.003   0.046   0.069    0.325     0.065    -0.005   0.064    0.070      0.231   0.057     0.000   0.080       0.101   0.257
                     500   -0.50    -0.498   0.040   0.060    0.378     0.059    -0.499   0.056    0.079      0.290   0.021    -0.493   0.056       0.077   0.309




40
                                                                                   Panel B: Linear trend model
                     200   1.00     0.963    0.058   0.263    0.710     0.075    0.969    0.107    0.062      0.218   0.078     0.989   0.169       0.109   0.171
                     200   0.98     0.952    0.054   0.208    0.624     0.081    0.959    0.086    0.053      0.204   0.085     0.972   0.174       0.119   0.170
                     200   0.95     0.924    0.052   0.173    0.591     0.081    0.931    0.079    0.046      0.192   0.088     0.954   0.177       0.107   0.168
                     200   0.90     0.877    0.052   0.152    0.529     0.065    0.884    0.080    0.057      0.189   0.068     0.906   0.173       0.103   0.162
                     200   0.80     0.775    0.057   0.132    0.452     0.070    0.781    0.082    0.057      0.180   0.077     0.801   0.172       0.106   0.170
                     200   0.50     0.477    0.069   0.112    0.330     0.079    0.483    0.093    0.060      0.154   0.079     0.507   0.162       0.107   0.177
                     200   0.00     -0.011   0.072   0.079    0.243     0.077    -0.008   0.098    0.061      0.153   0.051     0.009   0.126       0.098   0.183
                     200   -0.50    -0.498   0.064   0.072    0.246     0.078    -0.497   0.087    0.094      0.215   0.022    -0.484   0.087       0.075   0.192
                     500   1.00     0.986    0.028   0.152    0.850     0.057    0.987    0.081    0.026      0.323   0.056     1.003   0.115       0.103   0.208
                     500   0.98     0.971    0.026   0.104    0.804     0.059    0.974    0.047    0.030      0.309   0.062     0.981   0.116       0.106   0.205
                     500   0.95     0.941    0.028   0.109    0.777     0.058    0.943    0.048    0.048      0.295   0.061     0.951   0.113       0.104   0.210
                     500   0.90     0.891    0.029   0.094    0.724     0.062    0.893    0.049    0.053      0.306   0.068     0.903   0.111       0.100   0.195
                     500   0.80     0.791    0.032   0.090    0.638     0.066    0.792    0.051    0.052      0.273   0.074     0.805   0.109       0.102   0.207
                     500   0.50     0.491    0.040   0.082    0.444     0.057    0.491    0.056    0.054      0.237   0.066     0.503   0.103       0.117   0.226
                     500   0.00     -0.004   0.045   0.060    0.353     0.065    -0.004   0.063    0.064      0.229   0.051     0.002   0.081       0.107   0.254
                     500   -0.50    -0.500   0.039   0.060    0.386     0.069    -0.498   0.053    0.067      0.269   0.023    -0.492   0.055       0.069   0.295



     Note: HD is the hybrid estimator. FD is the first differencing estimator. PH is the Phillips-Han (2008) estimator. T-test and J-test sizes are for 5 percent level. Power
     of the t-test is computed for the null of H0 : α = α0 − 0.05.
Figure 1: Distribution of the t-statistic for the largest autoregressive root in the intercept-only
model with α0 = 1. See Table 1 and the text for more details.



                                                41
