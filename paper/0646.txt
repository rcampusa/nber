                   NEER WORKING PAPER SERIES




                  P4ULTIPLE TIME SERIES MODELS
                     APPLIED TO PANEL DATA


                       Thomas B. MaCurdy


                     Working Paper No. 646




              NATIONAL BUREAU OF ECONOMIC RESEARCH
                    1050 Massachusetts Avenue
                       Cambridge MA 02138

                          March 1981




This research was supported by NSF grant SOC 77—27136. Portions of
this paper were presented at the Econometric Society Meetings held
in Montreal, Canada, June, 1979. I am grateful for comments from
Bronvyn Ball and Thm Mroz. The research reported here is part of
the NBER's research program in labor Studies. Any opinions
expressed are those of the author and not those of the National
Bureau of Economic Research.
                                            NEER Working Paper ff646
                                                          March 1981



                Multiple Tine Series Models Applied to Panel Data



                                     ABSTRACT



        This   study presents a general methodology for fitting multiple time
series models to panel data. The basic statistical framework considered
here consists of a dynamic simultaneous equation model where disturbances
followa permanent—transitory scheme with transitory components generated
by a multivariate autoregressive—moving average process. This error scheme
admits   a wide variety of autocovariance patterns and provides a flexible
framework for describing the dynamic characteristics of longitudinal data
with a minimal number of parameters. It is possible within this framework
to   consider generally specified rational distributed lag structures involving
both exogenous and endogenous variables which includes infinite order lag
relationships. This paper outlines the generalizations of standard time
series models that are possible when using panel data, and it identifies
those instances in which procedures found in the time series literature
cannot be directly applied to analyze longitudinal data. Data analysis
techniques in the tine series literature are adapted for panel data analysis.
These   techniques aid in the choice of a time series     model and prevent one
from choosing a specification that is broadly inconsistent with the data.
Several estimation procedures are proposed that can be used to estimate all
the parameters of a multiple tine series model including both regression
coefficients and parameters of the covariance matrix. The techniques
developed here are robust in the sense that they do not rely on any specific
distributional assumptions for their asymptotic properties, and in many
cases their implementation requires only standard computer packages.


                                            Professor    Thomas E. MaCurdy
                                            Department of Economics
                                            Stanford    University
                                            Stanford,    California 94305
                                            (415) 497—9694
Introduction

      This paper applies the apparatus of stationary time series analysis

to the analysis of panel data. Multiple time series models, such, as those

studied by Zellner and Palm (1974, 1975) and Wallis (1977), are combined

with the components of variance models of Balestra and Nerlove (1966) and

Flussain and Wallace (1969). Grafting these two distinct models together

offers a natural framework for pooling cross section and time series data.

      The statistical model considered in this paper is based on what is

known in the time series literature as a dynamic simultaneous equation model

(OSEN).   This model merges multiple time series analysis with the analysis
of simultaneous equations. It consists of a system of structural equations

where endogenous variables are related not only to one another and to exogenous

variables, but also to lagged values of these variables. Generalized variants

of this 'model offer a rich statistical framework for the analysis of panel data.

Virtually any empirical specification that is linear in measured variables can

be analyzed within this framework, including generally specified distributed

lag relationships which may be of infinite order involving either endogenous

or exogenous variables.

      To model the correlation properties of disturbances over time, this study

recognizes a broader class of error processes than has been considered in

existing work on panel data. Disturbances are assumed to consist of permanent

components and time varying components that follow generally specified auto-

regressive—moving averages (APXA) processes. Previous studies typically impose

specific autocorrelation schemes on the data, and they do not perform systematic

tests among competing specifications. This study, on the other hand, approaches

the problem of choosing an error structure for panel data the same way that


                                     1
                                     2



a time series analyst decides on a particular time series specification.

Given the class of models considered in this paper, an important part

of the statistical problem is to find those specifications that are con-

sistent with the data.

        The statistical framework considered below goes beyond the single

equation case and proposes the use of vector APJIA processes as a method for

pooling cross section and time series data in the multi—equation case.

Multiple time series models offer a robust scheme for combining systems of

equations like those found in seemingly unrelated or simultaneous equation

models. These models allow one to use time series techniques to estimate

simultaneously several structural distributed lag relationships involving

both endogenous and exogenous variables and still allow for a general auto-

correlation pattern for disturbances.

        This study provides a general method for fitting multiple time series

models to panel data. It is especially tailored for analyzing a panel data

set that has a large number of individuals and a relatively short time series.

Special problems arise when one uses panel data to estimate time series models,

and some procedures found in the time series literature are not directly

applicable. On the other hand, panel data permits the consideration of more

general parameterization than is possible in standard time series analysis.

This study proposes solutions to the special problems, and it identifies the

generalizations of empirical specifications possible in the analysis of panel

data.

        To aid in the choice of model specifications, techniques found in the

time series litnrature for identifying the forms of distributed lag relation-

ships and the orders of ARMA processes are adapted for application in a panel

data setting. These techniques prevent one from choosing specifications for
                                    3



the DSEM and for the error process that are broadly inconsistent with the

data. Treating residuals as dependent variables in a seemingly unrelated

regression analysis, it is possible to compute estimates of the sample

covariogram, correlogram, and partial correlation function associated with

disturbances and the standard errors of these estimates. As in the case of

standard tine series analysis, these estimates provide information to choose

among competing ARMA specifications for the error process and they provide

the basis for simple tests for several forms of nonstationarity and hetero-

scedasticity.

      This study also develops general methods for estimating the models and

the error processes described above. Specifically, these methods provide

for the estimation of the parameters of a system of seemingly unrelated

regression or simultaneous equations including parameters of the covariance

matrix when these parameters are subject to an arbitrary set of nonlinear

constraints. These constraints may simultaneously involve regression coeffi-

cients and parameters of the covariance matrix. Estimators based on least

squares and on quasi—maximum likelihood procedures are proposed that do not

rely on any specific distributional assumptions for their asymptotic properties.

Both limited information and full information estimation procedures arc

developed. These procedures are computationally efficient and simple to

implement.

      Section I presents a general statistical framework and considers a

wide range of issues associated with model specification. Section II develops

data analysis techniques, and Section lIT describes estimation procedures.
                                                     4



                           I. A General Statistical Framework


           Panel data offers observations on a sample of "individuals" in more

than one time period. An individual here refers to an observational unit

such as a household or a firm. The models developed below are designed to

estimate the structures relating an individual's variables both within and

across time periods using all the available data.

           Dynamic simultaneous equations offer a flexible framework for describ-

ing the relationships between an individual's measured variables. A set of

structural equations from a DSEN, suitably modified for a panel data analysis,

may be written as



                                =                        +      B.X.(t-j) + Ujt), t =
(1)
             •   rY11(t_i)           j=O
                                           j2i
                                                                                      i=    1,...

wh e r e



                  Y1.(t) = g x 1 vector of endogenous variables for individual

                               i in time period t,


                  Y2.(t)
                           =   h    x 1 vector of endogenous variables,

                   Xjt) =           x 1 vector of exogenous variables,


                   r., j =     0,... ,n,     f., j       0,.. .,r, and B., j =   0,...,s,   are

                               coefficient matrices of order g x g, g x h, and

                               g x in,     respectively,
and

                   U1(t) = g x 1 disturbance vector.
                                      S


In terms of matrix lag operators, this system of equations may alternatively

be written as


(2)     r(LYY,(t) =    L)Y2jt)    + B(L)x(t) + u.(t),




where L is the lag operator (i.e., LX.(t) =    Xjt-j)),   and r(L)
                                                                     j03
                   and B(L) E     B.L3 are finite order matrix lag operators.
       j=O -'               j=O
Period 1 (i.e., t = 1) in this model refers to the first period of the panel

in which one observes all of the current and lagged values of both the endo—

genous and the exogenous variables. Thus, T = T* — max (n, r, s) where T*

total number of time periods supplied by the panel data source and n, r, and

s are the orders of the lag polynomials F(L), 'Y(L) and B(L). The following

analysis assumes that time dummies or polynomials in time are included among

the exogenous variables to capture period effects that are common across

individuals. Thus, we have N independent sets of T time series observations

with which to estimate the parameters of model (2). The above specification

assumes that a researcher wishes to analyze g equations at once, and the pro-

cedures developed below apply to the analysis of this case. A researcher may

desire to consider more than one equation at a time in order to achieve parameter

identification, to impose restrictions across equations, or to obtain more

efficient estimates.

      Sepcification (2) includes virtually all econometric models that are

linear in measured variables as special cases. If a researcher chooses to

analyze a single structural equation (or one equation at a time), then in (2)

one sets g =    1, h = the number of separate endoganous variables on the right—
hand side of the equation, and n     the number of separate exogenous variables.1


      'Notice that a variable is said to enter the equation if either its
current or its lagged value appears on the right—hand side of equation (2).
                                           6



If r(L)   =    and   L) =         where    and   are coefficient matrices of

orders g x g and g x h, respectively, the system of equations given by

(2) reduces to a standard simultaneous equation model (i.e., there are

no lagged endogenous variables) with g equations per period. If, in

addition, '0 =        and 'F0 =   0, we obtain a seemingly unrelated or a multi—
                 tg
variate regression model with g equations per period.1 If, still further,

g =    1, we have a standard multiple regression model where there is a single
equation per period for each individual.

        The statistical framework given by (2) permits the consideration of

a wide variety, of distributed lag relationships between the elements of

J Y2, and X, including infinite order schemes. The assumption that the

lag polynomials r(L), 'F(L), and B(L) are of finite order is not as restric-

tive as it may first appear. It is possible to estimate any infinite order

distributed lag relationship which can be written as a ratio of finite order

lag polgnomials using model (2). Such lag schemes, known as rati?nal dis-

tributed lags, admit flexible weight patterns on past variables and contain

many well known schemes as special cases.2

         To see how it is possible to analyze this type of lag structure within

the framework of (2), consider a single equation version of the model where

      and X each represent a single variable. Assuming that         is related to

      and to X through rational distributed lag schemes, we have



      1"Seemingly unrelated regression model" in this paper refers to any
system of regression equations whose disturbances are not assumed to be
uncorrelated. These equations may or may not contain the same set of
explanatory variables, and there may be parameter constraints across equations.
Also, there may exist some for-tu of covariance restrictions for disturbances
included in a system, and there is no requirement that disturbances are un—
correlated across systems.
         2See Griliches (1967) for a survey of these distributed lag schemes.
                                             7




                                         c (L)
                     b1(L)
(3)
           Y1•(t)            Y2.(t) + c12(L) Xjt) + U(t)
                     b2(L)


where b1(L), b2(L), c1(L), and c2(L) are lag polynomials of finite (and

typically low) order, and Ur(t) is a disturbance.' Multiplying both sides

of this equation by b2(L)c2(L) converts it into the form of equation (2)

with F(L) =
               b2(L)c2(L),    'V(L) =
                                        b1(L)c2(L), EL) = c1(L)b2(L),   and !Jjt) =

                    where F(L), 'Y(L) and B(L) are all finite order. There is,
b2(L)c2(L)tJ(t)
then, an equivalent relationship between equations (2) and (3). Analyzing

rational distributed lag schemes using specification (2) will, in general,

imply nonlinear restrictions relating the coefficients of the polynomials

r(L), P(L), and B(L). This, however, introduces no significant complications

in the following rliscussion. Both the data analysis and the estimation pro-

cedures developed below permit the imposition of such restrictions. While

imposing these constraints will, in general, yield more efficient parameter

estimates, it is important to recognize that one can estimate all the parameters

needed to construct rational distributed lag structures without imposing any

restrictions in the estimation of equation (2). One can always convert the

estimates of equation (2) into those of equation (3) by observing that

w(L) -
     —   b1(L)   and B(L) -
                          —   c1(L)
r(L)     b2(L)       (L)
         The following analysis assumes that the error terms, TJi(t).. are dis-

tributed independently across individuals (i.e., across the index 1), but

that these disturbances are autocorrelated over time (i.e., over the index t)

for the same individual. It is often useful or necessary to restirct the

         1The polynomials b2(L) and c2(L) are assumed to have roots that lie
strictly outside the unit circle.
                                       8


form of this autocorrelation. Imposing such restrictions reduces the

number of parameters in a model; it can create a statistical model that

may be used for prediction outside the sample period; or, in the case of

a simultaneous equation model, it can aid in securing the identification

of structural parameters. A natural specification for the error process

in a pandel data setting is one that merges linear multiple time series

models with variance component schemes.

        This study considers such an error structure. In particular, 131(t)

is assumed to follow a permanent-transitory scheme of the form


(4)      U1(t) =       +


where

                   = g x 1 vector of permanent components with

                                     if i=j
                            =

                                0    otherwise,


and the u(t) is a g x 1 vector of transitory components uncorrelated

with 4. and generated by the multiple time series process


                   -                        +
(5)       v(t) =
                           Af(t-i) + e(t)       j=l

which may be equivalently written as


         A(L)u(t) =
(6)
                       M(L)ci(t)

where
                                              9



                  p       .
                                              q
        A(L) E         A.L3 and M(L) E        I    N.L3 are g x g matrix lag
                 j=O                         j=0
                 operators with A0 = N0 =    and the roots of IM(L) = 0
                                          18
                 are assumed to lie on or outside the unit circle,1

and

       ajt) =    g x 1 vector of white noise with

                                         1           if t   t*and ij
                 E(E.(t)c"jt*)) =

                                         0           otherwise,


Thus, U,(t) is the sum of a vector of correlated permanent components,

4., and a vector of individual specific variates, v.(t), which follows a

multivariate AJUIA process. There are two sources of autocorrelation

accounted for in this error specification: one is due to the presence

of permanent components which capture the effects of unmeasured charac-

teristics unique to the individual that remain constant over the sample

period; and the other source is the time series components which account

for the existence of unobserved variables that vary systematically from

one period to the next. This error process admits a wide variety of

autocorrelation patterns and provides a flexible scheme for describing the

time series aspect of panel data with a minimal number of parameters.

      Previous studies on longitudinal analysis have considered special

cases of the error specification proposed above for a single equation

(which implies g = 1 in (4) and (6)). The most popular specification is

one that combines a permanent component with a pure autoregressive scheme


      1The restriction on the coefficients of M(L) is the usual one imposed
in the time series literature to guarantee that these coefficients are
identified.
                                          10



(i.e., p ' 0 and q = 0 in (5))1 A few studies consider a permanent

component and a low order moving average process (i.e., g                    1, p =   0,
and q >   O).2 No study considers a mixture of an autoregressive and a

moving average process.

      There are several other error specifications found in the literature

that can be analyzed using the statistical model for disturbances given

by (4) and (6). One such specification attempts to generalize the above

model following the suggestions of Nerlove (1967) in his work on "unobserved

components.'1 Disturbances are assumed to depend on more than one transitory

component. In particular, instead of (4), it is assumed that

                                    J
                   u,(t) =    +
                    1                     v2.(t)
                                            1
                                   SL=1
                                                             i   =   1,...

where the transitory components 'i9(t),            =   1,... ,J,     are mutually independent

and each follows a restricted ABMA process of the form


                               =


where A(L) and N(L) are matrix lag operators with the same properties as

A(L) and M(L) defined above, and the ci(t)'s are white noise vectors.

According to this specification, the disturbance vector of the DSEM depends

on permanent components and a sum of J time varying components that are each

generated by a unique multivariate ARMA process. Since this new error process

includes the simpler process proposed above as a special case with J =                     1,

      1David (1971), ilause (1977), Lillard—Willis (1978) and Lillard—Weiss
(1979) are examples of studies that estimate first order autoregressive
schemes (i.e., p = 1). Ashenfelter (1978) considers higher order processes.
      2.
      Friedman   (1954, p. 353) and Hause (1977) are examples of such studies.
                                         11


some authors have offered this new process as a way of providing f or a

wider class of autocorrelation structures for disturbances.' This more

complex error process, however, does not admit more general autocorrelation

structures. This result is a direct consequence of the fact that summing

disturbances generated by ARNA processes yields a nev disturbance that

also follows an ARt4A process.2 Thus, any autocorrelation pattern produced

by the complex error process can be duplicated by the simpler process given

by (4) and

      'Hause (1977), Lillard—Weiss (1978) and Lillard—Weiss (1979) estimate
elementary specifications of this error process for the case J   2. The
most widely estimated specification assumes that i1.(t) follows a first order
autoregressive process and 'o2.(t) is white noise.
                                                   J
      2flefine the disturbance vector n(t) =           v(t) where each of the

follows an ARNA process of the form assumed in the text (i.e., A(L)v2(t) =

MjL)c1(t)     £ =   1,... ,J).   It is always possible to represent the ARNA process
for v(t) as A(L)k(t) =                             E N(L)c(t) where [A(L) and
                                  A(L)MjL)c(t)
A(L) are the determinant and the adjoint matrix associated with AL(L), and
M*(L)is a fine order matrix defined as At(L)N(L). Thus, premultiplying
                    J                             LI

(t) by p(L)         fl A(L) yields p(L)(t) =
                                           1=1
                                                       e(L)M*(L)cjt) where e(L) =
P(L)/fA(L)1 is a finite order polynomial. The right—hand side summation
expression is known to have a vector moving average representation since its
autocorrelation function is zero after finite order (see Hannan (1970, p. 66)).
Thus, we see that (t) follows a vector APIIA process.
      3mere is, then, a fundamental identification problem associated with
the use of the complex specification of the error process. To estimate
this specification, one requires a priori restrictions on each of the ARNA
processes generating transitory components.
                                    12


      One can also analyze error specifications in which permanent

components are not uncorrelated with itexogenotisti variables or in which

disturbances depend on individual specific growth rate terms as well as

permanent and time series components. For those specifications where

is correlated with the X.(t)'s, first—differencing equations (i.e., multiply-

ing both sides of each equation by (l—L)) eliminates permanent components

and creates a new model that satisfies the assumptions of the DSEM and error

process proposed above. Similarly, if disturbances depend on individual

specific growth rate terms and, instead of (4), Ujt) =       + bt + v.(t)

where b. is a g x 1 vector of permanent components distributed randomly across

individuals,1 then first—differencing can once again be used to transform this

error specification into a model like those proposed above.2 In the case of
                                                   2
an individual specific quadratic trend (i.e., b,t ),   second—differencing   puts

the model into the appropriate form. Differencing, then, offers a way of

collapsing more general DSEM's and error processes into a specification which

is nested within the framework considered in this paper. It is important to

recognize that differencing changes the specification of the DSEM and the

ARMA process for transitory disturbances in a known way and introduces no

new parameters. Thus, the effects of differencing can easily be undone in

the sense that one can construct the model associated with levels using only

the parameter estimates of the differenced equations.

      One can estimate rational distributed lag relationships using the

strategy outlined above when disturbances are assumed to follow the error


      1such error specifications are common in the empirical literature
on earnings (see, for example, Hause, 1977 and Lillard and Weiss, 1979).
      2First differencing in this case reduces random trends to permanent
components.
                                      13


scheme proposed above. If the disturbance U(t) appearing in (3)

follows a permanent—transitory scheme of the sort given by (4) and (6),

then the transformed disturbance U(t) in specification (2) (which equals

b2(L)c2(L)U(t)) follows an error scheme of the same form. Thus, using

model (2) along with error processes (4) and (6), one can fully estimate

rational distributed lag structures while imposing covariance restrictions.

Translating from equation (3) to specification (2) will, in general, imply

nonlinear restrictions relating the coefficients of the lag polynomials

r(L), T(L) and 5(L) associated with measured variables and the polynomials

A(L) and M(L) determining the autocorrelation of disturbances. The data

analysis techniques and the estimation procedures developed below permit

one to consider and to incorporate these types of constraints. Introducing

the possibility of covariance restrictions in the analysis of rational lags

can lead to an increase in the efficiency of estimation, and it can provide

a source of parameter identification which relaxes the need for exclusion

restrictions and exogeneity assumptions.


A Familiar Representation

         Combining all the structural equations for an individual into one

model creates an alternative representation of the above DSEM that is partic-

ularly useful for the analysis of panel data. Stacking the equations given

by (1) for individual i in descending order starting with the last period

flelds
                                                       14




                    j=O                 I              j=O

(7)             I




                I    Z    Fv ('.-j)
                        jlI             I
                                               I
                                               1
                                                        '
                    j=O
                [                       I

                                                              S
                                                                   E x (T-j)
                                                                               'ii
                                                                                I      I       (T)
                                                             j=d                I




                                                              S
                                                             I     B X (1—i)    I

                                                                                           U1(1)
                                                             j=o
                                                                               j



In    matrix notations this system of equations may be written as




(8)
                          'ft -4-—i
                     -ii= -21     BX         + U
                                                   I
                                                                                           =




with
                                    o          i=j
                     lj
                E(U.U')    =

                                    o          otherwise


where V
           ii
                     (Y'.(T),
                      lx
                                VII (Tj)
       =   0'                               X = (x:(T),...,x:(l—s)),
                                                             1
                                                                                TY =
                                                                                 1
 21

U(lfl, and the coefficient matrices I, !, and B are block diagonal band

matrices with the matrices                                                     and JBQ,...,B5 ]      running
                                          15


down the diagonal of 1', !, and B, respectively.1 Written in the form of (8),

we have constructed a system of simultaneous equations in which disturbance

vectors are independently distributed over observations so that it is poss-

ible to estimate the unknown parameters of the coefficient matrices 1', 1', and

B using standard simultaneous equation estimation procedures.2

        The main consequence of assuming a DSEM of the sort presented in

equations (1) or (2) when analyzing panel data is the imposition of constraints

across equations in different time periods for a given individual. Inspection

of equation (7) reveals that the specification assumed in (1) implies equality

constraints across the rows of r, w, and B.

        One obvious generalization of the above DSEN that is possible when this

model is used to analyze panel data rather than time series data concerns the

constancy of matrix lag operators over time. In specifying the 08124 given



               matrix Q   is   a diagonal band or a block diagonal hand matrix if t
 has   the form                                                 -



                          abcd
                               abc d
                                   ....        0

                               0
                                       abcd
                                        abcd
 If a, b, c, and d are constants, then Q is a diagonal band matrix. If a, b,
 c, d are matrices, then Q is a block diagonal band matrix with the matrix
 Ia, b, c, d] running down the diagonal.

         2
         These procedures include two stage and three stage least squares methods
that assume disturbances are correlated across equations but not across obser-
vations. When a nonsimultaneous specification of the 05111 consits of g multiple
regression equations per period (i.e., r(L) =     and 'Y(L) = 0 in (2)), we have
                                              ]:g
    1Tg and V = 0, and (8) becomes a seemingly unrelated regression model which
can be estimated using standard joint generalized least squares procedures. See
Section III for further discussion.
                                                  16


by (2), it is not required in a panel data analysis to assume that the

matrix lag operators r(Ij, 49(L) and B(L) are the same across time periods.

Instead, one can add a tttel subscript to these matrices indicating that there

is a new    set   of coefficients for each period and, thus, a different distributed

lag relationship. The consequence of this generalization in (8) is the relax-

ation of equality constraints relating the rows of F, 49, and B.


A Specification for the Covariance Natrix

          The consequence of assuming that disturbances appearing in (1) follow

the error specification given by (4) and (6) is the imposition of restrictions

on the covariance matrix 8 =
                                        E(L71U)
                                                   associated     with the stacked representation

of the DSEN       given   by (8). The following analysis derives the exact restrictions

on autocovariances implied by the combined variance component—multiple time

series process assumed above, and it formulates an explicit parameterization

foi the covériance matrix 0.

          According to (4), each component of the disturbance vector U. is generated

by an error model of the form tJ.(t) =                 4).   + vjt), t =   l,...,T,   where


E(q1q)
             4, and o1(t) follows the         multivariate        ARMA   process,   A(L)v1(t) =

             which is distributed independently of 4). with E(c:jt)c!(t))                         E.
M(L)c(t)
Defining =          (v!(T),... v(l).)      and         as a T x 1 vector of ones, we have

   = (1
          &J.) +      u..; so,



(9)          0 =    E(U.U!) =    (u.'   ®)   + E(v.v).


To specify e, we need the implied parameterization for E(vv).

             According to (5), v. is determined by the system of equations
                                             17



                                        p                          q
                     v.(T)              E A.v.(T-J)
                                                  -
                                                                   E M.e.(T—j)
                                                                  j=0
                                                                        :i
                                       j=l
(10)                                                         +
                       :
                                                                   q
                     v.(l)
                      1
                                       j=1    '
                                             A.v(1—j)              E M.c(1—j)



where     =   I.           This system does not represent a one—to—one transfor-

mation from the cjt)'s,           =   l,...,T     to v.• One cannot, then, derive the

covariance matrix for           from (10) if given only distributional assumptions

for c.,(T),. ..    ,(l).     Also appearing in (12) are the variables v.(O),...,

v.(l—p)       and c.(O)...(l—q) which are kno.-n in the time series litera-

ture as initial conditions or starting values for the error process. To

derive a parameterization for E(.u!), one requires a treatment for initial

conditions.

        This paper treats initial conditions for disturbances as random

variables. Conventional time series techniques that treat starting values

as known constants (usually chosen to be zero) result in inconsistent

estimates for the parameters of the error process if applied in a panel data

analysis where T is fixed because, in contrast to time series analysis) initial

conditions do not become "irrelevant" as the sample size increases. Similarly,

time series procedures that "backforecast° or treat initial conditions as

parameters introduce an incidental parameters problem in a panel data analysis

which under most circumstances also leads to inconsistent estimates for all

parameters of the error proeess) Treating initial conditions as random

variables avoids problems with inconsistency by introducing only a finite

number of new parameters: those determining the distribution of the starting

        1This problem of incidental parameters and inconsistent estimation is
examined by Andcrson—flsiao (1981)
                                                            18


values, and those relating the distribution of the starting values to the

distribution of disturbances realized in periods 1 through T.

          There are several complications associated with choosing a distri-

bution for the two sets of initial conditions specified above. If we assume

that the stochastic process generating disturbances during the sample period

is also operative prior to this period, then one would expect the v.(t)'s,

2. =   l—p,.   ..   ,0,   to be not only correlated with one another and with the c.(j)'s,

j =    l—q,.... ,0,        but also with all the v(t)'s realized after period 0.        Further-

more, the correlations relating these variables will, in general, depend

directly on parameters of the ARNA process given by (6), and one must account

for these restrictions to achieve efficient estimation. It is possible to

minimize these complications by specifying the system of equations given by (10)

and considering an alternative expression for

          A moving average representation of an ARNA process provides the basis

for this new expression for v.,. Assuming the multiple time series process

given by (7) started sometime in the finite past, say between the periods

-u—b and r with b >                0,   it is possible to write each of the '.(t)'s realized

after period t            as   a moving average scheme of the form


                               t—i—1                        t--+b
                                                        +           K.f.(tj)
(11)
               v1(t)           '
                               j0        Kci(t_i)

where

                          K0 I g
                          K1 = N1 - A1

                          K2 = N2       -        -
                                            A2       A1K1




                                        -
                                            jl A3_
                                            19

and the fjt)'s, z      = rb,. ..,r,    are error vectors distributed independently

of ci(t*) for all t* > t. Formally, one can derive a relation like (11)
                                                   p
by starting with the ARMA representation j1(t) = — E A.u(t—i) +
 q                                                jl
 Z M.c(t—j) and successively substituting out for past v1(t—j)1s using
j=O
their ARMA representations until t—j = r• One can readily verify that (11)

is indeed a valid expression for vi(t) since premultiplying this equation

by A(L) yields A(L)vjt) =                    for t > T+q.   The .(z)'s in (11) may
                                M(L)ci(t)
be interpreted as the true initial conditions of the ARMA process. Specifying

the distribution of these variables determines exactly how and when the ARNA

process generating v1(t)'s began.

       Using (11), it is possible to reformulate the system of equations

given by (10). To avoid the need for dealing with several possible cases, it

is convenient to introduce the notation K. = 0 for j < 0 (for j > 0, K. is
                                                                         c
defined in (11)) and the definition that a summation of the form Z is equal

to zero whenever c < 0. Using this notation, equations (10) and (11) imply

          p
                            -
                                  -q
           E A.i(T_i)              E M.c(T—j)                 0
         j=O                      j=O


          p                        q
          Z A.v.(p+l—j)            E M.E.(p+1—j)              0
                                         1
         j=0                      j=0
                                  q— 1
(12)           v.(p)
                1                  Z K.1(p—j)           + n.(p)
                                                            1
                                  j=O
                                  q—l
                                                            n(p—l)
                                  j=o



                                  q—l
               vi(l)                     K. 41c1(p—j)       nJl)
                                  jO
                                                  20


where
                      t—-r—l                      t—r+b
         ru(t) =               K4c..(t—j) +                K.f.(t—j),           t =   I,.. .,p.
                      jt—p+q                      j=t—t


The first set of T—p sets of equations in (12) are simply the standard

representation of the ARMA          process generating                        ,v(T), and the

second p sets of equations are the moving average representations of the

ARNA process for vjl),.. .,v(p) with the n,(LYs, i =                        1,... ,p,   defined

to include all disturbances realized prior to period p—q+l (i.e., the

c.(t)''s and the f.(t)'s for t c p—q). The formulation of (12) assumes
                1
that T < p—q.        In matrix notation, (12) may be written as


(13)     F .
                    = [ii]
                       lii

with
                     v.(T)               cJT)                       rijp)


           i                       1
                                                            TL=
                                                             i
                     v..(l)
                                         c1(p—q+1)


and F is a gT x gT matrix and C is gT x g(T+q) matrix defined a

                                   Fl
                        F =                   =                 I

                               F        F          0   I
                               21        22                gp


       1This assumption concerning the starting time of the ARNA process
generating the v.(t)'s is a weak restriction and follows inmiediately from
the assumption that v1(pf 1) can be represented by the specification given
by (5). This restriction on T ensures that no fjt)'s appear in the moving
average component of (5) for t = p+l.
                                                21



                                   1l          012
                                                                      N               0



                                               0              0           K
                                                                          —           I
                                                23                                        gp


where: F1 =    A is a g(T—p) x g'              block diagonal band matrix with

the matrix [A0,... ,A] running down the diagonal; F21 = 0 is a gp x g(T—p)

matrix; F22 =         is an identity matrix of dimension gp; 011 = M is a g(T—p)
                'gp
x g(T—p+q) block diagonal band matrix with the matrix [M0,.. Nq] running

down the diagonal; 012 = 0 is a g(T—p) x gp matrix; 021 = 0 is a gp x g(T—p)

matrix, 023= 'gp; and 022 =           Kis     a gp x gqwith


                                      K
                                        1
                                              ...         K                 K
                                                           q—2                q—l
                               K
                                —l
                                      K
                                        0
                                              ...         K                 K
                                                           q—3                q—2




                               K      K       ...         K     K
                                1—p     2—p                q—p—l q—p


When forming the partioned matrices associated with F and C, the above

analysis assumes that any matrix with an implied dimension equal to zero is

deleted from the specification. Thus, when p = 0, F = [F )                                      and   C =   [011 1;
and when q = 0, K is eliminated and G HC"
                                                     ro
                                                        1
                                                                  C
                                                                  C
                                                                                    The above specification
                                                     L21              23j
for   K is written    in   general terms to handle all possible orders of the AR}IA

process   including p c q, p =        q, and p > q.           Recognizing                      = 0 for .j < 0 and

   # 0 for j   )0    reveals that K has a fairly simple form with nonzero elements

in those K's on and above the positions CL, 1), (2, 2),. ..,(min(q,p), min(q,p)),

and zero elements in those K's below and left of these positionsJ

          'Formally, the matrix K has [K0,
                                                     K1,..I,K                   1    as its first set of
                                            22



        Given the expression for v. implied by (13), the problem of parame-

terizing E(v.v) becomes one of specifying a correlation structure for the

disturbance vectors c. and              Since each of the components of      follow

a white noise error process, we have


(14)        E(c.c) =
                       (IT_p+q   ® E)
where E =
             E(c.(t)c(t))     for t =   p—q+l,... ,T and * is   defined by the

Kronecker product. Inspection of the formula for the ri.(i)'s reveals three

facts: (i) the n.(Z)'s depend on a common set of disturbances; (ii) all of

these disturbances are realized prior to period p-I-q—l; (iii) included among

these disturbances are the initial conditions for the ARNA process (i.e.,

the f.(t)'s). Since each of the components of c. are realized during and

after period p+q—l, fact (ii) implies E(ri.c!)          0. Fact (i) implies that

the components of          are mutually correlated, so E(n.ri') contains no zero

elements in general. In addition, without imposing rigorous restrictions on

both the number and the correlation structure of initial conditions, fact

(iii) indicates that no restrictions will exist on the form of E(O.T1). In

the general case, the          will possess an arbitrary covariance structure which

we may formally express as


(15)        E(n.n!)    A


where A is any positive definite symmetric matrix.

        Combining the above results, we obtain the following specification

for 0


g rows,jO, K0,.. .,K —2 as its second set of g rows, and so on until the
                   q                               th
set of g rows is reached, or if q > p, until the q    set of g rows is reached
after which the rows of K contain zeros.
                                              23


                                                   0
 (16)            o =
                       E(v1)   =   F1   C
                                                   Jo' F_i'


This parameterization imposes all of the restrictions implied by the

ARMA process unless one is willing to introduce precise information about

how and when this process started.

            There are two modifications of the above parameterization for A that

may be useful in applied work. First, to simplify the construction of the

matrix K, one can replace each of the nonzero elements of K (i.e., all the

IC's,   j    >   0) by arbitrary parameters rather than using the coefficients of

the ARRA process and the formulas specified above to form these elements.

This modification avoids the need for imposing nonlinear restrictions, but

it introduces new parameters and reduces the efficiency of estimation.

            The second modification concerns the parameterization of A defined

by (15). This matrix is purely a theoretical construct and represents

nuisance parameters. An unattractive feature of this parameterization is

that one cannot easily infer an approximate value for A using preliminary

data analysis techniques or estimation methods that do not require the full

estimation of a. The availability of approximate values greatly reduces

computational burden when used as starting values for parameters in a nonlinear

computer program which is invoked to estimate 0. A way around this problem

is to replace A by A* E(ninj) + (Iq®E) K' which is also only restricted to be

positive definite and symmetric. Substituting this new parameterization into (16)
implies
                                        M     N'                 [o'
                                        -     —
                                                              NE*l,
(17)                   0=F     1                                 [i- —1'
                                                                      F
                                   [0 K] y*   H'
                                    24



According to this new specification for 0, A* E(v') where the vector

v'= ('uj(p),... ,v(l)) includes the last p components of u. In contrast

to the previous paranieterization, Mis directly interpretable and can be

easily estimated prior to the full estimation of 0.

       There are several ways in which the above specification of i3 can

be generalized in the analysis of panel data that are not possible in

'standard time series analysis. Each of these generalizations involves a

form of nonstationarity.

          First, there is no requirement for the roots of the autoregressive

matrix lag operator (i.e., the roots of (A(L)) = 0) to lie outside the unit

circle. Thus, it is possible to consider such error processes as random

walks when using panel data. Whereas in a time series analysis the existence

of such nonstationarityhas significant consequences on the asymptotic

porperties of estimators, it has no such effects in the case of panel

data where asymptotic results rely on a large number of individuals rather

thai a large number of time periods.

          Second, it is possible in the analysis of panel data to permit

the white noise vectors, c.(t), to be heteroscedastic over time, which

introduces yet another form of nonstationarity. To account for this

heteroscedasticity in the above analysis, one only needs to define E* appearing

in (14) as a block diagonal matrix of the form Dia(ET. 'T—p—q              =

                In standard time series analysis this sort of nonstationarity
E(c1(t)c(t)).
does not necessarily create any conceptional difficulties, but it does require

an explicit paratneterization of the suspected form of the heteroscedasticity that

avoids an incidental parameters problem. In the case of panel data, however, it

is possible to allow for arbitrary forms of heteroscedasticity of white noise

disturbance vectors over time.
                                              25



             A third form of nonstationarity permitted in panel data analysis

 concerns the constancy of the autoregressive and the moving average matrix

 lag operators appearing in the multiple time series error process given

 by (6). The matrices A(L) and N(L) can be allowed to vary arbitrarily

 across periods so that there is a new set of autoregressive and moving

 average parameters for each t. To modify the above analysis to account

 for A(L) and N(L) being period specific1 one must subscript the A., the

 M., and the K. matrices appearing in the specifications of F and G defined

 by (13) to indicate the time period each matrix is associated with. This

 subscripting has the effect of relaxing the equality constraints across

 the rows of the matrices A and N, and it essentially voids any constraints

 relating the nonzero elements of K to one another or to the coefficients

 of the ARMA process.


 A Reduced Form

          In the following analysis on estimation we require a reduced form

 specification for the simultaneous equation model given by (B). Write

 this specification as


                     =
                         11X1   + VI

(18)

           E(V4V') =
                          c       i=j
              s
                                  otherwise

where     is a vector that includes all the endogenous variables appearing

in (8), the vector         contains all exogenous variables for each period

including lags, II is a coefficient matrix, and V. is a vector of distur—
                                                    1

bances.
                                              26
           The various specifications of the DSEM considered above imply
different restrictions on the II        and   the 2 matrices. If considering a non—
simultaneous    specification (i.e., when r(L) =          I    and 14(L)      0 in (2)),

then (8) is obviously its own reduced form which implies TI =                     B and £2 =   S.

If   considering the special simultaneous specification where there are no

right—hand—side endogenous variables                 and no lagged endogenous variables

(i.e., when F(L) =
                         F0
                              and ?(L) = 0 in (2)), then t          is   a nonsingular matrix,

and (8) can be solved in the usual way for the reduced form by premulti—

plying through by Li which implies TI =                  and   £2        £1 OFl
           If analyzing the general specification of the TISEM, however, (8)

does not constitute a complete system of equations in the sense that there

are more endogenous variables than there are equations; so, it is not poss-

ible to solve (g) for reduced form specification and determine the restric-

tions on II and £2 without introducing additional equations. The strategy

followed here to add the needed equations is the one normally used in

limited information analysis of simultaneous equations; namely, a prediction

ecluation is introduced for each endogenous variable that isnot determined

by the structural model under consideration. There are two sets of endogenous

variables that are not directly determined by the DSEM considered above.

The elements of the vector             defined 'by (B) constitute the first set,

and the elements of the            ralized prior to period 1 (i.e., the initial

conditions for the Yii(t)'s) make up the second set which we group into the

vector      =
                 (Yj1(o). .. ,Y'j(l
                                       —   n)).    This study assumes that these variables

are determined by the equations



                ii

                     =         +
(19)
                         2xi
            Y.
             2x
                                            27


where H is an unrestricted coefficient matrix, and V 2i is an error vector
       2

that is uncorrelated with all the elements of X1. These specifications

for prediction equations are not restrictive. It is always possible to

define the disturbance vector V         so that it has zero mean, and it is
                                   2i
uncorrelated with all the exogenous variables of the model. There is no

guarantee that the covariance matrix of this error vector will be indepen-

dent of X.,, but most of the procedures described below do not require the

assumption that V2. is homoscedastic across individuals. We maintain this

assumption only to simplify the exposition.

            Combining equations (19) with the structural model given by (B)

implies a complete system of equations that can be solved for a reduced

                                                                       are defined by (8)J
form like (18) with Y =    (Y.,    Y.) where Y. and
The parameter constraints implicit in (8) translate into restrictions on

the Ji and the 2 matrices. These restrictions can be shown to take the

                                                 and   12 =
                                                                ll 122 where
following form. Partitioning 11 =                                                      112 is
                                            2                    21    22

the unrestricted set of coefficients and E(V .V' .) = 12                     is the unrestricted
                                                        2i 2i          22

covariance matrix associated with (19), we have ll =                  r1(I —       !2' ll       =

r1or'1 —            — 12i2!t   —   F_ly12!tr?l                and 1212 =
                                                                            1221
                                                                                   is an unconstrained

matrix. These restrictions, of course, collapse to those presented above

for the special cases of the PSEM.




            1The complete system of equations can be written as II?. = CX. +                        f
where H =
         {   J, C = [J and
                                  [iJ.
                                        = The reduced form is obtained by'
premultiplying by H which implies the restrictions H =                       H     C and 12 =
               1
B   (   I )H
                                         28

                         It. Techniques for Data Analysis


               This section develops simple procedures that provide the basis for

choosing the orders of the lag polynomials appearing in the above specif 1—

cations and for determining whether or not it is reasonable to assume that

the coefficients of these polynomials are constant over time. These pro-

cedures prevent a researcher from choosing a model specification that is

broadly inconsistent with the data. Methods for choosing an appropriate

specification for the lag polynomials determining distributed lag relation-

ships (i.e., F(L), t'(L) and B(L) in (2)) are a by—product of results contained

in the next section on estimation and will be discussed

there. This section focuses on the more complex problem of correctly speci—

fying the form of the autoregressive and the moving average lag polynomials

generating the ARNA component of the error process given by (6).

               The two principle items used in the time series literature for

choosing the specification of an ARMA model are the sample covariogram (or

correlogram) and the sample partial correlation function) To provide formal

definitions of these concepts, let 13(t) denote a random vector which is

generated by some time series process. Given a sequence of realizations of

13(t), the covariogram is a plot of the covariance or autocovariance between

any two elements of this sequence as a function of the number of time periods

between realizations. The kth order matrix of autocovariances is.E(U(t)ut(t_k))

Plotting this matrix as a function of k creates the covariogram. The (j, 2-)

element of this plot, E(U.(t)U2-(t—k)), is called an own covariogram when

j =   Ic   and cross covariogram when j # k. The partial correlation function,


        1Granger—Newbold (1977, Chs. 3 and 7) provide an extensive discussion
of how to use the covariogram and the partial correlation functions to build
time series models. Nelson (1973) provides a more elementary discussion.
                                        29

 on the other hand, is a plot of the partial correlation coefficients against

 the length of the lag between random variables. The kth order matrix of

 partial correlation coefficients denoted as A.K is defined by the regression

                     k
 equation U(t) =   —I      A.U(t—j) + c(t) where E(c(t)U'(t—j)) = 0 for j =
                    j=i.
 Plotting Ak as a function of k produces the partial correlation function.

 The covariogram is particularly useful for identifying the presence of a

 moving average process. The partial correlation function greatly aids in

 identifying an autoregressive process.

           This section formulates simple procedures for estimating the

 covariogram and the partial correlation function using panel data. The

 discussion develops these procedures for the analysis of the time series

 properties of the distrubances 13(t) appearing in the specification of the

DSEM given by (2) and (8).2 Thus, it provides information that is useful

when choosing a specification for the error process given by (4) and (6)

To simplify the following exposition, the data analysis procedures are

formulated for the situation in which a researcher is investigating the

properties of a distrubance from a single equation (i.e., g = 1 in (1)),

which implies that 13(t) is a scalar. These procedures, however, immediately

generalize to the multi—equation case.


      1Pormally, A cannot be interpreted as partial correlation coefficients
unless the stochastic process generating U(t) is stationary.
      2When considering a simultaneous equation specification, it is
Implicitly assumed that the parameters of the £     and   matrices in (B) are
identified and can be estimated without using any covariance restrictions. Under
these circumstances, it is possible to obtain consistent estimates for each U(t)
using standard two or three stage least squares procedures which neglect covariance
constraints. If this assumption is violated, then it is not possible to directly
analyze the time series properties of the structural disturbances, and one must
apply the following data analysis techniques to the reduced form disturbances
given by (17).
                                              30


Procedures for Estimating the Covariograin and the Partial Correlation Function


             Suppose, for the moment, that one knows the true values of the

disturbances. The discussion below shows how these disturbances can be used

to estimate and test hypotheses concerning the form of the covariogram and

the partial correlation function using a standard seemingly unrelated

regression framework.

             To construct the sample covariograrn, we require estimates of auto—

covariances for each order or length of lag. To estimate the kth order

autocovariance, consider the following set of regression equations


(20)         Ujt) Ujt — k    )   = 0kt   + iLjt)                        t =


where Ujt) U.(t—k)       is a dependent variable,          is a parameter, and .(t) is

an error term distributed independently across individuals. Stacking these

equations for a given individual yields a seemingly unrelated regression

model of the form


            U(T) U.(T—k)

                                          6k +

            Ujk+1)Ujl)

(21)
                                          C        i:=j
                     E(4i)        =


                                          0        otherwise


where 6' =                        is a vector of intercepts, ip'!   =
              (ekT...ek(k÷l))
is   a disturbance vector, and we have implicitly assumed that the fourth moments

of iJi(t) exist and are constant across individuals. The intercepts of these
                                          31


 equations ek = E(U1(t) iJ(t-k))      represent the kth order autocovariances

 associated with periods t, t —   k+l,,.   .
                                               ,T.

          Thus, estimating equations like (20) or (21) by least squares or

joint generalized least squares using data on individuals provides all the

information needed to construct the sample covariogram and to test hypotheses

concerning its form. Unconstrained estimation yields multiple estimates of

ktF order autocovariances (ie one for each period t =      k+l,...,T)   which

reflects the fact that in a panel data analysis one can permit the parameters

of time series processes to be different in each period.1 Constrained

estimation, on the other hand, of seemingly unrelated regression model given

by (21) restricting the intercepts to be constant across equations (i.e.,

constraining the elements of ek to be the same) produces a unique estimate

of the kth order autocovariance which uses all available data. Estimating

models like (21) for each k, then, with equality constraints ott intercepts

yields unique autocovariances for each order. Plotting these constrained

estimates against k creates the sample covariogram.

          Combining the system of equations given by (21) for all values of

k yields a model of the form

(22)     St(U1U) =   6 + C1           i        1,...

                      T       i=j

                     0        otherwise




           'As discussed in the previous section, in a panel data analysis where
 asynptotics rely on a large number of individuals, it is possible to allow the
 autoregressive and/or the moving average lag polynomials (i.e., A(L) and 11(L)
 in (6)) to differ in each period, or allow for heteroscedastic white noise
 over time. Permitting variation in parameters of this sort generally implies
 that the 8kt are different for each t.
                                      32


where St(•) denotes an operator that stacks the rows of a matrix

and deletes      all the elements that lie below the diagonal, 6 =   St(s)   a

               is a vector of intercepts, and   is an error vector that
St(t(t31U1))
contains the J1(t) disturbances appearing in (20) for all values of k.

This expanded seemingly unrelated regression model provides a framework

in which one can simultaneously estimate or test constraints involving auto-

covariances of different orders. An especially interesting hypothesis in

this regard is stationarity of the error process which implies that Q is a

Topletz matrix.) This hypothesis translates into simple equality constraints

relating the elements of a which are easily tested jointly using standard

generalized least squares procedures applied to (22).
           To construct the sample partial correlation function we require

estimates of partial correlation coefficients for each order. To estimate

the kth order partial correlation coefficient, consider the following set

of regression equations


                                           + kt U.jtk) + ejt)                t =
(23)       U1(t) =       U(t—l) +


          matrix Q is   a Topletz or a block Topletz matrix if it has the form

                                     ab cd e
                                     babcd
                                     cbabc
                                     d cb ab
                                     edcba
If a, b, c, d, and e are constants, then Q is a Topletz matrix. If a, b, c,
d, and e are matrices, then Q is a block Topletz matrix with a, b, c, d, and
e as its submatrices. When lJi(t) represents a univariate time series, which
is the case considered here (e.g., g = 1), stationarity implies that autocovar-
lances of a given order are constant, or equivalently that 0 is a Topletz matrix.
In the multivariate case (i.e., g > 1), stationarity of Ui(t) implies that 0
is a block Topletz matrix.
                                         33


where the p's are parameters and ei(t) is a disturbance distributed

independently across individuals. Stacking these equations for a given

individual yields an equation system of the form


                 13(T)                U (T—j)                 0
                                  k
                  •                               •                   p.+e.1   i=l,...,N,
                  •          jl                       .

              U.(k+l)                       0
                                                          U1(k—j+l)


(24)                          R       i=j
              E(e.e)
                              0       otherwise


where
         p        JT""J(k+l)' j =      l,...,k,   are parameter vectors and

        (ejT),. ..e,(k+l)) is a disturbance vector. The parameters kt represent

the kth order partial correlation coefficients associated with periods t,

t =    k÷l,... .,T.
              Estimating the parameters of the seemingly unrelated regression

model given by (24) for the different values of k, then, allows one to form

the partial correlation function and to test hypothese relating to its structure.

Unconstrained estimation yields estimates of partial correlation coefficients

that are period specific. Estimating the parameter vector              constraining its

elements to be equal creates a unique estimate of the kth order partial correla-

tion coefficient. Graphing these constrained estimates for k against k produces

the sample partial correlation function.
                                                34

Using Residuals in Place of Disturbances


                An apparent difficulty with the preceding discussion is that one

does not have the true values of the disturbances available for data analy-

sis. This turns out, however, not to be a problem. All of the estimation

procedures and properties of estimators described above remain valid if

one uses consistent estimates of the disturbances in place of the true

values. Thus, one can use residuals and standard seemingly unrelated

regression packages to estimate and to test hypotheses concerning the forms

of the covariogram and the partial correlation function.

                Verifying this proposition requires two theoretical results.

Letting Ti. denote the vector of residuals associated with the stacked repre-

sentation of the PSEM given by (B),1 the needed theorems are

                             N                 N
(25)            plim [
                       --     E         —      E     U.U!) = 0
                             i=l            i/Ni=]. -'


and

                             N                                    N        A   A       AA
(26)            plim [            vec(TJ.U!) vec(U.U!)' —              vec(U1IY) vec(IJ.U!)'] = 0
                                                                                   1   11
                            i=l                                  i=l

where vec(•) denotes an operator that stacks the rows of a matrix into a

column vector. Proofs of (25) and (26) are presented in Appendix A.2


                1                                      A    A          A

A     A     A
                The                         i —ii
                      residuals are defined as U. = F?
                                                       —2i——1 BX,. The estimators
                                                                   —

            B are assumed to converge in probability at a rate so that U. — U.
1', v and                                                                                           is

  (Nh) for h c 1/2 which is satisfied for familiar estimators (e.g., least
squares, generalized least squares, two— and three—stage least squares, maximum
likelihood, etc.).

          2Similar results are proved by Hannan (1970, Ch. 7) who considers the
use of least squares residuals from a time series regression to estimate auto—
covariances and the spectrum.
                                       35


              These theoretical results imply that replacing U1 by U in regression

models (20) — (24) yields estimators with the same asymptotic properties as

those computed using the true disturbances. Proof of this proposition is

presented in Appendix B. The central fact used in this proof is that all

unconstrained and constrained joint least squares or generalized least

squares estimators of models (20) — (24) are linear functions of the matrix

 N
 I JiM!, and their asymptotic normality depends on the large sample behavior
i 1

of this matrix multiplied by the normalizing constant .        Similarly,   when


residuals are used in place of true disturbances, the asymptotic normality

of the new estimators depends on the behavior of the matrix

      N,.,.
-     z   U.U!.   Condition (25) guarantees that the asymptotic distributions of
             1
,/Ni=l

these new estimators is the same as the estimators computed using true dis-

turbances. Condition (26) guarantees that use of standard techniques to

compute the covariance matrix of estimators based on residuals and on true

disturbances yield equivalent results.

              Therefore, when residuals are used to estimate either model (21), (22)

or (24), all parameter estimates, standard errors, and test statistics

reported by a standard seemingly unrelated regression package are asymptoti-

cally valid! Constrained estimation of models like (21), (22) and (24) using



          1Notice that these results do not require any special distribu-
tional assumptions other than the existence of fourth moments and the con-
stancy of moments across individuals. More precisely, the application of
the central limit theorems requires the existence of any absolute moment
greater than fourth order.
                                      36



residuals, then, offers a simple way to estimate the covariogram and the

partial correlation function and to test hypotheses concerning their

structure. It ia also possible to construct estimates of the correlogram

which is another data analysis tool found in the time series literature.

The correlogram is like the covariogran except that it is a plot of the

autocorrelations instead of the autocovariances. It is often used instead

of the covariogram in time series analysis because correlations are unit

free and normalized to lie between —l and 1, and, so, they are more easily

interpreted. Using the estimated values of ek from equations (21) or (22),
                       th
an estimate of the k order autocorrelation coefficient equals Sk =      h()
where a is a vector defined by ci' =
             C
                                       °k'   0o and h is a function defined

by h(a) =   Qs. A   standard application of stochastic limit theorems implies
              0
that       is approximately normally distributed with the true value of the

kth order autocorrelation coefficient as its mean, and a variance given

by -o o where S is the covariance matrix of the estimates contained
       S

in c. Thus no further estimation is required to compute estimates and

standard errors for the correlogram.
            Using standard seemingly unrelated regression packages and residuals,

then, one can test between completing specifications of the time series process

generating the structural disturbances. If, for example, the estimated covar—

iogram is not significantly different from zero after a short lag, then a

pure moving average process is implied. In this instance, if we further

test to see whether autocovariances are constant across years, we can deter-

mine if the white noise error process is homoscedastic over tine. By

testing further to see whether autocovariances are constant across samples

composed of individuals of a given characteristic, we can determine if the

moving average process is the same across individuals. If, instead, the
                                       37

covariograri converges to a positive constant after a short lag, then the

error terms may be generated by a moving average process and a permanent

component.

             In a more general context, testing among various specifications of

Ok'S in (21) or (22) and k' in (24) allows one to test for a pure moving

average process, a pure autoregressive process, and many kinds of mixed

processes. One can also test for several forms of nonstationarity and

heteroscedasticity. One can distinguish between a fairly wide class of

alternative specifications of the error process if one analyzes the data

in first and second differences along with analyzing the data in levels.

As in standard time series analysis, identifying the specifications of the

Ok'S and the Pk'S that are consistent with the data only narrows the class

of models one needs to consider. Rarely does this type of data analysis identi-

fy a unique specification. Several models will often explain the data just

as well. -   In   the analysis of panel data, this is likely to be even more of

a problem because there is typically available only a short time series.
                                  38


                       Ifl. Estimation Methods


         This section presents methods for estimating time series models

that are especially tailored for panel data where T is fixed and asymptotic

results depend on large N = number of individuals. These estimation

methods are very general; given all equations are linear in the variables,

they can be applied to estimate any simultaneous equations model that

involves any set of nonlinear restrictions between parameters including

covariance restrictions. Two kinds of estimation procedures are considered:

the first is "least squares methods" which includes generalized, and two

and three stage least squares procedures; and the second is "quasi—maximum

likelihood methods. For each set of procedures, we consider both limited

and full information methods.

        The following analysis does not present any formal identification

conditions. For the standard multivariate AENA model, Kashyap—Nasburg

(1974) develop necessary and sufficient conditions for identification.

liannan (1969) presents sufficient conditions. These conditions are not

easily applied in practice. Panel data introduces additional complications.

The length of the time series becomes a crucial factor. The treatment of

initial conditions reduces the effective length of the panel and at the

same time introduces new parameters. Notice, on the other hand, that

adding permanent components to a multiple time series model does not

complicate the identification conditions. First differencing equations

eliminates permanent components, and it does not introduce any new

parameters.     The standard identification criteria can be applied directly

to the first—differenced specification of the model. Introducing
                                     39

permanent components, then1 has the effect of reducing the length of the

time series by one period. Identification will be lost only in those

cases in which the orders of the autoregressive and the moving average

components are sufficiently high to make the length of the time series

a crucial factor.
         The statistical models proposed in Section I involve two sets of

parameters: the first set——hereafter called the structural coefficients——

consist of all those coefficients appearing in the stacked representation

of the DSEN by (9) (i.e., the elements of 1',! and B);1 and the second set——

 hereafter called the covariance parameters—-includes those parameters

involved in the specification of the covariance matrix of the disturbance

vector in (8), ® =   E(U.U!)   (i.e., the coefficients of the autoregressive

and moving average lag polynomials and the elements of the covariance

matrices of the white noise error vectors and initial conditions).


Least Squares Methods




         Three estimation procedures based on "least squares methods" are

proposed below: one to estimate the set of structural coefficients,

another to estimate the set of covariance parameters, and a third to

estimate both sets of parameters simultaneously.

         Standard procedures can be employed to estimate the set of struc-

tural coefficients. If one is analyzing a nonsimultaneous specification



         11iopefully this terminology will not result in any confusion. When
analyzing a nonsimultaneous specification of the DSEM (i.e., I' =   I
                                                                   and
  = 0), the set of structural coefficients obviously just Includes regression
coefficients.
                                   40

of the OSEM (i.e., F =   I   and   0 in (8)), then joint generalized least

squares procedures that permit the

imposition of equality constraints across equations can be applied to (8)

to estimate these coefficients (i.e., the elements of B). If, on the

other hand, one is analyzing a simultaneous equation specification of the

DSEM, then two     or three stage least squares methods that allow for

linear restrictiors across equations can be applied to estimate the struc-

tural coefficients of (8)) Recall that the need for imposing equality

constraints across equations is a direct implication of the assumption

that distributed lag relationships are constant over time. All of the

above procedures may be classified as limited information in the sense

that they do not simultaneously estimate the parameters of the covariance

matrix 0. They require a consistent estimate of 0, but this obviously can

be constructed without directly estimating any parameters of the time

series process generating disturbances. In those cases where one chooses

not to introduce any assumptions regarding error processes or covariance

restrictions, the above procedures yield estimates of the structural

coefficients that use all available information and restrictions.

         These estimation procedures provide a natural framework for per-

forming preliminary data analysis to determine the length of the lag poly-

nomials associated with distributed lags (i.e., r(L), (L) and B(L) in (2)),

and to test whether or not the coefficients of these polynomials are constant


         1The asymptotic properties of estimators obtained from these pro-
cedures do not require the assumption that the disturbances in the predic-
tion equations given by (19) are homoscedastic. Thus, to employ these
simultaneous equation estimation procedures, one requires no assumptions
in addition to those presented in (8),
                                    41


across periods. This form of data analysis involves standard tests of

linear hypotheses. Identifying the orders of lag polynomials involves

tests of whether coefficients on lagged endogenous and exogenous variables

are significant or not. Checking for the constancy of distributed lag

relationships over time involves tests of equality of coefficients across

equations. An attractive feature of the data analysis techniques is that

they can be implemented in complete ignorance of the stochastic process

generating error terms. An unconstrained estimate of the covariance matrix

can be used in the computation of coefficient estimates and test statistics.

           The results developed in the previous section offer a general

method for estimating the set of covariance parameters.      The seemingly

unrelated regression model given by (22) is particularly well suited for

estimating parameters of the covariance matrix 0 =   E(U.U!) associated with
the stacked representation of the PSEN given by (8). If one assumes that

the components of U. are generated by the combined variance component—ARNA

error process given by (4) and (5), then as showr

in   Section I, the elements of 0 are functions of the parameters of this error

process where the exact functions are implied by the relations (16) or (17).

Substituting these functions for the elements of 0 =    St(G),   one can estimate

the entire set of covariance parameters using a standard nonlinear joint

generalized least squares procedure applied to (22). As noted in the

previous section, conditions (25) and (26) imply that residuals can be used in place

of the true disturbances as dependent variables in the estimation of a

model like (22) without any need for adjusting the output reported by the

computer package; all reported standard errors and test statistics are

asymptotically valid.
                                      42

          This, then, provides a simple method for estimating the para-

meters of any covariance matrix which must satisfy an arbitrary set of

nonlinear constraints. This procedure may be classified as limited inf or—

mation in the sense that it does not simultaneously estimate the set of

structural coefficients. One requires consistent estimates of these

coefficients to compute residuals, but these can be obtained from the methods

described above for estimating structural coefficients. The fact that one

is not required to estimate all parameters simultaneously is an attractive

feature of this procedure since it means that a researcher can concentrate

on correctly specifying and estimating the error structure for any

given   specification of DSEM.
          Thus,   while the above methods for estimating structural coeff i—
dents permit ohe to neglect specifying the precise form of the covariance
structure,   this covariance estimation procedure allows one to   ignore the
specification     of the relationships between measured variables once it

has been chosen. This, of course, does not mean that the results of, one
procedure when used as input for the other will not lead to a different set

of parameter estimates. Using, for example, estimates from the second

procedure to construct a consistent estimate of S needed in the first

will in general produce different estimates of the structural coefficients;

and, in turn, using these estimates to form new residuals is likely to

imply different estimates for the covariance parameters. In theory,

however, these differences should not be statistically significant, and

there should be no changes in inferences as a result.

           The two limited information procedures outlined above can be

combined into one that simultaneously estimates both structural coefficients

and covariance parameters. Stacking the system of equations given by (8)

on top of those given by (22) yields
                                            43

                  F?.               'YY .+BX
                  —11                   2' -i
                             =                       -I-a.               i=l...,N
              st(U1U)                   St(O)

 (27)

                                    H            i=j
               E(a.a)        =

                                    0            otherwise


where a = (U!, C) is a disturbance vector. Substituting St(U.u!) for

 St (U1Ji) (i.e., residuals for disturbances), it is possible to estimate

this expanded model by three stage least squares techniques that permit

the imposition of nonlinear constraints on parameters. In those cases

where one is not considering a simultaneous specification of the DSEM

(i.e., £    =I   and j' =   0),   joint generalized least squares procedures can be

applied instead to estimate (27). In Appendix B, it          is   shown once again
that    the treating of residuals as if they were the disturbances is
appropriate in the sense that all the output reported by the standard estimation
procedures   applied to (27) is asymptotically valid.

           Simultaneously estimating structural coeffièients and covariance

parameters yields estimates that are more efficient than those obtained

from the above limited information methods. There are two sources of

increased efficiency. First, in those instances where third moments of

  are nonzero (which implies that E(UiC) # 0 so Ii           E(a.a) is not block

diagonal), the estimates based on (27) will be more efficient than those

obtained from the above procedures for the sane reason that generalized

least squares estimates are more efficient than ordinary least squares

estimates. The second source of efficiency gain arises if there are any

constraints involving both structural coefficients and covariance parameters;

it is possible to impose these restrictions when estimating (27).
                                   44

         An important assumption maintained in the above discussion of

"least squares methods" is that all structural coefficients are identif led

without the use of any covariance restrictions. If this assumption is not

true, it is obviously not possible to estimate structural coefficients using

the first procedure, and without these estimates one cannot compute residuals

to serve as dependent variables for the second and third procedures. In those

cases where covariance restrictions are required for identification, one must

work instead with the reduced form specification of the DSEM given by Q8)

and apply the above procedures to estimate its parameters. The third full

information procedure then can be used to obtain a complete set of parameter

estimates.


Quasi—Maximum Likelihood Methods

        The technique usually applied to estimate models where one is

interested in estimating parameters of a covariance matrix is the method

of maximum likelihood. Typically, a researcher assumes that disturbances

are normally distributed and, then, computes estimates by maximizing the

kernel of a multivariate normal density function. Such a procedure is

computationally efficient, and it can incorporate nonlinear constraints

involving both regression coefficients and covariance parameters. Below

we consider the application of these techniques to estimate structural

and covariance parameters.

         The reduced form specification of the DSEM given by (18) provides

the appropriate formulation for application of maximum likelihood methods.

As outlined in the discussion following (18), the DSEM implies restrictions

on both the elements of the matrix of regression coefficients, 11, and the

parameters of the covariance matrix .   Denote these restricions by
                                             45

writing 11 and 12 as functions of the form 11(y) and 12(w) where y and to are

vectors of parameters.1

         The maximum likelihood estimates of these parameters are defined

as those values of y and w that maximize the function


                                         N
(28)             12(w), Y, X) =          E
                                        1=1

                                =
                                    I          (-in     12(wfl -
                                                                   (Y1
                                                                         —   fl(y)X.)'(w)(Y.   -
                                                                                                   ll(y)x)

                                =   — in      12(w)l -       E     (Y. -     n(y)Xj'c2(w)(Y.       fl(x)x.)


                                =   - in      j12(w)J -   tr[121(w)S(fi(y))
where


        S(n(y)) =              -                      - E(y)X.)'

                                                                        N
                 =
                     (    E
                         iti
                                    —
                                              i1
                                                  E   Y1X)fl(y — neY)(I z x.Y!)
                                                                                 i=l
                                                                                  N
                                                                   +              z
                                                                       fl(y)(4   i= 1



and q. is the function of y, w, '1. and X. defined by the second expression

for       Under the assumption that reduced fort' disturbances are normally

distributed, it is well known that maximum likelihood estimates of the

parameter vector cx'      (y', w) in large samples are approximately distributed

according to a normal distribution of the form

       1The following analysis does not rule out the possibility that y and
w contain conon elements.
         2The reader will inniediately recognize that   is proportional to the
kernel of a multivariate normal density function. N' then, is the distance
function one would use to obtain maximum likelihood estimates assuming that
the reduced form disturbance vectors are identically and independently
distributed according to a multivariate normal distribution.
                                     46

(29)       a 't           .i.fl)
                  N(a0,

                                                             a
where a denotes the true value of the parameters and C                 is an
       0                                                 1   Ba3a
                                                                 a
estimate of the matrix of second partials which is also known as the

information matrix. All maximum likelihood computer packages report standard

errors and test statistics based on (29), and it is these results that many

researchers use in their empirical analysis.

            An apparent disadvantage of this method of estimation composed with

the least squares methods described above is that it relies on specific

distributional assumptions. The assumption that reduced form disturbances
are normally distributed, however, is not needed in order to use the

estimates produced by maximum likelhood procedures to make statistical

inferences. Below we briefly describe the properties of these "quasi—

maximum likelihood estimates" in absence of the normality assumption

and indicate how the output reported by standard maximum likelihood

computer packages must be modified to avoid specific distributional

assumptions.' The proofs of the results summarized below are contained

in MaCurdy (l9BOa), and the reader should consult this reference for

further details.

            Under fairly weak conditions, it is possible to show that the

estimate of ci obtained by maximizing N defined by (28) is consistent

and asymptotically normally distributed even if reduced form disturbances

are not distributed according to a multivariate normal distribution. In

particular, it can be shown that the "quasi—maximum likelihood estimate"

of a in large samples approximately possesses a normal distribution of the

form


         'In the absence of the normality assumption, estimates obtained by
maximizing N defined by (28) are commonly called "quasi—maximum likelihood
estimates" (see Malinvaud, 1966).
                                         47



 (30)   ci
             N(ci0,   4 ç' c2    C;')


               N q. q,                                              1
where C2 =     L                    is the matrix of outer partials.    This result
                             a
              1=1       ci



depends on exactly the same assumptions as those required to prove consistency

and asymptotic normality of the estimators based on least squares methods

described above.

         According to the above results, a researcher may make incorrect

inferences if he uses output from a standard maximum likelihood computer

program and disturbances are in fact not normally distributed. Many of

the reported standard errors and test statistics are invalid without

normality.2 The correct asymptotic distribution is given by (30). If

the reduced form disturbance vectors are distributed according to a

multivariate normal distribution, then C2 =           in the probability limit
                                                 C1
and the covariance matrix          C2 C1' reduces to Cj which is the one

reported and used by most computer packages. Using instead cj C2 C11 —-

which is readily computable —— as the covariance matrix of a avoids the

requirement of any special distributional assumptions.

         There also exists quasi—maximum likelihood techniques that can be

applied to estimate subsets of parameters analogous to the limited informa-

tion least squares methods proposed above. These estimation procedures are

         1In addition to the assumptions implicit in the specification of the
reduced form given by(18), two conditions are required to prove this result:
(1) the absolute moments of the disturbance vector exists for any order greater

than fourth; and (2) pun          L X.X! exists and is positive definite. See
                                        1 1
                                 1=1
)laCurdy (1980a) for details.

          2The standard errors and test statistics associated with regression
 coefficients remain valid without the normality assumption. The standard
 errors and test statistics associated with covariance parameters, however,
 are all invalid. See MaCurdy (1960a) for details.
                                        48
particularly useful if the aim of an analysis is to estimate only covariance

parameters (i.e., only the elements of 2 or w) or only regression coefficients

(i.e., only the elements of TI or y).

            To estimate only covariance paramerers, it can be shown that

evaluating the distance function             defined by (28) at a consistent estimate

of TI and maximizing the resulting function with respect to w yields estimates

of the covariance parameters that are consistent and asymptotically normally

distributed. Evaluating the function N at TI, which

Is      consistent estimate for the true value of the regression coefficients,1

creates a new function          that looks like N except that the matrix S
                                                                         I.

— 1 V.V is replaced by the matrix —                V V where V. = '1.—
N.1 ii                                       N.1    ii          1  1
                                                                         TIX
                                                                               i is a
vector of reduced form residuals. The function Q may be interpreted as a

"likehood function" that treats residuals as if they were the true values

of the disturbances. Given the same conditions assumed for each method

of estimatiob proposed above, one can prove that the estimates of w obtained

by maximizing Q in large samples are approximately distributed according

to a normal probability law of the form


                                                     aq
                 r     1 -   32Q    _ll N
                                                             - 2Q —9
(3l     w '
                NW0,
                             awa'
                                               E; iJ;                :J
where       is the true value of w, and qt is the function q. defined by (28)

evaluated at II.       Notice that the estimate of       derived from this computa—

tionally simpler procedure has the same asymptotic distribution as the full

Information quasi—maximum likelihood estimate for u proposed above, Thus,

there is no efficiency gain in simultaneously estimating either TI and cs or y and


            1By "consistent" I mean that convergence in probability is o (Nh)
                                                                        p
            1
where h <
                                        49

tii   when using quasi—maximum likelihood methods. These results are important

because they imply that a researcher can use maximum likelihood computer

routines to compute estimates of covariance parameters using only the matrix
of the outer product of residuals as input. Modifying the standard error

and test statistic output reported by this routine along the lines

described above avoids the requirement that disturbances are normally

distributed.

             A quasi—maximum likelihood procedure analogous to the one

described above for estimating covariance parameters also exists for only

estimating regression coefficients. Evaluating the function N at a

consistent estimate of ,     rather    than TI, and maximizing the resulting

function with respect to -r   yields    estimates of the regression coefficients

that are consistent and asymptotically normally distributed, This proposi—

tion is obvious once one recognizes that this is completely equivalent to

a joint generalized least squares procedure. It is well known that the

estimates produced by such a procedure have the same asymptotic properties

as the full information estimates proposed above where one simultaneously

estimates all parameters.

             Quasi—maximum likelihood techniques, then, offer an attractive

method of estimating the parameters of the OSEM and the error processes

considered in Section I. They are not only competitive with the above

least squares methods in terms of their computational efficiency, they

are also as robust in the sense that they rely on the same assumptions

as the least squares methods to product consistent parameter estimates

and to test hypotheses. For the nonsimultaneous specification of the DSEM,

the computationally simpler procedures that condition on consistent estimates
                                   50

of subsets of parameters can be used without loss of estimation efficiency.

For the simultaneous specification of the model, one can apply the full

information method in which one simultaneously estimates all parameters and

imposes all constraints. This full information method not only allows one

to estimate all structural coefficients directly, it also produces more

efficient estimates if there are constraints involving both regres-

sion coefficients and elements of the reduced form covariartce matrix.



                              IV, Summary


        This paper presents specifications of a dynamic simultaneous

equations model that can be applied to analyze panel data. This model

allows for generally specified error structures and rational distributed

lag relationships involving both endogenous and exogenous variables. One

has a 'wider choice of specifications in the analysis of panel data than

in standard time series analysis: one can permit parameters to vary freely

over time in a panel data setting; permanent components can be combined

with multiple time series error processes; and it is possible to relax

many stationarity and homoscedasticity assumptions maintained in time

series analysis. To derive explicit parameterizations for the covariance

matrix associated with disturbances, this study presents a general treat-

ment for initial conditions in a panel data framework.

        For purposes of data analysis, simple procedures for estimating the

covariogram and the partial correlation function are developed. These pro-

cedures use residuals as dependent variables in a seemingly unrelated regres-

sion framework. It is shown that the estimates based on residuals have the

same asymptotic properties as estimates based on the true disturbances.

Thus, using standard computer packages, it is possible to narrow the choice

of time series models and test among competing specifications.
                                   51



        General nonlinear estimation procedures are formulated to

estimate the full set of parameters of the dynamic simultaneous

equations model and the error process, Both "least squares" and

"quasi—maximum likelihood" methods of estimation are discussed.

These procedures permit any form of nonlinear relationship between

parameters in a simultaneous equation model, including restrictions

involving both regression coefficients and parameters of the covariance

matrix. Simple limited information estimators are proposed to estimate

only regression coefficients or only parameters of the covariance

matrix. All of these estimation methods generate estimators that are

consistent and asymptotically normally distributed without any specific

assumptions regarding the distribution of the disturbances.
                                        52




                                    APPENDIX A


            The purpose of this appendix is to verify propositions (25)

and (26).

            The disturbance associated with the stacked specification of

the DSEN given by (8) can be written as


                                                Y.
                                                 ii

                            U. =   (r,
                                    - —,
                                       - —B)     1 .
                                                  2i   ER Z.
                                                               1
                             1

                                                 X.
                                                  1


where This defined as a matrix of coefficients and Z. is defined as the
                                                    1
vector of.observed variables for individual i. Let U. =
                                                    1                  it   1. denote the
                                                                             1

vector of residuals for individual i where R is a consistent estimator

for R so that R — R is o(Nk) for k c            (i.e., it— R is o(Nk) if

plim(Nk(R —    R)) = 0).    Defining vec(') as an operator that stacks the

columns of a matrix into a column vector, it can be shown that vec(P C Q) =

Q'®         vec(C). Thus,


                             vec(U.U) =
                                   11        vec(RZ.Z
                                                   11   R')


                                             (R®R) vec(Z.Z!)


                                        E H   vec(Z.Z)
                                                   11

where the matrix H is defined as the Kronecker product between R and it.

The analogous expression for the residuals is vec(UT3!) = H vec(ZZj)

where H =    (R®R).    Since R —    it is o(Nk) for k <       4-, it   is easy to verify

that H — H is
                                                53

          Proposition (23) follows from the observation that


(A.l)    Plim{1 E (vee(U1U) -



               =   plimj        (Ii —   H)
                                              :ui}
                                              E vec(Z.V)
                                              1=1



                   plim          (B -
                                                          E vec(Z.Z)J
                                         R)j iimJi
                   0 •                                      0
                                             vec(Zic)} =
                             Plim{
                                                                       1
where the last line uses the fact that H — B is a (N'42) for k c                     which
                          k
implies VN (H — H) is a (N ),and the assumption that fourth

moments of observed variables exist which implies plim(- E z.z.) <
                                                                           N

                                                                           i=l   -
                                                                                       .
          Noting that vec(vec(U.U!)vec(U.U!)') vec(B vec(Z.Z)vec(Z.Zfl'H')
=   (H&JH) vec(vee(Z1Z)vec(Z.Z)'), proposition (26) follows from the
                         -

observation that


(A.2)    plim{ E [vee(vee(UU) vec(U.UT)                              vec(vee(U1V1)vec(U.U) ') ) }



              = plim{i [(H®H) — (BØH)]                          vec(vec(Z.Z!)vec(zz!)'))
                                                                                 ii
                                                          i=l

              =0
                                                      A                                3.
where the last line uses the fact that ((H GB) —                    (BQH)) is o(N2')
and fourth moments of observed variables exist.

          In Appendix B we require a generalization of (A.l). Let B. i —                     1,..
denote a set of matrices satisfying the condition
                                              54


                   INE B1 vec(Z1Z). .c .
(A.3)       plim
                   I.   1=1               J




We   have

             N                                            N
                  B.(vec(U.1J) —         vec(U.U))   =         BJH — H)vee(Z.Z)
                   1           11            11           LI
                                                                1          11
            1=1                                          i=1


                                                          N
                                                     =    LI   vec(BJH — H)vec(Z.Z.))
                                                         1=1




                                                         [l
                                                     =          (vec(Z.Z)' ØB.)jvec(H - H)



where the last line uses the matrix algebra theorem stated above. Thus,



(A.4)
                   IN
                    -          B.(vec(U.U) —   vec(UU))
            plim          LI
                                1       11         11

                   = plim           E (vec(Z.Z)'                         vec(
                                                                plim {

                   =0

where the last line follows from (A.3) and the fact that (H — H) is 001
                                        55



                                  APPENDIX B


        The purpose of this appendix is to prove that when residuals are

used in place of true disturbances to estimate models (22), (24), or (27),

the output reported by a standard application of estimation procedures based

on "least squares methods" is asymptotically valid. To simplify the exposi-

tion, proofs are only given for the case in which there are no constraints

on parameters. Proofs of the following propositions when constraints are

present (including nonlinear constraints) involve more algebra, but they are

conceptually equivalent to those presented below.

        Consider first the use of residuals in estimating model (22) and the

parameters of the covariance matrix. Since the seemingly unrelated regression

models proposed for estimating the covariogran given by (21) are nested in (22),

the following results apply to these models as well. Replacing disturbances

by residuals in (22) yields


(B.l)   St(U.U!) = a +    + (St(UU) — St(U.TJ!))

                                  T          i=j
                              =

                                  0          i#j

where the .'s are independently distributed error vectors.

        Estimating (B.l) by a joint generalized least squares computer program

yields an estimate of U equal to


(B.?)   6GLS = ULS =          st(uiU)
where we have used the fact that the generalized and the ordinary least squares

estimators are equivalent since all equations contain the same exogenous variables.
                                        56

This procedure prints standard errors assuming the covariance matrix of

0GLS is


                            1
                                N                       -

(8.3)     V(OCLS) =     T       E (St(U.U) — e5)(St(UjIfl)                —


and, it reports test statistics assuming that 6GLS is approximately normally
distributed with mean B and covariance matrix V(@GLS) or, equivalently,


(8.4)            N(B,
          BGLS


           Using (Li), (A.2) and (8.1), standard applications of asymptotic

theory yields


          plim{T} = T


and


          dlim {v (BGLS
                                = dim                   .    +    E     (St(U.U) -
                                                                                        St(U.U))}
                                          I      N                    1N
                                = dim    J       E      Ci   + plim J     Z
                                                                                   ii
                                                                              (StOJ lID —   St(U.IJ.)')
                                                                                                11
                                          {61i:ri       'J
                                = N(O,   T)


where dlim denotes convergence in distribution as opposed to pun which is

convergence in probability. So, in large samples, we have


                                        N(e, 4   'r).

This result verifies that the estimator          given       by (3,2) is consistent and

the reported output of the generalized least squares procedure given by (8,3)

and (8.4) is valid asymptotically.
                                            57



        Consider next the use of residuals to estimate the seemingly unrelated

regression model given by (24) proposed for estimating the partial correlation

function. A convenient representation of (24) is




(B.5)   (IGIJ)J =        (I®U)Q      +

                          R        ij
             E(e.e) =

                          0        i#j

where I is the identity matrix, J is a vector and Q is a matrix of known

constants, p   is!   a vector of parameters, and e. is an independently distrib-

uted error vector. With the appropriate choice of J and Q and dimensioning

of I, p contains the          partial correlation coefficients for a prespecified

order. Suppose, for example, we are interested in estimating the kth order

partial correlation coefficient. To do this, we set the dimension of I equal

to (T—k) define 5 so that (I(Tk)                     =    (U.(T),..1,U.(k+l))';   and,

treat Q as a block matrix of the form Q              dia(Q1,.. 'T—k where the T x k

matrices Q., j       l,...,T—k, are defined so that UQ.              (U(T—j),...UJT—j—k+l)).

The implied parameter vector for this specification is p' =

where    =
             lt'- 'kt' =            k+l,.   .   - ,T, with kt being interpreted as the
kth order partial correlation coefficients associated with the period t.

As discussed in the text, constraining                   to be constant for all t   produces
a unique estimate for the kth order partial correlation coefficient, but such

constraints are not explicitly considered here.

        Using residuals to estimate model (B.5), the estimator for R and the

generalized least squares estimator for p are
                                       58




                =   E   (I®U1)(J +                      +



           CLS =
                            (I®U.)R(I®Uj)]Q1                       IN   (IØui;(IØUj)}
where

                =
           LS
                    ['®! UiU}QJ                 [i®

Using   (aLl), we see that        is consistent for p. The properties of R and
                                            N
CLS depend on matrices of the form              (I®U.)C(I®U!) where C is a
                                         i=l



quadratic form is
                        -
                        c
                               N
consistent estimate of some positive definite matrix C. The (g, h) block of this

                               E UU where C h
                                                A
                                                            is the (g, ii) element of C.
                            h
                              i=l
Since Plim(Cgh) =           we know from (A.l) that this (g, h) block divided by
                    Cgh
N or V has the same asymptotic properties as the (g, h) block of
 N
 E (I ®U.)C(I®U) divided by the sane normalizing factor. It directly
i=l                                                 A          A          rN         -

follows,   then, that the quantities of R, GLS' LS' and Q'I                 E
                                                                                 (I®U.)R
                                                                          [i=l
I®UjQ have the same asymptotic properties as the analogous quantities

computed using the true values of the disturbances. Thus, standard appli-

cations of asymptotic theory yield the conclusion


(B.6)
         CLS N[ F[1 (IUi)R1(I®Uj)]Q1].
 The result given by (B.6) is exactly the one assumed by a joint generalized

least squares computer package when it reports output.

           Finally, consider the use of residuals in estimating model (27)

which includes both structural coefficients and covariance parameters.

Replacing disturbances by residuals in (27), one may write this system of

equations as
                                                            59


                                            0           y                                       0
             XlI                  Zi

(B.7)                       =                                    +a.+
         St(U.1J)
             11                   0         I           e                    (St(U.U) -
                                                                                         11         St(U.U))
                                                                                                        11
                                                ij
              E(a.a)
                1J
                            =

                                  0             ij
                                                                                                         is a matrix
where Yj. =
               (YJT)..                ,Yj.(l)), y is a parameter vector1

containing both endogenous and exogenous variables (i.e. the elements of

Y1±, Y2±, and X.), and the disturbance vectors a1 are independently distributed.
                                                    -
                                                                             L       0
                                                                                 1
                                                                                              and         (i', 6')
         Defining I             = (!I' St(0.U1Y)
                                           1
                                                                      WI =
                                                                             0       I
                                                                                         ,          6'



a three stage least squares procedure applied to (B,7) yields an estimate of
6 equal to

                       N-
                  =          Wi                                  W.          a.
         3LS                                    1                 1           1
                      1=1


where W. is the niatrix WI with all endogenous variables replaced by their

predicted values,

                                        —                        —
                      N .E1                 W62LS)(aI                 W.6zs)'
and
                        N               -l N
        6         =     E       W'W.             E W'.ct..
            2LS                                 i=l
                       1=1


This procedure prints standard errors and test statistics assuming that



(B.8)   63LS '                                      j-l
                      N[65                                   c]l]
                                                                 60

              To determine the asymptotic properties of                                              and 63Ls' observe

that using (B.7)

                                            N           N
              62LS — 6)        =   N1       L Wa + 1(1 Z B (St(TJ ii
                                                                  U )                                  St(Uii
                                                                                                            U'))
                                        1=111 11=1 ii

and


                                            N
              63LS — 6) = N1                E
                                                          1   2 1=1 2i    ii — St(U.U))
                                                    WH a. + M E B .(St(U.U'.)      11
                                                    N
                           2                         1
                                        1=1


where M =
          1      .
                     Z
                          ii
                         WW., N =
                                   2            .
                                                    E       W'.N1 w.' B
                                                             1        1       ii
                                                                               .
                                                                                   = W'.
                                                                                       1
                                                                                               ,
                                                                                                        2i
                                                                                                               l0
                                                                                                   and B . = W!H
                                                                                                                1
                i=1                             1=1                                        I

Assuming the existence of fourth moments, it can be shown that                                                 the   matrices


B1. satisfy
                     condition (A.3) of Appendix A and 0 <
                                                                                               p1im{   N1) <   .     Thus,


PlimfS2LS) = 6 and plim{H} = H.                                       Using standard stochastic limit theorems

 (e.g.,   Theorem 2 of Nann—Wald), it can be further shown that the matrices
B2. also satisfy condition (A.3) and that P = Plim{i N2) exists and is
positive definite. Therefore,

              dlIm{61(63LS             6))


(B.9)                =   P dlim I                       Z    WH1
                                                              1
                                                                 a +      i        P' plim             E B2i.stu.u:
                                                                                                                 ii
                                                                                                                             -   St(U.U)
                                                                                                                                     11

                     =   p1   dlim 1
                                        (
                                            . EN 1                    i

                         N(0, P1)



where the last lines use asymptotic results typically applied in deriving

the properties of three stage least squares estimators. Since the matrix
                                 61




   L        W is a consistent estimate for P, (B9) implies that (B.S)is valid
N.i=1   i    1

as an approximation in large samples; and, so, the output reported by

standard simultaneous equation procedures is valid asymptotically.
                                     62




                                 REFERENCES


Anderson, T. W. The Statistical Analysis of Time Series. New York:
        John Wiley & Sons, 1971.

Anderson, T. W. and Hsiao, C. ttpormulation and Estimation of Dynamic
        Models Using Panel Data." Journal of Econometrics, forthcoming, 1981.

Ashenfelter, 0. "Estimating the Effects of Training Programs on Earnings."
        Review of Economies and Statistics 40, no. 1 (February 1978): 47—57.

Balestra, P. and Nerlove, N. "Pooling Cross Section and Time Series Data
        in the Estimation of a Dynamic Model." Econometrim34 (July 1966):
           585—612.

David, N. "Lifetime Income Variability and Income Profiles." Proceedings
        of the Annual Meeting of the American Statistical Association,
        August, 1971, pp. 285—92.

Friedman, N. and Kuznets, S. Income from Independent Professional Practice.
        New York; National Bureau of Economic Research, 1945.

Granger, C.W.J. and Newbold, P. Forecasting Economic Time Series. New
        York: Academic Press, 1977.

Griliches,Z. "Distributed Lags: A Survey." Econometrica (1967): 16—49.

Hannan, E. Multiple Time Series. New York: John Wiley & Sons, 1970.

__________   "The Identification of Vector Auto—regressive—Moving Average
        Systems." Biometrika (1969): 223—25.

Hause, J. "The Covariance Structure of Earnings and the On—the—Job
        Training Hypothesis." Annals of Economic and Social Measurement
        (Fall 1977): 335—66.

 Hussian, A. and Wallace, T. "The Use of Error Components in Combining
         Cross Section with Time Series Data." Econometrica 37 (1969):
           52—72.

Kashyap,     . and Nasburg,  .  "Parameter Estimation in Multivariate
           Stochastic Difference Equations.t' IEEE Transactions on Automatic
           Control AC—l9 (1974): 784—97.

Lillard, L. and Weiss, Y. "Components of Variation in Panel Earnings Data:
         American Scientists 1960—1970." Econometrica (1979); 437—54.

 Lillard, L. and Willis, IL "Dynamic Aspects of Earnings Mobility."
         Econometrica (1978): 985—1012.
