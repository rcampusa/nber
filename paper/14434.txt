                                 NBER WORKING PAPER SERIES




                    IDENTIFICATION WITH IMPERFECT INSTRUMENTS

                                              Aviv Nevo
                                            Adam M. Rosen

                                         Working Paper 14434
                                 http://www.nber.org/papers/w14434


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                     October 2008




We thank Joel Horowitz, Chuck Manski, Rob Porter, and Elie Tamer as well as seminar participants
at Northwestern University, University College London, SITE, University of Paris I, and the University
of Toronto for comments. Adam Rosen gratefully acknowledges financial support form the Center
for the Study of Industrial Organization, the Eisner Memorial Fellowship at Northwestern University,
and the Economic and Social Research Council through the ESRC Centre for Microdata Methods
and Practice grant RES-589-28-0001. The authors are solely responsible for any and all errors. The
views expressed herein are those of the author(s) and do not necessarily reflect the views of the National
Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2008 by Aviv Nevo and Adam M. Rosen. All rights reserved. Short sections of text, not to exceed
two paragraphs, may be quoted without explicit permission provided that full credit, including © notice,
is given to the source.
Identification with Imperfect Instruments
Aviv Nevo and Adam M. Rosen
NBER Working Paper No. 14434
October 2008
JEL No. C30,C31,C33

                                             ABSTRACT

Dealing with endogenous regressors is a central challenge of applied research. The standard solution
is to use instrumental variables that are assumed to be uncorrelated with unobservables. We instead
assume (i) the correlation between the instrument and the error term has the same sign as the correlation
between the endogenous regressor and the error term, and (ii) that the instrument is less correlated
with the error term than is the endogenous regressor. Using these assumptions, we derive analytic
bounds for the parameters. We demonstrate the method in two applications.


Aviv Nevo
Department of Economics
Northwestern University
2001 Sheridan Road
Evanston, IL 60208-2600
and NBER
nevo@northwestern.edu

Adam M. Rosen
Department of Economics
Gower Street
London WC1E 6BT
adam.rosen@ucl.ac.uk
                      Identi…cation with Imperfect Instruments

                                Aviv Nevoy                                    Adam M. Rosenz
               Northwestern University and NBER                        UCL, IFS, and CEMMAP

                                                    July 16, 2008



                                                       Abstract

          Dealing with endogenous regressors is a central challenge of applied research. The standard solution

      is to use instrumental variables that are assumed to be uncorrelated with unobservables. We instead

      assume (i) the correlation between the instrument and the error term has the same sign as the correlation

      between the endogenous regressor and the error term, and (ii) that the instrument is less correlated with

      the error term than is the endogenous regressor. Using these assumptions, we derive analytic bounds for

      the parameters. We demonstrate the method in two applications.




1     Introduction

Dealing with endogeneity is one of the central issues in non-experimental studies. A common method for

dealing with potential endogeneity issues in econometric models is to use an instrumental variable (IV). In

linear models an IV has to be correlated with the endogenous covariate and uncorrelated with the econometric

unobservable. The former condition is known as relevance, or strength of the IV, and the latter as exogeneity,

or validity.

    The exogeneity assumption cannot be tested in many cases and therefore the validity of the identifying

restrictions is based on the subjective judgement of the researcher. Unfortunately, even when great care is
     We thank Joel Horowitz, Chuck Manski, Rob Porter, and Elie Tamer as well as seminar participants at Northwestern
University, University College London, SITE, University of Paris I, and the University of Toronto for comments. Adam Rosen
gratefully acknowledges …nancial support form the Center for the Study of Industrial Organization, the Eisner Memorial
Fellowship at Northwestern University, and the Economic and Social Research Council through the ESRC Centre for Microdata
Methods and Practice grant RES-589-28-0001. The authors are solely responsible for any and all errors.
   y Address: Aviv Nevo, Department of Economics, Northwestern University, 2001 Sheridan Road, Evanston, IL 60208-2600,

nevo@northwestern.edu.
   z Address: Adam Rosen, Department of Economics, University College London, Gower Street, London WC1E 6BT,

adam.rosen@ucl.ac.uk.



                                                            1
taken, the validity of the instruments chosen is often a matter of faith. Empirical …ndings are often called into

question as a result of debate over the IV assumptions. Furthermore, the search for exogenous IVs sometimes

leads the researcher towards IVs that are weak, i.e., are only weakly correlated with the endogenous variable.

   In this paper we examine identi…cation when we replace the standard exogeneity assumption with a

weaker inequality. We set up the model in general, but in order to obtain analytic results we focus on a

linear equation with one endogenous regressor, when the researcher has at least one imperfect instrumental

variable (IIV). The IIV is allowed to be correlated with the error term. We show that without further

assumptions this variable does not identify even the direction of the bias of the least squares estimator.

   We assume that the sign of the correlation between the IIV and the error term is the same as the

correlation between the endogenous variable and the error term. We show how this assumption can be used

to partially identify the parameter of interest. In particular, we show that if the IIV and the endogenous

variable are negatively correlated, the parameter of interest can be bounded both from above and below.

We add the assumption that the correlation coe¢ cient between the IIV and the error term is less than the

correlation coe¢ cient between the endogenous variable and the error term. That is, the IIV is thought to

be correlated with the unobservable in the equation of interest, but less so than the endogenous regressor.

Using this assumption we improve the bounds. Finally, we discuss the case where several IIVs are available.

Of particular interest is the case where none of these variables provide useful bounds on their own. We show

how di¤erencing the variables to create an IIV can generate meaningful bounds.

   A strength of our approach is its simplicity. By focusing on the linear model we are able to analytically

characterize the identi…ed set. Furthermore, the bounds we compute are well known estimators that are

computed by standard econometric packages. Thus, the estimation and inference are simple to do, and

require little additional programming. Indeed, many of the numbers we need to compute our bounds are

available in many published papers.

   We apply our estimator to two common examples in the Industrial Organization literature: production

function estimation and demand estimation. While our motivation lies in applications such as these, the

method is more generally applicable in practically all …elds of economics. The applications demonstrate that,

at least in the cases we examine, our approach can generate useful bounds on the parameters of interest.


1.1    Related Literature

Three related papers that also consider inference when instrument exogeneity fails are Conley, Hansen, and

Rossi (2006), Hahn and Hausman (2003a), and Manski and Pepper (2000).              Like our own work Conley,


                                                       2
Hansen, and Rossi (2006) consider a linear model with endogenous regressors and invalid instruments.

However, they take an alternative approach to ours by parameterizing the degree of instrument endogeneity

by its associated coe¢ cient, denoted     , in a linear regression of the outcome variable on regressors and

instruments jointly.     They …rst investigate the degree to which knowledge of only the support of           can

be used to infer the parameter of interest     .   They then consider the added bene…t of specifying a prior

distribution for , and the implications of imposing a prior distribution over all model parameters jointly.

This provides useful (set) identi…cation results, inferential procedures, and methods of sensitivity analysis,

though for a di¤erent set of modeling assumptions than those considered here.           Rather than derive the

identi…ed set for model parameters in the presence of imperfect instruments, Hahn and Hausman (2003a)

compare the mean-squared-error and bias of the OLS and 2SLS estimators when the instrument exogeneity

assumption fails.      Manski and Pepper (2000) characterize the identi…cation region for model parameters

when, instead of the usual exogeneity condition, the expectation of the outcome variable conditional on the

instrument is assumed to be monotone for any given value of the endogenous covariate. Unlike our analysis,

their analysis applies to nonparametric models, but focuses on a bounded dependent variable. When a linear

parametric model is imposed, as done in Manski and Pepper (1998), the Manski and Pepper assumptions

neither nest nor are nested by ours. The Manski and Pepper monotone IV assumption implies monotonicity

of the mean of the unobservable conditional on the instrument, whereas we employ weaker restrictions on

correlations. However, in addition to this we also impose restrictions on the relative correlations of exogenous

regressors with errors and instruments with errors, and there are no such restrictions imposed by Manski

and Pepper. A bene…t of the assumptions imposed in this paper, at least in the context of linear models,

are analytic results on the bounds of the parameter of interest, which are easy to compute with standard

regression software.

   Related papers that bound parameters of a linear regression include those of Frisch (1934), Leamer (1981),

Klepper and Leamer (1984), and Bontemps, Magnac, and Maurin (2006). Frisch (1934) develops bounds

for the slope parameter in a simple linear regression model with measurement error. Klepper and Leamer

(1984) generalize this result to multivariate regression with errors in all variables, deriving bounds on the true

regression coe¢ cient vector. Leamer (1981) considers an under-identi…ed simultaneous equations model of

supply and demand without instruments to overcome the simultaneity problem. He shows that knowledge

of the signs of regression parameters, e.g. downward sloping demand, upward sloping supply, can be used

to bound the slope of either demand or supply via directed and reverse regression.          Bontemps, Magnac,

and Maurin (2006) study set identi…cation in linear models, providing an estimation procedure that relies


                                                        3
on an estimator for the identi…ed set’s support function. In addition, they generalize the Sargan test for

overidenti…cation to their setup, establishing a test of supernumerary restrictions whose intersection may in

fact be a proper set, a point, or empty. While each of these papers provides bounds on the coe¢ cients of a

linear regression model, the models considered are quite di¤erent from the one we study in this paper.

   Whereas this paper addresses the potential failing of the exogeneity condition for an instrument, the

recent literature on weak instruments and weak identi…cation confronts the possibility of the instruments

having little relevance.   This literature focusses on complications that arise when the relevance condition

is not strongly satis…ed, i.e. when the correlation between the endogenous regressor and the instruments is

small. Standard methods for inference with instrumental variables are known to perform poorly when the

instruments are weak (e.g. Rothenberg (1984), Nelson and Startz (1990), Bound, Jaeger, and Baker (1995),

and Dufour (1997)) or when the necessary rank condition fails altogether, see Phillips (1989). The literature

has thus sought to develop methods to test for weak IVs (e.g. Hahn and Hausman (2002) and Stock and

Yogo (2005)), as well devise inferential methods that are robust to weak instruments, such as Anderson and

Rubin (1949), Anderson and Rubin (1950), Staiger and Stock (1997), Wang and Zivot (1998), Zivot, Startz,

and Nelson (1998), Stock and Wright (2000), Dufour and Jasiak (2001), Chioda and Jansson (2005), Dufour

and Taamouti (2005), Guggenberger and Smith (2005), Kleibergen (2005), Andrews, Moreira, and Stock

(2006), and Andrews and Marmer (2008). An extensive review of this literature is well beyond the scope

of this paper, but surveys are given by Stock, Wright, and Yogo (2002), Dufour (2003), Hahn and Hausman

(2003b), and Andrews and Stock (2007).

   We focus on the possibility that the variables employed as instruments fail to satisfy the exogeneity

condition rather than the relevance condition.     Interestingly, we show that the correlation between the

instrumental variable and the endogenous regressor plays a key role, much like it does in the case of weak

IV. In our case, however, the key condition is that this correlation be negative. The larger its magnitude,

the tighter are the bounds.

   In section 2 we lay out the more general setup and develop conditions that de…ne the identi…ed set.

In section 3 we …rst focus attention on the special case of a simple linear regression with one imperfect

instrument, and characterize analytically the identi…cation region for the slope parameter      .   We then

extend the results to the multiple regression model and discuss the case of several imperfect IVs. Section

4 then discusses estimation and inference.     Section 5 provides two empirical illustrations and section 6

concludes.




                                                      4
2     The Model

2.1     The Setup

The econometrician is interested in identifying the parameters of a regression with one endogenous variable

and an arbitrary number of exogenous variables. For each observation, the outcome variable Y is a function

of an endogenous covariate X, a 1        kw vector of additional covariates W , and U , an additively separable

mean zero error unobserved by the econometrician. We assume a parametric functional form.

Assumption AP (parametric functional form):


                                                 Y = m (X; W; ) + U ,


where    is a vector of parameters and m (X; W; ) is twice continuously di¤erentiable in .

    For some results, we focus on the case where the function m ( ; ; ) is linear in parameters.
                                                 0 0
Assumption AL (linear model):           =    ;         and


                                                 Y = X + W + U:


We further assume that there exists an observable vector of random variables Z which has dimension kz .

These will be imperfect instruments in the sense de…ned in our assumptions A3 and A4 below. In addition,

we assume the existence of a 1       kw vector of valid instruments Z w , which may include elements of W if

some of the regressors are themselves exogenous. If the number of valid instruments exceeded the dimension

of W , then model parameters would be point-identi…ed under the usual rank condition for Z w0 (X; W ), and

model parameters could be consistently estimated by standard instrumental variable methods such as GMM.

For this reason, we restrict the number of valid instruments to be equal to the number of elements of W .

    We assume the econometrician observes a random sample of (Y; X; W; Z; Z w ) drawn from population

( ; F; P).   We use the subscript i to denote observations drawn from this population, and subscript j to

denote individual elements of these vectors.
                                                                 0
Assumption A1 (random sampling): (yi ; xi ; wi ; zi ; ziw ; i ) , i = 1; :::; n are iid realizations from P.

      We assume that the distribution P is such that the valid instruments Z w are exogenous, in the sense

that each component of Z w is uncorrelated with the unobservable U , but X is endogenous.

Assumption A2 (Z w exogenous, X endogenous): E (Z w0 U ) = 0, E (XU ) 6= 0.



                                                             5
    The variable X may be correlated with U . This can happen, for example, when the value of X is chosen

by an economic agent who observes U prior to choosing X. In this case, in the linear model identi…cation

and estimation of the model parameters             typically relies on the use a vector of instrumental variables that

are correlated with X but not with U .             Such IV assumptions are often called into question in empirical

work, and in some cases may be untestable. We assume instead that the econometrician has observations

of some imperfect instruments, Z that are also correlated with the unobservable U , but less so than the

endogenous regressor X, which we formalize below in assumptions A3 and A4.                              We assume that kz , the

number of imperfect instruments, is …xed.

    Formally, the assumptions we impose on our imperfect instruments (IIV) are as follows.                            We use the

notation      ab   and   ab   throughout to denote the covariance and correlation, respectively, between any two

random variables a; b.

Assumption A3 (same direction of correlation):


                                                   xu zj u     0 , j = 1; :::kz .


Assumption A4 (instruments “less endogenous” than x):


                                               j   xu j      zj u       , j = 1; :::kz .


Assumption A3 asserts that the endogenous regressor X and the IIV have the same direction of correlation

with the error term. Assumption A4 then adds the condition that the IIV be less correlated with the error

term than the endogenous regressor.           Were we to replace A3 and A4 with the stronger assumption that

 zj u   = 0, then Zj would be a valid instrument in the classical sense.

    We also make use of the standard rank conditions which are needed for the probability limits of the

standard OLS and 2SLS estimators to be well-de…ned in the linear model.
                                                                    0                                             0
Assumption A5 (rank and order): rank E (Z; Z w ) (Z; Z w )                            = kz + kw , rank E (X; Z w ) (X; Z w )   =
                                   0
kw + 1, and rank E (Z; Z w ) (X; W )          = kw + 1, kz              1.


2.2       Identi…cation

We now show how the modeling assumptions can be used to identify the parameters. The assumptions yield

a system of moment equalities and inequalities that restrict the feasible values of model parameters . In



                                                                    6
general, these restrictions will not be su¢ cient for point identi…cation, but may still embody information

regarding the value of .

     A useful starting point is to assume we know the relative correlations of the instrument Zj and regressor

X with the econometric error term. De…ne this correlation as


                                                         j       zj u = xu ,                                                         (2.1)


Assumptions A3 and A4 are equivalent to the restriction that                       j    2 [0; 1]. If   j   were known, then it could

be used to construct a weighted average of Zj and X that is uncorrelated with the error term. To see this,

de…ne the function Vj ( ) as

                                                Vj ( )          x Zj           zj X.                                                 (2.2)


By de…nition, Vj       j   is uncorrelated with U , and satis…es the moment condition E (Y                       m (X; W; )) Vj         j    =

0. Although       j   is unknown, it o¤ers a convenient parameterization of the relative correlations of                        zj    and

 x   , which will prove useful for characterization of the identi…ed set. The hope is that by varying                       j   between

0 and 1 we will be able to bound the parameters of interest.

     The implied restrictions of assumption A1 - A4 are:


                                             E Zjw0 (Y         m (X; W; )) = 0,                                                  (2.3a)

                                   E (Y     m (X; W; )) Vj               j   = 0, j = 1; :::; kz ,                               (2.3b)

                                                  j   2 [0; 1] , j = 1; :::; kz ,                                                 (2.3c)


where    j   and Vj ( ) are as de…ned in (2.1) and (2.2), respectively. Thus, the model parameters must satisfy

the kw + kz moment conditions (2.3a), (2.3b). If                     =       1;     ;    kz   were known and the standard rank

conditions were satis…ed, these moment conditions would identify                        locally under the parametric model (AP)

and globally in the linear model (AL). These conditions could then be used for consistent estimation via

GMM. However, since              is only known to belong to the unit cube in Rkz ,                         will generally not be point

identi…ed. The identi…ed set for          is the set of parameter values that satisfy these restrictions.

Proposition 1 Let (AP), A1 - A4 hold. Then                      = f : conditions (2.3a)-(2.3c) holdg

Corollary 1 Suppose that AL holds. Then                      is convex.

     The characterization of        above can be used to perform estimation and inference on the identi…ed set

                                                                 7
using a variety of methods recently considered for models comprised of moment equalities and inequalities,

see for example Chernozhukov, Hong, and Tamer (2007) and the references therein.                       For example, if the

researcher is willing to sign       xu ,   one way this can be done is by rewriting (2.3b) and (2.3c) as 2kz moment

inequalities obtained by setting            j    = 0 and      j   = 1 into (2.3b) to obtain lower and upper bounds on

E (Y    m (X; W; )) Vj          j    . This allows for estimation and inference on the entire parameter vector ,

though in some cases one may be interested in inference on the marginal e¤ect of X.



3      The Linear Model

The characterization of the identi…ed set, provided in Proposition 1, is not very informative. For example, it

does not tell us if the identi…ed set is bounded in all directions. In other words, Assumptions 3 and 4 might

not be very informative. In this section, in order to obtain an analytic characterization of the identi…ed set,

we restrict attention to the linear model. We show that in the linear model, our modeling restrictions lead

to a straightforward characterization of the identi…ed set that can be exploited to perform (set) estimation

with standard linear regression methods.                We start with the simple linear model and then generalize the

results to multiple IIVs as well as additional regressors.


3.1    The Simple Linear Model with One Imperfect Instrument


Consider the simple linear model



                                                            Y =    + X + U,                                          (3.4)


where E [U ] = 0.
                                                OLS          IV
    For notational ease, we de…ne                     and    z    to be the probability limits of the standard OLS and IV,

with Z as the instrument, estimators for , respectively. That is


                                                       OLS         xy               xu
                                                                   2
                                                                            =   +    2
                                                                                       ,                             (3.5)
                                                                   x                 x


and
                                                        IV         zy               zu
                                                        z               =       +        .                           (3.6)
                                                                  xz                xz

b OLS and ^ IV are taken to be their corresponding estimators.
            z


                                                                        8
      The true value of the parameter can be bounded when assumptions (A1)-(A3) are imposed.

                                                                                                                   OLS           IV         OLS
Lemma 1 Let (A1), (A2), and (A3) hold.                        If        xz   < 0 then               lies between         and     z    .           is the
                                         OLS
upper bound if xu > 0, while                    is the lower bound if                        xu < 0.      If instead   xz      0, then if       xu    > 0,
        n         o                                          n                                o
    min OLS ; IVz  , while if            xu    < 0,      max OLS ;                       IV
                                                                                         z     .

                                                                                                                                          OLS           IV
      Lemma 1 gives a simple characterization of bounds for                                   : Given assumptions A1-A3,                        and     z

provide either two-sided or one-sided bounds for , depending on the correlation between X and Z, which

is identi…ed and can be consistently estimated.

      If   xz   < 0 assumption A3 can provide …nite, hopefully economically helpful, bounds on the parameter

of interest. Before exploring the gains from imposing assumption A4, we ask under what conditions can the

direction of bias of the OLS estimator to be ascertained based on the IV estimator.

Lemma 2 Let A1-A3 hold. If                xz   < 0 or       xz   >           then


                                                      OLS            IV                        OLS
                                          sgn                        z        = sgn                         .


                                                                   OLS                                          OLS      IV
      When        xz   < 0 it follows that the bias of                       has the same sign as                        z    , which is identi…ed.

Since           is unknown however, the second condition generally will not be veri…able. A common intuition –

that, even if the IV is not valid, the IV estimate corrects the OLS estimate in the right direction –probably

comes from the case when               is small. Recall that                     = 0 is the valid IV case. So                  close to zero means

the IV is "almost" valid and then the conditions of the lemma are likely to hold. If the conditions of the

Lemma do not hold then an IIV will not even identify the direction of the bias in OLS.

      Up to this point we did not impose Assumption A4, and were able to get two-sided bounds only if

 xz    < 0. We now ask what we get from further imposing Assumption A4. As we said above the hope is

that by bounding              between 0 and 1 we obtain sharper identi…cation results than lemmas 1 and 2. The

following result shows exactly what we get from also imposing Assumption A4.

Proposition 2 Let A1-A4 hold. If                 xz   < 0, then

                                                            h                       i
                                                                   IV        IV
                                                                   z    ;    v(1)       if    xu    >0
                                                 B = h                              i
                                                                   IV         IV
                                                                   v(1) ;     z         if    xu    <0


where
                                                       IV                   z xy             x zy
                                                       v(1)                                           .                                               (3.7)
                                                                        x(       z x           xz )


                                                                             9
If, on the other hand,         xz   > 0, then

                                                                                                i
                                                                            IV       IV
                                                           1; min           z    ;   v(1)            if       xu   >0
                                           B = h
                                                                   IV          IV
                                                      max          v(1) ;      z     ;1             if       xu   < 0.


These bounds are sharp.

                     IV
      The bound      v(1)   has a particularly simple characterization. It is the probability limit of the traditional

IV (2SLS) estimator for             when V (1) is used as an instrument for X, where V ( ) is as de…ned in (2.2), i.e.
                              IV                                                                                        OLS
V (1)        zX      x Z.     v(1)   can also be written as a weighted average of                                               and      IV   , since


                                    IV             v(1)y            1              2 OLS                           IV
                                    v(1)   =                =                    z x                          x xz z
                                                  xv(1)            xv(1)
                                                              2
                                                            z x                OLS                            x xz              IV
                                           =                                                                                    z    .
                                                  x   (    z x          xz )                    x    (       z x         xz )



In cases where       xz   > 0, Proposition 2 implies that imposing assumption A4 only slightly improves upon the

bound of Lemma 1 (in the sense that the bound is still one sided.) However, when                                                              xz   < 0, Proposition

2 improves, in a somewhat more meaningful way, on the bounds given in Lemma 1. So while Assumption

A3 and A4 jointly bound               between zero and one, they do not in general provide two-sided bounds for the
                                                                                                             IV
parameter of interest, . To see why, note that if                              =     xz   then               v(    )   is not de…ned, so even though              is

bounded       might not be bounded. If               xz   > 0, we cannot rule out this case and                                      is not bounded from both

sides. If    xz   < 0 we can rule out            =    xz   since 0                   1, and                  is bounded from above and below.

      While Assumption A4 does not give us two-sided bounds where they did not already exist, it does help

us tighten the bounds. Corollary 2, applies to the case where                                            xz   < 0, and where Proposition 2 achieves
                                                                                                                                               IV
two-sided bounds for beta.             This corollary characterizes the degree to which use of                                                 v(1)   improves upon
                                                                            OLS                 IV
the bounds provided by Lemma 1, which are just                                       and        z        . The implication is that the greater the
                                                                                                                                          IV                    OLS
magnitude of the correlation between x and z, the tighter the bound achieved using                                                        v(1)     instead of         ,

with maximal improvement obtained when                       xz    =        1, in which case the size of the bounds is halved.


Corollary 2 If        xz    < 0, then
                                                            IV            IV
                                                            v(1)          z                 1
                                                            OLS           IV
                                                                                 =                       .
                                                                          z
                                                                                     1          xz

      Our results are consistent with the …nding of Hahn and Hausman (2003a) that if E [ZU ] 6= 0, it is possible
       OLS                                  IV
for          to be closer to        than    z    . However, Proposition 2 further shows that this is possible even if the


                                                                            10
instrument Z is less correlated with U than is the endogenous regressor X. Consider, for example, the case
                                         h           i
where xu < 0 and xz < 0, so that 2 IV      v(1) ; IV
                                                  z    , and OLS    IV
                                                                    v(1) .
                                                                           IV
                                                                           v(1) is a feasible value for , and
 IV                               OLS             IV             IV          OLS               IV         IV
 v(1)   is in fact closer to            than is   z    because   v(1)              =   xz
                                                                                       x   z   v(1)       z    . In this case, if     is
                         IV        IV                            OLS
su¢ ciently close to     v(1) ,    z    “over-corrects” for            . Thus, even if the instrument is “less endogenous”
                                    IV                                                         OLS
than X, it is possible that         z     lead the researcher further astray than                     , and it may not be the case
                                                                                            OLS
that the instrumental variables estimator o¤ers an improvement over                               .


3.2      Additional Regressors

This section generalizes beyond the simple linear model, allowing for the presence of additional regressors,

as well as valid instruments in addition to the IIV. The regression of interest is


                                                        Y = X + W + U,                                                             (3.8)


where X is univariate, W is a 1              kw vector of additional regressors (possibly including a constant), and

E [U ] = 0. Z is a univariate IIV such that satis…es assumptions A3 and A4 with respect to the endogenous

regressor X.      The additional regressors W may include exogenous and endogenous components, but we

assume that A2 holds, so that there is a 1                 kw vector of valid instruments Z w such that E [Z w0 W ] is

nonsingular and E [Z w0 U ] = 0.           If any components of W are exogenous, then those will also be included

as components of Z w . Note that if the dimension of Z w exceeded that of W , there would be more valid
                                                                         0                                         0 0
instruments than regressors. Then, as long as E (X; W ) Z w had full rank,                              =      ;         would be point-

identi…ed and consistently estimable by the usual IV estimation procedures. Thus, we restrict attention to

the case where Z w has the same dimension as W

   The remainder of this section …rst generalizes our results from the simple linear model to derive bounds

on . We then construct the identi…cation interval for each individual component of . The identi…cation

results are constructive and the bounds take the form of expressions that are trivially estimable with standard

regression software.      We focus on the case where there is only one IIV, as the identi…ed set for                             or any

component of        when there are multiple IIVs is the intersection of the intervals derived from using each of

the multiple IIVs individually, see section 3.3.




                                                                 11
3.2.1       Bounds on

As shown in section 2.2, assumptions A3 and A4 are equivalent to the assertion that V ( ) =                                                                   xZ      zX

is a valid instrument for X, for some unknown value of                                          2 [0; 1], which we denote                            . Thus   is given by

the usual IV formula:
                                                                          0                         1                               0
                                           = E (V (             ) ; Z w ) (X; W )                       E (V (             ) ; Zw) Y                                 (3.9)


The residuals of an IV regression of X and Y on W using Z w as the instruments are given by


                                                  ~                                                          1
                                                  X               X       W E [Z w0 W ]                          E [Z w0 X] ,                                      (3.10a)
                                                                                                             1
                                                  Y~              Y       W E [Z w0 W ]                          E [Z w0 Y ] .                                     (3.10b)


Isolating the …rst component of                   in the IV regression above gives

                                                         h                             i    1    h                         i
                                                       =E V (                   ~
                                                                               )X               E V(                   ) Y~ ,


or equivalently that

                                                                      Y~ = X
                                                                           ~ + U.


Applying our analysis of the simple linear model to this regression then delivers the following bounds for ,

where       z y~   denotes the covariance of Z and Y~ ,                   zx
                                                                           ~
                                                                                                      ~ etc.
                                                                                   that between Z and X,


Proposition 3 Let A1, A3, and A4 hold, and assume that E [Z w0 W ] is nonsingular and E [Z w0 U ] =

0.     Assume WLOG that                  xu       0.       If (   zx
                                                                   ~ x                  x z)
                                                                                       x~               zx
                                                                                                         ~        > 0, then B is a closed interval, while if

(    zx
      ~ x           x z)
                   x~      zx
                            ~        0, B is an open interval Speci…cally, if (                                          zx
                                                                                                                          ~ x            x z)
                                                                                                                                        x~      zx
                                                                                                                                                 ~   > 0 then B is given

by

                                                                      h                         i
                                                                              IV           IV
                                                       B         =            v(1) ;       z            if        zx
                                                                                                                   ~   < 0,
                                                                      h                         i
                                                                              IV        IV
                                                       B         =            z    ;    v(1)            if        zx
                                                                                                                   ~   > 0.


If (   zx
        ~ x           x z)
                     x~         zx
                                 ~     0 then

                                                            h         n                         o
                                                                              IV        IV
                                              B    =            max           z ;       v(1) ; 1 if                       zx
                                                                                                                           ~    < 0,
                                                                                n                 oi
                                                                                        IV   IV
                                              B    =              1; min                z  ; v(1)    if                    zx
                                                                                                                            ~    > 0,


                                                                                       12
where


                                         IV               z y~
                                         z        =              ,
                                                          zx
                                                           ~
                                       IV                 v(1)~
                                                              y                z y~ x        x~
                                                                                              y z
                                       v(1)       =                  =                              .
                                                          v(1)~
                                                              x                zx
                                                                                ~ x          x~
                                                                                              x z



   Proposition 3 is a straightforward generalization of Proposition 2 from the simple linear regression to
                                                                          IV
the multiple linear regression model.             The bound               z     corresponds to the probability limit of         from
                                                                                                            IV
an IV regression of Y on (X; W ) using (Z; Z w ) as instruments, and the bound                              v(1)   corresponds to the

probability limit of   from an IV regression of Y on (X; W ) using (V (1) ; Z w ) as instruments, where as

before V (1)     zX     x Z.   Consistent estimation of B can thus be achieved by employing standard linear

IV regression.


3.2.2   Bounds on Other Coe¢ cients

In this section we shift focus from   to other regression coe¢ cients, and show that these coe¢ cients are also

interval identi…ed. Whether or not their identi…cation regions are one or two-sided corresponds to whether

the identi…cation region for    is one or two-sided.

   To facilitate the analysis, de…ne Y            Y    X , so that


                                                      Y = W + U.


Given the assumption that the instruments Z w are valid, it follows that


                                                                          1
                                                  = E [Z w0 W ]               E [Z w0 Y ] .


Then the j-th component of       is given by

                                                 h        i               1     h       i
                                          j
                                                       ~j
                                              = E Zjw0 W                       E Zjw0 Y~ ,                                     (3.11)


where


                                                                                        1
                                ~j
                                W             Wj      W     jE       Z w0j W       j        E Z w0j X ,
                                                                                        1
                                Y~            Y       W    jE        Z w0j W      j         E Z w0j Y   ,


                                                                     13
and subscript                                                                     ~ j and Y~ correspond to the
                     j denotes a vector with its j-th component removed. That is, W

residuals of IV regressions of Wj and Y          on W        j       employing Z wj as instruments. The latter regression is

infeasible as       and therefore Y are unknown. However,                        is interval-identi…ed, and the identi…cation region

for    j   can be obtained by tracing out implied values of                      j   over admissible value of       . The formal result,

given in the following proposition, is that the bounds for                       j   are given by evaluating (3.11) at the endpoints

of the identi…cation region for .

Proposition 4 Let the conditions of Proposition 3 hold.                              Then the identi…cation region for any             j,   j =

1; :::; kw is given by the interval ranging from        j0   to        j1   where

                                       h        i   1  h        i                h        i    1  h       i
                              j0
                                             ~j
                                      E Zjw0 W        E Zjw0 Y~                        ~j
                                                                                E Zjw0 W                ~
                                                                                                 E Zjw0 X      L,
                                       h        i   1  h        i                h        i    1  h       i
                              j1
                                             ~j
                                      E Z w0 W        E Z w0 Y~                        ~j
                                                                                E Z w0 W                ~
                                                                                                 E Z w0 X      U,
                                           j                     j                     j             j



and where       L   and   U   are the extreme points of B .


3.3        Multiple Imperfect Instruments

Up to this point we have assumed that we have a single imperfect IV. We now ask what the researcher

gains from multiple imperfect IVs for the endogenous regressor X. We show that this could help tighten the

identi…cation region for , serve as a speci…cation test of sort, and with additional assumptions helps us get

two sided bounded where they were previously unavailable.

      When there are multiple imperfect instruments that satisfy A3 and A4, i.e. kz > 1 and Z = (Z1 ; :::; Zkz ),

then Proposition 3 can be used to derive bounds on                              (one or two-sided, depending on the sign of                 xzr )

for each Zr , r = 1; :::; kz .
                             For each r, denote the bounds implied by Proposition 3 as Br =                                         l;r ;   u;r
                                                             h           i
(where one of the two is possibly 1). In addition, let Dj;r = jl;r ; ju;r denote the bounds on                                  j   for each

j = 1; :::; kw given by Proposition 4. It follows that the identi…cation region for                           is the intersection of all of

the intervals Br , and the identi…cation region for each                    j   is the intersection over r = 1; :::; kz of the intervals

Dj;r .

Proposition 5 Assume AL and A1-A4. Then the identi…ed set for is B = maxr l;r ; minr u;r , and
                                                            h                     i
the identi…cation region for j , each j = 1; :::; kw is Dj = maxr jl;r ; minr ju;r . These bounds are sharp.

      This proposition is a result of applying Propositions 3 and 4 to each of the instruments Zj , j = 1; :::; J.

Furthermore, this exploits all the identifying power of the multiple IIVs, in the sense that there is no

                                                                      14
additional identifying power from imposing A3 and A4 jointly with respect to multiple instruments. This

is because for every value of                       2 B , there is an admissible data generation process that satis…es all of

our modeling assumptions. The sharpness of the bounds for each element of                                                            are a direct result of their

characterization in Proposition 4 as functions of . Note, that in principle B and the intervals Dj can be

empty, which may be used to serve as a speci…cation test.

   A potential drawback is that when (                        zx
                                                               ~ x            x z)
                                                                             x~             zx
                                                                                             ~   < 0, Proposition 5 will only provide a one-sided

bound on          .   In some cases the researcher may be willing to assert that one instrument is better than

another in the sense that it is both more relevant and more valid. In such cases, bringing this knowledge

to bear can achieve tighter identi…cation. In particular, in the case where the bounds of Proposition 5 are

one-sided, such an assumption can provide two-sided bounds.

   Let there be two instruments, Z1 and Z2 , each satisfying assumptions A3 and A4, and                                                        xz1   > 0,   xz2   > 0:

De…ne the following weighted average of Z1 and Z2 :


                                                              ! ( ) = Z2                (1            ) Z1 ,


where     2 [0; 1]. For the purpose of exposition in what follows we assume that                                                      xu   > 0, which is with out

loss of generality, as long as we continue to assume A3.


Proposition 6 Let A1, A2, and A3 hold for both Z1 and Z2 , and in addition assume that there exists

   2 [0; 1] such that         !(     )u       0 and          !(        x x
                                                                      )~        x~
                                                                                 x !(            )        !(    )~
                                                                                                                 x           0. This yields the following bounds

for .
                                                                                   n                                 o
                                                       IV                               IV           IV        OLS
                                                       !(    )               min        z1   ;       z2   ;              .


   The proposition simply says that if there exists a                                   such that the conditions on ! (                        ) are met, then we

can apply Proposition 5 to obtain a two-sided bound. The following Lemma provides more basic conditions

that guarantee the required assumption, and allow one to test it with the data in the case where the additional

regressors W are exogenous.


Lemma 3 Let W = Z w and let A3 hold, for both Z1 and Z2 . Then the following statements are equivalent:

(i) there exists         2 [0; 1] such that             !(       )u     0 and          !(    )~
                                                                                              x      < 0; (ii) there exists a known                   2 [0; 1], such
        x
        ~z1                        z1 u
that          >           >               ; (iii)    z1 y~ x
                                                           ~z2    <           ~z1 .
                                                                        z2 y~ x         Furthermore, a su¢ cient condition for the inequality
        x
        ~z2       1                z2 u
  !(     x x
        )~            x~
                       x !(   )       !(     )~
                                              x       0 is that the partial correlation between X and ! (                                    ) controlling for W ,

is negative.


                                                                                   15
   The Lemma shows the assumptions on the unobserved covariances that guarantee that the weighted

average ! (    ) both satis…es A3 and the conditions in Proposition 2. More importantly it provides conditions

to test the assumption in the data.

   Note that while Proposition 6 provides two-sided bounds, it relies on knowing                                : Lemma 3 shows that

by checking if    z1 y~ x
                        ~z2   <   z2 y~ x
                                        ~z1   holds, we can test if there exists some value of                  such that the required

conditions hold. However, it does not reveal for which values of                              this holds, it only reveals that a set of

such values exists. To exploit the results of the proposition, we need to assume a value for                           . For example,

assuming      = 0:5 implies        xz1   >    xz2   and   z1 u   <    z2 u ,   so the more relevant variable is also weakly better

in terms of validity. Another natural choice is                   =    z1 = ( z1   +   z2 ).




3.4     Relation to Manski and Pepper

Our approach for handling the problem of correlation between instruments and econometric error terms

is in part motivated by the work of Manski and Pepper (2000) on monotone instrumental variables, or

MIVs. They focus on models where the outcome variable is bounded between zero and one, and examine

nonparametric models. We, on the other hand, derive most of our results for the linear regression model.

While their results are nonparametric, an earlier version of their paper, Manski and Pepper (1998), available

through the NBER, devotes a section to linear models as well. However, even in the context of the linear

model, our modeling assumptions neither nest nor are nested by theirs. Here, we examine the implications

of our assumptions relative to those of Manski and Pepper (1998) (henceforth MP).

   In section 4 of their NBER paper, MP invoke the following assumptions:

   Assumption MP1 (Linear Response Model)1 :


                                                          Y =         + X + U;

                                                                 E (U ) = 0.


   Assumption MP2 (MIV) for all pairs z2 , z1 such that z2                             z1 :


                         8x 2 supp(X), E ( + x + U jZ = z2 )                        E ( + x + U jZ = z1 )


The second assumption is equivalent to assuming E (U jZ = z2 )                                 E (U jZ = z1 ) for all z1 ,z2 pairs where
   1 Manski and Pepper in fact do not use an intercept and do not restrict U to have mean zero. This is only a di¤erence in

notation, as including the intercept makes the mean zero restriction on U meaningless.



                                                                      16
z2    z1 . Under these restrictions, Manski and Pepper derive the following sharp bounds for :

     Proposition (Manski and Pepper (1998) Proposition 2) Assume (MP1) and (MP2). Then

                            E [Y jZ   = z2 ]   E [Y jZ = z1 ]
                                                              if E [XjZ = z2 ]   E [XjZ = z1 ] > 0, and
                            E [XjZ    = z2 ]   E [XjZ = z1 ]
                            E [Y jZ   = z2 ]   E [Y jZ = z1 ]
                                                              if E [XjZ = z2 ]   E [XjZ = z1 ] < 0.
                            E [XjZ    = z2 ]   E [XjZ = z1 ]

These bounds are sharp.

     In this paper, we mostly focus on a linear model, which is equivalent to MP1. Instead of MP2, we assume

A3 and A4, and then assuming that the sign of           xu   is known, it is WLOG to assume that      xu         zu    0. The

inequality     zu      0 is implied by MP1 and MP2, although their restriction of monotonicity of E (U jZ = z)

is stronger.        The case where the two assumptions coincide is that where E (U jZ = z) is linear in z, but

otherwise E (U jZ = z) is a sharper restriction. The MP assumption is a restriction on the distribution of U

conditional on Z for each value of z, while our restriction is a restriction on the average relation between U

and Z over their entire support.

     If E (XjZ = z) is monotone then MP get only one sided bounds, just like we would if                   xz   > 0: In order

to get two sided-bounds they require E (XjZ = z) to be non-monotone, while we require that                       xz   < 0:



4      Estimation and Inference

So far we have focused solely on identi…cation.               However, these identi…cation results are constructive,

naturally leading to consistent estimators since the derived bounds are probability limits of OLS and IV

estimators. Consistent estimators of our bounds can thus be computed using standard regression software.

     Regarding statistical inference, there are a variety of methods from the recent literature that appear

applicable in the present context, including Pakes, Porter, Ho, and Ishii (2005), Chernozhukov, Hong, and

Tamer (2007), Andrews and Guggenberger (2007), and the references cited therein. In Section 5, we employ

a variant of the inferential procedure proposed by Chernozhukov, Lee, and Rosen (2008).                         Their method

is speci…cally designed for settings where there is interval identi…cation, where the identi…ed set is the

intersection of many intervals, which is precisely the case here.

     Speci…cally, we use the method of Chernozhukov, Lee, and Rosen (2008) to construct 95% con…dence

intervals for each of the interval identi…ed parameters in our regressions. In each case, we use a sample analog

estimator for the identi…ed set of the form of [max fLn1 ; :::; LnR g ; min fUn1 ; :::; UnS g], where each Lnr ; Uns


                                                               17
are consistent estimators of all the lower and upper bounds on the parameter of interest, respectively. To

construct con…dence intervals, we …rst construct con…dence bands for each of the estimated bounds, and then

take the intersection of these. Intuitively, this adjusts each of the estimates by an amount that depends on

the precision with which it is estimated. Those estimates whose standard errors are high require a larger

adjustment than those whose standard errors are low. To be precise, the con…dence intervals are given by

CI = [L ; U ], where


                            L         max Ln1        s^l11 q l (   n ) ; :::; LnR      s^lR q l (   n)   ,

                            U         min fUn1 + s^u1 q u (        n ) ; :::; UnS   + s^uR q u (    n )g ,




where s^lr ; s^ur are the standard errors of Lnr ; Unr , respectively.               q l ( ) and q u ( ) are the        -quantiles of

the maxima of mean zero multivariate normal random vectors with variance matrices equal to the estimated

joint variance covariance matrices [Ln1 ; :::; LnR ], [Un1 ; :::; UnS ], respectively, and are computed via simulation.

The value of    n   is chosen to provide the desired nominal coverage for the object of interest, the identi…ed

set or the parameter of interest. For inference on the identi…ed set we use                         n   = 0:975, for coverage for the

parameter we use 0:95, and for uniform asymptotic coverage we use an appropriately de…ned intermediate

value that depends on the size of the estimated identi…ed set, in similar spirit to Imbens and Manski (2004)

and Stoye (2007). A by-product is that this also provides a speci…cation test: if the lower and upper bounds

of the con…dence set computed with         n   = 0:975 cross, then the model is rejected at the 0:05 level.                      For

further details we refer to Chernozhukov, Lee, and Rosen (2008).



5     Applications

In this section we provide two applications, both motivated by recent work in the Industrial Organization

literature. First, we examine the estimation of a Cobb-Douglas production function. Next, we examine the

estimation of a di¤erentiated-products demand system.




                                                             18
5.1     Estimation of a Production Function

5.1.1    The model

Consider the following Cobb-Douglas production function:


                                                     Qit = e Litl Kitk Ritr euit ,


where Qit is the output of …rm i at time t, Lit is labor, Kit is capital, Rit is R&D capital, uit is an error

term, ,       l,   k,   r   are parameters to be estimated. The error term, uit , includes technology or management

di¤erences, measurement errors and variation in external factors.

   Taking logs we obtain

                                             yit =     +   l lit   +    k kit   +   r rit   + uit ,


where: yit         log(Qit ), lit   log(Lit ), kit    log(Kit ), rit        log(Rit ).

   A major concern in the literature is that the variable input, lit , is chosen after the error term is observed

and is therefore correlated with it. Capital variables, both physical and R&D, on the other hand, are assumed

to be either exogenous or pre-determined (conditional on a …rm-…xed e¤ect). Assume that uit =              i +! it +"it ,

where    i   and ! it are “transmitted”(i.e., impact the choice of lit ), while "it is white noise that is uncorrelated

with any of the variables. Di¤erent methods have been proposed in the literature to consistently estimate

the parameters of the model. Each model imposes di¤erent restrictions on the error term. We discuss some

of these methods below as we present estimates. For a more detailed discussion see, for example, Griliches

and Mairesse (1998), Ackerberg, Caves, and Frazer (2006) and Bond and Soderbom (2005).


5.1.2    Results

To demonstrate our method we use the data from Griliches and Mairesse (1998) (see Hall (1990) for further

information on the data). The sample includes U.S. R&D performing …rms listed on the major stock ex-

changes during 1973-1988. The data are at 5 year intervals. Overall, the sample includes 2971 observations

from 4 di¤erent periods for 1400 …rms. Our analysis focuses on the …rms that existed for at least 2 (con-

secutive) periods. There are 1502 such observations with 820 …rms. The balanced panel, of …rms that were

present for the whole period, consists of 856 observations and 214 …rms. Table 1 provides some summary

statistics.

   We provide estimates of the slope coe¢ cients, using di¤erent assumptions. In all cases we focus on the


                                                                       19
set of …rms that were present in at least two cross sections. We ignore the issue of sample selection.2 Various

corrections for sample selection can easily be included in the analysis. The results are presented in Table 2.

    The …rst column presents results using OLS. These results are biased if …rm-speci…c e¤ects,                  i;   are present

and correlated with the choice of labor. A standard correction is to di¤erence the equation in order to get

rid of the …rm-speci…c e¤ects. The second column in the table presents the results from …rst di¤erencing.

The change in both the labor and capital coe¢ cients is signi…cant, suggesting that indeed the …rm-speci…c

e¤ects are present and correlated with both the …xed and variable inputs.

    One of the problems with the …rst di¤erence model is that it does not allow the transmitted productivity

shock to vary over time. The results in the third column, are based on an Olley and Pakes (1996) speci…-

cation.3 This estimator sets        i   to 0; but allows for a transmitted time-varying productivity shock, ! it , and

lets this shock vary over time according to a …rst-order Markov process. Note that both the Olley-Pakes

estimator and the …xed e¤ect estimator allow for a non-transmitted time varying shock, "it . The …rst order

Markov process does not allow for more than a one period persistence in the transmitted productivity shocks.

Where this impacts the Olley-Pakes analysis is in the inversion from investment to productivity shock. If

the two …rms are observed to invest the same amounts in the same period (controlling for di¤erences in

stocks), then they are inferred to have the same productivity shock. In reality, however, the …rms’expected

shocks could be di¤erent, which would be the case if the process was more persistent, implying that even

though their investment is the same their current productivity shock is di¤erent. Indeed, the estimates are

much closer to the OLS estimates than to those of the …rst-di¤erence estimator, which probably suggests

that the …rst-order Markov process might not be persistent enough to capture the process in the transmitted

productivity shock.

    The dynamic panel literature (e.g., Arellano and Bond (1991) and Blundell and Bond (1998)) o¤ers an

alternative way to estimate the parameters. The results in columns 4 and 5 present the estimates from these

methods. The results from the Arellano and Bond estimator are very noisy and the point estimates are

not reasonable. This is a common problem and is usually attributed to weak instruments. The "system"

estimator of Blundell and Bond aims to address this problem and indeed the results in column 5 are more

reasonable. The estimators in columns 4 and 5 are not consistent if the instruments are not valid, which
   2 Inthis data set selection does not appear to impact the estimates, despite the patterns observed in Table 1. OLS results
using all the observations, just those that are present 2 or more periods, or the balanced panel, are essentially identical. This
might not be the case for other data sets.
   3 Speci…cally, the parameters are estimated in two steps. In the …rst step, the labor co¢ cient is estimated by regressing the

log of output on the log of employment, and a second-order polynominal in the capital, R&D capital and investment. In a
second step the coe¢ cients on capital and R&D capital are estimated exploiting that the unexpected innovation in productivity
is mean independent of lagged capital. See Ackerberg, Caves, and Frazer (2006).



                                                               20
would be the case if ! it is autocorrelated. Indeed a test of over-identi…cation of the instruments fails.

   The last two columns in the table present estimates from our procedure. The estimated equation is in …rst

di¤erence. Our concern is that labor is not strictly exogenous, and therefore correlated with the transformed

error term. Both economic reasoning and the previous results suggest that the correlation is positive. We

explore two IIVs. In column 6 we use lagged R&D investment and in column 7 we use lagged capital

investment. It is reasonable to assume that lagged investment, conditional on all the other variables and a

…xed e¤ect, is positively correlated with the productivity shock. The logic is similar to that motivating the

Olley-Pakes estimator. Furthermore, both types of investment are negatively correlated with employment

(conditional on the other variables). Regressing the …rst di¤erence in log of employment on lagged R&D,

controlling for the other variables, we get a negative coe¢ cient with a t-value of -3.4. The same regression

using lagged investment yields a negative coe¢ cient with a t-stat of -5.9. Thus, both the variables satisfy

the conditions of Proposition 2 and thus are valid IIVs.

   The results in columns 6 and 7 are quite reasonable.       The results on column 6, using lagged R&D as

the IIV suggest that the labor coe¢ cient is between 0.50 and 0.71. In itself this is not a very useful bound

since it includes the OLS estimate and is very close to the …rst-di¤erence estimates. The bounds for both

the capital and R&D coe¢ cients are slightly more informative. For example, the bounds on the capital

coe¢ cients rule out most of the point estimates suggested by alternative methods. The results using lagged

capital investment yield similar results, although the bounds are tighter. For example, the point estimates of

the labor coe¢ cients do not include all but one of the previous estimates, the Blundell-Bond system estimate.

The fact that the two IIV yield somewhat similar regions is assuring. The bounds can be further tightened

by taking the intersection of the regions. The results are presented in the last column. Now the bounds are

both tight and do not include any of the previous estimates.

   The table reports standard errors in parentheses, for columns 6-8 we report 95% con…dence intervals (CI)

using the methods described in Section 4: the top number reports the CI for the identi…ed set, the bottom

number is the CI for the parameter and the middle number report the CI with uniform asymptotic coverage.


5.2    Di¤erentiated-Products Demand

In this section, we apply our method to the estimation of the demand for di¤erentiated products. We use

the Logit model. Assume that the indirect utility for consumer i for product j in market t is given by


                                                      0
                                        uijt = pjt + wjt +     jt   +   ijt ,



                                                      21
where wjt , pjt , and       jt   are observable characteristics, price, and unobservable characteristics of product j

in market t.        ijt   is an unobservable stochastic term that captures the idiosyncratic portion of consumer

i’s taste for project j in market t. We assume that when making their purchases, each consumer chooses

exactly one good, and also has the option to choose an “outside” good, i.e. not to buy any of the products.

We normalize the mean value of the outside good to be zero so that ui0t =                     i0t .   Furthermore,     ijt   is assumed

to be distributed iid extreme value, from which it follows that each product j has market share sjt in market

t, where
                                                                  0
                                                      exp pjt + wjt  + jt
                                           sjt =                                          ,
                                                      PJ
                                                   1+    exp (pkt + wkt +          kt )
                                                       k=1

and
                                                                           0
                                          log (sjt )   log (s0t ) = pjt + wjt +       jt .                                        (5.1)


If price pjt and market characteristics wjt are uncorrelated with the random unobservable                      jt ,   the parameters

of this equation,         and , could be estimated by ordinary least squares. However, it is commonly thought in

these markets that any given product’s price is correlated with unobservable shifters. In general, the error

term may include unobserved product quality or promotional activities, and both are likely to be correlated

with price. In this application, we control for unobserved product characteristics that are …xed over time

by using a product …xed e¤ects. Thus, the error term includes mainly unobserved promotional activities.

   We employ scanner data from the ready-to-eat cereal industry at the brand-quarter-MSA (metropolitan

statistical area) level, obtained from the IRI Infoscan Data Base at the University of Connecticut. We have

observations from 20 quarters, and for this application focus attention on the top 25 brands (in terms of

market share), and the San Francisco and Boston markets. The key variables observed for each product,

market, quarter combination are market share, price4 , quantity sold during promotional periods, and brand-

level advertising. For additional information on the data source and the details of the RTE cereal industry,

we refer the reader to Nevo (2000) and Nevo (2001). Table 3 provides a descriptive summary of the observed

data.

   The standard approach for dealing with the endogeneity of price in this setting is to use prices of the

product in other markets as an instrumental variable (e.g., Hausman, Leonard, and Zona (1994), Hausman

(1997), and Nevo (2001)). The idea is the IV is correlated with price through common marginal cost shocks.

Assuming that the errors in demand are independent across markets, these instruments are valid. This latter
  4 In   order to normalize per portion, we take price to be total revenue divided by quantity.




                                                                22
assumption has been challenged (e.g., the discussion of Hausman (1997) by Bresnahan (1997)). The demand

shocks could be correlated across cities for several reasons. For example, advertising could be at the regional

or national levels. Alternatively, the brand preferences could change over time. For instance if in the middle

of the sample …ber-rich cereal are found to be healthy the preferences for these cereals could vary. Any of

these stories would render the IV, and the implied estimates, not valid. There is evidence that despite these

theoretical concerns the IV are valid (e.g., Nevo (2001)), nevertheless, some concerns linger over the validity

of the estimates.

   We use prices in other cities as an imperfect IVs. The examples above suggest that usually we worry

about positive association between demand errors in di¤erent markets. Thus, it is natural to assume that

correlation of prices in other cities with the error term is positive. Unfortunately, in our data it is also the

case that prices in other markets are positively correlated with price. Therefore, these IIV will only yield one

sided bounds. However, we can exploit the fact that we have multiple cities to generate a valid IIV, using

the results in Section 3.3. For each of our markets, Boston and San Francisco, we use two IIVs. Denote by

Z1 the average price in the other markets in the region, New England for Boston and Northern California

for San Francisco, and by Z2 the average price in the other city. In Appendix B we provide a model that

justi…es the required assumptions. The intuitive idea is as follows. The average price in the region, Z1 , is

assumed to be more correlated with price because marginal cost shocks are common. It is also assumed to

be less correlated with demand because the composition of demand is assumed to be more similar between

Boston and San Francisco than with their surrounding regions.

   In the sample the correlations   pz1   = 0:81 and   pz2   = 0:48 satisfy the …rst of these assumptions. Further-

more, Lemma 3 allows to verify the second assumption. Indeed, we …nd that                 y~z1 x
                                                                                               ~z2   <0<            ~z1 ,
                                                                                                               y~z2 x       so by

the Lemma there exists a range of     such that ! ( ) = Z2           (1        ) Z1 satis…es the conditions required to

get a two sided bound. We explore two options. We explore            = 0:5 and        =   z1 = ( z1   +   z2 ) :   For both we

can verify that the correlation between price and ! ( ) = Z2              (1     ) Z1 is negative.

   The results are presented in Table 4. The dependent variable in all columns is log(sjt )                   log(s0t ). The

…rst column presents results from regressing this variable on price, brand and quarter dummy variables and

city San Francisco dummy variable. The estimated price coe¢ cient is negative and the advertising coe¢ cient

is positive, as expected. However, the own price elasticities are less than one in absolute value. Once we use

the average regional price as an IV the price coe¢ cient becomes more negative, as expected.

   The next four columns use weighted di¤erences between the average regional price and the price in the

other city as IIV. The di¤erence between the columns is in the weight used. Columns (3) and (4) use a weight


                                                        23
of 0.5, while the last two columns use      =   z1 = ( z1   +   z2 ) :Columns   (3) and (5) only impose assumption

A3, on the di¤erenced IV, while columns (4) and (6) also impose assumption A4.

    The results yield a fairly consistent picture. If we do not impose assumption A4 then the price coe¢ cient

is between -4 and -8.7 (with a con…dence interval of approximately -2.3 and -11.4). On the other hand, if we

impose assumption A4 the price coe¢ cient is between -6 and -8.7 (with a con…dence interval of approximately

-4 and -11.25). In all cases the OLS point estimate is outside the con…dence interval. When we impose

assumption A4, the IV estimate is very close to the boundary of the con…dence interval.

    In the Logit model price elasticities are proportional to the price coe¢ cient. So going from column (1)

to (2) doubles the price elasticities by roughly a factor of two. Further moving to the lower bound of the

identi…ed set increase the price elasticities by another factor of two. Generally, the results suggest that the

price elasticties are too low, in absolute value.

    There are two common uses for demand elasticities in the IO literature. Often the elasticities are used

in a …rst order condition, typically from a Bertrand pricing game, in order to compute price cost margins

(PCM). PCM computed in this way are used to test among di¤erent supply models (e.g., Nevo (2001)). Our

results suggest that the estimates of PCM using the standard IV assumption might too high. Another use of

demand estimates is for simulation of the e¤ects of mergers (e.g., Hausman, Leonard, and Zona (1994); and

Nevo (2000)). The results in Table 4 suggest that estimates using the standard IV assumption would tend

to underestimate the e¤ects of a merger, because they will tend to underestimate the substitution among

products.



6     Conclusion

In this paper we study identi…cation of the parameters of a single regression equation with endogenous

regressors and instruments that fail to satisfy the usual exogeneity condition.          Instead, we consider cases

where the instruments are assumed to have the same direction of correlation with the error as the endogenous

regressor, but where the instruments are less correlated with the error term than the endogenous regressor.

    Under our assumptions, we …rst derive an abstract characterization of the identi…ed set for all parameters,

and then focus primarily on identi…cation of the slope parameter on the endogenous regressor. Consistent

estimates of these bounds can be computed by standard OLS and linear 2SLS regressions under the usual

rank conditions (A5).     We found that the bounds that can be obtained for the slope parameter, and in

particular whether they form an open or closed interval, depend on the correlation between the endogenous


                                                        24
regressor and the instrument, which is point identi…ed. Furthermore, depending on both the sign of           xz   and
                                                                        OLS        IV
its magnitude relative to   xu   and   zu ,   may be closer to either         or   z    even if Z is less endogenous

than X, in the sense that our assumptions A3 and A4 are satis…ed.

   Relative to conventional instrumental variable assumptions, the cost of our approach is that the weaker

assumptions we impose generally yield partial identi…cation rather than point identi…cation of model para-

meters. The bene…t, however, is that inferences made are robust to a lack of instrument exogeneity, and

thus may be more credible in some circumstances. Additionally, for cases in which the applied researcher

wishes to impose instrument exogeneity, our approach provides one answer to the question of how much this

assumption drives their results.

   Our focus has been entirely on parametric models, with specialized results for linear models.              While

much empirical work relies on such models, an interesting extension would be to perform a similar analysis

in a nonparametric model. However, with a nonparametric functional form, it is doubtful that our assump-

tions on the correlations of endogenous regressors and imperfect instruments with econometric errors would

prove anywhere near as fruitful. More promising would be an extension of our assumptions to conditional

expectation analogs.   Indeed, as discussed in section 3.4, the identifying power of MIV assumptions was

examined in a nonparametric context by Manski and Pepper (2000). Their MIV assumption is a conditional

expectation analog of our assumption that the instrument is positively correlated with the error. One could

also posit a conditional mean version of our assumption that the instrument is less correlated with the latent

variate than is the endogenous regressor.       In light of the positive results of Manski and Pepper, it would

seem that such an assumption could have signi…cant identifying power in a nonparametric model.



References

Ackerberg, D. A., K. Caves, and G. Frazer (2006): “Structural Identi…cation of Production Func-

  tions,” working paper, UCLA.

Anderson, T., and H. Rubin (1949): “Estimation of the Parameters of a Single Equation in a Complete

  System of Stochastic Equations,” Annals of Mathematical Statistics, 20, 46–63.

         (1950): “The Asymptotic Properties of Estimates of the Parameters of a Single Equation system

  in a Complete System of Stochastic Equations,” Annals of Mathematical Statistics, 21(4), 570–582.




                                                        25
Andrews, D. W., and J. H. Stock (2007): “Inference with Weak Instruments,”in Advances in Economics

  and Econometrics: Theory and Applications, Econometric Society Ninth World Congress Proceedings Vol.

  III, ed. by R. Blundell, W. Newey, and T. Persson. Cambridge University Press.

Andrews, D. W. K., and P. Guggenberger (2007): “Validity of Subsampling and Plug-In Asymptotic

  Inference for Parameters De…ned by Moment Inequalities,” working paper, UCLA.

Andrews, D. W. K., and V. Marmer (2008): “Exactly Distribution-Free Inference in Instrumental

  Variables Regression with Possibly Weak Instruments,” Journal of Econometrics, 142, 183–200.

Andrews, D. W. K., M. J. Moreira, and J. H. Stock (2006): “Optimal Two-sided Invariant Similar

  Tests for Instrumental Variables Regression,” Econometrica, 74, 715–752.

Arellano, M., and S. Bond (1991): “Some Tests of Speci…cation for Panel Data: Monte Carlo Evidence

  and an Application to Employment Equations,” Review of Economic Studies, 58, 277–297.

Blundell, R., and S. Bond (1998): “Initial Conditions and Moment Restrictions in Dynamic Panel Data

  Models,” Journal of Econometrics, 87, 115–143.

Bond, S., and M. Soderbom (2005): “Adjustment Costs and the Identi…cation of Cobb-Douglas Produc-

  tion Functions,” working paper No. 05/04, Institute for Fiscal Studies.

Bontemps, C., T. Magnac, and E. Maurin (2006): “Set Identi…ed Linear Models,”working paper, IDEI.

Bound, J., D. A. Jaeger, and R. M. Baker (1995): “Problems with Instrumental Variables Estimation

  When the Correlation Between the Instruments and the Endogenous Explanatory Variable is Weak,”

  Journal of the American Statistical Association, 90(430), 443–450.

Bresnahan, T. F. (1997): “Comment on Valuation of New Goods under Perfect and Imperfect Competi-

  tion,”in The Economics of New Goods, ed. by T. F. Bresnahan, and R. J. Gordon, pp. 237–247. University

  of Chicago Press.

Chernozhukov, V., H. Hong, and E. Tamer (2007): “Estimation and Con…dence Regions for Parameter

  Sets in Econometric Models,” Econometrica, 75(5).

Chernozhukov, V., S. Lee, and A. Rosen (2008): “Intersection Bounds, Estimation and Inference,”

  working paper, MIT and CEMMAP.



                                                    26
Chioda, L., and M. Jansson (2005): “Optimal Conditional Inference for Instrumental Variables Regres-

  sion,” working paper, U.C. Berkeley.

Conley, T., C. Hansen, and P. E. Rossi (2006): “Plausibly Exogenous,” working paper, Chicago

  Graduate School of Business.

Dufour, J.-M. (1997): “Some Impossibility Theorems in Econometrics with Applications to Structural and

  Dynamic Models,” Econometrica, 65(6), 1365–1387.

         (2003): “Identi…cation, Weak Istruments, and Statistical Inference in Econometrics,” Canadian

  Journal of Economics, 36(4), 767–808.

Dufour, J.-M., and J. Jasiak (2001): “Finite Sample Limited Information Inference Methods for Struc-

  tural Equations and Models with Generated Regressors,”International Economic Review, 42(3), 815–843.

Dufour, J.-M., and M. Taamouti (2005): “Projection-Based Statistical Inference in Linear Structural

  Models with Possibly Weak Instruments,” Econometrica, 73(4), 1351–1365.

Frisch, R. (1934): Statistical Con‡uence Analysis By Means of Complete Regression Systems. University

  Institute for Economics, Oslo, Norway.

Griliches, Z., and J. Mairesse (1998): “Production Functions: The Search for Identi…cation,”in Econo-

  metrics and Economic Theory in the Twentieth Century: The Ragnar Frisch Centennial Symposium, ed.

  by S. Strom, pp. 169–203. Cambridge University Press.

Guggenberger, P., and R. J. Smith (2005): “Generalized Empirical Likelihood Estimators and Tests

  Under Partial, Weak, and Strong Identi…cation,” Econometric Theory, 21, 667–709.

Hahn, J., and J. Hausman (2002): “A New Speci…cation Test for the Validity of Instrumental Variables,”

  Econometrica, 70(1), 163–189.

        (2003a): “IV Estimation with Valid and Invalid Instruments,” working paper, MIT.

        (2003b): “Weak Instruments: Diagnosis and Cures in Empirical Econometrics,”American Economic

  Review, 93(2), 118–125.

Hall, B. (1990): “The Manufacturing Sector Master File: 1959-1987,” NBER working paper.




                                                  27
Hausman, J. A. (1997): “Valuation of New Goods under Perfect and Imperfect Competition,” in The

  Economics of New Goods, ed. by T. F. Bresnahan, and R. J. Gordon, pp. 209–237. University of Chicago

  Press.

Hausman, J. A., G. K. Leonard, and J. D. Zona (1994): “Competitive Analysis with Di¤erentiated

  Products,” Annales D’Economie et de Statistique, (34), 159–180.

Imbens, G., and C. F. Manski (2004): “Con…dence Intervals for Partially Identi…ed Parameters,”Econo-

  metrica, 72, 1845–1857.

Kleibergen, F. (2005): “Testing Parameters in GMM Without Assuming that They Are Identi…ed,”

  Econometrica, 74(4), 1103–1123.

Klepper, S., and E. E. Leamer (1984): “Consistent Sets of Estimates for Regressions with Errors in All

  Variables,” Econometrica, 52(1), 163–184.

Leamer, E. E. (1981): “Is It a Demand Curve, or Is It a Supply Curve? Partial Identi…cation through

  Inequality Constraints,” Review of Economics and Statistics, 63(3), 319–327.

Manski, C. F., and J. Pepper (1998): “Monotone Instrumental Variables: With an Application to the

  Returns to Schooling,” NBER working paper.

Manski, C. F., and J. V. Pepper (2000): “Monotone Instrumental Variables: With an Application to

  the Returns to Schooling,” Econometrica, 68(4), 997–1010.

Nelson, C. R., and R. Startz (1990): “Some Further Results on the Small Sample Properties of the

  Instrumental Variable Estimator,” Econometrica, 58(4), 967–976.

Nevo, A. (2000): “Mergers with Di¤erentiated Products: the Case of the Ready-to-Eat Cereal Industry,”

  Rand Journal of Economics, 31(3), 395–421.

           (2001): “Measuring Market Power in the Ready-to-Eat Cereal Industry,” Econometrica, 69(2),

  307–342.

Olley, S., and A. Pakes (1996): “The Dynamics of Productivity in the Telecommunications Industry,”

  Econometrica, 64, 1263–1295.

Pakes, A., J. Porter, K. Ho, and J. Ishii (2005): “The Method of Moments with Inequality Constraints,”

  working paper, Harvard University.

                                                   28
Phillips, P. C. (1989): “Partially Identi…ed Econometric Models,” Econometric Theory, 5, 181–240.

Rothenberg, T. (1984): “Approximating the Distributions of Econometric Estimators and Test Statistics,”

  in The Handbook of Econometrics, ed. by Z. Griliches, and M. Intriligator, vol. 2, pp. 881–935. Elsevier,

  New York: North Holland.

Staiger, D., and J. H. Stock (1997): “Instrumental Variables Regression with Weak Instruments,”

  Econometrica, 65(3), 557–586.

Stock, J. H., and J. H. Wright (2000): “GMM with Weak Identi…cation,” Econometrica, 68(5), 1055–

  1096.

Stock, J. H., J. H. Wright, and M. Yogo (2002): “A Survery of Weak Instruments and Weak Identi…-

  cation in Generalized Method of Moments,”Journal of Business and Economic Statistics, 20(4), 518–529.

Stock, J. H., and M. Yogo (2005): “Testing For Weak Instruments in Linear IV Regression,” in Identi-

  …cation and Inference for Econometric Models: A Festschrift in Honor of Thomas J. Rothenberg, ed. by

  D. W. Andrews, and J. H. Stock. Cambridge University Press, Cambridge, U.K.

Stoye, J. (2007): “More on Con…dence Regions for Partially Identi…ed Parameters,” working paper, New

  York University.

Wang, J., and E. Zivot (1998): “Inference on Structural Parameters in Instrumental Variables Regression

  with Weak Instruments,” Econometrica, 66(6), 1389–1440.

Zivot, E., R. Startz, and C. R. Nelson (1998): “Valid Con…dence Intervals and Inference in the Presence

  of Weak Instruments,” International Economic Review, 39(4), 1119–1144.



Appendix A: Proofs

Proposition 1

Proof. For any …xed value of , the left hand side of the system (2.3a), (2.3b) can be recovered from the

data, under random sampling assumption A1.           Under A2-A4, any value of       that satis…es each of these

restrictions as well as (2.3c) is a feasible value and all such values of   are observationally equivalent.




                                                        29
Corollary 1

                                       0                                         0
Proof. Let              =(       ;         ) and            =(          ;            ) such that both                 ;        are elements of       . Let       2 [0; 1]

and de…ne            =           + (1            )     . Because the left hand sides of conditions (2.3a) and (2.3b) are linear in

 ,       2     .


Lemma 1
                                                                                                                                 OLS          IV
Proof. The result follows directly from inspection of the expressions for                                                               and   z    given by (3.5) and
                                                                                OLS
(3.6), respectively. If               xu    > 0 (< 0), then                               is an upper (lower) bound for . If                       zu = xz   > 0 (< 0),
          IV
then      z    is an upper (lower) bound for .


Lemma 2

Proof. Using expressions (3.5) and (3.6),


                                                      OLS          IV                xu         zu            OLS              xu
                                                                   z        =         2
                                                                                                     ,                    =     2
                                                                                                                                  .
                                                                                      x         xz                              x


                        OLS           IV                         OLS                                                                  OLS     IV       OLS
To show sgn                           z      = sgn                                   it is equivalent to show that                            z                       > 0,

i.e.
                                                                                          2
                                                                   xu            xu xz    x zu
                                                                    2                2
                                                                                                                 > 0.
                                                                    x                x xz

                                                                                                                                                                           2
Case 1         xz   < 0. By A3              xu   and        zu   are either both non-positive or non-negative. Therefore                                 xu xz             x zu   =   xz

                                                       OLS             IV             OLS
has the same sign as                 xu ,   and                        z                                  > 0.


Case 2              <     xz ,   and        xz       > 0.        Isolating             zu   in the de…nition of                       (equation (2.1)) gives          zu   =
                                                       OLS             IV
       xu z = x .       Substituting into                              z    , it follows that


                                                                 OLS            IV          xu           1            z
                                                                                z     =                                    ,
                                                                                            x             x         xz



which has the same sign as                       xu   (given that            xz      > 0) if         xz         x z       > 0, or equivalently if        <     xz .




                                                                                            30
Proposition 2

Proof. Suppose that          xu   > 0. Then A4 gives


                                                                          xu            zu        0

                                                                     )      z xu               x zu

                                                                              2
                                                   ,     z       xy           x               x   (    zy          xz     )
                                                                                  IV
                                                                          ,       v(1)            ,


where as in (3.7),


                                        IV                   z xy               x zy
                                        v(1)       =
                                                             x   (   z x               xz )
                                                                     z x                 OLS                       xz               IV
                                                   =                                                                                z
                                                         (   z x              xz )                     (     z x          xz )



which makes use of the assumption that x and z are not perfectly correlated (implied by A5), as well as
                                                                                                                                                                  IV
the Cauchy-Schwartz Inequality (i.e.                     z x              xz ).        If instead            xu   < 0 then           z xu           x zu   and    v(1)            .
             IV            OLS                      IV
Note that    v(1)   =             + (1         )    z    , where                       z x= ( z x                  xz )   = 1= (1           xz ).

   First consider the case where                   xz   < 0.> 0 and 1Then        > 0, implying that IV v(1) lies between
                                                                h              i
  OLS         IV                                                    IV     OLS
       and z . If xu > 0, Lemma 1 implies that 2 z ;                             . Since in this case we also have that
                                                                                                            h          i
  IV              IV       OLS
  v(1)     , and v(1)          , it follows that v(1) provides a smaller upper bound for , i.e. 2 IV
                                                  IV                                                                IV
                                                                                                               z ; v(1) .
                                                        h              i
If instead xu < 0, then similar logic leads to 2 IV       v(1) ; z
                                                                   IV
                                                                         .
                                                                                                    n            o
                                                     IV
     Now suppose xz > 0. Then if xu > 0, v(1)                   , and Lemma 1 gives             min OLS ; IV   z   . An
                                            n                   o
immediate implication is that           min OLS ; IV       IV
                                                      z ; v(1) , but it turns out that
                                                                                              OLS
                                                                                                   is redundant in that
     n                   o          n           o
min OLS ; IV         IV
                z ; v(1)   = min IV         IV
                                       z ; v(1) .    This is because if OLS < IV                    IV
                                                                                        z , then v(1)
                                                                                                              OLS
                                                                                                                  , while
                                             n            o
clearly if instead OLS < IV  z , then min
                                                OLS
                                                    ; IV
                                                       z    = IV  z . The claim that
                                                                                           OLS
                                                                                                < IVz ) v(1)
                                                                                                             IV      OLS


holds because       xz   > 0 implies that 1                  =            xz = ( z x                  xz )   < 0, so that


                             IV            OLS                             IV                 OLS                             OLS        OLS
                             v(1)   =               + (1              )    z                          + (1          )               =          .

                                                                                                                                                           n                     o
                                                                                                                                                                 IV       IV
Symmetric reasoning applied to the case where                                     xz    > 0 and              xu    < 0 gives that                    max         z    ;   v(1)    .

The sharpness of the bounds follows as an implication of Proposition 5, which covers the case of multiple

imperfect instruments and is proven below.


                                                                                   31
Corollary 2

Proof . Using the above notation,


                         IV      IV             OLS                        IV          IV            OLS         IV
                         v(1)    z       =             + (1           )    z           z    =                    z    .


Therefore
                                                    IV           IV
                                                    v(1)         z                     1
                                                    OLS          IV
                                                                      =        =                 .
                                                                 z
                                                                                   1        xz




Proposition 3

Proof.A3, A4 and    xu     0 give


                                                             xu           zu       0

                                                     ,      xu z           zu x         0

                                    ,        x~
                                              y z          x~
                                                            x z            z y~ x           zx
                                                                                             ~ x      0,


where the third line uses Y~ =        ~ + U , which is shown below in Lemma 4 below.
                                      X                                                                                   The …rst inequality

provides an upper bound on      if       zx
                                          ~ x        x~
                                                      x z    is negative, and a lower bound if this expression is positive.

The second inequality provides an upper bound if                          zx
                                                                           ~    is positive and a lower bound if              zx
                                                                                                                               ~   is negative.

Combining these inequalities gives the conclusion of the proposition.


Lemma 4 Let the conditions of Proposition 3 hold. Then Y~ = X
                                                            ~ + U.

                                     1
Proof.Subtracting W E [Z w0 W ]          E [Z w0 Y ] from both sides of (3.8) gives


                                                             1
             Y~   = X +W                 W E [Z w0 W ]           E [Z w0 Y ] + U
                                                             1
                  = X +W                 W E [Z w0 W ]           E [Z w0 (X + W + U )] + U
                                                             1                                             1
                  = X +W                 W E [Z w0 W ]           E [Z w0 X]             W E [Z w0 W ]          E [Z w0 W ] + U

                    ~ + U.
                  = X




                                                                      32
Proposition 4

Proof.Starting with the de…nition of Y~ ,and making use of Y = Y                                              X , we have that


                                                                   1
               Y~   = Y             W   jE    Z w0j W     j            E Z w0j Y
                                                                             1                                                   1
                    = Y           X      W     jE     Z w0j W          j         E Z w0j Y + W         jE         Z w0j W   j        E Z w0j X

                    = Y~          ~ ,
                                  X


                                              1
where Y~ = Y        W   jE    Z w0j W    j         E Z w0j Y . Now plugging this into (3.11) gives

                                         h        i            1  h         i
                              j
                                               ~j
                                      = E Zjw0 W                 E Zjw0 Y~
                                         h        i            1  h        i   h        i                     1    h       i
                                               ~j
                                      = E Zjw0 W                 E Zjw0 Y~           ~j
                                                                              E Zjw0 W                                   ~ ,
                                                                                                                  E Zjw0 X


so that    j   is linear in , which delivers the conclusion of the proposition, since all other components of the

above expression are point-identi…ed.


Proposition 5

To establish sharpness of the bounds in this proposition, we make use of the following corollary to Proposition

3.

                                                                                                                                IV         IV
Corollary 3 If        xz   < 0, then any                2 [0; 1] is feasible.               If    xz   > 0, then if             v(1)   >   z    ,   2(    xz ; 1],
                             IV         IV
while if   xz   > 0 and      v(1)   <   z    , then           2 [0;        xz ).


Proof . First, write           as a function of :


                                                                  zu x              x y~z          x x
                                                                                                     ~z
                                                         =                   =                            ,                                                (6.1)
                                                                  xu z              z x~
                                                                                       y           z x
                                                                                                     ~x



where Y~ and X
             ~ are as de…ned in Lemma 4, so that the above equality follows from U = Y~                                                             ~ .
                                                                                                                                                    X       The

derivative of       with respect to           is

                                                    d                              x
                                                                                   ~x y~z        x
                                                                                                 ~z x~
                                                                                                     y
                                                              =        x z                             2,
                                                    d                        (   z x
                                                                                   ~y              ~x )
                                                                                                 z x


from which we see that                monotone in             in the direction of                x
                                                                                                 ~x y~z           x   y.
                                                                                                                  ~z x~




                                                                                   33
   First suppose the bounds are two-sided, in which case (                                                  zx
                                                                                                             ~ x                    x z)
                                                                                                                                   x~            zx
                                                                                                                                                  ~    > 0. Then the bounds on
                                z y~               z y~ x              x~
                                                                        y z
from Proposition 3 are                 and                                         . These correspond to values of                                           of 0 and 1, respectively,
                                zx
                                 ~                 zx
                                                    ~ x                x~
                                                                        x z
and as      lies between these values we have that                                        2 [0; 1]. If instead (                            zx
                                                                                                                                             ~ x            x z)
                                                                                                                                                           x~       zx
                                                                                                                                                                     ~        0, the bounds

are one-sided, and we can again trace out the feasible values of                                                                as a function of . We now further break

this case into the following subcases:

                                                            z y~       z y~ x                x~
                                                                                              y z
Case 3      zx
             ~    < 0. Then          2 max                         ;                                     ;1                 and         zx
                                                                                                                                         ~ x           x~
                                                                                                                                                        x z        0.
                                                            zx
                                                             ~         zx
                                                                        ~ x                  x~
                                                                                              x z


            z y~ x     x~
                        y z            z y~                                                                                     z y~ x           x~
                                                                                                                                                  y z                     d
  1. If                        >              , then        z y~ x~
                                                                  x                 zx
                                                                                     ~ x~
                                                                                        y        < 0,          2                                           ; 1 , and            < 0. Thus
            zx
             ~ x       x~
                        x z            zx
                                        ~                                                                                       zx
                                                                                                                                 ~ x             x~
                                                                                                                                                  x z                     d
                                                                                   z y~ x             x~
                                                                                                       y z
       the upper bound on                  is 1, when                  =                                       .        Using L’Hospital’s rule, if follows that the
                                                                                   zx
                                                                                    ~ x               x~
                                                                                                       x z
                                                             x x
                                                               ~z
       limit of      as      ! 1 is                =                   , which is                     0 from the inequalities                               zx
                                                                                                                                                             ~ x         x~
                                                                                                                                                                          x z       0 (since
                                                             z x
                                                               ~x
       (   zx
            ~ x       x z)
                     x~        zx
                                ~       0) and          zx
                                                         ~         < 0.

            z y~ x     x~
                        y z            z y~                                                                                      z y~         d
  2. If                                       , then         z y~ x~
                                                                   x                zx
                                                                                     ~ x~
                                                                                        y              0,          2                    ; 1 , and    0. Thus the lower
            zx
             ~ x       x~
                        x z            zx
                                        ~                                                                                      zx
                                                                                                                                ~             d
                                                     z y~
       bound on        is 0, when              =            . The upper bound on                                            is the limit of expression (6.1) as ! 1,
                                                     zx
                                                      ~
                          x x
                            ~z
       which is       =                    0, again since                  zx
                                                                            ~ x                  x~
                                                                                                  x z              0.
                          z x
                            ~x

                                                                        z y~          z y~ x            x~
                                                                                                         y z
Case 4      zx
             ~    > 0. Then            2         1; min                        ;                                                 and        zx
                                                                                                                                             ~ x            x~
                                                                                                                                                             x z        0. Following the
                                                                        zx
                                                                         ~            zx
                                                                                       ~ x              x~
                                                                                                         x z
                                                                                      z y~            z y~ x                x~
                                                                                                                             y z            d
same logic as when        zx
                           ~   < 0, we have that when                                        <                                          ,         < 0, and              as a function of
                                                                                      zx
                                                                                       ~              zx
                                                                                                       ~ x                  x~
                                                                                                                             x z            d
                                                x x
                                                  ~z                                                    z y~                z y~ x              x~
                                                                                                                                                 y z                    x x
                                                                                                                                                                          ~z
takes values on the interval 0;                               . When instead                                                                           ,     2                 ; 1 . In both
                                                z x
                                                  ~x                                                    zx
                                                                                                         ~                  zx
                                                                                                                             ~ x                x~
                                                                                                                                                 x z                    z x
                                                                                                                                                                          ~x
           x x
             ~z
cases,            2 [0; 1] follows from the inequalities                                  zx
                                                                                           ~ x              x~
                                                                                                             x z                 0 and           zx
                                                                                                                                                  ~   > 0.
           z x
             ~x




   Having established Corollary 3, we now provide the proof of Proposition 5.

Proof .      2 B as de…ned in the statement of the proposition follows immediately from Proposition 2 in the

simple linear model, and more generally from Proposition 3. This is because Proposition 3 can be applied

to each Zj individually, yielding that                       must fall in the intersection of each of the bounds obtained with each

Zj individually. It remains to show that these bounds are sharp, i.e. that any value of                                                                                  2 B is feasible.

To this end, we make use of Corollary 3, which gives upper and lower bounds on                                                                                j   for each j. Consider

any      2 B . Then by (2.3b) there exists for each j a                                           j   contained in the bounds given by Corollary 3 such

that
                                                                                   y~zj      x        x~
                                                                                                       y zj         j
                                                                       =                                                    ,                                                          (6.2)
                                                                                   x x
                                                                                     ~zj              x
                                                                                                      ~x zj             j



                                                                                             34
where Y~ and X
             ~ are as de…ned in (3.10a) and (3.10b). De…ne U = Y~                                                       ~ . Then straightforward algebra
                                                                                                                        X

shows that (6.2) )            zu = xu      =     j,   for any j. Since by Corollary 3                               j   2 [0; 1], it follows that A3 and A4

hold. Thus the joint distribution of (W; Y; X; Z; U ) satis…es our modeling assumptions, verifying that the

conjectured value of          2 B is indeed feasible.


Lemma 3

Proof . We assume that condition (i) holds, and show it is equivalent to (ii). Condition (i) holds if and only

if there exists      2 [0; 1] such that                z2 u       (1             )     z1 u      0 and         x
                                                                                                               ~z2        (1         )   x
                                                                                                                                         ~z1   < 0, or equivalently


                                                                                       z1 u
                                                                                                        0,                                                          (6.3)
                                                                                z1 u   +      z2 u



and
                                                                                       x
                                                                                       ~z1
                                                                       <                                1,                                                          (6.4)
                                                                                x
                                                                                ~z1    +      x
                                                                                              ~z2


from which it follows that
                                                                  x
                                                                  ~z1                          z1 u
                                                                                                              0.
                                                         x
                                                         ~z1      +       x
                                                                          ~z2           z1 u +        z2 u


Substituting      zj u   =   zj y~        x
                                          ~zj    for j = 1; 2 and collecting terms gives                                z1 y~ x
                                                                                                                              ~z2   <          ~z1 ,
                                                                                                                                         z2 y~ x       condition (iii).

From the inequalities          zj u        0 and      x
                                                      ~zj   > 0 it also follows that (iii) ) (i). The equivalence of (i) and (ii)
                                                            x
                                                            ~z1                                                                                           z u
follows since (6.3) holds if and only if                           >
                                                      , and (6.4) holds if and only if     > 1 . That
                                               1            x
                                                            ~z2                        1       z2 u
the inequality delivering two-sided bounds is implied by the partial covariance between X and ! ( ) being

negative (which is part of condition (i)) follows from …rst noting that this partial covariance is equal to                                                     x
                                                                                                                                                                ~!(    ),

and then by using the fact that                  x~
                                                  x   > 0. This implies that                             !(   )~
                                                                                                               x   <0)              !(    x x
                                                                                                                                         )~            x~
                                                                                                                                                        x !(    )   < 0,

and consequently that                !(    x x
                                          )~           x~
                                                        x !(          )     !(         )~
                                                                                        x   > 0.


Proposition 6

Proof . The proof follows by application of Proposition 5 with ! (                                                 ) as an IIV for X.



Appendix B: A model of IIV in demand

In this appendix, we provide a model of demand in which the IIV assumptions hold. To make the example

as transparent as possible, we consider linear demand for a single product sold by a monopolist. This


                                                                                       35
basic model captures the essential features that make our assumptions plausible. The example extends to

di¤erentiated product markets, where the residual demand curve for any single di¤erentiated product plays

the role of the demand for the homogenous product in our example here.

   Consider the demand for the good in four distinct geographic markets, which we denote A1, A2, B1, and

B2. The markets A1 and A2 are in region A and are geographically close to one another, while B1 and B2

are in region B. But region A and B are far away from each other.

   Suppose demand in each market m 2 fA1; A2; B1; B2g at time t has the form


                                                    Qmt =          m    + pmt +       mt ,




where Qmt is quantity demanded, pmt is price, and                            mt   is an unobservable (to the researcher) demand

shifter, e.g. unobserved promotional activities. Quantity and price are observed for each market. Assume

for simplicity that for each market, V ar (          mt )   =      ".

   A monopolist maximizing one period pro…ts, with constant marginal cost and facing this demand curve

will set prices
                                                             mcmt             m   +   mt
                                                    pmt =                                  ,
                                                              2                   2

where mcmt denotes marginal cost. If                < 0 (downward sloping demand) then Cov(pmt ;                        mt )   > 0, and least

squares estimation of the demand equation will yield biased estimates.

   Assume that Cov(mcmt ;           nt )   = 0 for all m and n, and that V ar (mcmt ) =                    mc .    The covariance between

the price in city m and the the demand error in city n is

                                                                          1
                                              Cov(pmt ;     nt )   =        Cov(      mt ; nt ),
                                                                         2

If Cov(   mt ; nt )   = 0 then pmt is a valid instrument for pnt in estimating the demand equation. However, if

Cov(   mt ; nt )   > 0 then the price in the other city is not a valid IV. If Corr(                mt ; nt )   < 1 then Corr(pmt ;     mt )   >

Corr(pmt ;    nt )   > 0 and our assumptions A3 and A4 are satis…ed.

   In order to examine whether we get one- or two-sided bounds we examine the covariance of prices in any

two markets m and n given by

                                                    1                                   1
                               cov (pmt ; pnt ) =       cov (mcmt ; mcnt ) +               2 cov ( mt ; nt )   .
                                                    4




                                                                        36
As long as cov (mcmt ; mcnt )         0 then cov (pmt ; pnt ) > 0, and using pmt as an IIV will yield only a one-sided

bound. To get a two-sided bound we exploit regional di¤erences.

   Assume that marginal costs are more correlated within a region than across regions. So, for example,

cov (mcA1;t ; mcA2;t ) > cov (mcA1;t ; mcB1;t )         0: On the other hand assume that demand shocks are more

correlated across similar markets than within regions, so that cov (             A1;t ; B1;t )   > cov (   A1;t ; A2;t )   0;5 then

the conditions of Proposition 6 are satis…ed.




  5 As   we noted in section 3.3, this is actually a stronger assumption than we need.


                                                               37
                         Table 1: Summary Statistics Production Data

                                      All …rms             present in 2 periods   balanced panel

           Variable                   mean     st dev      mean    st dev         mean     st dev

           Log sales                  5.67     1.96        6.20    1.86           6.91     1.84

           Log employment             1.26     1.78        1.71    1.69           2.41     1.62

           Log capital                4.47     2.22        5.09    2.11           5.92     2.06

           Log R&D capital            3.40     2.03        4.00    1.98           4.89     1.93

           Log capital investment     2.67     2.17        3.19    2.14           4.07     2.06

           Log R&D                    1.79     2.05        2.32    2.07           3.22     1.99

                              N=             2971                 1502                   856

Source: date from Griliches and Mairesse (1998) (see Hall (1990) for further information on the data)




                                                      38
                                   Table 2: Production Function Estimates

                            (1)       (2)       (3)      (4)      (5)         (6)            (7)            (8)

                           OLS       1st di¤    OP       AB       BB          IIV1          IIV2        both IIV

 Log employment            0.54       0.74     0.58     0.14      0.63    [0.50 0.71]    [0.62 0.72]    [0.62 0.71]

                          (0.01)     (0.02)    (0.01)   (0.41)   (0.09)   (0.07 0.77)    (0.38 0.77)   (0.34 0.78)

                                                                          (0.10 0.77)    (0.38 0.77)   (0.35 0.78)

                                                                          (0.14 0.76)    (0.41 0.77)   (0.37 0.77)

 Log of capital            0.39       0.12     0.34     0.80      0.31    [0.13 0.23]    [0.12 0.17]    [0.13 0.17]

                          (0.01)     (0.02)    (0.01)   (0.31)   (0.06)   (0.09 0.44)    (0.09 0.29)   (0.09 0.47)

                                                                          (0.09 0.43)    (0.09 0.29)   (0.09 0.46)

                                                                          (0.10 0.41)    (0.10 0.27)   (0.09 0.44)

 Log of R&D Capital        0.05       0.04     0.06     -0.29     0.01    [0.05 0.08]    [0.04 0.06]    [0.05 0.06]

                          (0.01)     (0.02)    (0.01)   (0.23)   (0.05)   (0.01 0.16)    (0.01 0.15)   (0.01 0.18)

                                                                          (0.01 0.16)    (0.01 0.12)   (0.01 0.18)

                                                                          (0.02 0.15)    (0.02 0.11)   (0.02 0.17)

   The dependent variable in all columns is the log of sales. The sample includes 1502 observations and includes only

…rms that were present for 2 or more periods. The regressions also include a dummy variable for the computer industry

(SIC 357), and annual dummy variables interacted with the computer dummy. The column labeled OP presents results

from an Olley-Pakes speci…cation, AB from a Arellano and Bond type estimation (…rst di¤erences instrumented with

lagged values), and BB a Blundell-Bond "system" estimator (where we add moments that instrument for a levels

equation using di¤erences). Standard errors are reported in parentheses. In columns 6-8 we report 95% con…dence

intervals (CI) using the method described in Section 4: the top number reports the CI for the identi…ed set, the

bottom number is the CI for the parameter and the middle number report the CI with uniform asymptotic coverage.




                                                         39
                                Table 3: Summary Statistics Demand Data

                                            Standard                       Brand        City         Quarter
Variable                Mean    Median                     Min    Max
                                            Deviation                      Variation    Variation    Variation

 Price
                        20.5    20.0       4.9             8.5    40.9   88.5%         6.3%         1.8%
 (c/ per serving)

 Advertising
                        3.6     3.0        2.0             0.0    9.8    65.9%         N/A          1.8%
 (Mil.$ per quarter)

 % Share within
                        2.2     1.7        1.4             0.3    7.9    90.3%         0.1%         0.0%
 Cereal Market

 Source: IRI Infoscan Data Base, University of Connecticut, Food Marketing Center.




                                                    40
                                           Table 4: Logit Demand Estimates

                                (1)       (2)           (3)                    (4)                 (5)              (6)

                               OLS        IV           IIV                 opt IIV                IIV1           opt IIV1

      price                    -2.21     -4.08     [-8.69 -4.08]       [-8.69 -5.99]          [-8.55 -4.08]    [-8.55 -5.94]

                               (0.72)   (0.89)    (-11.44 -2.32)       (-11.44 -4.04)        (-11.25 -2.32)   (-11.25 -4.00)

                                                  (-11.00 -2.61)       (-11.00 -4.35)        (-10.82 -2.61)   (-10.82 -4.32)

                                                  (-11.00 -2.61)       (-11.00 -4.35)        (-10.82 -2.61)   (-10.82 -4.32)

      advertising/10            0.31     0.30       [0.28 0.30]         [0.28 0.29]            [0.28 0.30]      [0.28 0.29]

                               (0.01)   (0.01)     (0.16 0.41)          (0.16 0.41)           (0.16 0.41)       (0.16 0.41)

                                                   (0.16 0.41)          (0.16 0.41)           (0.16 0.41)       (0.16 0.41)

                                                   (0.18 0.39)          (0.18 0.39)           (0.18 0.39)       (0.18 0.39)



      …rst stage t-stat:                 42.42        -19.49               -33.40                -19.99           -34.01

   The dependent variable in all columns is log(sjt )              log(s0t ). The sample includes 990 observation: 25 brands

in two cities, Boston and San Francisco, over 20 quarters (one of the brands was only introduced after 5 quarters).

The regressions also include brand …xed e¤ects, a dummy variable for San Francisco and 20 quarterly time dummy

variables. Column (2) reports results using the average price in the region as an IV. Columns (3) reports results

using the di¤erence between the average price in the other city and the average price in the region as an IIV. Column

(4) optimally combines this IIV with the OLS to sharpen the bounds. Columns (5) and (6) repeat but put a weight,

 z1 = ( z1   +   z2 ),   on the price in the other city, and   z2 = ( z1   +    z2 )   on the average price in the region. Standard

errors are reported in parentheses. In columns 3-6 we report 95% con…dence intervals (CI) using the method described

in Section 4: the top number reports the CI for the identi…ed set, the bottom number is the CI for the parameter

and the middle number report the CI with uniform asymptotic coverage.




                                                                  41
