                                NBER WORKING PAPER SERIES




                  EVALUATION OF THE COLLEGE POSSIBLE PROGRAM:
                  RESULTS FROM A RANDOMIZED CONTROLLED TRIAL

                                          Christopher Avery

                                        Working Paper 19562
                                http://www.nber.org/papers/w19562


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                     October 2013




This paper was supported by a Spender Foundation grant. Jim McCorkell, Traci Kirtley, and Chris
Mitchell worked tirelessly at College Possible to support this project. Christine Phillips, Dave Shawver,
and Bob Ziomek at ACT made it possible to include ACT scores in this analysis. Special thanks to
Mike McPherson, who originally suggested the possibility of this randomized trial and also provided
useful comments and advice throughout the course of the work. The views expressed herein are those
of the author and do not necessarily reflect the views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2013 by Christopher Avery. All rights reserved. Short sections of text, not to exceed two paragraphs,
may be quoted without explicit permission provided that full credit, including © notice, is given to
the source.
Evaluation of the College Possible Program: Results from a Randomized Controlled Trial
Christopher Avery
NBER Working Paper No. 19562
October 2013
JEL No. I23,I24

                                              ABSTRACT

This paper reports the results of a randomized trial of the College Possible program, which provides
two years of college preparatory work for high school juniors and seniors in Minneapolis and St. Paul.
The trial involved 238 students, including 134 who were randomly selected for admission to the program.
The results indicate that the College Possible program significantly increased both applications and
enrollment to both four-year colleges and selective four-year colleges; we estimate that initial enrollment
at four-year colleges increased by more than 15 percentage points for program participants, but find
little evidence of any effect of the program on ACT performance or college enrollment overall.


Christopher Avery
Harvard Kennedy School of Government
79 JFK Street
Cambridge, MA 02138
and NBER
christopher_avery@hks.harvard.edu
I. Introduction

A number of recent studies have documented two related phenomena. First, while it is
common for most high school seniors in large urban school districts to indicate that they
plan to go to college, actual college enrollment rates in these districts are relatively low
(Avery and Kane (2004), Roderick et al. (2006)). Second, among the most academically
qualified high school graduates, many do not attend colleges that match their
qualifications, and some do not attend college at all (Roderick et al. (2008, 2009)). These
patterns are particularly pronounced among students from families with relatively low
incomes (Bowen, Chingos, McPherson, 2009; Hoxby and Avery, 2013).


One possible explanation for these phenomena is that many students lack adequate
college counseling. The American School Counselor Association recommends a ratio of
250 students per counselor (which already seems high), but estimates that the average
ratio in practice is more than 470 to 1.1 Recent research supports the connection between
counseling and college enrollment: Hurwitz and Howell (2013) estimate that the addition
of a single additional counselor results in a 10 percentage point increase in four-year
college enrollment for students in a given high school.


One natural response to the lack of school-based counselors is to provide counseling in
after school settings. For example, the federal government’s TRIO program incorporates
counseling in both the Talent Search and Upward Bound programs. Mathematica
conducted evaluations of the effects of both of these programs. Its randomized controlled
study of Upward Bound found some evidence that the program induces students to shift
from two-year to four-year colleges, but this result was not statistically significant (Seftor
et al., 2006). Its evaluation of Talent Search using historical observational data to
compare participants to similar students in nearby districts with no affiliation to Talent
Search found statistically significant increases in college enrollment for participants
(Constantine et al., 2009), but this result has yet to be validated in a randomized trial.




1
    http://www.schoolcounselor.org/content.asp?contentid=133

                                                    3
Two recent studies of demonstration programs conclude that near-peer college counseling
can have a significant positive effect of college outcomes. Bos, Berman, Kane, and
Tseng (2012) find that Los Angeles public school students who were offered advice on
college choices by current college students were significantly more likely to enroll in
four-year public colleges, to submit a FAFSA, and to receive scholarships or grants than
were students in a control group. Carrell and Sacerdote (2013) provided New Hampshire
public school students who had yet to apply to college late in high school with cash
incentives, and advising from Dartmouth college undergraduates. They estimate that this
intervention increased college enrollment by 15 percentage points for women, but had no
effect on college enrollment for men.


The Expanding College Opportunities (ECO) project conducted by Caroline Hoxby and
Sarah Turner provided college application fee waivers and semi-customized college
advising in a letter sent to high-achieving low-income students. The ECO project only
communicated with students by regular mail and thus is an extremely low-cost
intervention. Hoxby and Turner (2013) reports the results of their randomized controlled
trial and find significant increases in selective college enrollment as a result of the
intervention. Similarly, Avery’s pilot study of private college counseling for talented
low-income students in New England and New York found a 9 percentage point increase
in enrollment in colleges ranked “Most Competitive”, though given a limited sample size,
this result was not statistically significant (Avery, 2010).


There are now many well-established regional and national programs designed to help
minority and financially disadvantaged students gain admission and enroll in appropriate
four-year colleges. But, despite a wealth of evidence of the success of demonstration
programs, as described above, there is limited formal evidence of the effects of any of
these ongoing college access programs. This paper aims to fill that gap by analyzing a
randomized controlled trial of one such program -- the College Possible non-profit
program for low-income high school juniors and seniors in the Minneapolis and St. Paul
metro region. The results indicate that participation in the College Possible program had
a statistically significant effect on four-year college enrollment; these results may be the

                                               4
first statistically significant findings of positive effects of a non-profit college counseling
program.


The paper proceeds as follows. Section 2 describes the College Possible program.
Section 3 summarizes the results from a regression discontinuity analysis of historical
observational data to assess the effects of the programs and explains how this analysis
was used to guide the design of the randomized trial. Section 4 describes the details and
provides descriptive statistics for the randomized trial. Section 5 analyzes the results of
the trial, including separate analyses of the effects of the program on ACT scores, college
applications, and college enrollment. Section 6 concludes.




II. The College Possible Program

College Possible is a 501(c)(3) nonprofit organization based in St. Paul. It is designed to
serve low-income high school students who do not otherwise have the resources or the
guidance to earn admission to a four-year college or university. College Possible
provides a two-year after-school curriculum to high school juniors and seniors including
SAT and ACT test preparation services, college admissions and financial aid consulting,
and guidance in the transition to college.


Students apply as high school sophomores and enter the two-year program as juniors. The
program is limited to students from families below the median city/county household
income in city/county, with a suggested minimum GPA of 2.0 or above. Among current
participants in the program, average family household income is $25,000, 91 percent are
students of color and a vast majority (90 percent of those who responded to a recent
survey) would be first-generation college students. Over the course of two years, each
participant in the College Possible program receives a total of 320 hours of direct service.


There is no cost for students or their families to participate in the College Possible
program, and currently no cost to partner high schools. In exchange for program services,
all participating students perform at least eight hours of community service each

                                               5
year. High schools provide College Possible coaches with office space Monday-Thursday
for their fulltime office hours and classroom space after school in which to conduct lessons.


There are two natural ways that the College Possible program is hypothesized to increase
college enrollment and persistence. First, the program provides extensive tutoring
designed to help participants increase their ACT scores. Second, College Possible
provides directed assistance with college choice and applications. Thus, participants in
the program are hypothesized to be more ambitious in their college applications, more
likely to be admitted to and more likely to enroll at four-year and selective colleges
where in turn they are (presumably) more likely to persist and to complete BA degrees
than in they enroll at two-year or non-selective four-year colleges.


III. Regression Discontinuity Analysis of Historical Data from College Possible
In preparation for a randomized trial, College Possible matched the records for three prior
cohorts of students to the National Student Clearinghouse. All students who applied to
join the College Possible program as 10th graders in the spring of 2005, 2006, and 2007
are included in the data matched to the National Student Clearinghouse.2 These students
graduated from high school in 2007, 2008, and 2009 and the matched data includes at
least one year of college enrollment for each cohort.


College Possible staff members review and rate each application on a 1 to 3 scale. A
rating of 1 is most attractive. Ratings of 2A and 2B are next most attractive, where 2B
often indicates that the student is receiving substantial help from another organization.
A rating of 3 indicates an applicant who does not meet the criteria for admission, either
meaning that the student has a grade point average (well) below 2.0, cannot commit to
attending program sessions regularly, or has a family income that is too high.


Program admissions decisions for these years were stratified by high school. By
individualized prior agreements, each partner high school was assigned a number of slots


2
  All applicants to the program sign a consent form that provides College Possible with access to their
future academic records.

                                                     6
in the program, where these numbers of slots vary by size of high school and past
numbers of qualified applicants. Within each high school, students were admitted in
rating order, with students rated “1” getting first priority.


Of particular note, the ratings are on an absolute rather than relative scale: on average a
student with a rating of “1” at one high school should be viewed as similar in potential to
succeed in the program as a student with a rating of “1” at a different high school. Since
the rating scheme is consistent across schools, but the number of slots and applicants vary
by school, the rating cutoffs for admission to the program also vary by school. At some
participating high schools, admission to the program is sufficiently competitive that only
students with ratings of “1” can be admitted, but at other high schools, it is common for
students with ratings of 2A and 2B admitted to the program.


We make use of this historical variation in rating cutoffs for admission by school and
year to craft a regression discontinuity approach that yields a local estimate of the effect
of the program on college enrollment. Specifically, we examine the results for 142
students who had application ratings of 1 (highest rating) but who were not admitted to
the program to the results for 203 students had application ratings of 2A and were
admitted to the program when they applied in 2005, 2006 or 2007. Under the assumption
that application ratings follow a consistent absolute scale across high schools and time,
then in this restricted sample, each student admitted to the program was rated as less
promising than each student excluded from the program.


Table 1 presents results from regression analysis with matriculation to a (two-year or
four-year) college in the fall after high school graduation as the dependent variable.3 Each
row in the table reports a more expansive regression specification. Row 1 reports the
estimated coefficient for program participation in a regression specification that includes
dummy variables for gender, application year, and application rating. Rows 2 through 4


3
  Using “Intent to Treat” as the basis for identification of the treatment group, we include all students who
were admitted to College Possible in the program participation dummy variable even though some of these
students dropped out of the program (often because they transferred to high schools not associated with
College Possible) before high school graduation.

                                                      7
report the same coefficient after the sequential addition of independent variables. The
estimated coefficients indicate an increase of 19 to 21 percentage points in probability of
college enrollment as a result of program participation. Every estimated coefficient is
statistically significant at the 5% significance level.

Table 1: Regression Discontinuity Estimates of Historical Program Effects
  Control Variables       Enrolling in       Enrolling in a
                             College      Four-Year College
  Demographics and           .192**              .179**
 Application Ratings          (.056)              (.053)
 AND Other Program           .193**              .179**
                              (.056)              (.053)
 AND Race/Ethnicity          .214**              .173**
                              (.071)              (.069)
  AND High School            .213**              .210**
  Dummy Variables             (.082)              (.078)
                                342                 342
* = significant at the 10% level; ** = significant at the 5% level.
Each entry represents the coefficient on the “Admitted to Program” dummy variable in a Probit
specification, translating that coefficient into estimated change in probability for an applicant with all
explanatory variables at their sample mean values. The standard errors associated with each coefficient are
reported in parentheses. Three students were excluded from analysis because of missing values for “Male”.

In initial discussions about the possibility of a randomized trial, College Possible
administrators indicated that the organization typically receives applications from 900
qualified applicants for the 800 places in its program. Further, these administrators
indicated that they viewed the last 200 qualified applicants based on their rankings of
student applications to be roughly equivalent from the perspective of the program. This
naturally suggested a design with a randomized selection among these last 200 qualified
applicants for 100 places in the program.


The estimates from Table 1 seem directly applicable to this design because they apply to
students who were presumably on the borderline between admission or rejection from the
College Possible program -- students were only included in the historical analysis if they
were admitted to the program with a relatively poor application rating or excluded from
the program despite having a relatively strong application rating. A two-sample t-test of
proportions of college enrollment with 100 students in the treatment group and 100
students in the control group would yield a standard error of at most 7.1 percentage points

                                                     8
(under the conservative assumption of a 50% rate of enrollment in each group). Then, a
difference of 13.9 percentage points in enrollment rates between the two groups would be
required for statistical significance. This difference translates into statistical power of
approximately .81 if the program actually increases enrollment by 20 percentage points.
Although it was somewhat optimistic to assume that the historical regression
discontinuity analysis accurately estimated the effects of the program, this power
calculation suggested that it was plausible to go ahead with a randomized trial given
historical application patterns to the program.


IV. Logistics of the Randomized Controlled Trial

In the spring of 2010, College Possible initiated a randomized trial designed to produce
an unbiased estimate of the effects of the program. As mentioned above, program
administrators projected a total of 900 applications from qualified students (i.e. students
with ratings of 1, 2A and 2B) for the 800 spots in the program that it could offer. Thus,
College Possible decided to fill its first 700 spots using ordinary program procedures and
to follow a randomized procedure to choose the last 100 students to enroll.


A total of 239 students from eight high schools were included in the randomization in
May 2010; College Possible provided individualized information for these students
including (1) gender; (2) race/ethnicity; (3) family income; (4) grade-point average
(GPA) and (5) internal program rating. Based on this information, each student was
matched to a group of two to four similar students from the same school, and then 101
students were admitted to the program on a randomized group-by-group basis designed to
admit a predetermined number to admit from each high school.4 The remaining students
were placed on a wait list (except for one student who asked to be removed from the




4
  To try to balance the groups of admitted and not admitted students, we alternated between admitting the
student with lower GPA and admitting the student with higher GPA within each group of two students. We
followed similar alternation rules for admission of students from groups with three and four students.

                                                   9
study), and 33 were admitted in a separate randomization procedure to fill new spots in
the program on a school-by-school basis in Fall 2010.5

            Table 2: Descriptive Statistics for Treatment and Control Groups
                    Control        Treatment          Treatment       Treatment
                    Group            Group             Group 1         Group 2
       Male          41.3%           41.0%              40.6%           42.4%
     Hmong           58.7%           61.9%              58.4%           72.7%
       GPA           3.043            3.015             3.033           2.959
      Rank 1         56.7%           61.2%              57.4%           72.7%
      Family        $26,770         $25,863            $27,347         $21,321
     Income
    Number of         104              134                101             33
     Students

Table 2 presents descriptive statistics for the treatment and control groups in the
randomized trial. Approximately 60% of students in the original sample were women,
approximately 60% were from the Hmong racial group, and approximately 60% were
given application ratings of “1” by College Possible. There are some differences in the
mean values for these quantitative variables across the groups. In particular, students in
Treatment Group 2 have lower average incomes and are more likely to be ranked #1 as
applicants by College Possible by comparison to students in the Control Group and in
Treatment Group 1. However, Treatment Group 2 includes a quite small sample of
students. None of the differences shown in Table 2 between Control Group and (either)
Treatment Group is statistically significant at the 5% level.


Throughout all of the analysis below, we include each of the variables in Table 2 as
control variables to account for the possible effect of difference in the compositions of
the treatment and control groups on each outcome of interest.


Table 3 summarizes the average level of program activity for students in each treatment
group for the first year of the program. Almost all of the students who were selected in
the first round in Spring 2010 participated in the program the next fall. By contrast,

5
  These 33 students included 5 who were ineligible for the program because they had transferred to new
high schools; based on the “Intent to Treat” evaluation rule, we make the conservative choice of including
these five students in the second treatment group even though they never participated in the program.

                                                    10
relatively few of the students selected from the wait list in the second round in Fall 2010
actually participated in the program, and those who did participate did so much less
intensively on average than students who were admitted to the program in the first round
of randomization. Conditional on attending at least one program session, students
admitted initially attended an average of 42.2 sessions whereas students admitted from
the wait list in the second round of randomization attended an average of 27.8 sessions.



    Table 3: Junior Year Participation Rates in the College Possible Program
                               Attended At Least            Average Number of
                               1 Program Session             Sessions Attended
   Treatment Group 1            96 of 101 (95.1%)                   42.0
   Treatment Group 2             14 of 33 (42.4%)                   11.8
     Control Group                0 of 104 (0.0%)                    0.0


Five of the 33 students selected to join the program in the second round of randomization
were ineligible for the program as they were no longer attending high schools affiliated
with College Possible. We continue to include these five students in the Treatment Group
(based on an “Intent to Treat” criterion), but none of them received services of any sort
from College Possible. In addition, a number of the other students selected from the wait
list for the program were formally eligible but did not choose to participate.


With this as background, we use two separate approaches for evaluating the effects of the
program. First, using the “Intent to Treat” criterion, we simply compare outcomes for the
134 students in the Treatment Group to outcomes for the 104 students in the Control
Group, using a dummy variable in a regression framework while controlling for other
observable variables. Second, we use the two different sources of random assignment to
treatment (Spring 2010 and Fall 2010) as instrumental variables for participation in the
program in order to estimate the local average treatment effect of the program. Then we
code any student who attended at least one session of the program as a “Program
Participant”. In a first stage regression, we use two separate treatment indicators (one for
students selected in Spring 2010 and another to identify students selected from the wait
list in Fall 2010) to isolate the variation in program participation as a function of


                                              11
exogenous treatment assignment. In a second stage regression, we use predicted values
for program participation from the first stage to identify the causal effect of “Program
Participation” on ACT scores, college application choices, and college enrollment.


V. Data and Results


As part of its application process, College Possible collects demographic information
from each student, including race/ethnicity and family income for each student along
with each student’s cumulative GPA for the first two years of high school. College
Possible matched its records to the data from one partner school district to compile the
list of college applications for each of the students for the four high schools from that
district that participated in the study. In addition, ACT and the National Student
Clearinghouse matched all of the students in the study to their respective databases to
provide standardized test scores and enrollment information for the anonymized version
of the database used for the analysis in the study. Data provided by these two
organizations match very well – though not perfectly – with internal tracking data
compiled separately by College Possible for students in the Treatment Group. The
analysis below uses only the data provided by ACT and separately by the National
Student Clearinghouse rather than ACT score and college enrollment data compiled
separately by College Possible.


    A. ACT Scores


College Possible provides extensive ACT training and tracks the performance of student
participants from an initial pretest through a series of subsequent tests and ultimately to
each student’s actual ACT score. It usually observes an improvement of 3-4 points
during its program, with average ACT composite score of about 18 for its participants on
the actual test.


Table 4 provides descriptive statistics for students in the Control Group and each
treatment group. Although College Possible strongly encourages participants to take the

                                             12
ACT, it is not surprising that the ACT data indicates less than universal participation for
Treatment Group students. First, the ACT database match is likely imperfect, as some
students may be listed under slightly different names in the College Possible and ACT
data. Second, not all students in the Treatment Group formally participated in the
College Possible program (since we are using the “Intent to Treat” criterion for
evaluation purposes), and those who were selected but who did not complete the College
Possible program may be disproportionately unlikely to take the ACT.


Comparing test scores across the groups for the students who are matched with ACT
scores, Treatment Group students have slightly lower average scores on each component
of the ACT than did Control Group students, with differences ranging from about 1/4 to
3/4 points. Treatment Group 1 (the main treatment group) students actually had
comparable ACT scores to students in the Control group, while Treatment Group 2
students scored about 2 points less on each section of the test. Interestingly, the
performance of the Treatment Group students broadly matches the historical performance
of College Possible students in prior years. That is, the Treatment Group students
improved their scores from pretest to the actual ACT as expected, but apparently the
baseline performance level of Control Group students also improved from the start of 11th
grade to the time of the actual ACT test.

        Table 4: Average ACT Scores for Treatment and Control Groups
                 Control     Treatment         Treatment         Treatment
                 Group         Group            Group 1           Group 2
 Took ACT         72.1%        71.6%              74.3%            63.6%
ACT Comp          18.35        17.85              18.28            16.33
  ACT E           16.68        16.38              16.95            14.33
  ACT M           19.12        18.83              19.36            16.95
  ACT R           18.11        17.30              17.80            15.52
  ACT S           18.85        18.33              18.53            17.62
Observations       104          134                101               33



Table 5 reports results of regression analysis to assess the effect of the program on ACT
scores after controlling for demographic variables and academic achievement variables
for students prior to the start of the program. Column 1 reports the results for regressions

                                             13
with a single dummy variable to identify the 134 students who were ever randomly
selected for admission to the program. Columns 2 and 3 report the coefficients for a
different set of regressions with two dummy variables, where Treatment Group 1 refers to
the 101 students admitted in the initial randomization and Treatment Group 2 refers to the
33 students admitted from the wait list the following fall.

             Table 5: “Intent to Treat” Regression Analysis of ACT Results
                       Any Treatment          Treatment Group 1     Treatment Group 2
    Took ACT                     0.004                           0.037                          -0.105
                                (0.063)                         (0.067)                        (0.104)
   ACT Comp                      -0.113                          0.093                          -1.059
                                (0.553)                         (0.573)                        (0.904)
      ACT E                      0.305                           0.565                          -0.888
                                 (.724)                         (0.751)                        (1.185)
      ACT M                      0.069                           0.426                        -1.577 *
                                 (.542)                         (0.556)                        (0.877)
      ACT R                      -0.379                          -0.144                         -1.460
                                (0.693)                         (0.720)                        (1.135)
      ACT S                      -0.298                          -0.027                         -0.410
                                (0.581)                         (0.606)                        (0.956)
* = significant at the 10% level; ** = significant at the 5% level.
Each entry represents the coefficient on the “Admitted to Program” dummy variable in a regression
analysis with additional control variables for race, gender, prior GPA, family income, College Possible
application ranking, as well as dummy variables for each participating high school. We use an OLS
specification for all dependent variables except “Took ACT”. We use a Probit specification to predict the
dependent variable “Took ACT”, translating that coefficient into estimated change in probability for an
applicant with all explanatory variables at their sample mean values. The standard errors associated with
each coefficient are reported in parentheses.

The coefficients in Table 5 suggest that the program had little to no observable effect on
ACT outcomes. Students in Treatment Group 1 are assessed to have mildly positive
results by comparisons to students in the Control Group, but none of these results are
statistically significant. Students in Treatment Group 2, however, performed about 1
point worse than students in the Control Group in ACT composite score, but given the
small number of students in Treatment Group 2, none of these coefficients is significant
at the 5% level.


Table 6 reports detailed results of two-stage least squares instrumental variables analysis
of the effect of participating in the program on ACT outcomes, using separate dummy
variables for “Treatment Group 1” and “Treatment Group 2” to predict program
                                                    14
participation. The variable “Program Participant” is a binary variable identifying
students who participated in at least one program session.

                       Table 6: Instrumental Variables Analysis Analysis
                    for the Effect of Program Participation on ACT Results
                       Took         ACT      ACT E       ACT M      ACT R                                   ACT S
                       ACT         Comp
  Program              0.004        0.154      0.640      0.532      -0.072                                  -0.264
 Participant          (0.064)      (0.561)   (0.731)     (0.548)    (0.705)                                 (0.591)
    GPA               0.115**     3.000**    3.618**     3.162**    2.528**                                 2.385**
 Grade 9/10           (0.052)      (0.477)   (0.622)     (0.466)    (0.600)                                 (0.503)
   Hmong               0.006     -2.878** -4.273** -2.375**        -2.627**                                -2.385**
                      (0.069)      (0.646)   (0.843)     (0.632)    (0.813)                                 (0.681)
     Male              0.053        0.265     -0.697     1.474**     -0.996                                  1.051*
                      (0.059)      (0.529)   (0.690)     (0.517)    (0.665)                                 (0.558)
    Rank 1             0.055        0.457      0.208      0.839       0.690                                   0.024
                      (0.075)      (0.687)   (0.896)     (0.672)    (0.864)                                 (0.724)
   Income              0.001      0.041**    0.050**     0.041**     0.034*                                 0.045**
 (in $1000s)          (0.002)      (0.114)   (0.019)     (0.014)    (0.018)                                 (0.015)
  Constant            0.481** 10.212 ** 7.091 **         8.332**   12.405**                                13.100**
                      (0.189)      (1.639)   (2.137)     (1.603)    (2.061)                                 (1.727)
Observations            238          171        171        171         171                                     171
* = significant at the 10% level; ** = significant at the 5% level.
Second stage regression coefficients are reported for each independent variable and specification with standard errors
listed in parentheses. A linear probability model was used for each specification. All specifications include dummy
variables for each participating high school.

The first column of Table 6 reports the second-stage regression coefficients for each
variable on the probability of taking the ACT. The remaining columns report the second
stage coefficients for each variable on scores on different components of the ACT test.
The number of observations in these columns declines from 238 to 171 because the ACT
database only included (matched) scores for 171 of the 238 students in the study.


Several independent variables are predicted to have significant effects on ACT
performance. Ninth and tenth grade GPA and family income are both positive predictors
of ACT score, while self-identification as a Hmong student is a negative predictor of
ACT score. Male students score significantly better than female students (after
controlling for other variables) on the Math and Science sections of the test, but the
coefficient on “Male” is close to zero in predicting the composite ACT score.



                                                          15
By contrast, the coefficient on the fitted “Program Participant” variable is relatively small
in all cases. This coefficient takes a maximum value just more than one-half point for the
Math and English sections of the test, but neither of these coefficients is significant at
even the 10% level. “Program Participation” is also predicted to have a small positive
effect of 0.15 points on the ACT composite score, but this is a very small magnitude by
comparison to the standard error of nearly 0.6 points for this coefficient


    B. College Applications


This randomized controlled trial encompasses students from eight high schools affiliated
with College Possible. Four of these high schools are from the partner district that has
formal records of college applications; these schools compiled and provided a full list of
college applications for their students. We do not have a systematic record of college
applications for Control Group students in the other four schools in the study, thus we
limit our analysis of college applications to students in these four schools. Among the
238 students in the study, 139 attended these four high schools, including 66 Treatment
Group Students (56 in Treatment Group 1 and 10 in Treatment Group 2) and 73 Control
Group students. Since this application data was compiled by an external source, it should
be similarly complete for both Treatment and Control Group students, and thus an
unbiased source of evaluation data.6

Figure 1 compares the percentage of students in Treatment Group and Control Group
(from the partner district with records of college applications) who applied to each of a
number of popular colleges. For the three most popular four-year colleges: Mankato
State, St. Cloud State, and the University of Minnesota, Treatment Group students were
more than twice as likely to apply as Control Group students. Yet, at the same time,
Treatment Group students were less likely to apply to popular two-year colleges such as
Century Community College and St. Paul College than Control Group students.


6
 We reran the regressions reported in Tables 7 and 8 for the effect of the program on students in these four
schools alone and found a smaller effect than for students in all schools. This suggests that restricting the
analysis of applications to students in these four high schools yields an underestimate of the effect of the
program on application behavior overall.

                                                     16
           Figure 1: Percentage of Students Applying to Popular Colleges
   70%

   60%                                                                          Treatment
   50%                                                                          Control
   40%

   30%

   20%

   10%

     0%




For further analysis, we classify colleges according to the Barrons rankings. The Barrons
ranking is based on a combination of statistics, emphasizing admission rates and the
interquartile range of standardized test scores for each college. There are six separate
categories for four-year colleges: “Most Competitive”, “Highly Competitive”, “Very
Competitive”, “Competitive”, “Less Competitive” and “Non Competitive” (which is
essentially open access). Relatively few of the students in the study had sufficient
academic qualifications for admission to “Most Competitive” colleges, so we emphasize
the other ranking categories in the analysis below.


Table 7 reports results of regression analysis to assess the effect of the program on
college applications after controlling for demographic variables and academic
achievement variables for students prior to the start of the program. Column 1 reports the
results for regressions with a single dummy variable to identify the 134 students who
were ever randomly selected for admission to the program. Columns 2 and 3 report the
coefficients for a different set of regressions with two dummy variables, where Treatment



                                             17
Group 1 refers to the 101 students admitted in the initial randomization and Treatment
Group 2 refers to the 33 students admitted from the wait list the following fall.
The OLS regression results reported in Table 7 indicate strong positive effects of
admission to the program on the number of applications submitted. In particular,
students in each treatment group submitted significantly more applications to colleges at
almost every level of Barrons ranking. The magnitudes of these coefficients are so large
that they are even significant in most cases for Treatment Group 2 students despite the
very small number of those students in Treatment Group 2 for the partner district that
provided records of college applications.

    Table 7: “Intent to Treat” Regression Coefficients for College Applications
    Dependent         Any Treatment        Treatment Group 1        Treatment Group 2
     Variable
 Applied to Four-          0.304**                0.276**                 0.450**
   Year College            (0.066)                 (0.069)                 (0.132)
       Total               4.754**                4.441**                 6.392**
   Applications            (0.687)                 (0.721)                 (1.373)
  Applications to          1.075**                1.107**                 0.905**
  Highly Comp.             (0.209)                 (0.221)                 (0.421)
  Applications to          1.380**                1.313**                 1.731**
Very Competitive            (.229)                 (0.242)                 (0.461)
  Applications to          0.442**                0.457**                   0.364
   Competitive              (.114)                 (0.121)                 (0.230)
  Applications to          1.834**                1.622**                 2.943**
 Less Competitive          (0.301)                 (0.313)                 (0.596)
  Applications to         -0.276**                -0.404**                  0.398
Two Year Colleges          (0.192)                 (0.200)                 (0.381)
* = significant at the 10% level; ** = significant at the 5% level.
Each entry represents the coefficient on the “Admitted to Program” dummy variable in a regression
analysis with additional control variables for race, gender, prior GPA, family income, College Possible
application ranking, as well as dummy variables for each participating high school. We use an OLS
specification for all dependent variables except “Applied to Four-Year College”. We use a Probit
specification to predict the dependent variable “Applied to Four-Year College”, translating that coefficient
into estimated change in probability for an applicant with all explanatory variables at their sample mean
values. The standard errors associated with each coefficient are reported in parentheses.


Table 8 reports detailed results of two-stage least squares instrumental variables analysis
of the effect of participating in the program on college applications, using separate
dummy variables for “Treatment Group 1” and “Treatment Group 2” to predict program



                                                     18
participation. Once again, “Program Participant” is a binary variable identifying students
who participated in at least one program session.


As shown in Table 8, participation in the College Possible program is estimated to
increase the probability of applying to a four-year college by 31.7 percentage points
Similarly, participation in the program is also estimated to significantly increase the
number of applications to each selectivity ranking from “Highly Competitive” to “Less
Competitive” in the Barrons rankings.

                       Table 8: Instrumental Variables Analysis
          for the Effect of Program Participation on College Applications
                Applied         Apps    HC Apps VC Apps C Apps                                             LC Apps
                 4-Year
  Program       0.317**      4.972**    1.131**     1.445**      0.466**                                    1.912**
 Participant     (0.067)      (0.675)    (0.209)     (0.231)     (0.114)                                    (0.300)
    GPA           0.064      1.561**    0.916**     0.616**     -0.243**                                   -0.647**
Grade 9-10       (0.053)      (0.546)    (0.167)     (0.184)     (0.091)                                    (0.239)
   Hmong          0.074        -0.983   -0.542**      -0.329       0.202                                      0.174
                 (0.067)      (0.689)    (0.210)     (0.233)     (0.115)                                    (0.301)
    Male          -0.023       -0.386     -0.103      -0.281      -0.082                                     -0.438
                 (0.064)      (0.654)    (0.200)     (0.221)     (0.109)                                    (0.286)
   Rank 1         0.012        0.381      0.012       0.064        0.078                                     -0.138
                  (.072)      (0.741)    (0.226)     (0.250)     (0.124)                                    (0.324)
   Income         0.001        0.020      0.009       0.006        0.003                                     0.015*
 (in $1000s)     (0.002)      (0.018)    (0.006)     (0.006)     (0.003)                                    (0.008)
  Constant      0.482**        -2.432   -2.525** -1.732**        0.803**                                    2.206**
                 (0.189)      (1.939)    (0.591)     (0.654)     (0.324)                                    (0.847)
Observations        139         139        139         139          139                                        139
* = significant at the 10% level; ** = significant at the 5% level.
Second stage regression coefficients are reported for each independent variable and specification with standard errors
listed in parentheses. A linear probability model was used for each specification. All specifications include dummy
variables for each participating high school.




                                                          19
   C. College Enrollment

At the time of this report, students in the study were one year out of high school; we plan
to continue following them over time to assess the effect of the program on long-run
educational attainment. In this section of the report, we analyze enrollment for the fall
and spring semesters of the first year beyond high school separately.


Fall Semester Enrollment


Table 9 reports enrollment choices according to the Barrons Rankings for four-year
colleges. We group the three highest selectivity categories (“Most Competitive”, “Highly
Competitive”, “Very Competitive”) together, since relatively few of the students in the
study had sufficient ACT Scores to be admitted at colleges of that level of selectivity.
There is little difference in overall enrollment figures for Treatment and Control Group
students – a bit less than 2/3 of the students in each group enrolled in college this fall.


One conspicuous difference between the groups is that Treatment Group students were
much more oriented to four-year colleges. While students in the Control Group were
only slightly more likely to enroll in a four-year college than a two-year college (34.4%
enrolled in a four-year college while 29.5% enrolled in a two-year college, students in the
Treatment Group were more than twice as likely to enroll in a four-year college (45.2%
enrolled in four-year colleges and 18.8% enrolled in two-year colleges).

   Table 9: Fall College Enrollment for Treatment and Control Group Students
                       Control        Treatment        Treatment     Treatment
                        Group           Group           Group 1       Group 2
Most, Highly, or        10.6%           18.1%            18.8%         15.2%
Very Competitive
  Competitive           17.1%           21.1%            23.7%         15.2%
  Other 4-Year           6.7%            6.0%             6.9%          3.0%
 2-Year College         29.5%           18.8%            19.8%         18.2%
  Not Enrolled          36.2%           36.1%            30.7%         48.5%
    Students              104             134              101           33



                                              20
Table 10 lists the ten most popular colleges chosen by students in Treatment and Control
Groups. On the whole, these lists overlap nearly completely: eight colleges appear on
both lists. The most popular college for students in the Treatment Group, chosen by
7.8% of the students selected for the College Possible Program – was Augsburg College,
yet not even one student from the Control Group enrolled there.


The popularity of Augsburg College among Treatment Group students suggests that we
should expect to see differences in enrollment at the “Competitive” rank, where
Augsburg falls. In fact, as shown in Table 9 above, the program seems to have achieved
the largest gains in enrollment at selective colleges: students in the Treatment Group
were 7.5 percentage points more likely than Control Group students to enroll in a college
ranked at least “Very Competitive” (and 11.5 percentage points more likely to enroll in a
college ranked at least “Competitive”).

                    Table 10: Most Popular Colleges by Enrollment
            Treatment Group                             Control Group
           Augsburg College 10                         St. Paul College 9
             St Cloud State 10                   Century Community College 8
      Century Community College 8                       St. Cloud State 6
        University of Minnesota 7               Minnesota Community & Tech 6
        Northern Hennepin CC 5                      University of Minnesota 5
             St. Paul College 5                      U Minnesota, Duluth 5
     Minnesota Community & Tech 4                   Northern Hennepin CC 3
        College of St. Benedict 4                  U Wisconsin, River Falls 3
      Minnesota State U, Mankato 4                  College of St. Benedict 2
   University of Wisconsin, Eau Claire 3         Minnesota State U., Mankato 2*
* Three other colleges (Concordia, Metropolitan State, and Normandale CC) also enrolled at least two
students from the Control Group.

Table 11 provides separate estimates of the effect of the program on students in the two
separate Treatment Groups. As shown in the first column of Table 11, admission to the
program is estimated to have little effect on enrollment overall. However, as shown in
the subsequent rows of the table, we find a strong and significant effect of admission to
the program on enrollment in four-year colleges, and also to colleges with Barrons
ranking of “Competitive” or higher. In particular, the program is estimated to increase




                                                   21
the probability of enrollment at a four-year college (and similarly at colleges ranked
“Competitive” or higher) by approximately 15 percentage points.

     Table 11: “Intent to Treat” Regression Coefficients for Fall Enrollment
                     Any Treatment         Treatment Group 1        Treatment Group 2
   Enrolled at              .017                    .059                    -.125
  Any College              (.068)                  (.071)                  (.110)
   Enrolled at            .151**                  .178**                    .062
Four Year College          (.069)                  (.074)                  (.115)
   Enrolled at              .071                    .069                  .146**
 MC, HC or VC              (.042)                  (.049)                (0.116)
 Ranked College
   Enrolled at            .153**                  .168**                   0.118
MC, HC, VC, or C           (.065)                (0.072)                 (0.117)
 Ranked College
* = significant at the 10% level; ** = significant at the 5% level.
Each entry reports the estimated effect of the “Admitted to Program” dummy variable in a Probit
specification with a dependent variable for each row in the table. Each specification includes additional
control variables for race, gender, prior GPA, family income, College Possible application ranking, as well
as dummy variables for each participating high school. In each case, we translate that Probit coefficient into
an estimated change in probability for an applicant with all explanatory variables at their sample mean
values. The standard errors associated with each coefficient are reported in parentheses.

Comparing the coefficients in Table 11 for “Treatment Group 1” and “Treatment Group
2”, we estimate more positive effects of the program on college enrollment for students in
the first treatment group (admitted Spring 2010) than in the second treatment group
(admitted Fall 2010). But none of the estimated differences for the two treatment groups
is statistically significant.


Table 12 reports detailed results of two-stage least squares instrumental variables analysis
of the effect of participating in the program on college applications, using separate
dummy variables for “Treatment Group 1” and “Treatment Group 2” to predict program
participation. Once again, “Program Participant” is a binary variable identifying students
who participated in at least one program session. Columns 1 through 3 of the table report
the results of specifications that do not include fixed effects / high school dummy
variables. Columns 4 through 6 repeat the analysis from each of Columns 1 through 3
with the addition of these high school dummy variables.




                                                     22
                       Table 12: Instrumental Variables Analysis
        for the Effect of Program Participation on Fall Semester Enrollment
                Enrolled Four-Year         Barrons     Enrolled    Enrolled                                       Barrons
                              College     Ranking                 Four-Year                                       Ranking
                                          MC to C                                                                 MC to C
  Program           .072       .168**       .165**         .061     .160**                                         .152**
Participation     (.068)        (.067)       (.064)     (0.069)      (.068)                                         (.064)
    GPA          .122**        .271**         .260      .113**      .266**                                         .243**
 Grade 9-10       (.055)        (.054)       (.051)      (.057)      (.055)                                         (.053)
   Hmong           -.022         -.045        -.013       -.061       -.047                                           .002
                  (.068)        (.066)       (.063)      (.075)      (.073)                                         (.069)
    Male           -.031         -.003        .003        -.042       -.009                                          -.009
                  (.063)        (.061)       (.058)      (.064)      (.062)                                         (.059)
   Rank 1           .060         -.026        -.001        .038       -.008                                           .035
                  (.067)        (.065)       (.062)      (.082)      (.079)                                         (.076)
   Income        .003**         .003*       .004**       .003*       .003*                                          .003*
                  (.002)        (.002)       (.002)      (.002)      (.002)                                         (.002)
  Constant          .137      -.530**         -.607        .140    -.516**                                        -.462**
                  (168)         (.163)       (.155)      (.205)      (.200)                                         (.190)
High School          NO           NO           NO         YES         YES                                            YES
Fixed Effects
Observations        238           238          238         238         238                                               238
* = significant at the 10% level; ** = significant at the 5% level.
Second stage regression coefficients are reported for each independent variable and specification with standard errors
listed in parentheses. A linear probability model was used for each specification. All specifications include dummy
variables for each participating high school.

As shown in Table 12, “Program Participation" is estimated to increase enrollment
overall and enrollment at four-year colleges. The estimated effect of the program is a 15
to 17 percentage point increase in enrollment at four-year colleges and at four-year
colleges with ranking of “Competitive” or higher; each of these coefficients is significant
at the 5% level whether high school fixed effects are included or not. The estimated
effect of the program on enrollment at any college is smaller and not significant with or
without high school fixed effects.


Spring Semester Enrollment


Table 13 reports spring semester enrollment choices according to the Barrons Rankings
for four-year colleges. Most students who enrolled in both semesters chose the same




                                                          23
college for the spring as for the fall. However, 26 students enrolled in only the first
semester. (Relatively few students – just 10 - enrolled in the spring but not in the fall.)

 Table 13: Spring College Enrollment for Treatment and Control Group Students
                      Control       Treatment         Treatment    Treatment
                       Group           Group           Group 1       Group 2
 Most, Highly or       10.6%           15.7%            16.8%         12.1%
Very Competitive
  Competitive          13.4%           19.4%            21.8%         12.1%
  Other 4-Year          5.8%            3.0%             3.0%          3.0%
 2-Year College        30.8%           16.5%            17.8%         12.1%
  Not Enrolled         39.4%           45.5%            40.6%         60.6%
    Students             104             134              101           33



Some differences are apparent between fall semester enrollment (Table 9) and spring
semester enrollment (Table 13). While enrollment fell off to some degree among all
groups from fall to spring semester, it declined to a greater degree for Treatment Group
students, especially for Treatment Group 2 students. Overall, enrollment at four-year
colleges was still greater in the spring semester for Treatment Group students (38.1%) by
comparison to Control Group students (29.8%), but Control Group students were more
likely to be enrolled overall (60.6%) than were Treatment Group students (54.5%).

    Table 14: “Intent to Treat” Regression Coefficients for Spring Enrollment
                      Any Treatment        Treatment Group 1        Treatment Group 2
   Enrolled at              -.083                   -.042                 -.239**
  Any College              (.072)                  (.077)                  (.111)
   Enrolled at               .097                   .117                    .026
Four Year College          (.067)                  (.073)                  (.112)
 MC, HC or VC                .049                   .048                    .087
 Ranked College            (.049)                  (.046)                  (.100)
MC, HC, VC, or C           .123*                  .145**                    .045
 Ranked College            (.063)                  (.069)                  (.111)
* = significant at the 10% level; ** = significant at the 5% level.
Each entry reports the estimated effect of the “Admitted to Program” dummy variable in a Probit
specification with a dependent variable for each row in the table. Each specification includes additional
control variables for race, gender, prior GPA, family income, College Possible application ranking, as well
as dummy variables for each participating high school. In each case, we translate that Probit coefficient into
an estimated change in probability for an applicant with all explanatory variables at their sample mean
values. The standard errors associated with each coefficient are reported in parentheses.




                                                     24
As shown in Table 14, admission to the program is still estimated to have a positive
effect on spring semester enrollment at four-year and selective colleges, though the
magnitudes of these coefficients are somewhat diminished from those for the fall
semester. Further, admission to the program is now estimated to have negative effect on
spring semester enrollment overall. The only significant positive effects in these
specifications are that admission to the program increased enrollment at colleges ranked
“Competitive” or better by 12.3 percentage points overall and by 14.5 percentage points
for students admitted to the program in the first round of randomization. The only
significant negative effect in these specifications is that admission to the program reduced
enrollment overall for students admitted to the program in the second round of
randomization. But given the small percentage of students in the second round of
randomization who ever participated in the program, this finding is likely spurious.


                        Table 15: Instrumental Variables Analysis
       for the Effect of Program Participation on Spring Semester Enrollment
                Enrolled Four-Year          Barrons     Enrolled   Enrolled  Barrons
                               College     Ranking                Four-Year  Ranking
                                           MC to C                           MC to C
  Program           -.010       .125**       .154**         -.029     .102    .131**
Participation      (.069)        (.006)       (.062)       (.068)    (.064)    (.062)
    GPA           .187**        .294**       .254**       .156**    .278**    .233**
 Grade 9-10        (.055)        (.051)       (.050)       (.056)    (.053)    (.051)
   Hmong            -.008         -.006        -.019         .002     .006      -.013
                   (.686)        (.006)       (.062)       (.074)    (.070)    (.068)
    Male         -.160**          -.090        -.071     -.168**    -.098*      -.077
                   (.063)        (.058)       (.057)       (.063)    (.059)    (.058)
   Rank 1           -.015         -.040        -.005        -.061     -.044      .012
                   (.067)        (.062)       (.061)       (.080)    (.076)    (.073)
   Income         .004**         .003*       .003**        .003*      .002       .002
                   (.002)        (.002)       (.001)       (.002)    (.002)    (.002)
  Constant          -.005         -.604        -.572         .271  -.505**   -.414**
                   (.168)        (.156)       (.151)       (.202)    (.190)    (.185)
High School          NO            NO           NO          YES       YES       YES
Fixed Effects
Observations         238           238          238          238       238       238
* = significant at the 10% level; ** = significant at the 5% level.
Second stage regression coefficients are reported for each independent variable and specification with standard errors
listed in parentheses. A linear probability model was used for each specification. All specifications include dummy
variables for each participating high school.




                                                          25
Table 15 reports detailed results of two-stage least squares instrumental variables analysis
of the effect of participating in the program on college applications, using separate
dummy variables for “Treatment Group 1” and “Treatment Group 2” to predict program
participation. Columns 1 through 3 of the table report the results of specifications that do
not include fixed effects / high school dummy variables. Columns 4 through 6 repeat the
analysis from each of Columns 1 through 3 with the addition of these high school dummy
variables.


The results in Table 15 indicate that program participation had a significant positive
effect on spring semester enrollment at selective colleges, but little to no effect on spring
semester enrollment overall. Comparing the results of Tables 12 and 15, we estimate that
participation in the program increased in enrollment in four-year and selective colleges
by 15 to 17 percentage points in the fall semester, as opposed to 10 to 15 percentage
points in the spring semester. Further, the statistical significance of the spring semester
results is much more delicate than that of the fall semester results. The significance of
the predicted effect of program participation on enrollment at four-year colleges for the
spring semester turns on the inclusion of high school fixed effects. Though these
coefficients in columns 2 and 5 of Table 15 are similar (12.5% vs. 10.2%), the result in
column 2 without high school fixed effects is significant at the 5% level whereas the
result in column 5 with high school fixed effects is not even significant at the 10% level.




                                             26
VI. Conclusion

The results of this randomized trial provide strong but not unqualified support for the
efficacy of the College Possible program. We find significant evidence that the program
promoted both applications and enrollment at both four-year colleges and selective four-
year colleges, but little to no evidence of any effect of the program on ACT performance
or enrollment overall.


The randomized trial results are reasonably close to the results of the regression
discontinuity analysis of (non-experimental) historical data for initial enrollment in four-
year colleges. Our analysis of the historical data, presented in Table 1, suggested that
the program increased enrollment in four-year colleges by approximately 20 percentage
points, whereas our analysis of experimental data, presented in Tables 11 and 12,
suggests that the program increased enrollment in four-year colleges by approximately 15
percentage points. But, even taking as given the (non-significant) point estimate in
Table 12 that program participation increased fall semester enrollment in any college by
7.2 percentage points, the effect of the program on college enrollment (at any college) is
at most one-third the size of the magnitude of the point estimate from historical data. In
general, however, the results of the trial do seem to suggest that the sample size was
sufficient to detect the effects of the program on desired outcomes.


One surprising result in the trial is that the Treatment Group students achieved the ACT
scores that College Possible expected, but even though these scores represent
improvement from pretest results compiled by College Possible, they essentially match
the ACT scores for students in the Control Group. This finding suggests that some
Control Group students may have solicited and received help from other sources after
learning that they were not admitted to the College Possible program. (It is also possible
that another year of coursework in school contributed to increased ACT scores for
students in both Treatment and Control Groups.) Thus, the randomized trial should
probably be viewed as assessing the effectiveness of the College Possible program
relative to the effectiveness of alternative programs that would be chosen by (some)
students. This interpretation suggests a much higher threshold for the program to

                                             27
produce statistically significant results than we might have anticipated in advance of the
randomized trial.


Looking beyond the immediate results presented in this paper, the typical goal of college
access programs like College Possible is to help students complete a BA degree and to do
so as a steppingstone towards career success. In this context, initial college enrollment is
simply an intermediate outcome, but also one that can dramatically alter the long-term
path of individual students.


Previous descriptive studies by Bowen et al. (2009) and by Roderick et al. (2006, 2008,
2009) strongly suggest that a student’s chances of college graduation substantially
increase if that student enrolls at a selective four-year college instead of a less selective
college or at any four-year college instead of a two-year college, but these studies do not
attempt to demonstrate a causal link between college choice and college graduation. Two
recent studies using regression discontinuity methods provide evidence of a positive link
between college selectivity and future outcomes. Cohodes and Goodman (2013) find that
students induced by a Massachusetts scholarship to attend an in-state four-year college
are less likely to complete a BA degree than are students with similar but slightly lesser
qualifications who did not qualify for the scholarship (and thus were more likely to attend
a typically more selective out-of-state private college). On a similar note, Hoekstra
(2009) finds that students who barely met the test score cutoff for admission to the
flagship public university in one state had significantly higher long-run earnings than
students who barely missed that cutoff for admission.


The results of the trial primarily indicate that the program induced Treatment Group
students to shift enrollment from two-year colleges to four-year colleges and from both
two-year colleges and non-selective four-year colleges to selective four-year colleges.
Based on the findings of Cohodes and Goodman (2013) and Hoekstra (2009), we
hypothesize that these differences in enrollment patterns will translate into long-run
differences in educational attainment and in earnings between Treatment and Control
Group students. Yet, since not all students who enroll at a four-year college go on to

                                              28
graduate, we also should expect to see smaller differences in BA completion rates than in
initial enrollment rates between Treatment and Control Groups. For example, Hurwitz et
al. (2013) conclude that the choice by the state of Maine to make taking the SAT exam
mandatory for all public high school students resulted in a significant increase in four-
year college enrollment, but that this policy change had a smaller, not statistically
significant effect on BA completion.


There is already some suggestion that the effects of the College Possible program are
diminishing over time. The estimated effects of the program are smaller for the spring
semester than for the fall semester of the first year after high school graduation; the effect
of the program on second semester enrollment in four-year colleges is only on the
borderline of statistical significance. We plan to continue tracking outcomes for the
students in the treatment and control group for at least the next several years. It will be
interesting to see if the outcomes for Treatment and Control Group students converge or
continue to diverge in the future.




                                             29
References

Avery, Christopher (2010), “The Effects of College Counseling on High Achieving,
Low-Income Students,” NBER working paper 16359.

Avery, Christopher and Thomas J. Kane (2004), “Student Perceptions of College
Opportunities: The Boston COACH Program”, in College Choices: The Economics of
Where to Go, When to Go, and How to Pay for It, Caroline M. Hoxby Ed., University of
Chicago Press.

Bos, Johannes M., Jacqueline Berman, Thomas J. Kane, and Fannie M. Tseng (2012),
“The Impacts of SOURCE: A Program to Support College Enrollment through Near-
Peer, Low-Cost Student Advising”, working paper.

Bowen, William, Matthew Chingos and Michael McPherson (2009), Crossing the Finish
Line: Completing College at America’s Public Universities. Princeton University Press.

Carrell, Scott and Bruce Sacerdote (2013), “Late Interventions Matter Too: The Case of
College Coaching New Hampshire,” NBER Working Paper 19031.

Cohodes, Sarah, and Joshua Goodman (2013). Merit Aid, College Quality and College
Completion: Massachusetts' Adams Scholarship as an In-Kind Subsidy, Harvard
Kennedy School Working Paper RWP13-005.

Constantine, Jill, Neil Seftor, Emily Sama Martin, Tim Silva and David Myers (2006),
“A Study of the Effect of the Talent Search Program on Secondary and Postsecondary
Outcomes in Florida, Indiana and Texas, Mathematica Policy Research.

Hoekstra, Mark (2009), “The Effect of Attending the Flagship State University on
Earnings: A Discontinuity Based Approach,” Review of Economics and Statistics,
November 2009, 91:4, 717-724.

Hoxby, Caroline and Sarah Turner (2013),“Expanding College Opportunities for High-
Achieving, Low Income Students,” IEPR Discussion Paper No. 12-014, Stanford
University.

Hurwitz, Michael and Jessica S. Howell (2013), “Estimating Causal Impacts of School
Counselors Using Regression Discontinuity Designs." Journal of Counseling &
Development, forthcoming.

Hurwitz, Michael, Jonathan Smith, Sunny Niu, and Jessica Howell (2013), “How is Four-
Year College Enrollment Affected by Mandatory College Entrance Exams?”, working
paper.




                                          30
Roderick, Melissa, Jenny Nagaoka, and Elaine Allensworth; with Vanessa Coca,
Macarena Correa, and Ginger Stoker (2006), “From High School to the Future: A First
Look at Chicago Public School Graduates' College Enrollment, College Preparation, and
Graduation from Four-Year Colleges,” Consortium on Chicago School Research
Technical Report.

Roderick, Melissa, Jenny Nagaoka, Vanessa Coca, Eliza Moeller; with Karen Roddie,
Jamiliyah Gilliam, and Desmond Patton (2008), “From High School to the Future:
Potholes on the Road to College. Consortium on Chicago School Research Technical
Report.

Melissa Roderick, Jenny Nagaoka, Vanessa Coca, and Eliza Moeller (2009), “From High
School to the Future: Making Hard Work Pay Off,” Consortium on Chicago School
Research Technical Report, April 2009.

Seftor, Neil S., Arif Mamun, and Allen Schirm (2009), "The Impacts of Regular Upward
Bound on Postsecondary Outcomes 7-9 Years After Scheduled High School Graduation."
Mathematica Policy Research.




                                         31
