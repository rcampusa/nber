                                NBER WORKING PAPER SERIES




                        VECTOR MULTIPLICATIVE ERROR MODELS:
                           REPRESENTATION AND INFERENCE

                                          Fabrizio Cipollini
                                           Robert F. Engle
                                         Giampiero M. Gallo

                                        Working Paper 12690
                                http://www.nber.org/papers/w12690


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                    November 2006




We thank Christian T. Brownlees, Marco J. Lombardi and Margherita Velucchi for many discussions
on MEMs and multivariate extensions, as well as participants in seminars at CORE and IGIER-Bocconi
for helpful comments. The usual disclaimer applies. The views expressed herein are those of the author(s)
and do not necessarily reflect the views of the National Bureau of Economic Research.

© 2006 by Fabrizio Cipollini, Robert F. Engle, and Giampiero M. Gallo. All rights reserved. Short
sections of text, not to exceed two paragraphs, may be quoted without explicit permission provided
that full credit, including © notice, is given to the source.
Vector Multiplicative Error Models: Representation and Inference
Fabrizio Cipollini, Robert F. Engle, and Giampiero M. Gallo
NBER Working Paper No. 12690
November 2006
JEL No. C01

                                              ABSTRACT

The Multiplicative Error Model introduced by Engle (2002) for positive valued processes is specified
as the product of a (conditionally autoregressive) scale factor and an innovation process with positive
support. In this paper we propose a multi-variate extension of such a model, by taking into consideration
the possibility that the vector innovation process be contemporaneously correlated. The estimation
procedure is hindered by the lack of probability density functions for multivariate positive valued random
variables. We suggest the use of copulafunctions and of estimating equations to jointly estimate the
parameters of the scale factors and of the correlations of the innovation processes. Empirical applications
on volatility indicators are used to illustrate the gains over the equation by equation procedure.

Fabrizio Cipollini                                   Giampiero M. Gallo
Viale Morgagni                                       Dipartimento di Statistica "G.Parenti"
59-50134 Firenze                                     Viale G.B. Morgagni, 59
Italy                                                50134 Firenze  Italy
cipollin@ds.unifi.it                                 gallog@ds.unifi.it

Robert F. Engle
Department of Finance, Stern School of Business
New York University, Salomon Center
44 West 4th Street, Suite 9-160
New York, NY 10012-1126
and NBER
rengle@stern.nyu.edu
CONTENTS                                                                                                   2


Contents
1   Introduction                                                                                           4

2   The Univariate MEM Reconsidered                                                                        5
    2.1 Definition and formulations . . . . . . . . . . . . . . . . . .       .   .   .   .   .   .   .    5
    2.2 Estimation and Inference . . . . . . . . . . . . . . . . . . .        .   .   .   .   .   .   .    7
    2.3 Some Further Comments . . . . . . . . . . . . . . . . . . .           .   .   .   .   .   .   .    9
         2.3.1 MEM as member of a more general family of models               .   .   .   .   .   .   .    9
         2.3.2 Exact zero values in the data . . . . . . . . . . . . .        .   .   .   .   .   .   .   10

3   The vector MEM                                                                                        11
    3.1 Definition and formulations . . . . . . . . . . . . . . . . .     .   .   .   .   .   .   .   .   11
    3.2 Specifications for εt . . . . . . . . . . . . . . . . . . . . .   .   .   .   .   .   .   .   .   11
         3.2.1 Multivariate Gamma distributions . . . . . . . . .         .   .   .   .   .   .   .   .   12
         3.2.2 The Normal copula-Gamma marginals distribution             .   .   .   .   .   .   .   .   12
    3.3 Specification for µt . . . . . . . . . . . . . . . . . . . . .    .   .   .   .   .   .   .   .   13
    3.4 Maximum likelihood inference . . . . . . . . . . . . . . .        .   .   .   .   .   .   .   .   15
         3.4.1 The log–likelihood . . . . . . . . . . . . . . . . .       .   .   .   .   .   .   .   .   16
         3.4.2 Derivatives of the concentrated log-likelihood . . .       .   .   .   .   .   .   .   .   17
         3.4.3 Details about the estimation of θ . . . . . . . . . .      .   .   .   .   .   .   .   .   18
    3.5 Estimating Functions inference . . . . . . . . . . . . . . .      .   .   .   .   .   .   .   .   19
         3.5.1 Framework . . . . . . . . . . . . . . . . . . . . .        .   .   .   .   .   .   .   .   20
         3.5.2 Inference on θ . . . . . . . . . . . . . . . . . . .       .   .   .   .   .   .   .   .   20
         3.5.3 Inference on Σ . . . . . . . . . . . . . . . . . . .       .   .   .   .   .   .   .   .   21

4   Empirical Analysis                                                                                    24

5   Conclusions                                                                                           27

A Useful univariate statistical distributions                                                             28
  A.1 The Gamma distribution . . . . . . . . . . . . . . . . . . . . . . . . . .                          28
  A.2 The GED distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . .                        28
  A.3 Relations GED–Gamma . . . . . . . . . . . . . . . . . . . . . . . . . .                             29

B A summary on copulas                                                                                    29
  B.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                     30
  B.2 Understanding copulas . . . . . . . . . . . . . . . . . . . . . . . . . . .                         31
  B.3 The Normal copula . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                         32

C Useful multivariate statistical distributions                                                           32
  C.1 Multivariate Gamma distributions . . . . . . . . . . . . . . . . . . . . .                          32
  C.2 Normal copula – Gamma marginals distribution . . . . . . . . . . . . . .                            33

D A summary on estimating functions                                                                       34
  D.1 Notation and preliminary concepts . . . . . . . . . . . . . . . . . . . . .                         35
      D.1.1 Mathematical notation . . . . . . . . . . . . . . . . . . . . . . .                           35
      D.1.2 Martingales . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                         35
CONTENTS                                                                                                  3


  D.2   Set-up and the likelihood framework . . . . . . . . . . .    .   .   .   .   .   .   .   .   .   37
  D.3   Martingale estimating functions . . . . . . . . . . . . .    .   .   .   .   .   .   .   .   .   38
  D.4   Asymptotic theory . . . . . . . . . . . . . . . . . . . .    .   .   .   .   .   .   .   .   .   40
  D.5   Optimal estimating functions . . . . . . . . . . . . . . .   .   .   .   .   .   .   .   .   .   42
  D.6   Estimating functions in presence of nuisance parameters      .   .   .   .   .   .   .   .   .   43

E Mathematical Appendix                                                                                  46
1   INTRODUCTION                                                                          4


1    Introduction

The study of financial market behavior is increasingly based on the analysis of the dy-
namics of nonnegative valued processes, such as exchanged volume, high–low range, ab-
solute returns, financial durations, number of trades, and so on. Generalizing the GARCH
(Bollerslev (1986)) and ACD (Engle and Russell (1998)) approaches, Engle (2002) reck-
ons that one striking regularity of financial time series is that persistence and clustering
characterizes the evolution of such processes. As a result, the process describing the dy-
namics of such variables can be specified as the product of a conditionally deterministic
scale factor which evolves according to a GARCH–type equation and an innovation term
which is i.i.d. with unit mean. Empirical results (e.g. Chou (2005); Manganelli (2002))
show a good performance of these types of models in capturing the stylized facts of the
observed series.

More recently, Engle and Gallo (2006) have investigated three different indicators of
volatility, namely absolute returns, daily range and realized volatility in a multivariate
context in which each lagged indicator was allowed to enter the equation of the scale
factor of the other indicators. Model selection techniques were adopted to ascertain the
statistical relevance of such variables in explaining the dynamic behavior of each indi-
cator. The model was estimated assuming a diagonal variance covariance matrix of the
innovation terms. Further applications along the same lines may be found in Gallo and
Velucchi (2005) for different measures of volatility based on 5-minute returns, Brownlees
and Gallo (2005) for hourly returns, volumes and number of trades, Engle et al. (2005)
for volatility transmission across financial markets.

Estimation equation by equation ensures consistency of the estimators in a quasi-maximum
likelihood context, given the stationarity conditions discussed by Engle (2002). This sim-
ple procedure is obviously not efficient, since correlation among the innovation terms
is not taken into account: in several cases, especially when predetermined variables are
inserted in the specification of the conditional expectation of the variables, it would be
advisable to work with estimators with better statistical properties, since model selection
and ensuing interpretation of the specification is crucial in the analysis.

In this paper we investigate the problems connected to a multivariate specification and
estimation of the MEM. Since joint probability distributions for nonnegative–valued ran-
dom variables are not available except in very special cases, we resort to two different
strategies in order to manage vector-MEM: the first is to adopt copula functions to link
together marginal probability density functions specified as Gamma as in Engle and Gallo
(2006); the second is to adopt an Estimating Equation approach (Heyde (1997), Bibby
et al. (2004)). The empirical applications performed on the General Electric stock data
show that there are some numerical differences in the estimates obtained by the three
methods: copula and estimating equations results are fairly similar to one another while
the equation–by–equation procedure provides estimates which depart from the system–
based ones the more correlated the variables inserted in the analysis.
2     THE UNIVARIATE MEM RECONSIDERED                                                     5


2      The Univariate MEM Reconsidered

Let us start by recalling the main features of the univariate Multiplicative Error Model
(MEM) introduced by Engle (2002) and extended by Engle and Gallo (2006). For ease of
reference, some characteristics and properties of the statistical distributions involved are
detailed in appendix A.


2.1    Definition and formulations

Let us consider xt , a non–negative univariate process, and let Ft−1 be the information
about the process up to time t − 1. Then the MEM for xt is specified as

                                         x t = µt ε t ,                                 (1)

where, conditionally on the information Ft−1 : µt is a nonnegative conditionally deter-
ministic (or predictable) process, that is the evolution of which depends on a vector of
unknown parameters θ,
                                       µt = µt (θ);                                  (2)
εt is a conditionally stochastic i.i.d. process, with density having non–negative support,
mean 1 and unknown variance σ 2 ,

                                   εt |Ft−1 ∼ D(1, σ 2 ).                               (3)

The previous conditions on µt and εt guarantee that

                                   E(xt |Ft−1 ) = µt                                    (4)
                                   V (xt |Ft−1 ) = σ 2 µ2t .                            (5)


To close the model, we need to adopt a parametric density function for εt and to specify
an equation for µt .

For the former step, we follow Engle and Gallo (2006) in adopting a Gamma distribution
which has the usual exponential density function (as in the original Engle and Russell
(1998) ACD model) as a special case. We have

                                εt |Ft−1 ∼ Gamma(φ, φ),                                 (6)

with E(εt |Ft−1 ) = 1 and V (εt |Ft−1 ) = 1/φ. Taken jointly, assumptions (1) and (6) can
be written compactly as (see appendix A)

                              xt |Ft−1 ∼ Gamma(φ, φ/µt ).                               (7)


Note that a well-known relationship (cf. Appendix A) between the Gamma distribution
and the Generalized Error Distribution (GED) suggests an equivalent formulation which
2       THE UNIVARIATE MEM RECONSIDERED                                                                6


may prove useful for estimation purposes. We have:

            xt |Ft−1 ∼ Gamma(φ, φ/µt ) ⇔ xφt |Ft−1 ∼ Half − GED(0, µφt , φ).                         (8)

Thus the conditional density of xt has a correspondence in a conditional density of xφt ,
that is
                                      xφt = µφt νt                                  (9)
where
                                   νt |Ft−1 ∼ Half − GED(0, 1, φ).                                  (10)
This result somewhat generalizes the practice, suggested by Engle and Russell (1998), to
estimate an ACD model by estimating the parameters of the second moment of the square
root of the durations (imposing mean zero) with a GARCH routine assuming normality
of the errors. The result extends to any estimation carried out on observations xφt (with φ
known)1 assuming a GED distribution with mean zero and dispersion parameter µφt .

As per the specification of µt , following, again, Engle (2002) and Engle and Gallo (2006),
let us consider the simplest GARCH–type of order (1,1) formulation, and, for the time
being, let us not include predetermined variables in the specification.

The base (1, 1) specification of µt is

                                         µt = ω + αxt−1 + βµt−1 ,                                   (11)

which is appropriate when, say, xt is a series of positive valued variables such as finan-
cial durations or traded volumes. An extended specification is appropriate when one can
refer to the sign of financial returns as relevant extra information: well-known empirical
evidence suggests a symmetric distribution for the returns but an asymmetric response of
conditional variance to negative innovations (so–called leverage effect). Thus, when con-
sidering volatility related variables (absolute or squared returns, realized volatility, range,
etc.), asymmetric effects can be inserted in the form:
                                  1/2
                µt = ω ∗ + α(xt−1 sign(rt−1 ) + δ ∗ )2 + γxt−1 I(rt−1 < 0) + βµt−1 ,                (12)

where δ ∗ and γ are parameters that capture the asymmetry. Following Engle and Gallo
(2006), we have inserted in the model two variants of existing specifications for including
asymmetric effects in the GARCH literature: an APARCH-like asymmetry effect (the
squared term with δ ∗ – see Ding et al. (1993)) and a GJR-like asymmetry effect (the last
term with γ – see Glosten et al. (1993)). Computing the square, expression (12) can be
rewritten as
                                             (−)      (s)
                       µt = ω + αxt−1 + γxt−1 + δxt−1 + βµt−1 ,                        (13)
          (s)      1/2               (−)
where xt = xt            sign(rt ), xt     = xt I(rt < 0), ω = ω ∗ + αδ ∗2 and δ = 2αδ ∗ .
    1
   In raising the observations xt to the φ makes the ensuing model more similar to the Ding et al. (1993)
APARCH specification except that the exponent parameter appears also in the error distribution.
2       THE UNIVARIATE MEM RECONSIDERED                                                                       7


Both specifications can be written compactly as

                                      µt = ω + x∗0   ∗
                                                t−1 α + βµt−1 .                                           (14)
                                           (−)    (s)
where, if we assume x∗t = (xt ; xt ; xt ) and α∗ = (α; γ; δ) we get the asymmetric
specification (13)2 . The base specification (11) can be retrieved more simply taking x∗t =
xt and α∗ = α.

The parameter space for θ = (ω; α∗ ; β) must be restricted in order to ensure that µt ≥ 0
for all t and to ensure stationary distributions for xt . However restrictions depend on the
formulation taken into account: we consider here formulation (13). Sufficient conditions
for stationarity can been obtained taking the unconditional expectation of both members
of (14) and solving for E(xt ) = µ. This implies xt stationary if

                                            α + β + γ/2 < 1.


Sufficient conditions for non–negativity of µt can be obtained taking β ≥ 0 and imposing
                               1/2
ω + αxt + γxt I(rt < 0) + δxt sign(rt ) ≥ 0 for all xt ’s and rt ’s. Using proposition 1,
one can verify that these conditions are satisfied when

                                         β ≥ 0,  α ≥ 0,      α + γ ≥ 0,
                          if α = 0 then δ ≥ 0;  if α + δ > 0 then δ ≤ 0,
                                                                                                          (15)
                     δ 2 I(δ < 0) I(α > 0) I(δ > 0) I(α + γ > 0)
                                                                 
                  ω−                        +                       ≥ 0.
                     4            α                 α+γ


2.2      Estimation and Inference

Let us introduce estimation and inference issues by discussing first the role of a generic
observation xt . From (7), the contribution of xt to the log–likelihood function is

               lt = ln Lt = φ ln φ − ln Γ(φ) + (φ − 1) ln xt − φ(ln µt + xt /µt ).

                                                               
                                                         st,θ
The contribution of xt to the score is st =                         with components
                                                         st,φ
                                                                    
                                                        x t − µt
                         st,θ = ∇θ lt = φ∇θ µt
                                                            µ2t
                                                                                 
                                                                             xt           xt
                        st,φ = ∇φ lt = ln φ + 1 − ψ(φ) + ln                           −      ,
                                                                             µt           µt

                   Γ0 (φ)
where ψ(φ) =              is the digamma function and the operator ∇λ denotes the derivatives
                   Γ(φ)
    2
   The following notation applies: by (x1 ; . . . ; xn ) we mean that the matrices xi , all with the same
number of columns, are stacked vertically; by (x1 , . . . , xn ) we indicate that the matrices xi , all with the
same number of rows are stacked horizontally
2   THE UNIVARIATE MEM RECONSIDERED                                                         8


with respect to (the components of) λ.
                                                                   
                                                     Ht,θθ0 Ht,θφ
The contribution of xt to the Hessian is Ht =                           with components
                                                     H0t,θφ Ht,φφ
                                                                                   
                                    −2xt + µt                    x t − µt
            Ht,θθ0 = ∇θθ0 lt = φ         3
                                               ∇ θ µt ∇ θ 0 µt +          ∇θθ0 µt
                                        µt                           µ2t
                               x t − µt
             Ht,θφ = ∇θφ lt =           ∇ θ µt
                                   µ2t
                               1
             Ht,φφ = ∇φφ lt = − ψ 0 (φ),
                               φ

where ψ 0 (φ) is the trigamma function.

Exploiting the previous results we obtain the following first order conditions for θ and φ:
                                             T
                                           1X          x t − µt
                                                ∇ θ µt           =0                       (17)
                                          T t=1            µ2t
                                          T                 
                                       1X        xt         xt
                     ln φ + 1 − ψ(φ) +       ln         −        =0                       (18)
                                       T t=1     µt         µt


As noted by Engle and Gallo (2006), first–order conditions for θ do not depend on φ. As
a consequence, whatever value φ may take, any Gamma-based MEM or any equivalent
GED-based power formulation will provide the same point estimates for θ. The ML
estimation of φ can then be performed after θ has been estimated.

Furthermore, so long as µt = E(xt |Ft−1 ), the expected value of the score of θ evaluated
at the true parameters is a vector of zeroes even if the density of εt |Ft−1 does not belong
to the Gamma(φ, φ) family.

Altogether, these considerations strengthen the claim (e.g. Engle (2002) for the case
φ = 1) that, whatever the value of φ, the log–likelihood functions of any one of these
formulations (MEM or equivalent power formulation) can be interpreted as Quasi Likeli-
hood functions and the corresponding estimator θb is a QML estimator.

After some computations we obtain the asymptotic variance–covariance matrix of the ML
estimator                                                      −1
                                  T
                               1 X    1
                      φ lim              ∇ µ ∇ 0µ
                                        2 θ t θ t
                                                          0     
                       t→∞ T         µ
               V∞ =                    t                        .
                                                                
                                 t=1
                                                      0      1
                                     0                ψ (φ) −
                                                              φ
Note that even if φ is not involved in the estimate of θ the variance of θb is proportional to
1/φ. We note also that the ML estimators θb and φb are asymptotically uncorrelated.

Alternative estimators of φ are available, which exploit the orthogonal nature of the pa-
2       THE UNIVARIATE MEM RECONSIDERED                                                               9


rameters θ and φ. As an example, by defining ut = xt /µt −1, we note that3 V (ut |Ft−1 ) =
φ−1 . Therefore, a simple method of moment estimator is

                                                 T
                                           −1
                                              1X 2
                                          φ =
                                          d        û .                                             (19)
                                              T t=1 t

In view of (18), this expression has the advantage of not being affected by the presence of
xt ’s equal to zero (cf. also the comments below).

Obviously, the inference about (θ, φ) must be based on estimates of V∞ , that can be
obtained evaluating the average Hessian or the average outer product of the gradients
evaluated at the estimates (θ,
                            b φ).
                               b The sandwich estimator

                                            b −1 OPG
                                      Vb∞ = H    \ H   b −1 ,
                                              T      T   T

                    T                       T
                1Xb             \        1X 0
where HT =
      b                Ht and OPGT =          ŝt ŝ , get rid of the dependence of the
                T t=1                    T t=1 t
submatrix relative to θ on φ altogether.


2.3      Some Further Comments

2.3.1     MEM as member of a more general family of models

The MEM defined in section 2.1 belongs to the Generalized Linear Autoregressive Mov-
ing Average (GLARMA) family of models introduced by Shephard (1995) and extended
by Benjamin et al. (2003)4 . In the GLARMA model, the conditional density of xt is
assumed to belong to the same exponential family, with density given by5

                        f (xt |Ft−1 ) = exp [φ (xt ϑt − b(ϑt )) + d(xt , φ)] .                      (20)

ϑt and φ are the canonical and precision parameters respectively, whereas b(·) and d(·)
are specific functions that define the particular distribution into the family. The main
moments are

                               E(xt |Ft−1 ) = b0 (ϑt ) = µt
                               V (xt |Ft−1 ) = b00 (ϑt )/φ = v(µt )/φ.

Since µt may have a bounded domain, it is useful to define its (p, q) dynamics through
a twice differentiable and monotonic link function g(·), introduced in general terms by
    3
      We thank Neil Shephard for pointing this out to us.
    4
      For other works related to GLARMA models see, among others, Li (1994), Davis et al. (2002).
    5
      We adapt the formulation of Benjamin et al. (2003) to this work.
2   THE UNIVARIATE MEM RECONSIDERED                                                              10


Benjamin et al. (2003) as
                                    p                               q
                                    X                               X
             g(µt ) =   z0t γ   +         αi A(xt−i , zt−i , γ) +         βj M(xt−j , µt−j ),
                                    i=1                             j=1


where A(·) and M(·) are functions representing the autoregressive and moving average
terms. Such a formulation, admittedly too general for practical purposes, can be replaced
by a more practical version:
                            p                               q
                            X                             X
         g(µt ) = z0t γ +           αi g(xt−i ) − z0t−i γ +
                                      
                                                              βj [g(xt−j ) − g(µt−j )] .        (21)
                            i=1                                 j=1

In this formulation, for certain functions g it may be necessary to replace some xt−i ’s by
x∗t−i to avoid the nonexistence of g(xt−i ) for certain values of the argument (e.g. zeros,
see Benjamin et al. (2003) for some examples).

Specifying (20) as a Gamma(φ, φ/µt ) density (in this case v(µt ) = µ2t ) and g(·) as the
identity function, after some simple arrangements of (21) it is easily verified that the MEM
is a particular GLARMA model.


2.3.2   Exact zero values in the data

In introducing the MEM for modelling non–negative time series, Engle (2002) states that
the structure of the model avoids problems caused by exact zeros in {xt }, problems that,
instead, are typical of log(xt ) formulations. This needs to be further clarified here.

Considering the MEM as defined by formula (1), in principle an xt = 0 can be caused by
εt = 0, by µt = 0 or by both. We discuss these possibilities in turn.

    • As the process µt is supposed deterministic conditionally to the information Ft−1
      (section 2.1), the value of µt cannot be ’altered’ from any of the observations after
      time t − 1: in practice it is not possible to take µt = 0 if xt = 0. Hence: or µt is
      really 0 (very unlikely!) or an xt = 0 cannot be a consequence of µt = 0. As a
      second observation, a possible µt = 0 causes serious problems to the inference, as
      evidenced by the first–order conditions (17).

    • The only possibility is then εt = 0. However, as we assumed εt |Ft−1 ∼ Gamma(φ, φ),
      this distribution cannot be defined for 0 values when φ < 1. Furthermore, even re-
      stricting the parameter space to φ ≥ 1, the first order condition for φ in (18) requires
      the computation of ln(xt ), not defined for exact zero values. In practice the ML es-
      timation of φ is not feasible in presence of zeros which enforces the usefulness of
      the method of moments estimator introduced above.

The formulation of the MEM considered by Engle (2002) avoid problems caused by exact
zeros because it assume εt |Ft−1 ∼ Exponential(1), the density of which, f (εt |Ft−1 ) =
3       THE VECTOR MEM                                                                                    11


e−εt , can be defined for εt ≥ 0. Obviously, any MEM with a fixed φ ≥ 1 shares this
characteristic. We guess that, despite the QML nature of the θb estimator in section 2.2,
when there are many zeros in the data, more appropriate inference can be obtained if we
structure the conditional distribution of εt as a mixture between a discrete component (at
0) and an absolutely continuous component (on (0, ∞)).



3        The vector MEM

As introduced in Engle (2002), a vector MEM is a simple generalization of the univariate
MEM. It is a suitable representation of non–negative valued processes which have dy-
namic interactions with one another. The application in Engle and Gallo (2006) envisages
three indicators of volatility (absolute returns, daily range and realized volatility), while
the study in Engle et al. (2005) takes daily absolute returns on seven East Asian markets
to study contagion. In all cases, the model is estimated equation by equation.


3.1      Definition and formulations

Let xt a K–dimensional process with non–negative components;6 a vector MEM for xt
is defined as
                           xt = µt εt = diag(µt )εt ,                        (22)
where indicates the Hadamard (element–by–element) product. Conditional on the in-
formation Ft−1 , µt is defined as in (2) except that now we are dealing with a K– dimen-
sional vector depending on a (larger) vector of parameters θ. The innovation vector εt is a
conditionally stochastic K–dimensional i.i.d. process. Its density function is defined over
a [0, +∞)K support, with unit vector 1 as expectation and a general variance–covariance
matrix Σ,
                                    εt |Ft−1 ∼ D(1, Σ).                                (23)

The previous conditions guarantee that

                        E(xt |Ft−1 ) = µt                                                               (24)
                        V (xt |Ft−1 ) = µt µ0t      Σ = diag(µt )Σ diag(µt ),                           (25)

which is a positive definite matrix by construction, as emphasized by Engle (2002).


3.2      Specifications for εt

In this section we consider some alternatives about the specification of the distribution
of the error term εt |Ft−1 of the vector MEM defined above. The natural extension to
    6
    In what follows we will adopt the convention that if x is a vector or a matrix and a is a scalar, then the
expressions x ≥ 0 and xa are meant element by element.
3       THE VECTOR MEM                                                                      12


be considered is to limit ourselves to the assumption that εi,t |Ft−1 ∼ Gamma(φi , φi ),
i = 1, . . . , K.


3.2.1       Multivariate Gamma distributions

Johnson et al. (2000, chapter 48) describe a number of multivariate generalizations of the
univariate Gamma distribution. However, many of them are bivariate versions, not suffi-
ciently general for our purposes. Among the others, we do not consider here distributions
defined via the joint characteristic function, as they require numerical inversion formulas
to find their pdf’s. Hence, the only useful versions remain the multivariate Gammas of
Cheriyan and Ramabhadran (in their more general version, henceforth GammaCR, see
appendix C.1 for the details), of Kowalckzyk and Trycha and of Mathai and Moschopou-
los (Johnson et al. (2000, 454–470)). If one wants the domain of εt to be defined on
[0, +∞)K , it can be shown that the three mentioned versions are perfectly equivalent.7
As a consequence, we will consider the following multivariate Gamma assumption for
the conditional distribution of the innovation term εt

                                   εt |Ft−1 ∼ GammaCR(φ0 , φ, φ),

where φ = (φ1 ; . . . ; φK ) and 0 < φ0 < min(φ1 , . . . , φK ). As described in the appendix,
all univariate marginal probability functions for εi,t are, as required, Gamma(φi , φi ), even
if the multivariate pdf is expressed in terms of a complicated integral. The conditional
variance matrix of εt has elements
                                                                   φ0
                                        C(εi,t , εj,t |Ft−1 ) =                           (26)
                                                                  φi φj

so that the correlations are
                                                                φ0
                                       ρ(εi,t , εj,t |Ft−1 ) = p       .                  (27)
                                                                 φi φj

This implies that the GammaCR distribution admits only positive correlation among its
components and that the correlation between each couple of elements is strictly linked
to the corresponding variances 1/φi and 1/φj . These various drawbacks (the restrictions
on the correlation, the very complicated pdf and the constraint φ0 < min(φ1 , . . . , φK )),
suggest to investigate better alternatives.


3.2.2       The Normal copula-Gamma marginals distribution

A different way to define the distribution of εt |Ft−1 is to start from the assumption that all
univariate marginal probability density functions are Gamma(φi , φi ), i = 1, . . . , K and
to use copula functions.8 Adopting copulas, the definition of the distribution of a multi-
    7
        The proof is tedious and is available upon request.
    8
        The main characteristics of copulas are summarized in appendix B.
3       THE VECTOR MEM                                                                                   13


variate r.v. is completed by defining the copula that represents the structure of dependence
among the univariate marginal pdf’s. Many copula functions have been proposed in the-
oretical and applied works (see, among others, Embrechts et al. (2001) and Bouyé et al.
(2000)). In particular, the Normal copula possesses many interesting properties: the ca-
pability of capture a broad range of dependencies9 , the analytical tractability, the easy of
simulation. For simplicity, we will assume
                                                      K
                                                      Y
                             εt |Ft−1 ∼ N (R) −             Gamma(φi , φi ).                           (28)
                                                      i=1

where in the distribution, the first part refers to the copula (R is a correlation matrix) and
the second part to the marginals. As detailed in appendix C.2, this distribution is a special
case of multivariate dispersion distributions generated from a Gaussian copula discussed
in Song (2000). The conditional variance–covariance matrix of εt has a generic element
which is approximately equal to
                                                              Rij
                                     C(εi,t , εj,t |Ft−1 ) ' p       .
                                                               φi φj

so that the correlations are, approximately,

                                        ρ(εi,t , εj,t |Ft−1 ) ' Rij .

The advantages of using copulas over a multivariate Gamma specification are apparent
and suggest their adoption in this context: the covariance and correlation structures are
more flexible (also negative correlations are permitted); the correlations do not depend on
the variances of the marginals; there are no complicated constraints on the parameters;
the pdf is more easily tractable. Furthermore, by adopting copula functions, the frame-
work considered here can be easily extended to different choices of the distribution of the
marginal pdf’s (Weibull for instance).


3.3      Specification for µt

The base (1, 1) specification for µt can be written as

                                     µt = ω + αxt−1 + βµt−1 ,                                          (29)

where ω, α and β have dimensions, respectively, (K, 1), (K, K) and (K, K).

The enlarged (1, 1) multivariate specification will include lagged cross-effects and may
include asymmetric effects, defined as before. For example, when the first component of
                                                           1/2
xt is x1,t = rt2 and the conditional distribution of rt = x1,t sign(rt ) is symmetric with
    9
     The bivariate Normal copula, according to the value of the correlation parameter is capable of attaining
the lower Frechèt bound, the product copula and the upper Frechèt bound.
3   THE VECTOR MEM                                                                      14


zero mean, generalizing formulation (13) we have
                                               (−)      (s)
                       µt = ω + αxt−1 + γxt−1 + δxt−1 + βµt−1 ,                       (30)
         (−)                  (s)     1/2
where xt = xt I(rt < 0), xt = xt sign(rt ). Both parameters δ and γ have dimension
(K, K), whence the others are as before.

Another specification can be taken into account when the purpose is to model contagion
among volatilities of different markets (Engle et al., 2005). For example, we consider
the case in which each component of µt is the conditional variance of the corresponding
                               2
market index, so that xi,t = ri,t . If we assume that the conditional distribution of each
                       1/2
market index ri,t = xi,t sign(ri,t ) is symmetric with zero mean, the ’contagion’ (1,1)
                                                          (−)                        (s)
formulation can be structured exactly as in (30) where xi,t = xi,t I(rit < 0) and xi,t =
  1/2
xi,t sign(ri,t ).

The base, the enlarged and the contagion (1, 1) specifications can be written compactly as

                                µt = ω + α∗ x∗t−1 + βµt−1 .                           (31)
                                                                      (−)   (s)
Taking x∗t = xt and α∗ = α we have (29); considering x∗t = (xt ; xt ; xt ) (of dimen-
sion (3K, 1)) and α∗ = (α; γ; δ) (of dimension (K, 3K)), we obtain (30).

Considering formulation (31), we have θ = (ω; α∗ ; β). The parameter space of θ must
be restricted to ensure µt ≥ 0 for all t and to ensure stationary distributions for xt . To
discuss these, we consider formulation (30).

Sufficient conditions the stationarity of µt are a simple generalization of those showed
in section 2.1 for the univariate MEM and can be obtained in an analogous way: xt is
stationary if all characteristic roots of A = α + β + γ/2 are smaller than 1 in modulus.
We can think of A as the impact matrix in the expression

                                        µt = Aµt−1 .                                  (32)


Sufficient conditions for non–negativity of the components of µt are again a generaliza-
tion of the corresponding conditions of the univariate model, but the derivation is more
involved. As proved in appendix E, the vector MEM defined in equation (30) gives µt ≥ 0
for all t if all the following conditions are satisfied for all i, j = 1, . . . , K:

    1. βij ≥ 0, αij ≥ 0, αij + γ2ij ≥ 0 for all j;

    2. if αij = 0 then δij ≥ 0; if αij + γij = 0 then δij ≤ 0;
               K                                                    
            1 X 2 I(δij < 0) I(αij > 0) I(δij > 0) I(αij + γij > 0)
    3. ωi −      δ                     +                            . ≥0
            4 j=1 ij       αij                   αij + γij
3     THE VECTOR MEM                                                                                   15


3.4     Maximum likelihood inference

Given the assumptions about the conditional distribution of εt (section 3.2.2), we have
that εt |Ft−1 has a pdf
                                                                         K
                                                                         Y
                     f (εt |Ft−1 ) = c(F1 (ε1,t ), . . . , FK (εK,t ))         fi (εi,t )
                                                                         i=1

where fi (.) and Fi (.) indicate, respectively, the conditional pdf and cdf of the i-th com-
ponent of εt , c(.) is the density function of the chosen copula. In the case considered
here,

                                            φφi i φi −1
                              fi (εi,t ) =        ε     exp(−φi εi,t )
                                           Γ(φi ) i,t


                                      Fi (εi,t ) = Γ(φi ; φi εi,t )

                                                                 
                                        −1/2        1 0 −1
                          c(ut ) = |R|         exp − qt (R − I)qt
                                                    2

where Γ(ζ; x) indicates the incomplete Gamma function with parameter ζ computed at x
(or, in other words, the cdf of a Gamma(ζ, 1) r.v. computed at x),

                         qt = (Φ−1 (F1 (ε1,t )); . . . ; Φ−1 (FK (εK,t ))),

Φ−1 (x) indicates the quantile function of the standard normal distribution computed at x.

Hence, xt |Ft−1 has pdf
                                                                           K
                                                                           Y fi (xi,t /µi,t )
           f (xt |Ft−1 ) = c(F1 (x1,t /µ1,t ), . . . , FK (xK,t /µK,t ))                           ,
                                                                           i=1
                                                                                            µi,t

where
                                             fi (xi,t /µi,t )
                                                  µi,t

is the pdf of a Gamma(φi , φi /µi,t ).
3   THE VECTOR MEM                                                                                          16


3.4.1   The log–likelihood

The log–likelihood of the model is then
                                       T
                                       X            T
                                                    X
                                  l=         lt =         ln f (xt |Ft−1 ),
                                       t=1          t=1

where
                                                                   K
                                                                   X
        lt = ln c(F1 (x1,t /µ1,t ), . . . , FK (xK,t /µK,t )) +          (ln fi (xi,t /µi,t ) − ln µi,t )
                                                                   i=1
            1              1              1
          =   ln |R−1 | − q0t R−1 qt + q0t qt
            2              2              2
             K                                                               
            X                                                             xi,t
          +       φi ln φi − ln Γ(φi ) − ln xi,t + φi ln xi,t − ln µi,t −         .
            i=1
                                                                          µi,t
          = (copula contribution)t + (marginals contribution)t .


The contribution of the t-th observation to the inference can then be decomposed in the
contribution of the copula plus the contribution of the marginals. The contribution of the
marginals depends only on θ and φ, whereas the contribution of the copula depends on
R, θ, φ. This implies
                                                           0
                               ∂l    1         0      b = q q,
                                   =   (T R − q  q) ⇒ R
                              ∂R−1   2                     T
where q = (q01 ; . . . ; q0T ) is a T × K matrix. Hence the unconstrained ML estimator of R
has an explicit form.

Replacing the estimated R in the log-likelihood function we obtain a concentrated log-
likelihood
                                       T
                  T    b −1
                             X
            lc = − ln |R|        qt0 (R
                                      b −1 − I)qt + (marginals contribution)
                  2        2 t=1
                  T    b − T K − trace(R)
                             h                 i
               = − ln |R|                   b + (marginals contribution)
                  2        2

In deriving the concentrated log–likelihood, R is estimated without imposing any con-
straint relative to its nature as correlation matrix (diag(R) = 1 and positive definiteness).
Computing directly the derivatives with respect to the off–diagonal elements of R we
obtain, after some algebra, that the ML estimate of R satisfies the following equations:

                                                          q0 q −1
                               (R−1 )ij − (R−1 )i.            (R ).j = 0
                                                           T
for i 6= j = 1, . . . , K, where Ri. and R.j indicate, respectively, the i–th row and the j–th
3        THE VECTOR MEM                                                                                       17


column of the matrix R. Unfortunately, these equations do not have an explicit solution.10

An acceptable compromise which should increase efficiency, although formally it cannot
be interpreted as an ML estimator, is to adopt the sample correlation matrix of the qt ’s as
a constrained estimator of R, that is
                                                              1      1
                                                 e = D− 2 QD− 2 .
                                                 R    Q     Q

where
                              q0 q
                                 Q=   DQ = diag(Q11 , . . . , QKK ).
                               T
This solution can be justified observing that the copula contribution to the likelihood de-
pends on R exactly as if it were the correlation matrix of i.i.d. r.v. qt normally distributed
with mean 0 and correlation matrix R. Using this new estimate of R, the trace of R        e is
now constrained to K and the concentrated log-likelihood simplifies to
                         T
                   lc = −  ln |R|
                               e + (marginals contribution)
                         2"                               #
                                          K
                         T               X
                      =−     ln |q0 q| −     ln(q0.i q.i ) + (marginals contribution).
                         2               i=1



3.4.2        Derivatives of the concentrated log-likelihood

In what follows, let us consider λ = (θ; φ). As
                                                          T
                                                      02X
                                         ∇λ ln |q q| =       ∇λ q0t Q−1 qt
                                                       T t=1
                                       K                          T
                                       X                       2X
                                  ∇λ         ln(q0.i q.i )   =       ∇λ q0t D−1
                                                                             Q qt
                                       i=1
                                                               T t=1

the derivative of lc is given by
                        T
                        X
                           ∇λ q0t [D−1   −1
                                                                           
                ∇λ lc =             Q − Q ]qt + ∇λ (marginals contribution)t .
                           t=1

    10
         Even when R is a (2, 2) matrix, the value of R12 has to satisfy a cubic equation as the following:

                                        q 0 q2
                                                      0
                                                              q 0 q2         q 0 q2
                                                                        
                                                      q q1
                           R312 − R212 1 + R12 1 + 2 − 1 − 1 = 0.
                                          T            T        T              T
3   THE VECTOR MEM                                                                                18


To develop further this formula we need to distinguish the derivatives ∇λ q0t and ∇λ (marginals contribution)t
with respect to θ (the parameters of µt ) and φ. After some algebra we obtain

                                                  ∇θ q0t          = −∇θ µ0t D1t
                            ∇θ (marginals contribution)t          = ∇θ µ0t v1t
                                                  ∇φ q0t          = D2t
                            ∇φ (marginals contribution)t          = v2t ,

where
                                                                      
                                    fi (b
                                        εi,t )b
                                              εi,t
                 D1t    = diag                     : i = 1, . . . , K
                                    φ(qi,t )µi,t
                                                               
                               εbi,t − 1
                  v1t   = φi               : i = 1, . . . , K
                                  µi,t
                                                                              
                                        1 ∂Fi (b    εi,t )
                 D2t    = diag                              : i = 1, . . . , K
                                    φ(qi,t ) ∂φi
                  v2t   = (ln φi − ψ(φi ) + ln(b       εi,t ) − εbi,t + 1 : i = 1, . . . , K) .

and εbi,t = xi,t /µi,t . Then
                                   T
                                   X
                                           ∇θ µ0t −D1t (D−1   −1
                                                                        
                        ∇θ lc =                          Q − Q )qt + v1t
                                     t=1
                                T
                                X
                                   D2t (D−1   −1
                                                        
                        ∇φ lc =          Q − Q )qt + v2t
                                     t=1

                                       ∂Fi (b
                                            εi,t )
We remark that the derivatives                     must be computed numerically.
                                         ∂φi


3.4.3   Details about the estimation of θ

The estimation of the parameter θ involved in the equation of µt requires further details.
We explain them considering the more general version of µt defined in section 3.3
                                                         (−)         (s)
                           µt = ω + αxt−1 + γxt−1 + δxt−1 + βµt−1
                              = ω + α∗ x∗t−1 + βµt−1

(see the cited section for details and notation). Such a structure for µt depends in the gen-
eral case from K + 4K 2 parameters. For instance, when K = 3 there are 39 parameters.
We think that, in general, data do not provide enough information to capture asymme-
                       (s)        (−)
tries of both types (xt−1 and xt−1 ) but even removing one of these two components the
parameters become K + 3K 2 (30 when K = 3).

A reduction in the number of parameters can obtained estimating ω from stationary con-
3   THE VECTOR MEM                                                                          19


ditions. Imposing that µt is stationary we have

                                ω = [I − (α + β + γ/2)]µ,

where µ = E(xt ), and then
                                                (−)              (s)
          (µt − µ) = α(xt−1 − µ) + γ(xt−1 − µ/2) + δxt−1 + β(µt−1 − µ).

Replacing µ with its natural estimate, that is the unconditional average x, we obtain
                                             (−)     (s)
                          µ
                          e t = αe
                                 xt−1 + γe xt−1 + δe
                                                   xt−1 + β µ
                                                            e t−1
                                                                                          (33)
                              = α∗ x
                                   e∗t−1 + β µ
                                             e t−1

where the symbol x   e means the demeaned version of x. This solution save K parameters
in the iterative estimation and provides better performances than direct ML estimates of
ω in simulations.

To save time it is also useful take into account analytic derivatives of µ
                                                                         e t with respect to
parameters. To this purpose rewrite (33) as

                           µ    e 0 vec(β 0 ) + A
                           et = B               e ∗0 vec(α∗0 ),
                                  t−1             t−1

where                                                                         
                      µ
                      et 0        ...    0                   e∗t 0
                                                             x         ...    0
                    0 µ et       ...    0             ∗
                                                           0 x  e∗t   ...    0
               B
               et =                                  At =  ..
                                                                               
                     .. ..       ..     ..          e           ..   ..     .. 
                    .    .          .    .               .      .      .    .
                      0 0         ... µ
                                      et                     0 0           e∗t
                                                                       ... x
and the operator vec(.) stacks the columns of the matrix inside brackets. Taking deriva-
tives with respect to vec(β 0 ) and vec(α∗0 ) and arranging results we obtain
                                                    !
                                 ∂µet         B
                                              e t−1      ∂µe t−1
                                        = e∗          +
                               ∂ vec(θ)      At−1       ∂ vec(θ)

where vec(θ) = (vec(β 0 ); vec(α∗0 )).


3.5     Estimating Functions inference

A different approach to estimation and inference of a vector MEM is provided by Esti-
mating Functions (Heyde, 1997) which has less demanding assumptions relative to ML:
in the simplest version, Estimating Functions require only the specification of the first two
conditional moments of xt , as in (24) and (25). The advantage of not formulating assump-
tions about the shape of the conditional distribution of xt is balanced by a loss in efficiency
with respect to Maximum likelihood under correct specification of the complete model.
Given that even in the univariate MEM we had interpreted the ML estimator as QML, the
flexibility provided by the Estimating Functions (henceforth EF) seems promising.
3   THE VECTOR MEM                                                                         20


3.5.1   Framework

We consider here the EF inference of the vector MEM defined by (22) and (23) or,
equivalently, by (24) and (25). In this framework the parameters are collected in the
p∗ −dimensional vector λ = (θ; upmat(Σ)), with p∗ = p + K(K + 1)/2: θ includes
the parameters of primary interest involved in the expression of µt ; upmat(Σ) includes
the elements inside and above the main diagonal of the conditional variance–covariance
matrix Σ of the multiplicative innovations εt and represents a nuisance parameter with
respect to θ. However, to simplify the exposition, from now on we denote the parameters
of the model as λ = (θ; Σ), instead of λ = (θ; upmat(Σ))).

Following Bibby et al. (2004), an estimating function for the p∗ -dimensional vector λ
based on a sample x(T ) is a p∗ −dimensional function denoted as

                               g(λ; x(T ) ) in short    g(λ).

The EF estimator λ
                 b is defined as the solution to the corresponding estimating equation:

                                   λ
                                   b such that g(λ)
                                                 b = 0.                                  (34)

To be an useful estimating function, regularity conditions on g are usually imposed. Fol-
lowing Heyde (1997, sect. 2.6) or Bibby et al. (2004, Theorem 2.3), we consider zero
mean and square integrable martingale estimating functions that are sufficiently regular
to guarantee that a WLLN and a CLT applies. The martingale restriction is often moti-
vated observing that the score function, if it exists, is usually a martingale: so it is quite
natural to approximate it using families of martingales estimating functions. Furthermore,
in modelling time series processes, estimating functions that are martingales arise quite
naturally from the structure of the process. We denote as M the class of EF satisfying the
asserted regularity conditions.

Following the decomposition of λ into θ and Σ, let us decompose the estimating function
g(λ) as                                          
                                            g1 (λ)
                                 g(λ) =              :                             (35)
                                            g2 (λ)
g1 has dimension p and refers to θ; g2 has dimension K(K + 1)/2 and refers to Σ.


3.5.2   Inference on θ

From (24) and (25) we obtain immediately that “residuals” ut = xt µt − 1, where
denotes the element by element division, have the property to be martingale differences
with a constant conditional variance matrix: in fact

                                     ut |Ft−1 ∼ [0, Σ].                                  (36)
3   THE VECTOR MEM                                                                       21


Using these as basic ingredient, we can construct the Hutton–Nelson quasi-score function
(Heyde (1997, p. 32)) as an estimating function for θ:
                               T
                               X
                  g1 (λ) = −         E [∇θ u0t |Ft−1 ] V [ut |Ft−1 ]−1 ut
                               t=1
                             T
                             X
                         =         ∇θ µ0t [diag(µt )Σ diag(µt )]−1 (xt − µt ).         (37)
                             t=1

Transposing to the vector MEM considered here the arguments in Heyde (1997), (37) is
(for fixed Σ) the ”best” estimating function for θ in the subclass of M composed by all
estimating functions that are linear in the residuals ut = xt µt − 1. In fact, more
precisely, the Hutton–Nelson quasi score function is optimal (both in the asymptotic and
in the fixed sample sense – see Heyde (1997, ch. 2) for details) in the class of estimating
functions                     (                                    )
                                                  XT
                     M(1) = g ∈ M : g(λ) =             at (λ)ut (λ) ,
                                                        t=1

where ut (λ) are (K, 1) martingale differences and at (λ) are (p, K) Ft−1 –measurable
functions.

Very interestingly, in the 1-dimensional case (37) specializes to
                                               T
                                               X              x t − µt
                              g1 (λ) = σ −2          ∇ θ µt            ,
                                               t=1
                                                                  µ2t

which provides exactly the first order condition of the univariate MEM. Hence, formula
(17) can be also justified from an EF perspective. The main substantial difference from
the multivariate case, is that in the vector MEM explicit dependence from the nuisance
parameter Σ cannot be suppressed. About this point, however, we claim that EF infer-
ences of vector MEM are not too seriously affected by the presence of Σ, as stated in the
following section.


3.5.3   Inference on Σ

A clear discussion of inferential issues with nuisance parameters can be found, among
others, in Liang and Zeger (1995). The main inferential problem is that, in presence
of nuisance parameters, some properties of estimating functions are no longer valid if
we replace parameters with the corresponding estimators. For instance, unbiasedness of
g1 (λ) is not guaranteed if Σ is replaced by an estimator Σ.
                                                          b By consequence, optimality
properties also are not guaranteed.

An interesting statistical handling of nuisance parameter in the estimating functions frame-
work is provided in Knudsen (1999) and Jørgensen and Knudsen (2004). Their handling
parallels in some aspects the notion of Fisher-orthogonality or, shortly, F-orthogonality,
in ML estimation. F-orthogonality is defined by block diagonality of the Fisher informa-
3   THE VECTOR MEM                                                                       22


tion matrix for λ = (θ; Σ). This particular structure guarantees the following properties
of the ML estimator (see the cited reference for details):

    1. asymptotic independence of θb and Σ;
                                         b

    2. efficiency-stable estimation of θ, in the sense that the asymptotic variance for θ is
       the same whether Σ is treated as known or unknown;
    3. simplification of the estimation algorithm;
    4. θ(Σ),
       b     the estimate of θ when Σ is given, varies only slowly with Σ.

Starting from this point, Jørgensen and Knudsen (2004) extend F-orthogonality to EFs
introducing the concepts of nuisance parameter insensitivity (henceforth NPI). NPI is an
extended parameter orthogonality notion for estimating function, that guarantees proper-
ties 2-4 above. Among these, Jørgensen and Knudsen (2004) state that efficiency stable
estimation is the crucial property and prove that, in a certain sense, NPI is a necessary
condition for this.

Here we emphasize two major points concerning the vector MEM:

    1. Jørgensen and Knudsen (2004) define and prove properties connected to NPI within
       a fixed sample framework. However definitions and theorems can be easily adapted
       and extended to the asymptotic framework, that is the typical reference when stochas-
       tic processes are of interest.
    2. The estimating function (37) for estimating θ can be proved nuisance parameter
       insensitive.

About the first point, key quantities for inferences on estimating functions estimators
within the fixed sample framework are the variance–covariance matrix V (g) and the
sensitivity matrix E(∇λ g0 ) of the EF g. These quantities are replaced in the asymptotic
framework by their ’estimable’ counterparts: the quadratic characteristic
                                            T
                                            X
                                  hgi =           V (gt |Ft−1 ),
                                            t=1

and the compensator,
                                      T
                                      X
                                 g=         E (∇λ0 gt |Ft−1 ) ,
                                      t=1

of g, where the martingale EF g is represented as sum of martingale differences gt :
                                                  T
                                                  X
                                        g=              gt .                           (38)
                                                  t=1

As pointed by Heyde (1997, p. 28), there is a close relation between V (g) and hgi and
between E(∇λ0 g) and g. But, above all, the optimality theory within the asymptotic
3     THE VECTOR MEM                                                                     23


framework parallels substantially that within the fixed sample framework because these
quantities share the same essential properties. These properties can be summarized taking
into account two points. The first one: under regularity conditions (see Heyde (1997),
Bibby et al. (2004) and Jørgensen and Knudsen (2004) for details) we have,

                                     E(∇λ0 g) = C(g, s)

and
                                        T
                                        X
                                   g=          C(gt , st |Ft−1 ),
                                        t=1
            T
            X
where s =         st is the score function expressed as a sum of martingale differences. The
            t=1
second one: both the covariance and the sum of conditional covariances of the martingale
difference components share the bilinearity properties typical of a scalar product. Just
by virtue of these common properties, it is possibile to show that NPI can be defined and
guarantees properties 2–4 above also in the asymptotic framework. Hence, we sketch here
only the main points and we remind to Jørgensen and Knudsen (2004) for the original full
treatment.

We partition the compensator and the quadratic characteristic conformably to g as in (35),
that is                                          
                                          g11 g12
                                   g=
                                          g21 g22
and                                                       
                                               hgi11 hgi12
                                   hgi =                     ,
                                               hgi21 hgi22
and we consider Heyde information

                                        I = g0 hgi−1 g                                 (39)

as the natural counterpart, within the asymptotic framework, of the Godambe information

                               J = E(∇λ g0 )V (g)−1 E(∇λ0 g),

that instead is typical of the fixed sample framework. We remember that each of these ver-
sions of information, in the respective framework, provides the inverse of the asymptotic
variance covariance matrix of the EF estimator.

We say that the marginal EF g1 is nuisance parameter insensitive if g12 = 0. Using block–
matrix algebra, we check easily that NPI of g1 implies that the asymptotic variance–
covariance matrix of the EF estimator of θ is given by
                                         −10
                                        g11  hgi11 g−1
                                                    11 .                               (40)

Employing concepts and definitions above is possible to show, exactly using the same
devices, that the whole theory in Jørgensen and Knudsen (2004), and in particular their
insensitivity theorem, preserves substantially unaltered.
4   EMPIRICAL ANALYSIS                                                                           24


Returning now to the vector MEM, we can discuss the second point above: the EF (37) for
estimating θ is Σ–insensitive. In fact, expressing (37) as sum of martingale differences
g1,t = ∇θ µ0t (diag(µt )Σ diag(µt ))−1 (xt − µt ) as in (38), we have

        ∇σij g1,t (λ) = −∇θ µ0t diag(µt )−1 Σ−1 ∇σij ΣΣ−1 diag(µt )−1 (xt − µt ),

whose conditional expected value is 0. This implies g12 = 0 and hence Σ-insensitivity of
the EF g1 in (37). As a consequence, the asymptotic variance–covariance matrix of θ is
given by (40), that for the model considered becomes
                                   "   T
                                                                                      #−1
                                       X
        g−10       −1      −1
         11 hgi11 g11 = hgi11 =              ∇θ µ0t [diag(µt )Σ diag(µt )]−1 ∇θ0 µt         .   (41)
                                       t=1



Expressions (37) and (41) require values for Σ. Considering the conditional distribution
of the “residuals” ut = xt µt − 1 in (36), Σ can be estimated using the estimating
function:
                                              T
                                              X
                                g2 (λ) =             (ut u0t − Σ).                              (42)
                                               t=1

This implies
                                             T
                                          1X
                                       Σ=
                                       b        ut u0t .
                                          T t=1

This estimator generalizes (19) to the multivariate model. We note that the EF (42) is
unbiased, but in general does not satisfy optimality properties: improving (42) is possible
but at the cost of adding further assumptions about higher order moments of xt |Ft−1 , and
will not be pursued here.



4    Empirical Analysis

The multivariate extension of the MEM model is illustrated on the GE stock for which the
three variables are available as in Engle and Gallo (2006), namely absolute returns |rt |,
high-low range hlt and realized volatility rvt derived from 5-minutes returns as in Ander-
sen et al. (2003). The observations were rescaled so as to have the same mean as absolute
returns.

In Table 1 we report a few descriptive statistics relative to the variables employed. As one
may expect, daily range and realized volatility have high maximum values and there are
a few zeroes in the absolute returns. We also report the unconditional correlations across
the whole sample period showing that daily range has a relatively high correlation with
both absolute returns and realized volatility (which are less correlated with one another).
Finally, the time series behavior of the series is depicted in Figure 1.
4   EMPIRICAL ANALYSIS                                                                  25




Figure 1: Time series of the indicators |rt |, hlt , rvt – GE stock, 01/03/1995-12/29/2001.




                                          (a) |rt |




                    (b) hlt                                       (c) rvt
4        EMPIRICAL ANALYSIS                                                                            26


Table 1: Some descriptive statistics of the indicators |rt |, hlt , rvt – GE stock, 01/03/1995-
12/29/2001 (1515 obs.)

                                                               Indicator
                                     Statistics        |rt |        hlt      rvt
                                           min           0       0.266      .429
                                          max       11.123       6.318     6.006
                                         mean        1.194       1.194     1.194
                            standard deviation       1.058       0.614     0.424
                                     skewness         2.20        2.03      2.65
                                      kurtosis       12.42       10.97     18.69
                                   n. of zeros          60            0        0
                                  correlations         |rt |        hlt      rvt
                                           |rt |                 0.767     0.440
                                            hlt                            0.757



We applied the three estimators, namely, the Maximum Likelihood estimator with the
Normal Copula function (labelled ‘ML–R’), the Estimating Equations-based estimator
(labelled ‘EE’) and the Maximum Likelihood estimator equation by equation (labelled
‘ML–I own’). For each method a general–to–specific model selection strategy was fol-
lowed to propose a parsimonious model. The first table (4 compares single coefficients
(with their robust standard errors) by estimation method. ML–R and EE give the same
specification for both stocks, while consistently ML–I gives a different result. For com-
parison purposes, we added an extra column by variable to include the results of the
equation–by–equation estimation imposing the specification found with the system meth-
ods (labelled ‘ML-I’).

We can note that the selected model for GE is in line with the findings of Engle and Gallo
(2006) for the S&P500, in that the daily range has significant lagged (be they overall or
asymmetric) effects on the other variables, while it is not affected by them.11 A synthetic
picture of the total influence of variables can be measured by means of an Impact Matrix
A, previously defined in (32). We propose a comparison of the estimated values of the
non–zero elements of the matrix A in Table 3.

Overall, the estimated coefficient values do not differ too much between the system esti-
mation with copulas and with estimating equations. The estimates equation by equation
are quite different even when derived from the same specification and even more so when
derived from the own selection.

Similar comments can be had for the estimated characteristic roots of the impact matrix
ruling the dynamics of the system: in this particular case, the estimated persistence is
underestimated by the equation–by–equation estimator relative to the other two methods.

A comment on the shape of the distribution as it emerges from the estimation methods
    11
   This is not to say that one should expect the same result on other assets. An extensive analysis of what
may happen when several stocks are considered is beyond the scope of the present paper.
5   CONCLUSIONS                                                                          27


based on the Gamma distribution is in order (cf. Table 5). The estimated φ parameters are
quite different from one another (there is no major difference according to whether the
system is estimated equation–by–equation or jointly) reflecting the different characteris-
tics of each variable (with a decreasing skewness passing from absolute returns to realized
volatility).



5    Conclusions

In this paper we have presented a general discussion of the vector specification of the
Multiplicative Error Models introduced by Engle (2002): a positive valued process is
seen as the product of a scale factor which follows a GARCH type specification and a
unit mean innovation process. Engle and Gallo (2006) estimate a system version of the
MEM by adopting a dynamically interdependent specification for the scale factors (each
variable enters other variables’ specifications with a lag) but keeping a diagonal variance–
covariance matrix for the Gamma–distributed innovations. The extension to a multivariate
process requires interdependence among the innovation terms. The specification in a mul-
tivariate framework cannot exploit multivariate Gamma distributions because they appear
too restrictive. The maximum likelihood estimator can be derived by setting the multivari-
ate innovation process in a copula with Gamma marginals framework. As an alternative,
which provides estimators which are not based on any distributional assumptions, we also
suggested the use of estimating equations. The empirical results with daily data on the
GE stock show the feasibility of both procedures: both system estimators provide fairly
similar values, and at the same time show deviations from the equation by equation ap-
proach.
A   USEFUL UNIVARIATE STATISTICAL DISTRIBUTIONS                                          28


A     Useful univariate statistical distributions

In this section we summarize the properties of the statistical distributions employed in the
work.


A.1    The Gamma distribution

A r. v. X follows a Gamma(φ, β) distribution if its pdf if given by

                                             β φ φ−1 −βx
                                  f (x) =        x e
                                            Γ(φ)

for x > 0 and 0 otherwise, where φ, β > 0. The main moments are

                                      E(X) = φ/β
                                      V (X) = φ/β 2 .


An useful property of a Gamma r.v. is the following:

            X ∼ Gamma(φ, β) and c > 0 ⇒ Y = cX ∼ Gamma(φ, β/c).


Some interesting special cases of a Gamma distributions are the Exponential distribution
and the χ2 distribution:

                           Exponential(β) = Gamma(1, β)
                             χ2 (n) = Gamma(n/2, 1/2).


In this work we often employ Gamma distributions restricted to have mean 1, that implies
β = φ.


A.2    The GED distribution

The Generalized Error distribution (GED) is also known as Exponential Power distribu-
tion or as Subbotin distribution.

A random variable X follow a GED(µ, σ, φ) distribution distribution if its pdf if given
by                                                         !
                                φ−1                    1/φ
                               φ                x−µ
                      f (x) =        exp −φ
                              2Γ(φ)σ              σ
B   A SUMMARY ON COPULAS                                                              29


for x ∈ R, where µ ∈ R, σ, φ > 0. The main moments are

                                  E(X) = µ
                                          Γ(3φ) 2
                                  V (X) = 2φ    σ .
                                         φ Γ(φ)

An useful property of a GED r.v. is the following:

    X ∼ GED(µ, σ, φ) and c1 , c2 ∈ R ⇒ Y = c1 + c2 X ∼ GED(c1 + c2 µ, |c2 |σ, φ).

This property qualifies the GED as a location–scale distribution.

Some interesting special cases of a GED distributions are the Laplace distribution and
the Normal distribution:

                           Laplace(µ, σ) = GED(µ, σ, 1)
                                N (µ, σ) = GED(µ, σ, 1/2).


In the work we consider in particular GED distributions with µ = 0.


A.3    Relations GED–Gamma

The following relations links the GED and Gamma distributions:
                                                     1/φ
                                      X −µ
               X ∼ GED(0, σ, φ) ⇒ Y =                      ∼ Gamma(φ, φ)
                                        σ

Useful particular cases are:

                  X ∼ GED(0, 1, φ) ⇒ Y = |X|1/φ ∼ Gamma(φ, φ)
                  X ∼ Laplace(0, 1) ⇒ Y = |X| ∼ Exponential(1)
                      X ∼ N (0, 1) ⇒ Y = X 2 ∼ χ2 (1).



B     A summary on copulas

In this section we summarize main concepts and properties of copulas in a rather informal
way and without proofs. For a more detailed and rigorous exposition see, among others,
Embrechts et al. (2001) and Bouyé et al. (2000). For a simpler treatment, we assume that
all r.v. considered in this section are absolutely continuous.
B        A SUMMARY ON COPULAS                                                               30


B.1          Definitions

Copula. Defined ”operationally”, a K–dimensional copula C is the cdf of a continuous
uniform r.v. defined on the unit hypercube [0, 1]K , in the sense that each of the univariate
components of the r.v. has U nif orm(0, 1) marginals, whenever they may be not indepen-
dent.

Copula density. A copula C is then a continuous cdf with particular characteristics. For
some purposes it is however useful the associated copula density c, defined as

                                                           ∂ K C(u)
                                             c(u) =                     .                 (43)
                                                          ∂u1 . . . ∂uK

Ordering. A copula C1 is smaller then C2 , in symbols C1 ≺ C2 if C1 (u) ≤ C2 (u)
∀u ∈ [0, 1]K . In practice this means that the picture of C1 stay below that of C2 .

Some particular copulas. Among copulas, three play an important role: the lower
Frèchet bound C − , the product copula C ⊥ and the upper Frèchet bound C + , defined
as follows 12
                                           K
                                                          !
                                          X
                          C − (u) = max      ui − K + 1, 0
                                                           i=1
                                               K
                                               Y
                                   C ⊥ (u) =         ui
                                               i=1
                                   C + (u) = min(u1 , . . . , uK ).

We could show that
                                        C − (u) ≺ C ⊥ (u) ≺ C + (u).

More in general, for any copula C,

                                         C − (u) ≺ C(u) ≺ C + (u).


The concept of ordering is interesting because is connected with measures of concordance
(like the correlation coefficient). Considering two r.v., the product copula C ⊥ (u) identifies
the situation of independence; copulas smaller then C ⊥ (u) are connected with situations
of discordance; copulas greater then C ⊥ (u) describe situations of concordance.

The ordering on the set of copulas, called concordance ordering, is however only a par-
tial ordering, since not every pair of copulas is comparable in this way. However many
important parametric families of copulas are totally ordered on the base of the values of
the parameters (examples are the Frank copula and the Normal copula).
    12
         C − is not a copula for K ≥ 3 but we use this notation for convenience.
B   A SUMMARY ON COPULAS                                                                         31


B.2     Understanding copulas

We attempt to explain the usefulness of copulas starting from two important results.

The first one is relative to these well known relations concerning univariate r.v.:

                          X ∼ F ⇒ U = F (X) ∼ U nif orm(0, 1)

and, conversely,
                            U ∼ U nif orm(0, 1) ⇒ F −1 (U ) ∼ F
where with the symbol X ∼ F we mean ’X distributed with cdf F ’. F −1 is often called
the quantile function of X.

The second one is Sklar’s theorem. Sklar’s theorem is perhaps the most important result
regarding copulas.


Theorem 1 (Sklar’s theorem). Let F a K–dimensional cdf with univariate continuous
marginals F1 , . . . , FK . Then there exist an unique K–dimensional copula C such that,
∀x ∈ RK ,
                           F (x1 , . . . , xK ) = C(F1 (x1 ), . . . , FK (xK )).
Conversely, if C is a K–dimensional copula and F1 , . . . , FK are univariate cdf , then the
function F defined above is a K–dimensional cdf with marginals F1 , . . . , FK .


With these results in mind, one can verify that if F is a cdf with univariate marginals
F1 , . . . , FK , and we take ui = Fi (xi ) then F (F1−1 (u1 ), . . . , FK−1 (uK )) is a copula that
guarantees the representation in the Sklar’s theorem. We write then

                        C(u1 , . . . , uK ) = F (F1−1 (u1 ), . . . , FK−1 (uK )).              (44)

In this framework the main result of Sklar’s theorem it is then, not the existence of a
copula representation, but the fact that this representation is unique. We note also that
given a particular cdf and its univariate marginals, (44) represent a direct way to find
functions that are copulas.

Conversely, if in equation (44) we replace each ui by its probability representation Fi (xi )
we obtain
                        C(F1 (x1 ), . . . , FK (xK )) = F (x1 , . . . , xK )           (45)
that is exactly the formula within Sklar’s theorem.

In practice, the copula of a continuous K–dimensional r.v. X (unique by the Sklar’s theo-
rem) is the representation of the distribution of the r.v. in the ’probability domain’ (those
of ui ’s), rather then in the ’quantile domain’ (those of xi ’s, in practice the original scale).
It is however clear these two ways are not equivalent: the former forget completely the
information of the marginals but isolates the structure of dependence among the compo-
nents of X. This implies that in the study of a multivariate r.v. it is possible to proceed in
2 steps:
C        USEFUL MULTIVARIATE STATISTICAL DISTRIBUTIONS                                              32


     1. identification of the marginal distributions;

     2. definition of the appropriate copula in order to represent the structure of dependence
        among the components of the r.v.

This two step analysis is emphasized also by the structure of the pdf of the K–variate
r.v. X that follows from the copula representation (45):
                                                                                 K
                                                                                 Y
                          f (x1 , . . . , xK ) = c(F1 (x1 ), . . . , FK (xK ))         fi (xi ).   (46)
                                                                                 i=1

where fi (xi ) is the pdf of the univariate marginals Xi and c is the copula density. It is
a very interesting formulation: the product of the marginal densities, which corresponds
to the multivariate density in case of independence among the Xi ’s, is corrected by the
structure of dependence isolated by the copula’s density c. We note also as (46) is a way
to find functions representing copulas.


B.3          The Normal copula

Amongst the copulas presented in letterature an important role is played by the Normal
copula, whose density is given by
                                                            
                                  −1/2        1 0 −1
                       c(u) = |R|      exp − q (R − I)q ,                          (47)
                                              2

where R is a correlation matrix, qi = Φ−1 (ui ), Φ(.) means the cdf of the standard Normal
distribution. Normal copulas are interesting because possess interesting characteristics:
the capability of capture a broad range of dependencies (the bivariate Normal copula, ac-
cording to the value of the correlation parameter is capable of attaining the lower Frechèt
bound, the product copula and the upper Frechèt bound), the analytical tractability, the
simple simulation.



C          Useful multivariate statistical distributions

C.1          Multivariate Gamma distributions

The Cheriyan and Ramabhadran multivariate Gamma (shortly GammaCR) is defined as
follows13 . Let Y0 , Y1 , . . . , YK independent random variables with the following distribu-
tions:

               Y0 ∼ Gamma(φ0 , 1)              Yi ∼ Gamma(φi − φ0 , 1) for i = 1, . . . , K,
    13
         We adapt the definition in (Johnson et al., 2000, chapter 48) to our framework.
C   USEFUL MULTIVARIATE STATISTICAL DISTRIBUTIONS                                                        33


where 0 < φ0 < φi for i = 1, . . . , K. Then, the K–dimensional r.v. X = (X1 ; . . . ; XK ),
where
                                           1
                                     Xi = (Yi + Y0 ),                                   (48)
                                          βi
and βi > 0, has GammaCR distribution with parameters φ0 , φ = (φ1 ; . . . ; φK ) and
β = (β1 ; . . . ; βK ) that is
                               X ∼ GammaCR(φ0 , φ, β).
The pdf is given by

           1 Y
                   K
                        βi        PK            Z     v                      K
                                                                             Y
 f (x) =                        e− i=1 βi xi              y0φ0 −1 e(K−1)y0     (βi xi − y0 )φi −φ0 −1 dy0 ,
         Γ(φ0 ) i=1 Γ(φi − φ0 )                   0                          i=1

where v = min(β1 x1 , . . . , βK xK ) and the integral leads in general to very complicated
expressions. Each univariate r.v. Xi has however a Gamma(φi , βi ) distribution. The main
conjoint moments can be computed via formula (48); in particular the variance matrix has
elements
                                                     φ0
                                      C(Xi , Xj ) =
                                                    βi βj
and then
                                                    φ0
                                     ρ(Xi , Xj ) = p       .
                                                     φi φj
This implies that parameters φ0 plays an important role in determining the correlation
among the Xi ’s and that the GammaCR distribution admits only positive correlations
among the univariate components.


C.2     Normal copula – Gamma marginals distribution

The Normal copula – Gamma marginals distribution can be defined as a multivariate
distribution whose univariate marginals are Gamma distributions and the structure of de-
pendence is represented by a Normal copula. In symbols we write
                                               K
                                               Y
                              X ∼ N (R) −             Gamma(φi , βi )
                                               l=1

where φ = (φ1 ; . . . ; φK ), β = (β1 ; . . . ; βK ), and R is a correlation matrix.

Song (2000) defines and discusses multivariate distributions whose univariate marginals
are dispersion distributions and the structure of dependence is represented by a Normal
copula. Since the Gamma distribution is a particular case of dispersion distribution, the
Normal copula – Gamma marginals case is covered by the cited work. The author dis-
cusses various properties of multivariate dispersion models generated from Normal cop-
ula. In particular, exploiting first order approximations (as in Song (2000, pages 433–4))
D   A SUMMARY ON ESTIMATING FUNCTIONS                                                     34


and adapting notation, we obtain that the variance matrix has approximated elements
                                                  p
                                                    φi φj
                                C(xi , xj ) ' Rij
                                                   βi βj

and then
                                     ρ(xi , xj ) ' Rij .
With respect to the GammaCR distribution in section C.1, that considered in the present
section admits negative correlations also.



D     A summary on estimating functions

Estimating Functions (EFs, henceforth) are a method for making inference on the un-
known parameters of a given statistical model. Even if some ideas appeared before, the
whole development of the theory of EFs begins from the works of Godambe (1960) and
Durbin (1960). Ever since, the method became increasingly popular, with important theo-
retical developments and illuminating applications in different domains: biostatistics and
models for stochastic processes in particular. EFs are relatively less diffuse in the econo-
metric literature, but the ’gap’ is progressively filling up, perhaps even as thanks to some
stimulating reviews (Vinod (1998), Bera et al. (2006)).

Broadly speaking, EFs have close links with other inferential approaches: in particular,
analogies can be found with the Quasi Maximum Likelihood method (QML) and with the
Generalized Method of Moment method (GMM) for just identified problems. However
some other inferential procedures, as for instance M-estimation, have connections with
EFs. About the relations between EFs and other inferential methods see, among others,
Desmond (1997), Contreras and Ryan (2000) and the reviews cited above.

This summary aims to show how EFs can be usefully employed for making inference on
time series models. In particular, we focus our attention on martingale EFs and this choice
is motivated by at least three reasons. Firstly: martingale EFs are largely applicable,
because in modelling time series, EFs that are martingales arise often naturally from the
structure of the process. Secondly: martingale EFs have particularly nice properties and
a relatively simple asymptotic theory. Thirdly: because an estimating function has some
analogies with the score function, and because the score function, if it exists, is usually a
martingale, it is quite natural to approximate it using families of martingales EFs.

An interesting historical review on EFs can be found in Bera et al. (2006), that provides
also a long list of references. About the topics handled in this summary, important refer-
ences can be found, among others, in Heyde (1997), Sørensen (1999), Bibby et al. (2004).

This summary is organized as follows: in section D.1 we list some necessary concepts
about martingale processes; in section D.2 we discuss the likelihood framework as an
important reference for interpreting EFs; in section D.3 we define EFs in general and
martingale EFs in particular; in section D.4 we sketch the asymptotic theory of martingale
D   A SUMMARY ON ESTIMATING FUNCTIONS                                                      35


EFs; in section D.5 we consider optimal EFs; in section D.6 we show how nuisance
parameters can be handled.


D.1      Notation and preliminary concepts

As stated before, we aim to illustrate how martingale EFs can be usefully employed for
making inference on time series models. To this goal, we recall some concepts on mar-
tingale processes. We precede them summarizing the mathematical notation used.


D.1.1       Mathematical notation

We use the following mathematical notations:

    • if x ∈ RK , then kxk = (x0 x)1/2 (the Euclidean norm); if x ∈ RH×K then kxk =
         H X K
                    !1/2
         X
               x2ij      (the Frobenius norm);
            i=1 j=1

    • if f (x) is a vector function of x ∈ RK , the matrix of its first derivatives is denoted
      by ∇x0 f (x) or ∇x0 f ;

    • given two square, symmetric and non negative definite (nnd) matrices A and B, we
      say A not greater than B (or A not smaller than B) in the Löwner ordering if

                                              B−A

        is nnd and we write
                                              A  B;
        p
    • → denotes convergence in probability;
        a
    • ∼ denotes almost sure (with probability 1) convergence.


D.1.2       Martingales

Let (Ω, F, P ) a probability space and let {xt } a discrete time (t ∈ Z+ ) K-dimensional
time series process, that is xt : Ω −→ RK . Denoting as Ft (⊆ F) the σ-algebra repre-
senting the information at time t, we recall that {xt }:

    • is predictable if xt is Ft−1 -measurable; this implies

                                        E(xt |Ft−1 ) = xt ;
D   A SUMMARY ON ESTIMATING FUNCTIONS                                                  36


    • is a martingale if xt is Ft -measurable and

                                       E(xt |Ft−1 ) = xt−1 ;

    • is a martingale difference if xt is Ft -measurable and

                                            E(xt |Ft−1 ) = 0.

Using a Dobb’s representation, a martingale {xt } can be always represented as
                                                   t
                                                   X
                                    xt = x0 +              ∆xs ,                     (49)
                                                     s=1

where
                                      ∆xt = xt − xt−1                                (50)
is a martingale difference and x0 is the starting r.v. A martingale {xt } is null at 0 if
x0 = 0. In this case (49) becomes
                                               t
                                               X
                                       xt =           ∆xs                            (51)
                                                s=1

and E (xt |Ft−1 ) = 0 for all t. A martingale {xt } null at 0 is square integrable if
kV (xt ) k < ∞ for all t.

Taking into account, from now on, square integrable martingales null at 0, and considering
their representation as in (51)-(50), the following quantities can be defined:

    • hx, yit , the mutual quadratic characteristic between xt and yt :
                                             t
                                             X
                                hx, yit =          C(∆xs , ∆ys |Fs−1 );
                                             s=1


    • hxiT , the quadratic characteristic of xt :
                                               t
                                               X
                                    hxiT =           V (∆xs |Fs−1 ).
                                               s=1


Furthermore, if a martingale {xt } null at 0 depends on a parameter λ ∈ Rp , we define the
compensator of ∇λ0 xt as
                                      t
                                      X
                               xt =         E (∇λ0 ∆xs |Fs−1 ) .
                                      s=1
D        A SUMMARY ON ESTIMATING FUNCTIONS                                                         37


D.2          Set-up and the likelihood framework

The likelihood framework provides an important reference for the theory of EFs. In fact,
even if they can be usefully employed just in situations where the likelihood is unknown
or is difficult to compute, EFs generalize the score function in many senses and share
many of its properties. Moreover, the score function plays an important role in many
aspects of the EFs theory.

Hence, we consider a discrete time K-dimensional time series process {xt } and we denote
as x(T ) = {x1 , . . . , xT } a sample from {xt }. We assume that:

         • the possible probability measures for x(T ) are P = {Pλ : λ ∈ Λ}, a union of
           families of parametric models indexed by λ, a p-dimensional parameter belonging
           to an open set Λ ⊆ Rp ;
     • each (Ω, F, Pλ ), where λ ∈ Λ, is a complete probability space;
     • each Pλ , where λ ∈ Λ, is absolutely continuous with respect to some σ-finite
       measure. In this situation a likelihood is defined (even if it can be unknown).

We use the following notation:

     • L(T ) (λ) = f (x(T ) ; λ) is the likelihood function (the density of x(T ) );
     • l(T ) (λ) = ln LT (λ) is the log-likelihood function;
     • s(T ) (λ) = ∇λ0 l(T ) (λ) (provided that l(T ) (λ) ∈ C 1 (Λ) a.s.) is the score function.

Usually, the likelihood function for time series samples can be expressed as product of
the conditional densities Lt (λ) = f (xt |Ft−1 ; λ) of the single observations in the sample.
In this case
                                                 T
                                                 Y
                                   L(T ) (λ) =      Lt (λ)
                                                          t=1
                                                          T
                                                          X
                                            l(T ) (λ) =         lt (λ)
                                                          t=1
                                                          T
                                                          X
                                           s(T ) (λ) =          st (λ)                        (52)
                                                          t=1

where lt (λ) = ln Lt (λ) and st (λ) = ∇λ0 lt (λ).

We assume that the score function s(T ) 14 satisfies the usual regularity conditions: for all
λ ∈ Λ, and for all T ∈ Z+

     1. s(T ) is a square integrable martingale null at 0;
    14
         In many circumstances we suppress explicit dependence from the parameter λ.
D    A SUMMARY ON ESTIMATING FUNCTIONS                                                        38


    2. s(T ) is regular, in the sense that s(T ) ∈ C 1 (Λ) a.s.;
    3. s(T ) is smooth, in the sense that differentiation and integration can be interchanged
       in differencing expectations of st with respect to λ.

Under above regularity conditions, the ML estimator is the solution of the score equation

                                           s(T ) (λ) = 0.

Using the notation of section D.1, the Fisher information is defined by

                                        IT = hsiT = −sT ,

where the second equality is named second Bartlett identity. We can check easily that sT
and hsiT are linked, respectively, to the sum of the Hessians and to the outer product of
the gradients of lt ’s.


D.3     Martingale estimating functions

Within above framework, an estimating function (EF) for λ is a p-dimensional function
of the parameter λ and of the sample x(T ) :

                                           g(T ) (λ; x(T ) ).

Usually we suppress explicit dependence from the observations and sometimes from the
parameter also. In these cases we indicate compactly the EF as

                                       g(T ) (λ)    or     g(T ) .


An estimate for λ, is the value λ
                                b T that solves the corresponding estimating equation (EE)

                                           g(T ) (λ) = 0.                                   (53)

The score function is then a particular case of estimating function. However, there is a
remarkable difference between the score function and a generic EF. In fact s(T ) (λ) has
l(T ) (λ) as its potential function, since s(T ) (λ) = ∇λ0 l(T ) (λ). On the contrary, in general
an EF may not possess a potential function G(T ) (λ) such that g(T ) (λ) = ∇λ0 G(T ) (λ)
(Knudsen (1999, p. 2)).

As stated in Heyde (1997, p. 26), since the score function, if it exists, is usually a martin-
gale, it is quite natural to approximate it using families of martingales EFs. Furthermore,
in modelling time series processes, EFs that are martingales arise often naturally from the
structure of the process. In this spirit, we stress that many of the concepts discussed in this
section are particular to martingale EFs. For a more general handling see, among others,
Sørensen (1999).

Hence we consider martingale EFs that are square integrable and null at 0. As in (51), we
D    A SUMMARY ON ESTIMATING FUNCTIONS                                                     39


represent them as sums martingale differences gt ,
                                                     T
                                                     X
                                         g(T ) =           gt
                                                     t=1

where gt = g(T ) − g(T −1) . We denote the quadratic characteristic of g(T ) as
                                               T
                                               X
                                    hgiT =           V (gt |Ft−1 )
                                               t=1

and the compensator of ∇λ0 g(T ) as
                                         T
                                         X
                                  gT =         E (∇λ0 gt |Ft−1 ) .
                                         t=1



As in the likelihood theory, some regularity conditions on EFs g(T ) (λ) are usually im-
posed. In particular it is required that, ∀λ ∈ Λ,

    1. g(T ) is a square integrable martingale null at 0;

    2. g(T ) is regular, in the sense that g(T ) ∈ C 1 (Λ) a.s. and gT is non-singular;

    3. g(T ) is smooth, in the sense that differentiation and integration can be interchanged
       in differencing expectations of gt with respect to λ.

(We remark the analogies with the corresponding regularity conditions for the score func-
                                        (1)
tion in section D.2). We denote as MT the family of estimating functions satisfying
conditions 1, 2, 3 above and we name it as the family of regular and smooth EFs.

Regular and smooth EFs satisfy some interesting properties. Among these (details in
Knudsen (1999, ch. 1), Heyde (1997, ch. 2)):

    • The compensator of ∇λ0 g(T ) is strictly linked to the mutual quadratic characteristic
      between the EF and the score function, since

                                           gT = −hg, siT .                                (54)

       This follows from the relation

                                E (∇λ0 gt |Ft−1 ) = −C (gt , st |Ft−1 ) .                 (55)

    • gT non-singular (see the regularity condition 2 above) implies hgiT positive-definite.
      This implication can be obtained representing gT as in (54) and using considera-
      tions similar to Knudsen (1999, p. 4).
D      A SUMMARY ON ESTIMATING FUNCTIONS                                                    40


       • If A(λ) is a (p, p) non-random, non-singular matrix belonging to C 1 (Λ) a.s., then
         also the linear transformation
                                           A(λ)g(T ) (λ)                                (56)
                                   (1)
         is an EF belonging to MT .

       • If ξ(λ) is a smooth 1-to-1 reparameterization, then also g(T ) (λ(ξ)) is a regular and
         smooth EF for ξ (Knudsen (1999, p. 7)). In practice, invariance and smoothness
         conditions are invariant under smooth 1-to-1 reparameterizations.

                                            (1)
Using (56), from any EF g(T ) ∈ MT we can derive another EF
                                           (s)
                                      g(T ) = −g0T hgi−1
                                                      T g(T ) ,

                                                 (s)
called standardized version of g(T ) . g(T ) assumes a special role: as argued in section D.4,
 (s)
g(T ) and g(T ) are equivalent, in the sense that they produce the same estimator with the
                                     (s)
same asymptotic variance, but g(T ) is more directly comparable with the score function.


D.4       Asymptotic theory

A central part of the theory is devoted to find EFs that produce good, and possibly optimal,
estimators. In this work, however, we focused attention to martingale EFs: the optimality
criteria employed in this context have essentially an asymptotic justification. Following
these motivations, in this section we sketch the asymptotic theory concerning martingale
EFs. More details can be found, among others, in Heyde (1997), Sørensen (1999), Bibby
et al. (2004).

In generic terms, the asymptotic theory of martingale EFs moves from a Taylor expansion
of the EF in a neighborhood of the true parameter value and the consequent application of
some law of large numbers (LLN) and central limit theorem (CLT) for square integrable
martingales.

Hence, we consider the following Taylor expansion of g(T ) (λ
                                                            b T ) = 0 around the true
parameter value λ0 :
                                b T ) = g(T ) (λ0 ) + ∇λ0 g(T ) (λ∗ )(λ
                     0 = g(T ) (λ                                     b T − λ0 ),         (57)

where λ∗ = αλ0 + (1 − α)λ    b T with α ∈ (0, 1). Adding some other regularity conditions
(they are useful for inverting ∇λ0 g(T ) (λ∗ ), for approximating it by ∇λ0 g(T ) (λ0 ) and for
convergence of this last quantity to the compensator gT (λ0 ) – details in Heyde (1997,
ch. 2)) we have
                                b T − λ0 ' −g (λ0 )−1 g(T ) (λ0 ).
                                λ                                                         (58)
                                             T
D     A SUMMARY ON ESTIMATING FUNCTIONS                                                     41


Under mild regularity conditions a LLN,
                                                          p
                                              g−1
                                               T g(T ) → 0,                                (59)

and a CLT,
                                         −1/2         d
                                     hgiT       g(T ) → N (0, Ip ),                        (60)
apply. Combined with (58), these imply
                                                   p
                                                bT →
                                                λ    λ0                                    (61)

and
                                                              d
                           hgiT (λ0 )−1/2 gT (λ0 )(λ
                                                   b T − λ0 ) → N (0, Ip ).                (62)
             (2)     (1)                                                       (1)
We call MT ⊆ MT the class of martingale EFs g(T ) that belong to MT and satisfy the
additional regularity conditions needed for (61) and (62). Summarizing above results, EFs
                 (2)
belonging to MT give consistent and asymptotically normal estimators with asymptotic
variance matrix
                      
                 V∞ λ  b T = g (λ0 )−10 hgiT (λ0 )g (λ0 )−1 = JT (λ0 )−1 .           (63)
                                T                   T


The heuristic meaning of this result is that a small asymptotic variance requires EFs with
’small’ variability hgiT and ’large’ sensitivity gT . The random matrix

                                         JT = g0T hgi−1
                                                     T gT                                  (64)

can be interpreted as an information matrix: it is called martingale information (Heyde,
1997, p. 28) or Heyde information (Bibby et al., 2004, p. 30).

Now, let us return to the standardized version of g(T ) , that is
                                        (s)
                                      g(T ) = −g0T hgi−1
                                                      T g(T ) .                            (65)

                                                              (2)        (s)
We can check easily that, for EF belonging to MT , g(T ) and g(T ) are equivalent: they
produce the same estimator and have J(g(s) )T = J(g)T , that is they have the same Heyde
information. However (65) is more directly comparable with the score function than
                                                               (s)
g(T ) . In fact, replacing (54) into (65), we can check that g(T ) can be interpreted as the
orthogonal projection of the score function along the direction of g(T ) . This means that,
                       (2)     (s)
for any g(T ) ∈ MT , g(T ) is the version of g(T ) “closest” to the score function, in the
sense that
                                        0                                 0
                           (s)         (s)                   
                s(T ) − g(T ) s(T ) − g(T )  s(T ) − Ag(T ) s(T ) − Ag(T )

                                                                                     (s)
for any linear transformation of g(T ) as in (56). Moreover, we can check that g(T ) satisfies
a second Bartlett identity analogous to that of the score function:

                             J(g)T = J(g(s) )T = hg(s) iT = −g(s) T .                      (66)
D        A SUMMARY ON ESTIMATING FUNCTIONS                                                                   42


D.5        Optimal estimating functions

The interpretation provided in section D.4 of the Heyde information and its close link with
the asymptotic variance of λb T suggest the criterion usually employed to choose optimal
EF in the asymptotic sense: “maximize” the Heyde information. More precisely, once
                                 (2)
selected a subfamily MT ⊆ MT of EFs we have:

                             ∗
Definition 1 (A-optimality) g(T ) is A-optimal (optimal in the asymptotic sense) into
             (2)
MT ⊆ MT if
                                            J (g∗ )T  J (g)T
a.s. for all λ ∈ Λ, for all g(T ) ∈ MT and for all T ∈ Z+ . An A-optimal estimating
           ∗
function g(T ) is called a quasi score function, whereas the corresponding estimator λT is
                                                                                     b
called a quasi likelihood estimator15 .


Returning for a moment to the Heyde information, we already underlined that J(g)T
                                                                        (s)
equates the quadratic characteristic of the standardized version g(T ) (see (66)). Very
interestingly, it can be shown that, when the score function exists, A-optimality as defined
                                                               ∗(s)
above is equivalent to find into MT the standardized EF g(T ) “closest” to the score s(T ) ,
in the sense that
                                        0                              0 
                      ∗(s)             ∗(s)                   (s)           (s)
       E s(T ) − g(T )        s(T ) − g(T )       E s(T ) − g(T ) s(T ) − g(T )        (67)

                   (s)
for any other g(T ) into MT . (67) reveals another important result of the EFs theory: if
MT encloses s(T ) , just the score function is asymptotically optimal with respect to any
other EF g(T ) .

As stated in Heyde (1997), the criterion in the of A-optimality definition or the equivalent
formulation (67) are hard to apply directly. In practice the following theorem can be
employed:

                                    ∗                                (2)
Theorem 2 (Heyde (1997, p. 29)) If g(T ) ∈ MT ⊆ MT satisfies


                                      g∗−1 ∗      −1     ∗
                                       T hg iT = gT hg, g iT ,                                              (68)

                                                             ∗                                        (2)
for all λ ∈ Λ, for all g(T ) ∈ MT and for all T ∈ Z+ , then g(T ) is A-optimal in MT .

                                                ∗
Conversely, if MT is closed under addition and g(T ) is A-optimal in MT then (68) holds.


                                              (2)
As we always work on subsets of MT , a crucial point in applications is to make a good
choice of the set MT of the EFs considered. About this, an important family of EFs that
    15
     Despite a quasi likelihood function in the sense of potential function of g(T ) may not exist (see section
D.3).
D    A SUMMARY ON ESTIMATING FUNCTIONS                                                           43


often can be usefully employed in time series processes is
                        (                            T
                                                                     )
                                     (2)
                                                    X
                 MT = g(T ) ∈ MT : g(T ) (λ) =           αt (λ)vt (λ) ,
                                                             t=1

where vt (λ) is a K-dimensional martingale difference and αt (λ) is a (p, K) Ft−1 -measurable
function. Tacking
                        α∗t = −E (∇λ vt0 |Ft−1 ) V (vt |Ft−1 )−1
we can check immediately that condition (68) of theorem 2 is satisfied and then
                                    T
                                    X
                        ∗
                       g(T )   =−         E (∇λ vt0 |Ft−1 ) V (vt |Ft−1 )−1 vt                 (69)
                                    t=1


is A-optimal into MT . Furthermore, since g∗−1 ∗
                                           T hg iT = −Ip its Heyde information is
given by
                                    T
                                    X
    J (g∗ )T = hg∗ iT = −g∗T =            E (∇λ vt0 |Ft−1 ) V (vt |Ft−1 )−1 E (∇λ0 vt |Ft−1 ) . (70)
                                    t=1



(69) is known as the Hutton-Nelson quasi score function. We note that when a martingale
difference vt (λ) can be defined from the model under analysis, the Hutton-Nelson solu-
tion provides a powerful and, at the same time, surprisingly simple tool: (69) and (70),
the fundamental quantities for the inference, depend only on the conditional variance and
on the conditional expectation of the first derivative of vt (λ). See Heyde (1997, sect. 2.6)
for a deeper discussion.


D.6     Estimating functions in presence of nuisance parameters

Sometimes, the p-dimensional parameter λ that appears in the likelihood can be parti-
tioned as λ = (θ, ψ), where:

     • θ is a p1 -dimensional parameter of scientific interest called interest parameter;

     • ψ is a p2 -dimensional (p2 = p − p1 ) parameter which is not of scientific interest
       and is called nuisance parameter.

In this case the score function for λ can be partitioned as
                                                        
                                                  s(T )1
                                    s(T ) (λ) =            ,
                                                  s(T )2

where s(T )1 = ∇θ0 l(T ) and s(T )2 = ∇ψ0 l(T ) are marginal score functions for θ and ψ
respectively.
D    A SUMMARY ON ESTIMATING FUNCTIONS                                                   44


This structure can be replicated if instead of a score function we consider an EF
                                                        
                                                  g(T )1
                                    g(T ) (λ) =            ,
                                                  g(T )2

where g(T )1 and g(T )2 are the marginal estimating functions for θ and ψ respectively. In
particular, g(T )1 is meant for estimating θ when ψ is known whereas g(T )2 is meant for
estimating ψ when θ is known.

A clear discussion of inferential issues with nuisance parameters can be found, among
others, in Liang and Zeger (1995). The main inferential problem is that, in presence
of nuisance parameters, some properties of estimating functions are no longer valid if
we replace parameters with the corresponding estimators. For instance, unbiasedness of
g(T )1 is not guaranteed if ψ is replaced by an estimator ψ.
                                                          b By consequence, optimality
properties also are not guaranteed.

An interesting statistical handling of nuisance parameter in the estimating functions frame-
work is provided in Jørgensen and Knudsen (2004) (see also Knudsen (1999)). Their
handling parallels, in some aspects, the notion of Fisher-orthogonality (F-orthogonality,
henceforth) in ML estimation. F-orthogonality is defined by block diagonality of the
Fisher information matrix for λ = (θ, ψ). This particular structure guarantees the fol-
lowing properties of the ML estimator:

    1. asymptotic independence of θb and ψ;
                                         b

    2. efficiency-stable estimation of θ, in the sense that the asymptotic variance for θ is
       the same whether ψ is treated as known or unknown;

    3. simplification of the estimation algorithm;

    4. θ(ψ),
       b     the estimate of θ when ψ is given, varies only slowly with ψ.

Among these, Jørgensen and Knudsen (2004) state that efficiency-stable estimation is the
crucial property.

Starting from this point, Jørgensen and Knudsen (2004) extend F-orthogonality to EFs
introducing the concept of nuisance parameter insensitivity (NPI). Even if these authors
define and employ this concept within a finite sample optimality framework, NPI can be
easily extended asymptotic optimality framework. We discuss this in the following. See
the above reference for the original treatment.

We start our exposition partitioning the compensator gT and the quadratic characteristic
hgiT as                                                           
                         gT,11 gT,12                 hgiT,11 hgiT,12
                gT =                        hgiT =                                 (71)
                         gT,21 gT,22                 hgiT,21 hgiT,22
D        A SUMMARY ON ESTIMATING FUNCTIONS                                                          45


conformably to the parameters θ and ψ. Using this partition, the information JT and the
asymptotic variance matrix J−1
                            T have a block structure
                                                   −1
                                                   [JT ]11 [J−1
                                                                    
                        JT,11 JT,12         −1                  T ]12
                 JT =                     JT =                                     (72)
                        JT,21 JT,22                [J−1         −1
                                                     T ]21 [JT ]22

whose components can be derived from (71).

                                                                                           (2) 16
Definition 2 (Nuisance parameter insensitivity) The marginal EF g(T )1 ∈ MT                         is
NPI or ψ-insensitive if
                                   gT,12 = 0.

Using block matrix algebra, we can check easily that ψ-insensitivity of g(T )1 is a sufficient
condition for
                              [J−1       0−1           −1
                                T ]11 = gT,11 hgiT,11 gT,11 .                             (73)
(In practice, this result represents the only if part of the Insensitivity Theorem – see the-
orem 3 below). It implies that, under ψ-insensitivity, the asymptotic variance of θ can
be computed using the compensator and the quadratic characteristic of g(T )1 only (see
property 2 above).

Another important consequence of NPI is that θ(ψ),  b    the estimate of θ when ψ is given,
varies only slowly with ψ (property 4 above). In fact, as argued in Jørgensen and Knudsen
(2004, p. 97), if kψ−ψk
                   b       is Op (v −1/2 ) then kθ(ψ)−θk
                                                 b       is Op (v −1 ) if g(T )1 is ψ-insensitive,
                   −1/2
otherwise is Op (v      ).

Sensitivity can be removed by projection. In fact we can check easily that the marginal
EF
                             ∗                      −1
                           g(T )1 = g(T )1 − gT,12 gT,22 g(T )2                    (74)
is ψ-insensitive. In the Hilbert space sense, (74) is the projection of g(T )2 onto the ortho-
                                         ∗
complement of s(T )2 . This means that g(T )1 carries the same information about θ as g(T )1
and g(T )2 together. See Jørgensen and Knudsen (2004, p. 101) for a deeper discussion.

We think however that, from a theoretical point of view, the more interesting result of
the cited paper is that NPI is not only sufficient but also necessary for (73). This result
has been condensed by Jørgensen and Knudsen (2004) in the Insensitivity Theorem. We
provide it below, adjusting the formulation to the asymptotic framework considered here.
The theorem can be proved, with minor adjustments, as in the cited paper.

                                                                           (2)
Theorem 3 (Insensitivity Theorem) The marginal EF g(T )1 ∈ MT is ψ-insensitive if
and only if
                            [J−1       0−1           −1
                              T ]11 = gT,11 hgiT,11 gT,11
                                            (2)
for all regular marginal EFs g(T )2 ∈ MT .
    16                                                         (2)
    Strictly speaking, the marginal EF g(T )1 does not belong MT . In fact, g(T )1 has dimension p1
                        (2)                                                  (2)
whereas members of MT are p-dimensional functions. However, we use MT also for EFs with different
dimension, meaning that they have to satisfy the corresponding regularity conditions.
E    MATHEMATICAL APPENDIX                                                                       46


E      Mathematical Appendix

We prove sufficient conditions for nonnegativity of the components of µt .


Proposition 1 The relation
                                   n
                                   X
                                         ai x2i + bi xi + c ≥ 0
                                                       
                                                                                               (75)
                                   i=1

is satisfied for all xi ≥ 0 (i = 1, . . . , n) if and only if the coefficients ai , bi and c satisfy
all the following conditions:

    1. ai ≥ 0 for all i ∈ Sn ;

    2. bi ≥ 0 for all i ∈ Sn such that ai = 0;
               n
           1 X b2i
    3. c −          I(bi < 0) I(ai > 0) ≥ 0.
           4 i=1 ai


where Sn = {1, . . . , n}.


Proof:

The minimum of                        n
                                      X
                                            (ai x2i + bi xi ) + c                              (76)
                                      i=1

with respect to the xi ’s is simply the sum of c and the minima of the additive quantities
(ai x2i + bi xi ). Hence, considering assumption xi ≥ 0:

    1. if ai < 0, the minimum of (ai x2i + bi xi ) is always −∞;

    2. if ai = 0, the minimum of (ai x2i + bi xi ) = bi xi is nonnegative (0) only if bi ≥ 0;
                                                                                         bi
    3. if ai > 0, the minimum of (ai x2i + bi xi ) for xi ≥ 0 is reached for xi = −         I(bi <
                                                                                        2ai
                              b2i
       0) and is given by −       I(bi < 0).
                              4ai

Requiring that (76) has a nonnegative minimum, from the above conditions those in the
proposition follow immediately.


                                                                                                 
E    MATHEMATICAL APPENDIX                                                                                       47


As a corollary of the previous proposition, we prove sufficient conditions for nonnegativ-
ity of the components of µt in the vector MEM. For sake of generality, we give the result
for the general formulation in which more lags are included in the µ structure.


Corollary 1 Let
                                  L h
                                  X                                                        i
                                                                     (−)           (s)
                     µt = ω +            βl µt−l + αl xt−l +     γl xt−l   +   δl xt−l         ,                (77)
                                   l=1

the equation that describes the evolution of µt in the vector MEM of section 3.1, where:

    • xt , µt (t = 1, . . . , T ) and ω are (K, 1)-vectors;

    • βl , αl , γl and δl (l = 1, . . . , L) are (K, K) matrices;
                                              (−)                       (s)          1/2
    • in the ’enriched’ formulation: xt             = xt I(rt < 0), xt = xt                sign(rt );
                                                (−)                            (s)         1/2
    • in the ’contagion’ formulation: xt              = xt    I(rt < 0), xt = xt                   sign(rt ).

We assume that the starting values xt and µt , for t = 1, . . . , L, have non-negative com-
ponents.

Then µt has non-negative components if all the following conditions are satisfied (i, j =
1, . . . , K, l = 1, . . . , L):

    1. βijl ≥ 0, αijl ≥ 0, αijl + γijl ≥ 0;

    2. if αijl = 0 then δijl ≥ 0; if αijl + γijl = 0 then δijl ≤ 0;
               K   L                                                      
            1 X X 2 I(δijl < 0) I(αijl > 0) I(δijl > 0) I(αijl + γijl > 0)
    3. ωi −          δ                     +                                 ≥0
            4 j=1 l=1 ijl     αijl                    αijl + γijl


Proof:

We prove the result considering the ’enlarged’ formulation. For the ’contagion’ version
the proof is almost identical.

The proof is by induction on t. Assume that xt−l , µt−l ≥ 0 for l = 1, . . . , L. Then the
i-th subequation of (77) can be rewritten taking:

    (x1 ; . . . ; xn ) = (µt−1 ; . . . ; µt−L ; xt−1 ; . . . ; xt−L )1/2
    (a1 , . . . , an ) = (βi.1 , . . . , βi.l , αi.1 + γi.1 I(rt−1 < 0), . . . , αi.L + γi.L I(rt−L < 0))
     (b1 , . . . , bn ) = (00 , . . . , 00 , δi.1 sign(rt−1 ), . . . , δi.L sign(rt−L ))
                     c = ωi ,
E   MATHEMATICAL APPENDIX                                                                       48


where βi.l , αi.l , γi.l , and δi.l denote the i-th row of the corresponding matrix of coefficients
at lag l.

At this point we can apply proposition 1 to the formulation obtained. Rewriting conditions
1, 2 and 3 of the proposition in the model notation and considering that, in the whole time
series, the returns rt−l take both positive and negative signs, the result follows.

                                                                                                
REFERENCES                                                                              49


References
Andersen, T. G., Bollerslev, T., Diebold, F. X., and Labys, P. (2003). Modeling and
  forecasting realized volatility. Econometrica, 71, 579–625.

Benjamin, M. A., Rigby, R. A., and Stasinopoulos, M. (2003). Generalized autoregressive
  moving average models. Journal of the American Statistical Association, 98(461), 214–
  223.

Bera, A. K., Bilias, Y., and Simlai, P. (2006). Estimating functions and equations: An
  essay on historical developments with applications to econometrics. In T. C. Mills
  and K. Patterson, editors, Palgrave Handbook of Econometrics, volume 1. Palgrave
  MacMillan.

Bibby, B. M., Jacobsen, M., and Sørensen, M. (2004). Estimating functions for discretely
  sampled diffusion-type models. Working paper, University of Copenagen, Department
  of Applied Mathematics and Statistics.

Bollerslev, T. (1986). Generalized autoregressive conditional heteroskedasticity. Journal
  of Econometrics, 31, 307–327.

Bouyé, E., Durrleman, V., Nikeghbali, A., Riboulet, G., and Roncalli, T. (2000). Copu-
  las for finance: A reading guide and some applications. Technical report, Groupe de
  Recherche Opérationelle, Credit Lyonnais, Paris.

Brownlees, C. T. and Gallo, G. M. (2005). Ultra high-frequency data management. In
  C. Provasi, editor, Modelli Complessi e Metodi Computazionali Intensivi per la Stima
  e la Previsione. Atti del Convegno S.Co. 2005, pages 431–436. CLUEP, Padova.

Chou, R. Y. (2005). Forecasting financial volatilities with extreme values: The conditional
  autoregressive range (carr) model. Journal of Money, Credit and Banking, 37(3), 561–
  582.

Contreras, M. and Ryan, L. M. (2000). Fitting nonlinear and constrained generalized
  estimating equations with optimization software. Biometrics, 56, 1268–1272.

Davis, R. A., Dunsmuir, W. T. M., and Strett, S. B. (2002). Observation driven model for
  poisson counts. Working paper, Department of Statistics, Colorado State University.

Desmond, A. F. (1997). Optimal estimating functions, quasi-likelihood and statistical
  modelling (with discussion). Journal of Statistical Planning and Inference, 60, 77–
  121.

Ding, Z., Granger, C. W. J., and Engle, R. F. (1993). A long memory property of stock
  market returns and a new model. Journal of Empirical Finance, 1(1), 83–106.

Durbin, J. (1960). Estimation of parameters in time-series regression models. Journal of
  the Royal Statistical Society, Series B, 22, 139–153.
REFERENCES                                                                            50


Embrechts, P., Lindskog, F., and McNeil, A. J. (2001). Modelling dependence with cop-
 ulas. Technical report, Department of Mathematics, ETHZ, CH-8092 Zurich Switzer-
 land, www.math.ethz.ch/finance.
Engle, R. F. (2002). New frontiers for ARCH models. Journal of Applied Econometrics,
  17, 425–446.
Engle, R. F. and Gallo, G. M. (2006). A multiple indicators model for volatility using
  intra-daily data. Journal of Econometrics, 131(1–2), 3–27.
Engle, R. F. and Russell, J. R. (1998). Autoregressive conditional duration: A new model
  for irregularly spaced transaction data. Econometrica, 66, 1127–62.
Engle, R. F., Gallo, G. M., and Velucchi, M. (2005). An mem–based investigation of dom-
  inance and interdependence across markets. presented at the Computational Statistics
  and Data Analysis World Conference, Cyprus.
Gallo, G. M. and Velucchi, M. (2005). Ultra-high frequency measures of volatility: An
  mem-based investigation. In C. Provasi, editor, Modelli Complessi e Metodi Com-
  putazionali Intensivi per la Stima e la Previsione. Atti del Convegno S.Co. 2005, pages
  269–274. CLUEP, Padova.
Glosten, L., Jaganathan, R., and Runkle, D. (1993). On the relation between the expected
  value and volatility of the nominal excess return on stocks. Journal of Finance, 48,
  1779–1801.
Godambe, V. P. (1960). An optimum property of regular maximum likelihood estimation.
  Annals of Mathematical Statistics, 31, 1208–1212.
Heyde, C. (1997). Quasi-Likelihood and Its Applications. Springer-Verlag, New York.
Johnson, N., Kotz, S., and Balakrishnan, N. (2000). Continuous Multivariate Distribu-
  tions. John Wiley & Sons, New York.
Jørgensen, B. and Knudsen, S. J. (2004). Parameter orthogonality and bias adjustment for
  estimating functions. Scandinavian Journal of Statistics, 31, 93–114.
Knudsen, S. J. (1999). Estimating functions and separate inference. Monographs, Vol.1,
  Department of Statistics and Demography, University of Southern Denmark. Available
  from http://www.stat.sdu.dk/oldpublications/monographs.html.
Li, W. K. (1994). Time series models based on generalized linear models: Some further
  results. Biometrics, 50, 506–511.
Liang, K.-Y. and Zeger, S. L. (1995). Inference based on estimating functions in presence
  of nuisance parameters (with discussion). Statistical Science, 10, 158–199.
Manganelli, S. (2002). Duration, volume and volatility impact of trades. Working paper,
 European Central Bank, WP ECB 125 http://ssrn.com/abstract=300200.
Shephard, N. (1995). Generalized linear autoregressions. Working paper, Nuffield Col-
  lege, Oxford OX1 1NF, UK.
REFERENCES                                                                          51


Song, P. X.-K. (2000). Multivariate dispersion models generated from gaussian copula.
  Scandinavian Journal of Statistics, 27, 305–320.

Sørensen, M. (1999). On asymptotics of estimating functions. Brazilian Journal of Prob-
  ability and Statistics, 13, 111–136.

Vinod, H. D. (1998). Using Godambe-Durbin estimating functions in econometrics. In
  I. W. Basawa, V. P. Godambe, and R. L. Taylor, editors, Selected Proceedings of the
  Symposium on Estimating Functions, volume 32 of IMS Lecture Notes Series. Institute
  of Mathematical Statistics.
                                                                                                                                                                                                                                                                                                REFERENCES




                                                                                                                        |rt |                               hlt                                     rvt
                                                                                                         ML-R      EE           ML-I ML-I   ML-R       EE         ML-I       ML-I   ML-R       EE          ML-I
                                                                                                                                                                                                             ML-I
                                                                                                                                    (own)                                   (own)                           (own)
                                                                                              constant   0.0206 0.0185           0 0.0174   0.0772   0.0764       0.0663   0.0548   0.1172   0.1201 0.1504 0.1455
                                                                                              µt−1       0.7925 0.7883      0.7934 0.8052   0.7795    0.774        0.749   0.7693    0.645    0.638 0.5573 0.5621
                                                                                                         0.0442 0.0413      0.0602 0.0683   0.0287   0.0208        0.056   0.0452   0.0397   0.0222 0.0514 0.0506
                                                                                              |rt−1 |

                                                                                              |rt−1 |−   0.0709    0.06          0      0   0.0674   0.0642       0.0417            0.0434   0.0418       0.0488   0.0539
                                                                                                          0.021 0.0208           0      0   0.0106   0.0091       0.0324            0.0084   0.0077       0.0108   0.0069
                                                                                              hlt−1      0.1564 0.1678      0.2066 0.1538   0.1283   0.1361       0.1799   0.1892
                                                                                                         0.0447 0.0423      0.0602 0.0572   0.0184    0.017       0.0296   0.0329




results equation by equation with its own specification.
                                                                                                −
                                                                                              hlt−1                                0.0556                                            0.0359 0.0419 0.0274
                                                                                                                                   0.0232                                            0.0177 0.0179 0.0233
                                                                                              rvt−1                                                                                  0.2418 0.2467 0.3028 0.3007
                                                                                                                                                                                     0.0285 0.0183 0.038 0.0375
                                                                                                −
                                                                                              rvt−1                                                                                 -0.0302 -0.0351 -0.024
                                                                                                                                                                                     0.0154 0.0155 0.0197
                                                                                                                                                                                                                            Table 2: Multiple Indicator MEM: Comparison across Estimators. GE




the specification selected by the system estimators. ’ML-I (own)’ reports the estimated
through variance targeting. ’ML-I’ reports the estimated results equation by equation with
Note: Standard errors for the constant are not reported since the coefficient was estimated
                                                                                                                                                                                                                                                                                                52
REFERENCES                                                                          53



Table 3: Estimated Impact Matrices of the vMEM on the indicators |rt |, hlt , rvt – GE
stock, 01/03/1995-12/29/2000.

                                             EE
                                    |rt−1 |   hlt−1    rvt−1
                          |rt |   0.81827 0.16775 0
                          hlt     0.03208 0.91007 0
                          rvt     0.02091 0.02097 0.86716
                                            ML-R
                                    |rt−1 |   hlt−1    rvt−1
                          |rt |   0.82797 0.15642 0
                          hlt     0.03371 0.90783 0
                          rvt     0.02168 0.01797 0.87161
                                         ML-I (own)
                                    |rt−1 |   hlt−1    rvt−1
                          |rt |   0.80522 0.18163 0
                          hlt     0          0.95848 0
                          rvt     0.02694 0          0.86286
                                            ML-I
                                    |rt−1 |   hlt−1    rvt−1
                          |rt |   0.79342 0.20658 0
                          hlt     0.02085 0.92892 0
                          rvt     0.02439 0.0137     0.84803




Table 4: Characteristic roots of the estimated Impact Matrices of the vMEM on the indi-
cators |rt |, hlt , rvt – GE stock, 01/03/1995-12/29/2001.

                              Characteristic roots of A
                      ML-R        0.80064 0.85259 0.98827
                      EE          0.80118 0.84151 0.98738
                      ML-I (own) 0.78945 0.80948 0.97853




Table 5: Estimated parameters of the Gamma marginals of the vMEM on the indicators
|rt |, hlt , rvt – GE stock, 01/03/1995-12/29/2001 (1515 obs.)

                                         ML-R    ML-I
                                  |rt | 1.2808   1.272
                                  hlt 6.6435     6.689
                                  rvt 20.6836    20.84
