                                NBER WORKING PAPER SERIES




            A SCORE BASED APPROACH TO WILD BOOTSTRAP INFERENCE

                                           Patrick M. Kline
                                            Andres Santos

                                        Working Paper 16127
                                http://www.nber.org/papers/w16127


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                      June 2010




We thank Justin McCrary, Nicolas Lepage-Saucier, Graham Elliott, Michael Jansson as well as the
editor Jason Abrevaya and two anonymous referees for comments that helped greatly improve the
paper. The views expressed herein are those of the authors and do not necessarily reflect the views
of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

¬© 2010 by Patrick M. Kline and Andres Santos. All rights reserved. Short sections of text, not to exceed
two paragraphs, may be quoted without explicit permission provided that full credit, including ¬© notice,
is given to the source.
A Score Based Approach to Wild Bootstrap Inference
Patrick M. Kline and Andres Santos
NBER Working Paper No. 16127
June 2010, Revised May 2011
JEL No. C01,C12

                                               ABSTRACT

We propose a generalization of the wild bootstrap of Wu (1986) and Liu (1988) based upon perturbing
the scores of M-estimators. This "score bootstrap" procedure avoids recomputing the estimator in each
bootstrap iteration, making it substantially less costly to compute than the conventional nonparametric
bootstrap, particularly in complex nonlinear models. Despite this computational advantage, in the linear
model, the score bootstrap studentized test statistic is equivalent to that of the conventional wild bootstrap
up to order Op(n-1). We establish the consistency of the procedure for Wald and Lagrange Multiplier
type tests and tests of moment restrictions for a wide class of M-estimators under clustering and potential
misspecification. In an extensive series of Monte Carlo experiments we find that the performance of
the score bootstrap is comparable to competing approaches despite its computational savings.


Patrick M. Kline
Department of Economics
UC, Berkeley
508-1 Evans Hall #3880
Berkeley, CA 94720
and NBER
pkline@econ.berkeley.edu

Andres Santos
Department of Economics
9500 Gilman Drive
La Jolla, CA 92093-0508
a2santos@ucsd.edu
1         Introduction

The bootstrap of Efron (1979) has become a standard tool for conducting inference with economic
data. Among the numerous variants of the original bootstrap, the so-called ‚Äúwild‚Äù bootstrap of
Wu (1986) and Liu (1988) has been found to yield dramatic improvements in the ability to control
the size of Wald tests of OLS regression coefficients in small samples (Mammen (1993), Horowitz
(1997, 2001), Cameron, Gelbach, and Miller (2008)).

        Originally proposed as an alternative to the residual bootstrap of Freedman (1981), the wild
bootstrap has often been interpreted as a procedure that resamples residuals in a manner that
captures any heteroscedasticity in the underlying errors. Perhaps for this reason, the applications
and extensions of the wild bootstrap have largely been limited to linear models where residuals are
straightforward to obtain; see for example Hardle and Mammen (1993) for nonparametric regres-
sion, You and Chen (2006) for partially linear regression, Davidson and MacKinnon (2010) for IV
regression and Cavaliere and Taylor (2008) for unit root inference.

        We propose a new variant of the wild bootstrap (the ‚Äúscore‚Äù bootstrap) which perturbs the
fitted score contributions of an M-estimator with i.i.d. weights conditional on a fixed Hessian. In
the linear model, our score bootstrap procedure is numerically equivalent to the conventional wild
bootstrap for unstudentized statistics and higher order equivalent for studentized ones. However, in
contrast to the wild bootstrap, our approach is easily adapted to estimators without conventional
residuals and avoids recomputing the estimator in each bootstrap iteration. As a result, the score
bootstrap possesses an important advantage over existing bootstraps in settings where the model is
computationally expensive to estimate or poorly behaved in a subset of the bootstrap draws. For
example, computational problems often arise in small samples even in simple probit or logit models
where, for some nonparametric bootstrap draws, the estimator cannot be computed.1

        The score bootstrap is closely related to several existing bootstrap procedures in the literature.
Most notably, it bears a close relationship to the estimating equation bootstrap of Hu and Zidek
(1995) who propose resampling score contributions in the linear model conditional on a fixed Hes-
sian. Hu and Kalbfleisch (2000) generalize this approach to nonlinear models by resampling both
score and Hessian contributions evaluated at the estimated parameter vector. In the case of score
or Lagrange Multiplier tests, our approach can be interpreted as a wild bootstrap analogue to their
pairs resampling procedure. Also related is the generalized bootstrap of Chatterjee and Bose (2005)
    1
        See Kaido and Santos (2011) for a recent application of the score bootstrap to estimating the asymptotic distri-
bution of set estimators, an area where computational considerations are particularly important.


                                                             2
which perturbs the objective function of an M-estimator with i.i.d. weights. This approach is closer
to the weighted bootstrap (e.g. Barbe and Bertail (1995), Ma and Kosorok (2005)) than the wild
bootstrap and, in contrast to the score bootstrap, requires reoptimization of the estimator under
each perturbation of the criterion function. Finally, our procedure has an interpretation as a vari-
ant of the k-step bootstrap procedure of Davidson and MacKinnon (1999a) which involves taking a
finite number of Newton steps towards optimization of an M-estimator in a bootstrap sample. An-
drews (2002) showed that this procedure yields an Edgeworth refinement depending on the number
of optimization steps taken. Like the conventional nonparametric bootstrap however, the k-step
procedure may be difficult to compute if, in some bootstrap samples, the Hessian is poorly behaved
or of less than full rank, problems which the score bootstrap avoids.

    We provide results establishing the consistency of the score bootstrap for a broad class of test
statistics under weak regularity conditions and in the presence of potential misspecification. Our
framework is shown to encompass Wald and Lagrange Multiplier (LM) tests as well as tests of
moment restrictions. To assess the empirical relevance of these theoretical results, we conduct
an extensive series of Monte Carlo experiments comparing the performance of several different
bootstrap procedures in settings with clustered data. Our focus on clustered data is motivated by
the prevalence of such settings in applied work and a large literature (e.g. Bertrand, Duflo, and
Mullainathan (2004), Wooldridge (2003), Donald and Lang (2007), Cameron, Gelbach, and Miller
(2008)) finding that asymptotic cluster robust methods often perform poorly in small samples. We
find that variants of our proposed score based bootstrap substantially outperform analytical cluster
robust methods. The performance of these procedures is also comparable to that of competing
bootstrap methods, despite their large difference in computational cost.

    The remainder of the paper is structured as follows: Section 2 reviews the wild bootstrap, while
Section 3 introduces the score bootstrap and establishes its higher order equivalence. In Section 4
we develop the consistency of the score bootstrap under weak regularity conditions and illustrate its
applicability to a variety of settings. Our simulation study is contained in Section 5, while Section
6 briefly concludes. All proofs are contained in the Appendix.



2     Wild Bootstrap Review

We begin by reviewing the wild bootstrap and the reasons for its consistency in the context of a
linear model. A careful examination of the arguments justifying its validity provides us with the
intuition necessary for developing the score bootstrap and its extension to M-estimation problems.

                                                 3
      While there are multiple approaches to implementing the wild bootstrap, for expository purposes
we focus on the original methodology developed in Liu (1988). Suppose {Yi , Xi }ni=1 is an i.i.d.
sequence of random variables, with Yi ‚àà R, Xi ‚àà Rm and satisfying the linear relationship:

                                                    Yi = Xi0 Œ≤0 + i .                                      (1)

Letting Œ≤ÃÇ denote the OLS estimate of Œ≤0 and ei ‚â° (Yi ‚àíXi0 Œ≤ÃÇ) the implied residual, the wild bootstrap
generates new residuals of the form ‚àói ‚â° Wi ei for some randomly generated i.i.d. sequence {Wi }ni=1
that is independent of {Yi , Xi }ni=1 and satisfies E[Wi ] = 0 and E[Wi2 ] = 1. Common choices of
distributions for Wi include the Standard Normal, Rademacher,2 and the two-point distribution
advocated in Mammen (1993).3 Under these assumptions on {Wi }ni=1 it follows that:

                            E[‚àói |{Yi , Xi }ni=1 ] = 0           E[(‚àói )2 |{Yi , Xi }ni=1 ] = e2i .       (2)

Hence ‚àói is mean independent of {Yi , Xi }ni=1 and, in addition, captures the pattern of heteroscedas-
ticity found in the original sample. This property, originally noted in Wu (1986), enables the wild
bootstrap to remain consistent even in the presence of heteroscedasticity or model misspecification.4

      The wild bootstrap resampling scheme consists of generating dependent variables {Yi‚àó }ni=1 by

                                                     Yi‚àó ‚â° Xi0 Œ≤ÃÇ + ‚àói                                     (3)

and then conducting OLS on the sample {Yi‚àó , Xi }ni=1 in order to obtain a bootstrap estimate Œ≤ÃÇ ‚àó .
                    ‚àö
The distribution of n(Œ≤ÃÇ ‚àó ‚àí Œ≤ÃÇ) conditional on {Yi , Xi }ni=1 (but not on {Wi }ni=1 ) is then used as an
                                         ‚àö
estimate of the unknown distribution of n(Œ≤ÃÇ ‚àí Œ≤0 ). Since the former distribution can be computed
through simulation, the wild bootstrap provides a simple way to obtain critical values for inference.

      We review why the wild bootstrap is consistent by drawing from arguments in Mammen (1993).
First, observe that standard OLS algebra and the relationships in (1) and (3) imply that:
                                          n                                                   n
                 ‚àö                    1 X                           ‚àö                     1 X
                  n(Œ≤ÃÇ ‚àí Œ≤0 ) = Hn‚àí1 ‚àö       Xi i                   n(Œ≤ÃÇ ‚àó ‚àí Œ≤ÃÇ) = Hn‚àí1 ‚àö       Xi ‚àói ,   (4)
                                       n i=1                                               n i=1

where Hn ‚â° n‚àí1     Xi Xi0 . When Œ≤ÃÇ is viewed as the maximum likelihood estimator of a normal
                     P
                        i
                                                 P
model, Hn is the Hessian of the likelihood, while i Xi i is the gradient (or score) evaluated at
the true parameter value Œ≤0 . Since both the full sample score contributions ({Xi i }ni=1 ) and their
bootstrap analogues ({Xi ‚àói }ni=1 ) are properly centered, the expressions in (4) can be expected
  2
      A Rademacher random variable puts probability one half on the values one and negative one.
                       ‚àö                      ‚àö           ‚àö                                ‚àö
  3
    Here Wi equals 1‚àí2 5 with probability 25+1
                                             ‚àö
                                               5
                                                 and 5+1 2     with probability 1 ‚àí         5+1
                                                                                             ‚àö .
                                                                                           2 5
  4
    We refer to misspecification in model (1) as E[i |Xi ] 6= 0 but E[i Xi ] = 0.


                                                              4
to converge to a normal limit. Therefore, consistency of the wild bootstrap hinges on whether
these limits are the same or, equivalently, whether the asymptotic variances agree. However, since
E[Wi2 ] = 1 and {Wi }ni=1 is independent of {Yi , Xi }ni=1 , we may write:
                                              n                 n
                                        1 X              1 X
                                   E[( ‚àö       Xi i )( ‚àö       Xi i )0 ] = E[Xi Xi0 2i ]          (5)
                                         n i=1            n i=1
                               n                  n                                n
                         1 X               1 X                                  1X
                    E[( ‚àö       Xi ‚àói )( ‚àö       Xi ‚àói )0 |{Yi , Xi }ni=1 ] =       Xi Xi0 e2i ,   (6)
                          n i=1             n i=1                               n i=1

which implies, by standard arguments, that the second moments indeed agree asymptotically. As
         ‚àö                ‚àö
a result, n(Œ≤ÃÇ ‚àí Œ≤0 ) and n(Œ≤ÃÇ ‚àó ‚àí Œ≤ÃÇ) converge in distribution to the same normal limit and the
consistency of the wild bootstrap is immediate.

    While the ability of the wild bootstrap to asymptotically match the first two moments of the
full sample score provides the basis for establishing its validity, it does not elucidate why it often
performs better than a normal approximation. Improvements occur when the bootstrap is able to
additionally match higher moments of the score. If, for example, E[Wi3 ] = 1, then the third moments
match asymptotically and the wild bootstrap provides a refinement over the normal approximation
to a studentized statistic by providing a skewness correction (Liu (1988)). The additional require-
ment that E[Wi3 ] = 1 is satisfied, for example, by the weights proposed in Mammen (1993), as well
as for Wi = (Vi ‚àí 2) with Vi following a Gamma distribution with mean 2 and variance 1. Alterna-
tively, the Rademacher distribution, which satisfies E[Wi ] = E[Wi3 ] = 0 and E[Wi2 ] = E[Wi4 ] = 1,
is able to match the first four moments for symmetric distributions and can in such cases provide
an additional refinement (Liu (1988); Davidson and Flachaire (2008)).



3     The Score Bootstrap

The wild bootstrap resampling scheme is often interpreted as a means of generating a set of boot-
strap residuals mimicking the heteroscedastic nature of the true errors. However, the residuals only
influence the limiting distribution of the OLS estimator through the score. Thus, we may alterna-
tively view the wild bootstrap as creating a set of bootstrap score contributions ({Xi ‚àói }ni=1 ) that
mimic the heteroscedastic nature of the true score contributions ({Xi i }ni=1 ). In this section, we
develop the implications of this observation, which provides the basis for our procedure.

    The relationship between the wild bootstrap and the score is transparent from the discussion
of its consistency in Section 2. Since ‚àói = ei Wi , we learn from (4) that the wild bootstrap may
be interpreted as a perturbation of the score contributions ({Xi (Yi ‚àí Xi0 Œ≤)}ni=1 ) evaluated at the

                                                          5
estimated parameter value (Œ≤ÃÇ) that leaves the Hessian ( n1                   Xi Xi0 ) unchanged.5 More precisely, a
                                                                      P
                                                                          i

numerically equivalent way to implement the wild bootstrap is given by the following algorithm:

Step 1: Obtain the OLS estimate Œ≤ÃÇ and generate the fitted score contributions {Xi (Yi ‚àí Xi0 Œ≤ÃÇ)}ni=1 .

Step 2: Using random weights {Wi }ni=1 independent of {Yi , Xi }ni=1 and satisfying E[Wi ] = 0 and
E[Wi2 ] = 1, construct a new set of perturbed score contributions {Xi (Yi ‚àí Xi0 Œ≤ÃÇ)Wi }ni=1 .

Step 3: Multiply the perturbed score by the inverse Hessian to obtain Hn‚àí1 ‚àö1n
                                                                                                P
                                                                                                   ‚àí Xi Œ≤ÃÇ)Xi Wi
                                                                                                  i (Yi
                                                                                                   ‚àö
and use its distribution conditional on {Yi , Xi }ni=1        as an estimate of the distribution of n(Œ≤ÃÇ ‚àí Œ≤0 ).

      Unlike the residual based view of the wild bootstrap, the score interpretation is easily generalized
to more complex nonlinear models. One may simply perturb the fitted score contributions of such a
model while keeping the Hessian unchanged and, provided E[Wi ] = 0 and E[Wi2 ] = 1, the first two
moments of the perturbed score and true score will match asymptotically. Under the appropriate
regularity conditions, this moment equivalence will suffice for establishing the consistency of the
proposed bootstrap. For obvious reasons, we term this procedure a ‚Äúscore bootstrap‚Äù.

      To fix ideas, the following example shows how this intuition may be applied in a nonlinear model.

Example 3.1. Consider a standard probit model in which i ‚àº N (0, 1) and (Yi , Xi ) satisfy:

                                                Yi = 1{Xi0 Œ≤0 ‚â• i } ,                                              (7)

                                                                                                          ‚àö
where 1{¬∑} is the indicator function. Suppose we wish to approximate the distribution of                      n(Œ≤ÃÇ‚àíŒ≤0 ),
where Œ≤ÃÇ is the maximum likelihood estimate of Œ≤0 . The log-likelihood which Œ≤ÃÇ maximizes is then:
                                         n
                                      1X
                          Ln (Œ≤) ‚â°          {Yi log(Œ¶(Xi0 Œ≤)) + (1 ‚àí Yi )(1 ‚àí Œ¶(Xi0 Œ≤))} ,                          (8)
                                      n i=1

for Œ¶ the cdf of a standard normal random variable. Thus, the score for the model is given by:
                                  n
                             1X                                           œÜ(x0 Œ≤)(y ‚àí Œ¶(x0 Œ≤))
                    Sn (Œ≤) ‚â°       s(Yi , Xi , Œ≤)         s(y, x, Œ≤) ‚â°                         x,                   (9)
                             n i=1                                        Œ¶(x0 Œ≤)(1 ‚àí Œ¶(x0 Œ≤))

where œÜ is the derivative of Œ¶. The principles derived from the linear model then suggest bootstrap-
                          ‚àö
ping the distribution of n(Œ≤ÃÇ ‚àí Œ≤0 ) by: (i) Obtaining fitted score contributions {s(Yi , Xi , Œ≤ÃÇ)}ni=1 ;
(ii) Perturbing them by random weights {Wi }ni=1 to obtain {s(Yi , Xi , Œ≤ÃÇ)Wi }ni=1 ; (iii) Multiplying the
perturbed score by the inverse Hessian ([ n1 i ‚àáŒ≤ s(Yi , Xi , Œ≤ÃÇ)]‚àí1 ‚àö1n i s(Yi , Xi , Œ≤ÃÇ)Wi ) and employing
                                               P                        P
                                                                        ‚àö
its distribution conditional on {Yi , Xi }ni=1 to approximate that of n(Œ≤ÃÇ ‚àí Œ≤0 ).
  5
      In contrast, the weighted bootstrap perturbs the score and the Hessian (Barbe and Bertail (1995)).




                                                          6
3.1       Higher Order Equivalence
                                                                                 ‚àö
In the linear model, the wild and score bootstrap statistics for                     n(Œ≤ÃÇ‚àíŒ≤0 ) are numerically equivalent.
However, in most instances the statistic of interest is studentized, since only in this context is a
refinement over an analytical approximation available (Liu (1988), Horowitz (2001)). In accord
with the perturbed score interpretation, it is natural to simply employ the sample variance of the
perturbed score contributions for studentization. For this reason, we define the bootstrap statistics:
                                                                                                            n
                                       1   ‚àö                                                    1       1   X
   Tn‚àów   ‚â°   (Hn‚àí1 Œ£‚àón (Œ≤ÃÇ ‚àó )Hn‚àí1 )‚àí 2          ‚àó
                                               n(Œ≤ÃÇ ‚àí Œ≤ÃÇ)      Tn‚àós   ‚â°   (Hn‚àí1 Œ£‚àón (Œ≤ÃÇ)Hn‚àí1 )‚àí 2 Hn‚àí1 ‚àö          Xi ‚àói ,   (10)
                                                                                                        n   i=1


where Œ£‚àón (Œ≤) ‚â°       1
                                  Xi Xi0 (Yi‚àó ‚àíXi0 Œ≤)2 and Tn‚àów and Tn‚àós are the studentized wild and score bootstrap
                          P
                      n       i

statistics respectively. It is important to note that in the computation of Tn‚àós , the full sample
estimator Œ≤ÃÇ is used in obtaining the standard errors, and hence calculation of Œ≤ÃÇ ‚àó and its implied
residuals is unnecessary. As a result, the score bootstrap is computationally simpler to implement
than the wild bootstrap which requires obtaining bootstrap residuals.

   While for the statistics in (4) the wild and score bootstraps are numerically equivalent, such a
relationship fails to hold for the studentized versions. An important concern then is whether this
difference is of importance and, in particular, whether the refinement of the wild bootstrap over
a normal approximation (Liu (1988)) is lost due to this discrepancy. Somewhat surprisingly, the
differences between the wild and score bootstrap are asymptotically negligible even at higher order.
Specifically, the wild and score bootstrap statistics are asymptotically equivalent up to a higher
order than that of the refinement the wild bootstrap possesses over the normal approximation. As
a result, under appropriate regularity conditions, the score bootstrap not only remains consistent
despite not recomputing the estimator but can in addition be expected to obtain a refinement over
an analytical approximation in precisely the same instances as the wild bootstrap.

   In order to establish the higher order equivalence of Tn‚àós and Tn‚àów , we impose the following:

Assumption 3.1. (i) {Yi , Xi }ni=1 are i.i.d. E[Xi i ] = 0, and E[Xi Xi0 ], E[Xi Xi0 2i ] are full rank;
(ii) The moments E[kXi k4 ], E[4i ] and E[kXi k4 4i ] are finite; (iii) {Wi }ni=1 are i.i.d., independent of
{Yi , Xi }ni=1 with E[Wi ] = 0, E[Wi2 ] = 1 and E[Wi4 ] < ‚àû.


   Let P ‚àó and E ‚àó denote probability and expectation conditional on {Yi , Xi }ni=1 (but not {Wi }ni=1 ).
Under Assumption 3.1 we can then establish the higher order equivalence of Tn‚àów and Tn‚àós .

Lemma 3.1. Under Assumption 3.1, Tn‚àów = Tn‚àós + Op‚àó (n‚àí1 ) almost surely.


                                                               7
        If the conditions for an Edgeworth expansion of the bootstrap statistics Tn‚àów and Tn‚àós are satisfied,
then Lemma 3.1 implies that they can be expected to disagree only in terms of order n‚àí1 or smaller;
see Chapter 2.7 in Hall (1992) for such arguments.6 Therefore, in settings where the wild bootstrap
                                                                           1
obtains the traditional Edgeworth refinement of order n‚àí 2 over a normal approximation, the score
bootstrap should as well. Kline and Santos (2011) show that such a refinement may be available
even in models in which i is not mean independent of Xi but merely uncorrelated with it.

        The higher order equivalence of Tn‚àów and Tn‚àós is at first glance unexpected since the score boot-
strap appears to violate the usual plug-in approach of the standard bootstrap. However, this only
introduces a smaller order error due to the residuals {‚àói }ni=1 being mean independent of {Xi }ni=1
under the bootstrap distribution. Importantly, the higher order equivalence would fail to hold if the
residuals {‚àói } were sampled in a manner under which they were merely uncorrelated with {Xi }ni=1
under the bootstrap distribution.

Remark 3.1. The bootstrap estimator Œ≤ÃÇ ‚àó acquired from running OLS in the sample {Yi‚àó , Xi } may
easily be obtained from the score bootstrap procedure by the equality:
                                                                    n
                                                ‚àó              1    X
                                              Œ≤ÃÇ = Œ≤ÃÇ +   Hn‚àí1            Xi ‚àói .                                  (11)
                                                                n   i=1

Note that the right hand side of equation (11) is a single Newton-Raphson step towards the wild
bootstrap estimator Œ≤ÃÇ ‚àó starting from Œ≤ÃÇ. Thus, there is a close connection between our approach and
the k-step bootstrap procedure studied by Davidson and MacKinnon (1999a) and Andrews (2002).
However, as Lemma 3.1 suggests and our Monte Carlos confirm, in the linear model, computation
of Œ≤ÃÇ ‚àó may be avoided while still obtaining a refinement over an analytical approximation.



4         Inference

We turn now to establishing the validity of a score bootstrap procedure for estimating the critical
values of a large class of tests. Building on our earlier discussion we consider test statistics based
upon the fitted parametric scores of M-estimators, using perturbations of those scores to estimate
their sampling distribution. Since this approach does not depend upon resampling of residuals,
we do not distinguish between dependent and exogenous variables and instead consider a random
vector Zi ‚àà Z ‚äÜ Rm which may contain both.
    6
        More precisely, Lemma 3.1 is not sufficient for showing the equivalence of the first two terms in the Edgeworth
                                                                                     1              1          1
expansions. Such an equivalence can be established if P (P ‚àó (kTn‚àów ‚àí Tn‚àós k > (n 2 log n)‚àí1 ) > n‚àí 2 ) = o(n‚àí 2 ) and the
                                                                                         1
Edgeworth expansion is valid in the bootstrap sample with probability 1 ‚àí o(n‚àí 2 ) (Lemma 5 Andrews (2002)).

                                                            8
   We focus on test statistics Gn that are quadratic forms of a vector valued statistic Tn :

                                                       Gn ‚â° Tn0 Tn .                                                  (12)

Under the null hypothesis, the underlying statistic Tn is required to be asymptotically pivotal and
allow for a linear expansion. More precisely, we require that under the null hypothesis:
                                                                                                      n
                                               1                                                 1 X
        Tn = (An (Œ∏0 )Œ£n (Œ∏0 )An (Œ∏0 )0 )‚àí 2 Sn (Œ∏0 ) + op (1)                  Sn (Œ∏) ‚â° An (Œ∏) ‚àö       s(Zi , Œ∏) ,   (13)
                                                                                                  n i=1

where An (Œ∏) is a r√ók matrix, s(z, Œ∏) is a k√ó1 vector, Œ£n (Œ∏) is the sample covariance matrix of s(Zi , Œ∏)
and Œ∏0 is an unknown parameter vector. In accord with the terminology we employed for the linear
                                                        P
model, we refer to An (Œ∏) as the inverse of the Hessian, i s(Zi , Œ∏) as the score of the model, and
to {s(Zi , Œ∏)}ni=1 as the score contributions. Under appropriate regularity conditions, Tn is therefore
asymptotically normally distributed with identity covariance matrix and hence Gn is asymptotically
Chi-squared distributed with degrees of freedom equal to the dimension of Tn . Though we only
consider asymptotically pivotal statistics, our results readily extend to unstudentized ones as well.

   The bootstrap statistics employed to estimate the distributions of Gn and Tn are given by:
                                                                                                          n
                                                            1                                      1 X
 G‚àón ‚â° Tn‚àó0 Tn‚àó          Tn‚àó ‚â° (An (Œ∏ÃÇ)Œ£‚àón (Œ∏ÃÇ)An (Œ∏ÃÇ)0 )‚àí 2 Sn‚àó (Œ∏ÃÇ)            Sn‚àó (Œ∏) ‚â° An (Œ∏) ‚àö       s(Zi , Œ∏)Wi (14)
                                                                                                    n i=1

where Œ£‚àón (Œ∏) is the sample covariance matrix of s(Zi , Œ∏)Wi and Œ∏ÃÇ is a consistent estimator for Œ∏0 . As
discussed in the previous section, implementation of the score bootstrap only requires calculation
of the full sample estimator Œ∏ÃÇ; no additional optimization is needed in each bootstrap iteration.

Remark 4.1. An alternative to perturbing the fitted score contributions by random weights is to
instead resample them with replacement. Specifically, for sÃÉi ‚â° s(Zi , Œ∏ÃÇ), we may consider drawing
from {sÃÉi }ni=1 with replacement and approximating the distribution of Gn by that of GÃÉn , where:
                                                                                                              n
                                                            0 ‚àí 12                                    1 X
            GÃÉn ‚â°   TÃÉn0 TÃÉn        (An (Œ∏ÃÇ)Œ£ÃÉn (Œ∏ÃÇ)An (Œ∏ÃÇ) )        SÃÉn (Œ∏ÃÇ)       SÃÉn (Œ∏) ‚â° An (Œ∏) ‚àö       sÃÉi      (15)
                                                                                                       n i=1

for Œ£ÃÉn (Œ∏ÃÇ) the sample covariance matrix of {sÃÉi }ni=1 . In the linear model, this procedure corresponds
to that of Hu and Zidek (1995). For nonlinear problems, Hu and Kalbfleisch (2000) propose a
closely related approach that additionally resamples the Hessian. Specifically, when the inverse
Hessian takes the form A‚àí1
                                  P
                         n (Œ∏ÃÇ) =  i a(Zi , Œ∏ÃÇ), their bootstrap procedure samples with replacement

from both {sÃÉi }ni=1 and {a(Zi , Œ∏ÃÇ)}ni=1 to obtain a new score and inverse Hessian. This may be
thought of as an approximation to the traditional nonparametric (‚Äúpairs‚Äù) bootstrap. Like the pairs
bootstrap however, their procedure may encounter computational difficulties when the Hessian is
poorly behaved in some bootstrap draws, a problem which becomes more likely in small samples
when some of the covariates are discrete.

                                                                9
4.1        Bootstrap Consistency

We establish the consistency of the bootstrap under the following set of assumptions:
                                  p
Assumption 4.1. (i) Œ∏ÃÇ ‚Üí Œ∏0 with Œ∏ÃÇ, Œ∏0 ‚àà Œò ‚äÇ Rp and Œò a compact set; (ii) The limit point Œ∏0
satisfies E[s(Zi , Œ∏0 )s(Zi , Œ∏0 )0 ] < ‚àû and the matrix A(Œ∏0 )E[s(Zi , Œ∏0 )s(Zi , Œ∏0 )0 ]A(Œ∏0 )0 is invertible.

Assumption 4.2. (i) Under the null hypothesis Tn satisfies (13) and Œ∏0 is such that E[s(Zi , Œ∏0 )] = 0;
                                                    p
(ii) Under the alternative hypothesis Gn ‚Üí ‚àû.

Assumption 4.3. (i) {Zi }ni=1 is i.i.d. (ii) supŒ∏‚ààŒò kAn (Œ∏) ‚àí A(Œ∏)kF = op (1) with A(Œ∏) continuous.

Assumption 4.4. (i) {Wi }ni=1 is an i.i.d. sample, independent of {Zi }ni=1 satisfying E[Wi ] = 0
and E[Wi2 ] = 1; (ii) For conv(Œò) the convex hull of Œò, s(z, Œ∏) is continuously differentiable in
Œ∏ ‚àà conv(Œò) and supŒ∏‚ààconv(Œò) k‚àás(z, Œ∏)kF ‚â§ F (z) for some function F (z) with E[F 2 (Zi )] < ‚àû.


       In Assumption 4.1 we require Œ∏ÃÇ to converge in probability to some parameter vector Œ∏0 ‚àà Œò
whose value may depend upon the distribution of Zi . The compactness of the parameter space Œò
is employed to verify the perturbed scores form a Donsker class. This restriction may be relaxed at
the expense of a more complicated argument that exploits the consistency of Œ∏ÃÇ for a local analysis.
Though in the notation we suppress such dependence, it is important to note that Œ∏0 may take
different values under the null and alternative hypotheses. In Assumptions 4.3(ii) and 4.4(ii),
k ¬∑ kF denotes the Frobenius norm. Assumptions 4.2 and 4.3, in turn enable us to establish the
asymptotic behavior of Gn under the null and alternative hypotheses. Assumption 4.4(i) imposes
the only requirements on the random weights {Wi }ni=1 , which are the same conditions imposed for
inference on the linear model in previous wild bootstrap studies. Assumption 4.4(ii) allows us to
establish that the empirical process induced by functions of the form ws(z, Œ∏) is asymptotically tight.
Differentiability is not necessary for this end, but we opt to impose it due to its ease of verification
and wide applicability.7 We note, however, that estimation of A(Œ∏0 ) may be more challenging in
the non-differentiable case as this quantity usually depends on the population Hessian.

       Assumptions 4.1-4.4 are sufficient for establishing the consistency of the proposed score bootstrap
procedure under the null hypothesis.

Theorem 4.1. Let Fn and Fn‚àó be the cdfs of Gn and of G‚àón conditional on {Zi }ni=1 and suppose that
Assumptions 4.1, 4.2, 4.3 and 4.4 hold. If the null hypothesis is true, it then follows that:

                                            sup |Fn (c) ‚àí Fn‚àó (c)| = op (1) .
                                             c‚ààR

   7
       For non-differentiable settings, the relevant condition is that F ‚â° {ws(z, Œ∏) : Œ∏ ‚àà Œò} be a Donsker class.

                                                            10
   Theorem 4.1 justifies the use of quantiles from the distribution of G‚àón conditional on {Zi }ni=1 as
critical values. In order to control the size of the test at level Œ±, we may employ:

                             cÃÇ1‚àíŒ± ‚â° inf{c : P (G‚àón ‚â§ c |{Zi }ni=1 ) ‚â• 1 ‚àí Œ±} .                      (16)

While difficult to compute analytically, cÃÇ1‚àíŒ± may easily be calculated via simulation. Employing a
random number generator, B samples {{Wi1 }ni=1 , . . . , {WiB }ni=1 } may be created independently of
the data and used to construct B statistics {G‚àón1 , . . . , G‚àónB }. Provided B is sufficiently large, the
empirical 1 ‚àí Œ± quantile of {G‚àón1 , . . . , G‚àónB } will yield an accurate approximation to cÃÇ1‚àíŒ± .

   While Theorem 4.1 implies that the critical value cÃÇ1‚àíŒ± in conjunction with the test statistic Gn
delivers size control, it does not elucidate the behavior of the test under the alternative hypothesis.
As in other bootstrap procedures, the test is consistent due to the bootstrap statistic G‚àón being
properly centered even under the alternative. As a result, cÃÇ1‚àíŒ± converges in probability to the
1 ‚àí Œ± quantile of a Chi-squared distribution with r degrees of freedom, while Gn diverges to infinity.
Therefore, under the alternative hypothesis, Gn is larger than cÃÇ1‚àíŒ± with probability tending to one
and the test rejects asymptotically. We summarize these findings in the following corollary:

Corollary 4.1. Under Assumptions 4.1, 4.2, 4.3 and 4.4, it follows that under the null hypothesis:

                                       lim P (Gn ‚â• cÃÇ1‚àíŒ± ) = 1 ‚àí Œ± ,
                                      n‚Üí‚àû


for any 0 < Œ± < 1. Under the same assumptions, if the alternative hypothesis is instead true, then:

                                         lim P (Gn ‚â• cÃÇ1‚àíŒ± ) = 1 .
                                        n‚Üí‚àû




4.2    Parameter Tests

A principal application of the proposed bootstrap is in obtaining critical values for parametric
hypothesis tests. We consider a general M-estimation framework in which the parameter of interest
Œ∏M is the unique minimizer of some non-stochastic but unknown function Q : Œò ‚Üí R :

                                            Œ∏M = arg min Q(Œ∏) .                                      (17)
                                                      Œ∏‚ààŒò



   We examine the classic problem of conducting inference on a function of Œ∏M . Specifically, for
some known and differentiable mapping c : Œò ‚Üí Rl with l ‚â§ p, the hypothesis we study is:

                                 H0 : c(Œ∏M ) = 0          H1 : c(Œ∏M ) 6= 0 .                         (18)


                                                     11
Standard tests for this hypothesis include the Wald and Lagrange Multiplier (LM) tests. Intuitively,
the Wald test examines whether the value of the function c evaluated at an unrestricted estimator
Œ∏ÃÇM is statistically different from zero. In contrast, the LM test instead checks whether the first
order condition of an estimator Œ∏ÃÇM,R computed imposing the null hypothesis is statistically different
from zero. Therefore, in the nomenclature of Assumption 4.1(i), Œ∏ÃÇ equals Œ∏ÃÇM for the Wald test and
Œ∏ÃÇM,R for the LM test. Similarly, if Œ∏M,R denotes the minimizer of Q over Œò subject to c(Œ∏) = 0, then
Œ∏0 equals Œ∏M and Œ∏M,R under the Wald and LM test respectively.

   We proceed to illustrate the details of the score bootstrap in this setting for both generalized
method of moments (GMM) and maximum likelihood (ML) estimators. We focus on the analytical
expressions An (Œ∏) and s(z, Œ∏) take in those specific settings and provide references for primitive
conditions that ensure Assumptions 4.1, 4.2, 4.3 and 4.4 hold.


4.2.1   ML Estimators


For an ML estimator, the criterion function Q and its sample analogue Qn are of the form:
                                            n
                                         1X
                             Qn (Œ∏) ‚â°          q(Zi , Œ∏)           Q(Œ∏) ‚â° E[q(Zi , Œ∏)] ,                               (19)
                                         n i=1

where q : Z √ó Œò ‚Üí R is the log-likelihood. If q is twice differentiable in Œ∏, then we may define
the Hessian Hn (Œ∏) ‚â° n‚àí1 i ‚àá2 q(Zi , Œ∏). For notational convenience, it is also helpful to denote the
                        P

gradient of the function c evaluated at Œ∏ by C(Œ∏) ‚â° ‚àác(Œ∏).
                                                                                                               ‚àö
Example 4.1. (Wald) The relevant Wald statistic is the studentized quadratic form of                               nc(Œ∏ÃÇM ),
which under both the null and alternative hypothesis satisfies the asymptotic expansion:
                                                                            n
                ‚àö                                            1 X
                    n(c(Œ∏ÃÇM ) ‚àí c(Œ∏M )) = ‚àíC(Œ∏M )Hn‚àí1 (Œ∏M ) ‚àö       ‚àáq(Zi , Œ∏M ) + op (1) .                            (20)
                                                              n i=1

Therefore, the Wald statistic fits the formulation in (13) with An (Œ∏) = ‚àíC(Œ∏)Hn‚àí1 (Œ∏) and s(z, Œ∏) =
‚àáq(z, Œ∏). Under the alternative hypothesis, Gn diverges to infinity since c(Œ∏M ) 6= 0. Refer to Section
3.2 in Newey and McFadden (1994) for a formal justification of these arguments.

Example 4.2. (LM) In this setting, the LM statistic is the normalized quadratic form of:
                                                                 n
                                                           1     X
                                   C(Œ∏ÃÇM,R )Hn‚àí1 (Œ∏ÃÇM,R ) ‚àö            ‚àáq(Zi , Œ∏ÃÇM,R ) .                               (21)
                                                             n   i=1

Moreover, under conditions stated in Chapter 12.6.2 in Wooldridge (2002), we additionally have:
                             n                                                        n
                         1   X                                               1        X
 C(Œ∏ÃÇM,R )Hn‚àí1 (Œ∏ÃÇM,R ) ‚àö          ‚àáq(Zi , Œ∏ÃÇM,R ) =   C(Œ∏M,R )Hn‚àí1 (Œ∏M,R ) ‚àö               ‚àáq(Zi , Œ∏M,R ) + op (1) , (22)
                         n   i=1
                                                                                  n   i=1


                                                           12
under the null hypothesis. Thus, the LM statistic also fits the general formulation in (13) with
                                                                                                     p
An (Œ∏) = C(Œ∏)Hn‚àí1 (Œ∏) and s(z, Œ∏) = ‚àáq(z, Œ∏). Under the alternative, Gn ‚Üí ‚àû provided Œ∏M,R is not
a local minimizer of Q, C(Œ∏M,R )E[‚àá2 q(Zi , Œ∏M,R )] is full rank and Assumption 4.1(ii) holds.


4.2.2       GMM Estimators


In the context of GMM estimation, the criterion function Q and its sample analogue Qn are:
                             n                      n
                          1X                     1X
             Qn (Œ∏) ‚â° [         q(Zi , Œ∏)0 ]‚Ñ¶n [       q(Zi , Œ∏)]           Q(Œ∏) ‚â° E[q(Zi , Œ∏)0 ]‚Ñ¶E[q(Zi , Œ∏)] ,          (23)
                          n i=1                  n i=1

                                                                                                                            p
where q : Z √ó Œò ‚Üí Rk is a known function and ‚Ñ¶n , ‚Ñ¶ are positive definite matrices such that ‚Ñ¶n ‚Üí
‚Ñ¶. Assuming q is differentiable in Œ∏, let Dn (Œ∏) ‚â° n‚àí1 i ‚àáq(Zi , Œ∏) and Bn (Œ∏) ‚â° Dn (Œ∏)0 ‚Ñ¶n Dn (Œ∏). As
                                                      P

in the discussion of ML estimators, we also denote C(Œ∏) ‚â° ‚àác(Œ∏).

Example 4.3. (Wald) The Wald statistic for the hypothesis in (18) is given by the studentized
                 ‚àö
quadratic form of nc(Œ∏ÃÇM ). In the present context we therefore obtain an expansion of the form:
                                                                                           n
               ‚àö                                                           1               X
                   n(c(Œ∏ÃÇM ) ‚àí c(Œ∏M )) =    ‚àíC(Œ∏M )Bn‚àí1 (Œ∏M )Dn (Œ∏M )0 ‚Ñ¶n ‚àö                      q(Zi , Œ∏M ) + op (1) ,   (24)
                                                                                       n   i=1

which implies An (Œ∏) = ‚àíC(Œ∏)Bn‚àí1 (Œ∏)Dn (Œ∏)0 ‚Ñ¶n and s(z, Œ∏) = q(z, Œ∏) and Assumption 4.2(i) is satis-
fied provided E[q(Zi , Œ∏M )] = 0.8 Primitive conditions under which Assumptions 4.1-4.4 hold in this
context can be found in Section 3.3 of Newey and McFadden (1994).

Example 4.4. (LM) In this setting, the LM test statistic is the studentized quadratic form of:
                                                                             n
                                                                        1    X
                                 C(Œ∏ÃÇM,R )Bn‚àí1 (Œ∏ÃÇM,R )Dn (Œ∏ÃÇM,R )0 ‚Ñ¶n ‚àö              q(Zi , Œ∏ÃÇM,R ) ,                    (25)
                                                                        n       i=1

which, as shown in Section 9.1 of Newey and McFadden (1994), is asymptotically equivalent to:
                                                                                n
                                                                     1          X
                                 C(Œ∏M,R )Bn‚àí1 (Œ∏M,R )Dn (Œ∏M,R )0 ‚Ñ¶n ‚àö                 q(Zi , Œ∏M,R )                       (26)
                                                                            n   i=1

under the null hypothesis. Hence, An (Œ∏) = C(Œ∏)Bn‚àí1 (Œ∏)Dn (Œ∏)0 ‚Ñ¶n and s(z, Œ∏) = q(z, Œ∏).
   8
       Notice this is trivially satisfied in a just identified system. The extension to overidentified models in which
E[q(Zi , Œ∏M )] 6= 0 but E[‚àáq(Zi , Œ∏M )0 ]‚Ñ¶E[q(Zi , Œ∏M )] = 0 can be accomplished by letting s(z, Œ∏) depend on n and
setting sn (z, Œ∏) = Dn (Œ∏)0 ‚Ñ¶n g(z, Œ∏). Though straightforward to establish, we do not pursue such an extension.




                                                              13
4.3      Moment Restrictions

An additional application of the bootstrap procedure we consider is for testing the hypothesis:

                          H0 : E[m(Zi , Œ∏M )] = 0          H1 : E[m(Zi , Œ∏M )] 6= 0 ,                   (27)

where m : Z √ó Œò ‚Üí Rl is a known function and Œ∏M is the minimizer of some unknown non-
stochastic Q : Œò ‚Üí R. Such restrictions arise, for example, in tests of proper model specification
and hypotheses regarding average marginal effects in nonlinear models. As in Section 4.2, the
specific nature of the bootstrap statistic is dependent on whether Q is as in (19) (ML) or as in (23)
(GMM). For brevity, we focus on the former, though the extension to GMM can be readily derived
following manipulations analogous to those in Example 4.3.

   The Wald test statistic for the hypothesis in (27) is based on the studentized plug-in estimator:
                                                  n
                                              1 X
                                             ‚àö       m(Zi , Œ∏ÃÇM ) ,                                     (28)
                                               n i=1

where Œ∏ÃÇM is in this case the unconstrained minimizer of Qn on Œò. Hence, in this setting Œ∏0 equals
Œ∏M and Œ∏ÃÇ equals Œ∏ÃÇM in the notation of Assumption 4.1(i). Obtaining an expansion for Tn as in (13)
is straightforward provided m and q are once and twice continuously differentiable in Œ∏ respectively.
Defining the gradient Mn (Œ∏) ‚â° n‚àí1 i ‚àám(Zi , Œ∏) and Hessian Hn (Œ∏) ‚â° n‚àí1 i ‚àá2 q(Zi , Œ∏), standard
                                  P                                        P

arguments imply that under the null hypothesis:
           n                      n
       1 X                    1 X                                      1 X
      ‚àö       m(Zi , Œ∏ÃÇM ) = ‚àö       m(Zi , Œ∏M ) ‚àí Mn (Œ∏M )Hn‚àí1 (Œ∏M ) ‚àö       ‚àáq(Zi , Œ∏M ) + op (1) ;   (29)
        n i=1                  n i=1                                    n i=1

see Newey (1985a) for primitive conditions for (29). Thus, in this setting s(z, Œ∏) and An (Œ∏) are:
                         Ô£´           Ô£∂
                             m(z, Œ∏)
                                                                                
                                                            .
               s(z, Œ∏) = Ô£≠           Ô£∏          An (Œ∏) = I .. ‚àí Mn (Œ∏)Hn (Œ∏) .
                                                                           ‚àí1
                                                                                                 (30)
                            ‚àáq(z, Œ∏)

Moreover, if Œ∏M is an interior point of Œò, then E[‚àáq(Zi , Œ∏M )] = 0 because Œ∏M minimizes Q on Œò.
                  p
Therefore, Gn ‚Üí ‚àû under the alternative hypothesis due to E[m(Zi , Œ∏M )] 6= 0.

Remark 4.2. Similar manipulations may be employed to show the score bootstrap can be applied
to Wald tests in two stage parametric estimation problems. Unlike the nonparametric bootstrap,
however, such a procedure would require an analytical derivation of the effect of the first stage
estimator on the influence function of the second estimator.




                                                      14
4.3.1       ML Specification Tests


A prominent application of hypotheses as in (27) is in model specification testing. In particular, this
setting encompasses moment based specification tests (‚Äúm-tests‚Äù) for maximum likelihood models,
as considered in White (1982, 1994), Newey (1985b) and Tauchen (1985).9 Computations are
simplified for ML models by the generalized information matrix equality, which implies:

 E[‚àá2 q(Zi , Œ∏M )] = ‚àíE[‚àáq(Zi , Œ∏M )‚àáq(Zi , Œ∏M )0 ]             E[‚àám(Zi , Œ∏M )] = ‚àíE[m(Zi , Œ∏M )‚àáq(Zi , Œ∏M )0 ] .


       For example, as noted in Chesher (1984) and Newey (1985b), computation of the Wald test
statistic for the null hypothesis in (27) can be performed through the auxiliary regression:

                                      1 = m(Zi , Œ∏ÃÇM )0 Œ¥ + ‚àáq(Zi , Œ∏ÃÇM )0 Œ≥ + i .                               (31)

Equation (31) is often termed the Outer Product of Gradient (OPG) regression form for moment
tests; see Chapter 15.2 in Davidson and MacKinnon (2004) or Chapter 8.2.2 in Cameron and Trivedi
(2005). If R2 is the uncentered R-squared of the regression in (31), then under the generalized
information matrix equality result in (31) the Wald test statistic is asymptotically equivalent to:

                                                     Gn = nR2 .                                                   (32)


       The calculation of the score bootstrap simplifies in an analogous fashion. Under a uniform law
of large numbers, we obtain that An (Œ∏ÃÇM ) as defined in (30) satisfies,
                       n                                   n                              i‚àí1 i
            h .     1X                                 h1 X
 An (Œ∏ÃÇM ) = I .. ‚àí       m(Zi , Œ∏ÃÇM )‚àáq(Zi , Œ∏ÃÇM )0 √ó        ‚àáq(Zi , Œ∏ÃÇM )‚àáq(Zi , Œ∏ÃÇM )0       + op (1) , (33)
                    n i=1                               n i=1

under the null hypothesis. As a result, the score bootstrap has a simple interpretation in terms of
the multivariate regression of the moments m(Zi , Œ∏ÃÇM ) on ‚àáq(Zi , Œ∏ÃÇM ):

                                      m(1) (Zi , Œ∏ÃÇM ) = ‚àáq(Zi , Œ∏ÃÇM )0 Œ≤1 + 1,i
                                                ..            ..
                                                 .     =       .                  ,                               (34)
                                      m(l) (Zi , Œ∏ÃÇM ) = ‚àáq(Zi , Œ∏ÃÇM )0 Œ≤l + l,i

where m(j) (Zi , Œ∏ÃÇM ) is the j th component of m(Zi , Œ∏ÃÇM ). Letting ej,i ‚â° m(j) (Zi , Œ∏ÃÇM ) ‚àí ‚àáq(Zi , Œ∏ÃÇM )0 Œ≤ÃÇj
be the fitted residual of the j th regression and ei = (e1,i , . . . , el,i )0 , we obtain that:
                                                                 n
                                                           1 X
                                             Sn‚àó (Œ∏ÃÇM ) = ‚àö       ei Wi .                                         (35)
                                                            n i=1
   9
       A bootstrap construction for the Information Matrix Equality test was also developed in Horowitz (1994).

                                                           15
Therefore, G‚àón is simply the Wald test for the null hypothesis that the mean of ei Wi equals zero.

    In summary, if the generalized information matrix equality holds, then in testing (27) we may
employ the following simple algorithm:

Step 1: Run the regression in (31) and compute the uncentered R-squared to obtain Gn as in (32).

Step 2: Regress {m(Zi , Œ∏ÃÇM )}ni=1 on {‚àáq(Zi , Œ∏ÃÇM )}ni=1 to generate residual vectors {ei }ni=1 .

Step 3: Using random weights {Wi }ni=1 independent of {Yi , Xi }ni=1 with E[Wi ] = 0 and E[Wi2 ] = 1,
perturb the original residual vectors {ei }ni=1 to obtain a new set of residual vectors {ei Wi }ni=1 .

Step 4: Let G‚àón be the Wald test statistic for the null that E[ei Wi ] = 0 calculated using {ei Wi }ni=1 .
To control size at level Œ±, reject if Gn is larger than the 1 ‚àí Œ± quantile of G‚àón conditional on {Zi }ni=1 .



4.4     Clustered Data

Theorem 4.1 and Corollary 4.1 may be applied to clustered data provided clusters are i.i.d. with
the same number of observations. Extensions to settings where the clusters are unbalanced or there
is heteroskedasticity across them are feasible, essentially requiring an extension of Theorem 4.1 to
independent but not identically distributed observations.

    Let Zic denote observation number i in cluster c, J be the total number of observations per
cluster, n be the total number of clusters and Zc = {Z1c , . . . , ZJc }. Following (13), we consider test
statistics of the general form GÃÉn ‚â° TÃÉn0 TÃÉn , where TÃÉn satisfies:
                                                                                         n     J
                                       1                                             1 X 1 X
  TÃÉn = (An (Œ∏0 )Œ£ÃÉn (Œ∏0 )An (Œ∏0 )0 )‚àí 2 SÃÉn (Œ∏0 ) + op (1)        SÃÉn (Œ∏) ‚â° An (Œ∏) ‚àö      ‚àö      sÃÉ(Zic , Œ∏) , (36)
                                                                                      n c=1 J i=1

An (Œ∏) is again a r √ó m matrix, sÃÉ(z, Œ∏) maps each (Zic , Œ∏) into a m √ó 1 vector and Œ£ÃÉn (Œ∏) is a
robust covariance matrix that allows for arbitrary correlation within cluster. The Wald and LM
test statistics, as well as the moment restriction tests previously discussed all extend to this setting
when observations are allowed to be dependent within clusters.

    The applicability of Theorem 4.1 and Corollary 4.1 to the present context is immediate once we
notice that we may define s(z, Œ∏), mapping each (Zc , Œ∏) into a m √ó 1 vector, to be given by:
                                                              J
                                                          1 X
                                             s(Zc , Œ∏) = ‚àö       sÃÉ(Zic , Œ∏) .                                 (37)
                                                           J i=1

The statistics TÃÉn and SÃÉn (Œ∏) are then special cases of Tn and Sn (Œ∏) as considered in (13) but with Zc
in place of Zi . Hence, equations (13) and (37) indicate that the relevant bootstrap statistic should

                                                              16
perturb the data at the level of the cluster rather than the individual observation. We thus define:
                                                                                                        n     J
                                                                 1                                  1 X Wc X
GÃÉ‚àón   ‚â°   TÃÉn‚àó0 TÃÉn‚àó    TÃÉn‚àó   ‚â°   (An (Œ∏ÃÇ)Œ£ÃÉ‚àón (Œ∏ÃÇ)An (Œ∏ÃÇ)0 )‚àí 2 SÃÉn‚àó (Œ∏ÃÇ)   SÃÉn‚àó (Œ∏)   ‚â° An (Œ∏) ‚àö      ‚àö      sÃÉ(Zic , Œ∏)
                                                                                                     n c=1 J i=1

where Œ£ÃÉ‚àón (Œ∏) is a robust bootstrap covariance matrix for s(Zic , Œ∏)Wc .

      Given these definitions, it is readily apparent that GÃÉ‚àón , TÃÉn‚àó and SÃÉn‚àó (Œ∏) are themselves special cases
of the bootstrap statistics G‚àón , Tn‚àó and Sn‚àó (Œ∏). The consistency of the proposed score bootstrap then
follows immediately provided the clusters are i.i.d., the number of clusters tends to infinity and
s(z, Œ∏) as defined in (37) satisfies Assumption 4.1(ii), 4.2(i) and 4.4(ii).

Corollary 4.2. Under Assumptions 4.1, 4.2, 4.3 and 4.4, it follows that under the null hypothesis:

                                                   lim P (GÃÉn ‚â• cÃÇ1‚àíŒ± ) = 1 ‚àí Œ± ,
                                                  n‚Üí‚àû

for any 0 < Œ± < 1. Under the same assumptions, if the alternative hypothesis is instead true, then:

                                                      lim P (GÃÉn ‚â• cÃÇ1‚àíŒ± ) = 1 .
                                                     n‚Üí‚àû




5       Simulation Evidence

To assess the small sample behavior of the score bootstrap we conduct a series of Monte Carlo
experiments examining the performance of bootstrap Wald and LM tests of hypotheses regarding
the parameters of a linear model estimated by OLS and a nonlinear probit model estimated by
maximum likelihood. We also examine the performance of a test for the presence of intra cluster
correlation in the probit model. Because small sample issues often arise in settings with dependent
data, we work with hierarchical data generating processes (DGPs) exhibiting dependence of micro-
units i within independent clusters c. We consider balanced panels with 20 observations per cluster
and sampling designs ranging from 5 to 200 independent clusters.10

      In order to allow a comparison of the wild and score bootstraps with the traditional nonpara-
metric block bootstrap, we consider a setting with continuous regressors so that the block bootstrap
distribution may be computed in small samples. It is important to note, however, that in many
studies the regressor of interest will have discrete or binary support, in which case the statistic of
interest will be undefined in bootstrap samples where only one value of the regressor is sampled.
Moreover, even in bootstrap draws where the regressor of interest does exhibit variation, the Hes-
sian may not be full rank. In such settings the traditional resampling based bootstrap will not be
viable and the case for consideration of the wild and score bootstraps will be much stronger.
 10
      In unreported results we found our results to be insensitive to variation in the number of observations per cluster.

                                                                     17
5.1        Designs

As pointed out by Chesher (1995), symmetric Monte Carlo designs are likely to yield an overly
optimistic assessment of the ability of testing procedures to control size. For this reason we study
the performance of our proposed bootstrap procedures under a variety of different designs meant
to reflect realistic features of microeconomic datasets. Throughout, the linear model we examine is
given by:
                                            Yic = Xic + Dc + Œ∑c + ic ,                                        (38)

where the regressors (Xic , Dc ) and cluster level error (Œ∑c ) are generated according to:

                                  Xic = Xc + Œæic          Œ∑c = (1 + Dc + Xc )œÖc .                              (39)

The regressor of interest is Dc , which varies only at the cluster level. Note that the cluster level
random effect Œ∑c exhibits heteroscedasticity with respect to Dc and Xic . The designs are:

Design I: (baseline) We let (Xc , Dc , Œæic , ic ) be normally distributed with identity covariance
matrix, and œÖc independent of other variables with a t-distribution with six degrees of freedom.

Design II: (skewed regressor) Design I is modified to generate Dc according to a mixture between
a N (0, 1) with probability 0.9 and a N (2, 9) with probability 0.1 as in Horowitz (1997). This yields
a regressor with occasional ‚Äúoutliers‚Äù and substantial skew and kurtosis in its marginal distribution.

Design III: (misspecification) The model estimated is still (38), but the DGP is modified to:

                                        Yic = Xic + Dc + .1Dc2 + Œ∑c + ic ,                                    (40)

and other features remain as in Design I. Hence, the quadratic term in the regressor of interest
is ignored in estimation which yields a skewed reduced form regression error. Note that E[Dc3 ] =
E[Xic Dc2 ] = 0 which ensures the population regression coefficient on Dc is still one.

       To study the performance of the score bootstrap in a nonlinear model we consider probit esti-
mation of the following data generating process:

                           Yic = 1{Xic + Dc + Œ∑c + ic ‚â• 0}              Xic = Xc + Œæic .                      (41)

This is essentially a latent variable representation of the model in (38) without heteroscedasticity
in the group error Œ∑c . We consider the following two designs for our probit analysis:

Design IV: (baseline probit) In (41), we let (Xc , Dc , Œæic ) ‚àº N (0, I3 /16) and (Œ∑c , ic ) ‚àº N (0, I2 /2).11
  11
       Though the DGP contains a cluster level random effect, the marginal model for the outcome given covariates is
a standard probit ensuring that conventional maximum likelihood estimation is consistent.

                                                          18
Design V: (skew probit) We modify Design IV by generating Dc according to a mixture distri-
bution as in Design II, so that the regressor of interest is heavily skewed.

       Finally, we illustrate the methods of Section 4.3 by testing for the presence of intra cluster
correlation in the probit model. Specifically, we test the hypothesis:
                                 X20     X                                 20
                                                                           X           X
                          H0 : E[    ŒΩic   ŒΩjc ] = 0             H1 : E[         ŒΩic          ŒΩjc ] 6= 0 ,         (42)
                                   i=1     j6=i                            i=1         j6=i
                            p
where ŒΩic = [Yic ‚àípic ]/       pic (1 ‚àí pic ) is a generalized residual and pic = Œ¶(Xic +Dc ) is the conditional
probability that Yic equals one given Dc and Xic .12 Note that under the probit model E[ŒΩic ] = 0
and E[ŒΩic2 ] = 1. A test of H0 examines whether within cluster dependence is present in the data, the
finding of which might suggest the presence of an unmodeled cluster level random effect. In order
to ensure the null hypothesis is true, we employ designs IV and V but set Œ∑c = 0 almost surely and
change the variance of ic to equal one.



5.2        Results

Table 1 provides empirical false rejection rates from 10,000 Monte Carlo repetitions of Wald and
LM tests of the null that the population least squares coefficient on Dc in (38) is one. All tests
have a nominal size of 5% and are studentized using a recentered variance estimator.13 Bootstrap
tests are computed via bootstrap p-values based on 199 repetitions, and reject the null hypothesis
whenever the p-value is less than 0.05. Stata code for our Monte Carlo experiments is available
online.14

       We consider implementations of the score bootstrap using both Rademacher weights and the
skew correcting weights suggested by Mammen (1993). For comparison with the various score
bootstraps we also compute the empirical rejection rates of Wald and LM tests based upon analytical
clustered standard errors, the original wild bootstrap of Liu (1988), and the pairs-based block
bootstrap. Following the results of Davidson and MacKinnon (1999b) on the value of ‚Äúimposing the
null hypothesis‚Äù on bootstrap tests, we include in our exercise a variant of the wild bootstrap studied
in Cameron, Gelbach, and Miller (2008), which perturbs the restricted score contributions obtained
from estimates constraining the coefficient on Dc to equal one, a procedure we term ‚ÄúWild2‚Äù. We
also examine the performance of an analogous score bootstrap, which we term ‚ÄúScore2,‚Äù that works
  12
       See McCall (1994) and Card and Hyslop (2005) for further examples of the use of generalized residual correlations
as specification diagnostics.
  13
     We also make a finite sample degrees of freedom correction of n/(n ‚àí 1) to all variance estimators.
  14
     URL: http://www.econ.berkeley.edu/~pkline/papers/score.zip

                                                            19
                                                   Table 1: Empirical Rejection Rates - Linear Model

                                     Normal Regressor                            Mixture Regressor                         Misspecification
        Wald Tests       n=5     n = 10   n = 20   n = 50   n = 200   n=5     n = 10   n = 20   n = 50   n = 200   n=5     n = 10    n = 20   n = 50   n = 200
         Analytical      0.381   0.216    0.130    0.085     0.059    0.443   0.322    0.255    0.168     0.087    0.383    0.219    0.132    0.087     0.058
           Pairs         0.013   0.046    0.041    0.039     0.045    0.018   0.082    0.084    0.068     0.039    0.013    0.046    0.042    0.040     0.046
     Wild Rademacher     0.178   0.098    0.066    0.057     0.051    0.229   0.191    0.143    0.086     0.053    0.179    0.100    0.067    0.056     0.051
      Wild Mammen        0.175   0.123    0.090    0.069     0.054    0.229   0.185    0.149    0.107     0.063    0.177    0.125    0.093    0.071     0.053
     Wild2 Rademacher    0.057   0.059    0.055    0.052     0.051    0.077   0.083    0.070    0.056     0.050    0.062    0.060    0.056    0.052     0.050
      Wild2 Mammen       0.051   0.024    0.027    0.036     0.048    0.066   0.035    0.041    0.036     0.032    0.050    0.027    0.026    0.035     0.046
     Score Rademacher    0.267   0.184    0.127    0.092     0.063    0.325   0.301    0.268    0.203     0.107    0.269    0.189    0.130    0.093     0.062




20
      Score Mammen       0.374   0.248    0.165    0.104     0.063    0.437   0.363    0.310    0.222     0.114    0.375    0.253    0.165    0.105     0.062
     Score2 Rademacher   0.042   0.058    0.053    0.055     0.051    0.047   0.082    0.077    0.063     0.052    0.044    0.059    0.053    0.054     0.051
      Score2 Mammen      0.043   0.036    0.059    0.063     0.053    0.046   0.045    0.067    0.068     0.054    0.043    0.037    0.058    0.061     0.052


                                     Normal Regressor                            Mixture Regressor                         Misspecification
         LM Tests        n=5     n = 10   n = 20   n = 50   n = 200   n=5     n = 10   n = 20   n = 50   n = 200   n=5     n = 10    n = 20   n = 50   n = 200
         Analytical      0.157   0.090    0.065    0.056     0.052    0.143   0.088    0.054    0.037     0.039    0.157    0.092    0.067    0.055     0.050
           Pairs         0.020   0.041    0.039    0.039     0.047    0.021   0.039    0.045    0.044     0.035    0.020    0.041    0.040    0.038     0.048
     Score Rademacher    0.105   0.101    0.079    0.065     0.053    0.121   0.145    0.110    0.074     0.054    0.098    0.104    0.080    0.064     0.054
      Score Mammen       0.080   0.031    0.035    0.044     0.050    0.096   0.053    0.049    0.040     0.034    0.078    0.032    0.035    0.043     0.049
                                 Table 2: Empirical Rejection Rates, Probit

                                      Normal Regressor                                Mixture Regressor
       Wald Tests        n=5      n = 10   n = 20   n = 50    n = 200     n=5     n = 10   n = 20   n = 50   n = 200
       Analytical        0.326    0.167    0.104    0.070      0.051      0.308    0.170    0.100   0.065     0.058
          Pairs           n.a.    0.055    0.060    0.055      0.048       n.a.    0.052    0.060   0.052     0.050
 Score Rademacher        0.186    0.128    0.090    0.065      0.049      0.167    0.125    0.085   0.060     0.056
   Score Mammen          0.306    0.177    0.108    0.070      0.049      0.279    0.176    0.105   0.065     0.057
 Score2 Rademacher       0.140    0.090    0.069    0.059      0.050      0.140    0.113    0.092   0.071     0.058
  Score2 Mammen          0.159    0.096    0.070    0.057      0.050      0.160    0.121    0.092   0.070     0.058


                                      Normal Regressor                                Mixture Regressor
        LM Tests         n=5      n = 10   n = 20   n = 50    n = 200     n=5     n = 10   n = 20   n = 50   n = 200
       Analytical        0.171    0.106    0.080    0.062      0.050      0.158    0.110    0.080   0.060     0.055
         Pairs15          n.a.    0.083    0.082    0.064      0.053       n.a.    0.076    0.081   0.066     0.053
 Score Rademacher        0.081    0.079    0.066    0.057      0.049      0.075    0.082    0.069   0.057     0.054
   Score Mammen          0.061    0.023    0.038    0.050      0.048      0.058    0.025    0.041   0.053     0.053




with an estimator‚Äôs restricted score but employs an unrestricted variance estimate. Details of the
various procedures are described in the Implementation Appendix.

      The standard clustered Wald test severely over-rejects in samples with few clusters, with per-
formance further degrading when the regressor of interest is generated according to a mixture dis-
tribution. Mild misspecification of the sort captured by Design III has little effect on the rejection
rates of any of the procedures. A conventional pairs bootstrap of the Wald test yields dramatic im-
provements in size control though its performance degrades somewhat when the regressor of interest
exhibits outliers. Wild bootstrapping the Wald test yields improvements over analytical methods
but under performs relative to pairs regardless of whether Mammen or Rademacher weights are
used. As suggested by our theoretical results, the score bootstrap yields improvements over ana-
lytical methods but is somewhat outperformed by the wild bootstrap particularly in the skewed
regressor design. Our variants of the score and wild bootstrap Wald tests that work with restricted
residuals perform much better than their unrestricted counterparts. Both Wald2 and Score2 yield
performance on par with Pairs even under the relatively difficult skewed regressor design.

      In contrast to the Wald tests, the LM tests appear to perform well across a range of sample
sizes, regardless of the distribution of the regressors. It is only in samples with very few clusters
 15
      We were unable to compute the LM statistic in the majority of pairs draws with 5 clusters.



                                                         21
                         Table 3: Empirical Rejection Rates, m-Test (Probit)

                                    Normal Regressor                                            Mixture Regressor
      Wald Tests       n=5     n = 10   n = 20     n = 50        n = 200           n=5       n = 10   n = 20   n = 50   n = 200
       Analytical      0.767    0.448     0.257        0.136        0.076          0.772     0.439    0.253    0.142     0.072
 Score Rademacher      0.441    0.394     0.233        0.130        0.073          0.437     0.386    0.229    0.134     0.072
  Score Mammen         0.370    0.237     0.223        0.128        0.069          0.353     0.234    0.218    0.134     0.068




that the analytical LM test yields significant overrejection. Score bootstrapping the LM statistic
with Mammen weights largely removes these distortions as does application of the nonparametric
pairs bootstrap.

      Table 2 examines the performance of Wald and LM tests in the probit model. Here both
Wald and LM tests tend to overreject when asymptotic critical values are used. Use of the pairs
bootstrap corrects for this overrejection though in small samples we were sometimes unable to
compute the bootstrap distribution.16 Score bootstrapping the Wald test yields improvements over
analytical clustered standard errors but substantial overrejection remains in small samples. Use of
the restricted score variant of the test yields smaller improvements than were found with OLS. Score
bootstrapping the LM test with Mammen weights, on the other hand, yields size control roughly
on par with the pairs bootstrap.

      Table 3 examines the performance of tests for intra cluster correlation of the generalized residuals
in the probit model, as in (42).17 Because the information matrix equality holds under both DGPs
we use the outer product version of the test described in 4.3.1 generalized to allow for clustering.
We see that the analytical m-test procedure overrejects substantially in small samples. Both score
bootstraps partially correct this problem, though they significantly overreject as well. With 200
clusters, the analytical and bootstrap approaches appear to work equally well.

                               Table 4: Computational Time (in seconds)

                                            Score LM           Score2   Wild2        Pairs
                                    OLS           13            15          116       278
                                   Probit         18            21          n.a.    30,718




      Finally, to illustrate the dramatic computational advantages of the score bootstrap relative to
 16
      We discarded simulations for which we were unable to compute an estimate in some bootstrap draws.
 17
      A description of the implementation of this test can be found in the Implementation Appendix


                                                               22
the wild bootstrap and pairs resampling, Table 4 presents the time elapsed in conducting bootstrap
Wald tests using 9,999 bootstrap repetitions of the Score LM, Score2, Wild2, and pairs bootstrap
procedures on a simulated dataset with twenty clusters. These computations were performed in
Stata/SE 11.1 on a single core of a 2.3 Ghz Quad Core AMD Opteron Processor running Linux.

    For OLS, the score bootstrap yields nearly an order of magnitude improvement in computational
time over the Wild bootstrap and more than a twenty fold improvement over pairs. For the probit
model the results are even more striking. The score bootstrap is more than 1,000 times faster than
nonparametric pairs resampling.



6     Conclusion

The score bootstrap provides a substantial computational advantage over the wild and pairs boot-
straps and may easily be applied to estimators that lack conventional residuals. Our Monte Carlo
experiments suggest these computational advantages come at little cost in terms of performance.
Particularly when applied to LM test statistics, the score bootstrap tends to yield substantial
improvements over traditional asymptotic testing procedures in small sample environments and
exhibits performance comparable to more computationally expensive bootstrap procedures.




                                               23
                                       Implementation Details

   We provide here implementation details for the various bootstrap procedures discussed in the
Monte Carlo study of Section 5. We restrict our discussion to the linear model and to the test
of hypothesis (42), as the generalization to the probit model is straightforward but notationally
intensive. Throughout, the linear model we consider is given by:

                                                  Yic = Xic0 Œ≤0 + ic ,                                          (43)

where Œ≤0 ‚àà Rm and (Yic , Xic ) denotes observation i in cluster c with J the total number of obser-
vations per cluster and n the total number of clusters. We examine hypotheses of the form:

                                       H0 : RŒ≤0 = r               H1 : RŒ≤0 6= r ,                                (44)

where R is a d √ó m matrix and r a d √ó 1 column vector. In our Monte Carlo simulations, R is a
vector that selects the coefficient corresponding to Dc (in (38)) and r = 1.

Wald Tests

   A number of bootstrap procedures we consider provide approximations to the distribution of:

                           Tn ‚â° nJ(RŒ≤ÃÇu ‚àí r)0 (RHn‚àí1 Œ£n (Œ≤ÃÇu )Hn‚àí1 R0 )‚àí1 (RŒ≤ÃÇu ‚àí r) ,                           (45)
                                                  Pn PJ
where Œ≤ÃÇu is the OLS estimator, Hn ‚â°          1
                                             nJ      c=1    i=1   Xic Xic0 and for eic (Œ≤) ‚â° (Yic ‚àí Xic0 Œ≤) we let:
                           n    J                              J                       0
                     1    X    1X                          1 X
         Œ£n (Œ≤) ‚â°                   {Xic eic (Œ≤) ‚àí Xe(Œ≤)}          {Xic eic (Œ≤) ‚àí Xe(Œ≤)} ,                       (46)
                  (n ‚àí 1) c=1 J i=1                          J i=1
                1
                    Pn PJ
and Xe(Œ≤) ‚â°    nJ    c=1     i=1   Xic eic (Œ≤). Tests employing analytical critical values reject at level Œ± if
Tn is larger than the 1 ‚àí Œ± quantile of a Xd2 random variable. Note that in Section 5, d = 1 and
hence the analytical critical value is approximately 3.841.

Wald - Pairs Cluster Bootstrap

   Let Xc = {Xic }Ji=1 and Yc = {Yic }Ji=1 . The pairs cluster bootstrap draws with replacement
n observations from {Yc , Xc }nc=1 ‚Äì here an ‚Äúobservation‚Äù is an entire cluster. For {YÃÉc , XÃÉc }nc=1 the
                                                                    1
                                                                      Pn PJ               0
bootstrap sample, we then compute the OLS estimator Œ≤ÃÉu , HÃÉn ‚â° nJ       c=1   i=1 XÃÉic XÃÉic and:


                     TÃÉn ‚â° nJ(RŒ≤ÃÉu ‚àí RŒ≤ÃÇu )0 (RHÃÉn‚àí1 Œ£ÃÉn (Œ≤ÃÉu )HÃÉn‚àí1 R0 )‚àí1 (RŒ≤ÃÉu ‚àí RŒ≤ÃÇu ) ,                     (47)

where Œ£ÃÉn (Œ≤) is computed as Œ£n (Œ≤) with {YÃÉc , XÃÉc }nc=1 in place of {Yc , Xc }nc=1 . The pairs cluster
bootstrap test then rejects the null hypothesis at level Œ± if:

                                         P (TÃÉn ‚â• Tn |{Yc , Xc }nc=1 ) < Œ± .                                     (48)

                                                           24
In practice, the probability in (48) can be accurately approximated by simulation of TÃÉn . That is
we: (i) Draw B samples {YÃÉc , XÃÉc }nc=1 and obtain for sample b a statistic TÃÉn,b ; (ii) Reject the null
hypothesis if the proportion of bootstrap draws {TÃÉn,b }B
                                                        b=1 that is larger than Tn is less than Œ±.


Wald - Score Cluster Bootstrap

   The score bootstraps we study are constructed employing the score and variance statistics:
                     n      J
                  1 X      X
     Sn‚àó (Œ≤)   ‚â°        Wc     Xic eic (Œ≤)                                                               (49)
                 nJ c=1    i=1
                          n     J                              J                        0
                    1    X    Wc X                     ‚àó
                                                           W X
                                                              c                        ‚àó
    Œ£‚àós
     n (Œ≤)     ‚â°                   {Xic eic (Œ≤) ‚àí Xe(Œ≤) }          {Xic eic (Œ≤) ‚àí Xe(Œ≤) } ,              (50)
                 (n ‚àí 1) c=1 J i=1                           J i=1
                ‚àó         Pn
                      1
                                 Wc ( Ji=1 Xic eic (Œ≤)), and as in Section 4.4 {Wc }nc=1 is a sample of random
                                     P
where Xe(Œ≤) ‚â°        nJ    c=1

weights satisfying E[Wc ] = 0 and E[Wc2 ] = 1. The score bootstrap statistics we consider are then:

                       ‚àós
                      Tn,1 ‚â° (RHn‚àí1 Sn‚àó (Œ≤ÃÇu ))0 (RHn‚àí1 Œ£‚àós       ‚àí1 0 ‚àí1  ‚àí1 ‚àó
                                                         n (Œ≤ÃÇu )Hn R ) (RHn Sn (Œ≤ÃÇu ))                  (51)
                       ‚àós
                      Tn,2 ‚â° (RHn‚àí1 Sn‚àó (Œ≤ÃÇr ))0 (RHn‚àí1 Œ£‚àós       ‚àí1 0 ‚àí1  ‚àí1 ‚àó
                                                         n (Œ≤ÃÇu )Hn R ) (RHn Sn (Œ≤ÃÇr ))                  (52)

                                                                        ‚àós
for Œ≤ÃÇr the OLS estimate of (43) restricted to satisfy RŒ≤ÃÇr = r. Here, Tn,1 corresponds to ‚ÄúScore
                        ‚àós
Wald‚Äù in Section 5 and Tn,2 to ‚ÄúScore2 Wald‚Äù. Both procedures reject at level Œ± if:

                                              ‚àós
                                          P (Tn,i ‚â• Tn |{Yc , Xc }nc=1 ) < Œ±                             (53)

for i ‚àà {1, 2}. As is the case for the pairs cluster bootstrap, the probability in (53) need not be
computed analytically but may be approximated through simulation. To this end: (i) Draw B
                                                          ‚àós
samples of {Wc }nc=1 and employ each b sample to compute Tn,i,b for either i ‚àà {1, 2}; (ii) Reject the
                                             ‚àós
null hypothesis if the proportion of draws {Tn,i,b }B
                                                    b=1 that is larger than Tn is smaller than Œ±.


Wald - Wild Cluster Bootstrap

   The wild bootstrap procedures require the generation of new dependent variables:

                                           Yic‚àó (Œ≤) ‚â° Xic0 Œ≤ + eic (Œ≤)Wc ,                               (54)

where {Wc }nc=1 is independent of {Yc , Xc }nc=1 and satisfy E[Wc ] = 0 and E[Wc2 ] = 1. For Yc‚àó (Œ≤) ‚â°
{Yic‚àó (Œ≤)}Ji=1 , let Œ≤ÃÇ2‚àó and Œ≤ÃÇ1‚àó denote the OLS estimator on the bootstrap samples {Yc‚àó (Œ≤ÃÇr ), Xc }nc=1 and
{Yc‚àó (Œ≤ÃÇu ), Xc }nc=1 respectively ‚Äì i.e. Œ≤ÃÇ2‚àó differs from Œ≤ÃÇ1‚àó in that for the former the null hypothesis
is imposed in the bootstrap distribution. Similarly, let Œ£‚àów           ‚àów
                                                          n,1 (Œ≤) and Œ£n,2 (Œ≤) denote the analogues

of Œ£n (Œ≤) (in (46)) but respectively employing {Yc‚àó (Œ≤ÃÇu ), Xc }nc=1 and {Yc‚àó (Œ≤ÃÇr ), Xc }nc=1 in place of



                                                         25
{Yc , Xc }nc=1 . The two wild bootstrap statistics we consider are then:

                       ‚àów
                      Tn,1 ‚â° (RHn‚àí1 Sn‚àó (Œ≤ÃÇu ))0 (RHn‚àí1 Œ£‚àów     ‚àó   ‚àí1 0 ‚àí1  ‚àí1 ‚àó
                                                         n,1 (Œ≤ÃÇ1 )Hn R ) (RHn Sn (Œ≤ÃÇu ))                              (55)
                       ‚àów
                      Tn,2 ‚â° (RHn‚àí1 Sn‚àó (Œ≤ÃÇr ))0 (RHn‚àí1 Œ£‚àów     ‚àó   ‚àí1 0 ‚àí1  ‚àí1 ‚àó
                                                         n,2 (Œ≤ÃÇ2 )Hn R ) (RHn Sn (Œ≤ÃÇr )) ,                            (56)

       ‚àów                                          ‚àów
where Tn,1 corresponds to ‚ÄúWild‚Äù in Section 5 and Tn,2 to ‚ÄúWild2‚Äù. Both wild bootstrap procedures
then reject the null hypothesis at level Œ± whenever:

                                               ‚àów
                                           P (Tn,i ‚â• Tn |{Yc , Xc }nc=1 ) < Œ±                                          (57)

for i ‚àà {1, 2}. As for the score bootstrap, we may: (i) Draw B samples of {Wc }nc=1 and compute
 ‚àów
Tn,i,b for either i ‚àà {u, √∏}; (ii) Reject the null hypothesis if the proportion of computed bootstrap
             ‚àów B
statistics {Tn,i,b }b=1 that is larger than Tn is smaller than Œ±.

Lagrange Multiplier (LM) Tests

   Alternatively, we may also test the hypothesis in (44) employing a Lagrange Multiplier test. Let
                                                       n   J
                                                    1 XX
                                          Sn (Œ≤) ‚â°            Xic eic (Œ≤) ,                                            (58)
                                                   nJ c=1 i=1

which constitutes the full sample analogue of Sn‚àó (Œ≤) as in (49). The LM test statistic is then:

                      Ln ‚â° nJ(RHn‚àí1 Sn (Œ≤ÃÇr ))0 (RHn‚àí1 Œ£n (Œ≤ÃÇr )Hn‚àí1 R0 )‚àí1 (RHn‚àí1 Sn (Œ≤ÃÇr )) ,                        (59)

which converges in distribution to a Xd2 random variable under the null hypothesis. Therefore, the
analytical LM test rejects at level Œ± when Ln is larger than the 1 ‚àí Œ± quantile of Xd2 .

LM - Pairs Cluster Bootstrap

   As in the Wald pairs cluster bootstrap, let {YÃÉc , XÃÉc }nc=1 denote a bootstrap sample drawn with
replacement from {Yc , Xc }nc=1 . For Œ≤ÃÉr the OLS estimator on {YÃÉc , XÃÉc }nc=1 restricted to satisfy RŒ≤ÃÉr = r,
       1
         Pn PJ              0
HÃÉn ‚â° nJ   c=1   i=1 XÃÉic XÃÉic and Œ£ÃÉn (Œ≤) and SÃÉn (Œ≤) the analogues of Œ£n (Œ≤) (in (46)) and SÃÉn (Œ≤) (in

(58)) but employing {YÃÉc , XÃÉc }nc=1 in place of {Yc , Xc }nc=1 , define the bootstrap LM statistic:

 LÃÉn ‚â° nJ(R(HÃÉn‚àí1 SÃÉn (Œ≤ÃÉr ) ‚àí Hn‚àí1 Sn (Œ≤ÃÇr )))0 (RHÃÉn‚àí1 Œ£ÃÉn (Œ≤ÃÉr )HÃÉn‚àí1 R0 )‚àí1 (R(HÃÉn‚àí1 SÃÉn (Œ≤ÃÉr ) ‚àí Hn‚àí1 Sn (Œ≤ÃÇr ))) . (60)

The pairs cluster bootstrap for the Lagrange multiplier test then rejects at level Œ± if:

                                          P (LÃÉn ‚â• Ln |{Yc , Xc }nc=1 ) < Œ± .                                          (61)

In practice, we: (i) Draw B samples with replacement {YÃÉc , XÃÉc }nc=1 and for each sample b compute
the boostrap statistic LÃÉn,b ; (ii) Reject the null hypothesis if the proportion of bootstrap statistics
{LÃÉn,b }B
        b=1 larger than Ln is smaller than Œ±.


                                                            26
LM - Score Cluster Bootstrap

   Given a sample {Wc }nc=1 of random weights, independent of {Yc , Xc }nc=1 satisfying E[Wc ] = 0
and E[Wc2 ] = 1, and for Sn‚àó (Œ≤) as in (49) and Œ£‚àós
                                                 n (Œ≤) as in (50), define:


                     L‚àón ‚â° nJ(RHn‚àí1 Sn‚àó (Œ≤ÃÇr ))0 (RHn‚àí1 Œ£‚àós       ‚àí1 0 ‚àí1  ‚àí1 ‚àó
                                                         n (Œ≤ÃÇr )Hn R ) (RHn Sn (Œ≤ÃÇr )) .                               (62)

The score bootstrap Lagrange Multiplier procedure then rejects at level Œ± whenever:

                                               P (L‚àón ‚â• Ln |{Yc , Xc }nc=1 ) < Œ± .                                      (63)

We approximate this decision through simulation by: (i) Drawing B samples {Wc }nc=1 and employing
each sample b to obtain a score bootstrap LM statistics L‚àón,b ; (ii) Rejecting the null hypothesis if
the proportion of {L‚àón,b }B
                          b=1 larger than Ln is smaller than Œ±.


Intra-Cluster Correlation Test (Probit)

   The score bootstrap examined in Table 3 follows the discussion of Section 4.3.1. Specifically:

Step 1 Obtain a probit estimate of model (41) and let Œ∏ÃÇM = (Œ∏ÃÇ0 , Œ∏ÃÇX , Œ∏ÃÇD ) where Œ∏ÃÇX is the estimated
coefficient for Xic and Œ∏ÃÇD for Dc and Œ∏ÃÇ0 the intercept. As in Section 4.4 also let Zc ‚â° {Yic , Xic , Dc }20
                                                                                                           i=1 .


Step 2: Employing the probit estimate, construct the fitted prediction pÃÇic ‚â° Œ¶(Œ∏ÃÇ0 + Xic Œ∏ÃÇX + Dc Œ∏ÃÇD )
                             p
and let ŒΩÃÇic = (Yic ‚àí pÃÇic )/ pÃÇic (1 ‚àí pÃÇic ) denote the fitted generalized residual.

Step 3: Following the notation of Section 4.3.1, we may then define the statistics:
                    20                                               20
                    X            X                                  X   (Yic ‚àí pÃÇic )œÜ(Œ∏ÃÇ0 + Xic Œ∏ÃÇX + Dc Œ∏ÃÇD )
   m(Zc , Œ∏ÃÇM ) ‚â°         ŒΩÃÇic          ŒΩÃÇjc     ‚àáq(Zc , Œ∏ÃÇM ) ‚â°                                                Xic ,   (64)
                    i=1          j6=i                               i=1
                                                                                     pÃÇ ic (1 ‚àí pÃÇ ic )

and obtain the cluster level quantities {m(Zc , Œ∏ÃÇM )}nc=1 and {‚àáq(Zc , Œ∏ÃÇM )}nc=1 .

Step 4: As in Section 4.3.1, (i) Regress {m(Zc , Œ∏ÃÇM )}nc=1 on {‚àáq(Zc , Œ∏ÃÇM )}nc=1 and obtain residuals
{ec }nc=1 ; (ii) Perturb the cluster level residuals to obtain {Wc ec }nc=1 ; (iii) For Mn and Mn‚àó the Wald
test statistic for E[ec ] = 0 and E[ec Wc ] = 0 respectively, the bootstrap test then rejects (42) if:

                                                P (Mn ‚â• Mn‚àó |{Zc }nc=1 ) < Œ± .                                          (65)




                                                               27
                                                          Appendix

Proof of Lemma 3.1: First notice that by Markov‚Äôs inequality, E[Wi2 ] = 1 and the i.i.d. assumption
                          n                          n           n                 n
                   ‚àó   1 X      ‚àó           1    ‚àó
                                                    X
                                                           ‚àó 0
                                                                 X
                                                                       ‚àó       1 X 0
                 P (k ‚àö     Xi i k > C) ‚â§      E [(   Xi i ) (   Xi i )] =        Xi Xi e2i .                                    (66)
                        n                  nC 2                               nC 2
                             i=1                                   i=1              i=1                   i=1

Let H ‚â° E[Xi Xi0 ] and Œ£ ‚â° E[Xi Xi0 2i ]. Since n‚àí1                   0     2 a.s.                a.s.
                                                                P
                                                                    i Xi Xi ei ‚Üí          Œ£ and Hn ‚Üí H, we obtain from (66),
                                                          n
                            ‚àö    ‚àó           ‚àí1        1 X
                           k n(Œ≤ÃÇ ‚àí Œ≤ÃÇ)k ‚â§ kHn kF √ó k ‚àö     Xi ‚àói k = Op‚àó (1)                            a.s. ,                    (67)
                                                        n
                                                                          i=1

where k ¬∑ kF denotes the Frobenius norm. Next observe that for k ¬∑ ko the operator norm, we have:

  k(Hn‚àí1 Œ£‚àón (Œ≤ÃÇ ‚àó )Hn‚àí1 )‚àí1 ‚àí (Hn‚àí1 Œ£‚àón (Œ≤ÃÇ)Hn‚àí1 )‚àí1 ko

                  ‚â§ k(Hn‚àí1 Œ£‚àón (Œ≤ÃÇ)Hn‚àí1 )‚àí1 ko √ó kHn‚àí1 (Œ£‚àón (Œ≤ÃÇ) ‚àí Œ£‚àón (Œ≤ÃÇ ‚àó ))Hn‚àí1 ko √ó k(Hn‚àí1 Œ£‚àón (Œ≤ÃÇ ‚àó )Hn‚àí1 )‚àí1 ko . (68)

      (k)
Let Xi denote the k th element of the vector Xi . Arguing as in (66), it is straightforward to show that
   1 P  (k) (l) (s)
n‚àí 2 i Xi Xi Xi ‚àói = Op‚àó (1) almost surely for any indices k, l, s. Therefore, since k ¬∑ ko ‚â§ k ¬∑ kF and
E[kXi k4 ] < ‚àû, we conclude from (67) and direct calculation that we must have:
                                       n
                                    1X
    kŒ£‚àón (Œ≤ÃÇ) ‚àí Œ£‚àón (Œ≤ÃÇ ‚àó )ko ‚â§ k      Xi Xi0 {(Yi‚àó ‚àí Xi0 Œ≤ÃÇ)2 ‚àí (Yi‚àó ‚àí Xi0 Œ≤ÃÇ ‚àó )2 }kF
                                    n
                                      i=1
                                  n
                                1X
                             =k     Xi Xi0 {2‚àói (Xi0 (Œ≤ÃÇ ‚àí Œ≤ÃÇ ‚àó )) + (Xi0 (Œ≤ÃÇ ‚àí Œ≤ÃÇ ‚àó ))2 }kF = Op‚àó (n‚àí1 )             a.s. .       (69)
                                n
                                      i=1

Moreover, since E[(‚àói )k ] = E[Wik ]eki , we also obtain from the i.i.d. assumption and E[kXi k4 4i ] < ‚àû that:
                    n                                 m     m                   n
             ‚àó   1X                                XX        1 X (l) (s) ‚àó 2
            E [k    Xi Xi0 {(‚àói )2 ‚àí e2i }k2F ] =    E ‚àó [(    Xi Xi {(i ) ‚àí e2i })2 ]
                 n                                           n
                   i=1                                l=1 s=1               i=1
                                                         m X m        n
                                                      1 X           1 X (l) (s) 2
                                                  =                     (Xi Xi ) {(E[Wi4 ] ‚àí 1)e4i } = oa.s. (1) .                  (70)
                                                      n             n
                                                          l=1 s=1         i=1

Therefore, since n‚àí1              0 2 a.s.                  a.s.
                                             Œ£ and Hn‚àí1 ‚Üí H ‚àí1 , results (69) and (70) establish:
                         P
                            i Xi Xi ei ‚Üí


     kHn‚àí1 Œ£‚àón (Œ≤ÃÇ)Hn‚àí1 ‚àí H ‚àí1 Œ£H ‚àí1 kF = op‚àó (1)                        kHn‚àí1 Œ£‚àón (Œ≤ÃÇ ‚àó )Hn‚àí1 ‚àí H ‚àí1 Œ£H ‚àí1 ]kF = op‚àó (1)           (71)

almost surely. Next, for any normal matrix A, let Œæ(A) denote its smallest eigenvalue. Since Corollary
III.2.6 in Bhatia (1997) implies that |Œæ(A) ‚àí Œæ(B)| ‚â§ kA ‚àí BkF , it then follows from (71) that:

   Œæ(Hn‚àí1 Œ£‚àón (Œ≤ÃÇ)Hn‚àí1 ) = Œæ(H ‚àí1 Œ£H ‚àí1 ) + op‚àó (1)                      Œæ(Hn‚àí1 Œ£‚àón (Œ≤ÃÇ ‚àó )Hn‚àí1 ) = Œæ(H ‚àí1 Œ£H ‚àí1 ) + op‚àó (1)        (72)

almost surely. However, since for any normal matrix A, kA‚àí1 ko = 1/Œæ(A), result (72) and Assumption
3.1(i) imply kHn‚àí1 Œ£‚àón (Œ≤ÃÇ ‚àó )Hn‚àí1 ko = Op‚àó (1) and kHn‚àí1 Œ£‚àón (Œ≤ÃÇ)Hn‚àí1 ko = Op‚àó (1) almost surely. Hence,

                         k(Hn‚àí1 Œ£ÃÇ‚àón (Œ≤ÃÇ ‚àó )Hn‚àí1 )‚àí1 ‚àí (Hn‚àí1 Œ£ÃÇ‚àón (Œ≤ÃÇ)Hn‚àí1 )‚àí1 ko = Op‚àó (n‚àí1 )                  a.s.                (73)
                                                                                                      1                         1
as a result of (68) and (69). In turn, (73) yields that k(Hn‚àí1 Œ£ÃÇ‚àón (Œ≤ÃÇ ‚àó )Hn‚àí1 )‚àí 2 ‚àí (Hn‚àí1 Œ£ÃÇ‚àón (Œ≤ÃÇ)Hn‚àí1 )‚àí 2 ko =
Op‚àó (n‚àí1 ) almost surely, and the claim of the Lemma then follows by result (67).

                                                                   28
Lemma 6.1. Let {Wi }ni=1 be an i.i.d. sample independent of {Zi }ni=1 satisfying E[Wi2 ] = 1. If Assumptions
4.1, 4.3(i) and 4.4(ii) hold, then the class F = {s(z, Œ∏)s(z, Œ∏)0 w2 : Œ∏ ‚àà Œò} is Glivenko-Cantelli.


Proof: By Assumption 4.4(ii), s(z, Œ∏)w is continuous in Œ∏ ‚àà Œò, and hence so is s(z, Œ∏)s(z, Œ∏)0 w2 . Let
s(l) (z, Œ∏) be the lth component of the vector s(z, Œ∏). By the mean value theorem and Assumption 4.4(ii):

               |s(l) (z, Œ∏)| ‚â§ |s(l) (z, Œ∏) ‚àí s(l) (z, Œ∏0 )| + |s(l) (z, Œ∏0 )| ‚â§ F (z)kŒ∏ ‚àí Œ∏0 k + |s(l) (z, Œ∏0 )| .                            (74)

Hence, for D = diam(Œò) it then follows that |s(l) (z, Œ∏)s(j) (z, Œ∏)w2 | ‚â§ w2 (F (z)D + |s(l) (z, Œ∏0 )|)(F (z)D +
|s(j) (z, Œ∏0 )|), which is integrable for all 1 ‚â§ l ‚â§ j ‚â§ k by Assumption 4.1(i)-(ii) and 4.4(ii). We conclude
that F has an integrable envelope, and the Lemma follows by Example 19.8 in van der Vaart (1999).

Lemma 6.2. Under Assumptions 4.1(i), 4.3(i) and 4.4(i)-(ii), F ‚â° {ws(z, Œ∏) : Œ∏ ‚àà Œò} is Donsker.


Proof: Let k ¬∑ ko and k ¬∑ kF denote the operator and Frobenious norms. Using k ¬∑ ko ‚â§ k ¬∑ kF , Assumption
4.4(ii) and the mean value theorem, we obtain that for some Œ∏ÃÑ a convex combination of Œ∏1 and Œ∏2 :

  kws(z, Œ∏1 ) ‚àí ws(z, Œ∏2 )k

              = |w| √ó k‚àás(z, Œ∏ÃÑ)(Œ∏1 ‚àí Œ∏2 )k ‚â§ |w| √ó k‚àás(z, Œ∏ÃÑ)ko √ó kŒ∏1 ‚àí Œ∏2 k ‚â§ |w| √ó F (z) √ó kŒ∏1 ‚àí Œ∏2 k . (75)

Hence, the class F is Lipschitz in Œ∏ ‚àà Œò. For s(l) (z, Œ∏) the lth component of the vector s(z, Œ∏), let
F l ‚â° {ws(l) (z, Œ∏) : Œ∏ ‚àà Œò} and note that Theorem 2.7.11 in van der Vaart and Wellner (1996) implies:

                                            N[ ] (2kFÃÉ kL2 , F l , k ¬∑ kL2 ) ‚â§ N (, Œò, k ¬∑ k) ,                                              (76)

where FÃÉ (w, z) = |w|F (z). Let D = diam(Œò) and M 2 = E[FÃÉ 2 (Wi , Zi )] and notice that Assumptions
4.4(i)-(ii) imply M < ‚àû. Since by (75), the diameter of F l under k ¬∑ kL2 is less than or equal to M D:
                                                                                                            D
  Z     ‚àûq                                       Z   MD   q                                             Z
                                                                                                            2
                                                                                                                q
          log N[ ]   (, F l , k   ¬∑ kL2 )d ‚â§                log N[ ]   (, F l , k   ¬∑ kL2 )d = 2M               log N[ ] (2M u, F l , k ¬∑ kL2 )du
    0                                            0                                                      0
                                                          Z    D                               Z                D
                                                               2   p                                            2   p
                                                 ‚â§ 2M               log N (u, Œò, k ¬∑ k)du ‚â§ 2M                          p log(D/u)du < ‚àû (77)
                                                          0                                                 0

where the first equality follows by the change of variables u = /2M , the second inequality from (76) and
the third by N (u, Œò, k ¬∑ k) ‚â§ (diam(Œò)/u)p . Since E[FÃÉ 2 (Zi , Wi )] < ‚àû, (77) and Theorem 2.5.6 in van der
Vaart and Wellner (1996) implies F l is Donsker for 1 ‚â§ l ‚â§ k, and the claim of the Lemma follows.

Lemma 6.3. Suppose Assumptions 4.1, 4.2, 4.3 and 4.4(ii) hold. If the null hypothesis is true, it then
                     L                                                                                                          p
follows that Gn ‚Üí Xr2 . On the other hand, if the alternative hypothesis is true, then Gn ‚Üí ‚àû.


Proof: We first study the limiting behavior of Gn under the null hypothesis. For this purpose, notice
that Assumption 4.3(ii) implies that An (Œ∏0 ) = A(Œ∏0 ) + op (1), while Lemma 6.1 applied to Wi = 1 with
probability one yields Œ£n (Œ∏0 ) = Œ£(Œ∏0 ) + op (1) for Œ£(Œ∏0 ) = E[s(Zi , Œ∏0 )s(Zi , Œ∏0 )0 ]. Therefore, we conclude:

                                       An (Œ∏0 )Œ£n (Œ∏0 )An (Œ∏0 )0 = A(Œ∏0 )Œ£(Œ∏0 )A(Œ∏0 )0 + op (1) .                                              (78)

                                                                           29
It follows that An (Œ∏0 )Œ£n (Œ∏0 )An (Œ∏0 )0 is then invertible with probability tending to one by Assumption 4.1(ii).
Therefore, by Assumptions 4.2(i), 4.1(ii), 4.3(i) and the central limit theorem we conclude that:
                                                                          n
                                                               1 X                       L
                 Tn = (An (Œ∏0 )Œ£n (Œ∏0 )An (Œ∏0 )0 )‚àí1 An (Œ∏0 ) ‚àö    s(Zi , Œ∏0 ) + op (1) ‚àí‚Üí N (0, I) .                     (79)
                                                                n
                                                                          i=1

                                                                              L
Hence, by the continuous mapping theorem and (79), Gn ‚Üí Xr , which establishes the first claim of the
Lemma. The second claim of the Lemma was assumed in Assumption 4.2(ii).

Proof of Theorem 4.1: Let Œ£(Œ∏) = E[s(Zi , Œ∏)s(Zi , Œ∏)0 ]. As argued in (74), the matrix s(z, Œ∏)s(z, Œ∏)0 has
an integrable envelope. Hence, since s(z, Œ∏)s(z, Œ∏)0 is continuous in Œ∏ for all z by Assumption 4.4(ii), the
dominated convergence theorem implies Œ£(Œ∏) is continuous in Œ∏. Therefore, by Lemma 6.1 and Assumption
4.1(i), we obtain Œ£‚àón (Œ∏ÃÇ) = Œ£(Œ∏0 ) + op (1). In addition, An (Œ∏ÃÇ) = A(Œ∏0 ) + op (1) by Assumption 4.3(ii) and
                                             1 P
hence Assumption 4.1(ii) and supŒ∏‚ààŒò kn‚àí 2 i s(Zi , Œ∏)Wi k = Op (1) by Lemma 6.2 imply:
                                                     n
                                      1          1   X
          (An (Œ∏ÃÇ)Œ£‚àón (Œ∏ÃÇ)An (Œ∏ÃÇ)0 )‚àí 2 An (Œ∏ÃÇ) ‚àö          s(Zi , Œ∏ÃÇ)Wi
                                                 n
                                                     i=1
                                                                                               n
                                                                           0 ‚àí 12           1 X
                                               = (A(Œ∏0 )Œ£(Œ∏0 )A(Œ∏0 ) )              A(Œ∏0 ) ‚àö    s(Zi , Œ∏ÃÇ)Wi + op (1)
                                                                                             n
                                                                                              i=1
                                                                                  n
                                                                    0 ‚àí 12     1 X
                                               = (A(Œ∏0 )Œ£(Œ∏0 )A(Œ∏0 ) ) A(Œ∏0 ) ‚àö     s(Zi , Œ∏0 )Wi + op (1) ,              (80)
                                                                                n
                                                                                              i=1

where the second equality follows by Assumption 4.1(i) and Lemma 6.2. Let BLc be the set of Lipschitz
real valued functions whose Lipschitz constant and level are less than c. For two random variables Y , V :

                                      kY ‚àí V kBL1 ‚â° sup |E[f (Y )] ‚àí E[f (V )]| ,                                         (81)
                                                            f ‚ààBL1

metrizes weak convergence, see for example Theorem 1.12.4 in van der Vaart and Wellner (1996). Define:
                                                                                      n
                                                              1         1 X
                               TÃÑn‚àó ‚â° (A(Œ∏0 )Œ£(Œ∏0 )A(Œ∏0 )0 )‚àí 2 A(Œ∏0 ) ‚àö    s(Zi , Œ∏0 )Wi .                               (82)
                                                                         n
                                                                                     i=1

Using that all f ‚àà BL1 are bounded in level and Lipschitz constant by one, we obtain for any Œ∑ > 0:

     sup |E[f (TÃÑn‚àó ) ‚àí f (Tn‚àó )|{Zi }ni=1 ]| ‚â§ Œ∑P (|TÃÑn‚àó ‚àí Tn‚àó | ‚â§ Œ∑|{Zi }ni=1 ) + 2P (|TÃÑn‚àó ‚àí Tn‚àó | > Œ∑|{Zi }ni=1 ) .   (83)
    f ‚ààBL1

However, by the law of iterated expectations and (80), we have that P (|TÃÑn‚àó ‚àí Tn‚àó | > Œ∑|{Zi }ni=1 ) converges
to zero in mean, and hence in probability. As a result, since Œ∑ is arbitrary, result (83) in fact implies:

                                 sup |E[f (TÃÑn‚àó )|{Zi }ni=1 ] ‚àí E[f (Tn‚àó )|{Zi }ni=1 ]| = op (1) .                        (84)
                               f ‚ààBL1

     ‚àó ‚àº N (0, I). Since k ¬∑ k
Let T‚àû                        BL1 metrizes weak convergence, Assumptions 4.3(i) and 4.4(i) together with

Lemma 2.9.5 in van der Vaart and Wellner (1996) in turn let us conclude that:

                                      sup |E[f (TÃÑn‚àó )|{Zi }ni=1 ] ‚àí E[f (T‚àû
                                                                           ‚àó
                                                                             )]| = op (1) .                               (85)
                                    f ‚ààBL1


                                                                  30
    For any M > 0, define the map gM : Rr ‚Üí R to be given by gM (a) = min{a0 a, M } and notice that
                                               ‚àö
for any a, b ‚àà Rr we have |gM (a) ‚àí gM (b)| ‚â§ 2 M ka ‚àí bk and gM (a) ‚â§ M so that for M ‚â• 4 we have
gM ‚àà BLM . As a result, for any f ‚àà BL1 , f ‚ó¶ gM ‚àà BLM and M ‚àí1 f ‚ó¶ gM ‚àà BL1 , which implies:

   sup |E[f (gM (Tn‚àó ))|{Zi }ni=1 ] ‚àí E[f (gM (T‚àû
                                                ‚àó
                                                  ))]| ‚â§ M sup |E[f (Tn‚àó )|{Zi }ni=1 ] ‚àí E[f (T‚àû
                                                                                               ‚àó
                                                                                                 )]| = op (1) , (86)
 f ‚ààBL1                                                       f ‚ààBL1

where the final result follows by (84) and (85). Since G‚àón = Tn‚àó0 Tn‚àó and every f ‚àà BL1 is bounded by one,

                      sup |E[f (G‚àón ) ‚àí f (gM (Tn‚àó ))|{Zi }ni=1 ]| ‚â§ 2P (Tn‚àó0 Tn‚àó > M |{Zi }ni=1 ) .           (87)
                     f ‚ààBL1

                                                                L
By (80) and the continuous mapping theorem, Tn‚àó0 Tn‚àó ‚Üí Xr2 unconditionally and hence is asymptotically
tight. For an arbitrary Œ∑ > 0 it then follows by Markov‚Äôs inequality that for M sufficiently large:

                                                                         2
                  lim sup P (2P (Tn‚àó0 Tn‚àó > M |{Zi }ni=1 ) > Œ∑) ‚â§ lim sup P (Tn‚àó0 Tn‚àó > M ) < Œ∑ .              (88)
                    n‚Üí‚àû                                             n‚Üí‚àû Œ∑

Similarly, let G‚àó‚àû ‚àº Xr2 and notice that by selecting M appropriately large we may also obtain:

                              sup |E[f (G‚àó‚àû ) ‚àí f (gM (T‚àû
                                                        ‚àó             ‚àó0 ‚àó
                                                          ))]| ‚â§ 2P (T‚àû T‚àû > M ) < Œ∑ .                         (89)
                          f ‚ààBL1

Since Œ∑ is arbitrary, results (86), (87), (88) and (89) in turn allow us to conclude that:

                                   sup |E[f (G‚àón )|{Zi }ni=1 ] ‚àí E[f (G‚àó‚àû )]| = op (1) ,                       (90)
                                 f ‚ààBL1

which establishes the weak convergence of the distribution of G‚àón conditional on {Zi }ni=1 to that of G‚àó‚àû in
probability. Letting F be the cdf of G‚àó‚àû , we obtain by the Portmanteau theorem, G‚àó‚àû having a continuous
distribution, result (90) and Lemma 6.3 that for any c ‚àà R, Fn‚àó (c) = F (c) + op (1) and Fn (c) = F (c) + o(1).
The Theorem follows since convergence is uniform in c ‚àà R by Lemma 2.11 in van der Vaart (1999).

Lemma 6.4. Let Fn : R ‚Üí [0, 1], F : R ‚Üí [0, 1] be monotonic, supc‚ààR |Fn (c) ‚àí F (c)| = op (1) and define:

                           cŒ± ‚â° inf{c : F (c) ‚â• Œ±}              cn,Œ± ‚â° inf{c : Fn (c) ‚â• Œ±} .

If F is strictly increasing at cŒ± , it then follows that cn,Œ± = cŒ± + op (1).


Proof: Fix  > 0. Since by hypothesis F is strictly increasing at cŒ± it follows by definition of cŒ± :

                                            F (cŒ± ‚àí ) < Œ± < F (cŒ± + ) .                                      (91)

Moreover, since Fn (cŒ± + ) > Œ± implies that cn,Œ± ‚â§ cŒ± +  and Fn (cŒ± ‚àí ) < Œ± implies that cn,Œ± > cŒ± ‚àí ,

                     lim P (|cŒ± ‚àí cn,Œ± | ‚â§ ) ‚â• lim P (Fn (cŒ± ‚àí ) < Œ± < Fn (cŒ± + )) = 1 ,                    (92)
                    n‚Üí‚àû                            n‚Üí‚àû

where the final equality follows from (91) and supc |Fn (c) ‚àí F (c)| = op (1) by hypothesis.

Proof of Corollary 4.1: Let F denote the cdf of a Xr2 random variable and c1‚àíŒ± be its 1 ‚àí Œ±
quantile. As argued following (90), supc |Fn‚àó (c) ‚àí F (c)| = op (1), and hence by Lemma 6.4 it follows that

                                                           31
cÃÇ1‚àíŒ± = c1‚àíŒ± + op (1) provided 0 < Œ± < 1. The first claim of the Corollary then follows by Lemma 6.3 and
the continuous mapping theorem.

   For the second claim of the Corollary, observe that the bootstrap statistic Sn‚àó (Œ∏ÃÇ) remains properly
centered. In fact, (90) was established without appealing to Assumption 4.2(i). Therefore, cÃÇ1‚àíŒ± = c1‚àíŒ± +
                                                                                                p
op (1) under the alternative hypothesis as well. However, under the alternative hypothesis Gn ‚Üí ‚àû by
Lemma 6.3 and therefore the second claim of the Corollary follows.

Proof of Corollary 4.2: Given the definitions, this is a special case of Corollary 4.1.




References
Andrews, D. W. K. (2002): ‚ÄúHigher-Order Improvements of a Computationally Attractive k-Step Boot-
  strap for Extremum Estimators,‚Äù Econometrica, 70(1), 119‚Äì162.

Barbe, P., and P. Bertail (1995): The Weighted Bootstrap. Springer- Verlag, New York.

Bertrand, M., E. Duflo, and S. Mullainathan (2004): ‚ÄúHow Much Should We Trust Differences in
  Differences Estimates?,‚Äù Quarterly Journal of Economics, 119(1), 249‚Äì275.

Bhatia, R. (1997): Matrix Analysis. Springer, New York.

Cameron, A. C., J. B. Gelbach, and D. L. Miller (2008): ‚ÄúBootstrap-Based Improvements for
  Inference with Clustered Errors,‚Äù Review of Economics and Statistics, 90, 414‚Äì427.

Cameron, A. C., and P. K. Trivedi (2005): Microeconometrics - Methods and Applications. Cambridge
  University Press, New York.

Card, D., and D. R. Hyslop (2005): ‚ÄúEstimating the Effects of a Time-Limited Earnings Subsidy for
  Welfare-Leavers,‚Äù Econometrica, 73(6), 1723‚Äì1770.

Cavaliere, G., and A. M. R. Taylor (2008): ‚ÄúBootstrap Unit Root Tests for Time Series with
  Nonstationary Volatility,‚Äù Econometric Theory, 24, 43‚Äì71.

Chatterjee, S., and A. Bose (2005): ‚ÄúGeneralized Bootstrap for Estimating Equations,‚Äù Annals of
  Statistics, 33, 414‚Äì436.

Chesher, A. (1984): ‚ÄúTesting for Neglected Heterogeneity,‚Äù Econometrica, 52(4), 865‚Äì872.

         (1995): ‚ÄúA Mirror Image Invariance for M-estimators,‚Äù Econometrica, 63(1), 207‚Äì211.

Davidson, R., and E. Flachaire (2008): ‚ÄúThe Wild Bootstrap, Tamed at Last,‚Äù Journal of Economet-
  rics, 146, 162‚Äì169.

                                                   32
Davidson, R., and J. G. MacKinnon (1999a): ‚ÄúBootstrap Testing in Nonlinear Models,‚Äù International
  Economic Review, 40, 487‚Äì508.

          (1999b): ‚ÄúThe Size Distortion of Bootstrap Tests,‚Äù Econometric Theory, 15, 361‚Äì376.

Davidson, R., and J. G. MacKinnon (2004): Econometric Theory and Methods. Oxford University
  Press, New York, Oxford.

Davidson, R., and J. G. MacKinnon (2010): ‚ÄúWild Bootstrap Tests for IV Regression,‚Äù Journal of
  Business and Economic Statistics, 28, 128‚Äì144.

Donald, S. G., and K. Lang (2007): ‚ÄúInference with Differences-in-Differences and Other Panel Data,‚Äù
  Review of Economics and Statistics, 89, 221233.

Efron, B. (1979): ‚ÄúBootstrap Methods: Another Look at the Jacknife,‚Äù The Annals of Statistics, 7(1),
  1‚Äì26.

Freedman, D. A. (1981): ‚ÄúBootstrapping Regression Models,‚Äù The Annals of Statistics, 9(6), 1218‚Äì1228.

Hall, P. (1992): The Bootstrap and Edgeworth Expansion. Springer-Verlag, New York.

Hardle, W., and E. Mammen (1993): ‚ÄúComparing Nonparametric Versus Parametric Regression Fits,‚Äù
  The Annals of Statistics, 21(4), 1926‚Äì1947.

Horowitz, J. L. (1994): ‚ÄúBootstrap-based Critical Values for the Information Matrix Test,‚Äù Journal of
  Econometrics, 61, 395‚Äì411.

          (1997): ‚ÄúBootstrap Methods in Econometrics: Theory and Numerical Performance,‚Äù in Advances
  in Economics and Econometrics: Theory and Applications, Seventh World Congress, ed. by D. M. Kreps,
  and K. F. Wallis, vol. 3. Cambridge University Press.

          (2001): ‚ÄúThe Bootstrap,‚Äù in Handbook of Econometrics, ed. by J. J. Heckman, and E. Leamer,
  vol. 5, chap. 52. Elsevier.

Hu, F., and J. D. Kalbfleisch (2000): ‚ÄúThe Estimating Function Bootstrap,‚Äù The Canadian Journal
  of Statistics, 28(3), 449‚Äì481.

Hu, F., and J. V. Zidek (1995): ‚ÄúA Bootstrap Based on the Estimating Equations of the Linear Model,‚Äù
  Biometrika, 82(2), 263‚Äì275.

Kaido, H., and A. Santos (2011): ‚ÄúAsymptotically Efficient Estimation of Models Defined by Convex
  Moment Inequalities,‚Äù Working paper, University of California - San Diego.




                                                    33
Kline, P., and A. Santos (2011): ‚ÄúHigher Order Properties of the Wild Bootstrap Under Misspecifica-
  tion,‚Äù Working paper, University of California - Berkeley.

Liu, R. Y. (1988): ‚ÄúBootstrap Procedures under some Non-I.I.D. Models,‚Äù The Annals of Statistics, 16(4),
  1696‚Äì1708.

Ma, S., and M. R. Kosorok (2005): ‚ÄúRobust Semiparametric M-estimation and the Weighted Boot-
  strap,‚Äù Journal of Multivariate Analysis, 96, 190‚Äì217.

Mammen, E. (1993): ‚ÄúBootstrap and Wild Bootstrap for High Dimensional Linear Models,‚Äù The Annals
  of Statistics, 21(1), 255‚Äì285.

McCall, B. P. (1994): ‚ÄúSpecification Diagnostics for Duration Models: A Martingale Approach,‚Äù Journal
  of Econometrics, 60, 293‚Äì312.

Newey, W. K. (1985a): ‚ÄúGeneralized Method of Moments Specifcation Testing,‚Äù Journal of Economet-
  rics, 29, 229‚Äì256.

         (1985b): ‚ÄúMaximum Likelihood Specification Testing and Conditional Moment Tests,‚Äù Econo-
  metrica, 53(5), 1047‚Äì1070.

Newey, W. K., and D. L. McFadden (1994): ‚ÄúLarge Sample Estimation and Hypothesis Testing,‚Äù in
  Handbook of Econometrics, ed. by R. F. Engle, and D. L. McFadden, vol. IV, pp. 2113‚Äì2245. Elsevier
  Science B.V.

Tauchen, G. (1985): ‚ÄúDiagnostic Testing and Evaluation of Maximum Likelihood Models,‚Äù Journal of
  Econometrics, 30, 415‚Äì443.

van der Vaart, A. (1999): Asymptotic Statistics. Cambridge University Press, New York.

van der Vaart, A. W., and J. A. Wellner (1996): Weak Convergence and Empirical Processes: with
  Applications to Statistics. Springer, New York.

White, H. (1982): ‚ÄúMaximum Likelihood Estimation of Misspecified Models,‚Äù Econometrica, 50, 1‚Äì25.

        (1994): Estimation, Inference, and Specification Analysis. Cambridge University Press, New York.

Wooldridge, J. (2003): ‚ÄúJacknife, Bootstrap, and other Resampling Methods in Regression Analysis,‚Äù
  American Economic Review, 93(2), 133‚Äì138.

Wooldridge, J. M. (2002): Econometric Analysis of Cross Section and Panel Data. The MIT Press,
  Cambridge.




                                                    34
Wu, C. F. J. (1986): ‚ÄúJacknife, Bootstrap, and other Resampling Methods in Regression Analysis,‚Äù
  Annals of Statistics, 14(4), 1261‚Äì1295.

You, J., and G. Chen (2006): ‚ÄúWild Bootstrap Estimation in Partially Linear Models with Heteroscedas-
  ticity,‚Äù Statistics and Probability Letters, 76, 340‚Äì348.




                                                      35
