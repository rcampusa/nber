                                 NBER WORKING PAPER SERIES




UNDER PRESSURE: JOB SECURITY, RESOURCE ALLOCATION, AND PRODUCTIVITY
                       IN SCHOOLS UNDER NCLB

                                          Randall Reback
                                           Jonah Rockoff
                                         Heather L. Schwartz

                                         Working Paper 16745
                                 http://www.nber.org/papers/w16745


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                     January 2011


We thank Steve Rivkin and David Figlio for their detailed comments, as well as participants at the
American Economics Association meetings, the CALDER/Urban Institute NCLB Research Conference,
the Association for Public Policy and Management conference, the International Workshop on Applied
Economics of Education, and the American Education Finance Association conference for many thoughtful
suggestions. This research project was made possible by funding from the Institute for Education
Sciences and the Spencer Foundation, as well as seed grants from the Columbia University Institute
for Social and Economic Research and Policy and Barnard College, and support from the Paul Milstein
Center for Real Estate at Columbia Business School. The authors are solely responsible for any opinions
or errors in the paper. Molly Alter, Daisy Chu, Ben Lockwood, Julia Zhou, Sean Tom, and especially
Elizabeth Davidson provided outstanding research assistance. The authors thank the U.S. Department
of Education for providing access to the restricted-use versions of the Early Childhood Longitudinal
Survey and Schools and Staffing Survey. To comply with restricted-use data reporting requirements,
all sample sizes in this paper have been rounded to the nearest ten. The views expressed herein are
those of the authors and do not necessarily reflect the views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2011 by Randall Reback, Jonah Rockoff, and Heather L. Schwartz. All rights reserved. Short sections
of text, not to exceed two paragraphs, may be quoted without explicit permission provided that full
credit, including © notice, is given to the source.
Under Pressure: Job Security, Resource Allocation, and Productivity in Schools Under NCLB
Randall Reback, Jonah Rockoff, and Heather L. Schwartz
NBER Working Paper No. 16745
January 2011
JEL No. H4,H7,I28

                                              ABSTRACT

The most sweeping federal education law in decades, the No Child Left Behind (NCLB) Act, requires
states to administer standardized exams and to punish schools that do not make Adequate Yearly Progress
(AYP) for the fraction of students passing these exams. While the literature on school accountability
is well-established, there exists no nationwide study of the strong short-term incentives created by
NCLB for schools on the margin of failing AYP. We assemble the first comprehensive, national, school-level
dataset concerning detailed performance measures used to calculate AYP, and demonstrate that idiosyncrasies
in state policies create numerous cases where schools near the margin for satisfying their own state’s
AYP requirements would have almost certainly failed or almost certainly made AYP if they were located
in other states. Using this variation as a means of identification, we examine the impact of NCLB
on the behavior of school personnel and students’ academic achievement in nationally representative
samples. We find that accountability pressure from NCLB lowers teachers’ perceptions of job security
and causes untenured teachers in high-stakes grades to work longer hours than their peers. We also
find that NCLB pressure has either neutral or positive effects on students’ enjoyment of learning and
their achievement gains on low-stakes exams in reading, math, and science.


Randall Reback                                       Heather L. Schwartz
DepDUWPHQt of Economics                              RAND Corporation
Barnard College, Columbia University                 1776 Main Street
3009 Broadway                                        Santa Monica, CA 90401-3208
New York, NY 10027-6598                              hschwart@rand.org
rreback@barnard.edu

Jonah Rockoff
Columbia University
Graduate School of Business
3022 Broadway #603
New York, NY 10027-6903
and NBER
jonah.rockoff@columbia.edu
         On January 8, 2002, President George W. Bush signed into law the No Child Left Behind (NCLB)

Act, which many consider the most significant federal intervention into education in the United States

since the authorization of the Elementary and Secondary Education Act in 1965. Under NCLB, states

were required to adopt accountability systems based on student proficiency on statewide math and reading

exams, and to measure proficiency within student subgroups (e.g., students from low income families,

students with limited English proficiency). States must impose escalating sanctions on schools that fail to

satisfy Adequate Yearly Progress (AYP) requirements for exam proficiency, including allowing students

to transfer to other public schools, forcing schools to pay for students from low-income families to enroll

in after-school tutoring programs, and, ultimately, closing or restructuring persistently failing schools.1

         While school accountability has received much attention from economists, there is no nationwide

study of the impact of NCLB on school personnel and students. We aim to fill this gap by investigating

the links between the accountability incentives under NCLB and a wide array of outcomes measured for

nationally representative samples. To this end, we assemble a new dataset on the determination of AYP

status for schools nationwide during the introduction of NCLB, and use these data to measure the degree

to which schools faced moderate or severe risks of failing. We exploit the fact that each state selects its

own standardized tests and rules for satisfying AYP, generating numerous cases where a school near the

margin for satisfying its own state’s AYP requirements would have almost certainly failed or almost
certainly passed AYP if it were located in another state.2 This allows us to implement a difference-in-

differences style approach, comparing differences in outcomes for schools on and away from the AYP

margin within the same state to the difference in outcomes between similar schools in other states, neither

of which is on the AYP margin.

         We measure outcomes in nationally representative samples of teachers and students using the

non-public versions of the Schools and Staffing Survey (SASS) and the Early Childhood Longitudinal
Survey (ECLS). We find that accountability pressure from NCLB reduces teachers’ perceptions of job

security and increases work hours, particularly among untenured teachers. We also find evidence of


1
  States must also publish school report cards, and schools' AYP status may affect school prestige and local property
values (see Figlio and Lucas, 2004).
2
  As we demonstrate below, states vary widely in the percent of schools that fail to make AYP, and much of this
variation is due to policy parameters (e.g., rules regarding the minimum enrollment for subgroups to count towards
AYP, the grade levels that count towards AYP, and confidence or “safe harbor” adjustments to proficiency rates for
schools that would otherwise have not made AYP) rather than academic achievement.


                                                                                                                    1
teachers shifting time away from direct instruction and, for schools with very low chances of meeting

NCLB requirements, away from instruction in science and social studies.

        Despite concerns regarding the impact of NCLB on students, we find that short-term NCLB

pressure has either positive or neutral effects on student achievement in math, reading, and science on

low-stakes examinations. Students enrolled in schools with a moderate risk of failing to make AYP score

0.07 standard deviations higher in reading than comparable students in similar schools that were well

above the margin for making AYP. Estimated effects for math and science are also positive (0.04 and

0.05 standard deviations, respectively) but not statistically significant in our preferred empirical

specification. We also find no evidence of differential effects of NCLB pressure on students in

particularly crucial subgroups or students with scores close to the passing threshold on their states'

examinations. In addition, achievement gains from short-term NCLB pressure do not come at the
expense of students' reported enjoyment of learning or reported anxiety over testing.

        The paper proceeds as follows. In Section 2, we present a framework for how schools might be

expected to respond to incentives under an accountability system like NCLB and discuss prior related
empirical work. Section 3 describes the NCLB data we have collected as well as the SASS and ECLS

survey data. We present our methodology and results for predictions of AYP failure probabilities in

Section 4, and our estimated effects of NCLB on teachers and students in Section 5. Section 6 concludes.



2. Conceptual Framework and Related Literature

        We first present a framework for allocation of school resources under a system of accountability

such as No Child Left Behind. Schools have various resources they can use to improve student skills

(e.g., school staff, curriculum, facilities, parental involvement, etc.), and all resources have associated

costs. Subject to a budget constraint, schools choose an allocation based on preferences about the relative
importance of helping students improve different types of skills and the relative importance of helping

different types of students make improvements. There are also competing demands that constrain the

amount and allocation of school resources; school staff members care about their own leisure time and
local taxpayers care about their consumption of other goods and services.

        More formally, we classify resources into four types: the first (denoted u) helps to improve all

skills for all students (e.g., the overall effort level of teachers), the second (denoted as) is skill-specific and



                                                                                                                 2
serves all students (e.g., math lessons that equally help all students learn math), the third type (denoted bi)

is student-specific and serves all skills (e.g., providing individual students with lessons to improve study-

skills), and the fourth (denoted cis) is skill-specific and student-specific (e.g., individual math tutoring).

Suppose there are two categories of student skills that schools aim to improve, one which is measured on

standardized tests (s=m), and another which is not (s=z). Schools place weights (denoted  is) on skill
acquisition for each student, depending on the preferences of school staff and the community. Finally, let

l denote resources devoted to consumption of goods and services that are valued by the community and

school staff but are unrelated to skill acquisition (e.g., teacher leisure). Schools with N students and total

resources equal to K will choose an allocation of resources to maximize:
                                                                      N
(1)                                           U l                      is    f is u , as , bi , cis 
                                                            s  m , z i 1

                                                                                                                                              
                          subject to               is    1 and u                a    b   c
                                                                                             s                       i                    is
                                                                                                                                                 K  l
                                          s m ,z                                 s  m, z            i                      s  m, z          
In the equation above, the function U determines the value received by community members and school

staff for non-skill resources (l) , and the function fis maps other resources into the performance of student i
in skill s. Schools choose an optimal allocation of resources given this objective function and budget

constraint, which we will call “business as usual.”

        A system of accountability and ratings such as NCLB introduces benefits or costs that depend on

the fraction of students who pass standardized tests. Suppose that an additional resource (denoted di) is

available which increases the probability that student i passes the standardized tests but does not improve

skill acquisition. The school now chooses an allocation of resources to maximize:
                                               N
                                                                                                 1       N
                                                                                                                                                                 
(2)                       U l              is fis u, as , bi , cis  V                            g u , a     i         m      , bi , cim ,d i ,  i 
                                     s m , z i 1                                               N       i 1                                                   
                                                                                                                                                     
                          subject to               is    1 and u               a    b   c
                                                                                             s                   i                       is     d i   K  l
                                          s m ,z                                 s m ,z         i                         s m ,z                  
In this equation, i is idiosyncratic noise due to imperfect test measurement (with mean zero and known

variance), the function gi maps resources and test measurement error into whether student i passes the




                                                                                                                                                                     3
standardized tests, and the function V maps the school-wide pass rate into benefits or costs.3 Note that

resources that do not improve measured skills (az and ciz) do not enter in the function gi.
         The provisions of NCLB essentially impose costs on schools with pass rates below a certain

threshold (AYP). Formally, let V take the following form:

                                       1     N
                                                                                   
                               V if
                               
                                        
                                        N
                                               g u ,a
                                              i 1
                                                     i    m   ,bi ,cim , d i , i   P *
                                                                                   
(3)                         V                                                               where V  V
                               V if     1    N
                                                                                    
                                                 g is u , am ,bi ,cim , di , i   P *
                                        N                                         
                                             i 1


In other words, the school is worse off if the pass rate falls below some threshold P*, but all other

variation in the pass rate above or below that threshold does not have immediate consequences related to

NCLB. The “all or nothing” structure has important implications for the accountability pressure different
schools face. Because input allocations and the variance of test measurement error are known, schools

will form expectations about their probabilities of making AYP. If a school has a very high probability of

making AYP under its optimal pre-NCLB resource allocation, it will encounter very little pressure to

improve and, consequently, resource allocation under NCLB should resemble “business as usual.” In

contrast, if a school expects to be close to the margin of making AYP under its optimal pre-NCLB

resource allocation, the school and its community will face considerable pressure to improve student pass
rates. This is a key identifying assumption in our methodology.4

         Most optimistically, schools will respond to accountability pressure by improving their technical

efficiency or economic efficiency (or both) without any increased costs or negative effects on students'

skill acquisition. For example, a school might adopt a more cost effective reading curriculum which

improves gi for at least some students without decreasing fis for any students or skills. Similarly, a school

might improve economic efficiency by finding a more cost effective mix of inputs, perhaps by hiring a

different mix of instructional staff to provide new combinations of u, as, bi,, and cis..



3
  Alternatively, the function V could enter into the school’s budget constraint, rather than the utility function, but the
qualitative results from this alternative framework would be the same. Note that, for simplicity, V is based on the
overall student proficiency rate on a single test, whereas NCLB holds schools accountable for proficiency rates for
the overall student population and additional subgroups of students on both math and reading tests.
4
  Schools with a very low probability of making AYP in the current year will also face pressure to improve over a
longer period of time. Our empirical work focuses on comparisons of schools near the margin with schools with
high probabilities of making AYP, but we also test for effects of schools having low probabilities of making AYP.


                                                                                                                        4
        Without improvements to efficiency, changes in resource allocation in response to NCLB will

involve tradeoffs. School personnel and community members may reduce resources devoted to

consumption (l) and direct them towards improving student skills. Accountability pressure may also

induce schools to spend fewer resources promoting non-tested skills (az and ciz), more resources

promoting tested skills (am and cim), and more resources targeted to particular students (bi, cim) for whom
extra resources will most improve their probability of passing the standardized exams. Finally, schools

may allocate resources to activities that improve pass rates (di) but not skill acquisition.

        Most empirical research on school accountability focuses on the impacts of state and local

systems, many of which preceded No Child Left Behind (e.g., Ladd & Zelli, 2002; Hanushek &

Raymond, 2005; Chakrabarti, 2007; Rouse et al., 2007; Chiang, 2009; Rockoff & Turner, 2010). Several

studies find evidence that accountability pressure causes schools to reallocate resources in ways that raise

average student achievement. However, schools have also been found to shift resources towards students

and subjects that are most critical to the accountability rating (e.g., Booher-Jennings, 2005; Reback, 2008;

Neal & Whitmore Schanzenbach, 2010), teach to the test (Jacob, 2005; Figlio & Rouse, 2006), remove

low performing students from the testing pool (Figlio & Getzler, 2006; Figlio, 2006, Cullen & Reback,

2006), or cheat (Jacob & Levitt, 2003).

        Knowledge about the impacts of NCLB is still nascent. Among the few studies that apply
rigorous methods, most examine only student performance on high stakes tests in one state or one city

(Springer, 2008; Krieg, 2008; Ladd & Lauen, 2010; and Neal & Whitmore Schanzenbach, 2010). These

studies have found that students enrolled in schools failing AYP tend to make greater than expected gains
on high-stakes tests, though there is conflicting evidence concerning heterogeneous effects on students at

different parts of the performance spectrum. Only two prior studies examine the impact of NCLB

incentives in multiple states. Ballou and Springer (2008) examine variation in the grade levels tested for
NCLB across seven states and find that students generally perform better on low-stakes exams during

years when they took high-stakes tests, particularly for students near the margin of passing their high-

stakes exams. Dee and Jacob (2009) find that students in states with no prior accountability policies

experienced greater increases on the National Assessment of Educational Progress in some grades and

subjects after NCLB was introduced.




                                                                                                              5
3. Data and Descriptive Analysis

3.1 Data Description

         Our analysis focuses on the initial years of NCLB implementation following its passage in

January 2002. To measure NCLB pressure faced by schools during these years (and which subgroups and

subjects caused that pressure), our analysis requires a comprehensive, national database of schools’

NCLB-related outcomes. Because NCLB did not require states to report these data to the federal

government, we painstakingly collected them from individual school report cards or state-level data files

wherever available, and supplemented remaining states’ data with two existing but incomplete public

datasets.5 We present the categories of data collected and their sources in Appendix 1.

         We examine teacher-level outcomes from the 2003-2004 wave of the SASS and student-level

outcomes from the spring 2004 wave of the ECLS, when most students in the ECLS were in the fifth

grade. These surveys are sponsored and distributed by the National Center for Education Statistics. We

use the non-public-use versions of these data in order to link teachers and students to our constructed

measures of the degree to which NCLB placed short-term pressure on their school to make AYP.

         The SASS surveyed teachers in all 50 states and allows researchers to construct nationally-

representative samples with the use of sampling weights.6 For consistency with our examination of

student outcomes in the ECLS, we limit the sample of teachers to those working in regular public schools
that served at least five fifth graders as of 2001-2002. The first panel of Table 1 provides summary

statistics on the outcome variables we create from SASS survey questions.7

         The ECLS followed students for nine years, collecting data in both the fall and the spring of the

school years 1998-1999 and 1999-2000 (kindergarten and first grade), and in the spring of the school

years 2001-2002, 2003-2004, and 2006-2007 (third grade, fifth grade, and eighth grade). The ECLS has

5
  These two sources of NCLB-related data are the Council of Chief State of School Officers’ School Data Direct
(http://www.schooldatadirect.org/) and the American Institutes for Research National AYP and Identification
Database (http://www.air.org/publications/naypi.data.download.aspx). Whereas the first source includes AYP data
in most states for the years 2002-2003 through the current year, the latter dataset includes states’ yes/no
determinations regarding 2003-2004 and 2004-2005 subgroups and schools’ passage of AYP participation and
proficiency targets. In addition to missing data for some states, these sources also contain discrepancies with states’
school report cards. We prioritized school report card data where available since they are the final interface between
schools and the public and should reflect final adjustments such as school appeals to states' determinations of AYP.
6
  The SASS surveyed administrators but we did not feel these questions were relevant to NCLB pressure. Although
the ECLS surveyed teachers, the SASS offers a much larger sample size, surveys teachers across all grades levels,
and asks them pertinent survey questions about their time use, attitudes toward their job, and future career plans.
7
  We recoded teachers' reported work-related hours and instructional hours as missing if their reported 60 or more
instructional hours, a suspiciously high level of reported instructional time given the typical five day school week.


                                                                                                                     6
the widest coverage and array of student-level outcomes of any nationally representative longitudinal

dataset covering years before and after the passage of NCLB. Indeed, the timing of the ECLS survey is

serendipitous, as this cohort was tested just prior to the first year of NCLB and again two years later. The

ECLS sample was designed to be representative of kindergartners, their classrooms, and their schools in

the school year 1998-1999, (and representative of first grade students in 1999-2000). It includes students

from 40 relatively populous states.8

         The student-level data collection procedures in the ECLS result in samples of students that are not

necessarily representative of the student populations at their schools, particularly due to tracking
procedures for students making non-structural school transfers.9 In our analysis of ECLS data, we use

sampling weights to make our estimates nationally representative. However, we test the robustness of our

results to dropping child-level sampling weights and removing students who made non-structural

enrollment changes. This alternative specification does not change our main conclusions and has small

effects on the precision of our estimates, increasing precision in some cases and reducing it in others.

         In the ECLS data, we are particularly interested in measures of student performance on a series of

standardized tests in math, reading, and science. Unlike the tests that states administer under NCLB, the

ECLS tests were low-stakes, un-timed, and adaptive (i.e., subsequent questions are selected based on a

student’s performance on preceding questions), thus preventing floor or ceiling effects and increasing test
reliability. Students and schools became involved in the ECLS survey well before NCLB, and likely

were familiar with the ECLS surveyors and understood that these tests were not high-stakes. This reduces

concerns about teaching to the ECLS test or strategic responses to survey questions. Also, by examining

tests unrelated to NCLB, we avoid problems of mean reversion due to measurement error or other shocks

to high-stakes test scores that do not reflect real achievement but would nevertheless affect the

accountability pressure faced by the school.



8
  It used a multistage probability sample design, first selecting broad geographic areas (e.g., a county), then schools
within each area, and finally students within schools. On average, 23 kindergarteners were sampled in each school.
9
  The ECLS includes students who were retained within the same grade or skipped a grade level, but has some
attrition. In the school year 1999-2000, a randomly-selected 50 percent sub-sample of students who transferred
from their original school was surveyed, and another random sample of first graders in the same schools where
transfer students were followed was added. However, this “freshening” of the sample was not repeated in the third,
fifth, and eighth grades, and the ECLS simply sampled 50 percent of students who transferred schools for non-
structural reasons (i.e., students who switched schools for reasons other than moving from a K-4th grade school to a
5th-8th grade school in the same district).


                                                                                                                      7
         The second panel of Table 1 provides descriptive statistics for our ECLS outcome measures.

Since most surveyed students in the ECLS spring 2004 wave were fifth graders, we limit the sample of

students to those attending regular public schools in the spring of the school year 2003-2004 that also

served at least five fifth grade students as of 2001-2002. We standardize students’ scores within subject

and year so that the national mean score equals zero and the national standard deviation equals one.10 In
additional to standardized exams, we examine students’ reported enjoyment of math and reading, as well

as reported anxiety over standardized tests.11

         Table 2 provides descriptive statistics on control variables used in our regression analyses. We

show statistics separately for our samples of public school teachers from the SASS and public school

students from the ECLS. Along with variables from the surveys themselves, we also use school

characteristics from the Common Core of Data (CCD), compiled by the National Center for Education

Statistics (NCES), and aggregated student test performance variables from the National Longitudinal

School-Level State Assessment Score Database (compiled by American Institutes for Research).12 We

standardize test performance variables within states to have a mean of zero and standard deviation one.
         In addition to our analysis of the SASS and ECLS data, we examine a set of survey responses

from the Implementing Standards-Based Accountability (ISBA) study, conducted by the RAND

Corporation. As part of ISBA, principals and math teachers in three states (Pennsylvania, Georgia, and

California) were surveyed regarding their views on NCLB-related policies and the implementation of

these policies in their schools. While these data are not public, researchers at RAND generously provided

us with cross-tabulations of survey responses on a number of items, broken down by our measure of

NCLB pressure. We discuss our measure of pressure and present the ISBA results in Section 4.



10
   The ECLS data report t-scores of students’ IRT-based “theta scores,” which are estimates of students’ skill levels.
These t-scores are already constructed so that the national (cross-sectional) mean equals 50 and the national standard
deviation equals 10, so we simply subtract 50 from these scores and then divide by 10 to convert them to Z-scores.
Our sample means and standard deviations for these variables are not exactly equal to 0 and 1, respectively, because
we must exclude a small fraction of states and schools with missing data, and because we use longitudinal student
sample weights rather than cross-sectional sample weights.
11
   Answers to these specific questions, rather than an index based on a larger set of items, were obtained via special
application to the National Center for Education Statistics. Due to copyright restrictions we cannot report the exact
wording of these questions. For interest in and enjoyment of math and reading, we create dependent variables by
summing the subject-specific numeric values for four relevant questions. We use only one question regarding
feelings of test anxiety and create an indicator for reporting that such feelings were “mostly” or “very” true.
12
   Tennessee did not report school level demographic information to the federal government after 1998-1999.
Rather than drop Tennessee from our analysis, we use data from 1998-1999 in lieu of data from 2001-2002.


                                                                                                                    8
3.2 Descriptive Analysis of AYP Outcomes under NCLB

         For a school to make AYP, each of its numerically significant student subgroups must meet a test

proficiency rate threshold in both math and reading in addition to a test participation cutoff of 95 percent.

Secondary schools must also meet thresholds for graduation rates, and primary schools must also perform

sufficiently well on a state-selected “additional indicator,” typically the attendance rate. Beyond these

general parameters, states have a great deal of flexibility in setting a number of other rules and

regulations. Specifically, states must:

        select standardized tests in math, reading, and (starting in 2007-2008) science;

        select which grade levels to test (until 2005-2006) 13;

        establish proficiency rate thresholds, i.e., the percent of students that must score proficient or

         higher. Thresholds apply to the whole school as well as individual subgroups;

        determine whether to calculate proficiency rates using all students across tested grade levels

         within each school or within tested grade levels;14

        determine whether to calculate subgroup proficiency rates using multiple years of testing;
        define continuous enrollment, where only continuously enrolled students count towards

         calculation of subgroup size as well as test participation and proficiency rates;

        select the minimum number of students that must be enrolled in tested grade levels for a student

         subgroup to be numerically significant and thus count towards a school’s AYP determination;

        determine the generosity of confidence intervals applied to student subgroups’ raw proficiency

         rates, which effectively lower proficiency thresholds needed to make AYP;

        determine the nature of safe harbor provisions that allow schools to make AYP in spite of a

         subgroup not meeting the required proficiency rate that year; and,

        decide upon the appeals process for schools to appeal their AYP status from the state.




13
   From 2003 to 2005, states were allowed to choose which tested grade levels counted towards AYP determination,
so long as at least one level in each of three grade spans (3-5, 6-9, and 10-12) were included. Only beginning in
2005-2006 did states have to assess the math and reading proficiency of all third through eighth graders and at least
one level for grades 10 to 12.
14
   While most states determine subgroup size using students across all tested grades within a school, eight states
(Arizona, Colorado, Maine, New York, New Jersey, Rhode Island, Tennessee, and Washington) further disaggregate
subgroup size and subgroup results to the grade or grade span level.


                                                                                                                   9
        Even this long list does not fully capture all the minutiae of NCLB rulemaking. For example,

while most states consider the performance of five ethnic subgroups (Asian/Pacific Islander, black,

Hispanic, Native American, and white) in their AYP determinations, California and Alaska add additional

subgroups (Filipino and Alaskan Native, respectively) while Asian/Pacific Islander is not an AYP

subgroup in Texas.

        These seemingly esoteric decisions have real implications for whether schools fail to meet the

targets set for them under NCLB, as can be seen in the remarkable amount of variation in the fraction of

schools in each state that made AYP. In 2003, most states' failure rates fell between 20 and 40 percent,

but the range extended from roughly 1 percent in Iowa to 82 percent in Florida (see Figure 1).

        Importantly for our study, variation in the fraction of schools making AYP was mostly a function

of states’ rulemaking choices and bears little relation to measures of statewide academic achievement.
For example, the fraction of schools failing to make AYP by state is not significantly correlated with the

fraction of students in the state deemed proficient on the state’s own exams, because required proficiency

rates were often set at the 20th percentile of baseline (spring 2002) school performance.15 More
importantly, as shown in Figure 2, there is little relationship between the fraction of schools failing to

make AYP in a state and the state’s average student achievement as measured on the National Assessment

of Educational Progress (NAEP), a federal exam that has been administered to nationally representative

samples of students in grades 4 and 8 for several decades.16 States with the highest NAEP proficiency

rates have slightly lower AYP failure rates than other states, but this relationship is not statistically

significant and NAEP proficiency rates explain very little of the cross-state variation in AYP failure rates.

        We have been unable to find any single aspect of NCLB design that can explain the wide

variation in failure rates. However, by testing a number of factors we have come to the conclusion that

interaction of four features significantly influences the fraction of schools failing AYP: (1) state rules for
the numerical significance of student subgroups; (2) within-school heterogeneity, which influences how

many student subgroups are numerically significant; (3) the generosity of the state’s confidence intervals;

and (4) the generosity of the state’s safe harbor provisions. The complex manner in which these policy

15
   However, there was even wide variation in how states calculated the 20th percentile. For example, some states
based the 20th percentile measure on baseline school-wide pass rates and some used grade-specific and/or subject-
specific baseline pass rates.
16
   Note that we plot AYP failure rates for schools serving fifth grade students, which is the type of schools we
analyze in SASS and ECLS. In Figure 1, AYP failure rates are shown for all schools that receive AYP designations.


                                                                                                              10
details interact increases our confidence that the wide differences in the apparent leniency of NCLB

requirements across states can help identify its impact on schools and students.



4. Predicting the Probability of Failing AYP

         In the first stage of our analysis, we use our newly assembled data set to determine which student

subgroups and, by extension, which schools were on the margin of failing to make AYP in the first two

years during which NCLB was in effect. We begin by estimating state- and subject-specific probit

regressions to generate predictions of the likelihood that each numerically-significant student subgroup

would pass AYP proficiency targets in the spring of both 2003 and 2004. To do so, we use school

demographic characteristics (listed in Table 2) and 2001-2002 subgroup-level/school-level test

performance variables from the school year 2001-2002—after the passage of NCLB but prior to the first
AYP determinations.17 We conduct regressions separately by state, so that coefficients capture the

nuances of how states' NCLB rules affect schools' chances of making AYP. Regressions are run at the

student subgroup level and are restricted to those that were numerically significant in either 2003 or

2004.18 Because of the variation in NCLB rules across states, our variables differ somewhat across some

states. To be as consistent as possible, we applied a set of rules (described in Appendix 2) for how to

specify our regressions conditional on the available data.

         For each subject s, we estimate state-specific regressions of the following form:

                          1 if  q  X jks02 1  N jks04  2  XN jks  3  W j 02  4  M jks0304  5   jks  0
       (4) AYPjks03-04 = 
                         0 otherwise

where AYPjks03-04 denotes whether subgroup k at school j met its AYP proficiency rate targets in 2003 and
2004 in subject s. Xjks02 is a vector of test score variables for subgroup k based on performance on

statewide exams in subject s during the school year 2001-2002, Njks04 is a vector of student subgroup size


17
   In the vast majority of states, student test performance during the 2001-2002 school year did not directly affect the
proficiency rates used to formulate schools’ AYP determinations during 2002-2003 or 2003-2004. A few states
incorporated 2001-2002 proficiency rates into 2002-2003 AYP determinations by generating two-year or three-year
average proficiency rates for student subgroups; the remaining states used contemporaneous proficiency rates. Most
states calculated a "safe harbor" provision whereby a school could make AYP if the only subgroup not meeting its
target proficiency rate demonstrated sufficient improvement from the prior year. In 2002-2003, this would be based
on performance relative to 2001-2002.
18
   This means a single school will have as many AYP predictions per subject (math or reading) as it has numerically
significant student subgroups. For states that further disaggregate subgroup results to the grade or grade span level,
we also define subgroups at this disaggregated level.


                                                                                                                        11
variables in subject s for subgroup k in 2004, XNjks represents interactions of test score and subgroup size

variables, Wj02 is a vector of control variables for school-level demographics from the school year 2001-
2002 (listed in Table 2), and M jks0304 is a vector of two dichotomous indicators for whether student

subgroup j was numerically significant in subject s in only 2002-2003 or only 2003-2004, and ζjks is a

normally distributed disturbance term. The Xjks02 vector includes cubic terms for the test performance in
subject s among students in subgroup k at school j.19 The subgroup size variables (Njks04) and interactions

with test score measures (XNjks) are included to account for states’ confidence interval adjustments and

the mechanical decrease in the error variance of student pass rates as the number of tested students within

subgroup k increases. In particular, the Njks04 vector contains cubic terms for the inverse of the square root

of the number of accountable test-taking students in subject s in subgroup k in school j during the school.

We exclude subgroups from our sample if they were too small to be accountable under AYP in both 2003

and 2004. Appendix 2 provides detailed descriptions of each predictor and its data source.

         We restrict our sample to schools that were (a) operational from at least 2001-2002 through 2003-

2004, (b) neither technical/vocational nor only for special education students according to the

classifications in the Common Core of Data and (c) served at least five students in grade 5 as of the

school year 2001-2002.20 We are forced to omit nine states from the SASS sample and five states from

the ECLS sample due to missing data (e.g., 2002 test scores or a state’s AYP determinations for
subgroups). Our numerous attempts at gathering these data from state departments of education have

either been unsuccessful or, in most cases, states claim that the data simply do not exist or are too




19
   Because we focus on schools serving fifth grade, we prioritize using fifth grade students’ 2001-2002 proficiency
rates for these control variables. Because some states either did not test fifth graders in 2001-2002 or disaggregated
2002-2003/2003-2004 subgroup AYP status by grade level, the 2001-2002 test performance variables are in some
cases based either in part or wholly on tests from other grades, typically grade 4 or grade 6; full details are provided
in Appendix 2. In addition, subgroup-specific performance for 2001-2002 is unavailable for some states, in which
case we use overall student test performance in subject s, and include interaction terms between test performance
and the fraction of the overall student population at each school comprised of students in group k. In practice, we
find that subgroup-specific and overall measures of pre-NCLB test score performance work equally well in
predicting the likelihood that the schools’ pass rates will be near the NCLB required cutoff in 2003-2004.
20
   We use the restriction of having five fifth graders because some schools that should serve grade 5 according to
grade level ranges indicated in the CCD actually enrolled no fifth graders. In cases where we use test performance
from a grade other than grade 5 in the Xjks02 vector, the regressions also include subgroups from schools serving the
tested grade even if the school does not serve grade 5. For example, if a state tested fourth graders but not fifth
graders in 2001-2002, we use grade 4 test performance in Xjks02 and include K-4 schools in our first stage. Full
details are provided in Appendix 2.


                                                                                                                      12
unreliable to release. Fortunately, these states have relatively small populations; more than 92 percent of

the U.S. population resides in one of the 41 states with sufficient data for our analyses.



4.1 Defining the AYP Margin

            We use predicted subgroup-level AYP pass probabilities from the state- and subject-specific

regressions in Equation 4 to construct measures of accountability pressure under NCLB. Our measures

are based on the following logic. Schools where all numerically significant subgroups have high chances

of passing state proficiency targets in both math and reading likely faced little NCLB pressure. In

contrast, schools where any numerically significant subgroup was close to the margin of passing are likely

to have faced accountability pressure. However, schools where any subgroup has a very low probability

of passing are unlikely to be able to do anything to change their AYP outcome in the short term.
            Following this logic, we construct the following school level measures of NCLB pressure:

    (i)         A school is classified as above the AYP margin if all numerically significant subgroups have
                a high chance of making AYP in both math and reading;
    (ii)        A school is classified as below the AYP margin if it has at least one numerically significant
                subgroup with a low chance of making AYP in either math or reading;
    (iii)       A school is classified as on the AYP margin for a particular subject if (a) at least one
                numerically significant subgroup in the school has a moderate chance of making AYP in that
                subject, and (b) no numerically significant subgroup in the school has a low chance of
                making AYP in either subject;
    (iv)        A school is classified as on the AYP margin if it is on the AYP margin for math or reading.

            For all of our analyses below, we define a “moderate chance” of a subgroup making AYP as

between 25 and 75 percent, a “high chance” as above 75 percent, and a “low chance” as less than 25

percent. While these cutoffs are admittedly ad hoc, our results are not very sensitive to using other

cutoffs ranging from between 35 and 65 percent to between 15 and 85 percent.

            Table 3 summarizes our measures of NCLB pressure for schools in these 41 states. We classify

69.1 percent of schools above the AYP margin, 21.4 percent on the AYP margin, and 9.5 percent below

the AYP margin. The actual rates with which schools made AYP in both 2003 and 2004 were 87 percent

for schools above the margin, 38 percent for schools on the margin, and 7 percent for schools below the

margin, demonstrating that our specification has sufficient power to identify substantial variation in which


                                                                                                                13
schools were at risk of failing to make AYP. However, our analyses below are predicated on the idea that

the risks of AYP failure were foreseeable to school administrators and teachers. To the extent that

measurement error causes us to misclassify which schools believed they were on the AYP margin, our

estimated effects of NCLB pressure may be biased towards zero. This possibility motivates the need to

examine whether our estimates are related to teachers’ and administrators’ reported sense of

accountability pressure, which we do below.

        The results reported in Table 3 also reveal that, with the exception of white and economically

disadvantaged students, most student subgroups were typically not numerically significant and did not

count towards AYP. For example, 70 percent of schools did not have a sufficient number of disabled

(special education) students in either 2003 or 2004 to be held accountable for that group’s performance.

This rate varied across states depending on minimum subgroup size requirements, again underscoring the
importance of these regulations. For example, disabled subgroups were accountable under NCLB in

either 2003 or 2004 in just 7 percent of Arizona schools, compared with 61 percent in Massachusetts.

        Among subgroups that were numerically significant and thus accountable, the fraction we predict
to have a moderate or low chance of making AYP varies considerably. The subgroups most frequently

predicted to have a moderate chance of passing in reading were disabled and limited English proficient

(30 and 37 percent, respectively) and, in math, disabled and Black (26 and 27 percent, respectively).
Disabled student subgroups also have relatively high fractions (about 15 percent) predicted to have low

chances of passing proficiency targets in both subjects, as do Native American subgroups (17 percent in

math, 22 percent in reading) and Asian subgroups in reading (25 percent). In contrast, White subgroups

are nearly always predicted to have a high chance of passing proficiency targets.



4.2 Variation in Predicted NCLB Pressure across States
        Our identification strategy is predicated on the idea that similar schools faced different levels of

NCLB pressure because of the state in which they were located. However, it is still broadly true that

schools with high average achievement had greater chances of making AYP than schools with low
average achievement. To illustrate both of these ideas, we take our primary measure of NCLB pressure—

whether a school was on the AYP margin—and plot cumulative distributions of the percent of schools on




                                                                                                           14
the margin across 41 states, separating schools by quartile of within-state campus-wide test score

performance in the school year 2001-2002.

        As expected, we place more schools on the AYP margin within the lowest performance quartile,

but being on the margin is by no means the exclusive territory of low scoring schools (Figure 3, top

panel). The median state has 60 percent of its lowest quartile schools on the AYP margin, but also has 25

percent of its second quartile, 10 percent of its third quartile, and 5 percent of the top performing quartile

on the AYP margin. Importantly, in all of our analyses below, we always include flexible controls for

schools’ relative performance on statewide examinations in the school year 2001-2002. Thus, our

identification is not based on the general tendency of low-scoring schools to face more NCLB pressure.

        These cumulative distributions also illustrate that we put many relatively high performing schools

on the AYP margin in some states with “tough” NCLB rules. In 20 percent of states, the percentages of
schools on the AYP margin in the lowest through highest performing quartiles were at least 80 percent,

50 percent, 25 percent, and 10 percent, respectively. In contrast, for the 20 percent of states that appear to

have the lowest amount of NCLB pressure, the percentages of schools on the AYP margin in the lowest
through highest performing quartiles were at most 40, 12, 5, and 3 percent, respectively.

        When we plot similar cumulative distributions for the percentage of schools we place below the

AYP margin, we see less variation but similar qualitative results (see Figure 3, bottom panel). As
expected, far more schools are below the margin in the bottom quartile of school test performance.

Nevertheless, we place hardly any schools below the margin in a few states, while in some states we place

a substantial fraction of schools below the margin in the second or third quartiles of within-state

performance. In other words, schools with reasonably good test scores still had a low chance of passing

AYP in some states with “tough” NCLB rules.



4.3 Assessing our Measure of NCLB Pressure in the ISBA Surveys

        To get an initial sense of the validity of our measures of NCLB pressure, we examine aggregate

statistics from surveys of principals and math teachers in California, Pennsylvania, and Georgia by the
RAND Corporation concerning topics related to NCLB in the school-year 2003-2004. As mentioned in

Section 3, the micro data from these surveys are not publicly available and we present these cross-

tabulations as suggestive evidence. We pursue a more rigorous methodology in Section 5.



                                                                                                            15
        We are only able to examine principals’ survey responses in 21 schools that we classified as on

the AYP margin and 104 schools above the AYP margin. No principals were surveyed at any school that

we predict had a low chance of making AYP. Among principals working in schools above the AYP

margin, 96 percent felt they would make AYP in the school year 2003-2004, relative to only 71 percent

who worked in schools on the AYP margin. Indeed, among principals in schools above the AYP margin,

72 percent felt they would make AYP for the next five years, relative to only 48 percent in the marginal

group (Table 4, Panel A). Principals in schools on the AYP margin were between 9 and 14 percentage

points more likely to say that they had: encouraged teachers to focus more time on tested subjects;

distributed commercial test preparation materials; or distributed copies of previous state tests or test

items. All of these differences in responses across principals in the two groups are statistically significant

at approximately the one percent level.
        Because of the larger number of teachers surveyed, we can examine teachers working in schools

we classify as below the margin (19 teachers), on the margin (224 teachers), and above the margin (1,074

teachers) of AYP. Relevant survey questions included probes about teaching test-taking strategies,
focusing on students who are close to proficient on the high stakes test, emphasizing the topics and types

of problems given on the state test, spending more time teaching content, and searching for more effective

teaching methods. Teachers working in schools on the AYP margin were between 11 and 19 percentage
points more likely than teachers working in schools above the AYP margin to report having taken these

actions, while teachers in schools below the AYP margin were between 3 and 20 percentage points more

likely to report having taken these actions than teachers in schools on the margin. All of the differences

between responses from teachers in the schools above the margin and either of the other two teacher

groups are statistically significant at the one percent level, and help confirm that our constructed measures

of NCLB pressure align with principals’ and teachers’ reported perceptions.



5. Estimates of the Impact of Accountability Pressure Under NCLB

        We use our measures of whether a school is below, on, or above the AYP margin to predict
various outcomes for an individual i (i.e., a student or teacher) in school j and state q. Our basic

regression specification is shown by Equation 5:

(5)     Yij   q  Qij 1  W j 02  2  X j 02 3   (Margin_AYPj )   (BelowMargin_AYPj )   ij .


                                                                                                           16
Yij is an outcome of interest, q represents state fixed effects, Qij is a vector of (student- or teacher-level)

control variables, and Wj02 is a vector of school-level control variables (as in Equation 4). The Xj02 vector
differs slightly from the Xjks02 vector in Equation 4 in that it contains school-wide student achievement

measures in reading and math during the school year 2001-2002, normalized within the state to have a

mean of zero and standard deviation of one, and the square and cube of these achievement measures.
         The coefficient of interest is , which represents the average impact of the NCLB pressure

associated with being in a school on the AYP margin. This estimate of  measures the causal effect of

short-term accountability pressure under the assumption that, conditional on a host of observable school

characteristics, the variation across states in whether a school falls on the AYP margin is exogenous. We

believe that the evidence presented in Section 3 provides ample support for this identification strategy.

The estimate of (i.e., the impact of NCLB pressure associated with being below the AYP margin) is less
well-identified and should be interpreted more cautiously, though there are likely very few schools that

would find themselves below the AYP margin in every state nationwide (see Figure 3). Nevertheless,

because few schools are predicted to be below the AYP margin, our estimates of  always remain
qualitatively similar if we drop the “below margin” indicator ( from the regressions.

         Because our measures of NCLB pressure are derived from first stage probit regressions, we

estimate standard errors using a two-sample bootstrap adjusted for school-level clustering. We use 1,000

Monte Carlo simulations of both the first- and second-stage models, randomly sampling coefficients from

the first-stage model using the implied distribution from the variance-covariance matrix which allows for

school clustering, and randomly sampling schools (with replacement) in the second-stage models.



5.1 Impacts on Teachers

         We examine the effect of NCLB pressure on teachers using the SASS data from 2003-2004.

Because many of these teachers do not teach students or subjects tested under NCLB, we augment

Equation 5 with an indicator for whether the teacher taught math or reading in a NCLB-tested grade and

interact this with the indicators for whether the school was on or below the AYP margin.21 The Qij vector



21
  In cases of teachers covering multiple grades, we set the “high-stakes grade/subject” indicator variable equal to
one if the teacher covers either math or reading and more than half of the teacher’s covered grade levels were tested
for NCLB in that teacher’s state during the spring of 2004.


                                                                                                                   17
includes the teacher-level control variables listed in Table 2, with both linear and squared terms for

teachers’ years of experience.

        The first column of Table 5 (Panel A) displays estimated effects of NCLB pressure on whether

teachers strongly agreed with the statement: “I worry about the security of my job because of the

performance of my students on state and/or local tests.” Compared to teachers of high-stakes

grades/subjects at schools above the AYP margin, those in schools on the AYP margin are 5 percentage

points more likely to report concern over their job security related to student test performance—a large

increase considering that only 7.5 percent of teachers reported this concern overall. Like the ISBA results

above, this supports the notion that our measure of NCLB pressure is valid and captures significant

variation in school staff members' perceptions of pressure. Column 1 in Panel B of Table 5 displays

estimates from a specification that restricts the sample to untenured teachers.22 Although the resulting
drop in the number of observations decreases our precision significantly, we do find that untenured

teachers working in schools below the AYP margin, even those not teaching high-stakes grades/subjects,

tend to be very concerned about how student test performance will affect their job security.

        We next examine how NCLB pressure affects teachers' long-term career plans. We construct an

indicator variable from teachers’ survey responses concerning whether they plan to teach until retirement.

If a school is on the AYP margin, untenured teachers are at least as likely to have long-term teaching
career plans as their counterparts teaching in schools with higher probabilities of making AYP. However,

teachers working in high-stakes grades/subjects in a school below the AYP margin are 13 percentage

points less likely to plan to teach until retirement than those working in a school above the AYP margin
(Table 5, Panel A, Column 2). This is in line with findings by Feng et al. (2010) that schools in Florida

that received very low accountability ratings subsequently experienced higher rates of teacher turnover.

The effect is stronger for untenured teachers (Table 5, Panel B, Column 2), suggesting that teachers at

greatest risk of job loss might be discouraged by the challenge of raising student proficiency rates at

schools that are unlikely to make AYP and face state sanctions.

        The third column of Table 5 presents results concerning how NCLB pressure affects teachers'

total weekly work hours, measured several months ahead of NCLB testing. The largest and most

22
  The SASS does not measure tenure, so we created an indicator for whether a teacher’s total years of experience
(measured in the SASS) exceeds the state’s required number of years for tenure. Data on state requirements come
from 2002-2003, see Brunner and Imazeki (forthcoming), and we thank Eric Brunner for providing them.


                                                                                                               18
significant differences in work hours occur for untenured teachers who work in high-stakes areas at

schools on the AYP margin. These teachers report working about four hours more per week than their
untenured co-workers teaching low-stakes grades/subjects at their schools, a difference with a p-value of

0.03.23 Given that the standard deviation of work hours is under 10 hours (see Table 1), this difference in

hours worked by untenured teachers is substantial.
         We also estimate the impact of NCLB pressure on teachers' self-reported number of instructional

hours per week, which is a subset of their total work hours. Unlike total hours, instructional hours do not

increase for any type of teacher in response to NCLB pressure (Table 5, Column 4). Moreover,
instructional hours actually decline by one to two hours per week for teachers in schools below the AYP

margin and for untenured teachers in schools on the AYP margin, though not differentially for those

teaching high-stakes grades/subjects. The fact that untenured teachers in schools facing short-term NCLB

pressure report working more hours but spending less time on instruction presumably means that part of

their work day shifts towards other activities such as student assessment, grading, lesson planning, or

other non-instructional activities. This heightens the concern that NCLB pressure may have negative
effects on student achievement, particularly for material not covered on high-stakes exams, and provides

further motivation for our analysis of student outcomes via the ECLS data.

         In the SASS data, surveyed teachers are randomly selected within each school, so their activities

should be an unbiased (albeit noisy) measure for all teachers in the school. Nevertheless, one concern for

our analysis is that principals at schools facing NCLB pressure strategically place teachers into high-

stakes grades and subjects. However, we believe such behavior would most likely create bias against our

findings. If principals wish to boost high-stakes test performance, they should assign their most talented

teachers to high-stakes areas. Yet we find these teachers are more concerned about their job security, are

less likely to plan to teach until retirement, and work longer hours only if they do not have tenure.

         While the SASS data are too coarse to identify the specific activities to which teachers devote

more time, we can explore shifts in instructional time across subject areas using self-reports of teaching

content during the previous week. The SASS surveyed teachers in the fall, well ahead of NCLB testing,

23
  We computed the p-value of this estimate using the bootstrapped standard error for the sum of the relevant
coefficients. The 3.07 point estimate for the interaction of teaching high-stakes with working in a school on the
AYP margin is marginally significant, with a p-value of .11, suggesting that high-stakes teachers in schools under
accountability pressure also work longer hours than high-stakes teachers in schools facing little pressure under
NCLB.


                                                                                                                     19
and survey responses should reflect general shifts in instruction rather than last-minute preparation for

high-stakes tests. To examine whether NCLB pressure shifted resources away from low-stakes subjects,
we focus on whether the teacher taught at least one science lesson or at least one social studies lesson

during the prior week. The estimates displayed in Table 6 suggest that schools on the AYP margin

slightly change the proportion of teachers offering science lessons. Compared to teachers at schools

above the margin, these teachers are 4.0 percentage points less likely to have taught a science lesson in

the last week. They are also 1.3 percentage points less likely to have taught a social science lesson, but

this estimate is not statistically significant. The effects on science and social studies offerings in schools

below the AYP margin are even larger and both statistically significant. Compared to teachers at schools

above the margin, teachers in schools with a low chance of making AYP are 10 percentage points less

likely to offer a science lesson and 6 percentage points less likely to offer a social studies lesson. These
are large differences considering that 63 percent and 65 percent of teachers in this sample taught science

and social studies lessons, respectively, in the previous week (see Table 1). Schools with little short-term

chance of making AYP may still try to shift instruction toward the high-stakes subjects in order increase
their chances of eventually making AYP. 24


5.2 Impacts on Students
         Changes in teachers’ work and instructional time measured in the SASS data provide somewhat

ambiguous evidence on whether NCLB pressure leads to broad increases in student achievement or a

more narrow focus on the material included on high-stakes tests. We address this issue directly using the
ECLS data. Our student-level regression specifications are based on Equation 5, and control for all of the

variables listed in Table 2 as well as state fixed effects and a third degree polynomial of the student's

standardized math and reading performance in both the first and third grade waves of the ECLS. Thus,

our identification comes only from comparing students with very similar prior learning trajectories who

attend schools with similar demographic and academic characteristics but different levels of NCLB

pressure because of the state in which they are located.


24
  A reduction in science and social studies lessons could be driven by schools using specialists who teach intensive
amounts of science and social studies to compensate for decreased lessons from other instructors. However, if we
examine hours taught in science or social studies conditional on teaching at least one lesson in the subject we find no
support for this explanation.


                                                                                                                    20
         Panel I of Table 7 displays coefficient estimates for our indicator of whether a school was on the

AYP margin.25 Estimated coefficients for our indicator that a school is below the AYP margin were never
even marginally significant and are not reported for ease of exposition. Our estimates suggest that NCLB

pressure has either neutral or positive effects on student achievement in both low- and high-stakes

subjects. Students' reading scores are .073 of a standard deviation greater on average when schools are on

the AYP margin (Table 7, Panel I, Column 1). This estimate is statistically significant at the .05 level and

is a considerably large effect; previous estimates of the impact of accountability pressure on high-stakes

tests are typically between 0.1 and 0.2 standard deviations (e.g., Rouse et al., 2007; Rockoff and Turner,
2010). Students' math scores are 0.043 of a standard deviation greater on average when they attend a

school that is on the AYP margin, though this estimate is not statistically significant at the 10 percent

level. Science scores are also greater (0.049 of a standard deviation), though again this estimate is not

statistically significant. Although we are examining results for multiple dependent variables, a power test

suggests that these three estimates are far too large to occur by chance.26

         Importantly, our results also suggest that when schools face NCLB pressure, gains in
achievement do not decrease students’ enjoyment of reading or math, or increase anxiety over testing.

Respective point estimates for the impact of NCLB pressure on students' enjoyment of reading and math

are -0.031 standard deviations (statistically insignificant) and 0.148 standard deviations (significant at the

5 percent level). We also find a small and statistically insignificant decrease of 0.05 standard deviations

in students’ reported anxiety over testing.

         The framework presented in Section 2 motivates the idea that the impacts of NCLB may differ

across students within a school. We first examine whether our estimates depend on whether schools

faced strong pressure to raise proficiency rates for the overall student population or for the focal student's

own subgroup(s). Estimates presented in Panel II come from specifications where we replace the single

"on the AYP margin" variable with three mutually exclusive indicators for whether the school was on the

AYP margin due to: (1) the overall student group, (2) the student's own subgroup (and not the overall

25
   We focus on the AYP margin for most relevant subject(s): math for math test performance or enjoyment, reading
for reading test performance or enjoyment, and either math or reading for science test performance or anxiety about
standardized tests. We lack power to separate relevant-subject and cross-subject effects using the ECLS.
26
   To test the joint significance of these test score estimates, we simulated estimations of these three models after
randomly reassigning schools to different AYP status. Out of 1,000 simulations, none produced three estimates that
were, respectively, at least as large in absolute value of as the actual highest, second highest and third highest
estimate reported in the first three columns of Panel I of Table 7.


                                                                                                                  21
student group as well), and (3) other subgroups (and not the student's own subgroup or the overall student

group). The point estimates for all three subjects in Panel II are positive, regardless of whether the

students are members of subgroups whose performance is most critical to the schools' AYP ratings, and

the relative magnitudes of the three coefficients are different across the three subject areas. Thus, we find

no systematic patterns suggesting that our results are driven by targeting particular student subgroups.27
        To produce the largest increase in student proficiency rates, schools might also direct resources to

students who are likely to score close to the threshold of passing the exam. Previous studies find

evidence of distributional effects on student achievement on high-stakes exams (e.g., Reback, 2008; Neal

and Whitmore Schanzenbach, 2010), and we provide evidence here concerning distributional effects on

low-stakes exams. We classify a student as “on the bubble” for passing their state exam if their third

grade test score was within 15 percentiles below or 5 percentiles above their states' NCLB exam passing
threshold.28 The coefficients on the interaction of our indicator for whether a student is "on the bubble"

and whether the school is on the AYP margin are generally positive but statistically insignificant (Panel

III of Table 7). In other words, students on the bubble of passing high-stakes exams do not appear to

perform differently on low-stakes exams when their schools face strong NCLB pressure, although our

estimates are too imprecise to rule out small effects.



6. Conclusion

        As a result of the No Child Left Behind act, virtually every public school in the U.S. is now

accountable for meeting measured targets for student test performance, yet our understanding of the

impact of NCLB has been hindered by a lack of national data on implementation and nationally

comparable data on outcomes. Assembling an extensive national data set of school and student subgroup


27
   Of course, it is still possible that schools facing NCLB pressure target resources toward subgroups in ways that
affect performance on high-stakes tests. However, the highly reliable examinations in the ECLS provide some
assurance that NCLB pressure does not systematically lead to adverse achievement outcomes for students with more
or less influence on whether the school makes AYP.
28
   The National Center for Education Statistics (2007) estimates NAEP score equivalents associated with the passing
threshold for most states' NCLB exams, and we obtained national percentile equivalents for these NAEP scores. We
are unable to do this for eight ECLS states that were not included in the National Center for Education Statistics
(2007) publication. Using ranges smaller than 20 percentiles would lead to highly imprecise estimates, and we use a
wider range below the cutoffs than above the cutoffs because schools may have anticipated their capacity to improve
student performance over time—i.e., most states experienced upward trends in proficiency rates over the first few
years of NCLB. For reading and math outcomes our indicator is subject specific; for science tests and test anxiety
we use an indicator for being on the bubble in either math or reading.


                                                                                                                22
performance on the examinations required under NCLB, we exploit extensive cross-state variation in

rules and standards to examine how NCLB affects school personnel and students. In schools facing

strong short-term NCLB pressure, we find that teachers report greater concern over how student test

performance will affect their job security and that untenured teachers in high-stakes grades/subjects work

longer hours. We also find evidence that schools which will almost certainly fail state requirements in the

short term allocate less time to science and social studies instruction, and that untenured teachers in these

schools feel much lower job security and expect to exit the teaching profession sooner.

        Relative to students in schools facing little NCLB pressure, students in schools facing strong

short-term incentives to improve student proficiency raise achievement by 0.07 standard deviations on

low-stakes reading exams, do at least as well on low-stakes math and science tests, do not report less

enjoyment of reading or math, and do not report more test anxiety. These results do not differ
significantly across student subgroups that have differential influence on whether a school fails NCLB

requirements, nor between students close to the passing threshold and those farther away.

        Our finding that short-term NCLB pressure does not negatively affect student learning or
enjoyment of learning is quite important, given widely held concerns about the use of test-based

accountability systems. On the other hand, our results also raise questions concerning whether NCLB

pressure motivates both tenured and untenured teachers alike, whether talented teachers are discouraged
from working in schools with little chance of meeting NCLB requirements, and whether schools neglect

low-stakes subjects if their performance lags far below NCLB standards. These issues loom larger every

year as NCLB standards become more stringent and more schools fail to meet those standards. As

Congress is likely to debate revisions to No Child Left Behind in the near future (see Dillon, New York

Times, 2010), policymakers may wish to ensure that schools along the entire performance spectrum face

more continuous incentives to improve along a wide array of outcomes. A revised accountability system
could offer rewards designed so that incentives are independent of students’ prior achievement levels and

the scaling of test scores.29

        Policymakers may also want to consider the large differences in rules and regulations across

states, which we as researchers use to identify the effects of NCLB pressure on schools. Thus far, the

29
  See Barlevy and Neal (2010) for a detailed discussion and analysis of one example of such a system. They
propose a system for rating teacher performance, but a similar system could instead be used to assess school
performance.


                                                                                                               23
minutiae of state rules have largely determined the difficulty of meeting AYP. Although a majority of

states have adopted "Common Core State Standards" that may increase the consistency of student

achievement tests across states, most of the current variation in AYP failure rates across states is not

driven by the difficulty of state exams. If policymakers would like to establish more uniformity across

states’ school accountability standards, then reforms must address the other sources of variation within

state formulae. Ideally, accountability pressure should stem from the performance of students along the

entire distribution of achievement, rather than the idiosyncrasies of state rules.




                                                                                                           24
References


Ballou, D. & Springer, M.G. (2008). Achievement Trade-Offs and No Child Left Behind. Working Paper.
        Urban Institute.

Barlevy, G. & Neal, D. (2010). Pay for Percentile. mimeo, University of Chicago, October 2010.

Booher-Jennings, J. (2005). Below the Bubble: “Educational Triage” and the Texas Accountability
       System. American Educational Research Journal 42: 231-268.

Brunner, E. & Imazki, J. (forthcoming). Probation Length and Teacher Salaries: Does Waiting Pay Off?
       forthcoming in Industrial Relations and Labor Review.

Chakrabarti, Rajashri. (2007). Vouchers, Public School Response, and the Role of Incentives: Evidence
       from Florida. Federal Reserve Bank of New York Staff Reports, no. 306.

Chiang, H. (2009). How accountability pressure on failing schools affects student achievement. Journal
        of Public Economics 93, 1045-1057.

Cullen, J.B. & Reback, R. (2006). Tinkering toward accolades: School gaming under a performance
        accountability system. In T. Gronberg & D. Jansen (Eds), Advances in Applied Microeconomics,
        14.

Dee, T. & Jacob, B. (2009). The impact of No Child Left Behind on student achievement. NBER
        Working Paper No. 15531. Retrieved November 20, 2009, from NBER’s web site:
        http://www.nber.org/papers/w15531

Dillon, S. (2010). Obama to Seek Sweeping Change in ‘No Child’ Law. New York Times, February 1,
        2010.

Feng, L., Figlio, D., and Sass T. (2010) “School Accountability and Teacher Mobility,” NBER Working
        Paper 16070.

Figlio, D. (2006). Testing, crime, and punishment. Journal of Public Economics 90, 837-851.

Figlio, D. & Getzler, L. (2006). Accountability, ability, and disability: Gaming the system? In T.
         Gronberg & D. Jansen (Eds), Advances in Applied Microeconomics, 14.

Figlio, D. & Lucas, M. (2004). What's in a grade? School report cards and the housing market. American
         Economic Review 94(3), 591-604.

Figlio, D. & Rouse, C. (2006). Do accountability and voucher threats improve low-performing schools?
         Journal of Public Economics 90, 239-255.

Figlio, D. & Winicki, J. (2005). Food for thought? The effects of school accountability plans on school
         nutrition. Journal of Public Economics 89, 381-394.

Hanushek, E. & Raymond, M. (2005). Does school accountability lead to improved school performance?
      Journal of Policy Analysis and Management 24(2), 297 – 327.



                                                                                                          25
Jacob, B. (2005). Accountability, incentives and behavior: the impact of high-stakes testing in the
        Chicago Public Schools. Journal of Public Economics 89(5-6), 761-796.

Jacob, B. & Levitt, S. (2003). Rotten apples: An investigation of the prevalence and predictors of teacher
        cheating. Quarterly Journal of Economics 118(3), 843-877.

Krieg, J. 2008. Are students left behind? The distributional effects of No Child Left Behind. Education,
        Finance and Policy 3(2), 250-281.

Ladd, H.F. & Lauen, D.L. (2009). Status versus growth: The distributional effects of school
       accountability policies. Working paper. Urban Institute.

Ladd, H.F. & Zelli, A. (2002). School-based accountability in North Carolina: The responses of school
       principals. Educational Administration Quarterly 38(4), 494-529.

National Center for Education Statistics. (2007). Mapping 2005 State Proficiency Standards Onto the
       NAEP Scales (NCES 2007-482). U.S. Department of Education. Washington, DC: Author.

Neal, D. & Whitmore Schanzenbach, D. (2010). Left behind by design: Proficiency counts and test-based
        accountability. Review of Economics and Statistics. 92(2): 263-283.

Reback, R. (2008). Teaching to the rating: School accountability and the distribution of student
       achievement. Journal of Public Economics 92, 1394-1415.

Rockoff, J., and Turner, L. (2010). Short-run Impacts of Accountability on School Quality, American
       Economic Journal, Economic Policy 2(4): 119-147.

Rouse, C., Hannaway, J., Goldhaber, D., & Figlio, D. (2007). Feeling the Florida heat? How low-
       performing schools respond to voucher and accountability pressure. National Bureau of
       Economic Research, working paper 13681.

Springer, M.G. (2008). The influence of an NCLB accountability plan on the distribution of student test
       score gains. Economics of Education Review 27(5), 556-563.




                                                                                                           26
                       Figure 1: Distribution of AYP Failure Rates Across States, 2003



                   5
                   4
Number of States
                   3
                   2
                   1
                   0




                        0            20              40             60           80
                                          Percent Failing AYP in 2003
Figure 2: AYP Failure Rates vs. NAEP Proficiency Rates by State, 2003




                                      80
                                                                                                    FL
                                                                                           SC
 % of Schools Failing to Make AYP
                                                                                           AK

                                      60
                                                                      HI                             DE
                                                                                TN
                                                                     NV                                   SD
                                                                                                               CO
                                      40


                                                                                                    UT
                                                                          CA                       IL
                                                                                       AR               NE      VA    NJ
                                                                                     KY   RI
                                                                                     WV               MO PA
                                                                                                     MD
                                                      NM                       OK               ID             OH KS
                                                      MS                                                       ME
                                                                                                                            NH
                                      20




                                                                               AZ                        MT                  CT
                                                                                                         MINY      NC      MA
                                                                                      GA              OR
                                                                                                            IN WA WY   VT
                                                                                                                        MN
                                                                                                        ND
                                                                AL
                                                                LA                             TX         WI IA
                                      0




                                           15              20                  25           30             35         40
                                               Statewide average of NAEP 4th grade Proficiency Rates in Math and Reading

                                           Note: Line represents a locally weighted regression.


                                    Failure rates are based on schools serving at least five fifth grade students.
           Figure 3: State Variation in the Percentage of Schools Facing NCLB Pressure
                                                                          Schools on the AYP Margin




                                    1
               Percentage of 41 Sample States (CDF)
                  .2             .5 0          .8




                                                      0   .1         .25                 .5               .75                   1
                                                           State Percentage of Schools On AYP Margin in Math or Reading
                                                            Bottom Quartile       2nd Quartile     3rd Quartile       Top Quartile



                                                                      Schools Below the AYP Margin
                                    1
              Percentage of 41 Sample States (CDF)
                 .2             .5  0         .8




                                                      0   .1          .25                .5                .75                  1
                                                          State Percentage of Schools Under AYP Margin in Math or Reading

                                                            Bottom Quartile       2nd Quartile      3rd Quartile      Top Quartile



Note: These figures show cumulative distributions of the percentage of schools we consider on the margin of
making Adequate Yearly Progress (top panel) and below the margin of making Adequate Yearly Progress (bottom
panel) for 2003 and 2004 for the 41 states in our Schools and Staffing Survey analysis. Quartiles reflect schools’
positions in their own state’s distribution of student test performance during the school year 2001-2002.
Table 1: Summary Statistics for Dependent Variables

                                                                                        Mean             SD
Teacher-level Dependent Variables from the SASS
  Concerned about Job Security due to Student Test Performance                          7.5%
  Plan to Teach Until Retirement                                                        78%
                           †                                                            52.4             8.91
  Work Hours per weekeek
                                      †                                                  29.1            5.17
  Instructional Hours per weekeek

  Gave at Least One Science Lesson Last Week                                             63%
  Gave at Least One Social Studies Lesson Last Week                                      65%

  Untenured Teachers Only:
    Concerned about Job Security due to Student Test Performance                         11%
    Plan to Teach Until Retirement                                                       73%
                          †                                                              53.8            9.46
    Work Hours per week
                                 †                                                       29.5            5.40
    Instructional Hours per week

Student-level Dependent Variables from the ECLS
  5th Grade Reading Score (Standardized)                                                .009             .967
  5th Grade Math Score (Standardized)                                                   .028             .982
  5th Grade Science Score (Standardized)                                                .081             .950
  Enjoyment of Reading (Standardized)                                                   -.002            1.01
  Enjoyment of Math (Standardized)                                                      .037             1.01
  Anxiety about standardized tests                                                      42%

Notes to Table 1: Means and standard deviations using relevant sample weights provided by the SASS and ECLS to
produce nationally representative estimates. The sample is restricted to observations used in the main analyses:
teachers in 41 states for the SASS sample and students in 35 states in the ELCS sample. The sample sizes are
approximately 7,870 teachers for the SASS sample (1,440 for untenured teachers only) and approximately 6,860
students for the ECLS sample, (rounded to the nearest 10 due to restricted-use data reporting requirements).
Standardized variables are Z-scores that were standardized based on the national, cross-sectional student
distribution; their means and standard deviations above differ from zero and one respectively because some
states/students are omitted due to missing data and because we use longitudinal sampling weights rather than cross-
sectional sampling weights.
†We set teachers' work-related hours and instructional hours to missing if their reported instructional hours were 60
hours or greater, a suspiciously high level of reported instructional time given the typical five day school week. The
work hours per week variable is based on teachers’ self-reported hours spent on “all teaching and other school-
related activities during a typical full week.”
Table 2: Descriptive Statistics for Control Variables
                                                              SASS Sample                        ECLS Sample
 Variable                                                   Mean        SD                     Mean       SD
     School characteristics
     Within-state Z-score for 2001-2002 Reading              0.007      0.949                   0.125    0.957
     Within-state Z-score for 2001-2002 Math                 0.043      0.925                   0.100    0.960
     Eligible for Title I                                     69%                               60%
     Number of enrolled students                              587        258                     587      251
     Percent Asian students                                    4%        9%                      5%      10%
     Percent Hispanic students                                19%       28%                     16%      24%
     Percent African American students                        18%       26%                     19%      26%
     Percent economically disadvantaged students             47%        30%                     44%      30%
     Number LEP students in the grade                                                             5       13
     Missing Number of LEP students in the grade                                                14%
     Teacher characteristics (from the SASS)
     Total years of experience                                13.9       10.1
     Teaches Math                                             77%
     Teaches Reading (or English)                             83%
     Teaches a high-stakes subject/grade                      34%
     Teaches grades 2 or 3                                    41%
     Teaches grades 4 or 5                                    42%
     Teaches grades 6 or 7                                    14%
     Teaches grades 8 or 9                                    7%
     Teaches grade 10 or higher                                3%
     Teaches grades 2 or 3 and grades 4 or 5                  15%
     Teaches grades 4 or 5 and grades 6 or 7                   7%
     Teaches grades 6 or 7 and grades 8 or 9                   5%
     Teaches grades 8 or 9 and grade 10 or higher              2%
     Family characteristics (from the ECLS)
     Two parent household                                                                       67%
     Mother's education level unknown                                                            9%
     Mother has at least a high school diploma                                                  89%
     Mother possesses a B.A.                                                                    31%
     Family income missing                                                                      16%
     Family income under $20,000                                                                15%
     Family income $20,000 -$35,000                                                             18%
     Family income $35,000 - $50,000                                                            14%
     Family income $50,000 - $75,000                                                            14%
     Family income $75,000 - $100,000                                                           11%
     Student characteristics (from the ECLS)
     Reading Z-score in spring 2000                                                             0.017    0.950
     Math Z-score in spring 2000                                                                0.029    0.919
     Reading Z-score score in spring 2002                                                      -0.001    0.981
     Math Z-score in spring 2002                                                                0.029    0.970
     African American                                                                           18%
     Hispanic                                                                                   20%
     Asian                                                                                       3%
     Other                                                                                       5%
     Female                                                                                     48%
     Date of birth (measured in days)                                                         3/18/93     140
 N = approximately 7,870 teachers for the SASS sample and approximately 6,860 students for the ECLS sample.
 Table 3: Predictions of AYP Outcomes
 Panel A: School-wide Outcomes
                                                                                  On the AYP Margin               Below the AYP Margin              Above the AYP Margin
 Percent of Schools                                                                      21.4%                             9.5%                             69.1%
  Percent Actually Made AYP 2003 and 2004                                                37.9%                             7.4%                             86.5%
 Panel B: Subgroup Outcomes                                                                                                Conditional on Numerical Significance

                                                                                Numerically Significant         Predicted Moderate Chance           Predicted Low Chance
                                                                                     Subgroup                     Math            Reading           Math            Reading
 Overall School Population                                                               92.8%                    7.2%             9.0%             2.1%             2.5%
   Actually made AYP in subject in '03 and '04                                                                    51.9%            52.4%            10.7%            8.9%
 Economically Disadvantaged                                                              60.5%                    14.2%            17.4%            3.7%             4.6%
   Actually made AYP in subject in '03 and '04                                                                    54.0%            53.0%            12.8%            12.7%
 Limited English Proficient                                                              20.0%                    18.6%            36.7%            4.8%             10.6%
   Actually made AYP in subject in '03 and '04                                                                    58.3%            49.9%            13.5%            19.5%
 Disabled                                                                                30.0%                    26.1%            30.0%            13.9%            15.8%
   Actually made AYP in subject in '03 and '04                                                                    52.0%            53.1%            14.1%            12.3%
 White                                                                                   69.5%                    1.2%             0.9%             0.1%             0.0%
   Actually made AYP in subject in '03 and '04                                                                    55.2%            61.5%            15.8%            25.0%
 Black                                                                                   29.7%                    26.9%            23.2%            9.5%             7.8%
   Actually made AYP in subject in '03 and '04                                                                    51.5%            52.5%            16.7%            15.3%
 Hispanic                                                                                28.7%                    10.9%            18.6%            1.2%             2.7%
   Actually made AYP in subject in '03 and '04                                                                    56.8%            54.6%            13.8%            15.0%
 Asian/Pacific Islander/Filipino                                                         12.3%                    0.7%             3.5%             0.0%             25.2%
   Actually made AYP in subject in '03 and '04                                                                    54.6%            53.6%            33.3%            8.8%
 Native American / Alaskan Native                                                         5.9%                    14.7%            14.5%            17.1%            22.1%
   Actually made AYP in subject in '03 and '04                                                                    56.7%            52.4%            5.7%             7.3%

Notes to Table 3: This sample includes all public schools used to estimate Equation 4. These schools provide 2001-2002 student test performance data for the relevant grade
level, typically fifth grade. For more details on chosen grade levels, please consult the "Student test performance in focal subject in 2001-2002" row in Appendix 2.
Table 4: Evidence on NCLB Pressure from the ISBA Survey in California, Georgia, and Pennsylvania
                                                                                                              Above                  On
                                                                                                            AYP Margin           AYP Margin
Panel A: Principals                                                                                          (N=104)               (N=21)
Do you agree with the following statement:
  My school can attain the AYP targets for 2003-2004                                                          96.1%                71.4%
  My school can attain the AYP targets for the next five years                                                71.6%                47.6%
Has your school and/or district done any of the following:
  Encouraged or required teachers to spend more time on tested subjects and less time on other
subjects                                                                                                     49.0%                61.9%
  Distributed commercial test preparation materials                                                          67.0%                81.0%
  Distributed released copies of the state test or test items                                                76.9%                85.7%
                                                                                                              Above                 On                  Below
                                                                                                           AYP Margin           AYP Margin           AYP Margin
Panel B: Math Teachers                                                                                      (N=1074)             (N=224)               (N=19)
As a result of the state mathematics test:
  I focus more effort on students who are close to proficient                                                 25.9%                41.3%                52.6%
  I spend more time teaching general test-taking strategies                                                   52.6%                66.7%                73.7%
  I look for particular styles and formats of problems in the state test and emphasize those in my
instruction                                                                                                   66.5%                79.9%               100.0%
  I focus more on topics emphasized in the state test                                                         69.4%                81.3%               84.2%
  I spend more time teaching content                                                                          54.1%                73.4%               79.0%
  I search for more effective teaching methods                                                                72.7%                83.9%               94.4%

Notes to Table 4: Percentages shown in this table refer to the percentage of respondents who agreed with the corresponding statement. Above, on, and below
the AYP margin correspond to our classifications of how likely the school was to make AYP in 2003 and 2004. See Section 4 of the paper for details. No
principal surveyed was in a school classified by our analysis as below the AYP margin. All of the differences in rates between the groups above the AYP
margin and either of the other two groups are statistically significant at approximately the .01 level or better. Differences in rates between teachers in schools
above the AYP margin and those in schools on the AYP margin are statistically significant at the .05 level for "I focus more effort on students who are close to
proficient," and at the .01 level for "I look for particular styles..." and "I search for more effective teaching methods."
Table 5: Effects of NCLB Pressure on Teacher Attitudes and Work Hours


                                     Concerned about                                                      Instructional
                                     Job Security due         Plan to Teach          Work Hours            Hours in a
                                      to Student Test             Until              in a Typical           Typical
                                       Performance             Retirement               Week                  Week
 All Teachers
 On the AYP Margin                          -0.011                 0.005                   0.07               -0.41
                                            (.013)                (.024)                  (.54)               (.30)
 Below the AYP Margin                        0.025                 0.004                 -1.10                -1.17**
                                            (.021)                (.035)                  (.84)                (.40)
 Teach High-stakes                           0.014                 0.008                 -0.41                -0.27
                                            (.014)                (.024)                  (.48)                (.29)
 On the AYP Margin                           0.050**              -0.045                 -0.03                -0.38
  *Teach High-stakes                        (.024)                (.036)                  (.78)                (.45)
 Below the AYP Margin                        0.025                -0.128**                1.00                0.23
  *Teach High-stakes                        (.035)                 (.046)               (1.02)                (.56)

 Untenured Teachers Only
 On the AYP Margin                           0.017                 0.114*                -0.74                -1.93**
                                            (.042)                (.059)                (1.44)                 (.81)
 Below the AYP Margin                        0.124**               0.116                  0.04                -2.21**
                                            (.061)                (.083)                (2.04)               (1.03)
 Teach High-stakes                           0.022                 0.096*                 0.87                0.19
                                            (.043)                (.057)                (1.18)                (.77)
 On the AYP Margin                           0.020                -0.089                  3.07                 0.34
  *Teach High-stakes                        (.064)                (.087)                (1.94)               (1.18)
 Below the AYP Margin                      -0.050                 -0.286**                2.70                -1.21
  *Teach High-stakes                       (.085)                  (.120)               (2.03)               (1.31)


Notes to Table 5: Each column displays estimates from two separate teacher-level regressions using data from the
2003-2004 wave of the Schools and Staffing Survey (SASS). The top panel uses a sample of both tenured and non-
tenured teachers (sample size approximately 7,870), and the bottom panel restricts the sample to untenured teachers
(sample size approximately 1,440). Teachers' tenure status is not reported directly in the SASS, so we impute it using
teachers' reported years of experience and their states' tenure policies (see footnote 24). All models control for the
independent variables with summary statistics listed in the "SASS sample" column of Table 2, and also control for state
fixed effects, for a squared term for the number of Limited English proficient students in the grade, for a squared term
for the teacher’s years of experience, and for squared and cubic terms for schools' within-state standardized 2001-2002
test score performance in both math and reading. All models use the SASS cross-sectional sample weights to make the
estimates nationally representative. Bootstrapped standard errors, adjusted for school-level clustering using 1,000
Monte Carlo simulations of both the first-stage and second-stage models, are displayed in parentheses below each
estimate.
 ** significant at .05 level; * significant at .10 level.
Table 6: NCLB Pressure and Instruction in Low-stakes Subjects

                                                     Teacher gave at          Teacher gave at least
                                                    least one science          one social studies
                                                    lesson last week            lesson last week

 On the AYP Margin                                       -0.040 *                    -0.013
                                                          (.024)                      (.021)
 Below the AYP Margin                                    -0.104 **                   -0.062 **
                                                          (.038)                      (.029)


Notes to Table 6: Each column displays estimates from a teacher-level regression using data from the 2003-2004 wave
of the Schools and Staffing Survey (SASS). Sample size is approximately 7,870. These models control for the
independent variables with summary statistics listed in the "SASS sample" column of Table 2, except for the indicators
for whether the teachers covered math or reading and the indicator for whether the teachers covered a high-stakes
grade/subject. Similar to Table 5, the models also control for state fixed effects, for a squared term for the number of
Limited English proficient students in the grade, for a squared term for the teacher’s years of experience, and for
squared and cubic terms for schools' within-state standardized 2001-2002 test score performance in both math and
reading. All models use the SASS cross-sectional sample weights to make the estimates nationally representative.
Bootstrapped standard errors, adjusted for school-level clustering using 1,000 Monte Carlo simulations of both the first-
stage and second-stage models, are displayed in parentheses below each estimate.
** significant at .05 level; * significant at .10 level.
Table 7: Effects of NCLB Pressure on Student Learning and Motivation
                                             Reading Score               Math Score             Science Score            Enjoyment of        Enjoyment of           Anxious About
                                                                                                                           Reading               Math             Standardized Tests
Panel I (35 states)
   On the AYP Margin                                   0.073**              0.043                    0.049                -0.031                 0.148*                  -0.051
                                                      (.033)               (.041)                   (.035)                 (.069)                (.082)                   (.037)
Panel II (35 states)
 On the AYP Margin based on the performance of…
    Overall student group                              0.010                0.092                    0.040                   0.061                0.282**                  0.005
                                                      (.051)               (.073)                   (.058)                 (.116)                (.144)                  (.063)
    Student's subgroup (not overall)                   0.053                0.014                   0.074                   -0.012                0.109                   -0.038
                                                      (.051)               (.074)                    (.05)                 (.103)                (.153)                    (.05)
    Other subgroup(s) (not overall or                  0.108**              0.030                    0.034                  -0.078                0.099                  -0.086 *
     student's subgroup)                              (.041)               (.051)                   (.038)                 (.082)                (.099)                  (.044)

Panel III (27 states)
 On the AYP Margin *                                 -0.035                 0.080                    0.075                   0.113               0.081                    0.018
 Student on the bubble for Passing                    (.060)               (.090)                   (.053)                  (.147)               (.191)                  (.064)

 On the AYP Margin                                     0.056               -0.013                   0.034                   0.002                0.146                   -0.059
                                                      (.039)                (.047)                  (.039)                  (.078)               (.101)                  (.044)
Notes to Table 7: Each column displays estimates from three student-level models using data from the Early Childhood Longitudinal Survey-Kindergarten Cohort (ECLS). Panel
I displays estimates of the coefficient on whether the school was on the AYP margin in the relevant subject: math for math test performance or enjoyment, reading for reading test
performance or enjoyment, and either math or reading for science test performance or anxiety about standardized tests. To decompose the first panel results by the type of
subgroup(s) that were on the AYP margin, Panel II's models use three mutually exclusive indicators that sum to “On the AYP Margin” variable. Panel III's models use the same
independent variable as in Panel I, but add an interaction term with a dummy variable for whether the student is on the bubble for passing the state's NCLB exam in the relevant
subject; this dummy variable also enters the model separately and its creation is described in the text of Section 5. All models control for the variables listed in the "ECLS sample"
column of Table 2, plus state fixed effects, an indicator for whether the school is predicted to be below the margin for making AYP, and squared and cubic terms for the student's
standardized math and reading performance in both the first and third grade waves of the ECLS. Dependent variables are from the fifth grade wave of the ECLS. Sample sizes are
approximately 6,860 students for the first two panels and 5,630 students for Panel III, (rounded to the nearest 10 to comply with data reporting requirements). The smaller sample
for Panel III is due to missing information concerning the difficulty of six states' NCLB exams. The estimates in Panels I and II remain fairly similar if we restrict the sample to
the roughly 5,630 observations used to estimate the models of Panel III. All models weight observations using the student-level longitudinal sample weights provided in the ECLS
data. Bootstrapped standard errors, adjusted for school-level clustering using 1,000 Monte Carlo simulations of both the first-stage and second-stage models, are displayed in
parentheses below each estimate.
** significant at .05 level; * significant at .10 level.
Appendix 1. Sources of Collected AYP Data

                                   Available in                                                  State Abbreviations
                                                        We have
                                     existing                              Not available         Where Data are Not
                                                        collected
                                    databases                                                         Available


States in 2002-2003
School made AYP                         24                  44                   0                         —

                                                                                                   ii
                                                                                     i          AL , IA, ME, NE, NM,
Subgroup made AYP                        5                  38                   9
                                                                                                 ND, OK, WI, WY

Percent proficient by
                                        16                  41                   5               AL, ME, NE, NH, WV
subgroup


Number of students in                    2                  34                  15             AL, CO, DE, HI, ID, IA,
subgroup                                                                                       ME, MS, NE, ND, OH,
                                                                                                 OK, SD, WV, WY


States in 2003-2004
School made AYP                         48                  46                   0                         —

                                        39                  40                   4                 IA, NE, NM, ND
Subgroup made AYP

Percent proficient by                   16                  44                   3                      AL, NE, NH
subgroup

Number of students in                    1                  37                  10                   CO, ID, IA, ME, MS,
subgroup                                                                                             NE, ND, OH, SD, WY
Notes to Appendix 1: Existing databases refer to School Data Direct and the National AYP and Identification Database
Number of states per row can exceed 50 because we collected data in states included in existing databases.
(i) For schools in Arizona, New Jersey, and Pennsylvania, due to otherwise missing data, we impute whether some
subgroups made AYP in 2002-2003 using their 2002-2003 proficiency rates and their states’ published standards.
(ii) Although Alabama did not publish whether student subgroups made AYP in 2002-2003, we can include Alabama schools
in our analyses because Alabama (incorrectly) did not even base schools’ AYP status in 2002-2003 on student subgroup
performance.
Appendix 2: Predicting the Probability of Making AYP

We run state-specific regressions using the data described below to generate predictions of the likelihood
that each numerically-significant student subgroup and (by extension) their school would pass AYP in the
spring of both 2003 and 2004 in the subjects of reading and math. To be as consistent as possible in our
state-by-state predictions of which student subgroups were on the AYP margin, we applied a set of rules
to the construction of data to generate subgroup-level AYP failure predictions. The table on the following
page explains the data construction in detail.

We use a specific subgroup’s 2001-2002 proficiency rate wherever available to predict that subgroup’s
likelihood of making AYP in 2003 and 2004 (note these are cross-sectional measures of a subgroup’s
performance). For privacy protection, the 2001-2002 test score data is typically missing for groups below
a state-determined minimum size (e.g., fewer than 20 students). Thus, for schools where subgroup
enrollment grew between 2001-2002 and 2004, there might be AYP determinations for a subgroup in
2004 but no 2001-2002 proficiency rate. (In the rare case, the 2001-2002 suppression rules redacted data
for groups larger than minimum subgroup size requirements for AYP accountability.) To retain these
cases in our sample, we specified an alternate version of the probit regression, where we assign the
school-wide 2001-2002 proficiency rate to all student subgroups within the school regardless of whether
we possessed subgroup-specific 2001-2002 proficiency rates. In this case, we add an interaction term
with a variable measuring the fraction of the school-wide population composed of students in the relevant
subgroup. We then use predictions from the alternate probit version in cases when predictions were
missing from the main specification.

Sometimes entire subgroups were dropped from probit regressions when there was not any within-
subgroup variation in the subject in the state (e.g., there were only 11 numerically-significant Asian
subgroups in 2004 among Washington’s elementary schools and all 11 passed AYP their math and
reading proficiency targets). In cases where subgroups’ success or failure was perfectly determined, we
overwrote their missing probabilities of making AYP with predicted probabilities obtained from OLS
regressions that used the same set of predictors. This practice was of little consequence, because
subgroups in these cases were always classified as having either low or high likelihoods of making AYP
(they never fall in the moderate category).
                           Model Specification and Data Construction for State Probits Estimating Likelihood of Making AYP in 2003 and 2004

        Variable description                                    Data sources                                                         Variable coding

Dependent variable
Subject-specific subgroup AYP            Wherever available, school report card data from states’       Equals 0 if the subgroup failed its AYP subject-specific proficiency target
proficient indicator                     departments of education listing state’s own determinations    in either 2002-2003 or 2003-2004.
Subjects are math and reading.           of whether student subgroups passed their proficiency          Equals 1 if the subgroup (a) passed its AYP proficiency target in the
                                         targets in the years 2002-2003 and 2003-2004. State’s final    given subject in 2002-2003 and 2003-2004, or (b) passed in one year
Student subgroups are: school-wide;      yes/no determinations typically account for all forms of
African American; Asian/Pacific                                                                         and numerically insignificant in the other year.
                                         adjustment of subgroup raw proficiency rates (e.g., 2- or 3-
Islander; Hispanic; White; Native        year averaging; confidence intervals; safe harbor; and         Equals missing if the subgroup was numerically insignificant in both
American; Limited English Proficient;    appeals).                                                      years (according to the state’s own definition of numerical significance).
Disabled; Economically                                                                                  For states that further break out AYP proficiency targets by grade level
Disadvantaged; Filipino (when used       When not available from state DOE sources, data is from
                                         SchoolDataDirect.org or the National AYP and Identification    or grade span, subgroup indicators are specific to each accountable
by state); Asian (when used by                                                                          grade level/span, using the same rules for creating values of missing,
state); Pacific Islander (when used by   Database (for 2003-2004 only).
                                                                                                        zero, or one.
state); and Alaskan Native (when         In two states which lacked 2002-2003 proficiency target
used by state).                          data from all three sources of data, we constructed the        Two states did not use subgroup-level pass rates to determine schools’
                                         variable using each state’s published raw subgroup             AYP status in 2002-2003. In each case, only 2004 subgroup-level AYP
                                         proficiency rates, which we adjusted using the state’s         proficiency target data was used to construct the dependent variable.
                                         documented confidence interval methods (if applicable) to      Two states only published whether the subgroup passed AYP in each
                                         determine whether each subgroup passed, failed, or was         subject overall (a measure that includes both the subgroup’s proficiency
                                         not applicable. This approximation method had greater than     rate and its participation rate for that subject). In these cases, we used
                                         90% accuracy when tested in two populous states with           this overall subject measure in lieu of proficiency-only indicators.
                                         complete data.
Independent variables

Subgroup test performance in             National Longitudinal School-Level State Assessment Score      When available, we use the subgroup’s unadjusted 5th grade proficiency
focal subject in 2001-2002               Database                                                       rate on the statewide test administered in 2001-2002 for the focal
                                                                                                        subject. (We selected grade 5 because our second stage of analysis
(entered into model as linear,                                                                          examines ECLS student outcomes in 2003-2004, when the majority of
squared, and cubed terms)                                                                               ECLS students are fifth graders.)

                                                                                                        For states not reporting performance for particular subgroups, we use
                                                                                                        the overall student performance in the focal subject in the selected grade
                                                                                                        level in that school. As described in the text, we supplement those
                                                                                                        models with interaction terms between the test performance variable and
                                                                                                        the fraction of students who are members of that subgroup.

                                                                                                        For 6 states where proficiency rates are unavailable, we instead use the
                                                                                                        reported percentile rank scores or scale scores.
                           Model Specification and Data Construction for State Probits Estimating Likelihood of Making AYP in 2003 and 2004

        Variable description                                    Data sources                                                       Variable coding
                                                                                                      For states that did not test grade 5 in 2001-2002, we use the next closest
                                                                                                      lower tested grade level (i.e., grade 4, grade 3) or, if that is unavailable,
                                                                                                      the next closest higher tested grade (i.e., grade 6, grade 7). The models
                                                                                                      then include observations for all schools in that state with test
                                                                                                      performance variables in the relevant grade levels. When these models
                                                                                                      include test performance from two different grade levels (e.g., 4th and
                                                                                                      6th), we also include a dichotomous dummy variable indicating whether
                                                                                                      the test variable values come from students in the higher grade.

                                                                                                      In states that further break out subgroups’ AYP proficiency targets by
                                                                                                      grade levels or grade spans, we run separate models for each high-
                                                                                                      stakes grade for schools serving 5th graders. Depending on availability,
                                                                                                      we use 2001-2002 test performance variables from either the same
                                                                                                      grade, the next lowest grade, or the next highest grade.


Pct. that the student subgroup            National Longitudinal School-Level State Assessment Score   Equals 1 when the subgroup’s own proficiency rate available from 2001-
comprised of the denominator for          Database                                                    2002. Otherwise, ranges from 0 to 1, and is equal to the ratio of enrolled
its 2001-2002 proficiency rate                                                                        students in the given subgroup in 2001-2002 within the school (from
value                                     Where student subgroup size not present in State            CCD) to the total number of enrolled students in the school. Since data
                                          Assessment Score database, data is from the Common          about the number of LEP students and disabled students is not available
(entered as a main effect, and            Core of Data.                                               at the school level in the CCD, we substituted in 2003-2004 AYP
interacted with the three 2002                                                                        subgroup size ratios for the LEP and disabled subgroups. If this
proficiency rate terms)                                                                               subgroup size data not available in a state for 2003-2004, then we use
                                                                                                      district-level LEP and disabled ratios (applicable to three states).


Size of the student subgroup in           Wherever available, school report card data from state      This variable is derived from the state’s count of continuously enrolled
2003-2004                                 departments of education that list student subgroup size    students per student subgroup accountable under NCLB (note that
                                          (using AYP definitions). Where not available from state     states’ definitions of “continuous enrollment” for the purposes of AYP
(entered as 1/sqrt(size), and this term   sources, then drawn from 2003-2004 data in the National     accountability differ somewhat from state definitions for state
is also interacted with the three 2002    Longitudinal School-Level State Assessment Score            accountability systems or just cross-sectional enrollment counts as of the
proficiency rate terms and the three      Database or the 2003-2004 Common Core of Data.              fall in the school year).
2002 proficiency rate x 2002 pct.
group interaction terms)                                                                              Where state sources are not available, size is estimated using 2004
                                                                                                      State Assessment Score data about number of students tested per
                                                                                                      subgroup. If this source is not available for the state, we used grade-
                                                                                                      specific CCD enrollment data and district-level LEP and disabled ratios
                                                                                                      and applied them to school-by-grade-level membership.
                            Model Specification and Data Construction for State Probits Estimating Likelihood of Making AYP in 2003 and 2004

       Variable description                                  Data sources                                                     Variable coding

Indicators for years held               The same data source used to obtain the dependent         Two dichotomous variables indicating whether the subgroup was only
accountable                             variable.                                                 numerically significant in 2003 (but not 2004) in the focal subject and,
                                                                                                  vice versa, numerically significant in 2004 (but not 2003) in the focal
                                                                                                  subject. The omitted category is the subgroup is numerically significant
                                                                                                  in both 2003 and 2004.


Subgroup indicators                     Constructed                                               A series of dichotomous variables indicating the student subgroup to
                                                                                                  which the observation belongs. The omitted category is the campus-
                                                                                                  wide student group.


School-level characteristics in         Common Core of Data 2001-2002 school-level data           We constructed the racial and economic demographic using total student
2001-2002:                                                                                        membership as the denominator. In cases where categories of school-
(a) percent of students who are                                                                   level data were missing from 2002 state files, the variables were
     black                                                                                        constructed using the next closest year in which those variables were
(b) percent of students who are                                                                   present in CCD files (2000-2001, then 2002-2003, then 1999-2000, etc.)
     Hispanic
(c) percent of students who are
     Asian
(d) percent of students who qualify
     for a free- or reduced-price
     meal
(e) whether the school is Title I
     eligible
 (f) total student membership
