                               NBER WORKING PAPER SERIES




     CONSISTENT EVIDENCE ON DURATION DEPENDENCE OF PRICE CHANGES

                                       Fernando E. Alvarez
                                       Katarína Borovicková
                                          Robert Shimer

                                        Working Paper 29112
                                http://www.nber.org/papers/w29112


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                      July 2021




We are grateful to David Argente, Jaroslav Borovicka, Stephane Bonhomme, Luca Dedola, Mark Gertler,
Simon Gilchrist, Yuriy Gorodnichenko, Lars Hansen, Herve Le Bihan, John Leahy, Simon Mongey,
Emi Nakamura, Diego Perez, Edouard Schaal, Alex Torgovitsky, Ivan Werning and participants at
Berkeley, NYU, and ECB seminars for comments and suggestions. We thank the excellent research
assistance of Aleksei Oskolkov and Olivia Bordeu Gazmuri. This material is based, in part, on work
supported by the National Science Foundation under grant number SES-1559459 and SES-1559225.
An earlier version of this paper circulated under the title "The Proportional Hazard Model: Estimation
and Testing using Price Change and Labor Market Data". The views expressed herein are those of
the authors and do not necessarily reflect the views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2021 by Fernando E. Alvarez, Katarína Borovicková, and Robert Shimer. All rights reserved. Short
sections of text, not to exceed two paragraphs, may be quoted without explicit permission provided
that full credit, including © notice, is given to the source.
Consistent Evidence on Duration Dependence of Price Changes
Fernando E. Alvarez, Katarína Borovicková, and Robert Shimer
NBER Working Paper No. 29112
July 2021
JEL No. C14,C41,E31,E50

                                            ABSTRACT

We develop an estimator and tests of a discrete time mixed proportional hazard (MPH) model of duration
with unobserved heterogeneity. We allow for competing risks, observable characteristics, and censoring,
and we use linear GMM, making estimation and inference straightforward. With repeated spell data,
our estimator is consistent and robust to the unknown shape of the frailty distribution. We apply our
estimator to the duration of price spells in weekly store data from IRI. We find substantial unobserved
heterogeneity, accounting for a large fraction of the decrease in the Kaplan-Meier hazard with elapsed
duration. Still, we show that the estimated baseline hazard rate is decreasing and a homogeneous firm
model can accurately capture the response of the economy to a monetary policy shock even if there
is significant strategic complementarity in pricing. Using competing risks and spell-specific observable
characteristics, we separately estimate the model for regular and temporary price changes and find
that the MPH structure describes regular price changes better than temporary ones.


Fernando E. Alvarez                                Robert Shimer
University of Chicago                              Department of Economics
Department of Economics                            University of Chicago
1126 East 59th Street                              1126 East 59th Street
Chicago, IL 60637                                  Chicago, IL 60637
and NBER                                           and NBER
f-alvarez1@uchicago.edu                            shimer@uchicago.edu

Katarína Borovicková
New York University
19 W 4th Street, 6th floor
New York, NY 10012
kb103@nyu.edu
1     Introduction
This paper makes two contributions. First, we develop a linear generalized method of mo-
ments (GMM) estimator for a discrete time mixed proportional hazard (MPH) model for
duration data, including an extension to an environment with competing risks. Second, we
apply our methodology to data from two large panels for the duration of price spells. We
then use the estimated model to re-examine three aspects of sticky price models commonly
explored in macroeconomics: the shape of the hazard of price changes as a diagnostic device
for distinguishing between different structural models; how the interaction between individ-
ual firms' price setting decision affects aggregate price stickiness; and the different behavior
of temporary sales versus regular price changes.

Methodological Contribution. The first contribution of this paper is to develop an
estimator for a discrete time MPH model for duration data. Here we briefly describe our
benchmark MPH model, for concreteness in terms of the elapsed time between price changes
of products at the retailer level. We assume that the probability that a price changes t
periods after the last price change, conditional on not having changed earlier, is equal to bt .
The frailty parameter  is specific to a particular product (defined as a bar code and retailer
in our empirical application), and is fixed over time. We refer to  as the product type and
assume throughout that it is unobserved. The value of bt is the baseline hazard at duration
t. It is common across products, but can vary arbitrarily with the elapsed time since the
last price change. The MPH model assumes that, conditional on the product type , the
duration of any two spells is independently and identically distributed. Thus the model is
completely specified by a value of bt for each duration t and a distribution G for the frailty
parameter . We are interested in estimating bt as well as some measure of heterogeneity as
captured by G.
    Our estimator builds on continuous time identification results from Honor´        e (1993). We
first extend his identification results to discrete time, and then turn those into a set of moment
conditions, linear in bt , which allow us to estimate the baseline hazard using linear GMM.
Our estimator is consistent, it is robust to the shape of the unknown frailty distribution, it
allows that the MPH structure only holds on some interval of durations, and it handles both
left- and right-censored data. We then extend the set-up and results to an environment with
competing risks and observable characteristics, where the MPH structure may only hold for
a subset of risks and observables.
    We also estimate the Kaplan-Meier hazard Ht , the probability that a typical spell ends
in period t conditional on not having ended earlier. We prove that Ht = bt E[|t] where E[|t]


                                                1
is the mean frailty parameter among the spells that have had a constant price until duration
t. We prove that in the model without competing risks, the average type E[|t] must be
decreasing with duration t, and use the degree to which it is decreasing as a measure of the
importance of heterogeneity.
    Our estimator of the baseline hazard is consistent in panel data sets that are large in
the cross-sectional dimension, so long as we observe at least two (possibly right-censored)
price spells for a positive fraction of products. That is, our inference relies on limits as
the number of products goes to infinity, but allows for a boundedly short time dimension.
Estimation and inference are simple and quick, even with large data sets, since we use linear
GMM. In realistic cases, the model is over-identified, and so we use the Hansen-Sargan J
test to explore the assumptions of the MPH model. We also test the prediction in the model
without competing risks that the average type E[|t] is decreasing in duration t.
    Cox (1972) and Lancaster (1979) pioneered the analysis of the MPH model in continu-
ous time; see also Lancaster (1990, Chapter 4). The main contributions in terms of non-
parametric identification using single spell data and observable covariates are Elbers and
Ridder (1982) and Heckman and Singer (1984). Heckman and Honor´           e (1989) extend this to
a competing risks framework. The main contribution on non-parametric identification using
repeated spells is Honor´ e (1993). Abbring and van den Berg (2003) extend this to handle
competing risks.
    The bulk of the literature estimates the continuous time MPH model using maximum
likelihood, either with continuous records or with records that are time aggregated. We
believe that there are several advantages to our approach. We impose no restrictions on the
frailty distribution, while maximum likelihood requires specifying its family, e.g. a gamma
distribution. Our estimator is linear in the baseline hazard, which makes estimation fast and
inference straightfoward. In contrast, the likelihood function can be difficult to maximize,
particularly when the frailty distribution is a mixture of gammas or when there are competing
risks. Finally, our approach to the competing risks model allows the MPH structure to hold
only for some risks, with an unspecified hazard rate for other risks. In contrast, maximizing
the likelihood requires a parameterized hazard rate (typically an MPH with known frailty
family) for all risks.
    Horowitz and Lee (2004) build on Honor´     e's identification argument to construct an esti-
mator for two-spell continuous time data with continuous records. Their estimator, like ours,
does not require specification of the frailty distribution. Still, there are again several advan-
tages to our estimator. It is linear, and hence simple to implement and conduct inference.
It allows us to easily use all spells, while they only provide an explicit formula using the first
two spells. It handles competing risks. It imposes no restrictions on the joint distribution


                                                2
of the unobserved type and the time a product is in the sample, while Horowitz and Lee
(2004) impose they are independent. Finally, our estimator is formulated for data measured
in discrete time, which is the usual format of duration data sets. If the data generating
process were in continuous time and continuous records were available, Horowitz and Lee's
approach would be consistent and our method would generally not be.

Application to Price Setting. The second contribution of our paper is applying our
estimator of the MPH model to real-world data, and then using the estimates to improve
our understanding of macroeconomic models with sticky prices. For this we use the IRI
weekly store data, which record weekly revenue and weekly quantities for each product
(store and UPC code). This is a large data set, covering 30 categories of mostly packaged
products, e.g. razor blades, coffee, beer, and frozen pizza. We define the price as the ratio of
revenue to quantity, and a price spell as the time between two price changes for a product.
After cleaning, our data contains more than 21 million products. We also explore the Online
Micro Price Data from Cavallo (2018) which, while much smaller in size (250,000 products),
has daily frequencies and arguably much less measurement error.
    We use these data sets to ask three substantive questions. First, we examine the shape
of the baseline hazard rate as a diagnostic device for different structural models. As is well
known, heterogeneity across products always pushes the Kaplan-Meier hazard rate down
with duration due to dynamic selection. For this reason, we concentrate on the shape of
baseline hazard rate. State dependent models with persistent cost shocks (Golosov and
Lucas, 2007) imply that the hazard rate of price changes is increasing in the time since the
last price change for any particular product. In contrast, time dependent models impose no
restriction on duration dependence in the baseline hazard, although it is often assumed that
the hazard of price changes is constant for each product (Calvo, 1983).
    In the IRI data, we find a decreasing baseline hazard rate, despite uncovering a substan-
tial amount of heterogeneity. Figure 1 shows that the baseline hazard rate bt is generally
decreasing between durations 2 and 60 weeks, except for a noticeable spike near one year's
duration. The Kaplan-Meier hazard Ht is much steeper throughout the entire time period.
As a result, the average type E[|t] = Ht /bt , which we normalize to 1 at the start of a spell,
declines sharply to 0.4 during the first 20 weeks. It then keeps declining at a slower pace,
reaching 0.3 after one year. The pattern for the baseline hazard is common in most product
categories, and the one for the average type holds for essentially all categories.
    We find a similar pattern using the daily Online Micro Price Data, with one important
exception: we uncover a sharp spike in the hazard each week, suggesting that many prices
only change on a particular day of the week. This justifies our analysis of a discrete time


                                               3
model, where the time period of one week corresponds to the timing of price change decisions.
     The second substantive question we examine is how the price setting decisions of hetero-
geneous firms are affected by and affect the path of prices following a monetary policy shock.
We explore a time-dependent pricing model, where the distribution of the duration of price
spells is exogenous and described by our estimated MPH model. When firms adjust their
price, they do so in order to maximize the expected discounted deviation from a moving
target, which in turn depends on both economic fundamentals and the average price set by
other firms. Because of the dependence on other firms' prices, the pricing decisions exhibit
strategic complementarities, which is known to increase aggregate price stickiness in this
type of environment.
     We first find the impulse response of the price level to a monetary policy shock using our
estimated MPH model. We then compare this with two artificial economies which share the
same Kaplan-Meier hazard. In one, all firms are homogeneous and the common hazard rate
of each is the Kaplan-Meier hazard. In the other, all firms set prices for a fixed duration as
in Taylor (1979, 1980), with a population distribution of durations that achieves the same
Kaplan-Meier hazard. If there is no strategic complementarity in pricing, we show that
all three economies have the same impulse response (see also Carvalho and Schwartzman,
2015). With strategic complementarity, the exact aggregation result does not hold, and so
the relationship between the three impulse responses is a quantitative question. We find that
the price level initially adjusts most quickly when firms are homogeneous and least quickly
when prices are set for a fixed duration. The response of the price level after about two
years is the reverse, largest when prices are set for a fixed duration and smallest when firms
are homogeneous. The response in our estimated model lies in between, but is closer to the
one in the homogeneous firm economy. We therefore conclude that researchers can safely
estimate the Kaplan-Meier hazard and assume that all firms are homogeneous and have this
hazard rate. On the other hand, assuming all firms are homogeneous with a constant hazard
rate (Calvo, 1983) would give the wrong shape to the impulse response.
     The third question we examine is how to distinguish between regular and temporary price
changes. We use the extended framework with observable characteristics and competing risks
to estimate separately four different baseline hazard rate functions, depending on whether
the spell starts and ends with a price increase or decrease, effectively introducing a statistical
filter for sales and other temporary price changes. We think, for example, of sales as being
a subset of price decreases followed by price increases. Relative to the existing literature
(Nakamura and Steinsson, 2008), our approach to regular price changes is consistent with
the statistical models used for estimation and testing, and allows for the full generality
of the MPH model presented above. When applied to the IRI data, the baseline hazard


                                                4
function for price increases followed by subsequent price increases is much flatter than the
one for price decreases followed by price increases. That is, regular price increases have
a much flatter baseline hazard rate than the one for sales, consistent with the price plan
model in Eichenbaum, Jaimovich, and Rebelo (2011), whose hazard rate Alvarez and Lippi
(2020) analyze. A caveat is that the data appears to be much more consistent with an MPH
structure for regular price increases as compared with sales, as evidenced by the results of
the J test. For this reason, it is reassuring that our econometric approach gives consistent
estimates of the baseline hazard for regular price increases, even if sales do not have an MPH
structure.

Related Literature. Our application of duration data to price spells builds on the seminal
work of Bils and Klenow (2004). Nakamura and Steinsson (2008) and Fougere, Le Bihan, and
Sevestre (2007) offer the most thorough analyses of the shape of the baseline hazard in the
presence of unobserved heterogeneity for price changes. Nakamura and Steinsson (2008) use
both CPI and PPI data for the US, and Fougere, Le Bihan, and Sevestre (2007) use CPI data
for France. Both papers use maximum likelihood to estimate the parameters of a continuous
time duration model with a parametric frailty distribution. Nakamura and Steinsson (2008)
assume the monthly and bimonthly data comes from continuous time records, while Fougere,
Le Bihan, and Sevestre (2007) correct for time aggregation.
    The Nakamura and Steinsson (2008) approach and ours yield qualitatively similar results
on our data set, although there are some significant quantitative differences. To see this, we
maximize the likelihood function for a continuous time model with continuous time records
using the IRI data set. This estimator recovers less heterogeneity than our GMM estimator.
For instance, the average type is estimated to decrease from E[|t] = 1 during the first
week to E[|t] = 0.37 at six months using the GMM estimates. Instead it decreases to only
E[|t] = 0.48 using the maximum likelihood estimates. Equivalently, the baseline hazard
rate estimated using GMM is flatter than the one estimated using maximum likelihood with
continuous records. We also compare our results with those we obtain by maximizing the
likelihood for time-aggregated records. We conclude that in our data set, time aggregation
is quantitatively important but the shape of the frailty distribution and whether we assume
time is continuous or discrete is less so.
    The closest paper in terms of application, if not results, is Fougere, Le Bihan, and Sevestre
(2007). They estimate the baseline hazard for almost 400 product categories at a similar
level of aggregation to ours. They find very little evidence of unobserved heterogeneity
within these categories. They test whether the baseline hazard is constant and fail to reject
this hypothesis in more than half of the categories. These results contrast with ours. We


                                               5
hypothesize that one reason is the lower frequency of their data (their price data is gathered
monthly while ours is gathered weekly). Another more important reason is that they have
many fewer price spells per category, roughly three orders of magnitude fewer than ours.
Hence, they have less power to reject the null hypothesis of a constant baseline hazard.
Fougere, Le Bihan, and Sevestre (2007) also specify and estimate a competing risks model
where a price spells can end with a price increase or a price decrease. They estimate this
model without unobserved heterogeneity and conclude that this extension barely affects the
shape of the baseline hazard, a result that is again very different than ours. Finally, they note
that their estimator did not converge for the competing risks model with unobserved het-
erogeneity. Since our GMM estimator is linear in the baseline hazard even in the competing
risks extension, this is not an issue for us.
    Our three substantive questions are related to a variety of papers, and we turn to those
next. The first is about the shape of the hazard rate in structural price setting models. The
simplest model of price adjustment is Calvo (1983), where the hazard of price adjustment
is a constant function of duration. If this probability differs across firms, as in Carvalho
(2006), the model has an MPH structure with a constant baseline hazard and a decreasing
Kaplan-Meier hazard. Another large class of models assumes state-dependent prices, e.g.
Golosov and Lucas (2007). In a canonical version of this model, the desired price follows
a stochastic process, exogenous to any firm, and a firm can adjust its price at any time by
paying a fixed cost. Under some regularity conditions, a firm optimally adjusts its price
when the difference between the current and desired price is too high, and the hazard of
price changes is increasing in duration.
    In most cases, adding heterogeneity across firms into a structural model of price setting
will typically not lead to an MPH representation. For example, Nakamura and Steinsson
(2010) use a model with heterogenous firms following a combination of time and state-
dependent price rules to investigate the degree of monetary non-neutrality. In this case, the
resulting statistical model will not be exactly an MPH model. Still, we have found that in
quantitative versions of these models, the shape of the baseline hazard recovered using an
MPH model resembles the "typical" hazard rate for the original model.
    The second substantive question we analyze is the effect of monetary policy in an environ-
ment with time-dependent price setting rules. Caballero (1989), Reis (2006), and Alvarez,
Lippi, and Paciello (2011) analyze models of firms' price setting based on costly information
gathering. In an environment with neither strategic complementarity nor strategic substi-
tutability in price setting, they show that optimal price setting rules are pure time-dependent.
Alvarez, Lippi, and Paciello (2011) show how these decision rules apply to a once and for
all monetary shock. Carvalho (2006) and Nakamura and Steinsson (2010) use models where


                                               6
firms follow time-dependent and state-dependent price setting rules, respectively, to evaluate
real effects of monetary policy in presence of heterogeneity. Both papers find that not taking
heterogeneity into account leads to underestimation of the real impact of monetary policy.
Carvalho and Schwartzman (2015) obtain an analytical characterization of the cumulative
impulse response of the aggregate price level to a monetary shock in the presence of hetero-
geneity for a general time-dependent pricing models. Our analysis further generalizes this
to a case not previously considered in the literature mentioned above, where price setting
exhibits strategic complementarity.
    The third substantive question is related to price setting models of price plans, as intro-
duced by Eichenbaum, Jaimovich, and Rebelo (2011) and further analyzed by Alvarez and
Lippi (2020). In these models, a firm can adjust costlessly between a set of prices constitut-
ing a "price plan," but it has to either pay a fixed cost or just wait for a free adjustment
opportunity to switch its price plan. A firm that follow a price plan will show frequent rever-
sal of prices, since changes within the plan are assumed costless. As a consequence, in these
models the hazard of changing the price may be decreasing; see Section F of the Appendix
of Alvarez and Lippi (2020). In particular, when a price plan containing two prices can be
modified with probability  in each period, the hazard of changing the price is 1/(2t) +  at
duration t.
    This paper proceeds as follows. In Section 2, we describe the discrete time MPH model,
prove it is nonparametrically identified using repeated spell data, and define the Kaplan-
Meier hazard. In Section 3, we discuss measurement issues, especially left- and right-
censoring. We then present our estimators for the baseline and Kaplan-Meier hazards. Sec-
tion 4 extends the framework to allow for observable characteristics and competing risks. We
then discuss our data sets in Section 5 before turning to our applied results. In Section 6 we
present our estimates of the baseline hazard in both data sets, and then use the estimated
model to analyze the aggregate implications of microeconomic heterogeneity for a monetary
policy shock. Section 7 distinguishes temporary and regular price changes, showing that the
baseline hazard is much flatter for regular price increases compared to the end of sales, and
that moreover only the former fits the MPH structure. Section 8 compares our results with
those we obtain using other estimation techniques, especially maximum likelihood estimation
of a continuous time model with continuous records. Finally, we conclude in Section 9.




                                              7
2     Discrete Time MPH
2.1    Model
We consider a continuum of products. Each product has a fixed type  with cumulative
distribution function G(), also known as the frailty distribution. The fixed type may be
correlated with some observable individual characteristics, but we are interested in cases
where the econometrician does not observe  perfectly. For expositional simplicity, we focus
on the case where the econometrician does not observe any individual characteristics.
    Time is discrete and the amount of time between price changes is a random variable
taking values in the positive integers, 1, 2, . . . . We call this elapsed time the spell length.
The MPH model specifies that conditional on a spell length at least equal to t, the probability
that the length is exactly t, i.e. the hazard at duration t, is the product of two components,
the product's type  and the baseline hazard bt , which is common to all products.
    We assume the baseline hazard bt is strictly positive for at least one value of t. We also
assume that the frailty distribution G() has a bounded support [L , H ] with H bt  1 and
0  L bt < 1 for all t = 1, 2, . . . . We also allow that different types of products are observed
with different probability and let  () > 0 denote the weight on observations for type 
products. In our application, we will specify this to be the frequency weight (number of
spells per unit of time) for a type  product. It will be convenient to define the weighted
frailty distribution
                                                  
                                                 L
                                                     ( )dG( )
                                      G(| )  H                  .                             (1)
                                                 L
                                                     ( )dG( )
   The sequence of baseline hazards {b1 , b2 . . . }, the frailty distribution G, and weights 
determine the distribution of spell lengths in the population. The duration distribution can
be described by its cumulative distribution function, or equivalently, by its survival function

                                          H    t
                              t ( )                 (1 - bs )dG(| ),                         (2)
                                         L    s=0


where for notational convenience we define b0 = 0. This is the fraction of spells that last
strictly more than t periods.
    Only the product of the baseline hazard bt and the type  enters the survival function.
This implies that we can multiply the baseline hazard at all durations by a positive multi-
plicative constant and divide the type of each product by the same constant without affecting
the probability of any outcome. In what follows, we therefore identify the baseline hazard
up to a multiplicative constant.


                                                   8
2.2     Identification with Multi-Spell Data
We now show that the model is non-parametrically identified with data on two spells. To
do this, we first define the two-spell survival function:

                                    H      t1                       t2
                    t1 ,t2 ( )                  (1 - bs )                 (1 - bs ) dG(| ).                (3)
                                   L      s=0                       s=0


This is the  -weighted probability that first spell length is greater than t1 and the second
spell length is great than t2 given our model structure. It uses the assumption that the
length of the two spells is independent conditional on the product type .
    In this section, we think of the one- and two-spell survival functions as something we
can observe in the data,1 and ask how we can use them to recover the baseline hazard
b  {b1 , b2 , . . . } (up to the aforementioned multiplicative constant) and the weighted frailty
distribution G(·| ). Our proof is an adaptation of the identification result of Honor´  e (1993)
to the discrete time model.

Proposition 1 For an arbitrary weight  , the baseline hazard b is identified up to a multi-
plicative constant using the two-spell survival function t1 ,t2 ( ). Given b, the frailty distri-
bution G(·| ) is identified using the one-spell survivor function t ( ).

Proof. We first show how to identify the baseline hazard and then show how to identify the
frailty distribution.

Baseline Hazard. The definition of the two-spell survival function in equation (3) implies

                                                 H      t1 -1                   t2 -1
       t1 -1,t2 -1 ( ) - t1 ,t2 -1 ( ) = bt1                      (1 - bs )            (1 - bs ) dG(| ),
                                                L        s=0                     s=0
                                                 H      t1 -1                   t2 -1
       t1 -1,t2 -1 ( ) - t1 -1,t2 ( ) = bt2                       (1 - bs )            (1 - bs ) dG(| ).
                                                L           s=0                  s=0


It follows immediately that

               bt2 t1 -1,t2 -1 ( ) - t1 ,t2 -1 ( ) = bt1 t1 -1,t2 -1 ( ) - t1 -1,t2 ( ) .

By varying t1 and t2 , we can recover b up to a multiplicative constant.
   1
     If the duration distribution is defective, limt t ( )  0, there is a positive probability that we would
not observe a second spell. Here we ignore that issue by assuming t1 ,t2 ( ) is known, but our estimator in
Section 3 handles defective duration distributions.



                                                        9
                                                 
Frailty Distribution. Let µk  L       H
                                        k dG(| ) denote the k th moment of the frailty dis-
tribution G(·| ). It exists since the distribution is bounded. Once we know the baseline
hazard b up to a multiplicative constant, the model implies that the probability that the
completed duration of a spell is t is t-1 ( ) - t ( ), a known function of µk , k = 1, . . . , t:

                                          H      t-1                          t-1
          t-1 ( ) - t ( ) = bt                         (1 - bs )dG(| ) = bt         k (t - 1; b)µk+1 ,   (4)
                                         L       s=0                          k=0


where for all t, k  1, the coefficients k (t; b) are defined recursively as follows:
                              
                               1                                              if k = 0
                              
                    k (t; b) = 0                                              if k > t                   (5)
                              
                               (t - 1; b) - b  (t - 1; b)
                              
                                                                              if t  k > 1
                                 k           t k -1


We know b. Setting t = 1 in equation (4) gives us an equation for µ1 . Having found
µ1 , . . . , µk-1 , setting t = k in equation (4) gives us an equation for µk . Thus by induction
we can find all the moments µk of G(·| ). Since the support of G(·| ) is a bounded interval
[L , H ], its moments uniquely determine distribution G(·| ).

    Proposition 1 is behind our approach to estimation, where we convert this logic into
moment conditions for the case where we have measures of the survival function from a
finite sample.


2.3    Kaplan-Meier Hazard
The Kaplan-Meier hazard is the probability that the spell length is exactly t conditional on
it being at least t, but not otherwise conditional on the product's type:
                                                                H     t-1
                           t-1 ( ) - t ( )                      L
                                                                      s=0 (1 - bs )dG( | )
                   Ht ( )                  = bt                 H    t-1
                                                                                           .             (6)
                              t-1 ( )
                                                                L    s=0 (1 - bs )dG( | )


This is the baseline hazard bt times the average type among those products with spell length
              H     t-1
              L     s=0 (1-bs )dG( | )
at least t,   H    t-1
                                         . This gives a clear decomposition of the evolution of the
              L    s=0 (1-bs )dG( | )

Kaplan-Meier hazard Ht ( ) into the component explained by structural duration depen-
dence, captured through the baseline hazard bt , and the component explained by dynamic
selection of heterogeneous products, captured through changes in the average type over time.
    An implication of the MPH model is that the average type declines with duration:


                                                           10
Proposition 2 Assume bt > 0 for all t. For any weights  , the ratio of the Kaplan-Meier
hazard to the baseline hazard, Ht ( )/bt , is strictly decreasing in t.

Proof. We let Gt (| ) be the distribution of  among those products whose duration is at
least t,
                                         t-1
                                     L   s=0 (1 -  bs ) dG( | )
                        Gt (| )  H t-1                            .
                                    L     s=0 (1 -  b s ) dG( | )
Consider the double ratio of the densities at 1 < 2 and t1 < t2 :

                                      t1 -1                   t2 -1                 2  t -1
         dGt1 (2 | ) dGt2 (1 | )      s=0     (1 - 2 bs )     s=0     (1 - 1 bs )      1 - 1 bs
                                 =    t2 -1                   t1 -1               =             ,
         dGt2 (2 | ) dGt1 (1 | )      s=0     (1 - 2 bs )     s=0     (1 - 1 bs ) s=t1 1 - 2 bs

                                                 t2 -1 1-1 bs
Since 1 < 2 , 1 - 1 bs > 1 - 2 bs and so         s=t1 1-2 bs    > 1. That is,

                                   dGt1 (2 | )   dGt1 (1 | )
                                               >             .                                       (7)
                                   dGt2 (2 | )   dGt2 (1 | )

This in turn implies that Gt1 first order stochastically dominates Gt2 , Gt1 (| ) < Gt2 (| )
for all   (L , H ). To prove this, suppose to the contrary that there exists a   (L , H )
with Gt1 (| )  Gt2 (| ). Since these are distribution functions, it follows that

                                                          H                        H
                dGt1 ( | )        dGt2 ( | ) and              dGt1 ( | )               dGt2 ( | ),
            L                 L                                                 


and in particular that there exists a 1  [L , ] and a 2  (, H ] such that

                   dGt1 (1 | )  dGt2 (1 | ) and dGt1 (2 | )  dGt2 (2 | ).

That contradicts equation (7).
   Since Gt1 first order stochastically dominates Gt2 , the expected value of  is higher under
the former distribution than the latter,
                 H      t1 -1                          H         t2 -1
                 L
                        s=0 (1 - bs )dG( | )           L
                                                                 s=0 (1 - bs )dG( | )
                 H     t1 -1
                                                  >    H        t2 -1
                                                                                      .
                 L     s=0 (1 - bs )dG( | )            L        s=0 (1 - bs )dG( | )


Using equation (6), this implies Ht1 ( )/bt1 > Ht2 ( )/bt2 .

   This result reflects dynamic sorting and is intuitive: products with a higher type have a
higher chance of changing their price early and thus exit the pool of surviving products. As
duration increases, products with lower type disproportionately remain. Lancaster (1979)

                                                  11
discusses the same point in a related continuous-time setup.


3     Estimation and Testing
In this section, we turn the identification result in Proposition 1 into moment conditions
for the baseline hazard. We work in a more general environment, where there is an MPH
                                                     ¯ }, 1  T < T
data generating process for durations t  {T, . . . , T            ¯ < , but not necessarily
                                             ¯               ¯
for durations outside this interval. More precisely, let ht () be the hazard at duration
                                                             ¯}, we assume ht () = bt , but
t = 1, 2, . . . for a product with type . For t  {T, . . . , T
                                                       ¯
we allow for arbitrary ht ()  [0, 1] outside this interval. For notational convenience, let
h0 () = 0 for all .
    Our estimator works with right-censored duration data and allows for a defective dura-
tion distribution. It recognizes that some products may have only a single spell, but uses
information from all available spells. This is important in our empirical application.
    After we show how to estimate the baseline hazard, we turn to estimation of the Kaplan-
Meier hazard rate, the average hazard rate among all products with spells lasting t periods.
Here we seek an estimator that does not rely on the MPH structure at all, but simply uses
stationarity assumptions on the data generating process.


3.1    Censoring and Measurement
We start by introducing left- and right-censoring into the duration model. A product i is
described by a type i and a censoring time ci , possibly correlated. We continue to let G()
denote the frailty distribution and G(| ) denote the frailty distribution weighted by  , as in
equation (1). We let Qc () be the cumulative distribution of censoring times conditional on
type, with density qc ()  Qc () - Qc-1 (), allowing for an arbitrary correlation structure.
The product type affects the true duration of spells through the hazard function ht (i ), while
the censoring time is equal to the number of (consecutive) periods during which we observe
the product.
    For results concerning the Kaplan-Meier hazard, we make an additional assumption on
the duration of the in-progress spell when we first observe the product: it is a random draw
from the stationary ergodic duration distribution for that product. More precisely, let   ~ t ()
denote the probability that a type  product last changed its price t periods ago in the
stationary ergodic duration distribution. We measure this immediately after firms change
their price, so ~ 0 () is the probability a firm changes its price. For arbitrary t  0, this




                                              12
                                                             
satisfies ~ t+1 () = (1 - ht+1 ())~
                                  t (). Using                t=0   ~ t () = 1, this implies

                                                       t
                                                       s=0 (1 - hs ( ))
                                      ~ t () =             t
                                                                            .                                     (8)
                                                   t =0    s=0 (1 - hs ( ))


If the initial duration for a type  product is a random draw with density                     ~ t (), we call this
                                      2
a stationary mixture model.
    If we observed product i for infinitely long, we would see a vector of completed durations
                                                                                                       ¯i
  i
 = {0    i   i
           , 1           i              ¯i
                         ¯ i }, where K is either a non-negative integer or infinite and
               , . . . , K                                                                            K    i       3
                                                                                                      j =0 j = .
But we do not observe any product for infinitely long. The censoring time ci affects the
measured duration of spells, which we denote by  i = {0             i
                                                                      , 1i           i
                                                                           , . . . , K i }. In particular, since

the censoring time c is finite, we only observe a finite number of spells, K i  K
                             i                                                                        ¯ i.
    To define K i and  i , it is useful to first define the residual censoring time after the start
of each spell, ci               i   i
                     j with c0 = c . The initial spell j = 0 may be in progress when we first observe
                                                                                          i      i
the product, and hence its measured duration may be left-censored, 0                           0   . Subsequently
we define the residual censoring time for the j spell as cj = cj -1 - j -1 . The j th spell is
                                                         th            i         i          i

uncensored if its completed duration is less than the residual censoring time, ji  ci                         j ; in
                                               i     i      i  i
this case, the measured duration is j = j . If j > cj  0, the measured duration is right
                   i
censored at j          = ci             i                       i
                           j + 1  j ; additionally, we set K = j .
                                                                         4
                                                                              We do not observe anything
                           i                                        i
about spells j > K and they are not part of the vector  .
    We assume that the baseline hazard b0 = {b0,T , . . . , b0,T    ¯ } is nontrivial, by which we mean
                                                           ¯
b0 = 0, and let T0 be the smallest t  T with b0,t > 0. We then impose the following rank
                                                 ¯
condition:
                                   ¯ with G-positive probability.
Assumption 1 K  2, 1 = T0 , and 2  T

This ensures that we have enough data to compare bT0 to bT
                                                         ¯ . It holds if and only if


                              H                         ¯-1
                                                        T
                                   1 - QT0 +T
                                            ¯-1 ( )            (1 - ht ())dG() > 0.
                             L                           t=1


This requires that there is a positive measure of product types  with (i) a positive probability
    2
        The stationary ergodic distribution is defined for a type  product if and only if the expected duration
of a spell is finite, since this is a necessary and sufficient condition for the denominator in equation (8) to be
finite. We assume that this is the case when we examine the Kaplan-Meier hazard.
      3                                                                     ¯ i will be almost surely be finite with
        If the duration distribution is defective, s=1 (1 - hs (i )) > 0, K
K i                       ¯ i
  ¯ i = . Otherwise K is almost surely infinite.
      4
        The addition of one period to the measured duration of the right-censored spell captures the fact that
we know the spell lasts strictly longer than ci                                   i
                                                   j periods and ensures that K i is a tight lower bound on the
                                                                             K i -1 i             Ki
duration of the final spell. These definitions imply that if ci
                                                              1 > 0,         j =1 j      ci
                                                                                          1 and
                                                                                                       i
                                                                                                  j =1 j   > ci
                                                                                                              1 , with
  i        i    i
K = 1 if 1 > c1 .


                                                        13
                                           ¯ and (ii) a positive probability of a spell lasting at
of a censoring time at least equal to T0 + T
      ¯ periods. If either of these assumptions were violated, no product would have K  2
least T
with 1 = T0 and 2  T    ¯. In particular, the first assumption ensures that we can observe the
product long enough to have K = 2, with spell 0 ending in the first period, spell 1 ending
T0 periods later, and spell 2 lasting at least until T¯.
    When this combination of assumptions determines the distribution of measured duration
 and the rank assumption 1 is satisfied, we say that  is drawn from an MPH model with
baseline hazard b0 . We turn next to a consistent estimate of the baseline hazard when
measured duration  is drawn from an MPH model with baseline hazard b.


3.2    Moment Conditions for the Baseline Hazard
In Section 2, we argued that for any durations t1 , t2 and any weighting function  , the model
implies
                               i        i                i       i
                       bt2 Pr[1  = t1 , 2  t2 ] = bt1 Pr[1  t1 , 2 = t2 ].

If we observed two completed spells per product, it would be straightforward to turn this
result into a moment condition:

                             E bt2 11
                                    i =t , i t - bt 1 i t , i =t
                                        1 2   2    1 1   1 2    2
                                                                  = 0,

since expected value of the indicator function is the probability of the corresponding event.
                                                               i      i
Censoring affects our ability to use such conditions since 1     and 2  are not always observed.
                                                        i
For example, if there is a positive probability that 1    > ci
                                                             1  t1 , then there is no function of
data that is equivalent to 11 i t , i =t . To see why, note that the first spell is right censored
                                 1 2    2
                        i     i                                                            i
for any product with 1 > c1  t1 , and so this record does not depend at all on 2             and in
                                             i
particular does not depend on whether 2 = t2 .
    We use two key observations to circumvent this. Consider a product where we observe
at least two spells, K i  2. First, if ci      1  t1 + t2 , we can evaluate whether the event
  i        i
1   = t1 , 2  t2 occurred using objects we observe. This is because we see the product for
long enough to tell if the first spell lasts exactly t1 periods; and if it does, we see it long
enough to tell if the second spell lasts at least t2 periods. The second is that the model
                                                                              i          i
implies probabilities are symmetric across the two spells, so the events 1        t1 , 2   = t2 and
  i        i
1   = t2 , 2  t1 are equally likely. While we cannot evaluate an indicator function for the
first event, we can evaluate one for the second event for all individuals with ci      1  t1 + t2 .




                                                14
This motivates the following moment condition:

                          E bt2 1K i 2,1
                                       i =t , i t - bt 1K i 2, i =t , i t
                                           1 2   2    1       1    2 2   1
                                                                           = 0.

Our main result formalizes these observations and shows how to develop a moment condition
that uses information from two arbitrary spells, not just the first two spells:

Proposition 3 Assume  = (0 , 1 , . . . , K ) is drawn from an MPH model with baseline
hazard b0 . Define

                                                  bt2 1j =t1 ,k t2 - bt1 1j =t2 ,k t1 .
                   [b ]
                  ft1 ,t2 ( ; b)                                                            (9)
                                    (j,k):1j<kK


        [b]                                   ¯ if and only if b = b0 for some number .
Then E ft1 ,t2 ( ; b) = 0 for all T  t1 < t2  T
                                  ¯
We postpone the proof of this Proposition, since we can obtain it as a special case of Propo-
sition 5 below. See Appendix A for the proof of that proposition and the explanation for
why Proposition 3 is a special case.
    We use Proposition 3 to build a GMM estimator of b0 for some strictly positive . Let
T =T   ¯ - T . We have T (T + 1)/2 moment conditions of the form E ft[b,t    ]
                                                                                ( ; b0 ) = 0 for
             ¯                                                              1 2
                           ¯
some T  t1 < t2  T , each linear in the T + 1 vector b0 . The basic idea of GMM is
       ¯
to replace the expected value with the sample mean, so we have the moment condition
1    I    [b]     i
I    i=1 ft1 ,t2 ( ; b) = 0. We estimate b0 by minimizing the quadratic form of the error
in the moment conditions, weighted by a positive-definite matrix W . The "if" part of
Proposition 3 gives us the necessary condition for this estimator to be consistent, while the
"only if" part gives us sufficiency. We discuss further details of the GMM estimator, including
standard errors and clustering, in Appendix B. Here we highlight one important feature of
our approach: since the moment conditions in equation (9) are linear in the baseline hazard,
we obtain the GMM estimator of b0 in closed form.
    The multiplicative structure of the MPH model is restrictive and can be tested using
a J -test of overidentifying restrictions. In particular, Proposition 3 gives us T (T + 1)/2
moment conditions to estimate T parameters in the vector b0 . For T  2, the model is thus
overidentified.
    We can also build on the proof of Proposition 1 to find moment conditions for the moments
of the type distribution. Unfortunately, unless we impose that the proportional hazard
structure holds at the shortest duration, T = 1, and that censoring time ci      1 and type 
                                                                                               i
                                             ¯
are independent, these conditions are difficult to interpret. We therefore relegate them to
Appendix C.4 and do not report them in our main analysis.

                                                   15
3.3     Moment Conditions for the Kaplan-Meier Hazard
Proposition 2 tells us that the ratio of the Kaplan-Meier hazard to the baseline hazard,
Ht ( )/bt , is decreasing in t for any weighting function  . Generalizing equation (6) to the
case of an arbitrary type-dependent hazard function ht (), the Kaplan-Meier hazard rate at
duration t is
                                      H          -1
                                     L
                                        ht () t s=0 (1 - hs ( ))dG( | )
                          Ht ( )        H    t-1
                                                                        .                (10)
                                        L    s=0 (1 - h s ( ))dG(  | )
We seek to test this prediction, but first must describe our choice of  and how we estimate
Ht ( ).
    Anticipating our theoretical model of price setting described in Section 6.3, we would like
to measure the hazard rate for a typical spell, rather than a typical product. In a stationary
mixture model, defined in Section 3.1, a product changes its price with a fixed probability
per unit of time. We thus want weights   equal to the expected number of times that a
type  product changes its price during the observation window:
                                                       
                                                       c=1 cqc ( )
                                     ()                t                  .                           (11)
                                                t=0    s=0 (1 -  h s ( ))

The numerator is the expected censoring time for a type  product. We multiply this by the
expected number of price changes per unit of time for that product type,        ~ 0 () defined in
              5
equation (8).
    Unfortunately, censoring limits our ability to estimate Ht (  ) for t  {T, . . . , T   ¯}. In
                                                                                   ¯
particular, if c  T¯, we cannot observe a spell j  1 which lasts T      ¯ periods.6 This means
that we cannot estimate the Kaplan-Meier hazard at duration T     ¯ for such products. Since we
want to estimate the Kaplan-Meier hazard at all durations {T, . . . , T  ¯} across a fixed set of
                                                                 ¯
products, we must focus attention on the subset of productst that we observe for more than
T¯ periods. That is, we thus seek to estimate Ht ( f ), where the feasible frequency weight  f
satisfies                                     
                                                 ¯   cqc ()
                                ()   c=tT +1
                                f
                                                                .                             (12)
                                          t=0  s=0 (1 - hs ( ))
                                                                    ¯ periods, but is otherwise
This gives zero weight to any product that we observe for less than T
the same as   ().
   We now give a consistent estimator of the Kaplan-Meier hazard H ( f ) = (HT ( f ), . . . , HT  f
                                                                                               ¯ ( )):
                                                                                           ¯

   5
     The denominator is also equal to the expected duration of a price spell for a type  product.
   6
     Recall all observations start with an ongoing spell j = 0 of unknown duration. Thus ci  T¯ implies the
                                                           i   i               i   ¯
residual censoring time at the start of spell 1 satisfies c1  c - 1 and hence c1 < T .



                                                      16
Proposition 4 Assume  = (0 , 1 , . . . , K ) is drawn from a stationary mixture model with
Kaplan-Meier hazard H0 ( f ). Define
                                       
                     [H ]
                                           c
                                          c-T¯
                                                  K
                                                  j =1   Ht 1j t,cj T
                                                                    ¯ - 1j =t,cj T
                                                                                 ¯          ¯
                                                                                     if c > T
                    ft,T
                       ¯    ( ; H )                                                                         (13)
                                       0                                                   ¯
                                                                                     if c  T

                                                                       [H ]                          ¯
where c = K   j =0 j - 1 and cj =
                                                 K
                                                 j =j   j - 1. Then E ft,T
                                                                         ¯ ( ; H ) = 0 for all T  t  T
                                                                                               ¯
if and only if H = H0 ( f ).
                                                                                                 ¯i
    We prove this result in Appendix A. The basic idea is that                       1
                                                                                     I
                                                                                         I
                                                                                         i=1
                                                                                                K
                                                                                                j =1   1ji t and
              ¯i
1
I
      I
      i=1
             K
             j =1   1ji =t are consistent (but infeasible) estimates of the survivor function and den-
sity at duration t. For t  T      ¯ and ci  T  ¯, the indicator functions are equivalent to ones
                                          j
                               i
using measured duration j        instead of completed duration ji , suggesting a path towards a
feasible estimator. The only difficulty is that this underweights products with short censor-
ing times ci . We prove that when durations are drawn from a stationary mixture model, the
                          i
multiplicative factor cic
                        -T  ¯ is the correct reweighting.
    We test monotonicity of the ratio Ht ( f )/bt by looking at the inequalities

                                                                                  ¯ - 1.
              log Ht ( f ) - log bt - log Ht+1 ( f ) - log bt+1  0 t = T, . . . , T
                                                                       ¯

This gives us T inequalities, which we test jointly using Chen and Szroeder (2009).


4      Observables and Competing Risks
We now consider two extensions to our basic model, allowing for (possibly spell-specific)
observable characteristics which affect the hazard, and permitting competing risks for why a
spell ends. In our empirical application, the observable characteristics include the product's
category and whether the spell starts with a price increase or decrease; and the competing
risk is a spell ending with a price increase or decrease.


4.1         Setup
We assume that each product is characterized by an unobserved type vector  with popula-
tion distribution G( ). In addition, we assume that each product and spell has an observable
characteristic, say j  {1, . . . , X } for the j th spell. We allow for correlation between ob-
servables and unobservables, as discussed below.



                                                            17
    Both the observed and unobserved characteristics affect the joint distribution of the
duration of a spell and the reason why the spell ends. We let hr           t (,  )  0 denote the
probability that a spell with observable  and unobservable  ends at duration t  {1, 2, . . . }
for reason r  {1, . . . , R} conditional on not ending earlier.  captures all observables that
affect the hazard, and so in particular conditioning on past observables is not useful for
forecasting the hazard.7 Let ht (,  )  R          r
                                             r=1 ht (,  ) denote the probability of a duration t
spell with observable  and unobservable  ending in period t. We assume that ht (,  )  1
for all  , so this is a proper probability, and ht (,  )dG( ) < 1, so there is a chance of
observing spells with observable  at any duration.
    The initial (left-censored) observable characteristic 0 is a random variable. Let 0 (x| ) 
0 denote the probability that 0 = x given  , with X            x=1 0 (x| ) = 1 for all  . There-
after, the observable characteristic follows a first order Markov process. For j  1, let
 (x|j -1 , j -1 ,  )  0 denote the probability that j = x conditional on the observable
characteristic of the previous spell j -1  {1, . . . , X }, the reason the previous spell ended
j -1  {1, . . . , R}, and the unobserved type, with X      x=1  (x|j -1 , j -1 ,  ) = 1 for all j -1 ,
j -1 , and  .8 This structure is rich enough to allow for an arbitrary relationship between
observable and unobservable characteristics.
    For at least one observable characteristic x, reason r, and set of durations {T, . . . , T    ¯},
                                                                                          ¯
we assume that there is a proportional hazard representation, hr          t (x,  ) = ( )bt for all
T tT      ¯, where ( ) is a scalar function of the unobserved type vector  and bt  0 for all
¯
               ¯}. We focus throughout on this pair (x, r) and seek to estimate b = {bT , . . . , bT
t  {T, . . . , T                                                                                   ¯}
      ¯                                                                                   ¯
up to a multiplicative constant.
    We do not impose any restrictions on hr    t (x ,  ) for (x , r ) = (x, r ). However, we allow
for the possibility that multiple hazards have a proportional hazard representation, with
potentially different scaling functions  and different baseline hazards b. In this case, we can
jointly estimate all the baseline hazards. We note, however, that even if all the hazards have
a proportional hazard representation, the hazard of a spell with characteristic x ending for
any reason, ht (x,  ), generally does not have a proportional hazard representation. Thus we
may reject the MPH model but not fail to reject this more general specification.
    7                     ^ r (1 , . . . , j ,  ) denote the probability that a spell with current and lagged observables
       To be precise, let h t
1 , . . . , j and unobservable  ends at duration t  {1, 2, . . . } for reason r  {1, . . . , R} conditional on not
ending earlier. Then we assume hr                         ^r
                                              t (j ,  ) = ht (1 , . . . , j ,  ). One can view this as a definition of the
observable state j .
     8
       We assume a Markovian structure for notational simplicity, but can easily relax this assumption. A
substantive assumption is that the duration of one spell does not directly affect the duration of later spells,
i.e. we assume that there is no lagged duration dependence. We do not know how to relax this assumption
without imposing additional structure elsewhere in the model. Still, we allow for the possibility that the
reason one spell ends can influence the duration of the next spell. This is important in our empirical
application.


                                                           18
4.2    Identification
To understand identification, it is useful to consider an environment with left-censoring but
no right-censoring. Consider a product where spell 0 has observable characteristic x0 and
ends for reason r0 . We compute the conditional probability that the first uncensored spell has
observable characteristic x and ends for reason r at duration t1 , while the second uncensored
spell also has characteristic x and lasts at least t2 periods. We assume T  t1 < t2  T   ¯ and
                                                                           ¯
so use hrt1 (x,  ) = ( )bt1 :



  Pr[1 = t1 , 2  t2 , 1 = r, 1 = 2 = x|0 = x0 , 0 = r0 ]
                 H                                       t1 -1                     t2 -1
         = bt1       ( ) (x|x0 , r0 ,  ) (x|x, r,  )             (1 - hs (x,  ))           (1 - hs (x,  )) dG( ).
                 L                                       s=1                       s=1


Reversing the role of t1 and t2 gives

  Pr[1 = t2 , 2  t1 , 1 = r, 1 = 2 = x|0 = x0 , 0 = r0 ]
                 H                                       t2 -1                     t1 -1
         = bt2       ( ) (x|x0 , r0 ,  ) (x|x, r,  )             (1 - hs (x,  ))           (1 - hs (x,  )) dG( ).
                 L                                       s=1                       s=1


Combining these two we get

  bt2 Pr[1 = t1 , 2  t2 , 1 = r, 1 = 2 = x|0 = x0 , 0 = r0 ]
                           - bt1 Pr[1 = t2 , 2  t1 , 1 = r, 1 = 2 = x|0 = x0 , 0 = r0 ] = 0.

Since this equation holds for any (x0 , r0 ), it is true when we integrate across that left-censored
spell distribution. This gives us a moment condition:

                 E [bt2 11 =t1 ,2 t2 ,1 =r,1 =2 =x - bt1 11 =t2 ,2 t1 ,1 =r,1 =2 =x ] = 0.

We can then vary t1 and t2 to recover b up to a multiplicative constant. This generalizes the
identification argument in Proposition 1 to a framework with observable characteristics and
competing risks.
   We cannot use this moment condition for GMM in our setting because we have censored
data. Moreover, it does not make use of available data after the end of the second spell. The
remainder of this section shows how to adapt this insight to our framework, building on the
approach in Section 3.



                                                    19
4.3     Censoring and Measurement
As in the MPH model, we assume that we observe product i for ci periods, where the
censoring time ci may be correlated with the product's type  . We still let Qc ( ) denote the
cumulative distribution of censoring times conditional on type  and G( ) denote the frailty
distribution.
    As before, we let  i = (0           i   i
                                          , 1           i
                                              , . . . , K i ) be the vector of measured durations, with the
                                                                                          i
first spell left censored and the last spell right censored, so ci = K                       i
                                                                                       j =0 j - 1. We also let
i = (i          i           i
          0 , 1 , . . . , K i ) be a vector recording the observable characteristic of each spell and
i = (i        i           i
         0 , 1 , . . . , K i -1 ) be a vector recording the risk that ended each spell. Since the last
spell is right-censored, we do not observe why it ended, and hence  is of length K i rather
than K i + 1.
    We assume that the baseline hazard b0 = {b0,T , . . . , b0,T             ¯ } is nontrivial, b0 = 0, and let
                                                                    ¯
T0 denote the smallest t  T with b0,t > 0. We then generalize the rank condition in
                                       ¯
Assumption 1 to the environment with observable characteristics and competing risks:

Assumption 2 With positive probability, there exists a 1  j < k  K with j = T0 ,
k  T¯, j = r, and j = k = x.

This guarantees that we have variation in the data to compare bT       ¯ to bT0 . It holds, for

example, if there is a positive probability that the left-censored spell has observable x0 and
ends for reason r0 for some (x0 , r0 ), and

             H                                                    ¯-1
                                                                  T
                 1 - QT0 +T
                          ¯-1 ( )  (x|x0 , r0 ,  ) (x|x, r,  )           (1 - ht (x,  ))dG( ) > 0,
            L                                                      t=1


so there is a positive probability that the censoring time is at least T0 + T¯, spells 1 and 2
both have observable x, and the second spell lasts at least T¯ periods.
    We highlight two special cases in which this reduces to the rank condition in Assump-
tion 1. First, the observable distribution may have full support for each of the first two
spells for all r and  . This is the case in our empirical analysis when x measures whether
the spell starts with a price increase or decrease and r measures whether it ends with a price
increase or decrease. Second, the observable may be fully persistent and 0 (x| ) > 0. This is
the case in the empirical analysis when the observable characteristic measures the product's
category. Of course, combinations of these assumptions are consistent with Assumption 1
as well, e.g. we can observe both the product's category and whether the spell starts with a
price increase or decrease.
    When this set of assumptions determine the joint distribution of ( , , ) and the rank


                                                      20
condition holds, we say that ( , , ) is drawn from a competing-risk model with baseline
hazard b0 for observable characteristic x and risk r.


4.4       Moment Conditions
We now show how to estimate the baseline hazard, extending the approach in Proposition 3:

Proposition 5 Assume ( , , ) is drawn from a right-censored competing-risk model with
baseline hazard b0 for observable characteristic x and risk r. Define

                                          btk 1j =t1 ,k t2 ,j =r,j =k =x - btj 1j =t2 ,k t1 ,j =r,j =k =x .
     [b,x,r]
    ft1 ,t2 ( , , ; b) 
                            (j,k):1j<kK
                                                                                         (14)
                [b,x,r]                                     ¯
Then E         ft1 ,t2 ( , , ; b)   = 0 for all T  t1 < t2  T if and only if b = b0 for some
                                                ¯
number .

The proof is in Appendix A.


5       Data
For most of our empirical applications, we use IRI weekly store data,9 described in Bron-
nenberg, Kruger, and Mela (2008). We also use Online Micro Price Data


5.1       Construction of Price Spells
The IRI data set contains weekly revenue and quantity sold for a large number of products
in chain grocery and drug stores for years 2001­2011. The data cover 30 large product cat-
egories, 10 such as coffee, carbonated beverages, and detergents, and include approximately
2.6 million distinct items defined by its store and barcode (Universal Product Code, UPC).
    We use revenue and quantity sold to compute the average weekly price for each product.
We turn data on price levels into data on price spells by first computing the price change from
the prior week and then defining a price spell as the time elapsed between two price changes.
In particular, suppose that the first price observation occurs at time t0 , the last one at time
tK +1 and that price changes occur at times t1 , . . . , tK . Then we construct j = tj +1 - tj for
j = 0, . . . , K - 1, and K = tK +1 - tK + 1, reflecting the fact that the earliest possible date
    9
       All estimates and analyses in this paper based on Information Resources Inc. data are by the authors
and not by Information Resources Inc..
    10
       There are 31 product categories in IRI but we exclude cigarettes from our analysis because their price
is regulated.


                                                     21
when the price set at tK can change is tK +1 + 1 and the hence the price spell will be at least
K periods long. Spell 0 has measured duration 0 and is left-censored. The censoring time
is c = tK +1 - t0 .
    We use price levels to determine whether the price spell follows a price decrease or
increase; we can determine this for every price spell except the left-censored ones. We
further use prices to determine whether a price spell ends with a price increase or decrease;
we can construct such indicator for every spell which is not right-censored. We use this
information to estimate the model with observable characteristics and competing risks.
    Missing observations are prevalent. For example, if the product has not been sold in
a given week, the store does not report quantity or revenue. We address this problem by
selecting the longest period with no missing observations for a given product and use only
this interval to construct price spells.
    We work with average weekly prices, which brings in two issues for the spell construction.
First, some changes in the average weekly price are due to the fact that some customers shop
with coupons, which we cannot directly observe. We impose a lower bound on the price
change of 0.1 percent to exclude such price changes. Second, if the product's price changes
in the middle of a week, it generates a spurious spell of duration one week.11 We therefore
set T = 2 and do not estimate the baseline hazard in week 1.
     ¯
    Table 1 shows summary statistics of the price spells by product category, focusing on price
spells longer than T = 2 weeks (which are not left-censored). The pooled sample contains
                    ¯
21,717,549 products, yielding 684,919,778 pairs of durations where both durations exceed T .
                                                                                             ¯
It is the total number of pairs of durations that enters into the sums in Propositions 3 and 5.


5.2                     ¯
        Choice of T and T
                  ¯
Since we observe average weekly prices, price changes occurring in the middle of the week
generate spurious price spells with duration one week. We think of these as coming from
a different model, possibly without an MPH structure, and so drop them to avoid biasing
estimates of the baseline hazard at all durations. At the same time, we want to choose the
lowest possible value for T . Therefore, we set T = 2.
                          ¯                     ¯
    We provide further justification for this choice. An implication of any mixture model
where each product has two independent spell durations from its type-specific distribution,
and of the MPH model in particular, is that the autocorrelation of the duration of two
completed spells is non-negative, and strictly positive when there is heterogeneity in mean
  11
    For example, suppose that the price of a product increases from $1 to $2 in the middle of a week. Then
we would measure average price of $1 in week 1, $1.5 in week 2 and $2 in week 3, which looks like as if there
were two price changes.


                                                     22
                                                                              i
                          number of products with    number of percentiles of j
                           K~i  1      K~i  2           pairs   50th       90th
        Yoghurt           1,402,766     1,155,766    98,999,368    3         10
        Carb. Beverage    1,819,607     1,321,762    90,836,025    3          8
        Salty Snack       2,481,250     1,670,539    72,485,278    3          9
        Frozen Dinner     2,272,888     1,693,017    70,495,598    3          8
        Cold Cereal       1,429,028     1,038,096    56,080,465    4         12
        Beer                701,604       470,815    37,454,496    3         11
        Milk                549,261       426,316    34,036,391    4         14
        Soup              1,286,921       897,080    33,873,770    4         14
        Spaghetti Sauce     501,088       353,379    25,015,292    3         11
        Frozen Pizza        711,065       519,293    24,984,150    3          8
        Margarine           244,844       204,293    23,833,374    4         13
        Hot Dog             213,598       172,031    19,603,427    3          9
        Coffee              793,004       455,555    13,969,362    3         10
        Toilet Tissue       412,746       312,604    10,791,034    3         11
        Laundry Det.        804,837       489,482     9,993,575    3          9
        Facial Tissue       250,134       185,450     9,557,189    3         11
        Peanut Butter       203,380       150,692     9,255,148    4         13
        Mayonnaise          186,392       136,585     7,992,048    4         14
        Mus. & Ketchup      217,559       143,485     7,659,886    4         16
        Paper Towel         340,032       252,339     6,939,886    3         13
        HH Cleaners         413,061       232,276     5,959,387    4         11
        Toothpaste          716,457       322,194     4,615,305    3          8
        Shampoo           1,134,428       352,570     2,483,449    3          7
        Diapers             602,164       247,864     1,918,554    3          7
        Sugar Sub.           94,528        56,644     1,818,682    4         17
        Deodorant           972,970       291,558     1,633,620    3          6
        Toothbrush          512,729       178,488     1,097,352    3          7
        Blades              297,314       114,407     1,076,134    3         10
        Photo                65,503        28,187       358,959    3          8
        Razors               86,391        26,001       102,574    2          6


Table 1: Descriptive statistics by product category, IRI data. For this table, we consider
only spells ji
                T = 2 and use K    ~ i to denote the number of such spells for the product
                 ¯
i. The first column shows the number of products with at least one spell longer than T .
                                                                                        ¯
The second column reports the number of products with at least two such spells. The third
column reports the number of pairs where both are longer than T . The last two columns
                                                                  ¯
show the median and 90th percentile value of the censored spell length.




                                           23
duration. To understand why, note that conditional on a product's type, the autocorrelation
of duration is zero by assumption. But with heterogeneity, the autocorrelation captures
differences in the type-specific means and is necessarily positive.
    Inspired by this, we measure the autocorrelation of the duration of price spells in the
data. If we use all price spells, including one-week spells, we find a correlation of 0.029 when
duration is measured in levels, and -0.042 when duration is measured in logs. This suggests
that the data are unlikely to come from a mixture model. But once we exclude spells lasting
one week, the correlation increases to 0.235 in levels and 0.233 when measured in logs. These
correlations increase a bit further to 0.248 in levels and 0.256 in logs when we consider spells
at least 3 weeks long. That is, once we exclude one-week spells, the correlation is different
from zero, and it is fairly stable if we exclude two-week spells. This is behind our decision
to set T = 2. We examine the robustness of our results to this assumption in Appendix D.3.
        ¯
    The choice of T ¯ is guided by the nature of the data and our need to balance two forces.
On the one hand, we want to choose a large value for T     ¯ to learn about the baseline hazard
at long durations. At the same time, the number of spells longer than T      ¯ decreases quickly
with T ¯. Indeed, Table 1 shows that depending on the product category, the median spell
duration is 2­4 weeks and the 90th percentile varies between 6 and 17 weeks. This means
that data are thin at durations longer than half a year. While this does not constitute a
problem for estimating the baseline hazard--smaller sample size will be reflected in larger
standard errors--the choice of T    ¯ affects our estimates of the Kaplan-Meier hazard at all
durations because we condition on c  T      ¯. Balancing these forces, we choose T ¯ = 60 weeks,
a little over a year, because there is an interesting pattern in the hazard at 52 weeks. Figure
9 in Appendix D.3 shows estimates beyond 60 weeks. The estimates are noisy but follow the
same trend from before T   ¯ = 60 so our main results are for T ¯ = 60.


5.3       Online Micro Price Data
We are also interested in looking at higher frequencies than are available in the IRI data.
To do this, we use the daily Online Micro Price data, the open access data from the Billion
Prices Project presented by Cavallo (2018).12 In this data set, we observe daily posted prices
for many products, which we use to construct price spells. We focus on the U.S. stores. We
impute some missing observations. If the last non-missing price before missing observations
is the same as the first non-missing price after the missing observations, we impute this price
to the missing observations. If these two prices are not the same, we do not impute any
price. After this imputation, we consider the longest window without missing data for each
  12
       http://www.thebillionpricesproject.com/datasets/



                                                    24
product in the sample, as we did in the IRI data. The resulting sample contains 71,925
products with at least one spell, K i  1, out of which 48,550 products have at least two
spells, K i  2. Since we observe the posted price directly, we do not need to exclude price
                                                                 ¯ = 70, that is, 10 weeks.
spells of length 1. For this data, we therefore choose T = 1 and T
                                                       ¯


6        Results
We start this section by presenting our estimates of the baseline and Kaplan-Meier hazards
for the MPH model. We first analyze our main data set with weekly price observations
before turning to a data set with daily price data to explore whether time aggregation
affects our results. Finally, we develop a simple theoretical framework in which we explore
how heterogeneity and duration dependence interact to create real effects from monetary
policy shocks.


6.1      Baseline Hazard and Heterogeneity
Here we present estimates for the basic model with X = 1, R = 1 on the pooled sample; see
Figure 1. Figures 14 and 15 in Appendix F show results for each product category separately.
    The left panel of Figure 1 shows the Kaplan-Meier and baseline hazards. The Kaplan-
Meier hazard13 is steeply declining throughout the whole investigated range, except for the
peak at 52 weeks. In contrast, the baseline hazard is constant until week 4, after which it
modestly declines, also showing a peak at 52 weeks.
    The distinction between the Kaplan-Meier and baseline hazard points to substantial
unobserved heterogeneity. Recall from equation (6) that the ratio of the Kaplan-Meier
hazard to the baseline hazard is the average type, which captures the extent of dynamic
sorting. A flat average type suggests that there is little dynamic sorting and hence little
heterogeneity, while a steeply declining average type suggests a lot of heterogeneity. The
right panel of Figure 1 shows that the average type is steeply declining. Within 20 weeks, the
average type drops by 60 percent and it continues to decline thereafter, albeit more slowly.
This means that heterogeneity plays an important role in shaping the Kaplan-Meier hazard.
    We now turn to the two tests of the model. Recall there are T = T  ¯ - T = 58 independent
                                                                           ¯
moments and M = 1,770 moment conditions. The J -statistic is J = 10,498, while the critical
value of the 2 distribution with M - T degrees of freedom is 1,749, implying that we reject
the model at any conventional significance level. In what follows, we investigate the source
    13
     To estimate the Kaplan-Meier hazard, we need an assumption that the data come from a stationary
mixture model. In Appendix D.1, we propose a test of the stationarity assumption and find that the data
look close to stationary.


                                                  25
                         Hazard Rates                                          Average Type
         0.32                                                        1
                                                                    0.8
         0.16




                                                     average type
hazard




         0.08
                                                                    0.4

         0.04        baseline
                     Kaplan-Meier
         0.02                                                       0.2
                10      20       30   40   50   60                        10   20       30   40   50   60
                             t in weeks                                             t in weeks


Figure 1: Kaplan-Meier and baseline hazard for pooled IRI data, log scale. The purple line
shows the Kaplan-Meier hazard, the blue line is the estimated baseline hazard. The red
line shows the "average type" at given duration, calculated as the ratio of Kaplan-Meier
and baseline hazards. Shaded regions show two standard error bands. Standard errors are
clustered at the store × product category level. The baseline hazard is normalized to equal
the Kaplan-Meier hazard at duration 2 weeks.

of this rejection in more detail, but note here that we have 21 million products and a model
that is significantly over-identified. Perhaps it is not surprising that we fail the J -test in
such a situation.
    Our second test is whether the average type is decreasing. The right panel of Figure 1
shows a declining trend through durations 2 to 60 weeks, but a formal test rejects the null
hypothesis due to the very tight standard errors.
    Our conclusion is that the baseline hazard is declining, although much less so than the
Kaplan-Meier hazard, suggesting the presence of substantial unobserved heterogeneity. We
find evidence of a mild spike in the baseline hazard at week 52, suggestive of certain time-
dependent pricing rules (Taylor, 1979, 1980). Still, all of these results are tempered by the
fact that the model fails the overidentifying test as well as the test for dynamic sorting.
In Section 7, we investigate whether this failure can be driven by the fact that sales follow
different dynamics than regular price changes, which we do not distinguish in this section.


6.2         Higher Frequency Data
We study the price data through the lens of a discrete time model and naturally wonder if
the frequency of the data affects our results. To explore this, we repeat our analysis using
daily Online Micro Price data.

                                                26
                           Daily Hazard                                   Daily Average Type

            0.256
                                                                    0.8
            0.128




                                                     average type
   hazard



            0.064
            0.032                                                   0.4
            0.016
            0.008
            0.004                                                   0.2
                         20         40    60                               20         40      60
                              t in days                                         t in days
                    baseline, daily       Kaplan-Meier, daily                   average type, daily
                    baseline, weekly      Kaplan-Meier, weekly                  average type, daily


Figure 2: Kaplan-Meier and baseline hazard for Online Micro Price Data using daily and
weekly data, log scale. Solid lines use daily data, dashed lines weekly data. The purple
line shows the Kaplan-Meier hazard, the blue line is the estimated baseline hazard. The
red line shows the "average type" at given duration, calculated as the ratio of Kaplan-Meier
and baseline hazard. Shaded regions show two standard error bands. The baseline hazard
is normalized to be equal to the Kaplan-Meier hazard at duration 1 day in the daily data,
or 1 week in the weekly data. Kaplan-Meier hazard and baseline hazard are in daily units
throughout.

    Figure 2 shows the estimates using daily data for T = 1 day and T       ¯ = 70 days (solid
                                                         ¯
lines). We observe that the price-change hazard spikes every seventh day. This suggests that
even though the data are daily, the decision to change prices is taken at the weekly frequency
and hence a week might be a natural time unit.
    To see how much information we gain from using daily data, we aggregate the data to
weekly frequency. That is, any spell lasting 1­7 days is recorded as duration of 1 week,
spells lasting 8­14 days as duration of 2 weeks, etc. We then estimate the model again. The
dashed lines in Figure 2 show the results, with hazards adjusted to have the same (daily)
units. We find that the estimates are similar, even though weekly data recover somewhat
less heterogeneity that the daily data.
    In Appendix D.2, we conduct a similar exercise with the IRI data, where we aggregate
weekly data up to a monthly frequency. We find that the monthly data understate the
extent of heterogeneity, even more so than in Online Micro Price Data (Figure 8). We think
that this is because in Online Micro Price Data, large spikes in the hazard every seven days
indicate that the decision to change prices is made at the weekly frequency, and hence using

                                                27
weekly instead of daily data does not make significant difference. With the IRI data, the
week is the relevant unit for measurement, and so monthly data miss selection which happens
within a month, leading to an understatement of the extent of heterogeneity.


6.3    Aggregate Implication of Firm-Level Heterogeneity
Finally, we discuss the macroeconomic implications of our estimates using a model of het-
erogeneous firms with time-dependent pricing rules. We are interested in understanding the
dynamic response of the price level to a monetary policy shock. In a richer model with price
rigidity, this would translate into the impact on output.
    We consider a discrete time economy populated by heterogeneous firms which use time-
dependent pricing rules. We assume firms are described by their type  with population
distribution G and support [L , H ]. t () is the survival function for type  firms, the
probability that a type  firm keeps the same price for more than t periods, with 0 () = 1.
We assume the expected duration of a completed spell is finite for all , which is equivalent
to assuming that       t=0 t ( ) is finite. We also assume that if a type  firm adjusts its price
at t, it sets it to a new log price denoted t ().
    We assume all products are in the stationary ergodic duration distribution. Let          ~ t ()
denote the fraction of type  firms that have kept the same price for t periods, measured
immediately after the price adjustment, so       ~ 0 () is the fraction of the time that a type 
firm adjusts its price. Generalizing equation (8) to the case of arbitrary survival functions
gives
                                                     t ()
                                         ~ t () =             .
                                                    s=0 s ( )

This means that the average log price charged by type  firms at time t is a weighted average
of past values of the new prices t ():
                                                          
                                                          s=0   s ()t-s ()
                        pt ()          ~ s ()t-s () =                      .                  (15)
                                 s=0                            s=0 s ( )


This varies over time if t () is not constant. We turn to its determinants next.
    For all t  0, we assume that all firms set a log price normalized to 0. Thereafter, we
assume that a type  firm, if it is able to adjust its price at t, sets a price to minimize the
discounted squared difference between the actual log price and a frictionless target log price
Pt+s during the time until the next price change:

                                               
                            t ()  arg min            s s ()( - Pt    2
                                                                 +s ) ,
                                           
                                              s=0


                                                  28
where   [0, 1] is the discount factor. The first order condition implies
                                                      s      
                                                 s=0  s ( )Pt+s
                                  t (  ) =              s
                                                                .
                                                   s=0  s ( )


We also assume that for t  1, the frictionless target log price Pt is a weighted average of
the new long-run steady state log price  = 0 and the average log price charged by other
firms Pt , Pt = (1 - ) + Pt , where

                                                 H
                                     Pt              pt ()dG().                             (16)
                                             L


To the extent that the target price is increasing in the average log price set by other firms, we
say there is strategic complementarity in pricing decisions. The parameter  with 0   < 1
captures this strategic complementarity.
   In summary, the log price set by firms at t satisfies
                                                          s
                              (1 - ) +               s=0  s ( )Pt+s
                                                                      if t > 0
                              
                                                            s
                     t () =                            s=0  s ( )                           (17)
                              0
                              
                                                                      if t  0.

We are interested in solving equations (15)­(17) to characterize the evolution of the average
log price Pt under the restriction that Pt is bounded. In Appendix C.1, we prove that there
is a unique such price sequence and find a contraction with modulus  which characterizes
the entire sequence Pt .
    When  = 0, we also obtain a useful aggregation result (see also Carvalho and Schwartz-
man, 2015, Proposition 2 ). We prove in Appendix C.2 that the dynamics of Pt are identical
in a heterogeneous firm economy and in an economy where there is only a single type of firm
with a survival function
                                           H
                                   ¯t 
                                              t ()dG(|  ~ 0 ).                           (18)
                                             L

This is a weighted average of the heterogeneous firms' survival functions, with weights given
by the frequency of price changes. Put differently, it is the survival function associated with
the Kaplan-Meier hazard Ht (  ), as defined in equations (10) and (11). Thus if we can
measure the Kaplan-Meier hazard, we do not need to understand the extent of heterogeneity
in the economy without strategic complementarity. This is a useful benchmark.
    For other values of the complementarity parameter , we do not have a closed-form
solution and the exact aggregation result fails. We therefore solve the model numerically
to investigate the difference between the path of the average log price in a heterogeneous


                                                     29
economy and in other economies with the same Kaplan-Meier hazard rate.
    More precisely, we calibrate the model using the estimated baseline hazard presented in
Section 6.1. See Appendix C.4 for details. We set the weekly discount factor to  = 0.999
(five percent annual discounting) and consider a monetary shock which increases the steady
(log) price from 0 to  = 1. We examine three different values of the complementarity
parameter,  = 0,  = 0.5 and  = 0.95, with the aggregation result not holding in the
latter two economies. We calibrate the distribution of survival functions using our estimated
moments of the frailty distribution. The estimated model does a good job of matching the
Kaplan-Meier hazard (Figure 6 in the Appendix).
    We then consider three different economies with the same Kaplan-Meier hazard. The
first is the estimated MPH model. The second is a homogeneous firm economy, where each
firm's hazard is the Kaplan-Meier hazard. The third is a heterogeneous firm economy where
every firm adjusts its price at fixed intervals (Taylor, 1979, 1980), but different firms have
different intervals so as to match the Kaplan-Meier hazard.14
    Figure 3 displays these results. The left panels show the evolution of the average log
price Pt , while the right panel shows the fraction of the gap between the current average log
price and the asymptotic log price  that is closed in period t, lt  ln( - Pt-1 ) - ln( - Pt ).
We observe that average log price dynamics in the MPH economy is similar to that in the
homogeneous firm economy.15 The average log price is somewhat less similar in the Taylor
economy, especially when strategic complementarities are strong ( = 0.95). For example,
the initial speed of price convergence in the MPH and homogeneous economies is significantly
faster than in the Taylor economy, while after half a year it is noticeably slower.
    At least for this question and this data set, our analysis shows that a researcher can
usefully approximate the true heterogeneous firm economy with a representative firm that
has the empirical Kaplan-Meier hazard. This approximation is useful for two reasons. First,
it may be easier to model a homogeneous firm economy. And second, as we have shown in
Proposition 4, it is possible to estimate the Kaplan-Meier hazard without first estimating a
mixture model.
    This approximate aggregation result, that only the Kaplan-Meier hazard affects aggre-
gate price dynamics, extends Carvalho and Schwartzman (2015), who analyze the role of
heterogeneity for the cumulative impulse response of the aggregate price level to a monetary
  14
      There is a sense in which the latter two economies represent polar cases of heterogeneity. In any
mixture model, the correlation between the duration of any two spells is non-negative. In a homogeneous
firm economy, this correlation is zero, while in the Taylor economy it is 1.
   15
      We use the model-implied Kaplan-Meier hazard for this exercise. An alternative would be to use the es-
timated Kaplan-Meier hazard without first imposing the MPH structure. Since the calibrated model hits the
empirical the Kaplan-Meier hazard almost exactly, the implied paths for the log price are indistinguishable.



                                                    30
                          Mean Log Price Pt                      Speed of Convergence lt
                 1                                     0.2

                0.8
                                                      0.15
                0.6
      =0


                                                       0.1
                0.4
                                                      0.05
                0.2

                 0                                      0
                 1

                0.8
                                                       0.1
       = 0.5




                0.6

                0.4                                   0.05
                0.2

                 0                                      0
                 1

                0.8                                   0.03
       = 0.95




                0.6
                                                      0.02
                0.4
                                                      0.01
                0.2

                 0                                      0
                      0   50       100    150   200          0        50       100    150   200
                               t in weeks                                  t in weeks

                           MPH         homogeneous           Taylor          Calvo


Figure 3: Mean log price Pt and the fraction of the gap closed in period t, lt , in a heteroge-
neous and single-firm economy. The top panels have  = 0 (no strategic complementarity in
pricing), middle have  = 0.5 (weak strategic complementarity), and the bottom panels have
 = 0.95 (strong strategic complementarity). Throughout we assume  = 0.999 per week.
The blue lines show the estimated MPH model. The red dashed line shows the economy
with a single firm with survival function equal to the Kaplan-Meier survival function. The
purple line shows the economy with heterogeneous firms, each with a fixed price duration
(Taylor, 1979, 1980), and the same Kaplan-Meier survival function. The green line shows
a single firm with a constant probability of changing its price (Calvo, 1983), and hence a
different Kaplan-Meier survival function.
                                                 31
shock in the case without strategic complementarity,  = 0. Their analysis implies that
the impulse response of aggregate prices to a one-time monetary shock in an economy with
heterogeneous sectors is the same as the impulse response of a one sector economy with
survival function given by equation (18). We use our estimated model to turn this into an
approximation result in an environment with a strategic complementarity. To be clear, for
arbitrary distributions of hazard rates, this approximation result would not hold.
    In addition to this exercise, we examine whether the path for Pt can be well approximated
by a single firm with a Calvo price setting rule, a common assumption in the literature. We
show in Appendix C.3 that in this case, the average price equals  (1 - xt ) where x depends on
the firm's Calvo parameter as well as the value of the complementarity parameter  and the
discount factor  . This implies a constant speed of convergence lt in the Calvo model. The
time path for the average price is quite far from this exponential structure in our estimated
model, and hence cannot be well-approximated by a single Calvo firm, depicted by the green
lines in Figure 3.


7    Disentangling Regular Price Changes from Sales
Nakamura and Steinsson (2008) show that distinguishing between sales prices and regular
prices has important implications both for the frequency and hazard of price changes. In
particular, sales are more transient than regular price changes and are not typically related to
macroeconomic conditions. Following their pioneering work, most researchers have dropped
all price changes associated with sales from the data set before estimating the hazard of
regular price changes. We are concerned that doing so may affect the estimated stochastic
process for the regular price changes. In our case, this problem is particularly acute, since
we do not observe sales directly, but instead must infer them from the nature of the price
change, e.g. a short-lived low price between two higher prices. Even if one could directly
observe sales prices, dropping a subset of price changes can bias estimates of the hazard for
the remaining price changes, a standard issue in competing risk models.
    Our approach instead allows us to view sales as part of the data, albeit a part that
does not necessarily fit the MPH structure, for example if sales have a fixed duration that
varies across products. We propose circumventing sales by focusing on outcomes--that is,
competing risks--that represent regular price changes, and measuring duration dependence
for only those risks. We focus on price increases following price increases and on price
decreases following price decreases, which we call price trends. Our approach can be used to
look at other risks, e.g. setting a price that has not been observed in the recent past. In a
data set with a reliable sales flag, one could use our competing risks framework to look at

                                              32
price spells that neither start nor end with sales.
     We distinguish spells based on whether they started with a price increase or price de-
crease. Thus we set X = 2 and for mnemonic convenience let i             j = + if the j
                                                                                        th
                                                                                           spell of
product i follows a price increase and i    j = - if it follows a price decrease. We also distin-
guish whether a spell ends with a price increase or decrease, R = 2, and let i        j = + if the
j th spell of product i ends with a price increase and i   j = - if it ends with a price decrease.
                i    i
Spells with j = j represent price trends, while other spells are price reversals.
     We separately estimate four different baseline hazards, one for each possible combination
of x and r. We use b++   t   to denote the baseline hazard that a spell after a price increase
                                                  -
ends with a price increase at duration t; b+    t   the baseline hazard that a spell following a
price increase ends with a price decrease at duration t. Similarly, b-    t
                                                                            +
                                                                              denotes the baseline
hazard that a spell following a price decrease ends with a price increase at duration t and b-- t
denotes the baseline hazard that a spell following a price decrease ends with a price decrease
at duration t. We allow for four different functions determining unobserved heterogeneity,
++ ( ), +- ( ), -+ ( ), and -- ( ).
     This richer model allows for the possibility that price trends have different dynamics than
price reversals. We estimate it using the moment conditions specified in Proposition 5. Figure
4 shows the results and some interesting patterns. The baseline hazards for price trends,
b++
 t    and b--
            t , are rather flat, especially the hazard for two consecutive price increases. The
baseline hazards for the price reversal are declining, with b-  t
                                                                  +
                                                                    showing the sharpest decline.
     We conclude from these findings is that the shape of the baseline hazard we recovered in
Figure 1 is primarily driven by price reversals, especially those associated with sales (price
increases following price decreases), where the hazard is steeply declining. Price reversal are
common in the data: 72.3 percent of spells starting with a price increase end with a price
decrease, while 72.4 percent of spells starting with a price decrease end with a price increase.
     The model is over-identified and so we can again apply the J-test. We run a separate J-
test for each hazard. This is conceptually correct since each baseline hazard can be estimated
without assuming a MPH structure for the other competing hazards. The five percent critical
value is 1,749 for each risk, and the test statistics are J ++ = 3,920, J +- = 8,737, J -+ =
7,910, and J -- = 3,401. Even though we still reject the model at the five percent level, the
rejection is "milder" for price trends than price reversals, and is especially mild compared
to the estimates of the MPH model, where we had J = 10,498.
     Figures 16 and 17 in Appendix F show estimated b++ and b-- for individual product
categories. The results are in line with those for the pooled sample. The hazard b++ declines
for about 6 weeks and then is flat, and the baseline hazard b-- is declining in most categories.
The value of the J -test for individual categories is lower than the value on the pooled sample.


                                                33
               1.6               b++                               b+ -

               0.8
      hazard




               0.4
               0.2
               0.1



               1.6               b- +                              b--

               0.8
      hazard




               0.4
               0.2
               0.1

                     10   20       30 40    50    60   10   20       30 40    50    60
                               t in weeks                        t in weeks


Figure 4: Baseline hazards in the competing risks model for pooled IRI data, log scale. b++t
is the baseline hazard for spell which begin and end with a price increase; b-- t   for spells
                                               -
which begin and end with a price decrease; b+t   for spells which begin with a price increase
                                    -+
and end with a price decrease; and bt for spells which begin with a price decrease and end
with a price increase. The shaded regions show two standard error bands. Standard errors
are clustered at the store × product category level. We normalize the baseline hazard to be
1 at duration = T = 2.
                 ¯




                                                 34
In particular, we cannot reject the specification for 8 categories for b++ and 21 categories for
b-- .
    We investigate the nature of the failure of proportional hazard assumption more system-
atically in Appendix D.3. We conclude that the dynamics of price trends is well described
by the MPH assumption, and that the baseline hazard is fairly flat. On the other hand, we
conclude that MPH assumption is not a good description of the dynamics of price reversals.
One possible reason is that temporary changes might have fixed duration which does not fit
into the MPH framework.
    Based on these test results we believe that the richer model with competing risks and
observable characteristics is a useful framework to analyze the data. The baseline hazard for
two consecutive price increases is decreasing until week 6 which covers a substantial amount
of price changes: 76.8% of complete spells which start after a price increase last at most
6 weeks (among complete spells which start and end with a price increase, 76.7% last at
most 6 weeks). During first 6 weeks, the baseline hazard drops by almost 50%. The hazard
is then flat until week 45. This shape of the hazard is consistent with price plan models
with Calvo-type switching between plans. There is a pronounced spike at around one year,
consistent with Taylor-type pricing. The baseline hazard for two consecutive price decreases
is mildly decreasing over the examined range.


8    Comparison to Other Estimation Methods
The usual approach to estimating the MPH model is via maximum likelihood for the con-
tinuous time model. Formulating the likelihood requires an assumption on the frailty dis-
tribution. It is convenient to assume a gamma distribution, since this makes it possible to
integrate out the frailty distribution analytically and obtain a simpler expression for the
likelihood function. Since the gamma distribution is described by its mean and variance,
and the mean can be normalized to 1, one then finds the baseline hazard and variance to
maximize the likelihood.
    One issue which arises is that time in the model is continuous but the data are typically
measured discretely. One approach, as in Nakamura and Steinsson (2008), is to assume that
the baseline hazard is piece-wise constant, b(t +  ) = bt for all   [0, 1) and nonnegative
integer t, and to assume that observing an integer duration t means that the exact (con-
tinuous time) duration was t. Maximum likelihood estimation can be done using a built-in
procedure in Stata with some tricks, as we explain in Appendix E.3.
    Figure 5 compares estimates from Stata with ours. In both cases, the average type is
normalized to 1, so the baseline hazards are equal to the Kaplan-Meier hazard at duration 2

                                              35
weeks. The baseline hazard estimated using maximum likelihood is somewhat steeper than
the one estimated using GMM. We think there are two potential biases in the maximum
likelihood estimates. First, maximum likelihood requires choosing a family for the frailty
distribution. Heckman and Singer (1984) pointed out that imposing a specific distribution
can bias the estimates of the baseline hazard. Another possibility is time aggregation. The
likelihood is formulated in continuous time while data are discrete, and failure to properly
account for time aggregation can bias the estimates. We investigate these reasons in detail
in Appendix E and conclude that in our data set, time aggregation plays a more important
role.
    In closing, we note that there are several advantages to using the GMM estimator we
developed over maximum likelihood. First, our estimator does not require us to specify the
frailty distribution. Second, it is linear in b and hence is very simple and fast to solve. Third,
we proved in Proposition 3 that we find a global maximum. In contrast, the log-likelihood is
non-linear b and finding its maximum can be slow.16 Importantly, there is no guarantee that
maximum likelihood finds a global maximum. Finally, we showed that our method is easily
extended to a competing risks framework with spell-specific observable characteristics. We
can handle these even if the proportional hazard assumption only holds for some risks and
some observables. This set of assumptions has proved to be extremely hard to handle in
the maximum likelihood framework. For example, Fougere, Le Bihan, and Sevestre (2007)
try to estimate a competing risks model with unobserved heterogeneity but say on page 260
that ". . . convergence of the maximum likelihood procedure is very difficult to reach."


9        Conclusion
We develop a new consistent estimator of the baseline hazard in the MPH model using
duration data with repeated observations. Our estimator has many desirable features: it is
linear in the baseline hazard and hence easy to implement; it does not require specifying
the frailty distribution; and it handles right-censored data, competing risks, and discrete
observable characteristics. Importantly, it works in an environment where duration takes on
one of a finite number of possible values, which is the format of real-world data. We further
propose and implement two tests of the MPH specification.
    We treat the frailty distribution as a nuisance parameter in most of our analysis. However,
in the Appendix, we present estimators of the first three moments of the frequency-weighted
    16
     Using our pooled IRI sample, it took 15 hours to estimate the baseline hazard using the ML method
in Stata on a computer with 256GB memory. It took 70 minutes to estimate it (including standard errors)
using GMM. A computer with 60GB memory failed to deliver ML estimates but produced GMM estimates.



                                                  36
                      Hazard Rates                                            Average Type
         0.32                  baseline: GMM                        1
                               baseline: ML                        0.8               GMM         ML
         0.16                  Kaplan-Meier




                                                    average type
hazard




         0.08
                                                                   0.4

         0.04

         0.02                                                      0.2
                10   20       30   40   50     60                        10   20       30   40   50   60
                          t in weeks                                               t in weeks


Figure 5: The left hand figure shows the Kaplan-Meier and baseline hazard for pooled IRI
data, log scale. The purple line shows the Kaplan-Meier hazard, the blue solid line is our
GMM estimate of the baseline hazard, and the blue dotted line is the maximum likelihood
estimate of the baseline model under the assumption that the data generating process is
in continuous time and the frailty distribution is gamma. The right hand figure shows the
average type, with our GMM estimate the solid line and the maximum likelihood estimate
the dotted line.

frailty distribution following the identification proof and insights from the construction of
the estimator of Kaplan-Meier hazard.
    We also estimate a competing risk model for the direction of price changes, distinguishing
between price trends, which we interpret as regular price changes, and price reversals, which
include sales. Our framework is general enough to handle different notions of sales. For
example, we could have defined a sale as a temporary cut in price from a "normal" price p to
a sale price p < p, followed by a reversal back to p. We could also include other variable into
the vector of observable characteristics, such as bins of marginal cost or of the average price
of competitors. All these options can be handled through appropriately defining observables
x and risks r in our framework.
    The model and its estimator can also be applied in other fields. In the labor market, it can
be used to study duration dependence in transitions between employment, unemployment
and out of labor force. Worker's current labor market status is an observable characteristic
and the next labor market status can be treated as a competing risk.




                                               37
References
Abbring, Jaap H., and Gerard J. van den Berg, 2003. "The Identifiability of the Mixed
 Proportional Hazards Competing Risks Model." Journal of the Royal Statistical Society:
 Series B (Statistical Methodology). 65 (3): 701­710.

Alvarez, Fernando, and Francesco Lippi, 2020. "Temporary Price Changes, Inflation
  Regimes, and the Propagation of Monetary Shocks." American Economic Journal:
  Macroeconomics. 12 (1): 104­52.

Alvarez, Fernando, Francesco Lippi, and Luigi Paciello, 2011. "Optimal Price Setting with
  Observation and Menu Costs." Quarterly Journal of Economics. 126 (4): 1909­1960.

Bils, Mark, and Pete Klenow, 2004. "Some Evidence on the Importance of Sticky Prices."
  Journal of Political Economy. 112 (5): 947­985.

Bronnenberg, Bart J., Michael W. Kruger, and Carl F. Mela, 2008. "Database paper: The
  IRI marketing data set." Marketing Science. 27 (4): 745­748.

Caballero, Ricardo J., 1989. "Time Dependent Rules, Aggregate Stickiness and Information
  Externalities." Discussion Papers 198911, Columbia University.

Calvo, Guillermo A, 1983. "Staggered Prices in a Utility-Maximizing Framework." Journal
  of Monetary Economics. 12 (3): 383­398.

Cameron, A. Colin, Jonah B. Gelbach, and Douglas L. Miller, 2011. "Robust Inference With
  Multiway Clustering." Journal of Business and Economic Statistics. 29 (2): 238­249.

Carvalho, Carlos, 2006. "Heterogeneity in Price Stickiness and the Real Effects of Monetary
  Policy." Frontiers of Macroeconomics. 2 (1).

Carvalho, Carlos, and Felipe Schwartzman, 2015. "Selection and Monetary Non-Neutrality
  in Time-Dependent Pricing Models." Journal of Monetary Economics. 76: 141­156.

Cavallo, Alberto, 2018. "Scraped Data and Sticky Prices." Review of Economics and Statis-
  tics. 100 (1): 105­119.

Chen, Le-Yu Chen, and Jerzy Szroeder, 2009. "Hypothesis Testing of Multiple Inequalities:
 The Method of Constraint Chaining.." Cemmap Working Paper CWP13/09.

Cox, David R., 1972. "Regression Models and Life-Tables." Journal of the Royal Statistical
  Society. Series B (Methodological). 34 (2): 187­220.

                                            38
Eichenbaum, Martin, Nir Jaimovich, and Sergio Rebelo, 2011. "Reference Prices, Costs, and
  Nominal Rigidities." American Economic Review. 101 (1): 234­262.

Elbers, Chris, and Geert Ridder, 1982. "True and Spurious Duration Dependence: The
  Identifiability of the Proportional Hazard Model." Review of Economic Studies. 49 (3):
  403­409.

Fougere, Denis, Herv´  e Le Bihan, and Patrick Sevestre, 2007. "Heterogeneity in Consumer
  Price Stickiness: A Microeconometric Investigation." Journal of Business and Economic
  Statistics. 25 (3): 247­264.

Golosov, Mikhail, and Robert Lucas, 2007. "Menu Costs and Phillips Curves." Journal of
 Political Economy. 115: 171­199.

Heckman, James J., and Bo E. Honor´ e, 1989. "The Identifiability of the Competing Risks
  Model." Biometrika. 76 (2): 325­330.

Heckman, James J., and Burton Singer, 1984. "A Method for Minimizing the Impact of
  Distributional Assumptions in Econometric Models for Duration Data." Econometrica. 52
  (2): 271­320.

Honor´
     e, Bo E., 1993. "Identification Results for Duration Models with Multiple Spells."
  Review of Economic Studies. 60 (1): 241­246.

Horowitz, Joel L., and Sokbae Lee, 2004. "Semiparametric Estimation of a Panel Data
  Proportional Hazards Model with Fixed Effects." Journal of Econometrics. 119 (1): 155­
  198.

Lancaster, Tony, 1979. "Econometric Methods for the Duration of Unemployment." Econo-
  metrica. 47 (4): 939­956.

Lancaster, Tony, 1990. The Econometric Analysis of Transition Data . No. 17 in Econometric
  Society Monographs, Cambridge University Press.

Nakamura, Emi, and J´
                    on Steinsson, 2008. "Five Facts about Prices: A Reevaluation of Menu
  Cost Models." Quarterly Journal of Economics. 123 (4): 1415­1464.

Nakamura, Emi, and J´
                    on Steinsson, 2010. "Monetary Non-neutrality in a Multisector Menu
  Cost Model." Quarterly Journal of Economics. 125 (3): 961­1013.

Newey, Whitney K., and Daniel McFadden, 1994. "Large Sample Estimation and Hypothesis
  Testing." Handbook of Econometrics. 4: 2111­2245.

                                           39
Politis, Dimitris N., 2011. "Higher-Order Accurate, Positive Semidefinite Estimation of
  Large-Sample Covariance and Spectral Density Matrices." Econometric Theory. 27 (4):
  703­744.

Reis, Ricardo, 2006. "Inattentive Producers." Review of Economic Studies. 73 (3): 793­821.

Taylor, John B., 1979. "Staggered Wage Setting in a Macro Model." American Economic
  Review. 69 (2): 108­113.

Taylor, John B., 1980. "Aggregate Dynamics and Staggered Contracts." Journal of Political
  Economy. 88 (1): 1­23.




                                           40
                                              Appendix

A      Omitted Proofs
A.1      Kaplan-Meier Hazard Moments
Proof of Proposition 4. Take a product i with measured durations  i = (0           i   i
                                                                                     , 1           i
                                                                                         , . . . , K i)
                              K  i
and hence censoring time ci = j =0 j
                                   i
                                     - 1. Let z i = {z1
                                                      i            i
                                                        , . . . , zc i } be a vector of length c
                                                                                                      i

with the following elements:
                                  
                                   i                   k -1 i
                          i        k     for s =       j =0 j   and k = 1, . . . , K i
                         zs   =
                                  0      otherwise.

 i
zs encodes the measured duration of any spell that starts s periods into the observation
window for the product, with zeros in any period when a new spell does not start.
                                                                            ¯ + 1,
   We first claim is that for any product i and any duration t = 1, . . . , T

                                       Ki                         ¯
                                                              ci -T
                                              1ji t,cij T
                                                        ¯ =           1zs
                                                                        i t ,

                                       j =1                   s=1


where we understand that the left hand side evaluates to 0 when ci  T          ¯. The left-hand sum
counts the number of spells (except the initial left-censored one) with duration at least t and
residual censoring time at least T ¯. The right-hand sum counts the same spells by dropping
                                ¯, when the residual censoring time would be less than T
all those that start after ci - T                                                                   ¯.
                                                        ¯
                                                    s=1 1zs
                                                     i -T
    Next, we compute the expected value of c                                           ¯ + 1 conditional
                                                            i t for any t = 1, . . . , T


on ci and i . Here we use the assumption that initial duration is drawn from the stationary
ergodic distribution. This implies that with probability         ~ 0 (i ) = 1/  t =0
                                                                                         t             i
                                                                                         s=0 (1 - hs ( ))
                                                                                                  i
(see equation 8), the firm changes its price in any period s  1, in which case zs                    > 0,
while otherwise zs = 1zs
                    i
                          i t = 0. If the firm does change its price, the probability that the


measured duration of the price spell is at least t is given by the type-specific survivor function
   t-1          i                                                                              ¯
   s=0 (1 - hs ( )). This use the fact that right censoring is not an issue for t  T + 1 and
s  ci - T  ¯.
    Putting this together, in any period s  {1, . . . , ci - T       ¯}, the expected value of 1zi t
                                                                                                       s

conditional on ci and i is
                                           t-1            i
                                           s=0 (1 - hs ( ))
                                               t
                                                                 .
                                                              i
                                       t =0    s=0 (1 - hs ( ))




                                                      41
It follows that
                                                                                                    t-1
                                                                                        i ¯                    i
          Ki                                          ¯
                                                  ci -T                                 (c -T )     s=0 (1-hs ( ))            ¯
                                                                                                                      if ci > T
                 1ji t,cij T                              1zs
                                                                                                    t          i
                               i       i                           i       i                        s=0 (1-hs ( ))
      E                    ¯ c ,            = E             i t c ,                =         t =0

          j =1                                     s=1
                                                                                       0                                     ¯
                                                                                                                      if ci  T.

Now condition only on i . Using the conditional distribution of c given  we get

                               K                                              i        t-1          i
                    c                                                ¯+1 cqc ( )       s=0 (1 - hs ( ))
                 E    ¯            1j t,cj T
                                           ¯ 
                                                    i
                                                          =        c=T
                                                                                t
                   c-T     j =1                                         t =0
                                                                                              i
                                                                                s=0 (1 - hs ( ))
                                                                           t-1
                                                              f        i
                                                          =  ( )                   (1 - hs (i )),
                                                                           s=0


where the feasible weight  f is defined in equation (12). Finally, integrating across  using
the frailty distribution G, we get

                                       K                           H                   t-1
                          c
                       E    ¯                 1j t,cj T
                                                      ¯ =                   ( )f
                                                                                             (1 - hs ())dG()                      (19)
                         c-T           j =1                       L                    s=0


                       ¯ + 1.
for all t = 1, . . . , T
    For any t = 1, . . . , T¯, this implies

                                       K                               H                 t
                       c
                    E    ¯                  1j t+1,cj T
                                                      ¯ =                    ()f
                                                                                              (1 - hs ())dG().
                      c-T          j =1                            L                    s=0


We can then take first differences for any such t to get

                                   K                          H                           t-1
                        c
                  E       ¯                1j =t,cj T
                                                    ¯ =             f ()ht ()                   (1 - hs ())dG().                  (20)
                       c-T     j =1                           L                           s=0


                                                                                                                     [H ]
Then using equations (13), (19) and (20), it follows immediately that E ft,T
                                                                           ¯ ( ; H ) = 0
                   ¯ if and only if
for t = 1, . . . , T
               H                      t-1                                          H               t-1
               L
                    f ()ht ()         s=0 (1 - hs ( ))dG( )                        L
                                                                                        ht ()      s=0 (1 - hs ( ))dG( | )
                                                                                                                          f
     Ht =          H               t-1
                                                                           =           H        t-1
                                                                                                                            ,
                   L
                        f ()       s=0 (1 - hs ( ))dG( )                               L        s=0 (1 - h s ( ))dG( | f)



where the last equation uses equation (1). This is equal to Ht ( f ), proving the result.




                                                                  42
A.2      Baseline Hazard Moments
Since Proposition 3 is a special case of Proposition 5, we prove the latter proposition first
and then turn to the special case.
    To simplify the exposition in this appendix, we introduce the following notation. For any
K vector  = (1 , . . . , K ) and k  K , we define k to be a vector consisting of the first k
elements of  , that is, k = (1 , . . . , k ). For k > K , we construct k by adding k - K zeros
to the end of  to construct a k vector, k = (1 , . . . , K , 0, . . . , 0}. Next, for j < k we let
k/j denote the vector k without the j th element, that is, k/j  (1 , . . . , j -1 , j +1 , . . . , k ).

    The key step in proving Proposition 5 is the statement and proof of Lemma 1.

Lemma 1 Assume  , ,  are drawn from a right-censored competing-risk model with base-
line hazard b0 for observable characteristic x and risk r. Take any k > j  1 and vector
                                                                  ¯}. Also take any x  {1, . . . , X }k with
t = (t1 , . . . , tk )  {1, 2, . . . }k with tj , tk  {T, . . . , T
                                                       ¯
xj = xk = x and r  {1, . . . , R}k-1 with rj = r. Define

  fj,k,t,x,r ( , , ; b) 
           btk 1K k,k =x,k-1 =r,k-1 =tk-1 ,k tk - btj 1K k,k =x,k-1 =r,k-1/j =tk-1/j ,j =tk ,k tj . (21)

Then E [fj,k,t,x,r ( , , ; b0 )] = 0.

Proof of Lemma 1. We first claim that the first indicator function in equation (21)
evaluates to 1 if and only if these conditions hold:

                                                                   ¯  k;
   1. without censoring, the product has sufficiently many spells, K

   2. the observable characteristics for the first k spells is x, k = x;

   3. the risk for the first k - 1 spells is r , k-1 = r ;
                                                              K           k
   4. we observe the product for sufficiently long,           l=1 l       l=1 tl ;


   5. the uncensored durations satisfy k-1 = tk-1 and k  tk .

If the first condition failed, we could never observe k spells. The second and third conditions
ensure we observe the desired pattern of observable characteristics and risks. The fourth
condition ensures we observe the product sufficiently long to see k-1 = tk-1 and k  tk .
Finally, if the last condition failed, we might observe k spells, but they would not satisfy
k-1 = tk-1 and k  tk . On the other hand, if all five conditions are satisfied, we measure
K  k , k-1 = k-1 , k  tk , k-1 = r , and k = x.

                                                    43
   Analogously, the second indicator function in equation (21) evaluates to 1 if and only if
the first four conditions hold and the uncensored durations satisfy k-1/j = tk-1/j , j = tk
and k  tj .
   Next, we use the MPH model to compute the probability of a realization of the event in
the first indicator function, conditional on  . This is

       Pr[k = x, k-1 = r , k-1 = tk-1 , k  tk | ]
                      k                                                       tl -1
       = 1 (x1 | )               (xl |xl-1 , rl-1 ,  )1l=1 hr
                                                            tl (xl ,  )
                                                              l        1l=k           (1 - hs (xl ,  ))
                     l=1                                                      s=1
                                     k                                                          tl -1
       = b0,tj ( )1 (x1 | )                (xl |xl-1 , rl-1 ,  )1l=1 hr
                                                                      tl (xl ,  )
                                                                       l          1l=j,l=k              (1 - hs (xl ,  )) .
                                    l=1                                                          s=1


The first equation uses the structure of the model, in particular the fact that we are comput-
ing the probability of a particular sequence of observable characteristics and spell durations.
The second equation uses the fact that rj = r, xj = x, and hr         tj (x,  ) = ( )b0,tj since
                ¯}. Integrating across the distribution of  conditional on censoring time equal
tj  {T, . . . , T
      ¯
to at least k    l=1 tl - 1 gives us


                E 1K k,k =x,k-1 =r,k-1 =tk-1 ,k tk =  (tk-1/j , tj , tk , x, r ; j, k )b0,tj ,                                (22)

where

                                                        k
   (tk-1/j , tj , kj , x, r ; j, k )       1-P              tl        ×
                                                      l=1
   H                       k                                                          tl -1
       ( )1 (x1 | )                (xl |xl-1 , rl-1 ,  )1l=1 hr
                                                              tl (xl ,  )
                                                               l          1l=j,l=k            (1 - hs (xl ,  )) dG      k           ( ).
                                                                                                                        l=1 tl -1
 L                        l=1                                                          s=1
                                                                                                                                     (23)

   Now swap the role of tj and tk but leave tk-1/j , r , and x unchanged. The same logic
implies

           E 1K k,k =x,k-1 =r,k-1/j =tk-1/j ,j =tk ,k tj =  (tk-1/j , tk , tj , x, r ; j, k )b0,tk .                          (24)

Moreover, equation (23) and the commutative property of multiplication implies

                            (tk-1/j , tk , tj , x, r ; j, k ) =  (tk-1/j , tj , tk , x, r ; j, k ).                           (25)



                                                                 44
The result then follows from equations (22), (24), and (25).
                                                                   [b,x,r]
Proof of Proposition 5. We first prove that E ft1 ,t2 ( , , ; b0 ) = 0 for all T  t1 <
                                                                               ¯
      ¯                                   [b,x,r]                                    ¯
t2  T and  (necessity). Then we prove E ft1 ,t2 ( , , ; b) = 0 for all T  t1 < t2  T
                                                                       ¯
only if b = b0 (sufficiency) for some .

                                                                   [b,x,r]
Necessity: We show in two steps that function ft1 ,t2 ( , , ; b) is the sum of functions
defined in Lemma 1, each of which have expected value zero. First, take 1  j < k , a
                                          ¯}, an observable characteristic x, and a risk r. Define the
pair (tj , tk ) with tj , tk  {T, . . . , T
                               ¯
following function

   fj,k,tj ,tk ,x,r ( , , ; b)  btk 1K k,j =tj ,k tk ,j =r,j =k =x - btj 1K k,j =tk ,k tj ,j =r,j =k =x .

Then let t be an arbitrary k vector of durations with j th element tj and k th element tk , x be
an arbitrary k vector of observables with j th and k th element x, and r be an arbitrary k - 1
vector of risks with j th element r. Summing across all such vectors, we get

                  fj,k,tj ,tk ,x,r ( , , ; b) =                            fj,k,t,x,r ( , , ; b),
                                                  tk-1/j ,xk-1/j ,rk-1/j


where this follows directly from the definition of fj,k,t,x,r ( , , ; b) in equation (21). Lemma 1
states that the expected value of each component of the sum is zero for b = b0 . Thus the
expected value of fj,k,tj ,tk ,x,r ( , , ; b0 ) is zero.
    Second, fix a pair of durations (t1 , t2 ) with T  t1 < t2  T     ¯, an observable characteristic
                                                       ¯
x, and a risk r. Sum fj,k,t1 ,t2 ,x,r ( , , ; b) across all pairs of spells (j, k ) with 1  j < k . By
                                    [b,x,r]
equation (14), this gives us ft1 ,t2 ( , , ; b). Since the expected value of each component of
                                       [b,x,r]
this sum is zero, this implies E ft1 ,t2 ( , , ; b0 ) = 0.
                                             [b,x,r]
    Finally, note that the function ft1 ,t2 defined in equation (14) is linear in the base-
               [b,x,r]               [b,x,r]
line hazard, ft1 ,t2 ( , , ; b) = ft1 ,t2 ( , , ; b) for all t1 , t2 ,  , , , b, and . Thus
    [b,x,r]
E ft1 ,t2 ( , , ; b) = 0 as well.

                                                           ¯} with b0,t > 0. We prove that
Sufficiency: Recall that T0 is the smallest t  {T, . . . , T
                                                ¯
any solution must take the form b = b0 where  = bT0 /b0,T0 .
   Equation (14) implies that

   bT0                 E 1j =t,k T0 ,j =k =x,j =r = bt                           E 1j =T0 ,k t,j =k =x,j =r .
         (j,k):1j<kK                                           (j,k):1j<kK




                                                        45
Assumption 2 states that E 1j =T0 ,k t,j =k =x,j =r > 0 for some 1  j < k < K and any
tT  ¯. Therefore the sum on the right hand side of this equation is strictly positive, allowing
us to pin down the ratio bt /bT0 :

                            bt       (j,k):1j<kK         E 1j =t,k T0 ,j =r,j =k =x
                               =                                                      .
                           bT0       (j,k):1j<kK         E 1j =T0 ,k t,j =r,j =k =x

From the `necessity' part of the proof, we know bt /bT0 = b0,t /b0,T0 solves this equation, so
this must be the only solution.

Proof of Proposition 3. Set X = R = 1. This implies 1 (1| ) =  (1|1, 1,  ) = 1, so
Assumption 1 is equivalent to Assumption 2 in this case. Then

                                       [b,1,1]                   [b ]
                                      ft1 ,t2 ( , 1, 1; b) = ft1 ,t2 ( ; b),

where 1 is a vector of 1's, and so the results in Proposition 5 imply the proof of this propo-
sition.


B       GMM Estimation
B.1      GMM Estimator
                                                                                         ¯:
Proposition 3 gives us one moment condition for the choice t1 , t2 such that T  t1 < t2  T
                                                                             ¯
                                                  [b ]
                                             E ft1 ,t2 ( ; b) = 0.

Let Y (T, T¯) = {(t1 , t2 ) : T  t1 < t2  T  ¯}. This set has M = T (T + 1)/2 elements which
       ¯                      ¯
we index with m and refer to it as ym = (ym1 , ym2 ). Let f [b] ( ; b) be a vector function with
                                                          ¯), given by fy[b]
mth element corresponding to the choice ym  Y (T, T                       m1 ,ym2 ( ; b).
                                                       ¯
    Since the baseline hazard is identified up to scale, we choose our normalization. Choose
T0  {T, T  ¯} to be the shortest for which there exists product i with at least two spells,
        ¯
K i  2, and 1  j < k  K i such that j       i        i
                                              = T0 , k  = t for any t  {T0 , T ¯}.17 Without loss of
generality, we normalize bT0 = 1.
    Let b·/T0 be the vector b without its component bT0 , that is, b·/T0 = (bT , . . . bT0 -1 , bT0 +1 , . . . bT
                                                                                                                ¯ ).
                                                                                          ¯

    If no product with at least two spells has a complete spell of duration t, then we estimate ^
   17
                                                                                                bt = 0 and
so we cannot use it for normalization.




                                                          46
               [b]
Linearity of ft1 ,t2 ( ; b) and normalization of bT0 implies that we can write

                                    f [b] ( ; b) = U [b] ( )b·/T0 - V [b] ( ),

where U [b] is M × T matrix, and V [b] ( ) is a vector of length M . With this notation, we can
write
                             E U [b] ( ) b·/T0 - E V [b] ( ) = 0.                          (26)
                                                                  ¯. Define f ¯ as a                             [H ]
   Proposition 4 gives us one moment condition for each T  t  T                   T
                       th                 [ H ]         ¯
                                                        T
                                                         ¯
vector function, with m element given by fm+T -1,T
                                                 ¯ ( ; H ) for m = 1, . . . , T + 1. Since
                                ¯                            [H ] ¯                 ¯           ¯
equation (13) is linear in H T , we can write fm+T -1,T          T
                                                          ¯ ( ; H ) = U
                                                                        [H ]
                                                                             H T - V [H ] , where U [H ]
                                                     ¯
is a (T + 1) × (T + 1) matrix and V [H ] is a (T + 1) × 1 vector. With this notation, the moment
condition from Proposition 4 becomes

                                                        ¯
                                E U [H ] ( ) H T - E V [H ] ( ) = 0.                                                    (27)

                                                                      ¯
   We stack these moment conditions for b and H T . Define

                b·/T0                        f [b ] (  ; b )                   U [b] 0                V [b ]
         =          ¯   , f ( ;  ) =         [H ]           ¯ ,U =                             ,V =          .
                HT                           ¯ ( ; H )
                                            fT             T
                                                                                0 U [H ]              V [H ]

Then the moment conditions are

                                        E [U ( )]  - E [V ( )] = 0.                                                     (28)

   To estimate the model, we replace expected values with sample means:

                                             I                                 I
                                        1                                 1
                              UI                  U ( i ),        VI                V ( i ).
                                        I   i=1
                                                                          I   i=1


The sample analog of (28) is UI  - VI = 0. For a given positive-definite (M + T + 1) × (M +
T + 1) weighting matrix W , the estimator  ^  R+ 2T +1
                                                       solves

                           ^ = arg min (UI  - VI ) W (UI  - VI ) .
                           
                                         R2
                                          +
                                            T +1




This is a linear-quadratic maximization problem and its solution is known in a closed form:

                            ^ = (U (W + W ) UI )-1 U (W + W ) VI .
                                  I                 I




                                                             47
In practice, we choose the identity matrix as a weighting matrix.
    Proposition 3 and 4 imply consistency of GMM without any other assumptions. In
particular, we do not need to impose that the space of possible parameters  is compact
since our estimator is linear; see Newey and McFadden (1994).18


B.2     Clustered Standard Errors
Recall that the GMM formula for the variance-covariance matrix of the parameter vector 
is
                                1
                       V AR  (F W F )-1 F W W F (F W F )-1 ,                        (29)
                                I
where F is the score matrix F  E [ f ] and  = E [f f ]. To get an estimate of the
variance-covariance matrix, we replace F and  with its sample analogs FI and I :

                          I                                                    I
                    1                      i                         1
                FI              f ( ;  ) = UI ,                   I                f ( i ;  )f ( i ;  ) ,
                    I    i=1
                                                                     I     i=1


where  is a GMM estimate of  .
    To implement one-way clustering, we follow Cameron, Gelbach, and Miller (2011). For-
mula (29) still applies but with cluster-robust sample analog of . Let Q denote the number
of clusters indexed by q = 1, . . . , Q. If a product i belongs to cluster q , we say 1iq = 1.
Define f¯q as the sum of the moment conditions across products in cluster q ,

                                                        I
                                                ¯q =
                                                f            f ( i ;  )1iq .
                                                       i=1


Then
                                                                                   Q
                               [cluster]          Q       I -1      1                  ¯q f
                                                                                          ¯,
                               I               =                                       f   q
                                                 Q - 1 I - (2T + 1) I          q =1

                                                   Q      I -1
where 2T +1 is the number of parameters. The term Q- 1 I -(2T +1)
                                                                   is adjustment for the degrees
of freedom; without this adjustment, the clustered standard errors are biased downwards.
                                                              [cluster]
We obtain the variance-covariance matrix by substituting I              into equation (29).
  18
    Theorem 2.7 states conditions for consistency of estimators without compactness. Example 1.2 on page
2134 then shows that these conditions are satisfied for the linear GMM estimators.




                                                             48
B.3     Practical Consideration
                                                   [cluster]
It is a known that in practice matrix I (or I                 ) can be badly scaled, especially with a
large number of moments as we have. This is not necessarily an issue for estimating of the
variance-covariance matrix V AR but is for the J -test which requires inverting the matrix
          [cluster]
I (or I             ).
    Moreover, in our application, I has some negative eigenvalues. This is a result of
                                                  [cluster]
numerical imprecisions; matrix I as well I                  is positive semidefinite in any sample by
construction.
    We address both of these issues in one step, following Cameron, Gelbach, and Miller
(2011) and Politis (2011). We construct matrix I , compute its eigenvalues and replace
all negative one and those close to zero in absolute term, with a small positive number
 to construct +        I , a positive definite matrix. Specifically, we write I = AA , where
 = Diag (1 , . . . , K ) are the eigenvalues of I , and A is a matrix of eigenvectors. We
define +                            +           +         +
         j = max(, j ) and  = Diag (1 , . . . , K ). We then construct I = A A .
                                                                                     +       +

    We need to balance two forces when choosing . It has to be small enough so that it does
not affect results as the sample size grows, and at the same time, it has to be big enough to
address the problem of ill-conditioned matrix. Politis (2011) suggests to choose  = I -a for
a  [1, 2]; we follow this suggestion and choose a = 1.5.
                                                  [cluster]
    We find that I with no clustering and I                 with one-way clustering has a small share
of negative eigenvalues, less than 2.5 percent, and that they are small in absolute value, of
the order of 10-13 . This gives us confidence that these are indeed numerical imprecisions
which we correct with the above described procedure.


C      Time-Dependent Pricing with Heterogeneous Firms
C.1     General Model
It is useful to define

                                   t t ()                             t ()
                     t () =           s
                                                 and       t ( ) =             ,
                                 s=0  s ( )                          s=0 s ( )




                                                 49
so that we can write equations (15) and (17) as

                                                 t
                                     pt () =         s ()t-s (),
                                               s=0
                                                                  
                                     t () = (1 - ) +                   s ()Pt+s
                                                                 s=0


for t  0. We have simplified the first equation using the assumption that t () = 0 for
t  0.
    Substitute the second equation into the first to get
                                                                       
                                 pt () = (1 - )p0
                                                t ( ) +                        kt,s ()Ps ,            (30)
                                                                     s=0


where
                            t                                              t
              p0
               t ( )   =         s ()       and      kt,s () =                        x ()x+s-t ().
                           s=0                                    x=max(0,t-s)

Here p0t ( ) is the average price among  -type firms which would prevail in an economy with
no strategic complementarity, and kt,s () is a -type kernel given by a convolution of t ()
and t (). One can verify that

                                                            t
                                               kt,s () =         x ()  1.
                                        s=0                x=0


Average both sides of equation (30) across the type distribution G() to get
                                                                   
                                      Pt = (1 -      )Pt0   +           Kt,s Ps ,                     (31)
                                                                  s=0


where
                                 H                                                H
                  Pt0                p0
                                      t ( )dG( )      and        Kt,s                 kt,s ()dG().
                                L                                               L

Since   s=0 Kt,s  1 for all t, the mapping (31) is a contraction with modulus  < 1, and so
has a unique solution in the space of bounded functions.




                                                           50
C.2     Special Case: No Strategic Complementarity
We say there is no strategic complementarity when  = 0. In this case, the optimal price
for a firm that adjusts its price is simply  , and so the average price in the economy is
                                                      H       t
                                                              s=0   s ()
                                         Pt =                            dG().
                                                      L       s=0   s ()

Changing the order of summation and integration, we get that

                        t          H                                    t      H
                                            s ()
             Pt =                                     dG() =                        s ()~
                                                                                        0 ()dG()
                     s=0         L         s =0 s ( )                  s=0    L
                             t                  H
                =                ¯s
                                                    ~ 0 ()dG()
                            s=0                L
                             t     ¯s
                                   
                             s=0
                =                  ¯s
                            s =0   

where  ¯ s is the frequency-weighted Kaplan-Meier survival function defined in equation (18).
In the last equality, we used that
                                                          H                    -1
                                               ¯s =
                                                              ~ 0 ()dG()            .
                                         s=0              L


Thus, the price level Pt is the same as in an economy with a single firm with the frequency-
weighted Kaplan-Meier survival function.


C.3     Special Case: Calvo
It is useful to analyze this problem for a single Calvo firm with parameter . In this case,
t = (1 -  (1 - )) t (1 - )t and t = (1 - )t , and so suppressing dependence on the
parameter , equations (15) and (17) then are

                                     t
                     pt =                (1 - )s t-s ,
                                   s=0
                                                                        
                     t = (1 - ) + (1 -  (1 - ))                               s (1 - )s pt+s .
                                                                       s=0




                                                              51
These can in turn be reduced to a pair of linear first-order difference equations,

             pt+1 = t+1 + (1 - )pt ,
                t = (1 - ) (1 -  (1 - )) + (1 -  (1 - ))pt +  (1 - )t+1 .

   The solution is of the form pt =  (1 - c1 xt       t
                                              1 - c2 x2 ) where x1 < x2 are roots of the quadratic
equation
               x2  (1 - ) - x 1 -  +  (1 - )(1 - (1 - )) + 1 -  = 0

and c1 and c2 are constants to be determined. It holds that 0 < x1 < 1 < x2 , and so we set
c2 = 0 to have a non-explosive path for the average price. The initial condition p0 = 0 then
pins down c1 = 1.


C.4     Calibration of the Model
For the numerical exercise, we use estimates from our baseline model. We assume that the
baseline hazard is given by our estimates presented in Section 6.1. We also want to estimate
moments of the frailty distribution. Since T > 1, it is not feasible to estimate moments
                                             ¯
distribution of dG(|  ~ f ) because we do not know what hs () is for s < T ¯. However, it is
                                                       f
feasible to estimate moments of the distribution G(·| ~T ), where
                                                              ¯

                                            
                                 f
                                ~T ()             cqc () ~T
                                                          ¯-1 ( ),
                                 ¯
                                            c=1


is the weight given by the expected censoring time for a type  product, multiplied by the
                                                 ¯. It is useful to note that manipulating this
probability that a spell of this product reaches T
expression using equations (8) and (12) leads to

                                                  T -1
                                                  ¯
                                  f         f
                                 ~T ()   =  ()           (1 - hs ()).
                                  ¯
                                                   s=0

                                                                                  f
   Denote µ = (µ1 , µ2 , µ3 ) the first three moments of the distribution G(·|   ~T ). Assuming
                                                                                  ¯
that  = (0 , 1 , . . . , K ) is drawn from a stationary mixture model, we can translate equation




                                                  52
(5) in the identification proof into moment conditions for µ1 , µ2 , µ3 :

                            K
                      c
 [µ 1 ]
fT
 ¯        ( ; µ, b)     ¯            1j =T,cj T
                                              ¯ - bT µ1 1j T,cj T
                                                                ¯
                     c-T    j =1
                                           ¯         ¯            ¯

                            K
                      c
 [µ 2 ]
fT
 ¯        ( ; µ, b)     ¯            1j =T +1,cj T
                                                 ¯ - bT +1 µ1 - bT µ2 1j T,cj T
                                                                              ¯
                     c-T    j =1
                                           ¯             ¯                    ¯                    ¯

                            K
                      c
 [µ 3 ]
fT
 ¯        ( ; µ, b)     ¯            1j =T +2,cj T
                                                 ¯ - bT +2 µ1 - (bT + bT +1 )µ2 + bT bT +1 µ3 1j T,cj T
                                                                                                      ¯ ,
                     c-T    j =1
                                           ¯             ¯                        ¯          ¯           ¯ ¯            ¯


                              c
where we use weights        c-T ¯ following the same logic as in equation (13).
                        [µ1 ]
     To see that E     fT
                        ¯ ( ; µ, b) = 0, we use equations (19) and (20). We                                    use equation (19)
                                                                                    1                                 [µ ]
and set t = T to find an expression for the expected value of the second term in fT
                                                                                  ¯ :
            ¯
                                     K                           H                T -1
                       c                                                          ¯
                    E    ¯                 1j T,cj T
                                                   ¯ =
                                                                      f
                                                                      ()                (1 - hs ())dG()
                      c-T           j =1
                                                ¯            L                    s=0
                                                                 H
                                                                      f
                                                         =           ~T ()dG().
                                                             L        ¯


In the next step, use equation (20) with t = T to find the expected value of the first term of
 [µ1 ]                                       ¯
 ¯ :
fT

                                K                            H                          t-1
                     c
                  E    ¯             1j =T,cj T
                                              ¯ =
                                                                  f
                                                                  ()ht ()                        (1 - hs ())dG()
                    c-T       j =1
                                            ¯            L                             s=0

                                                             H                        T -1
                                                                                      ¯
                                                                  f
                                                    =             ()bT                      (1 - hs ())dG()
                                                         L                    ¯
                                                                                      s=0
                                                                 H
                                                                       f
                                                    = bT              ~T ()dG().
                                                         ¯    L           ¯

                  H    f
                  L  ~T  ()dG()                                              2                          [µ ]
Since µ1 =        H f
                      ¯            , the result follows. The proof that E fT
                                                                           ¯ ( ; µ, b) = 0 and
                  L ~ T ()dG()
                     ¯
     3    [µ ]
E fT
   ¯ ( ; µ, b) = 0 is similar and so we omit it.
                                                                  f
    We find that µ ^ = (1, 1.331, 2.137). We assume that G(·|    ~T ) has a beta distribution
                                                                  ¯
over the interval [L , H ], and choose its two parameters ~, ~ together with L , H to match
the first two estimated moments of the distribution and to minimize the mean squared
error between the model-implied and estimated Kaplan-Meier hazard. We find L = 0.156,
H = 6.071,    ~ = 1.668,   ~ = 10. This distribution has a mass point at max = 1/^   b2 , with

                                                             53
                         Hazard Rates                                             Average Type
         0.32                                                            1
                                                                       0.8
         0.16




                                                        average type
                                                                       0.4
hazard




         0.08

         0.04
                                                                       0.2
         0.02

         0.01                                                          0.1
                  10    20       30   40   50     60                         10   20       30   40   50   60
                             t in weeks                                                t in weeks
                Kaplan-Meier, estimated         baseline, estimated                average type, estimated
                Kaplan-Meier, fitted                                               average type, fitted


Figure 6: Kaplan-Meier and average type implied by the calibrated model and its comparison
to estimates for pooled IRI data, log scale. The purple line shows the Kaplan-Meier hazard,
the blue line is the estimated baseline hazard, and the red line shows the "average type" at
given duration, calculated as the ratio of Kaplan-Meier and baseline hazards, as in Figure
1. The dashed lines show the fitted Kaplan-Meier hazard and the average type.

mass 0.0057, which ensures that 1 - bt is always positive for all t. The third moment of this
distribution, which is not targeted in the calibration, is 2.178, very close to the estimated
µ
^3 = 2.137.
    Figure 6 shows that we fit the estimated Kaplan-Meier hazard and average type for t  60
very well. Using the estimate baseline hazard and the frailty distribution with the above
parameters, we use equation (6) to compute the implied Kaplan-Meier hazard, call it H         ~ t,
and then compute the average type as H   ~ t /^
                                              bt . These are depicted with dashed lines in Figure
6.
    We use the Kaplan-Meier hazard estimated in Section 6.1 for t <= 60 and assume that
it is given by Ht = 0 + 1 /t for 60 < t  500, and zero for any t > 500. We estimate 0
and 1 by fitting this function using the estimated baseline hazard for weeks 10­60. We find
0 = 0.009 and 1 = 1.142. We then use the model structure to recover the baseline hazard
for t > 60 using the decomposition Ht = bt E [|t]. For any initial distribution dG(), we can
compute distribution of types among products surviving to t, dG(|t), using the distribution
dG(|t - 1) and baseline hazard at t, bt . We use this relationship together with Ht = bt E [|t],
where Ht is known, to recover bt .



                                                   54
                             Online Appendix

D     Additional Empirical Results
We report additional empirical results in this section.


D.1     Ergodic Distribution
To estimate the Kaplan-Meier hazard, we assume that when we first observe a product,
the duration of the in-progress spell is a random draw from the stationary ergodic duration
distribution for that product. A testable implication of that assumption is that, conditional
on censoring time, the share of products changing its price in any week is constant. We
implement a test in the following way. For all products with censoring time c, we compute
the fraction of price changes that occur by week t since the start of the in-progress spell;
                t
we call it Fc ( c                                                                           ¯,
                  ). We then average the cumulative distribution function Fc across all c > T
using the number of products with the corresponding value of c as weights. Figure 7 shows
that the corresponding empirical density lies within five percent of a uniform density. It is
close enough to uniform that we think the stationary mixture assumption is an empirically
useful starting point.


D.2     Aggregation to Monthly to Frequency
We aggregate weekly data to monthly frequency in the following way: spells with duration
2­5 weeks are coded as duration of one month, spells of with duration 6­9 as duration of
two months, and so on. We then estimate the MPH model using setting T = 1 and T  ¯ = 15
                                                                      ¯
months. To display the results, we convert the baseline hazard and Kaplan-Meier hazard
into weekly units by
                                 hw              m 1/4
                                   t = 1 - (1 - ht )   ,

where hm                                                                      w
        t is monthly hazard, either Kaplan-Meier or baseline, in month t and ht is a weekly
hazard in month t.
   Figure 8 compares the estimates using weekly and monthly data.


D.3                                                   ¯
        Sensitivity of Results to the Choice of T and T
                                                ¯
We examine the sensitivity of our results to the choice of T and T¯. This allows us to see
                                                           ¯
if there is a systematic failure of the MPH assumption. The idea is the following. Suppose
we want to learn about the relative baseline hazards at duration 10 and 20, b10 /b20 . The

                                             55
                  1.1



           PDF   1.05



                   1



                 0.95



                  0.9
                        0   0.1   0.2   0.3   0.4    0.5   0.6   0.7    0.8    0.9     1
                                                     t/c


Figure 7: Empirical density of times when products change prices, measured from the start
of an in-progress spell, pooled IRI data.

MPH model admits several ways of recovering the ratio. We can directly recover the ratio
b10 /b20 from equation (9) by choosing t1 = 10 and t2 = 20. But there are other options
which use information on spells at other durations. Specifically, we can use this moment
condition to recover b10 /bt and b20 /ht for some t = 10, 20, and combine them to find b10 /b20 .
Our estimator uses all such conditions. If it is the case that the MPH model is not correctly
specified at t, then including t into estimation will affect the relative hazards b10 /b20 .
                ¯) denote the GMM estimate of the baseline hazard at duration t  {T, . . . , T
     Let bt (T, T                                                                                 ¯}
             ¯                                                                          ¯
using some values T and T     ¯. We first fix T
                                              ¯ = 60 and estimate the model for different values
                       ¯
of T = 2, 3, . . . , 10. To help visualize the impact of T on the shape of the baseline hazard,
   ¯                                                     ¯
we normalize b2 (2, 60) = 1 and then recursively set bT (T, 60) = bT (T - 1, 60) for T > 2. If
                                                          ¯ ¯         ¯ ¯               ¯
                                                    ¯}, we should find that bt (T, T
the model is correctly specified for t  {T, . . . , T                              ¯ ) = bt ( T , T
                                                                                                  ¯)
                                              ¯                                 ¯            ¯
for all T < T < t  T       ¯. Substantial deviations from this indicate systematic violations of
        ¯      ¯
the MPH assumption.
     The left panel of Figure 9 shows the results for the benchmark model and Figure 10 for
the competing risks model. The choice of T affects the estimate of the baseline hazard in the
                                               ¯
benchmark model. This is in line with the fact that we reject the model using the J -test.
The choice of T has little effect on the hazard of price trend, b++ and b-- , consistent with a
                 ¯
correctly-specified model, but it substantially affects the hazard of price reversals, especially


                                                56
                                 Hazard Rates                                            Average Type
                0.32                                                            1
                                                                              0.8
                0.16
weekly hazard




                                                               average type
                0.08                                                          0.4

                0.04
                                                                              0.2
                0.02

                0.01                                                          0.1
                         10    20       30   40     50    60                        10   20       30   40   50    60
                                    t in weeks                                                t in weeks
                       baseline, weekly           Kaplan-Meier, weekly                    average type, weekly
                       baseline, monthly          Kaplan-Meier, monthly                   average type, monthly


Figure 8: Kaplan-Meier and baseline hazard using weekly and monthly pooled IRI data, log
scale. The solid lines uses weekly data, the dashed lines are data aggregated to monthly
frequency. The purple line shows the Kaplan-Meier hazard, the blue line is the estimated
baseline hazard. The red line shows the "average type" at given duration, calculated as the
ratio of Kaplan-Meier and baseline hazard. The baseline hazard is normalized to be equal to
the Kaplan-Meier hazard at duration 2 weeks in the weekly data, or 1 month in the monthly
data. Kaplan-Meier and baseline hazard are in weekly units.




                                                          57
                         Baseline Hazard                            Baseline Hazard
              1.6

              0.8

              0.4
    hazard




              0.2
                     T =2        T =3       T =4             ¯ = 90
                                                             T            ¯ = 80
                                                                          T             ¯ = 70
                                                                                        T
                     ¯
                     T =5        ¯
                                 T =6       ¯
                                            T =7             ¯ = 60       ¯ = 50        ¯ = 40
              0.1                                            T            T             T
                     ¯
                     T =8        ¯
                                 T =9       ¯
                                            T = 10           ¯ = 30       ¯ = 20        ¯ = 10
                     ¯           ¯          ¯                T            T             T
             0.05
                    10   20       30   40     50   60          20        40      60       80
                              t in weeks                               t in weeks


Figure 9: Baseline hazard for pooled IRI data, log scale, estimated using different values of
                       ¯ = 60 in the left panel, and using different values for T
T  {2, . . . , 10} and T                                                        ¯  {10, 20, . . . 90}
¯
and T = 2 in the right panel.
    ¯

so b-+ .
    To analyze the role of T ¯, we fix T = 2 and estimate the model for T ¯  {10, 20, . . . , 90}.
                                       ¯
We now normalize b2 (2, T ¯) = 1 for each value of T ¯. The right pane of Figure 9 and Figure
11 show that the choice of T  ¯ does not affect the estimates.
    This exercise does not reveal systematic violation of the MPH structure for b++ and b-- .
However, it brings up the concern that the hazards b+- and b-+ are not well described by the
MPH, at least at short durations. One hypothesis for the failure of the MPH model is that
the product type ( ) is not fixed over time. We investigate this by restricting the censoring
time ci to at most 80 weeks for every product. With the shorter censoring time, the choice
of T matters less for all four hazards, see Figure 12. The baseline hazards b++ or b-- are
   ¯
insensitive to the choice of T , supporting our conclusion that these are well described by the
                             ¯
MPH model. The estimates of b+- or b-+ still depend on the choice of T , but much less so
                                                                           ¯
than in the case of unrestricted censoring time. This is consistent with time-varying types.


E        Maximum Likelihood Estimators
We investigate reasons for differences between GMM and ML estimates presented in Figure
5. We formulate the MPH model in continous time and write down the likelihood function
under two different timing assumptions. First, we assume that the data are generating by a
continuous time model but durations are measured only in discrete times; we call this model
Continuous Time with Discrete Measurement (CT-DM). Second, we assume that the baseline

                                                 58
                               b++                                b+-
             1.6

             0.8

             0.4
   hazard




             0.2

             0.1

            0.05
                               b- +                               b--
             1.6

             0.8

             0.4
   hazard




             0.2
                                                       T =2         T =3       T =4
                                                       ¯
                                                       T =5         ¯
                                                                    T =6       ¯
                                                                               T =7
             0.1
                                                       ¯
                                                       T =8         ¯
                                                                    T =9       ¯
                                                                               T = 10
            0.05                                       ¯            ¯          ¯
                   10   20       30   40   50    60   10   20       30   40    50   60
                             t in weeks                         t in weeks


Figure 10: Baseline hazard for the competing risks model, pooled IRI data, log scale, esti-
                                                       ¯ = 60.
mated using different values of T  {2, . . . , 10} and T
                                ¯




                                                59
                             b++                                  b+ -
             1.6

             0.8

             0.4
   hazard




             0.2

             0.1

            0.05
                             b-+                                  b--
             1.6

             0.8

             0.4
   hazard




             0.2
                                                      ¯ = 90
                                                      T           ¯ = 80
                                                                  T            ¯ = 70
                                                                               T
             0.1                                      ¯ = 60
                                                      T           ¯ = 50
                                                                  T            ¯ = 40
                                                                               T
                                                      ¯ = 30
                                                      T           ¯ = 20
                                                                  T            ¯ = 10
                                                                               T
            0.05
                   10 20 30 40 50 60 70 80 90       10 20 30 40 50 60 70 80 90
                           t in weeks                       t in weeks


Figure 11: Baseline hazard for the competing risks model, pooled IRI data, log scale, esti-
                                ¯  {10, 20, . . . , 90} and T = 2.
mated using different values of T
                                                            ¯




                                            60
                           b++                                     b+ -
      1.6

      0.8

      0.4

      0.2

      0.1

     0.05
                           b-+                                     b--
      1.6

      0.8

      0.4

      0.2
                                                       T =2        T =3       T =4
                                                       ¯
                                                       T =5        ¯
                                                                   T =6       ¯
                                                                              T =7
      0.1
                                                       ¯
                                                       T =8        ¯
                                                                   T =9       ¯
                                                                              T = 10
     0.05                                              ¯           ¯          ¯
              10    20       30   40   50     60      10   20       30   40     50   60
                         t in weeks                             t in weeks


Figure 12: Baseline hazard for the competing risks model, pooled IRI data, log scale, esti-
                                                    ¯ = 60 and censoring time restricted to be
mated using different values of T  {2, . . . , 10}, T
                                ¯
at most 80 weeks.




                                             61
hazard is piece-wise constant and that observed discrete duration corresponds to continuous
time duration; we call this model Continuous Time with Continuous Measurement (CT-CM).
    We make two simplifying assumptions when formulating likelihoods for CT-DM and CT-
CM models. First, in line with the literature, we assume that censoring time c is independent
of product's types . Second, we use at most two spells per product which allows us to
represent the data in a simple way. For each combination of durations (t1 , t2 ), with t1  1
and t2  0, it is enough to store the number of products with these measured durations and
the share of these with the right-censored first and/or second spell. Due to this simplification,
maximizing the likelihood is very fast but we are aware of the fact that usefulness of this
trick disappears in a general setup where different products have a different number of spells.


E.1      Continuous Time with Discrete Measurement
We formulate a continuous time MPH model with discrete time measurement (CT-DM),
which is correctly specified in real-world data where durations are rounded to integer values.
We assume each product has a censoring time c  R+ with continuous cumulative distribution
P and a type  drawn from a Gamma distribution with mean m and variance v . We later
consider an extension to the case where the frailty distribution is a mixture of Gamma
distributions. In contrast to our GMM estimates of the discrete time model, we impose that
c and  are independent random variables.
      In the continuous time mixed proportional hazard model, we assume that for any t  R+ ,
the probability that the true duration of a spell is at least t for a product with type  is
         t
e- 0 b(s)ds for all t  0. With discrete measurement, we assume that the measured duration
is always rounded up to the next integer. That is, for t = 1, 2, . . . , the probability that
                                              t-1
measured duration is at least t is e- 0 b(s)ds .
      In the CT-DM model, there is no hope of recovering the baseline hazard at all real
durations, since we only observe integer outcomes. Instead, for any t = 1, 2, . . . , define
             t
bt  t-1 b(s)ds. Additionally, for notational convenience continue to assume b0 = 0. Our
objective is to recover b  {b1 , . . . , bT     ¯ , bT¯+1 }, where sparsity of data lead us to impose

bt = b T    ¯+1 for all t  T¯ + 1. It is also useful to define the integrated hazard zt  t bs =
                                                                                                        s=0
   t
  0
     b (s  )ds , so the probability that  measured       duration   of  a spell is at least t = 1, 2, . . . for a
type  product is e-zt-1 .
      We formulate the likelihood function for case where we observe two spells per product.
The data we observe is censored, (ci , di            i    i  i                                           i
                                                1 , d2 , 1 , 2 ) for a typical individual i, where j is the
measured duration of j th spell and di        j equals one if j
                                                                     th
                                                                        spell is censored. If the first spell
right-censored (and hence the second spell is not observed), we code the duration of the


                                                       62
                  i
second spell as 2   = 0 and di 2 = 1. Under our assumptions we can write down the likelihood
                                                                          i
of different outcomes. First, we may observe two completed spells, 1        = t1  {1, 2, . . . },
 i                           i     i
2 = t2  {1, 2, . . . }, and d1 = d2 = 0. The probability of this event is

  E 11
     i =t , i =t ,di =di =0
         1 2    2 1    2
                            =
                                                                                                           m2
                                                                                               - m   m v
                                                                                               e v
                1 - P (t1 + t2 - 1)             e-(zt1 -1 +zt2 -1 ) (1 - e-bt1 )(1 - e-bt2 )         v
                                                                                                                d.
                                         0                                                         (m2 /v )

The integrand is equal to the probability that the censoring time exceeds t1 + t2 , ci  t1 + t2 ,
                                                                i   i
multiplied by the probability that the uncensored durations (1    , 2 ) are exactly (t1 , t2 ) given
, multiplied by the density of a Gamma distribution with mean m and variance v . Here 
is the gamma function. Solve the integral to get

               E 11
                  i =t , i =t ,di =di =0
                      1 2    2 1    2
                                                                CT -DM
                                         = 1 - P (t1 + t2 - 1) f0      (t1 , t2 ; z , m, v )

where
                                                                     2                                 2
                                      v                   -mv        v              -mv
   CT -DM
  f0      (t1 , t2 ; z , m, v )    1 + (zt1 -1 + zt2 -1 )     - 1 + (zt1 + zt2 -1 )
                                      m                              m
                                                                  m2                                            2
                                                 v              -  v       v                               -mv
                                        - 1 + (zt1 -1 + zt2 )        + 1 + (zt1 + zt2 )                             .
                                                m                         m

We note the explicit dependence of this function on the integrated hazard z = {z1 , z2 , . . . },
as well as the mean and variance of the frailty distribution.
                                                                                         i
    Second, we may observe a completed spell followed by a censored spell, 1               = t1 
                i                        i       i
{1, 2, . . . }, 2 = t2  {0, 1, . . . }, d1 = 0, d2 = 1. The probability of this event is

  E 11
     i =t , i =t ,di =0,di =1
         1 2    2 1      2
                              =
                                                                                                   m       m2
                                                               
                                                                                                e- v m   v

                     P (t1 + t2 ) - P (t1 + t2 - 1)               e-(zt1 -1 +zt2 ) (1 - e-bt1 )      v
                                                                                                           d.
                                                           0                                     (m2 /v )

This is the probability that the censoring time is exactly t1 + t2 , ci = t1 + t2 multiplied by
                      i           i
the probability that 1  = t1 and 2  > t2 . Again, solve the integral to get

        E 11
           i =t , i =t ,di =0,di =1
               1 2    2 1      2
                                                                      CT -DM
                                    = P (t1 + t2 ) - P (t1 + t2 - 1) f1      (t1 , t2 ; z , m, v )




                                                         63
where
                                                                                         2                          2
                                              v                                       -mv         v               -mv
           CT -DM
          f1      (t1 , t2 ; z , m, v )    1 + (zt1 -1 + zt2 )                               - 1 + (zt1 + zt2 )         .
                                              m                                                   m
                                                   i
  Finally, we may observe a single censored spell, 1 = t1  {1, 2, . . . } and di    i
                                                                               1 = d2 = 1.
The probability of this event is

                                                                                                  m      m2
                                                                                      
                                                                                               e- v m   v

                  E 11
                     i =t ,di =1 = P (t1 ) - P (t1 - 1)                                  e-zt1      v
                                                                                                          d.
                         1 1
                                                                                  0             (m2 /v )

This is the probability that the censoring time is t1 , ci = t1 , multiplied by the probability
      i
that 1  > t1 . Solve the integral to get

                      E 11
                         i =t ,di =1
                             1 1
                                                             CT -DM
                                     = P (t1 ) - P (t1 - 1) f2      (t1 , 0; z , m, v )

where                                                                                             2
                                                        v   -m v
                                   CT -DM
                                  f2             1 + zt1
                                          (t1 , 0; z , m, v )    .
                                                        m
   We can use the probability of these three events to compute the log-likelihood. We treat
P as a nuisance parameter and take advantage of the fact that each of the probabilities is
multiplicatively separable in the terms involving P to get

                                                       N
                                CT -DM        1                 CT -DM i    i
                              L             =              log fdi +di (1 , 2 ; z , m, v ).                                 (32)
                                              N    i=1
                                                                      1       2




We impose z0 = 0, which holds by definition. We also normalize m = 1.19 Given a data set,
we can search for values of z and v to maximize this likelihood, subject to the constraint
zt+1 - zt = bT +1 for t  T . We then first difference the integrated hazard zt to recover the
baseline hazard, bt = zt - zt-1 .
    It is straightforward to extend this analysis to the case where the frailty is a mixture of
K gamma distributions. Let {mk , vk , wk } denote the mean, variance, and weight on each
distribution. Then the likelihood is
                                            N              K
                          CT -DM     1                                CT -DM i    i
                      L            =             log             w k fdi +di (1 , 2 ; z , mk , vk ) .                       (33)
                                     N     i=1             k=1
                                                                          1       2



                                                 K
We again impose z0 = 0 and fix                   k=1   wk = 1 and mk , vk , and wk all nonnegative to have
  19
       The likelihood is unaffected by doubling m, quadrupling v , and halving z .



                                                                 64
a mixture model. We also normalize K  k=1 wk mk = 1. We then search for values of z and
distributional parameters which maximize the likelihood for fixed K .


E.2     Continuous Time with Continuous Measurement
We next turn to the continuous time model with continuous time measurement (CT-CM). As
in CT-DM, we assume each product has a censoring time c  R+ with continuous cumulative
distribution P and a type  drawn from a Gamma distribution with mean m and variance
v . We later consider an extension to the case where the frailty distribution is a mixture of
Gamma distributions. We again impose that c and  are independent random variables.
    We also assume that for any t  R+ , the probability that the true duration of a spell is
                                                                                   t
at least t for a product with type  is e-z(t) for all t  0, where z (t)  0 b(s)ds. As usual,
measured durations may be censored, but here we assume that we can measure the exact
duration or censoring time for each spell.
    The data we observe is (ci , di     i   i   i
                                   1 , d2 , 1 , 2 ) for a typical individual i. Under the assumption
of a Gamma frailty distribution with mean m and variance v , independent of ci , we can
write down the likelihood of different outcomes. First, we may observe two completed spells,
  i            i
1   = t1  0, 2   = t2  0, and di          i
                                 1 = d2 = 0. The density of this event is


                                                                                              m             m2
                                                                           
                                                                                              e- v m   v

    E 11
       i =t , i =t ,di =di =0 = 1 - P (t1 + t2 ) b(t1 )b(t2 )                 2 e-(zt1 +zt2 )      v
                                                                                                         d.
           1 2    2 1    2
                                                                       0                       (m2 /v )

The integrand is equal to the probability that the censoring time exceeds t1 + t2 , ci  t1 + t2 ,
                                                             i   i
multiplied by the density that the uncensored durations (1     , 2 ) are exactly (t1 , t2 ) given ,
multiplied by the density of a Gamma distribution with mean m and variance v . Again, 
is the gamma function. Solve the integral to get

                E 11
                   i =t , i =t ,di =di =0
                       1 2    2 1    2
                                                              CT -CM
                                          = 1 - P (t1 + t2 ) f0      (t1 , t2 ; z , m, v )

where
                                                                                                        2
                                                                         v                      -2- mv
          CT -CM                                       2
         f0      (t1 , t2 ; z , m, v )    b(t1 )b(t2 ) m + v          1 + (z (t1 ) + z (t2 ))               .
                                                                         m
                                                                         i
  Second, we may observe a completed spell followed by a censored spell, 1 = t1  0,
i
2 = t2  0, di       i
            1 = 0, d2 = 1. The density of this event is

                                                                                                  m2
                                                                                      - m   m v
                                                                                      e v
          E 11
             i =t , i =t ,di =0,di =1 = h(t1 + t2 )b(t1 )             e-(zt1 +zt2 )         v
                                                                                                       d.
                 1 2    2 1      2
                                                               0                          (m2 /v )

                                                     65
This is the probability that the censoring time is exactly t1 + t2 , ci = t1 + t2 multiplied by
                      i           i
the probability that 1  = t1 and 2  > t2 . Again, solve the integral to get

                   E 11
                      i =t , i =t ,di =0,di =1
                          1 2    2 1      2
                                                             CT -CM
                                               = h(t1 , t2 )f1      (t1 , t2 ; z , m, v )

where                                                                                                2
                                                   v                -1- mv
                CT -CM
               f1      (t1 , t2 ; z , m, v )
                                    b(t1 )m 1 + (z (t1 ) + z (t2 ))        .
                                                   m
                                                     i
   Finally, we may observe a single censored spell, 1  = t1  0 and di        i
                                                                       1 = d2 = 1. The
probability of this event is

                                                                                   m   m2
                                                                                - v m v
                                                                         -zt1 e
                        E 11
                           i =t ,di =1 = h(t1 )                        e            v
                                                                                    2
                                                                                        d
                               1 1
                                                               0                (m /v )

This is the probability that the censoring time is t1 , ci = t1 , multiplied by the probability
      i
that 1  > t1 . Solve the integral to get

                            E 11
                               i =t ,di =1
                                   1 1
                                                    CT -CM
                                           = h(t1 )f2      (t1 , 0; z , m, v )

where                                                                                  2
                                                         v     -mv
                                 CT -CM
                                f2      (t1 , 0; z , m, v )
                                                 1 + zt1           .
                                                        m
    As in the CT-DM model, we use the probability of these three events to compute the
log-likelihood, taking advantage of the fact that each of the probabilities is multiplicatively
separable in the terms involving P to treat P as a nuisance parameter. This gives us the
portion of the likelihood that we are interested in:

                                                    N
                             CT -CM        1                 CT -CM i    i
                            L            =              log fdi +di (1 , 2 ; z , m, v ).                 (34)
                                           N    i=1
                                                                   1       2




As usual, we normalize m = 1.
    It is again straightforward to extend this analysis to the case where the frailty is a mixture
of K gamma distributions. Let {mk , vk , wk } denote the mean, variance, and weight on each
distribution. Then the likelihood is
                                         N              K
                                    1                              CT -CM i
                  LCT -CM =                   log             w k fdi +di
                                                                               i
                                                                          (1 , 2 ; z , mk , vk ) .       (35)
                                    N   i=1             k=1
                                                                       1       2



                      K
We again impose       k=1   wk = 1 and mk , vk , and wk all nonnegative to have a mixture model.


                                                              66
We also normalize K   k=1 wk mk = 1.
    Given any finite data set, we need to impose some restrictions on the baseline hazard
in order to maximize either likelihood (34) or (35). We assume that the baseline hazard is
piecewise constant and so z is piecewise linear.


E.3    Estimation of CT-CM Model in Stata
Stata has a built-in command for parametric estimation of the MPH model with multiple
spells (streg) and observable characteristics. Even though it is necessary to specify frailty
distribution as well as the functional form of the baseline hazard, one can use a full set of
dummy variables for duration to "over-ride" the parametric form of the baseline hazard and
estimate it flexibly. Since we are interested in estimating hazards up to duration T ¯, we have
only one dummy variable for spells longer than T   ¯. This dummy is equal to 1 if the measured
duration exceeds T ¯ + 1 and zero otherwise. We find that when we use two spells per product,
the maximum likelihood estimates in Stata coincide with the CT-CM model estimates with
one gamma distribution.


E.4    Results
We use IRI pooled sample data where we use first two spells per product. On this sample, we
estimate the baseline hazard using CT-CM, CT-DM as well as the discrete time model with
discrete measurement (DT-DM) using our GMM estimator. For the CT-CM and CT-DM
models we assume that the frailty distribution is either gamma or a mixture of gammas.
    Figure 13 shows the results. The hazards are normalized to be equal 1 at duration of
2 weeks. The blue line shows the baseline hazard estimated from the discrete time model
with discrete measurement (DT-DM) using GMM. The other solid lines show ML estimates
for the continuous time model, either with discrete measurement CT-DM(1) (black line)
or continuous time measurement CT-CM(1) (green line). The CT-DM(1) model, which
properly takes into account time aggregation, gives an estimate basically identical to our DT-
DM model. The CT-CM(1) baseline hazard is much lower, recovering little heterogeneity.
In general, CT-DM and DT-DM models are not the same and so we should not expect them
to deliver the same estimates. There is, however, an important special case when they are,
which is when the baseline hazard is constant.
    Heckman and Singer (1984) pointed out that imposing a specific distribution for the ML
estimation can bias the estimates of the baseline hazard. We investigate whether misspecifi-
cation of the frailty distribution can explain the difference between CT-CM(1) and DT-DM.
We cannot formulate the likelihood without choosing a frailty distribution but we can choose

                                              67
                               1
                             0.8



                             0.4
           Baseline Hazard



                             0.2       DT-DM
                                       CT-CM(1)
                                       CT-CM(2)
                             0.1       CT-DM(1)
                                       CT-DM(2)


                                   5   10   15   20   25      30 35     40   45   50   55   60
                                                           t in weeks


Figure 13: Baseline hazard estimated using different methods with two-spell IRI data, log
scale. The blue line is the discrete time model with discrete measurement (DT-DM). The
green lines correspond to continuous time with continuous time measurement (CT-CM),
where the frailty distribution is a single gamma distribution (green solid line) or a mixture
of 2 gamma distributions (green dashed line). The black lines correspond to the continuous
time, discrete measurement (CT-DM) model, where the frailty distribution is a single gamma
distribution (black solid line) or a mixture of 2 gamma distributions (black dashed line).

a more flexible distribution than a single gamma, for example a mixture of several gamma
distributions. In the CT-CM model, we could not find the second gamma distribution and
hence the estimates of CT-CM(1) and CT-CM(2) are identical. In the CT-DM model,
modeling the frailty as a mixture of distributions does not affect the baseline hazard and
CT-DM(1) and CT-DM(2) are very close. We therefore conclude that in this case, imposing
a specific functional form on the frailty distribution does not affect results.
    Our conclusion from this exercise is that the most important factor explaining the differ-
ence between the CT-CM and DT-DM model is the failure of CT-CM to deal with discrete
data.


F     Baseline Hazards for Product Categories
Here we present our results by product category. Figure 14 shows the baseline and KM
hazards and Figure 15 shows the average type estimated using the GMM conditions for

                                                           68
the MPH model. Figures 16 and 17 show the baseline hazard for price trends, b++ and
b-- respectively, estimated using the GMM conditions for the competing risks model with
observable characteristics.




                                          69
                       Yoghurt          Carb.Beverage      Salty Snack        Frozen Dinner         Cold Cereal
              0.32
     hazard   0.16
              0.08
              0.04
              0.02
              0.01
                         Beer               Milk              Soup            Spaghetti Sauce       Frozen Pizza
              0.32
              0.16
     hazard




              0.08
              0.04
              0.02
              0.01
                      Margarine           Hot Dog            Coffee            Toilet Tissue        Laundry Det.
              0.32
              0.16
     hazard




              0.08
              0.04
              0.02
              0.01
                     Facial Tissue      Peanut Butter      Mayonnaise         Mus&Ketchup           Paper Towel
              0.32
              0.16
     hazard




              0.08
              0.04
              0.02
              0.01
                     HH Cleaners         Toothpaste         Shampoo              Diapers             Sugar Sub.
              0.32
              0.16
     hazard




              0.08
              0.04
              0.02
              0.01
                      Deodorant          Toothbrush          Blades               Photo               Razors
              0.32
              0.16
     hazard




              0.08
              0.04
              0.02
              0.01
                       20   40     60      20   40    60    20   40      60      20   40       60     20   40      60
                     t in weeks          t in weeks        t in weeks           t in weeks          t in weeks

                                                      baseline           Kaplan-Meier


Figure 14: Kaplan-Meier and baseline hazards for individual product categories, IRI data,
log scale. Product categories are sorted by the number of spell pairs.


                                                           70
     average type           Yoghurt          Carb.Beverage      Salty Snack        Frozen Dinner         Cold Cereal
                    0.8
                    0.4
                    0.2
                    0.1
                              Beer               Milk              Soup            Spaghetti Sauce       Frozen Pizza
     average type




                    0.8
                    0.4
                    0.2
                    0.1
                           Margarine           Hot Dog            Coffee            Toilet Tissue        Laundry Det.
     average type




                    0.8
                    0.4
                    0.2
                    0.1
                          Facial Tissue      Peanut Butter      Mayonnaise         Mus&Ketchup           Paper Towel
     average type




                    0.8
                    0.4
                    0.2
                    0.1
                          HH Cleaners         Toothpaste         Shampoo              Diapers             Sugar Sub.
     average type




                    0.8
                    0.4
                    0.2
                    0.1
                           Deodorant          Toothbrush          Blades               Photo               Razors
     average type




                    0.8
                    0.4
                    0.2
                    0.1
                            20   40     60      20   40    60    20   40      60      20   40       60     20   40      60
                          t in weeks          t in weeks        t in weeks           t in weeks          t in weeks


Figure 15: Average type for individual product categories, IRI data, log scale. Product
categories are sorted by the number of spell pairs.



                                                                 71
              1.6    Yoghurt          Carb.Beverage      Salty Snack        Frozen Dinner         Cold Cereal

              0.8
     hazard

              0.4
              0.2
              0.1
              1.6      Beer               Milk              Soup            Spaghetti Sauce       Frozen Pizza

              0.8
     hazard




              0.4
              0.2
              0.1
              1.6   Margarine           Hot Dog            Coffee            Toilet Tissue        Laundry Det.

              0.8
     hazard




              0.4
              0.2
              0.1
              1.6 Facial Tissue       Peanut Butter      Mayonnaise         Mus&Ketchup           Paper Towel

              0.8
     hazard




              0.4
              0.2
              0.1
              1.6 HH Cleaners          Toothpaste         Shampoo              Diapers             Sugar Sub.

              0.8
     hazard




              0.4
              0.2
              0.1
              1.6   Deodorant          Toothbrush          Blades               Photo               Razors

              0.8
     hazard




              0.4
              0.2
              0.1
                     20   40     60      20   40    60    20   40      60      20   40       60     20   40      60
                    t in weeks         t in weeks        t in weeks           t in weeks          t in weeks


Figure 16: Baseline hazard b++ in the competing risks model for individual product cate-
gories, IRI data, log scale. Product categories are sorted by the number of spell pairs.



                                                          72
              1.6    Yoghurt          Carb.Beverage      Salty Snack        Frozen Dinner         Cold Cereal

              0.8
     hazard

              0.4
              0.2
              0.1
              1.6      Beer               Milk              Soup            Spaghetti Sauce       Frozen Pizza

              0.8
     hazard




              0.4
              0.2
              0.1
              1.6   Margarine           Hot Dog            Coffee            Toilet Tissue        Laundry Det.

              0.8
     hazard




              0.4
              0.2
              0.1
              1.6 Facial Tissue       Peanut Butter      Mayonnaise         Mus&Ketchup           Paper Towel

              0.8
     hazard




              0.4
              0.2
              0.1
              1.6 HH Cleaners          Toothpaste         Shampoo              Diapers             Sugar Sub.

              0.8
     hazard




              0.4
              0.2
              0.1
              1.6   Deodorant          Toothbrush          Blades               Photo               Razors

              0.8
     hazard




              0.4
              0.2
              0.1
                     20   40     60      20   40    60    20   40      60      20   40       60     20   40      60
                    t in weeks         t in weeks        t in weeks           t in weeks          t in weeks


Figure 17: Baseline hazard b-- in the competing risks model for individual product cate-
gories, IRI data, log scale. Product categories are sorted by the number of spell pairs.



                                                          73
