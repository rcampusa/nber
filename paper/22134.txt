                            NBER WORKING PAPER SERIES




                     RETHINKING PERFORMANCE EVALUATION

                                    Campbell R. Harvey
                                        Yan Liu

                                    Working Paper 22134
                            http://www.nber.org/papers/w22134


                    NATIONAL BUREAU OF ECONOMIC RESEARCH
                             1050 Massachusetts Avenue
                               Cambridge, MA 02138
                                    March 2016




A discussion with Neil Shephard provided the genesis for this paper - we are grateful. We
appreciate the comments of Yong Chen, Wayne Ferson, Juhani Linnainmaa, David Ng, Lubos
Pastor, and Luke Taylor. The views expressed herein are those of the authors and do not
necessarily reflect the views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

© 2016 by Campbell R. Harvey and Yan Liu. All rights reserved. Short sections of text, not to
exceed two paragraphs, may be quoted without explicit permission provided that full credit,
including © notice, is given to the source.
Rethinking Performance Evaluation
Campbell R. Harvey and Yan Liu
NBER Working Paper No. 22134
March 2016
JEL No. G10,G11,G12,G14,G23

                                          ABSTRACT

We show that the standard equation-by-equation OLS used in performance evaluation ignores
information in the alpha population and leads to severely biased estimates for the alpha
population. We propose a new framework that treats fund alphas as random effects. Our
framework allows us to make inference on the alpha population while controlling for various
sources of estimation risk. At the individual fund level, our method pools information from the
entire alpha distribution to make density forecast for the fund's alpha, offering a new way to think
about performance evaluation. In simulations, we show that our method generates parameter
estimates that universally dominate the OLS estimates, both at the population and at the
individual fund level. While it is generally accepted that few if any mutual funds outperform, we
find that the fraction of funds that generate positive alphas is accurately estimated at over 10%.
An out-of-sample forecasting exercise also shows that our method generates superior alpha
forecasts.


Campbell R. Harvey
Duke University
Fuqua School of Business
Durham, NC 27708-0120
and NBER
cam.harvey@duke.edu

Yan Liu
Texas A&M University,
College Station, TX 77843
y-liu@mays.tamu.edu
1       Introduction

In a method reaching back to Jensen (1969), most studies of performance evaluation
run separate regressions to obtain the estimates for alphas and standard errors. By
following this approach, each fund is treated as a distinct entity and has a fund specific
alpha. This is analogous to the fixed effects model in panel regressions where a non-
random intercept is assumed for each subject. We depart from the extant literature
by proposing a “random effects” counterpart of the performance evaluation model
(referred as the random alpha model ). In particular, we assume that fund i’s alpha
αi is drawn independently from a common distribution.
    There are many reasons for us to consider the random alpha model. First, the
fund data that researchers use (particularly, hedge fund data) are likely to only cover
a fraction of the entire population of funds. Therefore, with the usual caveats about
sample selection in mind, it makes sense to make inference on this underlying popula-
tion rather than just focusing on the available fund data. This is one of the situations
where a random effects setup is preferred over a fixed effects setup in panel regression
models.1
    Second, our random alpha model provides a structural approach to study the
distribution of fund alphas. It not only provides estimates for the quantities that
are economically important (e.g., the 5th percentile of alphas, the fraction of pos-
itive alphas), but also provides standard errors for these estimates by taking into
account various sources of parameter uncertainty, in particular the uncertainty in the
estimation of alphas.
    Currently, there are three main approaches to performance evaluation that rely on
return data alone, each having its own shortcomings. In the first method, fund-level
OLS are run in the first stage and hypothesis tests are performed in the second stage.
The regression t-statistics are obtained for each fund and used to test statistical
significance. Adjustments are sometimes used for test multiplicity. Recent papers
that follow this approach include Barras et al. (2010), Fama and French (2010),
Ferson and Chen (2015), and Harvey and Liu (2015a).
    There are several problems with this approach when it comes to making inference
on the cross-sectional distribution of fund alphas. First, it does not allow us to
extrapolate beyond the range of the t-statistics of the available data. For instance,
while the observed best performer might have a t-statistic of 3.0, we do not know
the fraction of funds that have a t-statistic exceeding 3.0 in the population. Second,
neither single tests nor multiple tests are useful when we try to make statements
about the properties of the population of alphas. For instance, one question that is
    1
     See, for example, Maddala (2001) and Greene (2003). Searle, Casella, and McCulloch (1992)
explore the distinction between a fixed effects model and a random effects model in more details.



                                                     1
economically important is: what is the fraction of investment funds that generate
a positive alpha? Under the hypothesis testing framework, one candidate answer
is the fraction of funds that are tested to generate a significant and positive alpha.
However, this answer is likely to be severely biased given the existence of many funds
that generate a positive yet insignificant alpha. Indeed, these funds are likely to be
classified as zero-alpha funds — funds that have an alpha that strictly equals zero
under hypothesis testing. In essence, equation-by-equation hypothesis testing treats
fund alphas as dichotomous variables and thus does not allow us to make inference
on the cross-sectional distribution of fund alphas.
    Our method allows us to estimate the underlying alpha distribution and make
inference on quantities that depend on the alpha population. Meanwhile, it provides
a density estimate for each fund’s alpha, making it possible to make inference on
individual funds and allowing us to answer the question: did the fund outperform?
We are also doing hypothesis testing at some level. Similar to the usual approach
to performance evaluation, time-series uncertainty in the estimation of alphas plays
a key role in our inference. However, in contrast to the standard approach, which
treats each fund as a separate entity and uses the individual t-statistic of alpha to
make inference, our method weights the fund specific time-series uncertainty relative
to the cross-sectional uncertainty for the alpha population, allowing us to efficiently
draw information from the entire alpha population to make inference on a particular
fund.
    The second approach to performance evaluation involves first running fund-level
OLS and then trying to estimate the distribution of the fitted alphas. By doing this,
it is possible to make inference on the alpha population. However, this approach
fails to take into account the various sources of estimation uncertainty, rendering the
inference problematic. For instance, Chen et al. (2015) try to model the cross-section
of fund alphas. Since the alphas are obtained from the first stage OLS, their model
cannot take into account the uncertainty in the estimation of the model parameters, in
particular, the uncertainty in the estimation of alphas. Such uncertainty is important
given the time-varying nature of fund returns and the fact that for some investment
styles standard factor models are only able to explain a small fraction of fund return
variance.2
   The third approach applies Bayesian methods to learn from the alpha population.
For example, Jones and Shanken (2005) impose a normal prior on the alpha for an
   2
      Chen et al. (2015) use the standard errors of the estimated alphas to control for the estimation
uncertainty of the alphas. However, these standard errors are also estimated quantities based on the
first stage OLS model and therefore have estimation uncertainty. Moreover, the estimation risk for
betas is also important and can materially change the estimates for alphas. Our structural estimation
approach allows us to jointly estimate the alpha distribution and the regression parameters for each
individual fund.




                                                        2
average fund and uses this to adjust for the performance of an individual fund.3 Con-
ceptually, their approach is closely related to ours in that we also try to make inference
on the alpha population. However, there are important differences. We build on the
frequentist approach and do not need to impose a prior distribution on the alpha pop-
ulation. We also allow fund alphas to be drawn from several subpopulations, which
builds on important insights in the recent literature on performance evaluation and
significantly enriches the structure of the alpha population.4 We provide a detailed
discussion of Bayesian methods and contrast them with our approach.
    By using portfolio holdings data, Cohen, Coval, and Pastor (2005) provide an
innovative approach that infers a manager’s skill from the skill of managers that
have similar portfolio holdings. Intuitively, if two managers have the same path of
holdings, their alpha estimates should be very close to each other. Cohen, Coval and
Pastor weight the cross-section of historical alpha estimates by the current portfolio
holdings to refine the alpha estimate of a particular fund. Their idea of learning
from the cross-section of managers is similar to ours. There are several differences
between their paper and ours. First, while their method learns through portfolio
holdings, we learn about a particular fund’s skill by grouping funds with similar alpha
estimates, after adjusting for the estimation uncertainty in the alpha estimation.
Second, while current holdings are informative about future fund performance, a
fund’s unconditional alpha estimate should depend on the entire history of holdings.
Finally, our method relies on the return data alone and is applicable to hedge fund
performance evaluation where we do not have holdings data for most funds.
    Our approach relies on the construction of a joint likelihood function that depends
on both the alphas and the betas. By finding the maximum-likelihood estimates
(MLE) of the model parameters, we make inference on the alpha distribution, con-
trolling for various sources of estimation uncertainty. We provide a unified framework
to assess performance, factor model estimation, and parameter uncertainty.
   Our empirical work begins with a simulation study that takes many realistic fea-
tures of the mutual fund data into account. We show that our method generates
parameter estimates that achieve both a low finite-sample bias and standard error,
dominating those that are generated under OLS. The superior performance of our
model applies to the alpha population as well as the individual funds. We also per-
form an out-of-sample exercise by estimating our model in-sample and forecasting
the alphas of individual funds out-of-sample. We show that our method provides a
substantial improvement over OLS with respect to forecasting accuracy.
   Application of the random alpha model leads to a much different answer to: What
proportion of mutual funds outperform? While the existing literature suggests few if
   3
      Other papers that apply Bayesian methods to study fund performance include Baks, Metrick,
and Wachter (2001), Pástor and Stambaugh (2002a,b), Stambaugh (2003), Avramov and Wermers
(2005), Busse and Irvine (2005), and Kosowski, Naik, and Teo (2007).
    4
      See Barras, Scaillet, and Wermers (2010), Ferson and Chen (2015), and Chen, Cliff, and Zhao
(2015).


                                                     3
any funds are deemed to outperform, our results suggest that over 10% of funds in
the 1983-2011 time period generate positive risk-adjusted performance.
    In the usual fund by fund regressions, about 29% of funds have positive alphas
and 0-1% have positive significant alphas. Our alphas are different than the OLS
alphas in that we have a structural model of the alpha distribution. Our method
allows us to shrink positive alphas towards zero through the cross-sectional learning
effect given that the median fund has a negative alpha. However, even after shrinking
positive alphas towards zero, we still find that 10.6% of funds generate positive alphas.
This 10.6% is accurately estimated in our framework — its 95% confidence bound is
[9.5%, 12.3%]. Notice that this 10.6% is a pure statement about the probability of
drawing a positive alpha from the alpha population, which, thanks to our structural
framework, can be estimated by the random alpha model. It does not apply to the
individual significance of an alpha from the perspective of hypothesis testing. Overall,
our higher proportion of funds with positive alphas is likely due to the fact that our
structural approach is more powerful in identifying small magnitude alphas.
    From a methodological perspective, we propose a new procedure to efficiently
estimate our structural model. It builds on and extends the standard Expectation-
Maximization algorithm that allows us to sequentially learn about fund alphas (which
are treated as missing observations) and estimate model parameters. Our method is
important in that it allows us to capture the heterogeneity in fund characteristics in
the cross-section. While we focus on performance evaluation in the current paper, a
contemporaneous paper (Harvey and Liu, 2016b) builds on the insight of our model
and studies the predictability of alpha. We expect our technique to be useful in other
applications as well.
    Our paper is organized as follows. In the second section, we present our model.
In the next section, we discuss the estimation method for our model and provide a
simulation study. In the fourth section, we apply our framework to mutual funds
to make inference on the distribution of fund alphas. Some concluding remarks are
offered in the final section.



2     Model

2.1    The Likelihood Function

For ease of exposition, suppose we have a T × N balanced panel of fund returns,
T denoting the number of monthly periods and N denoting the number of funds
in the cross-section. Importantly, balanced data is not required in our framework.
As we shall see later, both our model and its estimation can be easily adjusted for
unbalanced panel data.


                                                 4
   Suppose we are evaluating fund returns against a set of K benchmark factors.
Fund excess returns are modeled as

                       K
                       X
         ri,t = αi +         βij fj,t + εi,t , i = 1, . . . , N ; j = 1, . . . , K; t = 1, . . . , T,   (1)
                       j=1


where ri,t is the excess return for fund i in period t, αi is the alpha, βij is fund i’s
risk loading on the j-th factor fj,t , and εi,t is the residual.
      To simplify the exposition, let us introduce some notation. Let Ri = [ri,1 , ri,2 , . . . , ri,T ]0
be the excess return time-series for fund i. The panel of excess returns can be ex-
pressed as R = [R1 , R2 , . . . , RN ]0 . Let βi = [βi,1 , βi,2 , . . . , βi,K ]0 be the risk load-
ings for fund i. We collect the cross-section of risk loadings into the vector B =
[β10 , β20 , . . . , βN
                      0 0
                        ] . Similarly, we collect the cross-section of alphas into the vector
A = [α1 , α2 , . . . , αN ]0 . Let the standard deviation for the residual returns of the
i-th fund be σi . We collect the cross-section of residual standard deviations into the
vector Σ = [σ1 , σ2 , . . . , σN ]0 . Finally, let θ be the parameter vector that describes the
population distribution of the elements in A.
    Under the model assumptions, the likelihood function of the model is
                                                 Z
                        f (R|θ, B, Σ) =              f (R, A|θ, B, Σ)dA                                 (2)
                                                 Z
                                             =       f (R|A, B, Σ)f (A|θ)dA,                            (3)


where f (R, A|θ, B, Σ) is the complete data likelihood function (that is, the joint
likelihood of both returns R and alphas A), f (R|A, B, Σ) is the conditional likelihood
of returns given the cross-section of alphas and model parameters, and f (A|θ) is the
conditional density of the cross-section of alphas given the parameters that govern
the alpha distribution.
    Notice that the likelihood function of the model does not depend on the cross-
section of alphas (i.e., A). This is because, in our model, A is treated as missing data
and needs to be integrated out of the complete likelihood function f (R, A|θ, B, Σ).
However, once we obtain the estimates of the model parameters, the conditional
distribution of A can be obtained through the Bayes’ law:

                             f (A|R, θ̂, B̂, Σ̂) ∝ f (R|A, B̂, Σ̂)f (A|θ̂).                             (4)

This enables to us to evaluate the performance of each individual fund. Our ap-
proach to making inference on individual funds is distinctively different from current
methods. The two existing approaches, as mentioned previously, draw their infer-
ence based on either the time-series likelihood (i.e., f (R|A, B, Σ)) as in Barras et al.

                                                             5
(2010), Fama and French (2010), and Ferson and Chen (2015), or the cross-sectional
likelihood (i.e., f (A|θ)) as in Chen et al. (2015). Our method, as shown in (4),
combines information from both types of likelihoods, leading to a more informative
inference.
   Assuming that the residuals (i.e., εi,t ’s) are independent both across funds and
across time, the likelihood function can be written as:

                                           N
                                         Z Y
                    f (R|θ, B, Σ) =              f (Ri |αi , βi , σi )f (αi |θ)dA,             (5)
                                           i=1
                                         N
                                         YZ
                                     =           f (Ri |αi , βi , σi )f (αi |θ)dαi .           (6)
                                         i=1


Our goal is to find the maximum-likelihood estimate (MLE) of θ, which is the focus
of the paper, along with other auxiliary parameters (i.e., B and Σ) that govern the
return dynamics of each individual fund. To obtain an explicit expression for the
likelihood function, we assume that the residuals are normally distributed.
    Residual independence is not a key assumption for our model. When there is
residual dependency, the model will be misspecified. The likelihood function becomes
the quasi-likelihood function. Our QMLE still makes sense as the parameters govern-
ing the dependency structure are treated as auxiliary parameters with respect to the
goal of our analysis. Despite the model misspecification, in theory, the QMLE is still
consistent in that it gives asymptotically unbiased estimates. It will be less efficient
compared to the MLE of a correctly specified model. In our simulation study, we
consider residual dependency and quantify the loss in efficiency.



2.2     The Specification of the Alpha Distribution

What is a good specification for the alpha distribution, which we denote as Ψ? First,
the density of Ψ needs to be flexible enough to capture the true underlying distribution
for alpha. For instance, from both a theoretical and an empirical standpoint, two
groups of fund managers could exist, one group consisting of skilled managers, and
the other consisting of unskilled managers. Alternatively, we could think of five groups
of managers (i.e., top, skilled, neutral, unskilled, and bottom), similar to the five star
evaluation system used by Morningstar. These concerns suggest that the density of
Ψ should be able to display a multi-modal pattern, the density associated with each
mode capturing the alpha distribution generated by a particular group of managers.5
   5
     Our specification of Ψ makes it possible for the density to display a multi-modal pattern.
However, under certain parameterizations, a unimodal pattern is also possible. Our model estimation
will help us determine what pattern is most consistent with the data.



                                                        6
    On the other hand, having a flexible distribution does not mean that the distribu-
tion should be complicated. In fact, the very principle of regularization in statistics
is to have parsimonious models to avoid overfitting.6 Hence, without sacrificing too
much flexibility, we would like a distribution that is simple and interpretable.
    Driven by these concerns, we propose to model the alpha distribution by a Gaus-
sian Mixture Distribution (GMD) — a weighted sum of Gaussian distributions — that
is widely in science and medical research to model population heterogeneity. A one-
component GMD is just a standard Gaussian distribution. The two-component GMD
is a mixture of two Gaussian distributions and allows for considerable heterogeneity:

                                    Y = (1 − I) · Ye + I · Yh ,

where Y is the random variable that follows the GMD, and I, Ye and Yh are indepen-
dent random variables.7 I is an indicator variable that takes a value of 0 or 1, and it
is parameterized by π, which is the probability for it to equal 1 (i.e., P r(I = 1) = π).
Ye and Yh are normally distributed variables that are parameterized by (µe , σe2 ) and
(µh , σh2 ). To achieve model identification, we assume µe < µh .
    In our context, the model has a simple interpretation. With probability 1 − π, we
draw a manager from the population of unskilled managers (that is, I = 0), who on
average generate an alpha of µe (‘e’ = low alpha). With probability π, the manager
is drawn from the population of skilled managers (that is, I = 1), who on average
generate an alpha of µh (‘h’ = high alpha). The overall population of alpha is thus
modeled as the mixture of the two normal distributions.
    The two-component model can be easily generalized to multi-component models.
For a general L-component GMD, we order the means of its component distributions
in ascending order (i.e., µ1 < µ2 < · · · < µL ) and parameterize the probabilities of
drawing from each component distribution as

                                                                  L
                                                                  X
                                                         0
                              π = (π1 , π2 , . . . , πL ) ,             πl = 1.
                                                                  l=1



    With enough number of components in the model, the GMD is able to approx-
imate every density with arbitrary accuracy, the fact of which partly explains its
popularity. However, the model becomes more difficult to identify when the number
of components gets large.8 Therefore, between two models that produce similar likeli-
hood values, we prefer the parsimonious model. We rely on our simulation framework
   6
     See, for example, Bickel and Li (2006).
   7
     For applications of the Gaussian Mixture Distribution in finance, see Gray (1996) and Bekaert
and Harvey (1995).
   8
     See, for example, Figueiredo and Jain (2002) for a discussion on the identifiability problem for
a GMD and a potential solution.


                                                              7
to perform formal hypothesis testing on the candidate models and to select the best
model.9 The idea of using a mixture distribution to model the cross-section of fund
alphas has also been explored by the recent literature on performance evaluation, e.g.,
Chen et al. (2015). However, we offer a new path that takes the various sources of
estimation uncertainty into account.



2.3     The Identifiability and Interpretability of Ψ

The recent literature on investment fund performance evaluation attempts to group
funds into different categories. For example, Barras, Scaillet and Wermers (2010) and
Ferson and Chen (2015) assume that funds are drawn from a few subpopulations, with
“good” and “bad” managers coming from distinct subpopulations. Our parameteri-
zation of Ψ also bears this simple interpretation of a multi-population structure for
the alpha distribution. However, different from Barras, Scaillet and Wermers (2010)
and Ferson and Chen (2015), our structural estimation approach allows to take the
estimation risk into account when we classify funds into distinct performance groups.
Our empirical results show that our approach makes a material difference in the
classification outcome.
    Alternatively, we can think of Ψ as a parametric density to approximate the dis-
tribution of the population of fund alphas. The GMD is a flexible and widely used
parametric family to approximate unknown densities. As in most density estimation
problems, we are facing a tradeoff between accuracy and overfitting. In our appli-
cation, we pay special attention to the overfitting issue. In particular, we perform
a simulation-based model selection procedure to choose a parsimonious model. This
allows us to use the simplest structure — provided that it adequately models the
alpha distribution — to summarize the alpha population. This also makes it easier
to interpret the composition of the alpha population.
    To think about the identification of Ψ in our model, we first focus on an extreme
case. Suppose we have an infinitely long time-series for each fund so that there is no
estimation uncertainty in alpha. In this case, our model will force Ψ to approximate
the cross-section of “true” alphas. Suppose the left tail of the alpha distribution is
very different from the right tail. This suggests that a single component GMD is
probably not enough to capture the asymmetry in the two tails. A two-component
GMD may be a better candidate. Intuitively, we can first fit a normal distribution
for the alpha observations that fall below a certain threshold and another normal
distribution for the alpha observations that fall above a certain threshold (these two
   9
     Another benefit in using the GMD is that it reduces the computational burden for the estimation
of our model. In particular, when the components in A follow a GMD and the returns R follow a
normal distribution conditional on A, we show in Appendix A that the conditional distribution of the
components in A given R is also a GMD. This makes it easy for us to simulate from the conditional
distribution of A given R, which is the key step for the implementation of the EM algorithm that
we use to estimate our model.


                                                       8
thresholds are not necessarily equal). We then mix these two distributions in a way
that the mixed distribution approximates the middle part of the alpha distribution
well, that is, the alpha distribution that covers the non-extreme alphas.
    In reality, we have a finite return time-series. This introduces estimation uncer-
tainty in both the alphas and the other OLS parameters. As a result, instead of fitting
the cross-section of “true” alphas, our method tries to fit the cross-section of the dis-
tributions of the alphas, each distribution corresponding to the estimation problem of
the alpha of an individual fund and capturing estimation risk. However, our previous
discussion on the identification of Ψ when “true” alphas are available is still valid.
In particular, the parameters in Ψ are identified by capturing the departure of the
alpha distribution from a single normal distribution, only that this time the alpha
distribution is no longer the distribution of “true” alphas but a mixed distribution of
the estimated distributions of the alphas.
    More rigorously, the parameters in Ψ can be shown to be identified through high
order moments of the alpha population. For example, for a two-component GMD, its
five parameters can be estimated by matching the first five sample moments of the
data with the corresponding moments of the model.10 Despite its intuitive appeal, the
moments-based approach cannot weight different moments efficiently. Our likelihood-
based approach is able to achieve estimation efficiency. In our simulation study, where
we experiment with a two-component GMD, the model parameters seem to be well
identified and accurately estimated.



2.4     Discussion

The usual hypothesis testing framework with respect to making inference on the
population of fund alphas presents a number of challenges. While hypothesis testing
may be useful when we want to test the significance of a single fund, we need to
make adjustment for test multiplicity when the same test is performed on many
funds.11 Hypothesis testing is less useful when we try to make inference on the entire
alpha population. This is because hypothesis testing, by testing against a common
null hypothesis (e.g., alpha equals zero), essentially treats fund alphas as dichotomous
variables while more realistically they should be continuous. Our random alpha model
assumes that the true alpha is a continuous variable and provides density estimates
that can be used to evaluate each individual fund (similar to hypothesis testing) as
well as the alpha population.
   Hypothesis testing also places too much weight on the statistical significance of
individual alphas and overlooks their economic significance from a population per-
   10
      See Cohen (1967) and Day (1969) for the derivation of a two-component GMD based on the
method of moments approach.
   11
      For recent papers on investment fund performance evaluation that emphasize multiple hypoth-
esis testing, see Barras et al. (2010), Fama and French (2010), and Ferson and Chen (2015).


                                                     9
spective. For example, suppose we have two funds that both have a t-statistic of
1.5. One has an alpha of 20% (per annum) and the other has an alpha of 2% (per
annum). Should we treat them the same? We think not. The 20% alpha, albeit
volatile, tells us more about the plausible realizations of alphas in the cross-section
than the 2% alpha.12 Following the standard hypothesis testing framework, we not
only ignore the difference in magnitude between the two alphas, but we also classify
both funds as zero-alpha funds, causing an unnecessary loss of information regarding
the cross-sectional distribution of alphas.
    Our critique of the usual hypothesis testing approach is consistent with the recent
advances in statistics, and in particular in machine learning, that emphasize regu-
larization.13 In general, regularization refers to the process of introducing additional
information or constraints to achieve model simplification that often helps prevent
model overfitting. In the context of our application, we have a complex dataset given
the multidimensional nature of the cross-section of investment funds. The standard
hypothesis testing approach, by treating each fund as a separate entity and running
equation-by-equation (that is, fund-by-fund) OLS to obtain a separate t-statistic to
summarize its performance, does not reduce the complexity of the dataset. In con-
trast, our framework imposes a parametric distribution on the cross-section of alphas
and thereby substantially reduces the model complexity. It is unlikely to produce
a cross-sectional fit that is as good as the equation-by-equation OLS. However, the
better fit by the equation-by-equation estimation may reflect overfitting, which means
that the estimated cross-sectional distribution of alphas may be a poor estimate of
the future distribution. Our method seeks to avoid overfitting with the goal of getting
the best forecast of the future distribution.
    At the core of our method is the idea of extracting information from the cross-
section of funds. This information can be used both to make inference on the alpha
population and to refine our inference on a particular fund. To motivate the idea,
we use two examples throughout our paper. The first example is what we call a
one-cluster example. Suppose all the funds in the cross-section generate an alpha of
approximately 2% per annum and the standard error for the alpha estimate is about
4%. Since the t-statistics are all approximately 0.5 (=2%/4%), which is not even high
enough to surpass the single test t-statistic cutoff of 2.0, let alone the multiple testing
adjusted cutoffs, we would declare all the funds to be zero-alpha funds. Using our
method, the estimate of the mean of the alpha population would be around 2%. In
this case, we think our approach provides a better description of the alpha population
than the usual hypothesis testing approach. Declaring all the funds to be zero-alpha
funds ignores information in the cross-section.
    While the one-cluster example illustrates the basic mechanism of our approach, it
is too special. Indeed, a simple regression that groups all the funds into an index and
   12
      While some investment funds can use leverage to amplify gains and losses, they also face leverage
constraints. Therefore, 20% tells us more about the tails of the alpha distribution than 2%.
   13
      For recent survey studies on regularization, see Fan and Lv (2010) and Vidaurre, Bielza, and
Larrañaga (2013).


                                                        10
tests the alpha of the fund index will also generate a positive and significant estimate
for the mean of the alpha population. This motivates the second example, which we
call the two-cluster example. For the two-cluster example, suppose we have half of
the funds having an alpha estimate of approximately 2% per annum and the standard
error for the alpha estimate is about 4%. The other half have an alpha estimate of
approximately −2% per annum and also have a standard error of about 4%. Similar
to the one-cluster example, no fund is statistically significant individually. However,
we throw information away if we declare all the funds to be zero-alpha funds. Different
from the one-cluster example, if we group all the funds into an index and estimate the
alpha for the index fund, we will have an alpha estimate close to zero. In this case,
the index regression approach does not work for this example as it fails to recognize
the two-cluster structure of the cross-section of fund alphas. Our approach allows us
to take this cluster structure into account and make better inference on the alpha
population.
    The one-cluster and two-cluster examples are special cases of the alpha distribu-
tions that our framework can take into account. They correspond to essentially a
point mass distribution at 2% and a discrete distribution that has a mass of 0.5 at
−0.2% and 0.5 at 0.2%, respectively. Our general framework uses the GMD to model
the alpha distribution and seeks to find the best fitting GMD under a penalty for
model parsimony. It therefore extracts information from the entire cross-section of
alphas.
    After we estimate the distribution for the cross-section of alphas, we can use this
distribution to refine the estimate of each individual fund’s alpha. For instance, for
the one-cluster example, knowing that most alphas cluster around 2.0% will pull our
estimate of an individual fund’s alpha towards 2.0% and away from zero. Similarly,
for the two-cluster example, knowing that the alphas cluster at −2.0% and 2.0% with
equal probabilities will pull our estimate of a negative alpha towards −2.0% and a
positive alpha towards 2.0%, and both away from zero. In our general framework,
after we identify the GMD that models the alpha cross-section, we use it to update
the density estimate of each fund’s alpha, thereby using cross-sectional information
to refine the alpha estimate of each individual fund.
    We now discuss the details of our model. To see how our method takes esti-
mation
    QN uncertainty
            R               into account, we focus on the likelihood function in (6) (that
is, i=1 f (Ri |αi , βi , σi )f (αi |θ)dαi ). Suppose we already have an estimate of B
and Σ (e.g., the OLS estimate) and seek to find the estimate for θ. Notice that
f (Ri |αi , βi , σi ), the likelihood function of the returns of fund i, can be viewed as a
probability density on αi . In particular, under normality of the residuals, we have
                                                                 PT          0
                                                                  t=1 (rit −βi ft )
                                                         [αi −         T
                                                                                      ]2
                f (Ri |αi , βi , σi ) ≡ w(αi ) ∝ exp{−                                     },   (7)
                                                                 2σi2 /T




                                                    11
                                           0
where ft =  R [f1,t , f2,t , . . . , fK,t ] is the vectorR of factor returns at time t. Viewing in
this way, f (Ri |αi , βi , σi )f (αi |θ)dαi = w(αi )f (αi |θ)dαi is a weighted average of
f (αi |θ), with the weights (i.e., w(αi )) given in (7).
                √
    When σi / T is small, that is, when there is little uncertainty       PT
                                                                                      in the estimation
                                                                              (r −β 0f )
of αi , w(αi ) will be concentrated around its mean, i.e., t=1 Tit i t . In fact, when
σi → 0, i = 1, . . . , N and when      Q B andOLS     Σ are set at their OLS estimates, the likelihood
function in (6) converges to N             i=1 f (α̂i   |θ) — the likelihood function when the alphas
are exactly set at their OLS estimates. Therefore, ignoring the time-series uncertainty
in the estimation of the alphas, the likelihood function collapses to the likelihood
function constructed under the traditional approach, that is, running equation-by-
equation OLS first and then estimating the distribution for the fitted alphas. This
is the approach taken by Chen et al.               √ (2015). Our approach, by using a weighting
function w(αi ) that depends on σi / T , allows us to take the time-series uncertainty
in the estimation of the alpha into account.
    Moreover, the weighting function w(αi ) is fund specific, that  √ is, w(αi ) depends
on the particular level of estimation uncertainty for αi (i.e., σi / T ). Therefore, the
likelihood function in (6) allows different weighting functions for different funds. This
is important given the cross-sectional heterogeneity in estimation uncertainty across
funds, in particular across investment styles.
    Our approach offers more than just taking the estimation uncertainty for αi (i.e.,
    √
σi / T ) into account. As it shall become clear in later sections, our estimates of
both αi and σi2 not only rely on fund i’s time series, but also use information from
the cross-sectional distribution of the alphas. Hence, in our framework, the OLS t-
statistic is not an appropriate metric to summarize the significance of fund alphas.
Both its numerator and denominator need to adjust for the information in the alpha
population.
    On the other hand, our knowledge about the alpha population helps refine our
estimates of the risk loadings and the residual variances. Suppose we already have an
       R of θ and seek to estimate B and Σ. We again focus on the likelihood func-
estimate
tion f (Ri |αi , βi , σi )f (αi |θ)dαi , but instead view f (αi |θ) as the weighting function.
f (αi |θ) tells us how likely it is to observe a certain αi from a population perspective.
If αi is unlikely to occur over a certain range, the likelihood function will downweigh
this range relative to other ranges over which the occurrence of alpha is more plausi-
ble. In the extreme case when we have perfect knowledge about the alpha of a certain
fund (say, α̂i0 ), the likelihood function becomes f (Ri |α̂i0 , βi , σi ), essentially the likeli-
hood function for a linear regression model when the intercept is fixed. In general, the
MLE of βi and σi will be different from their unconstrained OLS estimates, reflecting
our knowledge about the alpha population.




                                                        12
3        Estimation

3.1      A New Expectation-Maximization Framework

A direct maximization of (6) is difficult. The size of the parameter space is large and
the likelihood function involves high-dimensional integrals. We offer a new implemen-
tation of the well-known Expectation-Maximization (EM) algorithm to facilitate the
computation.
    The idea of the EM algorithm is to treat the cross-section of alphas as missing
observations and iteratively update our knowledge of the alpha distribution and the
model parameters. With this approach, parameter estimates and learning about the
missing observations can be done sequentially. In the context of our application,
manager skill (i.e., alphas) are missing observations. In the “expectation” step of the
EM algorithm, for a given set of parameter values,14 we fill in the missing observations
with random draws from the conditional distribution of alphas given the parameter
values. We calculate the averaged value of the likelihood function across these random
draws. Essentially, at this step, we learn about manager skill to the best of our
knowledge of the model parameters and update the likelihood function accordingly.
In the “maximization” step of the algorithm, we maximize the updated likelihood
function, which takes into account our recently updated information about manager
skill. We obtain a new set of parameter estimates. These parameter estimates are
subsequently fed into another “expectation” step to start a new round of estimation.
The “expectation” step and the “maximization” step are performed iteratively to
arrive at the MLE.
   From a methodological perspective, our framework contributes to the literature
on EM algorithm by allowing heterogeneous funds in the cross-section and simulta-
neously estimating fund specific parameters and other structural parameters.15 In
particular, we allow both factor loadings and residual standard deviations to be fund
    14
      In our model, parameter values refer to fund specific factor loadings, residual standard devia-
tions, and parameters that govern the alpha population. The given set of parameter values could be
the initial set of parameters to start the entire algorithm, for which a reasonable choice is the factor
loadings and residual standard deviations from the equation-by-equation OLS estimates. It could
also be the optimization outcome following the intermediate step (i.e., the “maximization” step) of
the algorithm.
   15
      See Dempster, Laird, and Rubin (1977) for the original paper that proposes the EM algorithm.
See McLachlan and Krishnan (2007) for a more detailed discussion of the algorithm and its exten-
sions. Different from these papers on the EM algorithm, our method allows for heterogeneous factor
loadings and residual standard deviations in the cross-section. Chen, Cliff, and Zhao (2015) use a
modified EM algorithm to group funds into different categories. They employ a two-step estimation
procedure to first estimate the equation-by-equation OLS and then use the fitted alphas or the t-
statistics of alphas to classify funds. We put fund specific variables on an equal footing with other
structural parameters and simultaneously estimate the model parameters. As a result, we take into
account the estimation uncertainty for fund specific variables, including both the factor loadings and
the residual standard deviations.


                                                         13
  specific and update the entire cross-section of fund specific variables along with other
  structural parameters in the maximization step of the EM algorithm. This is an im-
  portant and necessary extension for the purpose of our application as we know there
  is estimation uncertainty as well as a large amount of heterogeneity in the risk-taking
  behavior of mutual funds. Failing to take either the heterogeneity or the estimation
  uncertainty into account may bias our estimate of the alpha population. On the other
  hand, allowing fund heterogeneity does not compromise the simplicity and the intu-
  itive appeal of the standard EM algorithm. We show that our new algorithm simply
  embeds a constrained OLS estimate for fund specific parameters (i.e., factor loadings
  and residual standard deviations) into an otherwise standard EM algorithm. This
  greatly reduces the computational burden of our model. We provide a comprehensive
  simulation study to demonstrate the performance of our estimation procedure.
     While we apply our framework to study fund performance in the current paper,
  we expect its general insight to be useful in other applications as well. Harvey and
  Liu (2016b) modifies the framework in this paper to study the predictability of alpha.



  4         Estimation Procedure

  We discuss the idea of the algorithm in the main text and describe the details in
  Appendix A. The following steps describe the procedure of the EM algorithm:

Step I Let G = [θ0 , B 0 , Σ0 ]0 denote the collection of parameters to be estimated. We
       start at some parameter value G (0) . A sensible initial choice is the equation-
       by-equation OLS estimate for B and Σ, and the MLE for θ based on the fitted
       OLS alphas.
Step II After the k-th iteration of the algorithm, suppose the model parameters are
        estimated as G (k) . We calculate the expected value of the log complete likelihood
        function, with respect to the conditional distribution of A given the current
        parameter values and R, i.e.,

                          L(G|G (k) ) = EA|R,G (k) [log f (R, A|G)],                                (8)
                                                       N
                                                       X
                                      = EA|R,G (k) [         log f (Ri |αi , βi , σi )f (αi |θ)].   (9)
                                                       i=1



            It is very likely that L(G|G (k) ) will not have a closed-form expression. But
            a variant of the EM algorithm — named the Monte Carlo EM algorithm —
            recommends replacing the expectation with the sample mean, where the sample
            is generated by simulating from the distribution of A|R, G (k) .16 We draw M (=
      16
           See Greg, Wei and Tanner (1990), McCulloch (1997), and Booth and Hobert (1999).

                                                             14
           100) A’s from the distribution A|R, G (k) and approximate the expectation in
           (9) by its sample counterpart:17

                                       M   N
                               (k)  1 XX
                          L(G|G ) =
                          b              [    log f (Ri |αim , βi , σi )f (αim |θ)].                (10)
                                    M m=1 i=1


                                                                        (k)
Step III We need to find parameter values that maximize L(G|G    b          ) and update the
                                   (k+1)
         parameter estimate as G         . This is usually not easy if the dimension of the
         parameter space is high. However, in our context, there is a simple solution. An
         inspection of (10) shows that (B 0 , Σ0 ) and θ can be updated separately. More
         specifically, (10) can be written as

                              N     M                                 M   N
                    (k)
                             X   1 X               m               1 XX
              L(G|G
              b         )=             log f (Ri |αi , βi , σi ) +           log f (αim |θ).        (11)
                             i=1
                                 M m=1
                                                                   M m=1 i=1


                             (k)
           Notice that L(G|G
                       b         ) splits into two parts, one involving B and Σ, and the other
                                                                (k)
           involving θ. We therefore can maximize L(G|G   b         ) by separately maximizing
           these two parts.

Step IV With the new parameter estimate G (k+1) obtained in Step III, we return back to
        Step II and start the (k+1)-th iteration. We iterate between Step II and Step
        III until the parameter estimates converge.


       The EM algorithm provides a tractable approach to find the MLE. It breaks
   the multi-dimensional optimization problem into smaller steps that are manageable.
   In theory, the EM estimator is guaranteed to converge to at least a local optimum
   of the likelihood function.18 It has been successfully applied to panel regression
   models with random effects when the the random effects do not follow a standard
   distribution.19 However, our model falls out of the realm of the standard application
   of the EM algorithm to panel regression models in that we allow heterogeneous risk
   loadings across funds. Therefore, the question remains as to whether the algorithm
   performs well in our application. We provide a detailed simulation study to evaluate
   the performance of the EM algorithm.
      17
          A larger number of M gives us a closer approximation to the expectation in equation 9. However,
   it also increases the computational burden. We find that M = 100 gives us an estimate of θ (notice
   that the estimates of B and Σ do not depend on M , as shown in Appendix A) that is very close to
   that under, say, M = 1, 000. This is due to the fact that we have a large cross-section of alphas so
   an insufficient sampling of the alpha distribution for individual funds do not have a large impact on
   the optimization outcome. We therefore set M = 100 to save computational time.
       18
          See Wu (1983) for the convergence properties of the EM algorithm.
       19
          See Chen, Zhang and Davidian (2002).




                                                          15
    We pay particular attention to the local optimum issue and construct a sequential
estimation procedure to maximize the chance that our estimator converges to the
global optimum. In particular, we first try a large number of randomly generated
vectors of parameters to start the algorithm. Under a mild convergence threshold,
we obtain many sets of initial parameter estimates. Some of these estimates may
correspond to a local optimum. We then select the top performers among these
estimates and apply tougher convergence thresholds to sequentially identify the global
optimum. See Appendix B for the details of the implementation of our algorithm.
    The steps of the EM algorithm make intuitive sense. They build on the idea
that our knowledge about the cross-section of alphas and the model parameters can
be sequentially updated. In Step I, we start with some initial parameter estimates,
possibly the standard OLS estimates. In Step II, given our starting estimates of
the model parameters, we calculate the expected value of the log likelihood function
conditional on the distribution of the alphas. An intuitive way to think about this
step is to replace A|R, G (k) with the best estimate of A given R and G (k) .20 By
doing this, we are trying to come up with our best guess of the missing alphas given
the return data and the model parameters. This is the step where we update our
knowledge about the cross-section of alphas given our current estimates of the model
parameters. In Step III, pretending that the estimated alphas in Step II are the true
alphas, we have complete data and can easily estimate the model parameters. This
is the step where we update our knowledge about the risk loadings and the residual
variances (i.e., B and Σ). It is through the iterations between Step II and Step III
that our estimates of the model parameters get refined.
    More insight can be gained into the EM algorithm by specifying the parametric
distribution Ψ. In Step II, assuming a Gaussian Mixture Distribution, Appendix
A shows that the conditional distribution of A given the current parameter values
(denoted as Ĝ) and R can be characterized as the distribution for N independent vari-
ables, with the i-th variable αi following a fund specific GMD that is parameterized
by θ̃i = ({π̃i,l }Ll=1 , {µ̃i,l }Ll=1 , {σ̃i,l
                                           2 L
                                               }l=1 ):

                                      σ̂l2                           σ̂i2 /T
                   µ̃i,l = (                      )ā i +     (                   )µ̂l ,                    (12)
                              σ̂l2 + σ̂i2 /T                    σ̂l2 + σ̂i2 /T
                     2                      1
                   σ̃i,l   =                                  ,                                             (13)
                             1/σ̂l + 1/(σ̂i2 /T )
                                   2

                                  π̂l φ(āi − µ̂l , σ̂l2 + σ̂i2 /T )
                   π̃i,l   = PL                                   2         2
                                                                                   , l = 1, 2, . . . , L,   (14)
                                 l=1   π̂ l φ(ā i −    µ̂ l , σ̂ l +    σ̂ i /T )

where
                                                    T
                                                    X
                                            āi ≡         (rit − β̂i0 ft )/T,
                                                    t=1

  20
       See Neal and Hinton (1998) for a more rigorous interpretation of the EM algorithm.


                                                                    16
and φ(µ, σ 2 ) is the density of the normal distribution N (0, σ 2 ) evaluated at µ.
    We can think of āi as the fitted alpha when βi is fixed at β̂i . It would be the
OLS estimate of alpha if β̂i were the OLS estimate of βi . The variance of the time-
series residuals is fixed at σ̂i2 . Taken together, āi and σ̂i2 /T can be interpreted as
the alpha estimate and its variance based on time-series information. On the other
hand, θ̂i = ({π̂i,l }Ll=1 , {µ̂i,l }Ll=1 , {σ̂i,l
                                              2 L
                                                  }l=1 ) is the current parameter vector governing the
GMD for the cross-sectional distribution of the alphas. Therefore, (12), (13) and
(14) update our estimates of the alphas by combining time-series and cross-sectional
information.
       We start with a L-component GMD specification for the alpha population. The
updated alpha distribution for each individual fund is also a GMD with the same num-
ber of components. However, the parameters that govern the GMD will be different
across funds. For each of L component distributions for the fund specific GMD, the
mean (i.e., µ̃i,l ) is a weighted average of the fitted time-series alpha and the original
                                                2
mean for the GMD, the variance (i.e., σ̃i,l       ) is the harmonic average of the time-series
variance and the original variance for the GMD, and the drawing probability (i.e.,
π̃i,l ) weights the original probability by φ(µ̄i − µ̂l , σ̂l2 + σ̂i2 /T ), which depends on the
distance between µ̄i and µ̂l (i.e., |µ̄i − µ̂l |) and the average of the variances σ̂l2 + σ̂i2 /T .
    Holding everything else constant, a lower time-series variance (i.e., σ̂i2 /T ) pulls
both the updated mean and variance closer to their time-series estimates, thereby
overweighing time-series information relative to cross-sectional information. On the
other hand, a smaller distance between µ̄i and µ̂l implies a higher drawing probabil-
ity (i.e., π̃i,l ), which means that compared to the original GMD, we are now more
likely to draw from the component distribution that has a mean that is closer to µ̄i .
Hence, we revise our estimate of the cross-sectional distribution based on time-series
information. The expressions in (12), (13) and (14) bear intuitive interpretations as
to how we update the alpha estimates based on both time-series and cross-sectional
information. This synthesis of information is important as it allows us to obtain
the most informative estimate of the A distribution, which is then used to evaluate
the likelihood function as in Step II of the EM algorithm. It also distinguishes our
method from existing approaches that only rely on one source of information, either
cross-sectional or time-series.
   Similar ideas that pool information across funds to make better inference on fund
performance have been proposed in Jones and Shanken (2005) and Cohen, Coval,
and Pastor (2005).21 Jones and Shanken rely on a Bayesian framework and impose
a normal prior on the alpha distribution. Imposing a one-component GMD in our
model, the posterior distribution of the alpha in their framework has an expression
that is similar to ours. For instance, similar to equation (12), the posterior mean for
the alpha of fund i in their framework is also a weighted average of the OLS alpha and
the prior mean of the alpha population. However, the important difference between
  21
    See Huij and Verbeek (2003) for a shrinkage approach that is similar to Jones and Shanken
(2005). Harvey and Liu (2015d) apply a similar idea to the selection of risk factors.


                                                       17
the two approaches is that the mean of the alpha population in Jones and Shanken
is fixed as a priori, whereas in our framework it is a freely estimated parameter.22 As
such, their model is better used to assess relative fund performance whereas ours can
be used both for relative and absolute performance evaluation. The same problem
exists in Jones and Shanken for the variance of each individual fund. On the other
hand, different from Cohen, Coval, and Pastor, which allows one to learn through
the portfolio holdings of managers, we learn about the skill of a particular manager
by grouping funds with similar alpha estimates, after adjusting for the estimation
uncertainty in the alpha estimation.
    Another way to interpret the formulas in equations (12)-(14) is to consider the
extreme case and assume that we have a single component GMD (that is, L = 1),
and moreover, its mean is zero (that is, µ̂1 = 0). In this case, we link the t-statistic
                                                                                   p
of fund i’s alpha (defined as µ̃i /σ̃i ) with its OLS t-statistic (defined as āi / σ̂i2 /T )
through:                                         s
                             µ̃i         āi             σ̂12
                                 =p 2          ×                  .                     (15)
                             σ̃i        σ̂i /T     σ̂12 + σ̂i2 /T
              q
                   σ̂ 2
Notice that σ̂2 +σ̂12 /T < 1 and the larger the time-series variance (that is, σ̂i2 /T ) is
                 1     i
relative to the cross-sectional variance (that is, σ̂l2 ), the smaller this number becomes.
Therefore, when the average alpha is zero inqthe population, we discount the OLS
                                                                σ̂12
t-statistic with a discount factor that equals            σ̂12 +σ̂i2 /T
                                                                        .   More time-series uncertainty
results in a harsher discount.
    The idea of discounting OLS t-statistic is consistent with the idea of multiple
testing adjustment, which has recently gained attention in both performance evalua-
tion and asset pricing in general.23 However, the mechanism in our model to deflate
t-statistics is different from standard multiple testing approaches. Our model, by
treating the alpha of an investment fund as random, takes into account the cross-
sectional uncertainty in alpha from a population perspective. Multiple testing meth-
ods, by treating the alpha as a fund specific variable (that is, a fixed effect), adjust
t-statistics by having a more stringent Type I error threshold. Despite the method-
ological difference, these two fundamentally different approaches arrive at the same
conclusion — we need to apply a “haircut” to the individual t-statistics of fund alphas.
   22
      Bayesians may suggest the use of uninformative priors, both for fund alphas and risk loadings.
However, Kass and Wasserman (1996) remind us that it is a dangerous practice to put faith in
any default choice of prior, especially when the sample size is small (relative to the number of
parameters). The sample size concern seems to be particularly relevant for the mutual fund and
hedge fund data in that we have a large cross-section of risk loadings to estimate. Any distortion
from the prior specifications of the risk loadings will feed into the estimation of the alpha population.
   23
      For recent finance applications of multiple hypothesis testing in asset pricing, see Barras et al.
(2010), Fama and French (2010), and Ferson and Chen (2015), Harvey, Liu, and Zhu (2016), and
Harvey and Liu (2015b,c).




                                                         18
   In Step III, we update our parameter estimates based on the conditional distri-
bution of the alphas. This is done is two steps. We first update the OLS parameters
except for the regression intercepts, and then update θ — the parameter vector that
governs the alpha population.
    For the update of the OLS parameters (see Appendix A), we derive analytical
expressions for the MLE of βi and σi2 . In particular, let m(αi ) = EA|R,G (k) (αi ) and
var(αi ) = V arA|R,G (k) (αi ) be the conditional mean and variance of αi . The MLE of
βi can be found as the regression coefficients obtained by projecting the return time-
series (i.e., {ri,t }Tt=1 ) onto the factor time-series (i.e., {ft }Tt=1 ), fixing the regression
intercept at m(αi ). Therefore, the MLE of βi in our model differs from the usual
OLS estimate in that the regression intercept is forced to equal m(αi ), the population
mean of αi given our current knowledge about the alpha distribution (i.e., A|R, G (k) ).
    The MLE of σi2 can be found by fixing βi at its MLE (i.e., β̆i ). In particular,
define
                              1X
                        ε2i ≡       (rit − β̆i0 ft − m(αi ))2 ,                (16)
                              T t=1

as the fitted residual mean squared error. Then the MLE of σi2 is given by

                                      σ̆i2 = ε2i + var(αi ).                                (17)

Notice that if we use (σi2 )M LE to denote the MLE of the residual variance for the
standard regression model that projects the time-series of returns (i.e., {ri,t }Tt=1 ) onto
{ft }Tt=1 , then we must have
                                    ε2i ≥ (σi2 )M LE

since the standard regression model seeks to minimize the sum of squared residu-
als without any parameter constraints. Therefore, two effects make the MLE of the
residual variance (i.e., σ̆i2 ) in our model larger than the standard model MLE (i.e.,
(σi2 )M LE ). First, ε2i is no less than (σi2 )M LE because we are considering a regression
model whose intercept is fixed at m(αi ). Second, there is uncertainty in αi as cap-
tured by var(αi ), which depends on the parameters given in (12), (13) and (14) of
the updated GMD (see Appendix A). Since, as discussed previously, the updated
GMD takes both time-series and cross-sectional information into account, var(αi )
also incorporates information about the cross-sectional dispersion of the alphas.
    These two effects implied by our model make intuitive sense as they allow us to
learn from both the mean and the variance of the alpha population. Additionally, the
learning effect is more pronounced in small samples and will go away when we have
a long enough time series of returns. This can be easily seen from the formulas of
our algorithm. When T goes to infinity and based on equation (12)-(14), the alpha
distribution collapses to the point mass at āi , which is the estimate based on time-
series information only. This implies that m(αi ) = āi and var(αi ) = 0. As a result,
our MLE of βi and σi converge to their OLS estimates. The fact that our method

                                                     19
implies differential adjustment to the alpha estimate between small and large samples
makes it an attractive method for performance evaluation, where a large fraction of
funds have short time series.
   For the update of θ, we seek for the parameter vector θ of a GMD that best
describes the alpha distribution. The optimization problem we are solving is:

                                       N     M
                                      X   1 X
                         θ̂ = arg max           log f (αim |θ),                    (18)
                                   θ
                                      i=1
                                          M m=1


where {αim }Mm=1 are randomly generated samples from the conditional distribution
of αi given R and G (k) . If there were just one fund in the cross-section, then θ̂ will
approximately equal the parameters that govern the GMD for a single fund that
are given in equation (12)-(14). With multiple funds in the cross-section, we have
multiple GMD’s, each one governing the alpha distribution of a particular fund. Our
method tries to find the best θ that describes the cross-section of GMD’s, which can
be viewed as a mixture distribution that chooses a fund with equal probability from
the cross-section of funds and, conditional on a fund being chosen, draws an alpha
from the fund’s GMD. Notice that this mixture distribution in our model is very
different than the alpha distribution in the equation-by-equation OLS model, where
it is simply the cross-section of fitted alphas. Our method allows us to capture the
estimation risk of each fund’s alpha and leads to a more informed estimate of the
alpha distribution.
    One concern about our model estimation is the large number of parameters to
estimate. Indeed, since we allow heterogeneity in fund risk loadings and residual
variances, the number of parameters grow almost proportionally with the number
of funds in the cross-section. However, the set of parameters that grow with the
number of funds are auxiliary parameters that govern the time-series dynamics of
each individual fund. The key parameter set of interest — θ that parameterizes Ψ —
does not change with the size of the cross-section. Intuitively, each additional fund
added to the cross-section, while creating a new set of parameters to estimate for its
time-series dynamics, will provide additional information for us to estimate θ. We
show in the simulation study that θ is accurately estimated when we have a large
cross-section.



4.1     A Simulation Study

4.1.1   Simulation Design

We provide a simulation study to examine the performance of the random alpha
model and compare it with the standard equation-by-equation OLS model.

                                                20
    We use mutual fund data as an example. For a detailed description of the mutual
fund data, see the next section where we apply our method to both mutual funds
and hedge funds. For our simulation study, we require that a fund has at least eight
months of return observations. This allows us to have enough time series to estimate
the factor model and is consistent with the existing literature (e.g., Fama and French,
2010, Ferson and Chen, 2015). Imposing this constraint, we have 3,619 funds in the
cross-section covering the 1983–2011 period. We obtain monthly returns for these
funds. Except for the restriction on sample length, we do not impose any further
restrictions on the data and we use all the funds in the data for our simulation study.
As a result, the cross-section for our simulation study is as large as that for the real
applications. This allows us to provide a more realistic evaluation of the performance
of our model.
     With this sample of mutual funds, we run equation-by-equation OLS based on
the full sample to obtain the initial estimates for B and Σ (i.e., B ∗ and Σ∗ ). We also
obtain the initial fitted alphas. We specify the number of component distributions
for the GMD and apply it to these fitted alphas and obtain the estimate for θ (i.e.,
θ∗ ). We collect these parameter estimates into G ∗ = [θ∗ , B ∗ , Σ∗ ]0 . G ∗ will be the
true underlying parameter vector that governs the data generating process. Special
attention is paid to funds that do not have enough data to cover the entire sample
period. In our simulations, we make sure that the simulated returns for these funds
cover the same time periods as the original fund data.
    We need to make a choice for the number of component distributions for the GMD
in our simulation study. Notice that our goal is not to find the best fitting GMD to
the OLS alphas, but to obtain a parameter set to initiate the simulation study. A
one-component GMD (i.e., a single normal distribution) is obviously the simplest
GMD one can specify, but it may be considered too special for a simulation study.
We therefore specify a two-component GMD — the simplest multi-component GMD
one can have.24
    Besides the number of components for the GMD, the particular value of G ∗ is not
essential for our simulation study. We could use an arbitrary set of parameters as the
underlying parameter vector that governs the data generating process. However, the
use of G ∗ makes our simulation study more realistic as we are using the actual fund
cross-section to extract the model parameters. It takes the cross-sectional heterogene-
ity in risk loadings into account as well as captures the multi-population structure for
   24
      The results of our simulation results to a large degree do not depend on the initial model we
choose for the GMD. We also try a three-component GMD. The results are qualitatively similar
in the sense that at the at the population level, the bias and variance (as measured by RMSE)
for the population parameters that are implied by the random alpha model are much smaller than
those that are implied by the equation-by-equation OLS, and that at the individual fund level, the
random alpha model generates alpha estimates that are more precise and less volatile than the OLS
model. We therefore expect the performance of the random alpha model to dominate that of the
equation-by-equation OLS under alternative parameter configurations.




                                                      21
a plausible set of alphas (i.e., the alpha estimates based on the equation-by-equation
OLS).
    Table 1 reports the summary statistics of the parameter vector θ∗ . The two-
component GMD separates the cross-section of fitted OLS alphas into two groups.
The first group has a mean that is mildly negative (−1.27%, per annum) and a
relatively small standard deviation (2.61%), and the second group has a mean that
is very negative (−6.44%, per annum) and a large standard deviation (16.95%). It is
less frequent for an alpha to fall into the second group as its drawing probability is
only 4.0%. Our model estimates are roughly consistent with the empirical evidence
documented by the literature using equation-by-equation OLS. A large fraction of
mutual funds exhibit alphas that are close to zero while a small fraction of funds
seem to significantly underperform.

             Table 1: Parameter Vector (θ∗ ) for the Simulated Model

                Parameter vector (θ∗ ) for the simulated model. We run equation-
                by-equation OLS for a cross-section of 3,619 mutual funds that at
                least have eight months of return observations for the 1983-2011
                period. We obtain the cross-section of fitted alphas. We then
                fit a two-component GMD on these alphas. µl and σl are the
                (annualized) mean and the (annualized) standard deviation for
                the l-th component normal distribution, and πl is the probability
                for drawing from the l-th component, l = 1, 2.


                           First component (l = 1)     Second component (l = 2)

                  µl (%)            −6.443                        −1.273
                  σl (%)            16.951                        2.606
                    πl               0.040                         0.960


    Based on G ∗ , we simulate D (=100) panels of fund returns, each one having the
same size as the original data panel.25 In particular, for each fund i, we randomly
generate its alpha based on the GMD that is parameterized by θ∗ . We then generate
ni N (0, (σi2 )∗ ) random variables, where ni is the sample size for fund i in the original
data. These random variables will be the simulated return residuals. Together with
the randomly generated alpha and the factor loadings βi∗ , these residuals enable us to
construct the simulated return series for fund i. To examine how residual correlation
affects our results, we allow the cross-section of residuals to be contemporaneously
correlated with a correlation coefficient of ρ.
  25
       We currently fix D at 100 to save computational time. We will later increase D to 1,000.




                                                       22
4.1.2    The Alpha Population

For each of the simulated return panel, we estimate our model, thereby obtaining D
sets of model estimates. Table 2 summarizes these estimates and compares with the
estimates of the standard OLS model, that is, we first run equation-by-equation OLS
and then fit a GMD for the cross-section of OLS alphas.26
    Based on the results in Table 2, the random alpha model stands out as superior to
the standard OLS model. In particular, its finite sample biases are uniformly smaller
(in absolute value) than those of the OLS estimates.
    We first focus on Panel A and examine the estimates for the means of the com-
ponent distributions. For the first component, for which the group mean is very
negative and the drawing probability is small (4.0%), the bias for the OLS model is
1.25% whereas the bias for the random alpha model is 0.20%. The estimation un-
certainty (RMSE) for the OLS model (2.31%) is also higher than the random alpha
model (1.44%). For the second component, which happens much more frequently than
the first group (drawing probability is 96.0%), the OLS model and the random alpha
model have similar performance with respect to the mean. Although the OLS model
is inferior than the random alpha model by making less precise and more noisy alpha
forecasts for individual funds (as we shall see later), when we pool the cross-section
of funds together to estimate the overall population mean, the noise at the individual
fund level cancels out and the OLS model does not seem to be significantly worse
than the random alpha model in terms of the estimation of the population mean.
This is particularly the case in the estimation of the second group as we have more
observations that fall into that group so the cancelation effect is stronger. For the
first group, for which we have fewer observations, the random alpha model appears
to be a better model in estimating the group mean.
    Turning to the estimates of the variances, the contrast in model performance
is starker. For example, we reduce the absolute values of the biases for the esti-
mates of the standard deviations of the component normal distributions from 25%
(= 4.27/16.95) and 33% (=0.86/2.61) to 0.4% (= 0.07/16.95) and 0.4% (= 0.01/2.61),
respectively. Therefore, the OLS model does not seem to be able to yield consistent
estimates of the standard deviations for the component distributions. Indeed, in our
simulations, the OLS model frequently overestimates the standard deviations of the
component normal distributions. This is not surprising since, by ignoring the time-
series uncertainty in the estimation of the fund specific alphas, it attributes all the
variation in the cross-section of the fitted alphas to the variation of the alpha popu-
lation.27 On the other hand, by taking both sources of uncertainty (i.e., time-series
  26
      Note that π1 + π2 = 1. However, we present both for completeness. Summary statistics for π1
and π2 in general will not sum up to one as we are averaging over the simulations.
   27
      The OLS model in our simulation study is the simplest two-stage model one can have by first
running equation-by-equation OLS and then fitting the cross-section of estimated alphas. Chen,
Cliff, and Zhao (2015) propose a generalization of this model by taking the estimated OLS variances
as given and feeding them into the estimation of the GMD. Their paper therefore partially takes the


                                                      23
uncertainty for individual fund returns and cross-sectional uncertainty for the alpha
population) into account, the random alpha model does not seem to be significantly
biased, and is able to estimate the parameters that govern the alpha distribution with
high precision.
    When return residuals are correlated and based on our approach, the estimates
for the means of the component normal distributions become more variable while
there are no material changes for the estimates of the other parameters. The more
variable estimates for the means are expected as we have less information in the
cross-section when return residuals are correlated. For example, compared to the case
when ρ = 0 in Panel A, when ρ = 0.2 in Panel B, the RMSE for µ1 increases from
1.44% to 1.70% for the random alpha model. The increased estimation uncertainty
is the price we have to pay for misspecifying the model likelihood function. However,
the increase seems small for reasonable levels of residual correlations, especially for
the random alpha model. Barras, Scaillet, and Wermers (2010) document that the
average pairwise correlation between the four-factor model residuals is 0.08. We think
our specification of ρ = 0.4 is a conservative upper bound for the average level of the
residual correlation.
    Overall, our results in Table 2 suggest that the OLS model, by first running
equation-by-equation OLS regressions to obtain the estimated alphas and then fit-
ting a parametric distribution to these alphas, is severely biased in estimating the
parameters that govern the cross-sectional alpha distribution. The random alpha
model, by explicitly modeling the underlying alpha distribution, seems to be able to
provide consistent and more precise parameter estimates.
    We have shown that the random alpha model produces superior parameter esti-
mates for the alpha population in comparison with the OLS model. Based on these
parameter estimates, we can calculate several important statistics that summarize the
alpha population. Not surprisingly, the random alpha model produces more accurate
and less volatile estimates for these statistics than the OLS model, as shown in Table
3.28
   Both methods generate similar results regarding the overall population mean of
the alpha distribution. Under the assumption of the GMD, the overall population
mean is simply the individual means of the two component distributions weighted
by the corresponding drawing probabilities. Given that the two methods generate
similar mean estimates for the second component of the GMD (as shown in Table 2)
and that it is more likely for an alpha to come from the second component (drawing
probability equals 96.0%), it is not surprising that the two methods have similar
time-series uncertainty into account. However, there are other important sources of estimation risk
that cannot be addressed in their framework, e.g., the estimation of risk loadings and the estimation
of residual variances themselves. Our structural approach allows us to take all of these sources of
estimation risk into account.
   28
      Given the similarity in model performance across difference levels of residual correlations, we
set the level of residual correlation at zero for the rest of the analysis in this section. We have tried
alternative correlation specifications and they do not change our results in any important way.

                                                         24
Table 2: A Simulation Study: Parameter Estimates for the Alpha Popula-
tion

      Model estimates in a simulation study. We fix the model parameters at G ∗
      (Table 1) and generate D sets of data sample. For each set of data sam-
      ple, we estimate our model using both the proposed random alpha model
      (“RA”) and the standard equation-by-equation OLS (“OLS”). ρ is the as-
      sumed level of correlation among the cross-section of return residuals. For
      a given parameter γ, let γd be the model estimate based on the d-th simula-
      tion run, d = 1, 2, . . . , D. “True” reports the assumed true parameter value
      given in G ∗ . “Bias” reports the difference between the average
                                                                   PD of the sim-
      ulated parameter estimates and the true value, that is, ( d=1 γd )/D − γ.
      “RMSE”qreports the square root of the mean squared estimation error,
                  PD                2
      that is,       d=1 (γd − γ) /D. “p(10)” reports the 10th percentile of the
      parameter estimates and “p(90)” reports the 90th percentile of the param-
      eter estimates. µl and σl are the (annualized) mean and the (annualized)
      standard deviation for the l-th component normal distribution, and πl is
      the probability for drawing from the l-th component, l = 1, 2.

                                       ρ=0               ρ = 0.2        ρ = 0.4
                                     RA      OLS        RA     OLS     RA     OLS
            µ1 (%)         Bias  0.020 1.252 −0.249 1.062 −0.103 1.141
       (True = −6.443)    RMSE   1.439 2.309   1.704 2.294   2.129 2.865
                          p(10) −8.322 −7.880 −8.958 −7.377 −9.633 −9.073
                          p(90) −4.614 −2.877 −4.477 −3.326 −4.063 −2.352
             σ1 (%)        Bias −0.067 4.268 −0.170 2.885 −0.111 3.887
        (True = 16.951)   RMSE   1.177 8.336   1.131 5.535   1.086 7.812
                          p(10) 15.323 14.914 15.327 15.315 15.383 15.517
                          p(90) 18.513 28.960 18.058 26.058 18.295 31.047
              π1           Bias     0.000   0.013 −0.001       0.014 −0.001   0.013
        (True = 0.040)    RMSE      0.005   0.018  0.005       0.018  0.006   0.018
                          p(10)     0.033   0.037  0.033       0.042  0.032   0.038
                          p(90)     0.046   0.069  0.045       0.068  0.047   0.070

            µ2 (%)         Bias  0.005 0.004 −0.045 −0.051   0.059 0.059
       (True = −1.273)    RMSE   0.059 0.064   0.530 0.577   0.874 0.956
                          p(10) −1.340 −1.348 −1.947 −2.021 −2.345 −2.507
                          p(90) −1.195 −1.186 −0.687 −0.661 −0.122 −0.022
            σ2 (%)         Bias −0.005      0.857 −0.058       0.790 −0.100   0.765
        (True = 2.606)    RMSE   0.065      0.861  0.094       0.794  0.150   0.777
                          p(10)  2.514      3.363  2.464       3.287  2.381   3.231
                          p(90)  2.680      3.590  2.623       3.495  2.683   3.570
              π2           Bias     0.000 −0.013        0.001 −0.014   0.001 −0.013
        (True = 0.960)    RMSE      0.005 0.018         0.005 0.018    0.006 0.018
                          p(10)     0.954 0.931         0.955 0.933    0.953 0.930
                          p(90)     0.967 0.963         0.967 0.958    0.968 0.962




                                                   25
             Table 3: A Simulation Study: Population Statistics

            Population statistics based on the model estimates in a simulation
            study. We fix the model parameters at G ∗ (Table 1) and generate
            D sets of data sample. For each set of data sample, we estimate our
            model using both the proposed random alpha model (“random al-
            pha”) and the standard equation-by-equation OLS (“OLS”). We
            then calculate several summary statistics for the alpha popula-
            tion for both models based on the estimated model parameters.
            “Mean” is the mean of the alpha distribution. “Stdev.” is the
            standard deviation of the alpha distribution. “Iqr.” is the inter-
            quartile range of the alpha distribution. “p10 ” is the 10 th per-
            centile of the alpha distribution. The other percentiles are sim-
            ilarly defined. “True” reports the population statistics based on
            the true model. “Estimate” reports the averaged estimate of the
            population statistics across the D sets of simulations. “RMSE”
            reports
                qPthe square root of the mean squared estimation error, that
                    D           2
            is,     d=1 (sd − s) /D, where s is the true statistic and sd is the
            estimated statistic based on the d-th simulated sample. Residual
            correlation is set at zero.

                                                 Random alpha       OLS

                    Mean(%)        Estimate            −1.470     −1.468
                 (True = −1.477)    RMSE                0.078      0.099
                    Stdev.(%)      Estimate             4.330       5.864
                  (True = 4.350)    RMSE                0.179       1.728
                     Iqr.(%)       Estimate             3.646       4.932
                  (True = 3.511)    RMSE                0.205       1.430
                      p5 (%)       Estimate            −6.126     −7.898
                 (True = −6.223)    RMSE                0.206      1.688
                     p10 (%)       Estimate            −4.884     −6.178
                 (True = −4.946)    RMSE                0.165      1.243
                     p50 (%)       Estimate            −1.295     −1.288
                 (True = −1.435)    RMSE                0.179      0.188
                     p90 (%)       Estimate             2.208       3.450
                  (True = 2.077)    RMSE                0.182       1.382
                     p95 (%)       Estimate             3.299       4.978
                  (True = 3.353)    RMSE                0.147       1.634



estimates for the overall population mean. However, the random alpha model has a
far better estimate of the dispersion of the alpha distribution than the OLS model. For
example, the dispersion for the underlying true model is 4.35%. The average estimate
for the random alpha model is 4.33%, and the RMSE is 0.18%. In contrast, the OLS
model overestimates the dispersion by 35% (= (5.86 − 4.35)/4.35) and the RMSE

                                                  26
is 1.73%. This difference in model performance is also reflected in the estimation of
the percentiles of the alpha population. For example, the average estimate for the
10 -th percentile based on the random alpha model is −4.88%, which is very close
to the true value (−4.95%). In contrast, the OLS model has an estimate that is
27%(= | − 6.18 − (−4.88)|/4.88) lower.


4.1.3   Individual Funds

Having discussed the simulation results regarding the alpha population, we now turn
to the inference of each individual fund. As mentioned previously, our method allows
us to make inference on each individual fund through equation (4). More specifically,
given a set of parameter estimates, the density forecast of an individual fund is given
by equations (12)-(14).
    In order to evaluate relative model performance, we need to choose a few statistics
that summarize a model’s forecasting accuracy at the individual fund level. We con-
centrate on two statistics. The first focuses on the point estimates. In particular, the
absolute deviation (AD) calculates the absolute distance between the alpha estimate
and the true alpha value. The second reflects estimation uncertainty. We calculate
the length of the confidence interval that is constructed to cover the true alpha value
with a certain probability. Notice that the t-statistic is not appropriate in our simula-
tion framework since, by assumption, fund alphas are nonzero. For example, suppose
the true alpha is 5% per annum for a certain fund and the point estimates based on
the random alpha model and the OLS are 4% and 7%, respectively. Additionally, sup-
pose the standard errors for the two models are the same. Clearly, the random alpha
model is a better model as it provides a more accurate point estimate without raising
the standard error. However, the OLS t-statistic will be higher than that based on
the random alpha model, suggesting a more significant finding under the OLS. This
is misleading. We therefore avoid the use of the t-statistic and separately show the
improvement of our model over the OLS for the numerator and the denominator of
the t-statistic, that is, the point estimate and the length of the confidence interval,
both of which can be easily obtained through the density forecast of the random al-
pha model. Ideally, a better performing model will imply both a more accurate point
estimate and a shorter confidence interval.
    Table 4 reports the results. In terms of point estimates, the average distance be-
tween the model estimate and the true alpha (as measured by the mean absolute de-
viation) is 1.29% for the random alpha model, which is about two-thirds (=1.29/1.85)
of that for the OLS model. In terms of estimation uncertainty, both methods gen-
erate confidence intervals that roughly achieve the pre-specified coverage rate (i.e.,
the probability for the confidence interval to contain the true alpha value) of 90%
and 95%. However, the length of the confidence interval generated under the random
alpha model is on average much shorter than that generated under the OLS model.
For instance, under 95% significance, the median length is 5.72% for the random al-

                                                27
pha model, which is 22%(= (7.34 − 5.72)/7.34) shorter than that of the OLS model.
Therefore, at the individual fund level, the random alpha model is able to generate
alpha estimates that are both more precise and less variable than the OLS model. Its
improvement over the OLS model seems substantial from an economic perspective.

                  Table 4: A Simulation Study: Individual Funds

   Summary statistics on model performance at the individual fund level. We fix the
   model parameters at G ∗ (Table 1) and generate D sets of data sample. For each set
   of data sample, we estimate our model using both the proposed random alpha model
   (“random alpha”) and the standard equation-by-equation OLS (“OLS”). For the
   random alpha model, given the parameter estimates, we use equations (12)-(14) to
   first construct the density forecast for each individual fund, and then obtain the point
   estimate and the confidence interval. For OLS, its point estimate is the estimate
   for the intercept, and its confidence interval is constructed using the point estimate
   and the standard error for the intercept. “Mean absolute deviation” is the averaged
   (across simulations) mean absolute distance between the estimated alpha and the
   true alpha for the cross-section of funds. “Stdev. of mean absolute deviation” is the
   averaged (across simulations) standard deviation of the absolute distance between
   the estimated alpha and the true alpha for the cross-section of funds. “Length, p”
   reports the averaged (across simulations) p-th percentile of the length of the 90%
   (or 95%) confidence intervals for the cross-section of funds. “Coverage probability”
   reports the averaged (across simulations) probability for the 90% (or 95%) confidence
   intervals to cover the true alpha values for the cross-section of funds. Other variables
   are similarly defined. Residual correlation is set at zero.

                                                                     Random alpha    OLS
                                  Mean absolute deviation(%)            1.289        1.851
                              Stdev. of mean absolute deviation(%)      1.196        3.336
    90% confidence interval             Length, p10 (%)                 2.932        3.297
                                        Length, p50 (%)                 4.793        6.161
                                        Length, p90 (%)                 6.938       12.461
                                      Coverage probability              0.882        0.893
    95% confidence interval             Length, p10 (%)                 3.496        3.929
                                        Length, p50 (%)                 5.719        7.341
                                        Length, p90 (%)                 8.327       14.848
                                      Coverage probability              0.938        0.944


   Overall, our results suggest that the random alpha model dominates the equation-
by-equation OLS, both in terms of modeling the alpha cross-section and in terms of
making inference on a particular fund’s alpha. Hence, under the assumption that
fund alphas can be viewed as coming from an underlying distribution, there seems to
be no reason to use the equation-by-equation OLS again for performance evaluation.




                                                             28
5        Results

5.1      Mutual Funds

We now apply our method to study mutual funds.29 We obtain the mutual fund data
used in Ferson and Chen (2015). Their fund data is from the Center for Research in
Security Prices Mutual Fund database. They focus on active, domestic equity funds
covering the 1984-2011 period. To mitigate omission bias (Elton, Gruber and Blake,
2001) and incubation and back-fill bias (Evans, 2010), they apply several screening
procedures. They limit their tests to funds that have initial total net assets (TNA)
above $10 million and have more than 80% of their holdings in stock. They also
combine multiple share classes. We require that a fund has at least eight months of
return observations to enter our test. This leaves us with a sample of 3,619 mutual
funds for the 1984-2011 period.30 We use the four-factor model in Fama and French
(1993) and Carhart (1997) as our benchmark model.


5.1.1     Parameter Estimates and Model Selection

A central issue is how we choose the number of components for the GMD that models
the alpha distribution in the cross-section. A more complex model (i.e., a model
with more component distributions) can potentially provide a better approximation
to the underlying alpha distribution, but may overfit, leading to a model that has
inferior forecasts out of sample. Standard model selection criteria (e.g., the Akaike
information criterion or the Bayesian information criterion) may not work well in
our context as they rely on asymptotic approximations. In our application, since
the number of parameters grow with the number of funds in the cross-section, it is
unclear what size of the cross-section would be regarded as large enough to warrant
asymptotic approximations. To have a rigorous model selection framework that takes
many aspects of our application into account (e.g., unbalanced panel, large number
of model parameters), we use a simulation-based model selection approach.31
    Consider two nested models M0 and M1 , with M1 being the bigger model. For
example, in our context, a GMD with a single component distribution will be nested
within a two-component GMD specification as, by setting the drawing probability
for one of the component distributions to zero, the latter collapses to the former.
To distinguish between M0 and M1 , we need a metric that evaluates relative model
performance. Given that our estimation relies on the MLE, a natural choice is the
    29
      In future research, we will apply our method to hedge fund returns.
    30
      We thank Yong Chen for providing us with the mutual fund data used in Ferson and Chen
(2015).
   31
      For a similar approach that bootstraps likelihood ratios to test the number of components in a
GMD, see Feng and McCulloch (1996).


                                                      29
likelihood-ratio statistic, which measures the difference in likelihoods between the two
candidate models. The likelihood-ratio statistic is also a key ingredient for many pop-
ular model selection criteria. In particular, let L0 (L1 ) be the value of the likelihood
function evaluated at the model estimates for M0 (M1 ). The likelihood-ratio statistic
(LR) is defined as:
                                LR = −2(log L0 − log L1 ).                            (19)

When the bigger model (i.e., M1 ) provides a substantial improvement over the smaller
model (i.e., M0 ), LR will be large and positive. Therefore, a large likelihood-ratio
statistic provides evidence against the smaller model.
    We simulate to find the cutoff value for LR. We first estimate M0 and obtain
its parameter estimates. Assuming M0 is the true model, we simulate normally dis-
tributed return innovations to generate D = 100 return panels, similar to what we do
in the simulation study. For each panel, we estimate both M0 and M1 , and calculate
the LR statistic. The 5 th percentile of these LR statistics will be used as the cutoff
for the LR statistic.
    We incrementally select the best performing parsimonious model. We first esti-
mate a one-component and a two-component model. Based on the parameter esti-
mates, the LR statistic between the two models is calculated to be 41.79 (×10−6 ).
Assuming that the one-component model is true and simulating the model based on
its parameter estimates, the 5 th percentile of the LR statistic is found to be 6.85
(×10−6 ), which is smaller than the realized likelihood statistic. Therefore, the two-
component model presents a significant improvement over the one-component model.
   Next, we estimate a three-component model. The LR statistics between the two-
component model and the three-component model is calculated to be 4.71 (×10−6 ).
This time, assuming that the two-component model is true and simulating the model
based on its parameter estimates, the 5 th percentile of the LR statistic is 14.30
(×10−6 ). Hence, the realized LR statistic is less than the simulated LR cutoff, sug-
gesting that we do not have enough evidence to discard the simpler two-component
model.
   Given the rejection of the three-component model, we do not need to further
consider the four-component model as its incremental contribution to the three-
component model is likely to be even smaller than the incremental contribution of
three-component model to the two-component model. We therefore select the two-
component model as the final model. It is the most parsimonious model that still
provides an adequate description of the cross-sectional distribution of fund alphas.
   Our finding of a two-group categorization of mutual fund managers is consistent
with the recent literature on mutual fund performance evaluation. For example,
Barras et al. (2010) use the false discovery approach to control for multiple testing
and find that 75% of the funds are zero-alpha funds and 24% are unskilled (i.e.,




                                                 30
significantly negative).32 The remaining 1% appear to be skilled but are statistically
indistinguishable from zero. We also find that a two-group classification is sufficient
to describe the universe of fund managers. In particular, unlike for underperformers,
we do not need a third component distribution to model outperformers.


5.1.2    Evaluating the Population of Fund Performance

Table 5, Panel A shows the parameter estimates for the GMD that describes the
alpha population. Panel B reports the estimates for several important population
statistics.
    The results in Panel A show substantial differences from the results in Table 1,
where we first obtain OLS alphas and then estimate the GMD that best describes the
fitted alphas. For example, in Table 1, the probability for drawing an alpha from the
“bad” group (4.0%) is much lower than the probability in Table 5, Panel A (28.3%).
However, conditional on drawing from this group, the alpha realization can be much
worse (i.e., negative) for Table 1 than for Panel A, since the “bad” group in Table 1
has both a lower mean (−6.44%, per annum) and a much higher standard deviation
(16.95%, per annum) than parameters that govern the “bad” group in Panel A. These
differences in parameter estimates reflect the differential treatment of estimation risk
between the equation-by-equation OLS and our model.
    Since the estimated GMD is composed of two component distributions, it may be
difficult to see how the differences in parameters for a single component affect the
overall distribution. A better way is to look at the population statistics, as shown in
Panel B of Table 5. There are again substantial differences between the results in Ta-
ble 1 and Panel B. First, by taking estimation risk into account, the overall population
mean in Panel B is −1.14% and its 95% confidence bound is [−1.19%, −1.08%]. This
estimate of the population mean is significantly higher than the estimate in Table 1
(−1.47%). Both the standard deviation and the inter-quantile range are also much
lower in Panel B than in Table 1. Therefore, by taking estimation risk into account,
we are able to obtain a more concentrated estimate for the alpha distribution than
the equation-by-equation OLS.
  32
     Barras et al. (2010) study 2,076 funds covering the 1975–2006 period. So their sample is
somewhat different from ours. However, given the 23 years overlap between our samples, we believe
their estimates should roughly apply to our sample as well.




                                                     31
         Table 5: The Alpha Population: Mutual Funds

Model estimates and population statistics for mutual funds. For a cross-section
of 3,619 mutual funds covering the 1983–2011 period, we estimate our model,
which is based on a two-component GMD specification for the alpha pop-
ulation. Assuming the estimated model is the true underlying model, we
simulate to find the percentiles of both the parameter estimates and the pop-
ulation statistics. Panel A reports the parameter estimates for the model.
µl and σl are the (annualized) mean and the (annualized) standard devia-
tion for the l -th component normal distribution, and πl is the probability for
drawing from the l -th component, l = 1, 2. Panel B reports the estimated
population statistics for the alpha distribution. “Mean” is the mean of the al-
pha distribution. “Standard deviation” is the standard deviation of the alpha
distribution. “Interquartile range” is the inter-quartile range of the alpha dis-
tribution. “10 th percentile” is the 10 th percentile of the alpha distribution.
The other percentiles are similarly defined. For both Panel A and B, “p(5)”
and “p(95)” report the 5 th and 95 th percentiles of the variable of interest
across simulations, respectively.

             Panel A: Parameter Estimates for the Alpha Population

                                     Estimate          p(5)            p(95)
                µ1 (%)                −2.277         −2.301            −1.948
                σ1 (%)                 1.513          1.424             1.654
                  π1                   0.283          0.280             0.330
                µ2 (%)                −0.685         −0.748            −0.894
                σ2 (%)                 0.586          0.569             0.615
                  π2                   0.717          0.670             0.720
             Panel B: Population Statistics for the Alpha Population

                                     Estimate          p(5)            p(95)
               Mean(%)                −1.135         −1.189            −1.075
        Standard deviation(%)          1.185          1.121             1.247
        Interquartile range(%)         1.142          1.085             1.234
          5 th percentile(%)          −3.689         −3.803            −3.445
          10 th percentile(%)         −2.862         −2.966            −2.652
          50 th percentile(%)         −0.894         −0.935            −0.851
          90 th percentile(%)          0.012         −0.016             0.096
          95 th percentile(%)          0.287          0.222             0.390
      Fraction of positive alphas      0.106          0.095             0.123




                                               32
    Figure 1 plots the density for the estimated alpha distribution as well as the
empirical density for the OLS estimates. The density for the OLS fitted alphas is
left skewed, indicating that there are more managers with large negative alphas than
there are managers with large positive alphas. Our model estimation picks this up by
having a separate component distribution that mostly covers negative alpha values.
Allowing multiple component distributions gives our model the flexibility to capture
the departure from normality in the data. Our results on model selection also show
that it is both necessary (i.e., statistically significant) and sufficient to have this
separate component distribution.
    Another important observation from Figure 1 is that our method does not try
to fit the OLS alphas. In fact, the overall density for the estimated GMD is more
concentrated around its population mean than the empirical density for the OLS al-
phas. This is because our method allows us to downweigh noisy alpha estimates of
individual funds when trying to make inference on the alpha population. Extreme
alpha estimates based on OLS are more likely to happen for funds with a short sam-
ple, more variable risk loadings, and/or more noisy return residuals. Our structural
approach allows us to take these sources of estimation risk into account.
    Our method allows us to make inference on important population characteristics
by deviating from the usual fund by fund hypothesis testing framework. For example,
we estimate the fraction of funds generating positive alphas to be 10.6%. This is in
contrast with Barras et al. (2010), who use the multiple testing approach and find
that less than 1% of funds generate a positive yet statistically insignificant alpha.
To interpret the difference between our results and those in Barras et al. (2010), we
need to bear in mind the difference between our method and the usual hypothesis
testing. Hypothesis testing, by testing against the null hypothesis that fund alphas
are zero, places more prominence on alpha equalling zero than alternative values.
Our method assumes that the alpha distribution is continuous and tries to back out
this distribution. It is therefore more appropriate to provide inference on population
characteristics.
    We will likely have more power in identifying alphas with a small magnitude in our
framework than hypothesis testing, provided that our parametric assumption of the
alpha distribution is a good approximation of reality. For example, for the one-cluster
example that we introduced previously, we assume that all the funds in the cross-
section generate an alpha of approximately 2% per annum and the standard error for
the alpha estimate is about 4%. Under the usual hypothesis testing approach, none
of the funds is statistically significant individually. Using our approach, the estimate
of the mean of the alpha population would be around 2%. For this example, we think
our approach provides a better description of the alpha population. Declaring all the
funds to be zero-alpha funds misses important information in the cross-section and
leads to a large loss in test power.
   Our results shed light on the important question of luck vs. skill for mutual
fund managers. For example, we estimate that the 95th percentile of the cross-


                                                33
                    Figure 1: Alpha Distribution for the Mutual Fund Population



                        Empirical density for OLS alphas
                        Fitted density for GMD, first component
             0.5        Fitted density for GMD, second component
                        Fitted density for GMD, overall


             0.4
 Frequency




             0.3



             0.2



             0.1



              0
              −12       −10        −8         −6         −4         −2          0   2   4   6   8
                                                           Annualized Alpha (%)




Density plots for the alpha population. For a cross-section of 3,619 mutual funds covering
the 1983–2011 period, we estimate our model, which is based on a two-component GMD
specification for the alpha population. The solid line shows the density for the estimated
GMD. The dotted line shows the density for the first component of the GMD that has
a negative mean. The dash-dotted line shows the density for the second component of
the GMD that has a positive mean. We also estimate the equation-by-equation OLS. The
dashed line shows the empirical density for the fitted OLS alphas.



section of alphas is 0.29% per annum. This number is accurately estimated as the
95% confidence interval is from 0.22% to 0.39%.33 Therefore, at least 5% of funds
are generating a positive alpha. From the hypothesis testing perspective, 0.29% is
not a big alpha and would likely to be overwhelmed by the standard error for a
typical fund. This would lead to an insignificant t-statistic and the conclusion that
almost no fund has skill, either from a single testing or a multiple testing perspective.
Our model offers a different way to interpret this 0.29%. Since we have a large
number of funds clustered around the 95th percentile in terms of fund performance,
pooling information across these funds should give us a good estimate of the average
performance among these funds. It is true that viewed in isolation, none of these
funds seems to display a significant alpha. But it would be misleading to conclude
      33
     In our framework, the 95th percentile of 0.29% is significant given that the lower bound of
the 95% confidence interval is above zero. However, our interpretation of significance should not be
confounded with the significance of individual funds that belong to the top 5% of alphas from the
perspective of the usual fund by fund hypothesis testing.


                                                                        34
that all of these funds are zero-alpha funds. A superior approach is to recognize
this population structure and explicitly model and estimate the underlying alpha
distribution that individual alphas are drawn from.
    Notice that our estimate (0.29%) of the 95th percentile of the alpha distribution
is substantially lower than the estimate (3.07%) based on equation-by-equation OLS.
This stems from the shrinkage effect that we mentioned previously. Two features
contribute to the shrinkage effect. First, since the median fund generates a nega-
tive alpha, cross-sectional learning forces us to pool alphas that are different from
the population mean towards the population mean. Second, large positive alphas
are usually generated with a higher level of residual standard deviation than large
negative alphas with the same magnitude. For example, the mean residual standard
deviation for funds with alphas above the 95th percentile (i.e., 3.07%) is 7.4% (per
annum) whereas the mean residual standard deviation for funds with alphas below
-3.07% is 5.9%. Intuitively, in a competitive market, it is more difficult to gener-
ate a positive alpha than a negative alpha with the same magnitude. As a result,
our method downweights the time-series information of funds with positive alphas
more aggressively than funds with negative alphas with the same magnitude. These
two features reinforce each other and generate the large discounts for positive alphas
within our structural framework.
    Linking to the existing literature, three approaches are proposed to evaluate mu-
tual fund performance. The first method uses the extreme test statistics and tries to
evaluate the significance of the best/worst funds, while controlling for test multiplic-
ity (see, for example, Kosowski et al. 2006, Fama and French, 2010, Harvey and Liu,
2015a). It is based on hypothesis testing and its null hypothesis is that each fund has
a zero alpha. It is designed to answer the question of whether there exists any fund
that significantly outperforms/underperforms and cannot further classify funds into
different performance groups. Using this approach, Kosowski et al. (2006) find that
there exist managers that significantly outperform. Refining the method in Kosowski
et al. (2006) to control for cross-sectional dependency, Fama and French (2010) find
no outperforming funds.
    The second approach tries to classify funds into broad categories. Papers that
follow this approach include Barras et al. (2010) and Ferson and Chen (2015). The
assumption of this approach is less stringent than the assumption under the previous
approach in that not all funds need to have a zero alpha. Certain funds can have
nonzero alphas and this approach tries to control the false discovery rate at 5%. Using
this approach, Barras et al. (2010) find that about 75% of funds are zero-alpha funds.
Ferson and Yong (2015) refine this method by allowing a non-zero probability for true
alphas to disguise themselves as zero, and find that 50% or fewer have zero alphas.
Neither paper finds evidence of funds that significantly outperform.
   From an methodological perspective, there are several important differences be-
tween our approach and the false classification (FC) method in Barras et al. (2010)
and Ferson and Chen (2015). The FC approach, being essentially a variant of the


                                                35
usual hypothesis testing framework, postulates that fund alphas can only take a small
number of values. While this offers a simplification of the inference problem, there is
no particular reason to think that fund alphas can only take a few specific values. As
a result, if a fund has a true alpha that is very different from these assumed values,
the estimation error by assigning this fund to any particular alpha group might be
large. Our approach allows us to realistically model the alpha population as following
a continuous distribution, thereby reducing the estimation error in the FC approach
where fund alphas are forced to take a small number of values.
    Second, the loss functions in our approach and the FC method are different. FC
relies on the multiple hypothesis testing approach and aims to strike a balance between
Type I (i.e., false discovery rate) and Type II error rates. Our maximum likelihood-
based approach tries to find the best parametric model that fits the data through
optimally weighting the likelihood from fitting the panel of return time-series and
the likelihood from fitting the cross-section of alphas. Hence, a material advantage
of our framework is that it allows us to take into account the parameter uncertainty
in estimating both fund alphas and other OLS parameters (i.e., factor loadings and
residual standard deviations) when we try to fit the cross-section of estimated alphas.
On the other hand, our structural approach also allows us to address the Type I error
concern that is the focus of the FC method. In particular, assuming all funds have
a zero alpha, if we estimate the alphas of a thousand funds, on average 25 funds
will appear to have a significant positive alpha from a single test perspective. In
our framework, these 25 funds will likely not have a significant positive alpha as the
posterior distribution of alpha weights the information from the time-series (which
is what the single test p-values are based on) by using information from the alpha
cross-section. Since our estimate of the mean of the alpha population will likely be
zero, learning across funds allows us to downwardly adjust the significance of each
individual fund, leading us to correctly declare the 25 funds as insignificant. Equation
(15) shows the precise formula for how our model adjusts the statistical significance
of individual funds when the alpha population has a zero mean.
   The third approach, as taken by our paper, is to treat alphas as continuous and
try to estimate the underlying distribution for alphas. We deviate from the usual
hypothesis testing approach in that we do not think an alpha of zero is any different
than an alpha of other value. Another salient feature of our model is that we take
various sources of estimation risk into account.
    One can think of the three approaches as following an order that tries to obtain
a finer and finer understanding of the alpha distribution. The first approach tries to
answer the very basic question of whether there exists any fund that has a non-zero
alpha. If the answer is yes, we proceed to the second approach to classify funds into
broad categories. Finally, viewing alphas as coming from an underlying distribution,
we use the third approach to provide a more precise description of this distribution.
    Fundamentally, our approach is different from the first two approaches that rely
on fund by fund hypothesis testing. In the context of performance evaluation, we have


                                                36
multiple funds in the cross-section so we have to perform multiple hypothesis tests.
However, compared to a single hypothesis test, there are many pitfalls associated
with performing multiple hypothesis tests, some of which are not well understood
by the literature. For example, the definition of test power is ambiguous given the
multi-dimensional nature of the hypothesis testing problem.
    Viewing fund alphas as coming from an underlying distribution, our model esti-
mates suggest that mutual fund managers are doing better than what people have
previously thought. We estimate that more than 10% of funds are generating a pos-
itive alpha. Our estimate is higher than those reported in the literature and likely
due to the fact our structural approach has more power in identifying small but
non-negligible alphas. If decreasing return to scale were the underlying economic
mechanism that drives alpha dynamics (Berk and Green, 2004), then small but pos-
itive alphas are usually associated with large funds. Given that larger funds have a
greater impact on the mutual fund industry than smaller funds, it would be a mistake
to label these funds as zero alpha funds from an economic perspective.


5.1.3   Individual Fund Evaluation: In-sample

We use our estimated model to make inference on the alphas of individual funds.
Given a set of parameter estimates, which use the information from the cross-section
of funds, we are able to refine the alpha estimate of an individual fund that is based
on time-series information alone, providing a more informative alpha estimate for an
individual fund. The intuition is given in the one-cluster and two-cluster examples
that we introduced previously. For example, for the two-cluster example, we assume
that half of the funds have an alpha estimate of approximately 2% per annum and
the standard error for the alpha estimate is about 4%. The other half have an alpha
estimate of approximately −2% per annum and also have a standard error of about
4%. Our model is able to recognize the two-cluster structure of the alpha population.
Knowing that the alphas cluster at −2% and 2% with equal probabilities, we will pull
the estimate of a negative alpha towards −2% and a positive alpha towards 2%, and
both away from zero.
   The formulas that provide density forecasts for individual funds are given in (12)-
(14). We compare our model with the equation-by-equation OLS both from an in-
sample fit and an out-of-sample forecasting perspective.
    Focusing on in-sample fitting, Figure 2 shows the density forecasts based on our
model for several exemplar funds. In particular, we rank funds by the t-statistics of
their OLS alpha estimates and choose several funds that represent different percentiles
of the cross-section of t-statistics.
   We see several noticeable differences between our density forecasts and the fore-
casts based on OLS. First, there is a shrinkage effect where the means of our forecasts
pull the OLS means towards the overall population mean. This is the cross-sectional

                                               37
learning effect that we mentioned previously. Knowing the alpha distribution of other
funds helps us make better inference on the alpha of a particular fund. Its OLS alpha
estimate based on time-series information alone needs to be adjusted for the informa-
tion in the cross-section. The shrinkage effect seems particularly strong for funds with
large positive OLS alphas. This is because we are more likely to observe a negative
alpha than a positive alpha for the alpha population. In addition, as we mentioned
previously, large positive alphas are usually associated with a larger residual standard
deviation than negative alphas with the same magnitude. The cross-sectional learning
effect therefore shrinks a positive alpha towards the population mean by more than
what it shrinks a negative alpha with the same magnitude towards the population
mean.
    Second, the dispersion for the density forecast of our model is uniformly lower
than that based on the OLS density forecast. This is consistent with our simulation
study where we show that the average length of the confidence interval based on our
method is substantially lower than that based on the OLS. Intuitively, our density
forecast combines information from both the cross-section and the time-series so it is
less disperse than the OLS density forecast, which only uses the time-series informa-
tion. (13) makes this intuition more precise. Suppose we have a single component
distribution for the GMD, then the variance of a fund’s alpha estimate following our
approach is always smaller than its variance based on time-series information alone.




                                                38
             Figure 2: Alpha Distributions for Individual Mutual Funds




                5th Percentile Fund                                          10th Percentile Fund

0.6                                                              0.6

      OLS implied
0.4                                     Random alpha             0.4
          density
                                        implied density
0.2                                                              0.2

 0                                                                0
       −10      −5              0             5       10               −10   −5         0           5   10

                     Median Fund                                             90th Percentile Fund

0.6                                                              0.6

0.4                                                              0.4

0.2                                                              0.2

 0                                                                0
       −10      −5              0             5       10               −10   −5         0           5   10


                                                     95th Percentile Fund

                                      0.6
                          Frequency




                                      0.4


                                      0.2


                                       0
                                            −10      −5        0         5        10
                                                    Annualized Alpha (%)




Density plots for individual funds. For a cross-section of 3,619 mutual funds covering
the 1983–2011 period, we estimate our model, which is based on a two-component GMD
specification for the alpha population. We also estimate the equation-by-equation OLS. We
rank the cross-section of funds based on the t-statistics of their OLS alpha estimates and
choose five funds whose t-statistics are the closest to the 5 th, 10 th, 50 th, 90 th, and 95 th
percentiles of the cross-section of t-statistics. Based on our model estimate, we plot the
density estimates for these funds using (12)-(14). We also plot the density estimates for the
OLS alphas.



                                                                  39
Table 6: Differences in Density Forecasts between OLS and the Random
Alpha Model

Differences in density forecasts between the OLS and the random alpha model. For a
cross-section of 3,619 mutual funds covering the 1983–2011 period, we estimate our model,
which is based on a two-component GMD specification for the alpha population. We also
estimate the equation-by-equation OLS. We group funds into several groups based on the
t-statistics of their OLS alpha estimates (denoted as tOLS
                                                        α   ). We calculate the average
difference in point estimates and confidence intervals between the random alpha model
and the OLS model. “Diff. in mean” reports the average difference in the mean forecast
between our model and the OLS. “% diff. in CI(90)” and “% diff. in CI(95)” report
the percentage differences in the length of the 90% and 95% confidence intervals between
our model and OLS, respectively. “# of funds” reports the number of funds for each
t-statistic category.

     tOLS
      α         Diff. in mean (%)    % diff. in CI(90)    % diff. in CI(95)   # of funds

 (−∞, −2.0)           3.391               −30.8%               −32.7%             523
 [−2.0, −1.5)         2.352               −42.1%               −42.5%             391
 [−1.5, 0)            0.688               −54.9%               −53.3%            1,640
 [0, 1.5)           −2.052                −63.8%               −61.7%             906
 [1.5, 2.0)         −3.774                −63.2%               −61.0%             98
 [2.0, ∞)           −5.722                −64.5%               −61.8%             61




    Finally, our density forecasts display non-normality, especially for funds with a
negative mean estimate for alpha. For funds with a positive mean estimate, although
the density looks unimodal, it is still a mixture distribution of two normal densities.
This shows the flexibility of the GMD specification to capture different shapes of a
probability density function. It also makes sense to have a non-normal density forecast
for individual funds if the the underlying distribution for the alpha population is non-
normally distributed. If this underlying distribution is more heavy-tailed and skewed
than the normal distribution, then the density forecasts for individual funds should
be able to reflect these non-normal features for the alpha population.
    Table 6 summarizes the differences in both point estimates and confidence intervals
between our model and the OLS. We group funds into different categories based
on their OLS t-statistics and calculate the average difference between our model
estimates and the OLS model estimates.
    Focusing on the mean estimates, we see the differential impact of the shrinkage
effect across different t-statistic groups. For example, for funds with an OLS t-
statistic below −2.0, on average our model pulls the OLS alpha estimate closer to
zero by 3.4% per annum. At the other extreme, for funds with significantly positive

                                                 40
OLS alpha estimates (i.e., OLS t-statistic > 2.0), we on average pull their alpha
estimates closer to zero by 5.7% per annum. The shrinkage effect seems to be more
pronounced for funds with large positive alpha estimates. This is attributable to, as
we mentioned previously, the differential treatment of positive and negative alphas by
the cross-sectional learning effect since we are more likely to observe a negative alpha
than a positive alpha for the alpha population and a large positive alpha is usually
generated with more uncertainty than a negative alpha with the same magnitude.
    For confidence intervals, our model is able to shrink the 90% and 95% confidence
intervals by at least 30% of the corresponding OLS confidence intervals. The reduc-
tions in estimation uncertainty seem substantial and are consistent with our results
in the simulation study (see Table 4), in which we show that the reduction in the
length of the confidence interval is not accompanied by a loss in the coverage rate.
In fact, we are able to achieve a pre-specified coverage rate (i.e., 90% or 95%) with a
much shorted confidence interval.
    The difference between Table 6 and Table 4 is that, unlike in the simulation study,
we no longer observe the true alpha for each individual fund. To better assess the
power of our approach, we perform an out-of-sample forecasting exercise in the next
section.


5.1.4   Individual Funds Evaluation: Out-of-sample

We perform an out-of-sample analysis of our method by splitting our data into an
in-sample estimation period and an out-of-sample holdout period. Notice that this is
not a true out-of-sample test as we have experienced the data. One way to interpret
our results is to assume that someone tries to assess the predictive power of our model
by following a simple strategy. She estimates our model at the end of the in-sample
period and uses the model estimates to forecast returns for the out-of-sample period.
We try to evaluate such a strategy from a historical perspective.
    Our sample runs from 1984 to 2011. We partition our sample into two parts,
with the first two-thirds as the estimation period and the last one-third as the out-of-
sample testing period. This way of partitioning the sample makes sure that we have
a long enough in-sample period to have a reasonable model estimate.
    For the in-sample period (i.e., 1984-2001), we estimate both our model and the
equation-by-equation OLS. Based on our model estimates, we construct a density
forecast for each fund’s alpha and use the mean of this density forecast to predict
fund alpha in the future. For OLS, we use its in-sample alpha estimate to forecast its
alpha in the future. The future alpha for each fund is obtained by running equation-
by-equation OLS for the out-of-sample period (i.e., 2002-2011). Notice that the out-
of-sample alpha is an estimated alpha and may not represent the true alpha.



                                                41
    For the in-sample period (i.e., 1984-2001), similar to our requirement for the full-
sample estimate, a fund needs to have at least eight monthly observations to be
considered in our estimation. This leaves us with 1,765 funds. Additionally, in order
to have a valid alpha proxy for the out-of-sample period, we again require a fund to
have at least eight monthly observations for the out-of-sample period. This further
requirement leaves us with 1,488 funds for the out-of-sample period. To sum up, our
in-sample estimation is based on 1,765 funds. Among these funds, 1,448 will be used
in out-of-sample testing.
    Table 7, Panel A shows the in-sample model estimates, and Panel B shows the
out-of-sample forecasting performance. Focusing on Panel A, there are noticeable dif-
ferences between the parameter estimates for the 1984-2001 period and for full sample
period (see Table 5). Compared with the estimates in Table 5, it is less likely (draw-
ing probability = 1.2%) to draw the alpha from the group with a very negative mean.
However, conditional on drawing from this group, the alpha dispersion (15.15%) is
much higher than the corresponding dispersion in Table 5 (1.51%). For the group
with a mildly negative mean, its mean (−0.35%) is higher than the corresponding
mean in Table 5 (−0.69%). At least two factors contribute to these differences in
model estimates. First, the average fund return (and OLS alpha) is significantly
higher for the in-sample period than for the full sample period. Second, compared to
the full sample estimation, we have fewer funds for the in-sample estimation. This
implies a lesser degree of learning across funds and may cause a larger estimate for the
dispersion of the alpha distribution. Despite these differences between the subsample
and the full sample estimation, it remains interesting to see how our model performs
out-of-sample.
    Panel B shows the out-of-sample forecasting results. We again group funds based
on their in-sample OLS t-statistics and present the average forecast error for each
group. Our model seems to provide a better alpha forecast for all except one group
of funds. The improvement of our model over the OLS is substantial. For example,
for the 610 funds that have an in-sample t-statistic between zero and 1.5, our model
is able to reduce the average forecast error from 5.54% to 2.61% (per annum). The
reduction in forecast error is more pronounced for funds with large (absolute) OLS
t-statistics. This is consistent with our finding based on the full sample estimation
that the shrinkage effect is stronger for funds with large (absolute) OLS t-statistics.
Across all groups of funds, the average percentage reduction in forecast error is 48%
(= (5.17% − 2.71%)/5.17%). Therefore, our model is able to provide substantially
better out-of-sample alpha forecasts compared to the OLS model.




                                                42
              Table 7: Out-of-sample Forecasts for Mutual Funds

In-sample model estimates (1984-2001) and out-of-sample forecasts (2002-2011) based on
OLS and the random alpha model. We partition the mutual fund data into two parts and
use the first part (1984-2001) for in-sample model estimation and the second part (2002-
2011) for out-of-sample testing. For the in-sample period, we require a fund to have at
least eight monthly observations. This leaves us with 1,765 funds. We estimate both our
model and the equation-by-equation OLS based on these 1,765 funds. Panel A shows the
parameter estimates for the random alpha model. µl and σl are the (annualized) mean and
the (annualized) standard deviation for the l -th component normal distribution, and πl is
the probability for drawing from the l -th component, l = 1, 2. For out-of-sample testing, we
additionally require a fund to have at least eight monthly observations for the out-of-sample
period. 1,448 out of the 1,765 funds satisfy this additional requirement. We evaluate the
out-of-sample forecasting performances of models based on these 1,448 funds. In particular,
based on the in-sample estimates for our model, we construct a density forecast for each
fund’s alpha and use the mean of this density forecast to predict fund alpha in the future.
For OLS, we use its in-sample alpha estimate to forecast its alpha in the future. The future
alpha for each fund is obtained by running equation-by-equation OLS for the out-of-sample
period. Panel B shows the forecasting results for both OLS and the random alpha model.
“tOLS
  α   ” denotes the in-sample t-statistic for the alpha estimate of the OLS model. “OLS
forecast error (%)” calculates the average absolute forecast error (i.e., the alpha forecast
based on the in-sample model minus the out-of-sample OLS alpha estimate) for OLS within
a group of funds. “RA forecast error (%)” calculates the average absolute forecast error for
the random alpha model within a group of funds.


 Panel A: In-sample Model Estimates, 1984–2001

                             Parameters                    Estimate
                                µ1 (%)                       −2.935
                                σ1 (%)                       15.146
                                  π1                          0.012
                                µ2 (%)                       −0.354
                                σ2 (%)                        1.065
                                  π2                          0.988

 Panel B: Out-of-sample Forecasts, 2002–2011

 In-sample, tOLS
             α         OLS forecast error (%)       RA forecast error (%)       # of funds
 (−∞, −2.0)                     6.613                          3.286                64
 [−2.0, −1.5)                   3.699                          3.089                75
 [−1.5, 0)                      2.916                          2.748                565
 [0, 1.5)                       5.542                          2.606                610
 [1.5, 2.0)                    10.469                          2.381                87
 [2.0, ∞)                      12.022                          2.766                87
 Overall                        5.165                          2.710               1,488




                                                   43
6     Other Issues

6.1    The Bayesian Approach

Bayesian methods have been applied to study fund performance. For example, Pástor
and Stambaugh (2002) and Kosowski, Naik and Teo (2007) use information in seem-
ingly unrelated assets to improve on the precision of performance estimates. Baks,
Metrick, and Wachter (2001) make inference on mutual funds’ alphas using informa-
tive priors about individual fund alphas. However, these studies focus on the inference
of each individual fund and cannot make inference on the overall alpha population.
Moreover, they do not allow us to learn from the entire alpha population to refine the
estimate of each individual fund’s alpha. As a result, there is a loss of information in
making efficient inference on fund alphas.
    Another concern for this strand of literature, as pointed out by Jones and Shanken
(2005) and Busse and Irvine (2006), is that the prior specification greatly affects the
predictive accuracy of Bayesian alphas. Many mutual funds in our sample have a
short time-series. The estimation uncertainty for the alpha seems high relative to its
point estimate, making the absolute value of the t-statistic small. In this situation,
a prior specification for alphas, no matter how uninformative it is, will likely weigh
heavily on the estimation of fund alphas. However, among all the prior specifications
one can choose, which one is the best? It is data mining (or model mining) in nature
if we chose the best prior that seems to fit the data, either in-sample or out-of-sample.
Therefore, although the Bayesian approach implies a shrinkage effect that is similar
to ours, the inherent subjectivity of the choice of the prior and the potentially large
impact of this choice on inference makes us hesitant to apply Bayesian methods to
performance evaluation. Our model offers a frequentist framework and does not rely
on the choice of a prior distribution.
     Among the papers that apply Bayesian methods, Jones and Shanken (2005) is the
closest to ours. They specify a normal prior for the alpha population in the cross-
section and allow diffuse and heterogeneous priors on the other OLS parameters (that
is, risk loadings and residual variances). As for most Bayesian models, the choice of
the diffuse prior, or any kind of uninformative prior, is not without consequences. In
particular, Kass and Wasserman (1996) show that it is a dangerous practice to put
faith in any default choice of prior, especially when the sample size is small (relative to
the number of parameters). The issue seems particularly relevant for the estimation
of risk loadings since we usually have a short time-series for fund returns (e.g., 24.5%
of mutual funds in our sample have no greater than 36 return observations). Any
distortion resulting from the prior specifications of the cross-section of risk loadings
will feed into the estimation of the alpha population. In contrast, our model follows
a frequentist framework and does not require any prior knowledge about parameters
of interest.


                                                 44
    The second advantage of our model is that it allows the use of the GMD to flexibly
model the alpha population. This is not a trivial extension of the single normal prior
assumption in Jones and Shanken (2005). As documented by Barras, Scaillet, and
Wermers (2010), Ferson and Chen (2015), and Chen, Cliff, and Zhao (2015), invest-
ment fund managers are better classified as coming from a few subpopulations. It is
therefore important to have a parametric specification for the alpha distribution that
is able to accommodate this subpopulation structure. The Bayesian framework per se
does not preclude a multi-population modeling of the alpha population. However, it
is not clear how to impose an uninformative prior while at the same time generating
a posterior distribution that features a multi-population structure. In addition, a
multi-population specification will likely force us to use non-conjugate priors, which
will significantly increase the computational burden of the Bayesian methods.



6.2    Sample Selection Bias

As with all approaches to performance evaluation, sample selection may bias our
results. On the one hand, studies that condition on fund survival overestimate fund
performance, see Brown, Ibboson, Ross (1992), Elton, Gruber, and Blake (1996),
and Carhart, Carpenter, Lynch, and Musto (2002). On the other hand, reverse-
survivorship may understate fund performance, see Linnainmaa (2013).
    We believe the bias will likely be smaller in our framework compared to the stan-
dard equation-by-equation OLS. For example, when there is reverse-survivorship bias,
a skilled fund may drop out of sample after having a bad (unlucky) shock. This
makes its in-sample alpha an understatement of its true population value. Hence,
using the equation-by-equation OLS, if we take the average of the cross-section of
fitted alphas, this average will underestimate the overall population mean if there
is reverse-survivorship bias. Funds that have a shorter history and a higher level of
idiosyncratic volatility are more likely to drop out after experiencing a bad shock. In
our framework, the importance of these funds is downwardly weighted. We know their
alpha estimates are more noisy so we put less weight on them in terms of learning
about the alpha population.



6.3    Random Alpha Model vs. Multiple Hypothesis Testing

By treating the alpha of an investment fund as random, our model takes into account
the cross-sectional uncertainty in alpha from a population perspective and helps de-
flate the fund alpha and its t-statistic, thereby imposing a more conservative inference
on the fund alpha. This is consistent with the idea of multiple testing that has been
applied to performance evaluation (see, Barras et al., 2010, Fama and French, 2010,
and Ferson and Chen, 2015) and to asset pricing in general (see, Harvey, Liu, and


                                                45
Zhu, 2016, and Harvey and Liu, 2015b,c). What is the connection between the two
methods?
    Suppose a researcher wants to test the effectiveness of a drug for all patients. The
researcher divides the sample into a female group and a male group and separately
tests the effectiveness of the drug. Since two tests have been tried, the chance of
finding a significant result is higher than the case with a one shot test. The researcher
can apply a multiple testing adjustment to these two tests so that the overall error
rate, however defined, is controlled at a pre-specified level. However, it does not
make sense to use the model in our paper since there are a limited number of gender
types in the population (i.e., we do not have hundreds of gender types). It is not
appropriate to view the means of the two groups — male and female — as coming
from an underlying distribution as there are only two samples from this distribution.
   The random alpha model applies when it is plausible to view the objects in the
cross-section as coming from a certain underlying population. For fund alphas, it
makes sense to think that the alphas for different funds are not independent of each
other since there are limited investment opportunities in the financial market and
funds compete with each other to generate alphas.34
    Despite their similarities in discounting fund alphas and their t-statistics, the two
models are fundamentally different. The multiple testing approach, and hypothesis
testing in general, treats the fund alpha as a dichotomous variable (that is, zero vs.
nonzero). Its objective function is also about controlling the probability or the frac-
tion of false discoveries, that is, a zero alpha fund being incorrectly classified as a
nonzero fund. On the other hand, the random alpha model preserves the continuity
of the alpha distribution. Its objective function is the goodness-of-fit of a parametric
model to the data. While the hypothesis testing framework is useful to roughly clas-
sify investment managers into different groups, the random alpha model is designed
to provide inference on the alpha population as well as refining inference about a
particular fund.



6.4     Misspecification of the Factor Model

Inference on fund alphas both at the population and at the individual fund level is
contingent upon the benchmark model being used. For instance, for mutual funds
performance evaluation, suppose the true benchmark model a five-factor model that
includes the Fama and French (1993) and Carhart (1997) four factors. Then misspec-
ifying the benchmark model as the four-factor model will likely lead to biased alpha
estimates, both for the alpha population and for the individual funds.35
  34
     See French (2008) for a similar argument on the competitiveness of the investment funds in-
dustry.
  35
     Harvey and Liu (2016a) examine the distortions in asset pricing tests when factor models are
misspecified.


                                                     46
     The concern about model risk is to some extent alleviated by considering the
random alpha model. Using the aforementioned five-factor model example, suppose
the fifth factor — the factor that is missing from the four-factor model — only applies
to a small fraction of funds. By using a misspecified four-factor model, the equation-
by-equation OLS will imply biased alpha estimates for this small fraction of funds.
Under the random alpha model, we are able to learn from the entire cross-section of
funds, including those that are not exposed to the fifth factor. As a result, the bias in
the alpha estimates for the small fraction of funds that are exposed to the fifth factor
is likely to be lower under the random alpha model than under the OLS model.
    When the benchmark model is missing a factor that applies to the majority of
funds, it is unlikely that any performance evaluation model performs well. One there-
fore needs to be cautious when trying to interpret the results of our paper. Our
inference relies on a pre-specified benchmark model for performance evaluation and
is sensitive to this choice. Harvey and Liu (2016b) explore this issue in greater detail
and provide alpha forecasts that take into account the choice of the benchmark model.
    Another possible misspecification of the factor model assumes a constant beta
while the true beta is time-varying. If fund-level characteristics and macroeconomic
variables can be used as instruments to model time-varying betas, then the static
factor model considered in our current paper would be missing factors that interact
these instruments with the benchmark factors. Harvey and Liu (2016c) study the
impact of beta variability for performance evaluation adapting the framework in this
paper to model dynamic risk exposures.



6.5    Time-varying Alphas

While our paper focuses on unconditional alphas, we can use fund-level characteris-
tics as instruments to study conditional alphas. Jones and Mo (2016) show that a
number of firm characteristics help forecast the cross-section of fund alphas. They
also find that the performance of many of these characteristics in explaining fund
alphas deteriorates through time. Our model can be easily extended to take into
account the predictability and the variation in predictability of fund returns by using
fund characteristics. Our framework allows one to make inference by drawing infor-
mation from the entire cross-section, which can potentially improve the out-of-sample
predictability of fund alphas. This is further explored in Harvey and Liu (2016b).



7     Conclusions

How do we evaluate investment fund managers? This is a question that bears im-
portant economic consequences for wealth management and capital reallocation. Our

                                                47
paper proposes a structural estimation approach to answer this question. Viewing
fund alphas as coming from an underlying population, our model first backs out the
distribution of the alpha population and then uses this distribution to refine the alpha
estimate for each individual fund. By drawing on information from the cross-section
of alphas, we show that our model is able to generate more accurate alpha estimates,
both in-sample and out-of-sample.
    The idea of our model is likely to be useful for other applications. Essentially, when
there is cross-sectional heterogeneity and when it is appropriate to view the effects
as coming from a certain population, we can apply our model to make inference on
both the population and the individual effects. Our use of the GMD is also flexible
enough to approximate a variety of parametric distributions for the population.
    Our framework can be extended along several important directions. First, while we
treat the alpha of a particular fund as fixed across time, we can relax this assumption
by allowing fund alphas to be time-varying. This allows us to study performance
persistence from a population perspective. Second, to capture the time variation
in risk loadings, we can allow betas to be time-varying as well, possibly through
the dependence of fund risk loadings on macroeconomic and financial variables. We
expect the random alpha model framework to be a fruitful area of future research for
performance evaluation and asset pricing in general.




                                                 48
References
Avramov, D., and R. Wermers. 2005. Investing in mutual funds when returns are
predictable. Journal of Financial Economics 81, 339–377.

Baks, K., A. Metrick, and J. Wachter. 2001. Should investors avoid all actively
managed mutual funds? A study in Bayesian performance evaluation. Journal of
Finance 56, 45–85.

Barras, L., O. Scaillet, and R. Wermers. 2010. False discoveries in mutual fund
performance: Measuring luck in estimated alphas. Journal of Finance 65, 179-216.

Bekaert, G., and C. R. Harvey. 1995. Time-varying world market integration. Journal
of Finance 50, 403–444.

Berk, J. B., and R. C. Green. 2004. Mutual fund flows and performance in rational
markets. Journal of Political Economy 112, 1269–1295.

Bickel, P. J., and B. Li. 2006. Regularization in statistics. Test 15, 271–344.

Booth, J. G. and J. P. Hobert. 1999. Maximizing generalized linear mixed model
likelihoods with an automated Monte Carlo EM algorithm. Journal of the Royal
Statistical Society. Series B. 61, 265–285.

Brown, S. J., W. Goetzmann, R. G. Ibbotson, and S. A. Ross. 1992. Survivorship
bias in performance studies. Review of Financial Studies 5, 553–580.

Busse, J. A., and P. J. Irvine. 2006. Bayesian alphas and mutual fund persistence.
Journal of Finance 61, 2251–2288.

Carhart, M. M. 1997. On persistence in mutual fund performance. Journal of Finance
52, 57–82.

Carhart, M. M., J. N. Carpenter, A. W. Lynch, and D. K. Musto. 2002. Mutual
fund survivorship. Review of Financial Studies 15, 1439–1463.

Chen, J., D. Zhang, and M. Davidian. 2002. A Monte Carlo EM algorithm for gen-
eralized linear mixed models with flexible random effects distribution. Biostatistics
3, 347–360.

Chen, Y., M. Cliff, and H. Zhao. 2015. Hedge funds: The good, the bad, and the
lucky. Journal of Financial and Quantitative Analysis, Forthcoming.

Cohen, A. C. 1967. Estimation in mixtures of two normal distributions. Technomet-
rics 9, 15–28.

Cohen, R. B., J. D. Coval, and L. Pástor. 2005. Judging fund managers by the
company they keep. Journal of Finance 60, 1057–1096.


                                              49
Day, N. E. 1969. Estimating the components of a mixture of normal distributions.
Biometrika 56, 463–474.
Dempster, A. P., N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from
incomplete data via the EM algorithm. Journal of the Royal Statistical Society 39,
1–38.
Elton, E. J., M. J. Gruber, and C. R. Blake. 2001. A first look at the accuracy of
the CRSP mutual fund database and a comparison of the CRSP and Morningstar
mutual fund databases, Journal of Finance 56, 2415–2430.
R. B., Evans. 2010. Mutual fund incubation. Journal of Finance 65, 1581–1611.
Fama, E. F., and K. R. French. 2010. Luck versus skill in the cross-section of mutual
fund returns. Journal of Finance 65, 1915-1947.
Fan, J., and J. Lv. 2010. A selective overview of variable selection in high dimensional
feature space. Statistical Sinica 20, 101–148.
Feng, Z. D., and C. E. McCulloch. 1996. Using bootstrap likelihood ratios in finite
mixture models. Journal of the Royal Statistical Society. Series B, 609–617.
Ferson, W., and Y. Chen. 2015. How many good and bad fund managers are there,
really? Working Paper.
Figueiredo, M. A., and A. K. Jain. 2002. Unsupervised learning of finite mixture
models. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 24, 381–
396.
French, K. 2008. Presidential address: The cost of active investing. Journal of Fi-
nance 63, 1537–1573.
Gray, S. F. 1996. Modeling the conditional distribution of interest rates as a regime-
switching process. Journal of Financial Economics 42, 27–62.
Greene, W. H. 2003. Econometric analysis. Pearson Education India.
Greg, C., G. Wei, and M. A. Tanner. 1990. A monte carlo implementation of the
EM algorithm and the poor man’s data augmentation algorithms. Journal of the
American Statistical Association 85, 699–704.
Harvey, C. R., Y. Liu, and H. Zhu. 2016. ... and the cross-section of expected returns.
Review of Financial Studies 29, 5–72.
Harvey, C. R., and Y. Liu. 2015a. Luck vs. skill and factor selection. in The Fama
Portfolio, John Cochrane and Tobias J. Moskowitz, ed., Chicago: University of
Chicago Press.
Harvey, C. R., and Y. Liu. 2015b. Lucky factors. Working Paper. Available at
http://ssrn.com/abstract=2528780.

                                               50
Harvey, C. R., and Y. Liu. 2015c. Backtesting. Journal of Portfolio Management 42,
13–28.

Harvey, C. R., and Y. Liu. 2015d. A structural approach to factor selection. Work
in Progress.

Harvey, C. R., and Y. Liu. 2016a. Factor model uncertainty and asset pricing tests.
Work in Progress.

Harvey, C. R., and Y. Liu. 2016b. Predicting alpha. Work in Progress.

Harvey, C. R., and Y. Liu. 2016c. Real-time performance benchmarking. Work in
Progress.

Huij, J., and M. Verbeek. 2007. Cross-sectional learning and short-run persistence
in mutual fund performance. Journal of Banking & Finance 31, 973–997.

Jensen, M. C. 1968. The performance of mutual funds in the period 1945-1964.
Journal of Finance 23, 389–416.

Jensen, M. C. 1969. Risk, the pricing of capital assets, and the evaluation of invest-
ment portfolios. Journal of Business 42, 167–247.

Jones, C., and J. Shanken. 2005. Mutual fund performance with learning across
funds. Journal of Financial Economics 78, 507–552.

Jones, C., and H. Mo. 2016. Out-of-sample performance of mutual fund predictors.
Working Paper.

Kass, R. E., and L. Wasserman. 1996. The selection of prior distributions by formal
rules. Journal of the American Statistical Association 91, 1343–1370.

Linnainmaa, J. T. 2013. Reverse survivorship bias. Journal of Finance 68, 789–813.

Maddala, G. S. 2001. Introduction to econometrics, John Willey and Sons Ltd. West
Sussex, England.

McCulloch, C. E. 1997. Maximum likelihood algorithms for generalized linear mixed
models. Journal of the American Statistical Association 92, 162–170.

McLachlan, G. and T. Krishnan. 2007. The EM algorithm and extensions. Vol. 382.
John Wiley & Sons, 2007.

Neal, R., and G. Hinton. 1998. A view of the EM algorithm that justifies incremental,
sparse, and other variants. In Jordan, M., editor, Learning in Graphical Models.
Kluwer Academic Press.

Pástor, L., and R. Stambaugh. 2002a. Mutual fund performance and seemingly
unrelated assets. Journal of Financial Economics 63, 315–349.


                                              51
Pástor, L., and R. Stambaugh. 2002b. Investing in equity mutual funds. Journal of
Financial Economics 63, 351-380.

Stambaugh, R. 2003. Inference about survivors. Unpublished working paper. Whar-
ton School, University of Pennsylvania.

Searle, S. R., G. Casella, and C. E. McCulloch. 1992. Variance components. John
Wiley & Sons, New York.

Vidaurre, D., C. Bielza, and P. Larrañaga. 2013. A survey of L1 regression. Inter-
national Statistical Review 81, 361–387.

Wu, C. F. J. 1983. On the convergence properties of the EM algorithm. Annals of
Statistics 11, 95–103.




                                             52
A       Implementing the EM Algorithm

A.1      Step II : Characterizing f (A|R, G (k) )

Using Bayes’ law, we have:

                              f (A|R, G (k) ) ∝ f (R|A, G (k) )f (A|G (k) ).                         (A.1)

Given the independence of the residuals and the αi ’s, the right-hand side of (A.1) is
the product of the likelihoods of all funds, i.e.:

                                                          N
                                                          Y
                 f (R|A, G (k) )f (A|G (k) ) =                  f (Ri |αi , G (k) )f (αi |G (k) ).
                                                          i=1


Therefore, to characterize f (A|R, G (k) ), it is sufficient for us to determine f (Ri |αi , G (k) )f (αi |G (k) )
for each fund i. For ease of exposition, we use G and G (k) interchangeably to denote
the known parameters at the k-th iteration.
    Under normality, we have
                                                          PT
                                     (k)                      t=1 (rit    − αi − βi0 ft )2
                     f (Ri |αi , G         ) ∝ exp{−                                       },
                                                                          2σi2
                                                                     PT          0
                                                                      t=1 (rit −βi ft )
                                                          [αi −             T
                                                                                          ]2
                                            ∝ exp{−                                            },
                                                                     2σi2 /T

which can be viewed as the probability density
                                       PT       for αi . Moreover, it can be recognized
                                                        0                     2
as a normal density with mean āi ≡       t=1 it − βi ft )/T and variance σi /T , i.e.,
                                             (r
N (āi , σi2 /T ).
   By assumption, f (αi |G (k) ) is the density for a GMD that is parameterized by
θ = ({πl }N          N       2 N
          l=1 , {µl }l=1 , {σl }l=1 ). It can be shown that f (Ri |αi , G
                                                                          (k)
                                                                              )f (αi |G (k) ) — the
product of a normal density (i.e., N (āi , σi2 /T )) and the density for a GMD — is also
a density for a GMD, whose parameters are given by

                                   σl2                         σi2 /T
                  µ̃i,l = (                   )ā i +    (                )µl ,
                             σl2 + σi2 /T                  σl2 + σi2 /T
                    2                   1
                  σ̃i,l   =                              ,
                            1/σl + 1/(σi2 /T )
                                 2

                                πl φ(āi − µl , σl2 + σi2 /T )
                  π̃i,l   = PL                              2       2
                                                                           , l = 1, 2, . . . , L,
                               l=1  π l φ(ā i −    µ l , σ l +   σ i /T )


                                                                53
where φ(µ, σ 2 ) is the density of the normal distribution N (0, σ 2 ) evaluated at µ.
   Therefore, f (A|R, G (k) ) can be characterized as the density for N independent
variables. The i-th variable follows a GMD that is parameterized by

                              θ̃i = ({π̃i,l }Ll=1 , {µ̃i,l }Ll=1 , {σ̃i,l
                                                                      2 L
                                                                          }l=1 ).


                                                    PN    1
                                                                 PM                   m
A.2      Step III : Maximizing                        i=1 M           m=1 log f (Ri |αi , βi , σi )

Given the independence of the residuals, we can find the MLE of B and Σ fund by
fund. In particular, the log-likelihood for fund i is given by

                  M                                  M   T
               1 X                                1 XX
                     log f (Ri |αim , βi , σi ) =           log f (rit |αim , βi , σi ),              (A.2)
               M m=1                              M m=1 t=1

through which we can find the MLE of βi and σi . Under the normality assumption,
it can be shown that the right hand side of (A.2) can be written as

   M     T                                                  T
 1 XX                                     T             1 X
           log f (rit |αim , βi , σi ) = − log(2πσi2 )− 2 [ (rit −βi0 ft −ᾱi )2 +T (αi2 −ᾱi2 )],
M m=1 t=1                                 2            2σi t=1
                                                                                          (A.3)
where ᾱi and αi2 are defined as:

                                       M                M
                                    1 X m            1 X m 2
                            ᾱi =         αi , αi2 =      (α ) .
                                    M m=1            M m=1 i

An inspection of (A.3) shows that the MLE of βi and σi can be found sequentially.
We find the MLE for βi first. Notice that the MLE β̂i is essentially the estimates
of the slope coefficients for the OLS that regresses the time-series of {rit − ᾱi }Tt=1 on
{ft }Tt=1 . As a result, we have

                                         β̂i = (F 0 F )−1 F 0 Yi ,

where                                                                            
                                          f1                          ri,1 − ᾱi
                                         f2                        ri,2 − ᾱi    
                        F(T ×K) =               , Yi(T ×1) =                     .
                                                                                 
                                           ..                              ..
                                           .                             .       
                                         fT                           ri,T − ᾱi



                                                             54
Fixing βi at its MLE, we take the first-order derivative of (A.3) with respect to σi2 to
obtain the MLE for σi2 , i.e.,

                                   1X
                         σ̂i2 =          (rit − β̂i0 ft − ᾱi )2 + (αi2 − ᾱi2 ).
                                   T t=1


Define ε̂2i ≡ T1 t=1 (rit − β̂i0 ft − ᾱi )2 and Vd  ar(αi ) = (αi2 − ᾱi2 ). The MLE of σi2 can
                P
be expressed as
                                      σ̂i2 = ε̂2i + Vdar(αi ).                            (A.4)

Note that {αim }M
                m=1 are simulated data. When the size of the simulated data is large,
the sample moments in (A.4) will be close to the population moments. We therefore
replace the sample moments with their population moments. This helps us obtain
the exact analytical solutions for βi and σi when the conditional distribution of A is
given in Appendix A.1. In particular, the exact MLE for βi is:

                                           β̆i = (F 0 F )−1 F 0 Ỹi ,

where Ỹi = [ri,1 − m(αi ), ri,2 − m(αi ), . . . , ri,T − m(αi )]0 and m(αi ) = EA|R,G (k) (αi ) =
PL                                     2
  i=1 π̃i,l µ̃i,l . The exact MLE for σi is:


                                  1X
                        σ̆i2 =          (rit − β̆i0 ft − m(αi ))2 + var(αi ),
                                  T t=1

where

                         var(αi ) ≡ V arA|R,G (k) (αi ),
                                            L
                                            X
                                       =           π̃i,l [(µ̃i,l − m(αi ))2 + σ̃i,l
                                                                                2
                                                                                    ].
                                             l=1


The parameter values in θ̃i = ({π̃i,l }Ll=1 , {µ̃i,l }Ll=1 , {σ̃i,l
                                                                2 L
                                                                    }l=1 )0 can be found in Appendix
A.1.


                                                   PM         PN               m
A.3      Step III : Maximizing                          m=1        i=1 log f (αi |θ)

The optimization of M
                     P      PN            m
                        m=1   i=1 log f (αi |θ) in itself needs to invoke the EM algo-
rithm. Our goal is to find the MLE of θ when M N observations are assumed to be
drawn from the GMD that is parameterized by θ. For ease of exposition, we replace



                                                              55
the subscript in αim with j so that {αim }(i=1,...,N ;               m=1,...,M )   = {αij }(i=1,...,N ;   j=1,...,M ) .
The starting value of θ is obtained from G (k) .


     • Suppose the initial parameter vector is θ̂ = ({π̂l }Ll=1 , {µ̂l }Ll=1 , {σ̂l2 }Ll=1 ).

     • Expectation Step: Compute the expected value of the indicator variable that
       indicates which population (e.g., the population of skilled or unskilled managers)
       αij is drawn from:

            p̂ijl = P
                    cr(αij comes from Group l)
                       π̂l φ(αij ; µ̂l , σ̂l2 )
                  = PL                             2
                                                       , i = 1, . . . , N ; j = 1, . . . , M ; l = 1, . . . , L,
                      l=1   π̂ l φ(αij ; µ̂ l , σ̂ l )

       where φ( · ; µ, σ 2 ) is the density of the normal distribution N (µ, σ 2 ).

     • Maximization Step: Compute the weighted means and variances, with weights
       obtained from the Expectation Step:

                                                                                         2
                                      P                        P
                                        ij p̂ijl αij     2         ij p̂ijl (αij − µ̃l )
                              µ̃l   =  P             , σ̃l =            P                  ,
                                          ij p̂ijl                          ij p̂ijl
                                      P
                                        ij p̂ijl
                              π̃l   =            , l = 1, . . . , L.
                                       MN


     • Iterate between the Expectation Step and the Maximization Step until conver-
       gence.



A.4       The Value of the Likelihood Function

We derive the value of the likelihood function given in equation (3). This is used to
evaluate relative model performance.
     Under the model assumptions, the overall likelihood function can be decomposed
as

                             L(G|R) ≡ f (R|θ, B, Σ),                                                          (A.5)
                                      YN Z
                                    =       f (Ri |ai , G)f (ai |G)dai ,                                      (A.6)
                                                i=1




                                                               56
where G is the model MLE. Therefore, to obtain R the overall likelihood, we only need
to calculate the component likelihood, that is, f (Ri |ai , G)f (ai |G). Under the model
assumptions, the integrand of the component likelihood can be written as:

    f (Ri |ai , G)f (ai |G)
     T                                                    L
    Y
               2 −1/2         (rit − αi − βi0 ft )2     X
                                                                      2 −1/2       (ai − µl )2
  =     (2πσi )       exp[−                         ] ×      π l (2πσl )     exp[−             ],
    t=1
                                     2σi2                l=1
                                                                                       2σl2
                    L                          PT
          2 −T /2
                   X
                                2 −1/2                 (rit − αi − βi0 ft )2 (αi − µl )2
  = (2πσi )             πl (2πσl )     exp[− i=1               2
                                                                             −       2
                                                                                          ],
                   l=1
                                                            2σ i                  2σ l
                           L
                           X
  = (2πσi2 )−T /2                 πl (2πσl2 )−1/2
                           l=1
                                                    PT          0
                                                     t=1 (rit −βi ft )         σ2
             (σ 2 + σ 2 /T )                                          σl2 + µl Ti 2
      × exp{− l 2 2i         [αi −                            T
                                                                                  ]
              2σl (σi /T )                                    σl2 + σi2 /T
             PT          0
                                                         PT
              t=1 (rit −βi ft )             σ2                  (r −β 0 f )2             σ2
         (   T
                     σl2 + µl Ti )2 ( t=1 itT i t σl2 + µ2l Ti )
    +                                 −                             },
       2(σl2 + σi2 /T )σl2 (σi2 /T )         2σl2 (σi2 /T )
                 XL                     q
         2 −T /2                2 −1/2
  = (2πσi )          πl (2πσl )        × 2π(σi2 /T )σl2 /(σl2 + σi2 /T )
                           l=1
                                                                                                PT           0
                                                                                                  t=1 (rit −βi ft )       σ2
                     1                         (σl2 + σi2 /T )                                            T
                                                                                                                  σl2 + µl Ti 2
      ×p                                 exp{−                 [αi −                                                          ]}
         2π(σi2 /T )σl2 /(σl2 + σi2 /T )        2σl2 (σi2 /T )                                            σl2 + σi2 /T
       |                                           {z                                                                           }
                                                                             2 )
                                                                 φ(αi ;µ0i ,σ0i
                      PT          0
                                                                 PT
                       t=1 (rit −βi ft )            σ2                 (r −β 0 f )2              σ2
                  (        σl2 + µl Ti )2
                             T
                                                                ( t=1 itT i t σl2 +           µ2l Ti )
      × exp{                               −                                                             },
             2(σl2 + σi2 /T )σl2 (σi2 /T )                               2σl2 (σi2 /T )

                    2
where φ(αi ; µ0i , σ0i ) is the density function for a normal distribution parameterized
by:
                                                         PT          0
                                                          t=1 (rit −βi ft )         σ2
                                                          T
                                                                   σl2 + µl Ti
                                           µ0i   =                              ,
                                                           σl2 + σi2 /T
                                            2
                                           σ0i   = (σi2 /T )σl2 /(σl2 + σi2 /T ).




                                                                         57
Therefore, by integrating over ai , the part involving the normal density becomes one,
and we have:

Z                                                  L
                                                   X   q
    f (Ri |ai , G)f (ai |G)dai =   (2πσi2 )−T /2     πl (σi2 /T )/(σl2 + σi2 /T )
                                                   l=1
                                             PT            0
                                                                                   PT          0
                                                t=1 (rit −βi ft )  σi2 2            t=1 (rit −βi ft )
                                                                                                     2        σ2
                                          (      T
                                                            +  µ l T
                                                                    σl2
                                                                      )        (          T
                                                                                                    σl2 + µ2l Ti )
                                   × exp{      2    2      2   2
                                                                           −                                       }.
                                            2(σl + σi /T )σl (σi /T )                     2σl2 (σi2 /T )

Define
                                              PT
                                                   t=1 (rit   − βi0 ft )
                                    α̂i =                        ,
                                                    T
                                           PT             0      2
                                       2      t=1 (rit − βi ft )
                                    αi =
                                    c                              ,
                                                     T
                                     c          σl2
                                   wl,i  =              ,
                                           σl2 + σi2 /T
                                     t            c
                                   wl,i  = 1 − wl,i ,

then the component likelihood can be written as

       Z                                                   L
                                                           X   q
           f (Ri |ai , G)f (ai |G)dai = (2πσi2 )−T /2            t
                                                             πl wl,i
                                                              l=1
                                                       c         t 2      c2 wc + µ2 wt )
                                                 (α̂i wl,i + µl wl,i ) − (αi l,i   l l,i
                                          × exp{                     2       2
                                                                                          }.
                                                         2[1/(1/σl + 1/(σi /T ))]

The overall likelihood can be calculated as the product of the component likelihoods
of the cross-section of funds, as given in equation (A.6).




                                                              58
B     Estimation Details

In this appendix, we detail the implementation of the estimation method that is
described in Section 3.
    In Step I, we choose a set of starting values to initialize our estimation. We have
a large number of parameters that are given by G = [θ0 , B 0 , Σ0 ]0 . However, the time-
series information of each fund helps us estimate each fund’s risk loadings (i.e., β)
and residual variance, providing a reasonable set of starting values. Therefore, for B
and Σ, we start with their equation-by-equation OLS estimates, that is:

                                B 0 = B OLS , Σ0 = ΣOLS ,                            (B.1)

where a superscript of zero denotes the starting values.
    For parameters that govern the GMD (i.e., θ), we randomly generate multiple sets
of starting values to avoid local optimums. In particular, for a L-component GMD
and for the L parameters that govern the means of the component distributions,
we randomly choose L numbers that are uniformly distributed over the interval of
[−20%, 20%] (per annum). The boundary of 20% reflects our knowledge of the mutual
fund data. Our prior is that it is unlikely to have a population of funds that are
concentrated around a mean that resides outside of the [−20%, 20%] interval. Our
estimation results confirm this prior. We never obtain optimal mean estimates for the
component distributions that are close to the boundaries. After randomly generating
the L mean parameters, we rank them in an ascending order for model identification.
    We follow a similar procedure to choose the starting values for the standard devi-
ations of the component distributions. In particular, we randomly choose L numbers
that are uniformly distributed over the interval of [0.1%, 20%] (per annum). Again,
the choices of the boundaries reflect our priors about the standard deviations of the
component distributions. Our estimation results confirm that these boundaries are
never violated for the optimized estimates of the standard deviations of the compo-
nent distributions.
     For the drawing probabilities, the selection of the starting values is more compli-
cated than the selection of the previous two sets of parameters as we now have the
parameter constraint that the sum of the L drawing probabilities should be one. We
therefore follow a sequential procedure to choose the staring values. We first draw
a number (i.e., p1 ) that is randomly distributed over the unit interval. After draw-
ing the first number, we draw a second number that is uniformly distributed over
[0, 1 − p1 ]. We continue in this way to draw the rest of the probabilities. In partic-
ular, after choosing the first l probabilities (i.e., {pi }li=1 ), we choose the (lP+ 1)-th
probability by drawing a number that is uniformly distributed over [0, 1 − li=1 pi ].
Lastly,
     PL−1after choosing the (L − 1)-th probability, the last probability is simply set as
1 − i=1 pi .


                                                 59
     After following the above steps, we now have a randomly generated set of initial
parameter values G 0 = [(θ0 )0 , (B 0 )0 , (Σ0 )0 ]0 , where θ0 contains the parameters that
govern the GMD. Taking this set of parameter values as input for our algorithm,
our estimation becomes automatic. In particular, staring from G 0 and following Step
II -IV, we arrive at a new set of parameters G 1 . Next, starting at G 1 , we follow
our algorithm and arrive at G 2 . We continue in this way and obtain a sequence of
parameter estimates {G k }Kk=0 . This sequence of parameter estimates are converging
as K gets larger. The speed of convergence for our algorithm seems high in that the
variations in parameter values become very small after ten to fifteen iterations. To
terminate the program, we set a tough threshold for the distance of the parameter
estimates between adjacent iterations. In particular, we stop the program at the K-
th iteration if the L1 distance between θK−1 and θK is within dlim . To prevent the
program from running too many iterations, another criterion we impose is that if the
program does not stop until the K lim -th iteration, we stop it at K lim . The choices of
dlim and K lim depend on whether the estimation is the intermediate step or the final
step, as we shall explain next.
    We have explained how our estimation works for one set of starting values. We
need to try multiple sets of starting values to avoid local optimums. In particular,
following the aforementioned generating procedure for starting values, we randomly
generate 100 sets of starting values. For each set, we run our algorithm by setting
dlim = 10−1 and K lim = 30 and obtain 100 sets of parameter estimates. This an
intermediate optimization step in which we try to save the computational time by
setting dlim and K lim at lenient thresholds and obtain 100 sets of rough estimates.
Next, we rank the 100 sets of parameter estimates by the corresponding values of
the optimized likelihood function. We choose the top 20 sets and rerun our program
by starting at the estimated parameter values. This time, we set dlim = 10−2 and
K lim = 50. We again rank the resulting 20 sets of parameter estimates by the
corresponding values of the likelihood function. We choose the top five sets and
rerun our program by starting at the estimated parameter values obtained from the
previous step. This is the final step estimate and we set dlim = 10−3 and K lim = 100.
We choose the best one (in terms of the value of the likelihood function) among the
five sets of estimates as our final estimate. We often see that five sets of parameter
estimates in the final step are very close to each other. This assures us that the local
optima have been thrown out during the intermediate steps.




                                                  60
C      FAQ

C.1     General Questions
    • In short, what is the biggest reason to consider the random alpha model for per-
      formance evaluation?

      Quoting Searle, Casella, and McCulloch (1992), “Effects are fixed if they are
      interesting in themselves or random if there is interest in the underlying popu-
      lation.” For performance evaluation, we are interested in both the effects them-
      selves (that is, to evaluate which manager outperforms) and the population
      (that is, the underlying distribution for alphas). The random alpha model pro-
      vides a suitable framework to think about both.


    • Why not do a random alpha model with a multiple testing adjustment?

      The mechanisms for the random alpha model and the multiple testing frame-
      work to discount the significance of fund alphas are different. The random alpha
      model forces the cross-section of alphas to fit a parametric density. Observations
      that are too extreme according to the fitted density are adjusted. Multiple test-
      ing adjustment invokes the hypothesis testing framework and uses the p-value
      to measure the distance between the estimated alpha and zero. A smaller p-
      value indicates a larger distance from zero and we are trying to identify alphas
      that are sufficiently distant from zero. It is possible to make mistakes by falsely
      declaring a zero alpha as nonzero. To control for the false discovery rate, we
      need to adjust the p-values upward. Both the quantities of interest (i.e., raw
      alpha vs. p-value of alpha) and the objectives (i.e., goodness-of-fit to a density
      vs. false discovery rate) are different between the two methods. Applying both
      will likely overkill the significance of fund alphas.


    • Why is MLE better than the moments-based approach for the estimation of a
      GMD?

      In general, we need an infinite number of moments — properly weighted —
      to achieve the estimation efficiency that MLE provides. For example, for a
      two-component GMD, although it is identified and its five parameters can be
      estimated using the first five sample moments alone, the sixth moment as well
      as other higher moments provide additional information for the estimation of
      the model and should be incorporated into the estimation to improve estimation
      efficiency.




                                                61
• How does model misspecification affect the results of the paper?

  There are different kinds of model misspecifications. A misspecification of the
  return residuals changes our MLE into a QMLE, which will not bias our es-
  timates. The loss in estimation efficiency is also small, as we show in the
  simulation study. On the other hand, a misspecification of the factor model
  (e.g., omitting a true factor) in general will introduce bias for the alpha esti-
  mates. Compared to existing models, our model can to some extent alleviate
  the model misspecification issue, thanks to its ability to use information in the
  entire cross-section to provide inference. We have a discussion on this towards
  the end of the paper. However, both our model as well as existing models are
  sensitive to the issue of model misspecification. See Harvey and Liu (2016a) for
  an examination on how factor model misspecifications affect asset pricing tests.


• Does it make sense to treat all funds equally? It seems that there is more infor-
  mation for a fund with a $1 trillion AUM than a fund with a $10 million AUM.

  From the perspective of making inference on the alpha population, we think
  that the alpha for the $10 million fund is just as important as the alpha for the
  $1 trillion AUM fund. If an investor invests $1 million in either fund, the alpha
  she gets is simply the alpha for either fund. The alpha for the smaller fund will
  not be discounted because the fund is smaller. It is likely that the returns for
  smaller funds are more noisy than returns for larger funds. Our method takes
  the estimation uncertainty into account.


• Why not use a three-component GMD for mutual funds in the simulation study?

  For mutual funds, a two-group separation is more in line with what the literature
  has found, that is, bad funds and average funds. We also did a three-component
  GMD specification in the simulation study. It reduces the proportion of aver-
  age funds to about 80% and split the rest 20% into bad funds and very bad
  funds. However, as we show in the actual application for mutual funds, a two-
  component GMD fits the data better than a three-component GMD. Therefore,
  we use the simpler two-component GMD specification for the simulation study.

• How many funds have a t-statistic over 2.0 under OLS?

  For our sample, under equation-by-equation OLS, 1.7% of funds have a sin-
  gle test t-statistic above 2.0. However, since we have run thousands of tests, we
  need to adjust for multiple testing. Applying multiple testing adjustments, few
  funds are found to be significantly outperforming.
  Our method departs from the hypothesis testing framework and assumes a
  continuous distribution for fund alphas. In our framework, alphas are almost

                                           62
  surely not zero by construction. To see the difference between our framework
  and hypothesis testing, suppose we have 100 funds, each one having an OLS
  intercept of 1% (per annum) and a standard error of 2% (per annum). Under
  hypothesis testing, there is no outperformer, as none of the t-statistics is able
  to pass the single test t-statistic threshold, let alone the multiple testing t-
  statistic threshold. Under our model, we estimate the alpha distribution to be,
  say, normal around a mean of 1%. If we test the significance of each alpha
  under our model, it might as well be the case that none of the t-statistics is
  above 2.0, especially if the 2% standard error is high enough at the individual
  fund level. However, this is not evidence against our model since it is not based
  on hypothesis testing. In our framework, it is possible that all individual funds
  have a t-statistic below 2.0 while at the same time the population mean is
  positive and statistically different from zero.

• What is the intuition behind the EM algorithm to refine the OLS estimates of
  alphas?

  Imagine that the parameters that govern the alpha population (i.e., the normal
  mixture distribution) are given. In the “expectation” step, we calculate the
  conditional distribution of alphas by mixing information from the time-series
  and the cross-section. Essentially, OLS estimates are adjusted for the informa-
  tion in the mixture distribution. More noisy OLS alpha estimates (which are
  likely due to higher levels of residual standard deviations) are adjusted more
  aggressively than less noisy OLS alpha estimates. Hence, the new alpha esti-
  mates after the “expectation” step are less noisy than the OLS estimates that
  are based on time-series information alone. However, these new alpha estimates
  should change our initial guess of the alpha population (i.e., parameters in the
  normal mixture distribution). As a result, in the “maximization” step, we try
  to find a new set of parameters that best explain these new alpha estimates. We
  iterate between the “expectation” step and the “maximization” step to refine
  our estimates of both the individual alphas and the parameters that govern the
  alpha population.




                                           63
