                               NBER WORKING PAPER SERIES




              REGULATING SYSTEMIC RISK THROUGH TRANSPARENCY:
                     TRADEOFFS IN MAKING DATA PUBLIC

                                         Augustin Landier
                                          David Thesmar

                                       Working Paper 17664
                               http://www.nber.org/papers/w17664


                     NATIONAL BUREAU OF ECONOMIC RESEARCH
                              1050 Massachusetts Avenue
                                Cambridge, MA 02138
                                   December 2011




The views expressed herein are those of the authors and do not necessarily reflect the views of the
National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2011 by Augustin Landier and David Thesmar. All rights reserved. Short sections of text, not to
exceed two paragraphs, may be quoted without explicit permission provided that full credit, including
© notice, is given to the source.
Regulating Systemic Risk through Transparency: Tradeoffs in Making Data Public
Augustin Landier and David Thesmar
NBER Working Paper No. 17664
December 2011
JEL No. D82,D83,H41

                                               ABSTRACT

Public or partial disclosure of financial data is a key element in the design of a new regulatory environment.
We study the costs and benefits of higher public access to financial data and analyze qualitatively
how frequency, disclosure lag and granularity of such open data can be chosen to maximize welfare,
depending on the relative magnitude of economic frictions. We lay out a simple framework to choose
optimal transparency of financial data.


Augustin Landier
the Toulouse School of Economics
21 Allée de Brienne
31000 Toulouse, FRANCE
augustin.landier@tse-fr.eu

David Thesmar
HEC Paris
1 rue de la libération
78351 Jouy-en-Josas cedex
France
thesmar@hec.fr
Regulating Systemic Risk through Transparency:
       Tradeoffs in Making Data Public
                     Augustin Landier (TSE) and David Thesmar (HEC)1




                Public or partial disclosure of financial data is a key element in the design of a
                new regulatory environment. We study the costs and benefits of higher public
                access to financial data and analyze qualitatively how frequency, disclosure lag
                and granularity of such open data can be chosen to maximize welfare, depending
                on the relative magnitude of economic frictions. We lay out a simple framework
                to choose optimal transparency of financial data.




According to many observers, the financial crisis is a product of the lack of transparency of
the financial system. In a recent speech, Fed chairman Ben Bernanke acknowledged that
opaqueness was a “structural weakness in the shadow banking system” and an important
element in the narrative of the crisis.2 Examples of such opaqueness are OTC markets such as
the CDS market, for which there is little detailed data about holdings, prices and collateral
posted. This made it difficult for the regulator to understand the web of counterparty
exposures and have its independent assessment of the overall resilience of the system.

Besides what was available to the regulator, public disclosure of financial information was
probably insufficient. For instance, the lack of micro-data on the content of securitized
products made it difficult for investor to price risk correctly. Market participants trading
securitized products such as collateralized debt obligations (CDOs) and collateralized
mortgage obligations (CMOs) were relying on imprecise, and sometimes flawed, ratings
(Benmelech and Dlugoszb (2008)). Beyond pricing, public availability of data is argued to
enhance the role that researchers outside the government (from universities, think-tanks or
other private institutions) can play in assisting, and to some extent monitor, regulators.3

This article is about the optimal degree of public disclosure of financial data. Theoretically,
however, increasing transparency may reduce welfare as well as increase it. For instance,
Dang, Gorton and Holmström (2010) argue that common ignorance of market participants
about the precise nature of some assets might be a desired feature rather than a bug of the
financial system. Such assets might deliberately be constructed as opaque to remain
information insensitive. Releasing more information in the public domain might thus be
welfare decreasing. Other voices, notably from the private sector, warn that extreme


1
  We thank Markus Brunnermeier, Arvind Krishnamurthy, Luigi Zingales, and other participants to the NBER
Systemic Risk meeting in Chicago, May 2011.
2
  Speech given at the 2010 Squam Lake conference.
http://www.federalreserve.gov/newsevents/speech/bernanke20100616a.htm
3
  Mary Graham, 2002, "Democracy by Disclosure", Brookings Institution Press, Washington DC
transparency might be detrimental to incentives of financial agents to produce information
and to innovate.

In this paper, we develop a framework to understand the costs and benefits of public
disclosure of financial data. We believe such a framework can help regulators to determine
the format under which data should be publicly accessible, as a function of the type of
information under consideration. In our framework, the welfare impact of public access to
data depends on three dimensions: (1) the frequency of data collection, (2) the time-lag of
their public release and (3) their level of granularity / anonymity. Granularity and anonymity
are linked as less granular (more aggregated) data make it easier to protect anonymity.

In doing so, our framework addresses the following questions:
    At what frequency should data be collected and made available?
    How long should the regulator wait before releasing the data?
    At what level of detail (granularity) should information be released? If the public
       information is detailed, should it be made anonymous?

Frequency, lag and granularity are important choice variables for regulators who wish to
publicly disclose data; yet, there is no systematic doctrine about their optimal level. To give
examples, some data are currently publicly available at very fine levels of granularity (13Fs
filings provide at the institution level an exhaustive position-by-position view of individual
long equity holdings at a quarterly frequency). By contrast some other data are available only
at an aggregated level (e.g. new lending by banks, or bank cross-country exposures). But
these choices owe more to history and political compromises than to a systematic cost and
benefit analysis.

In the first section of the paper, we review the various economic costs associated to public
data release and review the corresponding academic literature. In the second section, we
present several dimensions of data that can be optimized to mitigate each of these costs. Last,
we review the positive impact that might be expected from public access to financial data and
develop a framework to manage on a case-by-case basis the cost-benefit trade-off, notably
through the choice of disclosure lags and granularity.



1. The potential costs of transparency: a review of economic forces at stakes


There is a basic intuitive economic reason why more information is a priori better than less:
Individual decisions are closer to being privately optimal when agents have more accurate
information. Since, in a frictionless economy, private and social efficiency do not differ, it
follows that full public disclosure of information is always desirable from a welfare
perspective if we abstract from frictions. However, this prediction does not hold any more in
the presence of economic frictions. In this section, we review the various kinds of frictions
that can create a welfare cost to the public release of financial information.


       a. The (shrinking) material and clerical costs
A direct cost of transparency is simply that of producing, storing, certifying and disseminating
information. For a long time, such costs have been an important margin in deciding the
optimal level of public disclosure. Today, while these costs might not be trivial, they have
been enormously diminished (the cost of saving one bit of information on hard disk storage
space has decreased by almost 1.5M times since 1980).
Moreover, much financial information that one might be inclined to disclose consists in
information that financial firms already produce for internal use. For instance, disaggregated
data on holdings and liabilities is a necessary ingredient of sound risk-management by
financial institutions. The need to transmit them to regulators and/or to the public might thus
not be a high additional cost to companies. In what follows, we will focus on other costs than
these simple material costs of producing and managing information.


        b. Secrecy might be vital to some activities in finance

A second line of arguments against imposing higher transparency is that it might discourage
the production of information by financial intermediaries and consequently decrease market
efficiency. To have incentives to produce information, economic agents need to be able to use
it to make profits, which might require a temporary monopoly power on that information. For
instance, after paying the cost to identify an arbitrage opportunity, a hedge fund manager
might need time to exploit the opportunity before it gets revealed to the public. If positions
are disclosed quickly, the fund might not have had time to enter positions at a sufficient scale
to recoup the initial cost.

Thus, by decreasing rents of arbitrageurs, transparency might, according to that view, reduce
market efficiency (this is related to the Grossman-Stiglitz paradox). Opacity in finance might
be useful in protecting rents from information production, akin to trade secrets or patents in
other high-tech industries. One empirical example where this property right argument has
some empirical bite, however, seems to be that of analyst’s coverage. Gomes, Gorton and
Madureira (2004) study the consequences of the adoption of Regulation Fair Disclosure (“Reg
FD”) by the U.S. Securities and Exchange Commission in October 2000, a regulation that
prevents firms from selectively disclosing information to some market participants (typically
a few selected analysts), but not others. They find that this regulation led to a loss of analyst
coverage for small firms and to a higher cost of capital. Their interpretation is that by
eliminating the temporary monopoly power of some analysts on information, the regulation
discouraged them to work on small firms.

In finance, the property right argument is hard to reconcile with recent trends in the cost of
information accumulation and evolution of rents in the sector.4 There is evidence that wages
and profits in the financial sector have risen to abnormally high levels (see e.g. Philippon and
Reshef, 2008). At the same time, the cost of gathering, storing and processing information has
decreased massively. Hence, for the current level of financial rents to be optimal, we need
information acquisition to have become extremely (socially) valuable in today's economies.

        c. Transparency may generate instability...




4
  Moreover, the empirical evidence from non-financial industries also indicates that strong intellectual property
protection often decreases innovation speed (see e.g. Williams (2010) or Boldrin and Levine (2007)).
Various academic theories provide models where the disclosure of more public information
can generate financial instability, increased volatility and/or reduced liquidity. We provide in
this subsection a categorization of these theories.

   i.      …when it creates asymmetric information,

Several papers argue that when complex information is released to the public, only the most
sophisticated agents are able to process it. Thus, increased transparency can generate
asymmetric information among agents, which in turn can make markets less liquid or even
collapse à la Akerlof (1970). The more complex the information to be released is, the stronger
this effect. For instance, Pagano and Volpin (2010) have a model along this line of research
where they emphasize the view that transparency enhances liquidity only if market
participants are equally skilled at information processing. In their model, some investors have
limited ability to process information. Thus, releasing more public information can increase
adverse selection in the market and reduce liquidity. Similarly, Dang, Gorton, and
Holmström, 2010, also develop a model where common ignorance facilitates trading. More
transparency can lead to the inefficient emergence of adverse. Their paper suggests that
certain types of market securities, such as securitized products, are designed to be
information-insensitive and thus deliberately constructed to be opaque. Markets for such
securities can function better if all agents are kept uninformed about details so that no one
suspects other agents of being trading with superior information. Holmström (2008) notes that
when De Beers sells wholesale diamonds, the stones are placed in opaque packets that buyers
are forbidden to explore, to avoid the occurrence of a lemon’s problem.

Empirical evidence looking at the introduction of a fundamental change of the US bond
market tends however to find positive liquidity effects of transparency: After July 2002, the
Transaction Reporting and Compliance Engine (TRACE) requires bond dealers to report all
trades in publicly-issued corporate bonds. The National Association of Security Dealers,
makes the transaction data accessible freely to the public. Several studies find that the cost of
trading corporate bonds has decreased following the increase in transparency (see Goldstein et
al. (2007), Edwards et al. (2008), Bessembinder et al. (2006)).



   ii. …when it creates coordination failures,

 The ‘global games’ literature has examined the impact of public information in coordination
games where agents have both private and public information. Morris and Shin (2002) shows
that the provision of more precise public information can be detrimental to welfare. The
reason is that agents (rationally) overreact to public information compared to the social
optimum, and hence noise in public signals can cause social inefficiencies. An increase in the
precision of public information may have the perverse eﬀect of increasing aggregate
volatility, as economic activity becomes more sensitive to common noise. The reason is that
private signals are not used in a socially optimal manner in the presence of strong public
signals.

In the same vein, Amador and Weill (2010) show that the availability of more public
information can limit the incorporation of private signals into prices and thus be welfare
decreasing (see also Duffie-Malamud-Manso, 2010). In the presence of multiple equilibria,
public information can also lead to the selection of a bad equilibrium.
Morris and Shin (2007) have a model where coarser information has greater chance to being
understood by all market participants than granular information and thus leads to better
coordination among agents. They emphasize a tradeoff between the quantity of information
and its shared nature. If coordination requires a common understanding of information, it is
possible for increased transparency to be welfare decreasing. This is because some market
observers (possibly a small minority) fail to understand the information.


           iii.    …when it facilitates predatory trading

Investors in financial distress can be forced to unwind quickly their positions. When the
holdings of such investors are known by other traders, such liquidations can give rise to
predatory trading: Informed traders anticipate the fire-sale and initially trade in the same
direction as the distressed institution, which amplifies the shock to asset prices. Brunnermeier
and Pedersen (2007) provide a continuous time model that solves for the price trajectory in
the presence of fire sales and quantify the amplifying impact of predatory trading on price
swings. They emphasize the risk of systemic destabilization induced by such predatory
trading and use as a motivating example the alleged trading against Long Term Capital
Management’s (LTCM’s) positions in the fall of 1998 which led to the concern that LTCM’s
financial difficulties might destabilize the financial system as a whole. This kind of
destabilizing speculation is facilitated when both the individual shocks leading to fire-sales
(such as performance or outflows) and the individual holdings of investors are public
information.

Empirically, Coval and Stafford (2007) study the price impact of mutual funds outflows. They
emphasize the fact that, public disclosure of funds holdings makes future flow-driven
transactions predictable. This creates an incentive to front-run the anticipated fire-sales of
distressed mutual funds that are experiencing large capital outflows. Such front-running
strategies are shown to be profitable. They decrease the price at which these distressed funds
are able to liquidate positions. Looking at hedge fund holding data, Greenwood and Thesmar
(2011) find little evidence of such front-running by hedge funds. On average, hedge fund
trading is not correlated with mutual fund fire-sales. For some stocks, this correlation is
positive (front-running), but for others, this correlation is negative (liquidity provision).


           iv.     …when it generates perverse incentives to offset regulation.

Higher transparency imposed to institutions can generate large offsetting effects. Faced with
higher disclosure requirements, financial institutions might strategically attempt to find ways
to manipulate disclosures in a self-serving manner. The economic forces behind such
offsetting effects are well known: agents try to strategically manipulate to their advantage the
signals that are used by regulators or principals to evaluate them (Holmström 1999,
Holmström and Milgrom 1991). Such inefficient “signal jamming” can lead to substantial
distortions.

Directly related to the offsetting effects of disclosure requirements, a large literature studies
how publically traded companies engage into earnings smoothing and various accounting
manipulations to optimize market reactions to their corporate financial reporting. Bergstresser
and Philippon (2004), show that discretionary accruals are used strategically by CEOs to
manipulate reported earnings. Graham et al (2005) find that 78% of a representative sample of
top executives admits to “sacrificing long-term value to smooth earnings”. In the same vein,
Chevalier and Ellison (1997) show that mutual fund managers who are performing well
relative to the market gamble to make year-end lists of “top performers”: this is again an
example of “jamming” of publically observed signals.

Such offsetting effects due to regulatory constraints have actually been at paly in the current
crisis: The shadow banking system was partly developed as a way to bypass regulatory
constraints, notably capital requirements. Asset-backed commercial paper conduits became an
increasingly large source of funding for commercial banks, reaching US$1.4 trillion in June
2007 (see Acharya et al (2009) and Brunnermeier (2009)).



2. The three determinants of the costs and benefits to publicly disclose financial data


The costs of public disclosure listed above are potentially affected by three parameters. These
parameters are all choice variables of the regulator who is in charge of disseminating the
information. In this Section, we describe these parameters and their effect on the costs of
transparency. We defer the discussion on their impact on benefits of disclosure to the next
Section.

The three determinants of the costs and benefit of transparency are:

      Granularity: There are two levels of granularity, that of the reporting entity, and that
       of the reported positions. Regarding positions, data can be at the individual position
       level (e.g. quantity of common stocks of company i held) or at a more aggregate level
       (e.g. quantity of stocks in industry i held). Similarly, reports can give holdings at the
       individual legal entity level or at a more aggregate level. For instance 13F reports
       provide fully granular data on long equity holdings: Each reporting entity has to file its
       positions in each individual company. By contrast, BIS reports are more aggregated,
       as they give the consolidated exposure of all the banks of country i to any given
       foreign country j. Note that granularity and anonymity are to be determined jointly: to
       make data anonymous (i.e. such that the reporting entity cannot be identified by the
       user), it is usually needed to pick an appropriate level of aggregation, otherwise, users
       might infer from the data what institution/individual is reporting. E.g. the ranking of
       bank by asset size is known, and thus data that would provide total balance sheet size
       information would be de-facto non-anonymous.

      Frequency: Data can be reported at various frequencies. When frequencies are too low
       (e.g. annual or quarterly), a risk exists that institutions engage in window dressing by
       changing holdings right before they have to report. To avoid such gaming of the
       system without incurring the inconvenience of high frequency reporting, institutions
       could be asked to provide average numbers over a given period as advised in the
       Geneva Report.


      Lag of Disclosure: Data regarding holdings at date t can be reported at time t’ and
       made available at a later date t’’>t’ to the public. The public disclosure lag is t’’-t and
        the reporting lag is t’-t. For instance, form 13F is required to be filed to the SEC by
        large institutional owners within 45 days of the end of a calendar quarter and is made
        available immediately on the SEC’s website, i.e. t’-t=t’’-t=45days.
Let us now turn to the impact of these three determinants on the costs of transparency. Lag of
disclosure and granularity are key margins to reduce these costs. The revelation of “cold”
information won’t induce lemons problems, coordination failures, nor a substantial loss in
trading profitability. A delay of six months to one year seems reasonable in most cases.
Zingales (2010) suggests delays of one- to two-year before full disclosure of individual data
in private equity and hedge funds. His argument is the following: “This delay will eliminate
any competitive concern, since the half-life of a portfolio strategy is very short on Wall Street
(Grinold and Kahn [2000] estimate it as 1.2 years), while still providing the benefit of a
serious statistical analysis of this market, which will improve allocation. ». Still, some agents
might argue that key proprietary processes might be reverse-engineered by observing data and
could be detrimental to profitability. Facing such arguments, the regulator has to estimate
whether this is a credible threat to the business; if so, the solution is to reveal data at more
aggregate levels, thus limiting the possibility of reverse-engineering. However, less granular
information on holdings might be easier to manipulate than lists of individual holdings
(granular information).

3. The potential gains from public disclosure

We now turn to the potential welfare gains from public access to financial data. In each case,
we will review the extent to which the benefits depend on frequency, lags and granularity. A
frequent question in financial economics is why firms do not set the right level of disclosure
by themselves (Leuz and Wysocki, 2008). After all, if information is useful to buyers of
securities, they should be willing to pay a higher price to be informed. In the case of systemic
risk, the reason for the under-provision of public information by private actors is simple:
firms do not internalize the impact that their level of disclosure can have on the stability of the
financial system via the knowledge of other participants (including the regulator). There is
thus a motive for compulsory disclosure of information.

       a. “crowd-sourcing” and the value of “free-range” research

One option available to regulators is to behave as gate-keepers regarding the data and provide
access to them only to selected teams of researchers who present a project that is considered
valuable for regulatory purpose. However, there are several reasons that might make such
restrictive access suboptimal.

First, many research projects are not well defined at an early stage and it is often only after
thorough data exploration that researchers are able to spot new relevant questions or
anomalies worth to be investigated. If data are not publically accessible, such exploration is
less likely to take place. Second, any gate-keeping institution is likely to become excessively
protective about the use of data and favor internal or more connected researchers. It is
difficult to conceive a governance mechanism that would surely defeat the emergence of such
monopolistic behavior.

Data availability directly feeds research intensity: Kevin Murphy shows that the average
number of CEO pay papers produced per year has doubled after the Executive compensation
data for large US companies (EXECUCOMP) became available in 1992. In several instances,
such research has had direct regulatory impact. An instance of influential forensic analysis by
academics in finance is the work of Heron and Lie (2007, 2009). Using public Executive
Compensation data, they uncovered a widespread practice of backdating of option grant dates
among US executives. They estimate that 18.9% of option grants of US top executives during
the period 1996-2005 were backdated. Their study led to a large SEC inquiry into this illegal
practice. The impact of researchers using their free access to the data to identify new topics or
anomalies worth being investigated by regulators is historically validated by several other
examples. Notably, numerous academic studies have investigated the behavior of mutual
funds, taking advantage of public holdings and performance data provided by the SEC. For
instance, Carhart, Kaniel, Musto, and Reed (2002) document that the prices of stocks owned
by mutual funds exhibit positive abnormal returns at the end of the quarter. Duong, and
Meschke (2008) find no evidence for such manipulation by mutual funds post 2000
suggesting that the increased scrutiny following the publication of the results has led to a
decrease in manipulation of that sort. 13F filings have widely been used in research to study
the potentially destabilizing role of institutional investors in financial markets. Brunnermeier
and Nagel (2004) explore the behavior of hedge funds during the Internet bubble and show
that they were long technology stocks during the most part of the bubble, thus playing a
destabilizing role. Insider trading data are also largely exploited by researchers. It is easy to
imagine how much more could be done if wider data were available. For instance the fact that
short positions are not available in 13Fs has limited the scope of research on the market
impact of hedge funds.

The internal politics of bureaucratic organizations might create a climate favorable to status-
quo bias and reluctance to ask disturbing questions. An open-source approach to the use of
data might mitigate these biases, by letting researchers from various horizons free to ask
questions that might seem a priori unwarranted. The “wisdom of crowds” effect stemming
from a large pool of external researchers that are not filtered by (or affiliated to) the regulator
can hardly be replicated in-house. Small groups of experts tend to be prone to consensus and
are less likely to ask the disturbing questions. Researchers internal to regulatory agencies
might become too confident in the system and miss the build-up of excessive risk taking. New
forms of risks might emerge from unpredictable parts of asset classes that specialized
researchers might dismiss as innocuous out of habit. Such cognitive and organizational
“groupthink” effects and their causes are explored in Benabou (2010) (see also Janis (1982)).5

For the wisdom of crowd effect to take place, granular data might be important because
categories used by regulators to aggregate data can slowly become obsolete. In some cases,
regulators might even deliberately provide hard to use aggregates to preserve their monopoly
in the use of the data. New sources of risks are likely to come from unexpected parts of the
system, or innovations such as “game-changing” products that do not fit existing categories.
To address such changing environment, access to granular data is key. Cheng, Kirilenko and
Xiong (2011) show how using granular (but not public) data from the CFTC, the impact of
indexed traders on commodity prices can be inferred. Their analysis cannot be undertaken
using the public aggregate data reported by the CFTC.

Disclosure lags (as long as they are not too extreme) will not diminish the interest of
researchers for data, nor the relevance of their contributions: based on 2003-2004 data,
researchers may have produced highly relevant research for taking regulatory decisions in
2007-2008.

5
  The wisdom of crowds effect we are referring to is by no means akin to a form of “self-regulation”. Quite to
the contrary, contributions from unaffiliated researchers can enrich and complement the work of regulators and
help them reaching efficient decisions.
         b. avoiding regulatory capture

Regulatory capture occurs when the regulated industry manages to bias the regulator's
decisions in its favor (Stigler, 1971).6 Incentives of regulators might be improved if internal
data are made available to the large public ex-post, so that researchers and other outside
experts can externally monitor regulatory decisions. Moreover, even absent incentive
problems from regulators (i.e. even if they turned out to automatically make socially optimal
choices), the possibility to judge ex-post that their decisions were ex-ante justified based on
their information set is important: it reinforces the legitimacy of their choices and the
confidence of market participants.

Here again, a relatively long disclosure lag (e.g a couple of years) is not very detrimental to
the benefits of transparency, but it is important to get access to data with the same granularity
as the regulator to replicate his/her information set.

         c. Complementarity with the internal reporting system of financial institutions

The standardization of back-office booking of positions among banks and other financial
institutions creates a mutually beneficial situation for both regulators and financial
institutions. On the one hand, it makes regulation more efficient: reports to regulators can be
directly fed by internal risk-management systems. Also, highly granular (position level)
reports might be important to accelerate the standardization of new products between banks
and making regulators quickly aware of the emergence of such new products. The advantage
of forcing the adoption of a common asset classification is to force banks to improve back-
office and risk-management and help coordination among them.

On the other hand, standardization of back office booking also makes internal risk
management systems more efficient, which in turn elicits cooperation from the financial
industry. A good example is the case of transaction identifiers. In its willingness to create a
reporting standard, the Office of Financial Research seeks to impose a « Legal Entity
Identifier », a detailed identifying number for each financial transaction, including
derivatives. The gains of doing so for the financial industry are important. First, having
common identifiers makes settlement easier, which lowers back‐office costs. Second, it
lowers the cost of mergers, as it becomes easier to integrate different risk management
systems into a single one. This made the industry coopoerative, in particular big entities
who had grown through successive acquisitions.

For this positive coordination effect of data disclosure to take place, public disclosure is in
theory not needed (it is enough that regulators get the information). However, public data can
help private actors identifying crowded trades and staying away from them. This argument
might be reversed in the presence of bailout expectations, as financial institutions might then
have collective incentives to load on similar risks (see e.g. Fahri-Tirole (2010)). For
disaggregate data to be informative about counterparty risk, it is important for them to be
specific about the exact reporting entity, e.g. separate out assets for which the bank is liable
and assets for which banks are not, which suggests aiming for relatively fine granularity. Note
that such feedback into risk-management needs t take place in real time. It follows that in
6
  Several commentators (e.g. Johnson (2010)) have argued that regulatory capture of public agencies might have
been a driving force behind the financial crisis, resulting in a lax regulatory environment placing faith in “self-
regulation” via risk models internal to banks.
order to induce such private risk-management effects, short lags are more important than
when the purpose of public data release is the use by the academic community.



Conclusion:

While this paper focuses on the finance industry, there are general lessons that can be drawn
from our analysis for other areas of regulation (such as environmental and medical data).
After reviewing the potential costs of public disclosure of financial data, we have shown how
they can be mitigated by the choice of long disclosure lags and coarser levels of granularity
(i.e. more aggregate data). In reviewing the gains from access to public data, based on
observing the use of already available public data, we emphasize the fact that long lags of
disclosure (such as a couple of years) might not be a major impediment to the production of
relevant contributions by academic researchers but might negatively impact the feedback into
private risk-management. However, the relevance of academic contributions is more likely to
be impacted by the choice of granularity of the data. In particular aggregate data seem more
vulnerable to obfuscation by private actors and possibly by regulators themselves.
References

Acharya Viral V., Philipp Schnabl, and Gustavo Suarez, 2011, "Securitization Without Risk
Transfer", mimeo NYU Stern
Amador, Manuel, and Weill, Pierre-Olivier, "Learning from Prices: Public Communication
and Welfare", Journal of Political Economy. 118(5), pp 866-907.
Angeletos, George-Maria, and Pavan, Alessandro, 2004, "Transparency of Information and
Coordination in Economies with Investment Complementarities", American Economic
Review, 94(1), pp 91-98.
Ashcraft, Adam and Til Schuermann, 2008, “Understanding the Securitization of the
Subprime Mortgage Credit”, Federal Reserve Bank of New York staff reports.
Baker, Andrew, 2010, "Restraining regulatory capture? Anglo-America, crisis politics and
trajectories of change in global financial governance", International Affairs, 86(3), pp 647-
663
Benabou, Roland, 2010, "Groupthink", Princeton working paper
Benmelech, Efraïm, and Dlugoszb, Jennifer, 2009, “The Alchemy of CDO Credit Ratings.”
Journal of Monetary Economics, 56(5) pp 617-634.
Boldrin, Michele, and Levine, David, 2007, "Against Intellectual Monopoly", UCLA
manuscript, available at http://www.dklevine.com/general/intellectual/against.htm
Bessembinder, Hendrik, and Maxwell, William, "Transparency and the Corporate Bond
Market", Journal of Economic Perspectives, 22(2), pp 217–234
Brunnermeier, Markus, and Pedersen, Lasse, 2005, "Predatory Trading", The Journal of
Finance, Vol 60, pp 1825-1863.
Brunnermeier, Markus, 2009. “Deciphering the Liquidity and Credit Crunch of 2007-08”,
Journal of Economic Perspectives, 23(1), 77-100.
Carhart, Mark, Ron Kaniel, and Adam Reed, 2002, "Leaning for the Tape: Evidence of
Gaming Behavior in Equity Mutual Funds", Journal of Finance, 57(2), 661-693.
Cheng, Kirilenko and Xiong, 2011, "Commodity Index Traders", this volume
Coval, Joshua, and Stafford, Erik, 2007, "Asset fire sales (and purchases) in equity markets,
Journal of Financial Economics, 86(2), pp 479-512.
Dang, Gorton, and Holmström, 2010, "Opacity and the Optimality of Debt for Liquidity
Provision", working paper.
Edwards, Amy, Harris, Lawrence, and Piwowar, Michael, 2007, "Corporate Bond Market
Transaction Costs and Transparency", Journal of Finance, 62(3), pp. 1421-1451.
Gomes, Armando, Gorton, Gary, and Madureira, Leonardo, 2007, "SEC Regulation Fair
Disclosure, Information, and the Cost of Capital", Journal of Corporate Finance, 13(2-3), pp.
300-334.
Goldstein, Michael, Edith Hotchkiss, and Erik Sirri, 2007, "Transparency and liquidity: A
controlled experiment on corporate bonds", Review of Financial Studies, 20(2), pp 235-273.
Graham,, John, Harvey, Campbell and Rajgopalc, Shiva, 2005, "The economic implications
of corporate financial reporting", Journal of Accounting and Economics, 40, pp 3-73.
Greenwood, Robin and Thesmar, David, 2011, "Stock Price Fragility", forthcoming Journal
of Financial Economics
Heron Randall A. and Erik Lie, 2007, "Does backdating explain the stock price pattern around
executive stock option grants?", Journal of Financial Economics, 83, pp 271-295.
Heron Randall A. and Erik Lie, 2009, "What Fraction of Stock Option Grants to Top
Executives Have Been Backdated or Manipulated?", Management Science, 55(4), 513-525.
Holmström, Bengt, 2008, "Discussion of ‘The Panic of 2007,‘ by Gary Gorton,“ in
Maintaining Stability in a Changing Financial System, Proceedings of the Jackson Hole
Conference, Federal Reserve Bank of Kansas City.
Janis, Irving, 1982, "Groupthink", Houghton Mifflin Company.
Johnson, Simon, "The Quiet Coup", The Atlantic, 2009.
Leuz, Christian, and Wysocki, Peter, 2008, “Economic Consequences of Financial Reporting
and Disclosure Regulation: A Review and Suggestions for Future Research", working paper.
Morris, Stephen and Shin, Hyun Song, 2002, “Social Value of Public Information.” American
Economic Review, 92(5), pp. 1521–34.
Morris, Stephen, and Shin, Hyun Song, 2007, "Optimal Communication", Journal of the
European Economic Association, 5(2-3), pp 594-602.
Pagano, Marco, and Volpin, Paolo, 2010, "Securitization, Disclosure and Liquidity", working
paper.
Philippon, Thomas, and Reschef, Ariel, "Wages and Human Capital in the U.S. Financial
Industry: 1909-2006", working paper NYU
Stigler, Georges, 1971, "The Theory of Economic Regulation", The Bell Journal of
Economics, 2(1), pp 3-21.
Williams, Heidi, 2010, "Intellectual property rights and innovation: Evidence from the human
genome", NBER Working paper 16213.
Zingales, Luigi, 2009, "The Future of Securities Regulation", Journal of Accounting
Research, 47(2), pp 391-426.
