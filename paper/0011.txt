                           NBER WORKING PAPER SERIES




                             WEIGHTED RIDGE REGRESSION:

                    COMBINING RIDGE AND ROBUST REGRESSION METHODS


                                  Paul W. Holland*




                                Working Paper No.         11




           COMPUTER RESEARCH CENTER FOR ECONOMICS AND MANAGEMENT SCIENCE
                     National Bureau of Economic Research. Inc.
                               575 Technology Square
                           Cambridge, Massachusetts   02139




                                   September 1973




                          Preliminary: not for quotation

         NBER working papers are distributed informally and in limited numbers
    for comments only. They should not be quoted without written permission.
        This   report has not undergone the review accorded official NBER
    publications; in particular, it has not yet been submitted for approval by
•
    the Board of Directors.

        *NBER Computer Research Center. Research supported in part by National
    Science Foundation Grant GJ-ll54X2 to the National Bureau of Economic Research, Inc.
                                                                               1)
                                  Abstract

     Gives the formulas for and derivation of ridge regression methods when
there are weights associated with each observation. A Bayesian motivation is
used and various choices of k are discussed. A suggestion is made as to how
to combine ridge regres.sion with robust regression methods.




                                                                               C)
                                    Contents


                                                   1
1. Introduction

2. Ridge Regression When There are Weights         2

3. Motivations and Interpretations                 4

                                                   5
   Bayesian Background
                                                   7
    Interpreting Ridge Regression

4. The Choice of k                                 8

    Empirical Bayes Choices of k                   9

                                                   12
    Estimating Optimal k-values

    Further Study of These Choices of k            14

5. Combining Ridge and Robust Regression Methods   17

References                                         19
1.   Introduction

      We consider here the familiar regression problem specified by


                   y =    X13   +   c                                                       (1-1)


where y is Nxl, X is Nxp and 13 is pxi. We use the notation


                   Z         Gau(p, )                                                       (1-2)

to mean that Z has an N-dimensional multivariate Gaussian distribution with

mean vector p          E(Z) and covariance matrix             =   Cov(Z).   In this notation we

assume that


                   c'' Gau(O , a2 <w) _1)                                                   (1-3)


where( w) denotes a diagonal matrix with the vector w along its main diagonal.

( w> is assumed to be a. known matrix              ,   a   and 13 are unknown.

     The weighted least squares estimate of 13 is given by


                         =
                                (X<w)       x)_lxT <w y                                     (1-4)

and the weighted least squares "fitted values,"

                         =
                   LS            BLS
                                        '
                                                                                            (1-5)

satisfy   the following normal equations:


                   XT<w) y= XT(w) y                                    .      ..
                                                                                            (1-6)

     The problem we wish to attack here is how to improve on                        as an
estimator of 13.       Because,             is thebest linear unbiased estimator of 13,

to find an improvement we must consider estimators which are both non-linear
                                       -2-




functions of y and biased. The fundamental work of Stein [1956] and later

that of Baranchik [1970] and Sciove [1968] show that when the number of     regres-
sion parameters is sufficiently large, then uniform imporvements over

are possible using biased, non—linear estimators. The minimum p (not

including the constant term) is p =   3.   The results of Wermuth [1972] show

that the degree of improvement possible increases substantially as the x's

become more multicollinear.

       The particular class of estimators we will discuss is a slight exten-

sion   of the "ridge regression" estimators developed by Hoer] and Kennard
[1970] and studied by Wermuth [1972], Sciove [1973], Marquardt [1970],
Mayer and Willke [1973].
       We use the "weighted least squares" framework because it allows us to

use a suggestion of Tukey [1973] for doing robust regression and in effect to
combine ridge and robust regression methods. This combination is discussed
in Section 6.

2. Ridge Regression When There are    Wei9hts

       We now give a prescription for doing ridge regression when there is a

weight, w.,, associated with each observation. The weights are assumed to

be non-negative and they need not sum to unity --   but   they may if that is

convenient. In addition we also assume that we have a "prior mean" for .

This   will be amplified more fully in the next section. We denote the prior

mean for by 5. In the usual case of ridge regression,          is taken as zero,

and the weights, w., are all equal.

       In this paper, we always assume there is a constant term in the regres-

sion equation, but that this is not reflected in the choice of X-matrix.

Hence we assume that no column of X is constant. The constant term is
                                                    -3-




estimated separately from the other regression coefficients via the

following formula

                 A                 PA
                     0   =S-            jj                                (2-1)

where y and x. denote the weighted means of y and he j column of X,

respectively, i.e.,
                          E w.y.
                            11                  • w.x.
                                                z
                                                  113
                                                                          (2-2)
                          Ew.                   w.
                          i1
In (2-1), . denotes the estimate of the ji element of which we will

describe shortly.

       In the calculation of the regression coefficients, as opposed to

we assume that all variables have had their weighted mean (2-2) subtracted

out,


                '=y-                                      .
                                                                          (2-3)

The 'weighted" length of ''.           is   given by


                s. =1/w. "p..                                             (2-4)
                           'i 1        13

thus we give all the x. the same weighted length by setting

                X = 7(s)           1                                      (2-5)

       This scaling of the x's implies a rescaling of the 's via


                                                                          (2-6)


Hence the prior mean            must be rescaled also,


                *        =<s)    ó .
                                                                          (2-7)
                                            -4-

                                                                                          3.
         Having properly centered and scaled all variables we may now give the

 ridge regression estimates of the rescaled parameter,
                                                                      This is given by

                       =   * + (X*(w)   X + kI)_1X*T<w> (y - X*6*)      .
                                                                                  (2-8)

        The ridge regression estimator of         (rather than *) is given by


                           6 + T) + k(s2)         )(w) -)                         (2-9)

       We observe that         depends on the parameter, k. When k=O, then
                           R
   =        no matter what 6 is. When kc, then           =   6.   For intermediate
values of k, R     interpolates    between these extreme values.

       Hoerl   and Kennard [l97 suggest    using several values of k in       a diag-
nostic mode to identify those least-square parameter estimates which might
be improvable. Wermuth [l97 and Sclove [l97 suggest choosing k from the
data and obtaining a single point estimator of           rather than a one-parameter
family   of estimators. In Section 4 we discuss various choices of k that

are data dependent.

       In summary, the method of estimation we propose here is as follows.



           obtaining
                       #
                       y and X.
                               ,
       (a) Compute weighted means and subtract them from each variable,



       (b) Compute k via one of the methods discussed in Section 4.

       (c) Estimate the regression coefficients via equation (2-9),

           obtaining R
       (d) Estimate the constant term via equation
                                                        (2-1) using R       for
           the regression coefficients.


3. Motivations and Interpretations

    In this section, we shall give the Bayesian motivation for ridge
                                             — 5..




regression   as put forward     in    the previous section. In addition, we show

how ridge regression may be interpreted a a "smooth" selection of variables

method of estimating parameters.


Bayesian Background

     We begin with the statement of a useful lemma that allows us to pass

back and forth between conditioning U on V and then V on U when (u,v) has

a multivariate Gaussian distribution.


The back-and-forth lemma: If UJVl% Gau[a + B(V-d), C] and V".'Gau(d, E)

and if C and E are non-singular, then


                 (a) VU 'v Gau (d +(E_l4BTCB)_1BTC(U.,a), (E_.+BTC_lB))
             and (b) UAGau(a, C+BEBT).

The proof of the back-and-forth lemma is a straightforward exercise in

properties of the multivariate Gaussian distribution nd matrix algebra.

     Now suppose we consider the Bayesian analysis of the general linear

model given by


                 yIt3   it', GauN(X, a2 (w> _1)                              (3-1)


                 Gau(6,          A)                                          (3-2)


Shortly,   we shall specialize A to -r21, but for the moment we consider the

more general setting given in (3-2). Note that we do not give a prior dis-

tribution to 2 in this development. This is to keep the analysis simple.

In all cases we estimate 2 by the weighted residual mean square from the

least squares fit. This is


                        =
                            (N-p)1 w.        -
                                                 s))2                        (33)
                                                     -6-




Furthermore, we will often regard 2 as known and equal to the estimated

value, 02.   While this is not the way a full Bayesian analysis would proceed,

it is adequate for our purpose which is to motivate the procedure given in

the previous section.

     From the model (3-1) and (3-2) and the back-and-forth lema we may

obtain the posterior distribution of                  and the marginal distribution of y.


Theorem 1:   If yJA, GauN(X, 02(w) _1) and 3v Gau (S,A), then

(a) (Posterior distribution of )

             + (XT(w) X + 02A_l)_lxT<w> (y -                  X),
IY'-'Gau(6                                                          02(xT <w) X + G21)1)
(b) (Marginal distribution of y)
                             1
y..' GauN(X5, 2   (w)            +       xixT).

     From part (a) of Theorem 1 we see that if the prior covariance matrix

of   is taken as        =
                                                                                             )
                             -r21, then except for the replacement ofy by y and X

by X* the formula for I3 corresponds to the posterior mean of with


                  k =   —            .                                               (3-4)
                        T2

     Suppose we assume that                   is given, what conditions on would make the

assumption that


               A.Gau(5, T21)                                                        (3—5)


a reasonable one? By scaling the x's as we have, we have made them dimen-

sionless so that the * parameters reflect only the relative slopes of the

regression plane and not merely differences in the units in which the x's

are measured. The assumption (3-5) asserts that the . — ô. behave like a

sample from a Gaussian distribution with unknown variance and zero mean.
                                                                                             )
This is more plausible for the *'s than the original 's which may have
                                                   —7—




differing units. Finally, the constant term in a regression is usually of

quite a different character than the regression parameters.        It merely

centers the regression plane to pass through the "middled of the point cloud.

Hence we have centered each variable at its weighted mean and chosen             so

that the fitted regression plane passes through the point (.,            ...,

Thus      is not included in the parameters that have been given    pri ors.
       Under assumption (3-5) the posterior distribtion of    is

yA/ Gau(S + (XT(w) X + kI)_lXTw)(y - X5), 2(xT(w) X + kI1)                     (3-6)

and the marginal distribution of y is
                             -1
y.i Gau (X5, 2<                   ÷ T2 xxT)
                     w)                                                        (3-7)


Interpreeing Ridge Regression

       The Bayesian motivation for ridge regression may be satisfactory for

many purposes, but the following interpretation shows that it also has close

ties with regression on principal components.

       We may rewrite (2-8) in the following form:


               =   5* + (I        +   k(X*T(w) x*))1(    -                     (3-8)


where


                         =
                    is        '' is                                            (3...9)


       A particularly revealing form of ridge regression appears when we trans-

form to the "principle component axes." The usual orthogonal diagonalization

of X*T(w) X is given by


                   X*T(w)         x = v (x)   VT                               (3-10)


where V is pxp     orthogonal and (X)is the system of eigenvalues of X*T4w) X*.
                                             -8-



                                                                                               3
We   define the "principal component parameters" by


                         =
                             VT*                                                    (3-11)


and the corresponding transformed prior mean by


                   v                                                                (3-12)


One property of least squares is that


                   1* =                                                             (3-13)
                    LS

We may define        so that this is also true for ridge regression, i.e.,


                        =
                             VT                                                     (3-14)


       Starting with (3—8) and (3-14) we then obtain                                           )
                   1*   =    v +<.) (*   -
                                             v*)    .
                                                                                    (3-15)

Hence we see that the ridge regression estimators of the principal component

parameters are found by shrinking the least squares estimators of-y towards

'v' by an amount
 1                  that reflects the size of A.1       relative to k.   If x.   large, then
                                                                             1
         is   shrunk very little; when A. is small, then it is shrunk a lot.

Thus   when S = 0, y may be viewed as a type        of selection of variables tech-

nique using     the principle components as the variables and the size of the
eigenvalues as the selection criterion.
4. The Choice of k

       There are two types of choices of k which we shall discuss here. The

first type is in the spirit of empirical Bayes methods because prior param-

eters are estimated from the data. The second type is based on estimates of

certain optimum values of k.
                                                   -.9 -




      In   all cases 02 is treated as a known constant and set equal to its

estimated value 2 from (3-3). Furthermore, while the theoretical analysis

uses y, in the actual computations the centered values,                 are used. This

introduces an error of order N1 into the analysis, but this is more than

overcome by the resulting simplification in the resulting formulas.


Empirical Bayes Choices of k

       From (3-7) we have that the marginal distribution of y is given by

                                                    1
                  y' GauN(X*6*, 2 <w)                      + T2X*X*T)              (4-1)

       From (4-1) it follows that


(wi) yiv GauN( (wi) X6, o21 + -r2 <w X*X*T(wb> )                                   (4-2)

and hence that


                                   =   No2 + i2 trace(X*T(w) X*).
E[(y -     X)T<w)(y     -
                            X6)]                                                   (4..3)



If we let



                   UT(w)U I                                                        (44)

then (4-3) may be expressed as



                   E[LIY    -'l]        = N   02 + p T2                            (45)

since trace(X*Tw)X*) = p

       Therefore an unbiased estimate of T2 is given by


                   T2
                        :
                          (II'- '             -N   G2)/p .                         (4-6)
Thus the ratio of 02 to t2 yields a plausible though biased estimate of

k =   02/-r2.   We call this
                                                     -10-



                                              p O
                     k =                                                              (4—7)
                        a       __       __          -.
                                y    -   x fl2 - N
           Sciove [1973] suggests keeping this estimate of k positive by

replacing N by N-p. This yields


                    k1
                     a
                                —              p 2
                                                                                      (4-8)
                                IIt;7o 11W - (N_p)a2

           Alternatively we might use a "positive part" estimator of the form


                    ka2 = max(0,              k)                                      (4-9)


to keep k from being negative.

           Another set of empirical Bayes choices of k stem from the following

observation. If c2 is regarded as known, then                      is a sufficient statistic

(marginally) for -r2 so' that we may reduce by sufficiency to the marginal

dist1'ribution of


       =                                                                  + k11]). (4-10)
            (X*"<w) X*)_1X*T(w) yevGau(s*, 2[(x*T(w) X*)1

Equivalently, we may use the marginal distribution of

                    A            A
                            =

                            _ Gau(v*, a2(k +                x))                      (4-11)

If we set


                            =((k1 + 1))(*                   - *)                     (4-12)


then



                    5.s'Gau(0, 2I) .                                                 (4—13)
                                                          —11—




and we see that

                                         p (*1 -
                        *      2=                                 1

                                        1=1 k             +xT1

                                         a2 [chi-square on p d.f.]                                        (4-14)


Dempster (Wermuth [1972]) suggests settingli *fJ 2 equal to its expected

value and estimating a2 by G2. This yields the following equation for k

                       (y* -
                         1     1          —           2
                                                                                                           4-15
                  i     k +X       .1




We shall call the solution to (4-15) (if it exists) k. Sclove [1973]

suggests a method that is equivalent to the following observation.

II Qs112 and (N-p) a2 are independent with 2 x and
                                                                                         a2x,_
                                                                                                 distributions
respectively. Thus set
                                                  2

                  F
                      P, N—p
                               =1 L9                      •                                               (4-16)
                                   (N-p) a2


Sciove then suggest setting F        equal to its expected value, i.e.,
                             p, N— p


                  E(F                = NNP2                               (if N-p > 3)


This yields the following equation for k

                       (y_v*)2
                        2                                             N
                                              =               2           P
                  i    k +)I
We shall call the solution to (4-17) (if it exists) k.

     If we regard a2 = a2 as known, then the likelihood function for k based

on     is
                                                                              r
                                         (k1 +                                    .L
L(y,   k) =   (2ir)P/2 a                                                               1=1
                                                                                                          (4-18)
                                   1=1
                                                          A)exPt
                                                       -12-




Differentiating (4-18) in k yields the following equation for k

                  (y'-v)2
                   1   1                                               1
                                              =
                                                                                                      (4-19)
                I (k + A)2                                   I k +

We shall call the solution to (4-19) (if it exists) k.

      The expressions, k , k
                        a    al
                                  ,       k
                                              a2 kd k$
                                                  ,      ,        ,   and k ,
                                                                           m
                                                                                are     all of the empirical

Bayes choices for k that we will consider, except for some minor modifica-

tions we shall make later in this section.


Estimating Optimal k-values

      Consider the expected squared distance between                                and the true param-

eter value * (where expectations are now computed relative to the condition

distribution of y given * and 2) There is a value of k that minimizes

this squared distance, but it depends on the unknown parameters. If we use

the least squares estimates of                    in the resulting formula for the optimal

k-value we obtain a data-dependent choice of k that estimates this best

choice of k. This is the spirit in which we present the next two choices

of k. There are actually two meaningful "distances" in this problem and

we consider them in sequence. The first is the simple Euclidean distance

given by


                                          =
                Efl -                              E   E[(). -

              = EII-   -          =
                                              E[(y).
                                                              -
                                                                                                      (4-20)

But

                                      =
                                              E[v'     + d.((y*).          -   v)   -
                Ef(y).

                = E[d.((y). -             *) (*    -          -



                =d   E((y). *)2 + (l-d.)2(y' - v*)2
                                                  -13-




where d. =
       1     A./(A.
              1   1
                        + k)

     However, because t3ru          Gau(*,        02(x*T(w) X*y1)
we have

                            Gau(y, 02 (x) _1)            ,                                   (4-21)

and hence

                 E[(y). -       y*]2        d 2 xT1          + (l-d1)2(y -     v*)2

Therefore, we have



                 EI!        - *2        =         2 d                + (l-d.)2(y' -   v)2.   (4-22)


     If we differentiate (4—23) in k to find that value which minimizes the

expected squared (Euclidean) distance of                       to        we obtain the following

equation for k (after substituting y for y* and Q2 for o2)

                      kA.(y -
                        1   1     v)2
                                   1         =
                                                                    A.
                                                                     1
                                                   02                                        (4—23)
                i       (A. + k)3                        i (x. +


We shall denote the solution to (4-23) by
                                                             kob.
    The other notion of choseness that is relevant to this problem is the

expected squared weighted distance from              R XR toX. We now examine
the result of minimizing this quantity. We have


                EIIyR
                            - X         =
                                                 E[(R
                                                         -   )TT (w)          R -

                = E[( - *)TX*T(w x*(                         - *)]                           (4-24)

                =
                    E(y         y*)T<X)(y*        - *)
                =   E   A. E[(). -
                                                      -14-




Hence we may write


                 EJ!YR -
                                 XIJ2
                                             =
                                                      [d   02 + (l-d.)2A.(y' -     v*)2]   (4-25)


where d1 =   A.I(A.      + k). Minimizing in k produces the following equation

analogous to (4-23)

                      kX(y' - v*)2                                    2
                        1 1    1        —         =                   1
                                                       02 Z                                (4-26)
                 i       (A. + k)3                              (. + k)
We shall denote the solution to (4—26) by
                                                                k0.                 -




Further   Study of These Choices of k

     We now have eight possible methods for choosing k in ridge regression.

In order to thin down the candidates we begin by considering what they look

like in the important special case when the x's are orthogonal. By this we

mean that A.     1 which implies that
                                                                                                     )

                 x*T(w)          x=I.                                                      (4-27)

When this happens, all of the equations for determining k have easy solutions.

They are given by:


                     1       -     1
                                             -1            po2                               —
                                                                                           (428)
                i+kml+kd                              II*_*JI2
                                 l+k -l _______'N-p-2
                                                 ,N-p                                       429
                                                                                             -

                                                           LS

                     1           _1
                                  1+
                                                 =1-                  p2                   (4-30)
                                        k0                            6*112 + p2

Turning now to k a       we see that

                                                                                                    1)
                1+ka        =1—
                                                                      A
                                        y - X612           -   (N—p)a2
                                                                  —15—




                  IV'
                                "i li
But the following identity holds true

                                2=
                       y — xoJ' w  i
                                                         —
                                                                        •

                                                                       1j2
                                                                  .iSJ1w
                                                                                + ii y
                                                                                  jj
                                                                                       F'



                                                                                      LS
                                                                                            —

                                                                                                      flw
                                                                                                          2
                                                                                                                           (4—31)
                                                             A               A
                                         = (N-p)a2 + (
                                                      LS
                                                                                   - 6)TxT)                   X(Ls -

So that we have


                                J(2 = (N-p)o2 IjLS
                                    1w
                                               +I* - 6*112                                                                 (4-32)

and   hence we may express k              as
                                      a

                  ____
                  l+k = 1-
                   1                      ______________________________

                                                A                                                                          (4-33)
                                a
                                                             — 6*112
                                           II

Similarly we     have

                                                                            A

                   1+k
                            1
                                    =1 -        ______________________________________________

                                                     A
                                                                                                                           (4-34)
                                                                      - 6*112 ÷ PG2
                                                 II LS

and
                                                                                       F.

                  _______ = 1 — max(l,
                            1                                                     po2
                   1 +k                                                                          .)   •                    (4-35)
                                                                       lI LS
                                                                       U
                                                                          - 6*112
       Now   in this case (i.e., Ai                          1) we have
                   A

                       LS
                            " Gaup (*, o21)

with 02 an independent, chi-square distributed estimate of the common
variance     2
                                                                                                                       A
      James and Stein [1961] showed that in this type of situation                                                         can

be uniformly improved upon (in the sense of lowering the value of EII* - 3*1L2)
by an estimator of the form

                   *
                   A

                       JS                l+k
                                            1
                                                                 F'

                                                                  Ls
                                                                            - 6*)                                          (4-36)
                                                                  —16-




where       is given by

                                                                    6%
                                                              (
                               1

                                                      — —cj 0 2
                     l+kJS                     =
                               I




                                                              *LS _* 2
providing   that p exceeds 2. Further slight improvements can be achieved if

   is replaced by                              2 and if (l+ky1 is prevented from going negative
                              N-p+2
by a device like that used in (4-35). However, the bulk of the improvement

stems from the use of the factor (4-37). Comparing the corresponding values

for our proposed choices of k we see that k, kd. and k agree with

except for a Upil replacing the correct 'tp-211. The extra factor in
                                                                                                            k
appears to go in the wrong direction. kob,
                                                                              k0   and k1      all   agree on a

shrinking factor that is too small in general. If we use the value of

to calibrate the performance o1 our choices of k in the orthogonal case,

then we are motivated to alter the definitions of k                                       k       and k so that
                                                                                     a,       d       m
they agree with                    when A.                1. Because they fail to agree with                      we
will drop k          k             k               and k from further discussion.
              Ob ,       Oy
                              ,
                                    al     ,
                                                          S
     It is easy to change the definition of k so that it agrees with
                                                                                                                k5
in the orthogonal case. We shall use


                     k'a   =           -2            A
                                                     a2
                                                                         -.                                     (4—38)
                                       y       -
                                                   Xó112 - (N-2)         02

     It is also obvious how to change (4—15) so that kd agrees with                                               in

in the orthogonal case. We propose the following simple modification.

Instead of (4—15) use

                         (y' -
                           1
                                       v)2
                                        1                                A
                     z                                    = (p—2)        02                                     (439)
                         (k1 + AT1)
We shall call the solution to (4—39)
                                                                  k.
                                               —17—




       It is less obvious how to change (4-19) so that km agrees with             in

the orthogonal case. We suggest the following slight change in (4-19),

others might be better.

                      (.y* -
                        1
                               v)2
                                1                            1
                                          = (p-2)   G2 i                        (4-40)
                     ' (k1 + Al)2                          k +
We shall call the solution to (4-40) k'.

       We have now reduced our eight choices of k to 3, k', k' and k'. It
                                                        a   d      m
should be understood that none of this applies when p < 2 and that k is never

allowed to be negative for any of these choices. Equations (4-39) and (4-40)

may easily be solved (if solutions exist) by Newton's method starting at

k =   0.   When w.     1 and S =     0,   it is easy to show that (4-39) has a unique

solution if and only if the usual R2 exceeds (p—2)/N. Conditions for the

existence of solutions to (4-40) are similar in spirit bu1 more complicated.

       In order to distinguish further between k', k and k' we need compari-

sons of their respective performances. k' is appealing since it does not

require as much work to compute as the other two do.




5.    Combining Ridge and Robust Regression Methods

       Ridge regression was invented to deal with the problem of near multi-

collinearity in regression. Another problem that besets the user of

regression methods is outliers and other forms of non-Gaussian errors.

Robust regression methods have been proposed to deal with such problems

(see Huber [1972], Bickel [1973], Andrews [1973] and Tukey [1973] for reviews

of methods and discussions of current research). Considerable attention
                                              -18-



has been given to various versions of Huber's M-estimators. Tukey proposes

using    iteratively reweighted least squares as a device for computing
M-estimators and other related robust estimators. The weights are computed
sequentially from the residuals of the previous iteration. The end product
of Tukey's method is a set of weights w.} such           that the robust estimator

    is computed by

                           = (x (w)   x)_1xT(w) y                              (5-1)

        In   view    of   the analysis and development given in the previous sections

we propose here to use weighted ridge regression to combine ridge and robust

methods. The specific proposal is to take the weights found for the robust

method and do weighted ridge regression using formula (2-9) and (2-1). The

choice       of k is still problematic but two alternatives present themselves.
        (a) Use several k's in the diagnostic mode proposed by Hoerl and
              Kennard. This will help identify unstable parameter estimates.
        (b) Use k', k or k' computed from the data to obtain point
              estimates     of . Further work   is necessary to see if these choices

              of    k differ substantially.
     The expectation is that a ridgified robust estimator will combine the
benefits of both approaches and be no more difficult to compute than             or

R   separately.




                                                                                        )
                                    -19-.




                                 REFERENCES


Andrews, 0. [1973]. "Some Monte Carlo Results on Robust/Resistant Regression,"
(unpublished manuscript).

Baranchik, A. [1970].   "A family of minimax estimators of the mean of a
multivariate normal distribution," Annals of Math Statist 41, 642—645.

Bickel, P. [1973]. "On some analogues to linear combinations of order
statistics in the linear model ," Annals of Statist 1. 597—616.

Hoerl, A., and R. Kennard [1970]. "Ridge regression. Biased estimation
for nonorthogonal problems," Technometrics 12, 55-68.

Huber, P. [1972]. "Robust statistics: a review," Annals of Math Statist 43,
1041-1067.

James, W., and C. Stein [1961]. "Estimation with Quadratic loss," Proceedings
of the Fourth Berkeley Symposium 1, U. of Calif. Press, 361—379.

Marquardt, D. [1970]. "Generalized inverses, ridge regression, biased linear
estimation, and nonlinear estimation, Technometrics 12, 591-611.

Mayer, L., and T. Wilike. "On biased estimation in linear models,"
Technomety-ics 15, 497-508.

Sciove, S. [1968]. "Improved estimators for coefficients in linear regression,"
JASA 63, 596-606.

Sciove, S. [1973]. "Least squares with random regression coefficient,"
Technical Report 87, Economic series, Stanford University.

Tukey, J. [1973]. "A way forward for robust regression," (unpublished m.s.).

Wermuth, N. [1972]. An Empirical Comparison of Regression Methods.
Unpublished doctoral dissertation, Harvard Univeristy, Department of Statistics.
