                                 NBER WORKING PAPER SERIES




          NONPARAMETRIC OPTION PRICING UNDER SHAPE RESTRICTIONS


                                           Yacine Ait-Sahalia
                                            Jefferson Duarte


                                          Working Paper 8944
                                  http://www.nber.org/papers/w8944


                       NATIONAL BUREAU OF ECONOMIC RESEARCH
                                1050 Massachusetts Avenue
                                  Cambridge, MA 02138
                                       May 2002




We are grateful to seminar and conference participants, and in particular René Garcia, for their comments
and suggestions. The comments of the Editors and three referees were very helpful. This research was
conducted during the first author’s tenure as an Alfred P. Sloan Research Fellow. Financial support from the
NSF under grants SBR-9996023 and SES-0111140 (Aït-Sahalia) and from the Center for Research in
Security Prices at the University of Chicago Graduate School of Business (Duarte) is gratefully
acknowledged. The views expressed herein are those of the authors and not necessarily those of the National
Bureau of Economic Research.


© 2002 by Yacine Ait-Sahalia and Jefferson Duarte. All rights reserved. Short sections of text, not to
exceed two paragraphs, may be quoted without explicit permission provided that full credit, including ©
notice, is given to the source.
Nonparametric Option Pricing under Shape Restrictions
Yacine Ait-Sahalia and Jefferson Duarte
NBER Working Paper No. 8944
May 2002
JEL No. G12, C14



                                             ABSTRACT

        Frequently, economic theory places shape restrictions on functional relationships between
economic variables. This paper develops a method to constrain the values of the first and second
derivatives of nonparametric locally polynomial estimators. We apply this technique to estimate the state
price density (SPD), or risk-neutral density, implicit in the market prices of options. The option pricing
function must be monotonic and convex. Simulations demonstrate that nonparametric estimates can be
quite feasible in the small samples relevant for day-to-day option pricing, once appropriate
theory-motivated shape restrictions are imposed. Using S&P500 option prices, we show that
unconstrained nonparametric estimators violate the constraints during more than half the trading days in
1999, unlike the constrained estimator we propose.




Yacine Ait-Sahalia                                       Jefferson Duarte
Department of Economics                                  Department of Finance and Business Economics
Princeton University                                     University of Washington
Princeton, NJ 08544-1021                                 267 MacKenzie Hall
and NBER                                                 Box 353200
yacine@princeton.edu                                     Seattle, WA 98195-3200
                                                         jduarte@u.washington.edu
1    Introduction

In many settings, economic theory only restricts the direction of the relationship between variables,
not the particular functional form of their relationship. Typically, a theory would predict that some
economic variable Y should increase when some other variable X increases. Beyond that, the typical
economic theory is often not very restrictive about the speciÞc nature of the relationship between
Y and X, and if it is, it is often as a result of choosing a particularly tractable model which the
theorist understands to be for illustrative purposes only. Sometimes, economic theories manage
to put additional restrictions on the shape of the function that links X to Y . For instance, the
relationship may be predicted by the theory to be not only monotonic, but also concave. Or it may
satisfy some other inequality restrictions on the function and/or its derivatives. Or the function
may be homogenous of some degree, or homothetic (i.e., a positive monotonic transformation of the
function is homogenous of degree one).
    Examples of this nature abound in economics. The cost function of a standard perfectly com-
petitive Þrm must be increasing and convex. For such a Þrm, the production function linking its
inputs and outputs must be increasing and concave. The utility function of a typical economic agent
must be increasing and concave. In fact, the most speciÞc result in this literature, Afriat’s Theorem,
states that a utility function can be found to rationalize a set of observations on prices and quantities
if and only if it is nonsatiated, continuous, concave and monotonic [see Afriat (1967)]. No speciÞc
functional form can be deduced from the axioms of utility theory, yet one would often parametrize
the utility function as an exponential function, a power function, a logarithmic function or rely on
more complex functional forms.
    Of course, stringent parametric assumptions are very useful for a variety of reasons. First, they
allow extrapolation beyond the support of the observed data. Many economic policy questions
require that hypothetical experiments be performed in the context of the model (what would the
eﬀect of a tax cut be on consumption and investment?). Strategic decisions made by Þrm also
require extrapolation (how would proÞts be aﬀected if prices were raised further?). Second, it is
easy to specify a functional form that will necessarily satisfy the theory-determined restrictions
(for example, Y = Ln(X) will always be increasing and concave). Indeed, the common approach
in empirical work, for example in microeconometrics, has been to specify parametric functional
forms which satisfy the necessary shape restrictions [see e.g., Diewert (1973)]. Third, more general
parametric models can be built and tested against nested models that satisfy the restrictions imposed
by the theory to see if these restrictions are valid. For instance, if the function is predicted to be
increasing and concave and the adopted model is Y = X ρ , an estimate of ρ can be readily used to


                                                   1
test the concavity restriction, i.e., 0 < ρ < 1. Fourth, the theoretical restrictions can be imposed and
result in a decrease in the variance of the estimated parameters.
   Despite all their advantages, parametric assumptions have their drawbacks. First, any speciÞ-
cation error will typically lead to inconsistent estimates. Second, any test of the theory such as
that described above is a joint test of the theory and the (essentially arbitrary) parametric model.
Changing the parametric speciÞcation of the model will produce diﬀerent answers. As a result, non-
parametric methods are often used in empirical work, at least as a Þrst step in the analysis of the
data useful to guide the speciÞcation eﬀort. With nonparametric methods, it becomes possible to
examine say, whether Y increases with X, without assuming a particular model for the conditional
expectation of Y given X. Unfortunately, nonparametric estimators pay for their robustness to
speciÞcation errors in other ways. They converge more slowly than their parametric counterparts,
thereby requiring a larger sample size to achieve the same degree of accuracy —often, but not always,
a small price to pay for the elimination of misspeciÞcation risk. Moreover, their rate of convergence
deteriorates even further when derivatives of the function are estimated. Consequently, in small
samples, the estimated Þrst and second derivatives of the function of interest can often fail to satisfy
the restrictions that the theory imposes, simply because of sampling noise.
   It is therefore quite natural for the literature to have evolved towards estimates that are non-
parametric in nature, yet satisfy whatever theory-motivated properties are appropriate. The main
body of literature deals with the use of monotone restrictions to estimate a nonparametric regression
[see Barlow et al. (1972), Robertson et al. (1988) and Matzkin (1994) for an excellent survey]. A
common model is Y = m(X) + ε, where either the expected value or the median of ε given X is zero
and m(·) is estimated by minimizing the least squares or least absolute deviations of the residuals,
under the constraint that it be monotonous. Brunk (1970) and Hanson and Pledger (1973) proved
the consistency of the estimator under diﬀerent assumptions.
   While the rate of convergence of the least squares estimator is available [see Wright (1981)], its
asymptotic distribution is not yet known. The estimation of concave regression functions (same
context as above except that m(·) is known to be concave) has also been extensively considered
[see e.g., Hildreth (1954) and Hanson and Pledger (1976)] and its distribution is known in the least
squares case [see Wang (1993)]. Finally, algorithms that extend Hildreth’s to estimate a regression
curve under inequality restrictions have been proposed by Dykstra (1983) and Ruud (1997), again
in the constrained least squares context. To the best of our knowledge, the asymptotic distribution
of these estimators is unknown.
   Rather than attempt to solve the least squares (or least absolute deviations) problem, we propose
in this paper a method to impose shape restrictions as a simple modiÞcation of nonparametric


                                                   2
locally polynomial estimators. The standard Nadaraya-Watson kernel regression estimator is a special
case of a locally polynomial estimator, corresponding to a “locally constant” speciÞcation, i.e., a
polynomial of order zero. By modifying locally polynomial estimators, instead of attempting to
devise a new type of constrained nonparametric estimator, we can rely on a well-understood set of
tools in the unconstrained regression case [see e.g., Fan and Gijbels (1996)]. Moreover, our estimators
are smooth like any other kernel-type regression estimator, unlike for instance the estimator produced
by solving the constrained least squares problem. Our constrained nonparametric estimators satisfy,
by construction, the restrictions imposed by economic theory. We focus on locally linear estimators
and on the case where inequality constraints are imposed on the regression function and its Þrst two
derivatives.
   As is often the case, and the estimation of option-implied densities in Þnance is no exception, there
are many diﬀerent ways to smooth a curve —Nadaraya-Watson kernel regression as in Aït-Sahalia and
Lo (1998), splines with a penalty for lack of smoothness [e.g., Mammen and Thomas-Agnan (1998)],
constrained splines [Bates (2000)], ßexible parametric functional forms [in the context of SPDs, see for
example Abadir and Rockinger (1998)], neural networks [see Garcia and Gencay (2000) and Haefke,
White and Gottschling (2000)], etc. Bates’s paper in particular considers cubic splines estimated
under the same constraints as ours, while Bondarenko (1997) considers the same constrained least
squares problem we do.
   We focus on a particular method, locally polynomial regression. In our view, locally polynomial
estimators present a few advantages, some of which are shared by the other possible choices. First,
they are truly nonparametric. Second, they have well-documented good small sample behavior [see
e.g., Fan and Gijbels (1996)], especially relative to kernel regression estimators. Third, we are able
to implement the method in such a way that the locally polynomial estimator will always produce
estimates satisfying the constraints, which is also possible with some of the other methods, but
in our case turns out to require no modiÞcation to the estimator, only its application to some
transformed data. This said, we do not mean to suggest that local polynomials are necessarily a
dominating alternative to everything else nonparametric (otherwise there would not be such a long
list of available methods!), but rather our objective is to add to the nonparametric toolkit by showing
how this particular method can be amended to reßect shape constraints, especially those that are of
interest in derivative pricing. This is achieved in our main theoretical result, Proposition 1, which we
hope will be of independent interest beyond our application to the estimation of state-price densities.
   Our estimator extends the results of Mammen (1991). Mammen introduced a two-step kernel
regression that results in monotonic estimates. We extend Mammen’s results in three directions.
First, we incorporate restrictions in the Þrst and in the second derivatives, which is empirically


                                                   3
relevant in a large number of economic contexts. Second, we work with locally polynomial estimators
(locally linear in our speciÞc context) as opposed to the Nadaraya-Watson kernel regression estimator
used by Mammen, which is a locally constant polynomial estimator. Third, we allow for a broader
class of kernel functions than just the Gaussian kernel; in particular, the popular uniform and
Epanechnikov kernels are admissible.
        The remainder of the paper is organized as follows. We start in Section 2 by describing the main
example that motivates this paper, the kernel estimation of the state-price density implicit in the
market prices of traded options. In Section 3 we introduce our estimator and compare it to the
unconstrained Nadaraya-Watson and locally linear nonparametric estimators. We show in particular
that our estimator will satisfy the constraints imposed in sample and not just asymptotically. The
results of a Monte-Carlo analysis of these three estimators are presented in Section 4. In Section 5,
we apply our methodology to option pricing. Section 6 concludes. Technical proofs and results are
in the Appendix.


2        Monotonicity and Convexity of Option Pricing Functions

The motivation for our empirical work is the theory-imposed restriction that the price of a call option
must be a decreasing and convex function of the option’s strike price. Assuming that markets are
dynamically complete, the absence of arbitrage opportunities implies the pricing operator is linear.
Continuity and linearity of the pricing operator implies by the Riesz representation theorem the
existence of a state-price density (SPD), which we denote by p∗ (ST |St , τ , rt,τ , δ t,τ ).1 The call pricing
function at time t is then given by:


                                                               Z   +∞
                                                     −rt,τ τ
                  C(St , X, τ , rt,τ , δ t,τ ) = e                      max(ST − X, 0)p∗ (ST |St , τ , rt,τ , δ t,τ )dST   (2.1)
                                                               0
    1
        The existence and characterization of an SPD can be obtained either in preference-based equilibrium models, e.g.,
Lucas (1978), Rubinstein (1976), or in the arbitrage-based models by Black and Scholes (1973) and Merton (1973). In
the equilibrium framework, the SPD can be expressed in terms of a stochastic discount factor or pricing kernel such
that asset prices are martingales under the actual distribution of aggregate consumption after multiplication by the
stochastic discount factor.
    Among the no-arbitrage models, the SPD is often called the risk-neutral density based on the analysis of Cox and Ross
(1976) who observed that the Black-Scholes formula can be obtained by assuming that all investors are risk neutral and,
consequently, all assets in such a world must yield an expected return equal to the risk-free rate of interest. The SPD
also uniquely characterizes the equivalent martingale measure under which all asset prices discounted at the risk-free
rate of interest are martingales [see Harrison and Kreps (1979)], and the state-price deßator [see Duﬃe (1996)].




                                                                          4
where St is the underlying asset price at date t, X the strike price, τ the time-to-expiration, T = t+τ
the expiration date, rt,τ the risk free interest rate for that maturity, and δ t,τ the corresponding
dividend yield of the asset. In what follows, we will leave the conditioning information implicit, and
write p∗ (ST ) for p∗ (ST |St , τ , rt,τ , δt,τ ).
    In order to rule out arbitrage opportunities, C must be a decreasing function of X and the Þrst
derivative of C with respect to X must be greater than −e−rt,τ τ . This follows from (2.1) since
                                                                    Z +∞
                         ∂C(St , X, τ , rt,τ , δ t,τ )      −rt,τ τ
                                                       = −e              p∗ (ST )dST             (2.2)
                                  ∂X                                 X

thus from the positivity of the density and its integrability to one

                                                           ∂C(St , X, τ , rt,τ , δ t,τ )
                                         −e−rt,τ τ ≤                                     ≤0                      (2.3)
                                                                    ∂X

    By diﬀerentiating the call price function twice with respect to the strike price, one obtains, as in
Breeden and Litzenberger (1978) and Banz and Miller (1978):

                                      ∂ 2 C(St , X, τ , rt,τ , δ t,τ )
                                                                       = e−rt,τ τ p∗ (X) ≥ 0                     (2.4)
                                                 ∂X 2

i.e., ∂ 2 C(·)/∂X 2 is proportional to a probability density function and hence must be positive. Any
local non-convexity of the call pricing function implies negative state prices, which constitute a
violation of the no arbitrage principle.
    Thus the Þrst two derivatives of the “cross-sectional” option pricing function X 7−→ Ct,τ (X) ≡
C(St , X, τ , rt,τ , δ t,τ ) for given (St , X, τ , rt,τ , δ t,τ ), i.e., at each point in time t and for each maturity
τ , must satisfy the set of inequality constraints
                                       
                                        −e−rt,τ τ ≤ C 0 (X) ≤ 0
                                                      t,τ
                                                                                                                 (2.5)
                                        C (X) ≥ 0.
                                           00
                                                     t,τ

    The theory also imposes no arbitrage bounds for the call option pricing function itself:
                                 ³                          ´
                              max 0, St e−δt,τ τ − Xe−rt,τ τ ≤ Ct,τ (X) ≤ St e−δ t,τ τ .                         (2.6)

                                                     00 (X) ≥ 0 implies C (X) ≥ 0. Secondly, if
Note Þrst that it follows from (2.1) and (2.4) that Ct,τ                 t,τ

the price Ft,τ at t of a forward contract for delivery of the underlying asset at date T = t + τ is
observable, then by no arbitrage
                                                                 Z       +∞
                                                       −rt,τ τ
                                          Ft,τ   = e                          ST p∗ (ST )dST
                                                                     0
                                                 = St exp((rt,τ − δ t,τ )τ ).                                    (2.7)



                                                                   5
In this case, it follows from the fact that max(ST − X, 0) ≤ ST and from (2.1) and (2.7) that
Ct,τ (X) ≤ Ste−δ t,τ τ . It also follow these equations and the fact that p∗ is a density that Ct,τ (X) ≥
St exp(−δ t,τ τ ) − X exp(−rt,τ τ ). Indeed,
           n                                   o   Z +∞                      Z +∞
  rt,τ τ                   −δt,τ τ     −rt,τ τ                      ∗
 e          Ct,τ (X) − St e        + Ke          =      (ST − X)p (ST )dST −      ST p∗ (ST )dST + X
                                                    X                         0
                                                   Z X
                                                 =     (X − ST )p∗ (ST )dST
                                                         0
                                                     ≥ 0.

                                                             00
     These restrictions can be expressed as restrictions on Ct,τ (X), by writing them in the form
                                            Z   +∞
                                                      00
                                                     Ct,τ (X)dX = e−rt,τ τ                           (2.8)
                                              0
                                        Z    +∞
                                                    00
                                                  XCt,τ (X)dX = Ft,τ .                               (2.9)
                                         0

Therefore, the constraints imposed by the theory can all be summarized in terms of the functions
 0 (X) and C 00 (X), and our primary objective in this paper will be to construct nonparametric
Ct,τ        t,τ
                                   0 (X) and C 00 (X) that satisfy the constraints (2.5), (2.8) and
estimators of the functions X 7−→ Ct,τ        t,τ

(2.9).
     Aït-Sahalia and Lo (1998) proposed to estimate the SPD nonparametrically by using market
prices to estimate an option-pricing formula Ĉ(·) nonparametrically, then diﬀerentiate this estimator
twice with respect to X to obtain ∂ 2 Ĉ(·)/∂X 2 . Under suitable regularity conditions, the convergence
(in probability) of Ĉ(·) to the true option-pricing formula C(·) implies that ∂ 2 Ĉ(·)/∂X 2 will converge
to ∂ 2 C(·)/∂X 2 . Consequently, to arrive at the SPD from (2.4) it is suﬃcient to estimate the second
derivative of the call price function in relation to the strike price. Without any restrictions on the
full nonparametric regression of call prices of stock value, strike, time-to-maturity, interest rate and
dividend yield, the estimates are too variable to be useful in practice. Therefore Aït-Sahalia and Lo
(1998) reduced the dimensionality of the regression function by using a semiparametric speciÞcation.
Suppose that the call pricing function is given by the parametric Black-Scholes formula

                           CBS (Ft,τ , X, τ , rt,τ ; σ) = e−rt,τ τ {Ft,τ Φ(d1 ) − XΦ(d2 )}          (2.10)

where Ft,τ = St exp((rt,τ − δ t,τ )τ ) is the forward price for delivery of the underlying asset at date T
and

                                      ln(Ft,τ /X) + (σ2 /2)τ                   √
                               d1 ≡             √            ,      d2 ≡ d1 − σ τ                   (2.11)
                                               σ τ

except that the volatility parameter for that option is a nonparametric function σ(X/Ft,τ , τ ) of the

                                                             6
option’s moneyness Mt,τ ≡ X/Ft,τ and time-to-maturity τ :

                         C(St , X, τ , rt,τ , δ t,τ ) = CBS (Ft,τ , X, τ , rt,τ ; σ(X/Ft,τ , τ )).          (2.12)



   In this semiparametric model, they only need to compute the lower-dimensional kernel regression
of implied volatilities on moneyness Ft,τ , X and τ to estimate σ̂(·). The rest of the call pricing
function C(St , X, τ , rt,τ , δ t,τ ) is parametric, thereby substantially reducing the sample size of options
required to achieve the same degree of accuracy as the full nonparametric estimator. This approach
nevertheless has its own drawbacks. First, it is not fully nonparametric. Second, it still requires a
fairly large sample size to be eﬀective. In a typical cross-section of options at one point in time, one
often observes the prices of 20 to 50 options with diﬀerent strike prices (for a given maturity). This
limitation of the traded strikes is a consequence of a deliberate strategy on the part of the options
exchanges to insure that the market for each one of them remains suﬃciently liquid. Enlarging the
sample by gathering data from diﬀerent dates is useful for data description purposes but opens the
door to potential nonstationarity and regime shift issues. Moreover, the inputs of interest, such as
the underlying assets price, its volatility or the interest rate, can be volatile enough to preclude
aggregating data from diﬀerent days.
   Finally, it is possible for the implied volatility smile function σ(X/Ft,τ , τ ) to have suﬃciently
large derivatives with respect to the option’s moneyness Mt,τ for the resulting semiparametric SPD
to violate the nonnegativity constraint, especially for long-term options. That is, diﬀerentiating
(2.12) yields
                  
                     ∂C    ∂C BS     1    ∂σ ∂CBS
                      ∂X = ∂X + F          ∂M ∂σ
                                                                    ¡ ∂σ ¢2
                     ∂ 2C     2C
                           = ∂ ∂X BS
                                     +     2 ∂σ ∂ 2 CBS
                                                           +    1             ∂ 2 CBS
                                                                                        +    1 ∂ 2 σ ∂CBS
                      ∂X 2        2        F ∂M ∂X∂σ           F2    ∂M         ∂σ2         F 2 ∂M 2 ∂σ

and the right hand sides of these expressions need not satisfy the respective constraints that their
left hand sides should satisfy.
   Non- and semiparametric estimators of the call pricing function will satisfy the restrictions in
the Þrst and second derivatives only when the sample is large enough, and the true function veriÞes
them. This follows simply from the pointwise convergence of nonparametric regression estimators
and their derivatives. As in all the other examples from economic theory discussed above, nonpara-
metric estimates may violate the theory-imposed convexity restriction, but parametric estimates can
misspecify interesting properties of the SPD (such as its skewness and kurtosis patterns) because
they are overly rigid.
   As a result, the estimation of the SPD is an empirical problem where the sample size is small, and


                                                            7
where economic theory places no restrictions on the function other than the inequality restrictions
(2.5). Because of the potential risk involved in misspecifying the SPD, it is desirable not to impose
tight parametric restrictions on the density. And the constraints imposed by the theory provide no
guidance whatsoever in terms of specifying a parametric model for the SPD. In fact, as long as the
candidate parametric SPD is a proper density function, no matter how it is speciÞed parametrically,
the constraints will be satisÞed. Moreover, only when suﬃciently strong assumptions are made on
the underlying asset-price dynamics can the SPD be obtained in closed form. For example, if asset
prices follow geometric Brownian motion and the riskfree rate is constant, the SPD is log-normal–
this is the Black-Scholes/Merton case. For more complex stochastic processes, the SPD cannot be
computed in closed-form and must be approximated by numerically intensive methods. So this is a
typical situation where we need a nonparametric estimator that can be constrained to satisfy given
shape restrictions.


3     Constrained Nonparametric Estimation

To obtain a nonparametric estimator satisfying the required shape properties, we use a combination
of constrained least squares regression and smoothing.


3.1    Constrained Least Squares Regression

The problem of constrained least squares regression consists in Þnding the closest values mi , in
the sense of least squares, to a set of n observations y1 , y2 , ..., yn satisfying a set of constraints.
Bondarenko (1997) also uses constrained least squares in the same context as we do. The constraints
involve n observations on an explanatory variable, x1 , x2 , ..., xn . In our case, yi is the price of the call
option with strike xi . Without loss of generality assume that the observations on the explanatory
variable have been ordered, i.e., xi ≥ xj for i > j, i, j ∈ {1, 2, ..., n}.
    The constrained least squares regression consists in Þnding the vector m that solves, for the
observation vector y :


                                        n
                                        X
                                   minn   (mi − yi )2 = minn km − yk2                                    (3.1)
                                  m∈R                            m∈R
                                            i=1

subject to the slope and convexity constraints:
                        
                         −e−rt,τ τ ≤ mi+1 −mi ≤ 0               for all i = 1, ..., n − 1
                                       xi+1 −xi
                                                                                                         (3.2)
                         mi+2 −mi+1 ≥ mi+1 −mi                  for all i = 1, ..., n − 2
                               xi+2 −xi+1         xi+1 −xi



                                                             8
   If we were only imposing monotonicity of the pricing function, then this would reduce to the
classical isotonic regression [see e.g., Barlow et al. (1972)]. We can eliminate some constraints that
are redundant. The convexity constraints insure that the slopes Mi+1,i ≡ (mi+1 − mi )/(xi+1 − xi )
are nondecreasing. Therefore the inequality constraints on the interior slopes (i = 2, ..., n − 2) are
redundant and only the boundary slope constraints (lower bound for i = 1 and upper bound for
i = n − 1) matter. Therefore the constraints (3.2) can be rewritten as
                       
                        m1 −m2 ≥ −e−rt,τ τ and m
                           x1 −x2                       n−1 − mn ≥ 0
                                                                                                   (3.3)
                        i+2 i+1 ≥ i+1 i for all i = 1, 2, ..., n − 2
                           m   −m       m    −m
                              xi+2 −xi+1     xi+1 −xi

This reduces the total number of constraints from 2n − 3 to n, which has computational implications
when n is moderately large.
   Note that the price constraint corresponding to (2.5) can be imposed as
                   ³                             ´
                max 0, St e−δ t,τ τ − xi e−rt,τ τ ≤ mi ≤ St e−δ t,τ τ   for all i = 1, ..., n.

In light of the monotonicity constraints already present, these n constraints can be reduced to

                       St e−δt,τ τ − x1 e−rt,τ τ ≤ m1 ≤ Ste−δ t,τ τ   and mn ≥ 0.                  (3.4)

In any event, the three additional constraints (3.4) need not be implemented at this stage. As we
discuss later in Section 3.6, we will obtain an estimator of the pricing function Ct,τ directly from the
                           00
SPD estimator, i.e., from Ct,τ up to discounting, and provided the SPD estimator satisÞes constraints
(2.8)-(2.9) —which we will insure— our price function estimator will satisfy the constraints (2.5).
   When the strike prices are equally spaced, xi+1 − xi = ∆x for all i, which is the case in most if
not all options markets, the second constraint in (3.3) becomes

                                           mi+2 + mi − 2mi+1 ≥ 0                                   (3.5)

which says that the butterßy portfolio constructed by buying a call struck at xi+2 , one struck at xi
and selling two calls struck at xi+1 must have a nonnegative price.
   When solving the constrained minimization problem, we are eﬀectively “cleaning” the data yi in
a non-arbitrary manner. Of course, we mean to apply this step after obvious data recording errors
(such as a price recorded as 0, etc.) have been corrected. Solving this problem can be contrasted
to the commonly used practice of simply deleting from the sample the recalcitrant observations —
those that fail to satisfy the arbitrage restrictions— under the rationale that they must be the result
of unacceptable measurement errors. Besides being questionable as a general practice, deleting



                                                        9
observations can be quite damaging when the sample is tiny to start with.
     Naturally, in cases where the constraints are satisÞed by the original option prices, the solution
is simply mi = yi for all i = 1, 2, ..., n. But how often is this not the case empirically? Based on the
full year 1999, violations of the constraints (3.3) occurred 24% of the time in the raw high frequency
S&P 500 index option data from the Chicago Board Options Exchange (lower frequency observations
have lower violation occurrences). Hentschel (2001) provides more evidence regarding how noisy the
raw option data are.
     Finally, the least squares criterion function (3.1) can be weighted as in
                                               n
                                               X
                                          minn   (mi − yi )2 ω i                                   (3.6)
                                         m∈R
                                               i=1

to reßect the relative liquidity of diﬀerent options. In this framework, more actively traded options
would receive a higher weight ω i than those less actively traded. Readily available data can be used
for that purpose. In transaction-level data, the actual weights can be determined on the basis of
the size and time of the most recent transaction and the bid-ask spread. In closing prices, the open
interest and the bid-ask spread can be used to proxy for liquidity.
     Solving the constrained least squares problem has a long history. Von Neumann (1950) originally
proposed to solve it using alternative projections. While this insight remains at the heart of the more
modern algorithms, Von Neumann’s approach was limited in the possible set of constraints. Hildreth
(1954), then Dykstra (1983) progressively extended the set of possible constraints to convex cones (a
cone is such that if the solution vector m belongs to it then λm also belongs to it for any constant λ).
This would suit our purposes, except that the lower bound constraint on the slopes in equation (3.3)
make that constraint aﬃne (a convex set) instead of linear (a convex cone). We show in Appendix
A that we can Þrst transform it to one with conic constraints, to which we can then apply Dykstra’s
algorithm. We also describe Dykstra’s algorithm, applied to the transformed problem, in Appendix
A.


3.2    Locally Polynomial Kernel Smoothing

We now have the transformed data mi . The transformed data (not yi ) then serve as inputs to the
next and last step in our procedure. This step involves smoothing the transformed data mi and we
wish to do so in a way that preserves the constraints that were enforced in the previous step.
     Let us now turn to a brief description of locally polynomial regression, which allows us also
to introduce some notation. Suppose that the regression function m(z) ≡ E [Y |Z = z] is to be
approximated locally for z in a neighborhood of a given state value x by Taylor’s formula up to order


                                                     10
p
                                                   p
                                                   X
                                       m (z) ≈           β k (x) × (z − x)k                         (3.7)
                                                   k=0

with β k (x) ≡ m(k) (x) /k!. This representation of the function m suggests modeling m(z) around x
by a polynomial in z, and to use the regression of m(z)on powers of (z −x) to estimate the coeﬃcients
β k . To insure that the estimated coeﬃcients reßect the local nature of the representation, we should
intuitively use a weighted regression putting more weights on points close to x. A natural way to
achieve this is to introduce a kernel function K(.), a bandwidth h and to use as weights Kh (xi − x) ≡
K ((xi − x)/h) /h . This leads to the estimates of the coeﬃcients β b (x) as the minimizers of
                                                                                  k

                           n
                                 (       p
                                                                       )2
                           X             X
                                  mi −         β k,p (x) × (xi − x)k        Kh (xi − x)             (3.8)
                           i=1           k=0

which is, at each Þxed point x, a generalized least squares regression of the m0i s on powers of the
(xi − x)0 s with diagonal weight matrix formed by the weights Kh (xi − x) . This regression is “local”
in the sense that the regression coeﬃcients in equation are only valid in a neighborhood of each point
x.
     The estimates of the regression function (and its successive derivatives) are then given by

                                                               b (x) .
                                     m̂(k) (x) ≡ m̂k,p (x) = k!β                                    (3.9)
                                                                 k,p


                        b 0,p (x) is the coeﬃcient of the constant term in the polynomial regression
In particular, m̂ (x) ≡ β
of degree p. In this framework, the classical Nadaraya-Watson kernel regression corresponds to the
special case of a “locally constant” estimator where the polynomial is reduced to a constant term,
i.e., p = 0. Indeed,
                                               P
                                               n                            P
                                                                            n
                                                   Kh (xi − x) mi               ki mi
                                               i=1                       i=1
                                 m̂0,0 (x) =     Pn                  =     P
                                                                           n                       (3.10)
                                                       Kh (xi − x)                ki
                                                 i=1                        i=1

where the heteroskedastic weights are ki = Kh (xi − x) , is the generalized least squares (GLS)
regression coeﬃcient of the mi ’s on a constant.




                                                          11
   More generally, the GLS estimator β̂ p = (β̂ 0,p , β̂ 1,p, . . . , β̂ p,p )0 can be written as
                                                                        −1               
                                       Sn,0    Sn,1    ···       Sn,p                Tn,0
                                                                                         
                                                                                         
                                    Sn,1 Sn,2 · · ·            Sn,p+1         Tn,1       
                            β̂ p = 
                                    ..      ..   ..               ..
                                                                         
                                                                         
                                                                               
                                                                                ..
                                                                                            
                                                                                                      (3.11)
                                    .        .      .              .          .          
                                                                                         
                                     Sn,p Sn,p+1 · · ·          Sn,2p            Tn,p

where
                              Xn                                         Xn
                     Sn,j =            (xi − x)j ki   and Tn,j =                   (xi − x)j mi ki .   (3.12)
                                 i=1                                         i=1

   The sums Sn,j and Tn,j depend on x, but we leave that dependence implicit to keep the notation
simple. In particular if p = 0 (Nadaraya-Watson case), m̂0,0 (x) = Tn,0 /Sn,1 , while if p = 1 (locally
linear regression), we have

                                                           Sn,2 Tn,0 − Sn,1 Tn,1
                                    m̂0,1 (x) = β̂ 0,1 =                   2                           (3.13)
                                                            Sn,2 Sn,0 − Sn,1

which can be rewritten in the form
                                                         Pn
                                                          i=1 wi mi
                                              m̂0,1 (x) = P n
                                                            i=1 wi

where the regression weights are wi ≡ ki {Sn,2 − (xi − x) Sn,1 } compared to ki in the Nadaraya-
Watson case of equation (3.10). Therefore the locally linear estimator assigns weights that are
asymmetric, whereas the Nadaraya-Watson weights are always symmetric. This turns out to be a
critical improvement especially when x is near the boundaries of the support, i.e., in the tails of
the distribution. There, the locally polynomial regression assign weights that adjust for the relative
scarcity of the data, unlike those assigned by the locally constant Nadaraya-Watson estimator.


3.3     Estimation of Derivatives

To estimate the derivative of order k of the regression function m, we can simply set p = k + 1 and
use the estimator m̂k,p obtained from equation (3.9). For instance, a locally linear regression serves
to estimate the regression function m̂0,1 , a locally quadratic regression for the Þrst derivative m̂1,2
and a locally cubic regression for the second derivative m̂2,3 . This is generally the optimal choice on
the basis of asymptotics (see (3.17) below). But alternatives are available, and they may outperform
the asymptotic optimum in small samples. The Nadaraya-Watson estimator in equation (3.10) can
easily be diﬀerentiated to yield an estimator of the partial derivative of m(x) with respect to x.




                                                           12
                                               P               P             P      0
                                              ( ni=1 ki0 mi ) ( ni=1 ki mi )( ni=1 ki )
                                m̂00,0 (x)   = Pn            −       P                                (3.14)
                                               ( i=1 ki )          ( ni=1 ki )2

where ki0 = (1/h)K 0 ((x − xi )/h). Further diﬀerentiation of (3.10) will produce an estimator of the
second derivative m000,0 (x).
   We can also consider the estimators m̂0,1 for the regression function, m̂1,1 for its Þrst derivative
and m̂01,1 for the second derivative. In this case,
                                                            Pn−1 Pn
                                Sn,0 Tn,1 − Sn,1 Tn,0         i=1   j=i+1 (xi − xj )(mi − mj )ki kj
         m̂1,1 (x) = β̂ 1,1   =                 2
                                                      =          Pn−1 Pn                              (3.15)
                                  Sn,2 Sn,0 − Sn,1                i=1
                                                                                         2
                                                                         j=i+1 (xi − xj ) ki kj

from which m̂01,1 follows. Our shape-constrained estimator is based on applying the latter estimators
to the transformed data mi rather than the original data yi . We show below that this insures that the
desired shape restrictions are satisÞed in sample, not just asymptotically. For comparison purposes,
we also consider the unconstrained estimators m̂k,2 for k = 0, 1, 2, corresponding to a locally quadratic
regression, and m̂k,3 for k = 0, 1, 2, corresponding to a locally cubic regression.


3.4    A Word on Asymptotics

                                           b
Under standard regularity conditions, both m(x) and its derivatives converge pointwise to their true
values, as the sample size n goes to inÞnity. Assume that the conditional expectation m(x) admits q
continuous derivatives. The best achievable asymptotic rate of convergence of the estimator m̂(k) (x)
of the k-th derivative of m(x) —in the integrated mean-squared error sense— is given by:

                                                       n(q−k)/(1+2q)                                  (3.16)

This is actually the best rate of convergence that can be achieved by any nonparametric estimator
[see Stone (1982)]. The fact that the rate of convergence in equation (3.16) slows down as the order
k of the derivative to be estimated increases is often referred to as the curse of diﬀerentiation. This
rate is achieved for instance by the Nadaraya-Watson kernel regression when the bandwidth satisÞes
h = O(n1/(1+2q) ). In the case of locally polynomial estimators, the optimal choice of polynomial
order p on the basis of asymptotics is given by

                                                        p=k+1                                         (3.17)

(see Fan and Gijbels (1996, Section 3.3)).
   In theory, all the estimators we discussed so far have desirable asymptotic properties. In empirical



                                                            13
work, however, the slow rate of convergence of the derivative estimators can be a major hindrance.
In our empirical application, the object of interest is the second derivative of the call option pricing
function, Ct,τ (·), with respect to the options strike price, X, when the sample size is of the order
of 20 to 50 observations. The asymptotic guidance given by (3.17) would lead to locally quadratic
                        0                               00
estimators to estimate Ct,τ and locally cubic ones for Ct,τ . We compare below these unconstrained
(but asymptotically optimal) estimators to our constrained locally linear procedure. Monte Carlo
simulations immediately reveal that the asymptotics are a poor guide in terms of predicting the
behavior of the estimators for such small sample sizes and hence as a guide to selecting them. More-
over, as we illustrate in Figures 1 to 3, the constraints are quite often violated by the unconstrained
nonparametric estimators with these sample sizes. In addition, we would ideally like an increase in
the sample size n to correspond to an increase in the number of strike prices for which prices are
observed rather than additional prices obtained at a diﬀerent point of time for the same strikes.
The latter could potentially introduce nonstationarity, with prices at a diﬀerent instant drawn from
a diﬀerent state-price density. But then collecting data for additional strikes requires going to the
over-the-counter market where quotes can be obtained beyond and between the Exchange’s limited
traded strikes. Liquidity issues can be substantial. For all these reasons, we are interested in con-
structing estimators that will be nonparametric in nature, yet will not require large sample sizes to
satisfy the constraints — we want them to satisfy the desired constraints in sample, rather than just
asymptotically.


3.5   Bandwidth Selection

A bandwidth of h = 0 results in interpolating each data point (the most complex model), whereas
a bandwidth of inÞnity results in a single global polynomial Þt of degree p throughout the sample
(the simplest model). How to choose the bandwidth is therefore equivalent to choosing the model’s
complexity. Hence it is highly desirable to rely on automatic procedures that remove any potential
arbitrariness in the bandwidth’s choice. By minimizing the conditional mean-squared error at x
                           n h          i        o2     h          i
                            E m̂(k)(x)|x −m(k)(x) + V ar m̂(k)(x)|x                              (3.18)

the optimal local (i.e., variable with x) bandwidth is (see e.g., Fan and Gijbels (1996)):
                                              "                        #1/(2p+3)
                                                        v(x)       1
                          hlocal (x) = Ck,p       © (p+1) ª2     ×                               (3.19)
                                                   m     (x) π(x) n

where π(x) is the marginal density of the regressors and v(x) their variance.
   If we are interested in a global bandwidth (i.e., one that is independent of x), minimizing the

                                                      14
weighted mean integrated squared error with weight function ω(x)
                  Z ½n h             i         o2        h       i¾
                             (k)          (k)               (k)
                        E m̂ (x)|x −m (x) + V ar m̂ (x)|x ω(x)dx                                   (3.20)

produces the optimal bandwidth
                                             " R                             #1/(2p+3)
                                                  v(x)ω(x)/π(x) dx   1
                         hglobal = Ck,p        R©          ª2      ×                               (3.21)
                                                 m(p+1) (x) ω(x) dx n

  The constants Ck,p depends upon the choice of the kernel. For example, for the Gaussian kernel
                   √
K(u) = exp(−u2 /2)/ 2π, the relevant constants are C0,1 = 0.776, C0,3 = 1.161, C1,2 = 0.884 and
C2,3 = 1.006. The bandwidth expressions involve unknown quantities: π(x), v(x) and m(p+1) (x),
which all need to be estimated prior to the calculation of the optimal bandwidth. A simple way to
                                                                              P
do so is by Þtting a polynomial of order p + 3 globally to m(x), i.e., m (x) = p+3     k
                                                                               k=0 αk x estimate the
parameters αk by ordinary least squares, v by the sum of squares of residuals (so that the estimator
is independent of x), and m(p+1) (x) as the second order polynomial obtained by diﬀerentiation of
the polynomial Þt of order p + 3 of m(x), i.e.,
                                         Xp+3
                        m(p+1) (x) =                   αk k(k − 1)...(k − p + 1)xk−(p+1)           (3.22)
                                               k=p+1

For the global optimal bandwidth, a typical choice of weighting function would be ω(x) = ω0 (x)f(x)
where ω 0 (x) is a Þxed function (for instance ω 0 (x) is 1 for all x between the mean of the xi ’s minus
1.5 times the standard deviation of the xi ’s and the mean plus 1.5 times the standard deviation, and
                                             R             p
0 for x outside this interval). In this case, ω 0 (x)dx = 3 V ar(X), estimated by replacing V ar(X)
by the sample moment. The estimated optimal global bandwidth is then
                                         "             R                        #1/(2p+3)
                                                  ssr × ω 0 (x)dx             1
                       ĥglobal = Ck,p       Pn © (p+1)        ª2           ×                      (3.23)
                                                   m     (X i )   ω 0 (Xi )   n
                                              i=1

where ssr is the sum of squares of residuals from the regression (3.22).


3.6   The Result: Estimation Under Inequality Constraints

We now show that the two-step procedure we proposed, namely constrained least square regression
of the data followed by a locally linear estimation using the transformed data, results in an estimator
satisfying the constraints. The following proposition states our result. The shape-constrained esti-
mator we described will always satisfy the constraints for every sample size, not just asymptotically:

Proposition 1 Consider a set of n observations on the dependent variables, y1 , y2 , ..., yn and the
corresponding independent variable values x1 , x2 , ...., xn . Without loss of generality, let xi ≥ xj for

                                                          15
i > j, i, j ∈ {1, 2, ..., n}. Assume that the transformed data mi result from applying the constrained
least squares algorithm to the original data yi . Then the locally linear estimator obtained from
the transformed data and a log-concave kernel function satisÞes the required constraints in sample:
−e−rt,τ τ ≤ m̂(1) (x) ≤ 0, and m̂(2) (x) ≥ 0.

       Proof: See Appendix B.
    The last two constraints (2.8)-(2.9) on the function m̂(2) (x) are easily satisÞed. Restriction (2.8)
                                                                      R +∞
is a scaling constraint: replacing m̂(2)(x) by exp(−rt,τ τ )m̂(2) (x)/ 0 m̂(2)(z)dz produces the desired
result. Restriction (2.9) amounts to a Þxed translation of the estimated density to achieve the desired
expected value Ft,τ : replace m̂(2) (x) by the shifted function m̂(2) (x − z) with the Þxed shift amount
z determined by setting the expected value of the resulting function to the desired level Ft,τ . As we
show in Section 4 below, these two adjustments have very little eﬀect on the estimator in practice.
       We then deÞne the estimator m̂(0)(x) of the call pricing function from the SPD estimator by
                                           Z +∞
                                 (0)
                               m̂ (x) ≡          max(z − x, 0)m̂(2) (z)dz                       (3.24)
                                                    0

(with an obvious generalization if we wish to price another European-style payoﬀ: just replace
max(z − x, 0) by that contingent claim’s payoﬀ function). The estimator m̂(0) (x) will automatically
satisfy the no-arbitrage bounds (2.6) satisÞed by the call pricing function. In eﬀect, having a proper
SPD estimator in the form of exp(−rt,τ τ )m̂(2) (x) will automatically result in the price function
satisfying the arbitrage bounds appropriate for its payoﬀ structure (in particular, (2.6) for a call
option). In the case of American-style payoﬀs, this would include adding to the right hand side of
(3.24) a supremum over the dates over which exercise may occur.
       Finally, while we are motivated by the problem of constraining our locally polynomial estimator
to have bounded Þrst derivatives and to be convex, it should be noted from the proof that the
proposition in fact applies to more general inequalities on the Þrst two derivatives of the function,2
not just the speciÞc ones of interest in the context of estimating SPDs. The assumption that the
kernel density function is log-concave is not much of a restriction since that class of kernel functions
contains among others the Gaussian, uniform, Epanechnikov and Laplacian kernels, i.e., most of the
kernels used in practice.
   2
       If the inequalities are modiÞed, then the constraints in the constrained least squares need of course to be modiÞed
accordingly.




                                                             16
4     Monte Carlo Analysis

4.1   Comparison with Unconstrained Nonparametric Estimators

We perform a Monte-Carlo analysis to determine the performance of the shape-constrained non-
parametric SPD estimator and compare it to the standard unconstrained Nadaraya-Watson and
locally linear nonparametric estimators. The natural terrain to apply these tools involve S&P 500
index options, so we calibrate our Monte Carlo simulation experiments to match the basic features
of this market. We assume that the true price function is the Black-Scholes/Merton model with a
implied volatility smile curve. Naturally, the advantage of our nonparametric approach lies in its
robustness. If the options were priced by another formula, the nonparametric approach should be
able to approximate it as well since, by deÞnition, it does not rely on any parametric speciÞcation
for the underlying asset’s price process. Therefore, similar Monte Carlo simulation experiments can
be performed for alternative option-pricing models. However, we choose to perform the simulation
experiments under an implied volatility smile model designed to be realistic for a typical trading
day in 1999. The smile curve used as the data generating process for the simulations was calibrated
based on the smile observed on May 13, 1999 on options on the S&P 500 traded at the Chicago
Board Options Exchange (CBOE) with expiration in July. The assumed smile is a linear function
of the strike with volatility equal to 40% at the strike price 1000 and 20% at the strike price 1700.
We set the spot price St at 1365. The short term interest rate and the dividend yield are set at
rt,τ = 4.5% and δ t,τ = 2.5%, respectively. We consider both the 30 and 60 maturities and plot the
results for the 30-day options. The 60-day results are qualitatively similar.
    We assume that we observe n = 25 option prices with strike prices equally spaced between
1000 and 1700, as would be the case with actual data. To create simulated option prices, we add
uniformly distributed noise to the theoretical option prices. There are two possible rationalizations
for the amount of noise to introduce around the assumed “true” option prices in order to carry out
simulations. First, the noise can model the bid-ask spread and the diﬀerent liquidity of diﬀerent
options. Second, we can assume that there is a true set of option prices at one point in time and
introduce noise to capture the time series variations of the option prices in a short window of time
around that date, after accounting for the variation of the underlying asset price in the same window.
    In the Þrst approach, the assumed bid-ask spread, calibrated to the market data, is set to 5% of
the option’s ask price, with a ßoor at 50 cents and a cap at 2 dollars. The noise distribution around
the theoretical price is then uniform between 0 and half of the bid-ask spread value. We also account
for the diﬀerent liquidity of options with diﬀerent degrees of moneyness (most of the liquidity is near
the money). SpeciÞcally, recall that the option’s moneyness is Mt,τ ≡ X/Ft,τ (strike divided by the


                                                  17
forward value of the S&P 500). The noise distribution around the theoretical price is then uniform
between 0 and half of the bid-ask spread value times a liquidity factor given by 1 + (2/0.2)|Mt,τ − 1|.
This makes the liquidity factor 1 at the money (Mt,τ = 1) and 2 at Mt,τ = 0.8 or 1.2, and proxies
for the observed diﬀerences in liquidity of these options.
   In the second approach, we calibrate the noise to the typical intraday variation of S&P500 option
prices, using their range to calibrate the uniform distribution of the noise term. In percentage terms,
the range of values reached stretches from 3% of the option value for deep in the money options
to 18% for deep out of the money options. In terms of the performance of the estimators, both
models for the noise term produce qualitatively similar results with the provision that the lower the
amount of noise, the lower the RMSE performance advantage of the constrained estimator over the
unconstrained locally linear estimator (since fewer simulated data samples violate the constraints).
This being said, one may argue that any violation of arbitrage constraints (such as those produced
by the unconstrained estimator) is potentially much more damaging than its mere RMSE eﬀect (it
could for instance induce trading on a false perceived arbitrage) and should be penalized accordingly
when assessing an estimator’s performance. Also, other things equal, more noise tends to increase
the advantage of the constrained estimator. Nevertheless, the amount of noise we speciÞed above
is not unrealistically high. It is in fact, if anything, too conservative: see the empirical evidence in
Hentschel (2001).
   For estimation, we use a Gaussian kernel. We select a range of bandwidths including those given
in Section 3.5 and repeat the estimation steps for each bandwidth value. Then for each function
to be estimated, we selected the optimal bandwidth on the basis of minimizing the small sample
weighted mean integrated squared error given in equation (3.20). We discuss this further below.
The Monte-Carlo averages and conÞdence intervals for each bandwidth, estimator and function to be
estimated are based on 5, 000 simulations and we focus on simulations using the second speciÞcation
of the noise term, the results being qualitatively similar to the Þrst one.
   Figure 1 shows the average estimate, a 95% conÞdence interval, and the true functions for the
unconstrained Nadaraya-Watson estimator. Panel A of Figure 1 shows the call pricing function
estimator m̂0,0 , Panel B the Þrst derivative m̂00,0 of the pricing function with respect to the strike
price, and Panel C shows the state price density m̂000,0 . As observed in Panel C of Figure 1, standard
unconstrained Nadaraya-Watson estimates are, on average, negative near the left boundary, where
the true probabilities are low. Of course, kernel estimation near the boundaries is known to be
problematic, see e.g., Wand and Jones (1995).
   Figure 2 shows the same results for the (unconstrained) locally linear estimator, m̂0,0 in Panel
A, m̂0,1 in Panel B and m̂00,1 in Panel C. As observed in Panel C of Figure 2, the locally linear


                                                  18
estimator has much lower boundary bias than the Nadaraya-Watson estimator, but the SPD still can
be negative in the left boundary where the true probabilities are low.
   Figures 3 and 4 report the results for the locally quadratic (m̂k,2 for k = 0, 1, 2) and locally
cubic (m̂k,3 for k = 0, 1, 2) estimators, respectively. As expected, higher order locally polynomial
estimators perform poorly in this context because they eﬀectively correspond to more complex local
models in the absence of large enough samples. The net result is that the estimator’s biases can
be entirely eliminated but at the cost of a large variance penalty. At the optimal bandwidth choice
(which is what is plotted in the Þgures), the trade-oﬀ between squared bias and variance results in
relatively large biases and variances (see Panels C in Figures 3 and 4). In addition, these estimators
often violate the constraints near the boundaries (see Panels B and C). Comparing the results for the
unconstrained locally polynomial estimators corresponding to p = 0, 1, 2, 3, it appears that locally
linear estimators perform best in our context.
   Figure 5 reports the results for our estimator, m̂(0) , m̂(1) and m̂(2). As observed in Panel C of
Figure 5, the constrained estimator does not share the drawbacks of the unconstrained estimators.
First, the constrained SPD estimator does not have the same boundary bias as the locally constant
Nadaraya-Watson —it behaves rather like the locally linear estimator that it is. Second, unlike the
unconstrained locally linear estimator, the constrained estimator remains nonnegative even when
the true probabilities are low. Intuitively, imposing the constraints has the eﬀect of allowing lower
bandwidths than would be optimal for a locally linear estimator in their absence. This lowers the
bias of the estimator without increasing the variance correspondingly because the constraints prevent
the large deviations (which would violate the constraints) from occurring. The net eﬀect is a more
accurate estimator on the basis of its mean squared error properties. Recall that we scale our
density estimator and shift it, as discussed in Section 3.6. Even though these last two constraints are
necessary to rule out arbitrage opportunities, our Monte-Carlo analysis reveals that, in practice, they
make small diﬀerence on the estimated function m̂(2) (x).Indeed, for the optimal bandwidth case the
                R +∞
average value of 0 m̂(2) (z)dz is close to one (0.94) and the average shift z is 0.7% of the futures
price.
   We conÞrm that intuition by studying the mean squared error behavior of the various estimators,
both pointwise and global, for the sample size under consideration. Figure 6 reports the global
root integrated mean squared error (RIMSE) of the Þve estimators of the pricing function, the Þrst
strike-derivative and the SPD. The RIMSE is the square root of the integral given in equation (3.20).
For each function to be estimated (k = 0, 1, 2) and estimator (p = 0, 1, 2, 3, and shape-constrained
estimator) we used the bandwidth resulting in the lowest RIMSE. The fact that smaller (resp. larger)
bandwidths result in smaller (resp. larger) bias and larger (resp. smaller) variance produce these


                                                  19
U-shaped RIMSE curves with the bottom of the U identifying for each function and estimator the
globally optimal bandwidth used in Figures 1 through 5 respectively. Comparing speciÞcally our
constrained estimator to the unconstrained locally linear estimator conÞrms the initial intuition: the
shape-constrained estimator results in a lower RIMSE for lower bandwidths. For larger bandwidths,
the two estimators converge because larger bandwidths result in ßatter estimates, which consequently
tend to satisfy the constraints. This explains why the RIMSE curves for these two estimators converge
to one another to the right of their respective minima. However, the lowest RIMSE for the constrained
estimator of the SPD is about 25% lower than that of the unconstrained estimator because lowering
the bandwidth from the unconstrained optimum results in further decreases of the shape-constrained
RIMSE. Figure 7 shows the local, or pointwise, eﬀect of oversmoothing (higher bias, lower variance)
and undersmoothing (lower bias, higher variance) the constrained estimator relative to the optimal
bandwidth.
       Furthermore, we should note that, in all likelihood, MSE-based error measures alone underesti-
mate the true cost of using an estimator that can violate the constraints. The mean-squared error
does not attach any penalty to violations of the constraints by the unconstrained estimators. Eco-
nomic measures of the cost of violating the constraints could be quite large. For example, hedges
based on option deltas that violate the constraints could quickly become ineﬀective; pricing with
an estimated SPD that is negative in the left tail leads to underestimation of out of the money put
prices, trades could be put in place based on the false perception of arbitrage (locally negative SPD),
etc.
       Simulation results for n = 50 observations, and the Þrst simulation design, are qualitatively
similar. Overall, the results of the simulations suggest that for these types of sample sizes, imposing
the shape constraints (2.5) results in a substantial improvement of the estimators.


4.2      Comparison with Parametric Alternatives

Finally, we also compare our estimator to two parametric alternatives. We consider the Jarrow
and Rudd (1982) parametric extension of the Black-Scholes model where the lognormal density is
replaced by a four-parameter expansion, namely
                                  ©       ª
                               exp −z 2 /2 ³   µ ¡        ¢ µ ¡              ¢´
                 p (ST |St ) =    √         1 + 3 z 3 − 3z + 4 z 4 − 6z 2 + 3                     (4.1)
                                ST 2πτ σ        6           24

where
                                                             ¡          ¢
                                                 Ln(ST /St )− µ1 − σ2 /2 τ
                               z = z (ST |St ) =             √
                                                            σ τ



                                                   20
and the call price computed as
                                                   Z   +∞
                                             −rτ
                                     Pt = e                 (ST − K) p (ST |St) dST .                               (4.2)
                                                   K

      The 4 parameters µ1 , σ, µ3 , µ4 are estimated by minimizing the squared deviations between
market prices and parametric prices.3 Since there is no bandwidth choice involved in this parametric
formula, there is only one density per simulation. Given the sample sizes we consider, more ßexible
functional forms become essentially nonparametric in nature — if we have 25 observations and we
are Þtting a parametric model with, say, up to 10 parameters, then the choice of the number of
parameters becomes akin to the choice of the bandwidth in nonparametrics.
      The second parametric family we use in our comparisons is a Þve-parameter mixture of lognormal
densities which has been used in this context by Bahra (1996). The assumed model is

                         p (ST |St ) = αpLN (ST |St ; µ1 , σ 1 ) + (1 − α) pLN (ST |St ; µ2 , σ 2 )                 (4.3)

where
                                                  ½                               ¾
                                         1            1 ¡          ¡      2
                                                                             ¢ ¢2
                pLN (ST |St ; µ, σ) =   √      exp − 2 Ln(ST /St )− µ1 − σ /2 τ
                                      ST 2πτ σ      2σ τ

and the call price computed as in (4.2). The pricing formula corresponding to (4.3) is a linear com-
bination of Black-Scholes formulae (α times the Black-Scholes formula corresponding to parameters
(µ1 , σ 1 ) plus 1 − α times the Black-Scholes formula corresponding to parameters (µ2 , σ 2 )).
      The 5 parameters α, µ1 , σ1 , µ2 , σ2 are estimated by minimizing the squared percentage deviations
between market prices and parametric prices. The reason for using squared price errors in one case
and squared percentage errors in the other is that they produced the best results for the two methods
respectively. Attempting to minimize squared price errors with the mixture of lognormals often
produces nonsensical results, where one of the two densities is tailor-made to Þt in the money calls
where pricing errors in dollars are costly, resulting in that density having a very low value of its σ
parameter (in addition to a very negative value of its µ parameter).
      Both parametric models provide a better contrast between the results of a true parametric pro-
cedure and those of nonparametric ones. Panels A and B of Figure 8 report the results for the
estimated SPD resulting from these two methods, in the same format as Panel C of Figures 1-5.
Because they are global in nature as opposed to local, the two types of parametric estimators are
unable to cope well with arbitrage violations in the data. This is not due to the inadequacy of the
parametrizations: as we show in Panels C and D of Figure 8, the two models can Þt the true SPD
  3
      See Christoﬀersen and Jacobs (2001) for a discussion of the inßuence of the choice of loss function in this context.



                                                              21
assumed in the data generating process (with no noise) almost perfectly. The issues arise when we
attempt to Þt a set of price data that includes noise, i.e., sometimes local violations of convexity, as
this produces a global distortion of the estimator — in other words, the error propagate from the local
violation (which often occurs in one tail) throughout the estimated distribution (including near the
peak and in the other tail).
    This results in RMSE measures that, for the same simulation designs as the other estimators we
considered, are worse than what can be achieved by our proposed locally linear constrained estimator.
After all, avoiding this local-to-global contamination due to outliers, bad data, etc., is often why one
uses nonparametric estimators in the Þrst place. Locally polynomial estimators are particularly apt
at dealing with this issue.


5    Example: S&P 500 Implied SPD Under Shape Restrictions

Aït-Sahalia and Lo (1998, 2000) estimated the market call pricing function from a sample of 14,441
option prices on the S&P 500 index. They used the semiparametric approach described in (2.12).
They found empirically, without imposing shape constraints, that their SPD estimator is convex but
only because of the dimension reduction involved in the semiparametric speciÞcation, and because
of the very large size of their sample. In practice, it would be desirable to have similar guarantees
with substantially smaller samples. Indeed, as opposed to Aït-Sahalia and Lo (1998, 2000), we work
with samples of tiny sizes (a typical cross-section at one point in time of 20 to 30 options versus a
time-aggregated cross-section of 14, 431 options).
    The data consist of the closing prices on May 13, 1999 for call options on the S&P 500 traded
at the CBOE for a maturity of 65 days corresponding to the July 1999 expiration (July 17). The
closing spot price of the S&P 500 on that day was 1367.56, and the risk free interest rate for that
maturity was 4.83%. The dividend yield is implied through put-call parity for the put-call pair at
the money. The results from applying the Þve diﬀerent estimators (unconstrained Nadaraya-Watson,
unconstrained locally linear, quadratic and cubic, shape-constrained locally linear) are reported in
Figure 9. The bandwidths correspond to the optimum identiÞed in the previous section. The three
panels correspond to the three functions to be estimated. As is apparent from Panel A, all estimators
produce sensible looking (and visually indistinguishable) estimates for the pricing function as long
as strikes remain relatively near the money (strikes between 1200 and 1500). However, for values
of the strike price above 1600 the locally quadratic and locally cubic estimators display their high
variability tendency which was clearly apparent in the simulations. And the Nadaraya-Watson
estimator exhibits poor boundary behavior below 1100, clearly violating the convexity constraint on


                                                  22
prices.
    Naturally, diﬀerentiation tends to emphasize the diﬀerences between estimators. In Panel B,
all remaining estimators except the two locally linear ones (constrained and unconstrained) violate
the Þrst derivative constraints somewhere. Regarding SPD estimates in Panel C, all the uncon-
strained estimators either violate the positivity constraint in the left tail of the density, or are too
ßat when evaluated at the globally optimal bandwidth. The unconstrained locally linear estima-
tor tracks the constrained estimator relatively closely, except that the optimal bandwidth tends to
produce an estimator that is slightly too ßat, as was evidenced in the discussion of our simulation
results. By contrast, the optimal amount of smoothing for the shape-constrained estimator is slightly
lower which produces an estimator that is more sensitive to Þner features of the data. Indeed, our
shape-constrained estimator produces an estimate of the SPD which looks quite plausible, displaying
the expected level of negative skewness and excess kurtosis, while satisfying (by construction) the
positivity constraint.
    Finally, we report in Table 1 the results of repeating this analysis for every trading day during the
year 1999. We repeated the analysis for diﬀerent days (one set of quotes per day, each day treated
separately) and report the frequency of arbitrage violations during that year. The unconstrained
locally linear estimator violates the restrictions over 50% of the time, a percentage which rises to
close to 100% as we move to the (unconstrained) locally quadratic and cubic estimators. By contrast,
our estimator never violates the constraints (and still results in lower RMSE). The violation of the
arbitrage restrictions by the unconstrained estimators hold across a large spectrum of bandwidth
values. Substantial oversmoothing is required to make the unconstrained estimator no longer violates
he constraints. But this then results in a large bias.


6    Conclusions

This paper proposed a method to incorporate shape restrictions, such as monotonicity and convexity,
into nonparametric locally linear estimators. The estimator is motivated by the practical problem
of estimating state-price densities with option data, in a setting where no information other than
monotonicity and convexity is available, yet the sample size is typically small. The simulations
results indicate that nonparametric estimates can be quite feasible in sample sizes as small as twenty
observations, provided that appropriate theory-motivated shape restrictions, such as monotonicity,
and/or convexity, are imposed. As discussed in the Introduction, this is a frequent occurrence in
other areas of economics as well.
    In our speciÞc context of SPD estimation, the shape-constrained SPD we estimated can have


                                                   23
many uses. First, it provides us with an arbitrage-free method of pricing new, more complex, or
less liquid securities, e.g., OTC derivatives or non-traded ßexible options, given a subset of observed
and liquid “fundamental” prices, in this case basic call-option prices, that are used to estimate the
SPD. We are able to achieve this in the context where very few fundamental securities are available,
i.e., the observed cross-section is very sparse. Second, from a risk management perspective, our SPD
estimates provide information that is crucial to understanding the nature of the fat tails of asset-
return distributions implied by options data. Volatility cannot be used as a summary statistic for
the entire distribution when typical return series display events that are three standard deviations
from the mean approximately once a year. Our approach yields an estimate of the entire return
distribution, from which single points, such as value-at-risk, can easily be derived. Third, our
nonparametric estimator captures those features of the data that are most salient from an asset-
pricing perspective and which ought to be incorporated into any successful parametric model. It
also helps us understand what features are missed by tightly parametrized models, such as day-
to-day or even intraday changes in the shape of the SPD, since we can now estimate such SPDs
nonparametrically on the basis of very few observations. In fact, a nonparametric analysis can often
be advocated as a prerequisite to the construction of any parsimonious parametric model, precisely
because important features of the data are unlikely to be missed by nonparametric estimators.




                                                  24
                                           Appendix

A     The Constrained Least Square Regression Algorithm

A.1    Transforming the Constrained Least Squares Problem to One with Conic
       Constraints

We start by rewriting the constrained least squares problem in such a way as to reduce it to a convex
cone problem which is then amenable to Dykstra’s algorithm for constrained least squares under
conic constraints. Goldman and Ruud (1995) contain ideas along those lines, although not a formal
development. Write our constraints (3.3) in matrix form as A.m − b ≤ 0, where A is n + 1 (the
number of constraints) by n (the number of mi ’s) and b is (n + 1) × 1. In its original form, our
problem is therefore

                                               min km − yk2                                    (A.1)
                                               m∈Rn

                                       subject to A.m − b ≤ 0

    DeÞne
                                                              
                           m−y             z                   0
                  u=            =           ,     v=          ,    C = (A | A.y − b)
                            t              t                   1

where t is 1 × 1, and the 0 block in the vector v is n × 1. Then consider the problem

                                     min ku − vk2 = kzk2 + |t − 1|2                            (A.2)
                                 u∈Rn+1

                                 subject to C.u ≤ 0 and                 t=1

where minimizing over u means minimizing over (z, t). The solution u∗∗ = (z ∗∗ , 1) to problem (A.2)
gives the solution m∗∗ of our original problem (A.1) as m∗∗ ≡ z ∗∗ + y. Indeed, the solution u∗∗ of
(A.2) has set t = 1 and then minimized kzk2 over z under the constraint that C.u ≤ 0 and we have
                       
                        z
                   C.   ≤ 0 ⇔ A.z + (A.y − b) ≤ 0 ⇔ A.m − b ≤ 0.
                        1

    But problem (A.2) still does not have conic constraints (because of the constraint t = 1, which
is again aﬃne). So consider next the problem where we have relaxed the aﬃne constraint t = 1 to




                                                      25
the linear (or conic) constraint t ≥ 0 :

                                       min ku − vk2 = kzk2 + |t − 1|2                                (A.3)
                                     u∈Rn+1


                                     subject to C.u ≤ 0 and t ≥ 0

Now this problem is in Dykstra’s conic constraints form, and let its solution be denoted by u∗ =
(z ∗ , t∗ ).
     Let us see how the solutions to the two problems (A.2) and (A.3) are related. Note that because
u∗∗ satisÞes the constraint C.u∗∗ ≤ 0, we have

                                            A.z ∗∗ + (A.y − b) ≤ 0.

Since t∗ ≥ 0, it follows that

                                          A.z ∗∗ t∗ + (A.y − b)t∗ ≤ 0.

Therefore (z ∗∗ t∗ , t∗ ) satisÞes the constraints of problem (A.3). Since by deÞnition the optimum of
problem (A.3) is reached at u∗ = (z ∗ , t∗ ), it follows that

                                kz ∗ k2 + |t∗ − 1|2   ≤         kz ∗∗ t∗ k2 + |t∗ − 1|2

or

                                            kz ∗ k2   ≤        kz ∗∗ t∗ k2 .                         (A.4)

     Now, it is also the case that, since u∗ satisÞes the constraint C.u∗ ≤ 0, we have

                                            A.z ∗ + (A.y − b)t∗ ≤ 0.

Since t∗ ≥ 0, it follows that

                                         A.(z ∗ /t∗ ) + (A.y − b) ≤ 0,

so that ((z ∗ /t∗ ), 1) satisÞes the constraints of problem (A.2). But by deÞnition the optimum of
problem (A.2) is reached at u∗∗ = (z ∗∗ , 1), thus

                                           kz ∗∗ k2   ≤        k(z ∗ /t∗ )k2 .                       (A.5)

     Multiplying equation (A.5) by (t∗ )2 and combining with (A.4), it follows that kz ∗ k2 = kz ∗∗ t∗ k2 ,




                                                          26
so that the minimum of problem (A.2) is achieved at

                                                 z ∗∗ = z ∗ /t∗ .                                    (A.6)

Therefore the solution (z ∗∗ , 1) of problem (A.2).can be obtained from the solution (z ∗ , t∗ ) of problem
(A.3). Recall that the solution m∗∗ to our original problem (A.1) is obtained from the solution of
problem (A.2) by m∗∗ ≡ z ∗∗ + y. Hence solving problem (A.3) using Dykstra’s algorithm to Þnd
(z ∗ , t∗ ) ultimately gives us the the solution m∗∗ to our original problem (A.1).


A.2      Algorithm for Constrained Least Squares Under Conic Constraints

We now brießy describe Dykstra (1983)’s algorithm to solve the constrained least square regression
problem (A.3), which has conic constraints. DeÞne the following cones in Rn+1 . For j = 1, ..., n − 2,
let

                        zj+2 − zj+1 zj+1 − zj        yj+2 − yj+1 yj+1 − yj
Cj = {u ∈ Rn+1 s.t.                −          + t× (            −          ) ≤ 0} j = {1, ..., n − 2}
                        xj+2 − xj+1 xj+1 − xj        xj+2 − xj+1 xj+1 − xj

and

            Cn−1 = {u ∈ Rn+1 s.t. − zn + t × (−yn ) ≤ 0}
               Cn = {u ∈ Rn+1 s.t. zn − zn−1 + t × (yn − yn−1 ) ≤ 0}
            Cn+1 = {u ∈ Rn+1 s.t. − z2 + z1 + t × (−y2 + y1 − (x2 − x1 ) × e−rt,τ τ ) ≤ 0}
            Cn+2 = {u ∈ Rn+1 s.t. − t ≤ 0}.

      The minimization problem (A.3) can be written as:


                                                        n
                                                        X
                                               min
                                               T          (ui − vi )2                                (A.7)
                                                n+2
                                          u∈    j=1   Cj i=1


      The algorithm consists in repeatedly projecting the vector u onto the cones Cj :

      • Let u1,1 denote the projection of u onto the cone C1 . Let I1,1 = u1,1 − u denote the incremental
        change incurred by the projection, so that u1,1 = u + I1,1

      • Let u1,2 denote the projection of u1,1 onto the cone C2 . Let I1,2 = u1,2 − u1,1 denote the
        incremental change incurred by the projection, so that u1,2 = u + I1,1 + I1,2 .

      • Let u1,n+2 denote the projection of u1,n+1 onto the cone Cn+2 . Let I1,n+2 = u1,n+2 − u1,n+1
        denote the incremental change incurred by the projection, so that u1,n+2 = u + I1,1 + I1,2 +


                                                         27
      I1,3 + .... + I1,n+1 + I1,n+2 .

    • After u1,n+2 and I1,n+2 are found. Let u2,1 denote the projection of u + I1,2 ... + I1,n+2 onto
      the cone C1 . Note that we have removed the increment I1,1 before this projection. The new
      increment is I2,1 = u2,1 − (u + I1,2 ... + I1,n+2 ).
                               Tn+2
    • Continue, until u•,• ∈      j=1   Cj .

   The projections of u•,• onto cones Cj are easily obtained. If we represent the cone Cj by Cj =
              P
{u ∈ Rn+1 s.t. n+1
                i=1 aj,i ui ≤ 0}, then the projection of u onto Cj is given by:


                                                
                                                 u if Pn+1 a u ≤ 0
                                                         i=1 j,i i
                                    P (u|Cj ) =
                                                 u0 if Pn+1 aj,i ui > 0
                                                                     i=1

    where
                                                              Pn+1
                                                          (    l=1 aj,l ul )aj,i
                                           u0i   = ui −        Pn+1 2            .
                                                                  l=1 aj,l


B     Proof of Proposition 1

Part 1: Proof that exp(−rt,τ τ ) ≤ m̂1,1 (x) ≤ 0.
    The proof is based essentially on rearranging the terms in the numerators and the denominators
of the locally linear estimators in such a way that they can be signed. With ki = Kh (x − xi ) =
h−1 K(h−1 (x − xi )), the local linear estimator of the regression function is

                               Sn,2 Tn,0 − Sn,1 Tn,1
            m̂0,1 (x) = β̂ 0,1 =                2
                                  Sn,2 Sn,0 − Sn,1
                         Pn Pn                2
                                                          Pn Pn
                          i=1    j=1 (xj − x) mi ki kj −            j=1 (xj − x)(xi − x)mi ki kj
                       =   Pn Pn                  2
                                                          Pi=1
                                                            n Pn
                             i=1    j=1 (xj − x) ki kj −    i=1     j=1 (xi − x)(xj − x)ki kj
                         Pn−1 Pn
                          i=1     j=i+1 (xj − xi )((xj − x)mi − (xi − x)mj )ki kj
                       =              Pn−1 Pn                                                      (B.1)
                                                                2
                                         i=1    j=i+1 (xi − xj ) ki kj




                                                              28
while the locally linear estimator of the Þrst partial derivative of m(x) with respect to x is given by:

                                      Sn,0 Tn,1 − Sn,1 Tn,0
                  m̂1,1 (x) = β̂ 1,1 =                 2
                                         Sn,2 Sn,0 − Sn,1
                                 Pn Pn                             Pn Pn
                                   i=1    j=1 (xi − x)mi ki kj −               (xj − x)mi ki kj
                              = Pn Pn                2
                                                              Pn i=1  Pn j=1
                                 i=1    j=1 (xj − x) ki kj −      i=1  j=1 (xi − x)(xj − x)ki kj
                                  Pn Pn
                                    i=1    j=1 (xi − xj )mi ki kj
                              = Pn Pn
                                 i=1    j=1 (xj − x)(xi − xj )ki kj
                                Pn−1 Pn
                                 i=1     j=i+1 (xj − xi )(mj − mi )ki kj
                              =     Pn−1 Pn                                                                                            (B.2)
                                                              2
                                      i=1     j=i+1 (xj − xi ) ki kj

   Therefore if the bivariate sample (x1 , m1 ), ..., (xn , mn ) satisÞes the property that if xi < xj then
(mj − mi )/(xj − xi ) ≥ c, for all i and j > i, where c is a constant then
                        ¯                             ¯
                        n−1
                        X     n
                              X                                                       n−1
                                                                                      X     n
                                                                                            X
                                    (xj − xi )(mj − mi )ki kj              ≥ c                    (xj − xi )2 ki kj
                                                                             ¯
                        i=1 j=i+1                                                     i=1 j=i+1

and hence m̂1,1 (x) ≥ c. If in addition the bivariate sample (x1 , m1 ), ..., (xn , mn ) satisÞes the property
                      ¯
that if xi < xj then (mj − mi )/(xj − xi ) ≤ c̄, for all i and j > i, then
                        n−1
                        X     n
                              X                                                       n−1
                                                                                      X     n
                                                                                            X
                                    (xj − xi )(mj − mi )ki kj              ≤ c̄                   (xj − xi )2 ki kj
                        i=1 j=i+1                                                     i=1 j=i+1

and hence m̂1,1 (x) ≤ c̄. Applying this with c = exp(−rt,τ τ ) and c̄ = 0 gives the result.
                                             ¯

   Part 2: Proof that m̂01,1 (x) ≥ 0.
   Let Mi,j = (mi − mj )/(xi − xj ) = (mj − mi )/(xj − xi ) denote the local slope between xi and xj .
Also deÞne ki,j = (xi − xj )2 ki kj and let ki,j
                                             0 denote the partial derivative of k
                                                                                 i,j with respect to x.

Rewrite (B.2) as
                                                            Pn−1 Pn
                                                                i=1        j=i+1 Mi,j ki,j
                                              m̂1,1 (x) =       Pn−1 Pn
                                                                   k=1         l=k+1 kk,l

so that:

                  Ã                           !Ã                       !       Ã                           !Ã                      !
                    P
                    n−1    P
                           n
                                        0
                                                 P
                                                 n−1    P
                                                        n                      P
                                                                               n−1      P
                                                                                        n                    P
                                                                                                             n−1      P
                                                                                                                      n
                                                                                                                             0
                                  Mi,j ki,j                     kk,l       −                   Mi,j ki,j                    kk,l
                      i=1 j=i+1                    k=1 l=k+1                       i=1 j=i+1                    k=1 l=k+1
   m̂01,1 (x) =                                             Ã                         !2                                               (B.3)
                                                               P
                                                               n−1     P
                                                                       n
                                                                               kk,l
                                                                k=1 l=k+1

Rearranging the terms in (B.3) yields


                                                                   29
        Ãn−1 n               !2                                 n ³
         X X                                       n−1
                                                   X      n
                                                          X n−1
                                                            X X                      0
                                                                                        ´
                      kk,l        m̂01,1 (x)   =                    0
                                                                   ki,j kk,l − ki,j kk,l (Mi,j − Mk,l )       (B.4)
          k=1 l=k+1                                 i=1 j=i+1 k=i+1 l=k+1
                                                       n−1
                                                       X     n
                                                             X n ³
                                                               X                       ´
                                                                   0                0
                                                   +              ki,j ki,l − ki,j ki,l (Mi,j − Mi,l )
                                                       i=1 j=i+1 l=j+1

   We want to prove that m̂01,1 (x) ≥ 0, i.e., that the right hand side of (B.4) is nonnegative. Recall
that we assumed that the kernel function K(·) was a log-concave probability density. That is, log(K)
is concave, i.e., its Þrst derivative is decreasing:

                                                         K 0 (a)   K 0 (b)
                                                                 ≥
                                                         K(a)      K(b)

if b ≥ a. Therefore if k ≥ i and l ≥ j we have

                                       x − xi   x − xk                x − xj   x − xl
                                              ≥                and           ≥
                                         h        h                     h        h

and hence

                                                   ki0  k0            kj0  k0
                                                       ≤ k     and        ≤ l
                                                   ki   kk            kj   kl

where ki = Kh (x − xi ) and ki0 = h−1 Kh0 (x − xi ). Therefore

                                                   ki0  k0   kj0  k0
                                                       − k +     − l ≤0
                                                   ki kk kj       kl

and
                                                                             µ                       ¶
               0                   0                 2            2              ki0 kk0   kj0  k0
              ki,j kk,l   − ki,j kk,l = (xi − xj ) (xk − xl ) ki kk kj kl           −    +     − l       ≤0   (B.5)
                                                                                 ki kk kj       kl

if k ≥ i and l ≥ j.
   >From now on, let
                                                 ³                  0
                                                                       ´
                                                   0
                                       ci,j,k,l ≡ ki,j kk,l − ki,j kk,l (Mi,j − Mk,l )                        (B.6)

denote the generic term in the sums (B.4). In addition to (B.5), it is also the case that Mi,j ≤ Mk,l ≤
0, hence Mi,j − Mk,l ≤ 0 for all (i, j, k, l) such that k ≥ i and l ≥ j. Therefore for such (i, j, k, l) we
have ci,j,k,l ≥ 0. Throughout the Þrst sum in (B.4), the indices satisfy k > i, and in the second sum
k = i. Thus as long as l ≥ j, the terms ci,j,k,l are nonnegative throughout the two sums in (B.4).
That l ≥ j will be the case for all the terms in the second sum in (B.4), where l ≥ j + 1, but not
necessarily in the Þrst sum where there are quadruplets (i, j, k, l) such that k ≥ i but l < j. For


                                                                30
these, we cannot be sure that ci,j,k,l ≥ 0.
                                                                Pn−1 Pn            Pn−1     Pn
   Consider such a quadruplet (i, j, k, l) in the sum              i=1     j=i+1    k=i+1    l=k+1 ci,j,k,l   for which
nonnegativity of ci,j,k,l is not guaranteed. Such a quadruplet satisÞes i < k < l < j. The key
to the proof that these terms are not big enough to make the overall sum negative is to consider
this problematic quadruplet (i, j, k, l) together with the two permutations (i, k, l, j) and (i, l, k, j).
These two permutations are used up only with that particular quadruplet: any other problematic
quadruplet would not need to re-use the same permutations. For these two permutations, we have
ci,k,l,j ≥ 0 (since l > i and j > k) and ci,l,k,j ≥ 0 (since k > i and j > l) and it turns out that adding
these two terms to the problematic term produces a nonnegative result, that is

                                          ci,j,k,l + ci,k,l,j + ci,l,k,j ≥ 0.                                     (B.7)

   To prove this, we now show that
                                 ³                   0
                                                       ´                  ³                0
                                                                                              ´
                                     0                                      0
  ci,j,k,l + ci,k,l,j + ci,l,k,j =ki,j kk,l − ki,j kk,l (Mi,j − Mk,l ) + ki,k kl,j − ki,k kl,j (Mi,k − Ml,j )
                                    ³                  0
                                                           ´
                                        0
                                 + ki,l   kk,j − ki,l kk,j (Mi,l − Mk,j )
                                             µ 0                          ¶
                                               ki      kj0    k0      k0
                               = ki kj kk kl      ti + tj + k tk + l tl                                 (B.8)
                                               ki      kj     kk      kl

where


  ti ≡ (xk − xi )(xl − xi )(xj − xi ) {(Mi,j − Mi,l ) (2xj − xk − xl ) + (Mi,k − Mi,l ) (2xk − xj − xl )}



 t ≡ (x − x )(x − x )(x − x ) {(M − M ) (2x − x − x ) + (M − M ) (2x − x − x )}
    j     j     l   j     k    j     i     i,j    k,j       i    k    l       l,j   k,j       l    i    k

 tk ≡ (xj − xk )(xl − xk )(xk − xi ) {(Mi,k − Mk,l ) (2xi − xj − xl ) + (Mk,j − Mk,l ) (2xj − xi − xl )}




   tl ≡ (xj − xl )(xl − xk )(xl − xi ) {(Mi,l − Mk,l ) (2xi − xj − xk ) + (Ml,j − Mk,l ) (2xj − xi − xk )}
                                                                                                        (B.9)
   Note that
                               
                                t + t = 2(x − x )2 (x − x )2 (M − M )
                                  i    k      k    i      j    l      i,k     l,j
                                                                                                                (B.10)
                                tj + tl = 2(xk − xi )2 (xj − xl )2 (Ml,j − Mi,k )

therefore
                                            
                                            
                                                    ti + tk ≤ 0
                                            
                                                     tj + tl ≥ 0                                                (B.11)
                                            
                                            
                                            
                                                ti + tk + tj + tl = 0

   Recall now that we are dealing with a quadruplet (i, j, k, l) such that i < k < l < j : therefore we




                                                         31
have
                                     
                                     
                                      Mi,k ≤ Mi,l ≤ Mi,j ≤ Ml,j
                                     
                                       Mi,k ≤ Mk,l ≤ Mk,j ≤ Ml,j                                 (B.12)
                                     
                                     
                                     
                                              Mi,l ≤ Mk,l

These inequalities follow from repeated application of the fact that for any triplet (i, k, l) such that
i < k < l,

                                     mk − mi  m − mi    m − mk
                                             ≤ l       ≤ l                                       (B.13)
                                     xk − xi   xl − xi   xl − xk

which itself follows from
                                 µ             ¶            µ           ¶
                     ml − mi         xk − xi       mk − mi      xk − xi ml − mk
                             =                             + 1−
                     xl − xi         xl − xi       xk − xi      xl − xi   xl − xk

where 0 ≤ (xk − xi )/(xl − xi ) ≤ 1. Thus the middle slope Mi,l is a weighted average of the extreme
slopes Mk,l and Mi,l .
   As a consequence of (B.12), we have tk ≥ 0 and tj ≥ 0. Combined with (B.11), it follows that:
                                        
                                         t ≤ −t ≤ 0
                                           i      k
                                                                                           (B.14)
                                         −tj ≤ tl ≤ 0

We can now return to equation (B.8). The sign of its right hand side is determined by the sign of
                                 µ 0                          ¶
                                   ki     kj0     kk0     kl0
                                      ti + tj + tk + tl
                                   ki     kj      kk      kl

and since i < k < l < j, we have

                                            ki0  k0  k0   kj0
                                                ≤ k ≤ l ≤
                                            ki   kk  kl   kj

by the log-concavity of the kernel function. Since tk ≥ 0,

                            ki0     k0                ki0     k0     k0
                                tk ≤ k tk      ⇒          ti + k tk ≥ i (ti + tk )
                            ki      kk                ki      kk     ki

and since tj ≥ 0,

                            kj0     k0                kj0     k0     k0
                                tj ≥ l tj      ⇒          tj + l tl ≥ l (tl + tj ) .
                            kj      kl                kj      kl     kl

   Since now tl + tj ≥ 0,

                               kl0  k0               kl0             k0
                                   ≥ i      ⇒            (tl + tj ) ≥ i (tl + tj )
                               kl   ki               kl              ki


                                                        32
from which it follows that
                      µ 0                    ¶
                        ki     kj0  kk0  kl0   k0
                           ti + tj + tk + tl ≥ i (ti + tk + tl + tj ) = 0                         (B.15)
                        ki     kj   kk   kl    ki

hence the result (B.7).
   Hence m̂01,1 (x) ≥ 0, as desired. Setting m̂(1)(x) = m̂1,1 (x) and m̂(2) (x) = m̂01,1 (x) we therefore
have estimators of the slope and state-price density that will always satisfy the constraints in sample.




                                                  33
References
Abadir, K. and M. Rockinger, 1998, Density-Embedding Functions, Working paper, HEC School of Manage-
ment.
Afriat, S., 1967, The Construction of a Utility Function from Expenditure Data, International Economic
Review 8, 67-77.
Aït-Sahalia, Y., 1996, Nonparametric Pricing of Interest Rate Derivative Securities, Econometrica 64, 527-
560.
Aït-Sahalia, Y. and A. Lo, 1998, Nonparametric Estimation of State-Price Densities Implicit in Financial
Asset Prices, Journal of Finance 53, 499-547.
Aït-Sahalia, Y. and A. Lo, 2000, Nonparametric Risk Management and Implied Risk Aversion, Journal of
Econometrics 94, 9-51.
Aït-Sahalia, Y., Y. Wang and F. Yared, 2001, Do Option Markets Correctly Price the Probabilities of
Movement of the Underlying Asset?, Journal of Econometrics 102, 67-110.
Bahra, B., 1996, Probability Distributions of Future Asset Prices Implied by Option Prices, Bank of England
Quarterly Bulletin 36, 299—311.
Banz, R. and M. Miller, 1978, Prices for State-Contingent Claims: Some Estimates and Applications, Journal
of Business 51, 653-672.
Barlow, R.E., D.J. Bartholomew, J.M. Bremner and H.D. Brunk, 1972, Statistical Inference under Order
Restrictions, John Wiley and Sons, New York, NY.
Bates, D.S., 2000, Post-’87 Crash Fears in the S&P 500 Futures Option Market, Journal of Econometrics 94,
181-238.
Black, F. and M. Scholes, 1973, The Pricing of Options and Corporate Liabilities, Journal of Political
Economy 81, 637-659.
Bondarenko, O., 1997, Testing Rationality of Financial Markets, Working paper, Caltech.
Breeden, D. and R. Litzenberger, 1978, Prices of State-Contingent Claims Implicit in Option Prices, Journal
of Business 51, 621-651.
Chiew, C.K., 1976, Inequality-Constrained Least-Squares Estimation, Journal of the American Statistical
Association 71, 746-751.
Christoﬀersen, P. and K. Jacobs, The Importance of the Loss Function in Option Pricing, Working paper,
McGill University.
Diewert, W.E., 1973, Functional Forms for ProÞt and Transformation Functions, Journal of Economic Theory
6, 284-316.
Diewert, W.E. and C. Parkan, 1985, Tests for the Consistency of Consumer Data, Journal of Econometrics
30, 127-147.
Dole, D., 1999, Constrained Scatterplot Smoother for Estimating Convex, Monotonic Transformations, Jour-
nal of Business and Economic Statistics 17, 444-455.
Duﬃe, D., 1996, Dynamic Asset Pricing Theory, Second Edition, Princeton University Press, Princeton, NJ.
Dykstra, R.L., 1983, An Algorithm for Restricted Least Squares, Journal of the American Statistical Asso-
ciation 78, 837-842.
Escobar, L.A. and B. Skarpness, 1984, A Closed Form Solution for the Least Squares Regression Problem
with Linear Inequality Constraints, Communications in Statistics - Theoretical Methods 13, 1127-1134.
Fan, J. and I. Gijbels, 1996, Local Polynomial Modelling and Its Applications, London, UK: Chapman and
Hall.

                                                   34
Garcia, R. and R. Gencay, Pricing and Hedging Derivative Securities with Neural Networks and a Homo-
geneity Hint, Journal of Econometrics 94, 93-115.
Goldman, S.M. and P.A. Ruud, 1995, Nonparametric Multivariate Regression Subject to Constraint, Working
paper, UC Berkeley.
Haefke, C., H. White and A. Gottschling, 2000, Closed form Integration of ArtiÞcial Neural Networks with
Some Applications in Finance, Working paper, UC San Diego.
Hanson, D.L. and G. Pledger, 1976, Consistency in Concave Regression, The Annals of Statistics 4, 1038-
1050.
Hanson, D.L., G. Pledger and F.T. Wright, 1973, On Consistency in Monotonic Regression, The Annals of
Statistics 1, 401-421.
Härdle, W., 1990, Applied Nonparametric Regression, Cambridge University Press, Cambridge, UK.
Harrison, M. and D. Kreps, 1979, Martingales and Arbitrage in Multiperiod Securities Markets, Journal of
Economic Theory 20, 381-408.
Harvey, C. and R. Whaley, 1992, Market Volatility Prediction and the Eﬃciency of the S&P 100 Index
Option Market, Journal of Financial Economics 31, 43-73.
Hentschel, L., 2001, Errors in Implied Volatility Estimation, Working paper, University of Rochester.
Hildreth, C., 1954, Point Estimates of Ordinates of Concave Functions, Journal of the American Statistical
Association 49, 598-619.
Jackwerth, J.C. and M. Rubinstein, 1996, Recovering Probability Distributions from Contemporary Security
Prices, Journal of Finance 51, 1611-1631.
Jarrow, R. and A. Rudd, 1982, Approximate Option Valuation for Arbitrary Stochastic Processes, Journal
of Financial Economics 10, 347-369.
Judge, G.G. and T. Takayama, 1966, Inequality Restrictions in Regression Analysis, Journal of the American
Statistical Association 61, 166-181.
Liew, C.K., 1976, Inequality-Constrained Least-Squares Estimation, Journal of the American Statistical
Association 71, 746-751.
Mammen, E., 1991, Estimating a Smooth Monotone Regression Function, The Annals of Statistics 19, 724-
740.
Mammen, E and C. Thomas-Agnan, 1999, Smoothing Splines and Shape Restrictions, Scandinavian Journal
of Statistics 26, 239-252.
Matzkin, R.L., 1991, Semiparametric Estimation of Monotone and Concave Utility Functions for Polychoto-
mous Choice Models, Econometrica 59, 1315-1327.
Matzkin, R.L., 1992, Nonparametric and Distribution-Free Estimation of the Binary Choice and the
Threshold-Crossing Models of Monotone and Concave Utility Functions for Polychotomous Choice Mod-
els, Econometrica 60, 239-270.
Matzkin, R.L., 1994, Restrictions of Economic Theory in Nonparametric Methods, Chapter 42 in Handbook
of Econometrics Volume IV, Elsevier Science, Amsterdam, The Netherlands.
Matzkin, R.L. and M.K. Richter, 1991, Testing Strictly Concave Rationality, Journal of Economic Theory
53, 287-303.
Mukerjee, H., 1988, Monotone Nonparametric Regression, The Annals of Statistics 16, 741-750.
Merton, R., 1973, Rational Theory of Option Pricing, Bell Journal of Economics and Management Science
4, 141-183.



                                                   35
Robertson, T., F.T. Wright and R.L. Dykstra, 1988, Order Restricted Statistical Inference, John Wiley and
Sons, New York, NY.
Rubinstein, M., 1976, The Valuation of Uncertain Income Streams and the Pricing of Options, Bell Journal
of Economics, 407-425.
Ruud, P., 1997, Restricted Least Squares Subject to Monotonicity and Convexity Constraints, pp. 1676-1687
in Advances in Economics and Econometrics: Theory and Applications, Volume III, ed. by D.M. Kreps and
K.F. Wallis, Cambridge University Press, Cambridge, UK.
Schlee, W., 1980, Nonparametric Tests of the Monotonicity and Convexity of Regression, Colloquia Mathe-
matica Societatis Janos Bolyai, 823-836.
Tripathi, G., 1996, Semiparametric Eﬃciency Bounds under Shape Restriction, working paper, Northwestern
University.
Varian, H.R., 1982, The Nonparametric Approach to Demand Analysis, Econometrica 50, 945-973.
Varian, H.R., 1983, Nonparametric Tests of Models of Investor Behavior, Journal of Financial and Quanti-
tative Analysis 18, 269-278.
Varian, H.R., 1984, The Nonparametric Approach to Production Analysis, Econometrica 52, 579-597.
Wand, M.P. and M.C. Jones, 1995, Kernel Smoothing, Chapman and Hall, London, UK.
Wang, Y., 1993, The Limiting Regression in Concave Regression, working paper, University of Missouri-
Columbia.
Wright, F.T., 1981, The Asymptotic Behavior of Monotone Regression Estimators, The Annals of Statistics
9, 443-448.




                                                  36
                           Estimator                     p        Violation Frequency


                        Nadaraya-Watson                  0                 242/242

                          Locally Linear                 1                 130/242

                       Locally Quadratic                 2                 205/242

                          Locally Cubic                  3                 241/242

                  Constrained Locally Linear                                0/242




           Table 1: Occurrence of Arbitrage Restriction Violations during 1999


This table reports the percentage of trading days during year 1999 when the various estimators of the SPD
violated the arbitrage constraints (i.e., positivity of the SPD). By construction, our constrained estimator will
always satisfy the arbitrage restrictions. These results are for S&P500 index options with 30 to 90 (calendar)
days to expiration, and every day during year 1999 when such options are traded. Each day, we select the
25 most actively traded strikes, relying on put-call-parity as required to complete the range of traded in-the-
money calls on the basis of out-of-the-money put prices (which are more actively traded). For each estimator,
we used the bandwidths determined to be optimal in a sample of n = 25 strikes on the basis of our Monte
Carlo simulations reported earlier. The options data came from the CBOE.




                                                       37
          Figure 1: Nadaraya-Watson Estimator
                                        true function
                                        constraints
                                        average estimate
                                        95% confidence band



                             Panel A: Price Function

                   350
                   300
  option price




                   250
                   200
                   150
                   100
                    50
                     0
                     1000 1100 1200 1300 1400 1500 1600 1700
                                      strike


                          Panel B: First Strike-Derivative

                    0.2
                      0
first derivative




                   -0.2
                   -0.4
                   -0.6
                   -0.8
                     -1
                   -1.2
                      1000 1100 1200 1300 1400 1500 1600 1700
                                       strike


                                  Panel C: SPD

                     6
                     5
                     4
                     3
            spd




                     2
                     1
                     0
                    -1

                      -0.3   -0.2    -0.1    0         0.1   0.2
                                      logreturn
                   Figure 2: Locally Linear Estimator
                                        true function
                                        constraints
                                        average estimate
                                        95% confidence band



                             Panel A: Price Function

                   350
                   300
  option price




                   250
                   200
                   150
                   100
                    50
                     0
                     1000 1100 1200 1300 1400 1500 1600 1700
                                      strike


                          Panel B: First Strike-Derivative

                    0.2
                      0
first derivative




                   -0.2
                   -0.4
                   -0.6
                   -0.8
                     -1
                   -1.2
                      1000 1100 1200 1300 1400 1500 1600 1700
                                       strike


                                  Panel C: SPD

                     6
                     5
                     4
                     3
            spd




                     2
                     1
                     0
                    -1

                      -0.3   -0.2    -0.1    0         0.1   0.2
                                      logreturn
          Figure 3: Locally Quadratic Estimator
                                        true function
                                        constraints
                                        average estimate
                                        95% confidence band



                             Panel A: Price Function

                   350
                   300
  option price




                   250
                   200
                   150
                   100
                    50
                     0
                     1000 1100 1200 1300 1400 1500 1600 1700
                                      strike


                          Panel B: First Strike-Derivative

                    0.2
                      0
first derivative




                   -0.2
                   -0.4
                   -0.6
                   -0.8
                     -1
                   -1.2
                      1000 1100 1200 1300 1400 1500 1600 1700
                                       strike


                                  Panel C: SPD

                     6
                     5
                     4
                     3
            spd




                     2
                     1
                     0
                    -1

                      -0.3   -0.2    -0.1    0         0.1   0.2
                                      logreturn
                   Figure 4: Locally Cubic Estimator
                                        true function
                                        constraints
                                        average estimate
                                        95% confidence band



                             Panel A: Price Function

                   350
                   300
  option price




                   250
                   200
                   150
                   100
                    50
                     0
                     1000 1100 1200 1300 1400 1500 1600 1700
                                      strike


                          Panel B: First Strike-Derivative

                    0.2
                      0
first derivative




                   -0.2
                   -0.4
                   -0.6
                   -0.8
                     -1
                   -1.2
                      1000 1100 1200 1300 1400 1500 1600 1700
                                       strike


                                  Panel C: SPD

                     6
                     5
                     4
                     3
            spd




                     2
                     1
                     0
                    -1

                      -0.3   -0.2    -0.1    0         0.1   0.2
                                      logreturn
                    Figure 5: Constrained Estimator
                                        true function
                                        constraints
                                        average estimate
                                        95% confidence band



                             Panel A: Price Function

                   350
                   300
  option price




                   250
                   200
                   150
                   100
                    50
                     0
                     1000 1100 1200 1300 1400 1500 1600 1700
                                      strike


                          Panel B: First Strike-Derivative

                    0.2
                      0
first derivative




                   -0.2
                   -0.4
                   -0.6
                   -0.8
                     -1
                   -1.2
                      1000 1100 1200 1300 1400 1500 1600 1700
                                       strike


                                  Panel C: SPD

                     6
                     5
                     4
                     3
            spd




                     2
                     1
                     0
                    -1

                      -0.3   -0.2    -0.1    0         0.1   0.2
                                      logreturn
        Figure 6: Global Root Mean Squared Error and Bandwidth Selection


                                        Constrained estimator
                                        NadarayaWatson
                                        Locally linear
                                        Locally quadratic
                                        Locally cubic




Panel A: Square Root of the Integrated Mean Squared Error of the Price Function Estimators


        1800


        1600


        1400
rimse




        1200


        1000


         800


         600


         400
               0     50         100         150       200          250         300
                                            bandwidth
                                         Figure 6 continued

Panel B: Square Root of the Integrated Mean Squared Error of the First Strike-Derivative Estimators


             4




             3
     rimse




             2




             1




             0
                  0        50         100        150       200           250        300
                                                 bandwidth


             Panel C: Square Root of the Integrated Mean Squared Error of the SPD Estimators


             20




             15
    rimse




             10




              5




              0
                  0        50         100         150       200          250         300
                                                  bandwidth
Figure 7: Bias-Variance Trade-Off for the Constrained SPD Estimator

                                        true function
                                        average estimate
                                        95% confidence band



                         Panel A: Undersmoothed Bandwidth

                     6
                     5
                     4
               spd




                     3
                     2
                     1
                     0
                     -0.3    -0.2    -0.1    0      0.1       0.2
                                     logreturn


                            Panel B: Optimal Bandwidth

                     6
                     5
                     4
               spd




                     3
                     2
                     1
                     0
                     -0.3    -0.2    -0.1    0      0.1       0.2
                                     logreturn


                         Panel C: Oversmoothed Bandwidth

                     6
                     5
                     4
               spd




                     3
                     2
                     1
                     0
                     -0.3    -0.2    -0.1    0      0.1       0.2
                                     logreturn
Figure 8: Comparison with Two Parametric Estimators


                                 true function
                                 average estimate
                                 95% confidence band



      Panel A: Simulation Results for the Jarrow and Rudd Model


                                      5

                                      4

                                      3

                                      2

                                      1


    -0.3       -0.2       -0.1                   0.1        0.2


 Panel B: Simulation Results for the Mixture of Two Lognormals Model

                                      6

                                      5

                                      4

                                      3

                                      2

                                      1


    -0.3       -0.2       -0.1                   0.1        0.2
                           Figure 8 continued


                                      true function
                                      average estimate




     Panel C: Fit of the Jarrow and Rudd SPD to the True SPD with No Noise

                                          4


                                          3


                                          2


                                          1



         -0.3       -0.2       -0.1                  0.1       0.2


Panel D: Fit of theMixture of Two Lognormals SPD to the True SPD with No Noise

                                          4


                                          3


                                          2


                                          1



         -0.3       -0.2       -0.1                  0.1       0.2
                     Figure 9: S&P 500 Options, July Expiration on May 13, 1999

                                               Constrained estimator
                                               NadarayaWatson
                                               Locally linear
                                               Locally quadratic
                                               Locally cubic




                                    Panel A: Price Function Estimates


               350


               300


               250
option price




               200


               150


               100


                50


                 0
                           1000         1200             1400           1600      1800
                                                    strike
                                                 Figure 9 continued

                                        Panel B: First Strike-Derivative Estimates

                    0.2


                     0


                   -0.2
first derivative




                   -0.4


                   -0.6


                   -0.8


                    -1


                   -1.2
                                 1000            1200             1400                 1600         1800
                                                             strike


                                                Panel C: SPD Estimates
                     5


                     4


                     3


                     2
            spd




                     1


                     0


                    -1



                          -0.4      -0.3       -0.2       -0.1       0               0.1      0.2
                                                          logreturn
