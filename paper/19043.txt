                               NBER WORKING PAPER SERIES




         TWO-ARMED RESTLESS BANDITS WITH IMPERFECT INFORMATION:
                 STOCHASTIC CONTROL AND INDEXABILITY

                                        Roland G. Fryer, Jr.
                                          Philipp Harms

                                       Working Paper 19043
                               http://www.nber.org/papers/w19043


                     NATIONAL BUREAU OF ECONOMIC RESEARCH
                              1050 Massachusetts Avenue
                                Cambridge, MA 02138
                                     May 2013




We are grateful to Richard Holden, Peter Michor, Derek Neal, Ariel Pakes, Yuliy Sannikov, Mete
Soner, Josef Teichmann and seminar participants at Barcelona GSE and Harvard University for helpful
comments and suggestions. Financial support from the Education Innovation Laboratory at Harvard
University is gratefully acknowledged. Correspondence can be addressed to the authors by e-mail:
rfryer@fas.harvard.edu [Fryer] or pharms@edlabs.harvard.edu [Harms]. The usual caveat applies.
The views expressed herein are those of the authors and do not necessarily reflect the views of the
National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2013 by Roland G. Fryer, Jr. and Philipp Harms. All rights reserved. Short sections of text, not to
exceed two paragraphs, may be quoted without explicit permission provided that full credit, including
© notice, is given to the source.
Two-Armed Restless Bandits with Imperfect Information: Stochastic Control and Indexability
Roland G. Fryer, Jr. and Philipp Harms
NBER Working Paper No. 19043
May 2013
JEL No. J0,J24,L0

                                              ABSTRACT

We present a two-armed bandit model of decision making under uncertainty where the expected return
to investing in the "risky arm'' increases when choosing that arm and decreases when choosing the
"safe'' arm. These dynamics are natural in applications such as human capital development, job search,
and occupational choice. Using new insights from stochastic control, along with a monotonicity condition
on the payoff dynamics, we show that optimal strategies in our model are stopping rules that can be
characterized by an index which formally coincides with Gittins' index. Our result implies the indexability
of a new class of "restless'' bandit models.


Roland G. Fryer, Jr.
Department of Economics
Harvard University
Littauer Center 208
Cambridge, MA 02138
and NBER
rfryer@fas.harvard.edu

Philipp Harms
44 Brattle Street
Cambridge MA 02138
pharms@edlabs.harvard.edu
1    Introduction
Bandit models are sequential decision problems where, at each stage, a resource like time, effort
or money has to be allocated strategically between several options, referred to as the arms of the
bandit. When selected, the arms yield payoffs that typically depend on unknown parameters. Arms
that are not selected remain unchanged and yield no payoff. The key idea in this class of models is
that agents face a tradeoff between experimentation (gathering information on the returns to each
arm) and exploitation (choosing the arm with the highest expected value).
    Over the past sixty years, bandit models have become an important framework in economic
theory, applied math and probability, and operations research. They have been used to analyze
problems as diverse as market pricing, the optimal design of clinical trials, product search and the
research and development activities of firms (Rothschild 1974; Berry and Fristedt 1985; Bolton and
Harris 1999; Keller and Rady 2010). To understand how firms set prices without a clear under-
standing of their demand curves, Rothschild (1974) posits that firms repeatedly charge prices and
observe the resulting demand. Setting prices too high or too low is costly for firms (experimenta-
tion), but allows them to learn about the optimal price (exploitation). In the optimal design of
clinical trials, Berry and Fristedt (1985) formulate the problem as: given a fixed research budget,
how does one allocate effort among competing projects, whose properties are only partially known
at a given point in time but may be better understood as time passes. In product search, customers
sample products to learn about their quality. Their optimizing behavior can be described as in
Bolton and Harris (1999, 2000). In these models, news about the quality of the product arrive
continuously. The situation where news arrive only occasionally, e.g. in the form of break-throughs
in research, is modeled by Keller et al. (2005, 2010).
    An important assumption in the classical bandit literature is that the reward distributions of
arms that are not chosen do not evolve; they rest (Gittins, Glazebrook, and Weber 2011). This
assumption seems natural in many applications. Yet, in many other important scenarios, it seems
overly restrictive.1 Consider, for instance, the possibility of dynamic complementarities in human
capital production.2 Imagine a student who has the choice of whether or not to invest effort into her
school work. Today’s effort is rewarded by being more at ease with tomorrow’s course work, or the
ability to glean a deeper understanding from class lectures. As Cunha and Heckman (2010) note,
“learning begets learning.” Conversely, not doing one’s assignments today might give instantaneous
gratification, but makes tomorrow’s school work harder. More generally, this dynamic can be found
in the context of human capital formation when early investments in human capital increase the
expected payoff of future investments, while a lack of early investments has the reverse effect. These
dynamics require arms that evolve, even when they are not used.
    As a second example, consider an unemployed worker looking for a job. With every job applica-
tion, she gathers both information about the job market and experience in the application process,

  1. The importance of relaxing this assumption has been recognized early on in the seminal work of Whittle (1988),
who proposed clinical trials, aircraft surveillance and assignment of workers to tasks as potential applications.
  2. Cunha et al. (2006) make a similar argument in a different context.


                                                        2
which typically increases her chances of successful future job applications. Conversely, not actively
searching for a job may decrease the probability of finding a job in future applications. This could
be due to market penalties for unemployment spells, being disconnected from the changing charac-
teristics of the job market and the application process, or be considered a signal of low motivation
by potential employers.
    Bandits whose inactive arms are allowed to evolve are known as “restless bandits.”3 Generally,
optimal strategies for restless bandits are unknown.4 However, when a certain indexability condition
is met, Whittle’s (1988) index can lead to approximately optimal solutions (Weber and Weiss 1990,
1991). This index plays the same fundamental role for restless bandits that Gittins’ (1979) index
has for classical ones: it decomposes the task of solving multi-armed bandits into multiple tasks of
solving bandits with one known and one unknown arm. The known arm yields constant rewards and
can be interpreted as a cost of investment in the unknown arm. Deriving conditions that identify
general classes of indexable restless bandit models is an important contribution – permitting more
complete analysis of decision problems in which choices jointly effect instantaneous payoffs as well
as the distribution of those payoffs in the future – and the subject of this paper.
    The origins of this work are the classical bandit models of Bolton and Harris (1999), Keller
and Rady (2010), and Cohen and Solan (2013) that we extend to the restless case. In these
pioneering works, the reward from the unknown arm is Brownian motion, a Poisson process, or
a Levy process. The unobserved quantity is a Bernoulli variable called the “type” of the agent.
Optimal strategies are found by constructing explicit solutions of the Hamilton-Jacobi-Bellman
(HJB) equation, which is a non-linear second-order differential-difference equation. Our model
is an extension of these models containing them as special cases.5 Namely, we allow the same
generality of reward processes with both volatility and jumps, but make the reward distribution
dependent on the type of the agent and the history of past investments. The latter dependence
is mediated by a real valued variable that increases while the agent invests in the unknown arm
and decreases otherwise. In line with our motivating examples of human capital formation and
job search, we call this variable the agent’s human capital. The inclusion of human capital as a
state variable turns the HJB equation into a second order partial differential-difference equation.
It seems unlikely that explicit solutions to this equation can be found.
    Using new insights from stochastic control theory, along with a monotonicity condition on
the restless arm (e.g. today’s investments make tomorrow’s investments more profitable, while a
lack of investment today decreases the profits of future investments), this paper establishes two
results. The first result is a separation theorem (theorem 1) that establishes the equivalence of
the optimal control problem with partial observations to a fully observed control problem called
the separated control problem. The fact that this equivalence holds is crucial for the solution of

   3. Bandits where the active and passive action have opposite effects on payoffs are called bi-directional bandits
(Glazebrook, Kirkbride, and Ruiz-Hernandez 2006), and our model falls into this class.
   4. Numerical solutions can be obtained by (possibly approximate) dynamic programming or a linear programming
reformulation of the problem (Kushner and Dupuis 2000; Powell 2007; Nino-Mora 2001).
   5. However, these works focus on strategic equilibria involving multiple agents, whereas we only treat the single
agent case.


                                                         3
the problem and is implicitly used in many works, including Bolton and Harris (1999); Keller and
Rady (2010); Cohen and Solan (2013). Standard formulations of the partially observable control
problem involving Zakai’s transform (Fleming and Pardoux 1982) or time changes (El Karoui and
Karatzas 1994) do not work for restless bandit problems. However, we show that the frameworks
of Fleming and Nisio (1966); Wonham (1968); Kohlmann (1982) can be used and extended to
general semimartingale observation processes. We describe these issues in detail since they are
rarely discussed in the context of bandit problems. In our second, and main, result (theorem 2),
we establish the optimality of stopping rules and characterize optimal strategies by an index that
formally coincides with Gittins’ index. This allows us to deduce comparative statics which imply
the indexability of our model in the sense of Whittle (1988). We provide a sketch of our approach
below.
   The first sections of the analysis are dedicated to a rigorous setup of the stochastic control
problem in continuous time (sections 3.3-3.5). The bandit problem is first formulated as a problem
of stochastic optimal control under partial observations. The structure of our model requires a
strong measurability condition on controls to ensure that they do not depend on the unobserved
type, see section 3.3. The condition has been used in the early literature on optimal control (Fleming
and Nisio 1966; Wonham 1968; Kohlmann 1982) but came out of favor because it entails difficulties
in establishing the existence of optimal controls.
   We are able to circumvent these difficulties by showing that regardless of the strong measur-
ability constraint, the partially observable problem is equivalent to a so-called separated problem
with full observations. In the separated problem, admissibility of controls can be defined as usual
and the existence of optimal controls is well-known. The separated problem is derived from the
partially observable one by replacing the unobserved quantity by its filter, which is its conditional
distribution given the past observations. Put differently, the filter is the belief of the agent in being
the high type. The equivalence of the partially observable and the separated problem is established
in theorem 1. Notice: the monotonicity condition is not needed there.
   Our second, and main, result is that stopping rules are optimal. This result hinges on the
monotonicity condition and is established in theorem 2. Our proof is based on a direct investigation
of the sample paths of optimal strategies and an evaluation of the benefits of investing in the
unknown arm sooner rather than later. This interchange argument was originally developed by
Berry and Fristedt (1985) for classical bandit models, but the monotonicity assumption on the
payoffs is precisely what is needed to make a similar argument work in the more general setting of
restless bandits.
   The result on optimal stopping means that it always better to invest first in the unknown
arm and then in the known arm instead of the other way round. Intuitively, this sequence of
investments matters for two reasons. First, investments in the unknown arm reveal information
about the distribution of rewards. The sooner this information becomes available, the better.
Second, early investments in the known arm deteriorate the rewards of later investments in the
unknown arm. By contrast, early investments in the unknown arm do not make the known arm


                                                     4
any less profitable. It follows that agents find it optimal to invest in the unknown arm initially.
They then switch to the known arm and never start investing in the unknown arm again.6 The
time where this change occurs is a stopping time that depends on the history of obtained rewards.
    Once the optimality of stopping rules is established, it follows easily that optimal strategies
can be characterized by an index rule. Formally, the index is the same as the one proposed in
the celebrated result by Gittins (1979) on classical bandits, but unused arms are allowed to evolve.
The explicit formula for the index yields comparative statics of the frontier with respect to the
parameters of the model. Most importantly, subsidies of the known arm enlarge the set of states
where the known arm is optimal, which means that our bandit model is indexable in the sense
of Whittle (1988). More generally, any arm of a multi-armed restless bandit that satisfies our
monotonicity condition is indexable. To our knowledge, this is the first time that a sufficient
condition for indexability of a general class of restless bandits with continuous state space and a
corresponding rich class of reward processes has been formulated.7
    To explain the structure of optimal strategies, we consider how information is processed by the
agents in our model. We work in a Bayesian setting where the agent has a prior about being either
“high” or “low type.” Rewards obtained from the unknown arm depend on this type and are used
by the agent to form a posterior belief. The current levels of belief and human capital determine
at each stage whether it is optimal to invest in the unknown or known arm. Namely, there is a
curve in the belief–human capital domain such that it is optimal to invest in the unknown arm
if the current level of belief and human capital lies to the right and above the curve. Otherwise,
it is optimal to invest in the known arm. This follows from the index representation of optimal
strategies. The curve is called the decision frontier. It can be characterized as a level set of the
index or value function.
    Similar to classical bandit model, the dynamics of belief and human capital depend on the
position relative to the decision frontier. There is an important, and potentially empirically relevant,
difference: below the frontier, agents do not obtain any new information, and their belief remains
constant, while their human capital decreases continually. In other words, not only is the safe arm
absorbing – it is depreciating; agents drift further and further away from the frontier. Empirically,
this implies that there are very few “marginal” agents. Programs (e.g. lower class size, school
choice, financial incentives) designed to increase student achievement at the margin are likely to
be ineffective unless: (1) they are initiated when students get close to the decision frontier, or (2)
force inframarginal students to invest in the unknown arm (e.g. some charter schools (Dobbie and
Fryer 2011)). Consistent with Cunha et al. (2006), our model predicts that, on average, the longer
society waits to invest the more aggressive the investment needs to be.
    The situation is different for agents above the frontier. They continually obtain new information
about their type and update their posterior belief accordingly. At the same time, their level of

   6. This argument is made rigorous in the proof of theorem 2.
   7. Some sensor management models are indexable and have a continuous state space after their transformation
to fully observed Markov decision problems (Washburn 2008). However, this is not the case in their formulation as
partially observable control problems.


                                                       5
human capital increases. In the long run, there are two possibilities. Either there is some point in
time where they hit the frontier. This happens when they encounter a series of bad outcomes from
the unknown arm and their belief level drops down far enough. In this case, they meet the same
fate as agents who originally started out below the frontier. Or they never reach the frontier. In this
case, they invest in the unknown arm forever and learn their true type in the limit. In fact, under
reasonable assumptions, investing in the unknown arm for an infinite amount of time is necessary
and sufficient for asymptotic learning to occur (see theorem 3). To summarize, agents of the low
type eventually end up choosing the known arm, which is optimal for them. However, high type
agents can get discouraged by bad luck and stop investing in the unknown arm, even though the
unknown arm would be optimal. Possible limit points of agents’ trajectories in the belief–human
capital space are depicted in Figures 1 and 2.
   Our paper makes five contributions to the existing literature. First, we present an extension of
classical bandit models of investment under uncertainty motivated by dynamic aspects of resource
development. The model is new and has economic significance in a wide range of real world settings.
As an example, we present how our model can be used to describe the economics of investment
in education and discuss some potential policy implications. Second, we discover a new class of
indexable restless bandit models. While other classes of indexable bandits are known, they either
involve no learning about one’s type (Glazebrook, Kirkbride, and Ruiz-Hernandez 2006), do not
allow history-dependent payoffs (Washburn 2008), or work with very specific reward processes (e.g.
Markov chains on finite state spaces as in Nino-Mora (2001)). Third, we deal with the delicate
issue of setting up the partially observable control problem in continuous time. Recent standard
formulations of optimal control under partial observation do not apply in our setting. We rediscover
a framework that has been used mostly in the early control literature and show that it meets the
needs of both classical and restless bandit models. In addition to its importance to the theory of
optimal control, this is also a contribution to the bandit literature.
   Fourth, we present an unconventional approach to solve the bandit model. The work horse
of most of the bandit literature is either the Hamilton-Jacobi-Bellman equation or a setup using
time changes. However, these approaches are not well adapted to the generality of our model, in
particular the new dynamics. Our argument is based on an investigation of the sample paths of
optimal strategies. More specifically, we discretize the problem in time and show that any optimal
strategy can be modified such that the agent never invests after a period of not investing and
such that the modified strategy is still optimal. This interchange argument has been originally
developed by Berry and Fristedt (1985) for classical bandits. It turns out that the monotonic
dependence of the payoffs on the amount of past investment is exactly what is needed to generalize
the argument to restless bandits. Fifth, we provide a general bandit structure that encompasses
both the exponential bandit model of Keller, Rady, and Cripps (2005), where jumps can occur only
for high type agents, and the Poisson and Levy bandit models of Keller and Rady (2010, 2012) and
Cohen and Solan (2013), where it is assumed that one jump measure is absolutely continuous with
respect to the other.


                                                  6
    The paper is structured as follows. Section 2 provides a brief review of the bandit literature
in economics and applied math. Section 3.1 provides the model and section 3.2 connects our for-
mulation, using semimartingales, to classical bandit models. A precise formulation of the partially
observable problem is developed in section 3.3. The separated problem is defined in section 3.4 and
the equivalence of the two problems is established in section 3.5. In section 3.6, it is shown that
both problems are equivalent to optimal stopping. Furthermore, optimal strategies are character-
ized in terms of Gittins’ index. In sections 3.7 and 3.8, asymptotic learning and long term limits
of the belief process are studied. Finally, section 4 concludes.


2     A brief review of the multi-armed bandit literature
2.1    Models
Originally developed by Robbins (1952), bandit models have been used to analyze a wide range
of economic and applied math problems.8 The first paper where a bandit model was used in an
economic context is Rothschild (1974), in which a single firm facing a market with unknown demand
has to determine optimal prices. Subsequent applications of bandit models include partner search,
effort allocation in research, clinical trials, network scheduling and voting in repeated elections
(McCall and McCall 1987; Weitzman 1979; Berry and Fristedt 1985; Li and Neely 2012; Banks and
Sundaram 1992).
    Classical bandits with reward processes driven by Brownian motion or a Poisson process were
first solved by Karatzas (1984) and Presman (1990). Subsequently, Bolton and Harris (1999, 2000)
and Keller e.a. (2005, 2010, 2012) derived explicit formulas for optimal strategies in the case where
the unobservable quantity is a Bernoulli variable and treated strategic interactions of multiple
agents. Cohen and Solan (2013) unified the formulas obtained for the single agent case and solved
a bandit model where the reward is driven by a Levy process with unknown Levy triplet.
    Many extensions and variations of classical bandit problems have been proposed, including:
bandits with a varying finite or infinite numbers of arms (Whittle 1981; Banks and Sundaram
1992), bandits where an adversary has control over the payoffs (Auer et al. 2002/03), bandits
with dependent arms (Pandey, Chakrabarti, and Agarwal 2007), bandits where multiple arms can
be chosen at the same time (Whittle 1988), bandits whose arms yield rewards even when they
are inactive (Glazebrook, Kirkbride, and Ruiz-Hernandez 2006), and bandits with switching costs
(Banks and Sundaram 1994).
    One of the most mathematically challenging extensions is to allow inactive arms to evolve.
Such bandits are often referred to as “restless bandits.”9 This term was coined in the seminal
paper of Whittle (1988). Beyond mathematical intrigue, there are many practical applications:
aircraft surveillance, sensor scheduling, queue management, clinical trials, assignment of workers to
   8. Basu, Bose, and Ghosh (1990), Bergemann and Välimäki (2006), and Mahajan and Teneketzis (2008) are
excellent surveys of the literature on bandit models. The monographs by Presman and Sonin (1990), Berry and
Fristedt (1985) and Gittins, Glazebrook, and Weber (2011) contain more detailed presentations.
   9. Some bandits with switching costs can be modeled as restless bandits (Jun 2004).


                                                    7
tasks, robotics, and target tracking (Ny, Dahleh, and Feron 2008; Veatch and Wein 1996; Whittle
1988; Faihe and Müller 1998; La Scala and Moran 2006). In aircraft surveillance, Ny, Dahleh, and
Feron (2008) discuss the problem of surveying ships for possible bilge water dumping. A group
of unmanned aerial vehicles can be sent to the sites of the ships. The rewards are associated
with the detection of a dumping event. The problem falls into the class of sensor management
problems where a set of sensors has to be assigned to a larger set of channels whose state evolves
stochastically. In queue management, Veatch and Wein (1996) consider the task of scheduling
a make-to-stock production facility with multiple products. Finished products are stored in an
inventory. Too small an inventory risks incurring backorder or lost sales costs, while too large
an inventory increases holding costs. In robotics, Faihe and Müller (1998) consider the behaviors
coordination problem in a setting of reinforcement learning: a robot is trained to perform complex
actions that are synthesized from elementary ones by giving it feedback about its success.

2.2   Optimality of stopping rules
For classical bandit models with one known and one unknown arm, the optimality of stopping
rules is a well known result (Berry and Fristedt 1985; El Karoui and Karatzas 1994). Several
approaches to establish this can be found in the literature. In one approach, the rewards of each
arm are fixed in advance and strategies are time changes. The reward that is obtained under a
strategy is the time change applied to the reward process. This setup, which has been proposed
by Mandelbaum (1987), allows a very simple formulation of the measurability constraints on the
strategies. However, it is not well-adapted to bandits with evolving arms. In a second approach, one
solves the Hamilton-Jacobi-Bellman (HJB) equation for the value function. When this succeeds,
the explicit form of the value function can be used to establish the optimality of stopping rules
(Bolton and Harris 1999; Keller, Rady, and Cripps 2005; Cohen and Solan 2013). However, in
our model, the dynamics of the reward distribution introduce an additional state variable, which
turns the HJB equation into a non-local partial differential equation which we cannot solve directly.
Moreover, it is not clear a-priori if the value function is a solution in a classical sense. Pham (1995,
1998) showed that under suitable assumptions, the value function is a viscosity solution of the HJB
equation. However, it remains open how this could be used to show that stopping rules are optimal.
The third approach is to rewrite the problem as a linear programming problem. This makes both
classical and restless bandit problems amenable to efficient numerical computations and can also
yield some qualitative insight (Nino-Mora 2001).10 The fourth approach (and the one we emulate)
is based on a direct investigation of the sample paths of optimal strategies and an evaluation of the
benefits of investing in the unknown arm sooner rather than later. While this interchange argument
was originally developed by Berry and Fristedt (1985) for classical bandit models, it turns out that
the monotonicity assumption on the payoffs is what is needed to make the argument work in the
more general setting of restless bandits.


 10. Another numerical approach is dynamic programming/value function iteration.


                                                      8
2.3    Indexability
In the classical bandit model, Gittins (1979) characterized optimal strategies by an index that is
assigned to each arm of the bandit at each instant of time. The optimal strategy is to always
choose the arm with the highest index. The indices can be calculated for each arm separately,
which reduces the complexity of multi-armed bandits to that of two-armed bandits with one known
and one unknown arm.
    In general, optimal strategies in restless bandit models do not admit an index representation.
However, a Lagrangian relaxation of the problem proposed by Whittle (1988) yields index strate-
gies that are approximately optimal (Weber and Weiss 1990, 1991). The corresponding “Whittle
index” (Whittle 1988) is the Lagrange multiplier in a constrained optimization problem and has
an economic interpretation as a subsidy for passivity or a fair charge for operating the arm. A
major challenge to the deployment of Whittle’s index is that it can only be defined when a certain
indexability condition is met. In this condition, each arm of the restless bandit is compared to a
hypothetical arm with known and constant reward. The indexability condition holds if the set of
states where the known arm is optimal is increasing in the reward from the known arm.11
    The question of indexability of restless bandit models is subtle and not yet fully understood.
Gittins, Glazebrook, and Weber (2011) give an overview of various approaches to establish the
indexability of restless bandit models. Partial answers are known for bandits with finite or count-
able state spaces. Indexability of such models can be tested numerically in a linear programming
reformulation of the Markov decision problem (Klimov 1975). In another line of research, Nino-
Mora (2001) showed that indexability holds for restless bandits satisfying a partial conservation
law, which can be verified by running an algorithm. While this can be used to test the indexability
of specific restless bandit problems, it does not provide much qualitative insight into which restless
bandits are indexable. One would like to have conditions that identify general classes of indexable
restless bandit models – this is the subject of this paper.12


3     A Two-Armed Restless Bandit
3.1    Basic Building Blocks
Time t ∈ [0, ∞) is continuous and there is one agent. Nature moves first and assigns a type
Θ ∈ {0, 1} and an initial human capital H0 to the agent.13 The agent does not know her type
but believes in being of the high type (Θ = 1) with probability P0 . At each instant of time she

  11. This is a monotonicity condition on the optimal strategy, which is not to be confounded with our monotonicity
condition on the payoffs and the evolution of human capital.
  12. Some results in this direction have been obtained for various bandit models related to sensor management, see
the survey of Washburn (2008). Other classes of indexable problems are the dual speed problem of Glazebrook,
Nino-Mora, and Ansell (2002), the maintenance models of Glazebrook, Ruiz-Hernandez, and Kirkbride (2006), and
the spinning plates and squad models of Glazebrook, Kirkbride, and Ruiz-Hernandez (2006). The spinning plates
model is most similar to ours. It satisfies the same monotonicity condition as our model, but has a different reward
structure and assume perfect information.
  13. To fix ideas, we refer to H as the human capital of the agent, but our model is not bound to this interpretation.


                                                          9
decides what fraction of time to invest in the known and the unknown arm. Let Ut ∈ [0, 1] be her
investment decision at time t; Ut = 1 standing for investment in the unknown arm and Ut = 0
for investment in the known arm. Investments in the unknown arm increase her human capital,
whereas investments in the known arm allow it to depreciate. The resulting level of human capital
at time t is denoted by Ht . The rate at which human capital increases is denoted by α(1, Ht )
and the rate at which it decreases α(0, Ht ). Thus the human capital process solves the (pathwise)
differential equation
                                                                        
                                 dHt = Ut α(1, Ht ) + (1 − Ut )α(0, Ht ) dt.                                   (1)

At each instant of time, the agent receives a random reward dRt that is characterized by the
following three quantities:

                                 bt = Ut β(Θ, Ht ) + (1 − Ut )k, (drift)
                                 ct = Ut σ(Ht )2 ,                                   (volatility)              (2)
                           νt (dr) = Ut K(Θ, Ht , dr)                                (jump measure)

Vaguely speaking, equations (2) describe the drift, volatility and jump measure of the reward
process;                         Z    t                     Z     t
                          Bt =            bs ds,   Ct =               cs ds,    µ(dt, dx) = νt (dx)dt          (3)
                                  0                           0

are its semimartingale characteristics. These characteristics are defined with respect to some trun-
cation function χ : R → R that we fix once and for all. χ is equal to the identity on a neighborhood
of zero and bounded, continuous. If the jump measure K(θ, h, dr) is finite, the truncation function
is not needed and can be set to zero for all purposes.
   If follows that the payoff to the known arm is deterministic and equals k dt. The payoff to the
unknown arm is random and depends on the type and the level of human capital.14 Rewards are
discounted with a discount rate ρ > 0, and the agent tries to maximize her expected future rewards
                                                       Z     ∞                  
                                                                        −ρt
                                                   E              ρe          dRt .                            (4)
                                                          0

Only the reward of the arm that is chosen is observable. Formally, the coefficients α, β, σ, K, k, ρ of
the model and the history of the processes U, H, R are known, but Θ is not.15 Investment decisions
Ut are restricted to depend on available information. Thus the agent’s problem is a control problem
with partial observations. A precise formulation of the problem is developed in section 3.3. When
α = 0 or b, c, ν do not depend on human capital, the model reduces to a classical bandit model.
Otherwise, it is a bandit with evolving arms or restless bandit.
   To assure that the control problem is well-defined, we make the following assumptions on the
coefficients of the model.


 14. The volatility does not depend on the type because the inference problem would be trivial in that case.
 15. This is a key point of departure from Glazebrook, Kirkbride, and Ruiz-Hernandez (2006).


                                                                      10
Assumption 1 (Boundedness and regularity). The functions

                                α, β : {0, 1} × R → R,         σ:R→R                             (5)

are measurable and K is a transition kernel from {0, 1}×R to R\{0}. Furthermore, the expressions
                                                     Z
                                                          |r|2 ∧ |r| K(θ, h, dr)
                                                                    
                           α(u, h), β(θ, h), σ(h),                                               (6)
                                                     R

are uniformly bounded and α(u, h) is Lipschitz continuous in h.

   The uniform bounds on the coefficients and the Lipschitz condition ensure that human capital
process is well-defined and that the expectation in equation (4) exists. The integrability condition
on the jump measure implies that the reward process is a special semimartingale. This means that
it has a compensator, which is a predictable process of integrable variation that differs from the
reward process by a local martingale. Intuitively, the existence of a compensator means that agents
are able to form expectations about the infinitesimal increments of the reward process.

3.2   Relation to classical bandit models
In this section, we relate our model to a number of important classical bandit models. In these
models, the reward process is either Brownian motion with unknown drift (e.g. Bolton and Harris
(1999)), a Poisson process with unknown intensity (e.g. Keller and Rady (2010)), or more generally,
a Levy process with unknown Levy triple (e.g. Cohen and Solan (2013)). For consistency, we impose
our notation on the models that we discuss in this section.
   We begin with the simplest case where the reward process is Brownian motion with unknown
drift. This model was introduced by Chernoff and Ray (1965) and subsequently extended by Bolton
and Harris (1999, 2000). Using our notation, the reward process in this model is
                                                                    p
                           dRt = Ut β(Θ)dt + (1 − Ut )kdt +             Ut σdWt ,                (7)

where W is Brownian motion independent of Θ. The square root in equation (7) permits an
interpretation of [0, 1]-valued controls Ut as fractions of time devoted to the unknown arm. Namely,
              Rt√
the process 0 Us dWs is equal in distribution to the time changed process WTt , where Tt =
Rt
 0 Us ds is the accumulated amount of investment in the unknown arm (Kallsen and Shiryaev 2002,
theorem 1.5). The characteristics of the reward process in equation (7) are (B, C, µ) as in (3) with

                          bt = Ut β(Θ) + (1 − Ut )k,       c t = Ut σ 2 ,   νt = 0.              (8)

   The second case that we discuss is when the reward is a Poisson process with unknown intensity.
The first treatment of this model, by Presman and Sonin (1990), is extended by Keller and Rady
(2010). In their model, the probability that the agent receives a lump-sum payoff in the interval
[t, t + dt) is Ut λ(Θ)dt, where λ is a non-negative function of Θ, and the distribution of lump-sum

                                                     11
payoffs is given by a probability measure K on R\{0}. This statement can be made precise by
interpreting it as a specification of the characteristics of the reward process. They are (B, C, µ) as
in (3) with
                          bt = (1 − Ut )k,   ct = 0,           νt (dr) = Ut λ(Θ)K(dr).               (9)

   The last, and most general, case that we discuss are Lévy bandits (Cohen and Solan 2013).
The reward process in their model is driven by a Lévy process X whose Lévy triplet depends on
the unknown type Θ and is given by (β(Θ), σ, K(Θ, ·)). Under a strategy U , the agent obtains the
reward XTt from the unknown arm, where T is the time change
                                                     Z     t
                                              Tt =             Us ds.                               (10)
                                                       0

The reward from the known arm is k(t − Tt ). The approach of defining reward processes using time
changes is due to Mandelbaum (1987). It allows for a clean definition of admissible strategies and
circumvents difficulties that arise when trying to add a strategy-dependent jump term to equation
(7). An alternative to using time changes is to specify the characteristics of the reward process.
They are given by (B, C, µ) as in equation (3) with

                    bt = Ut β(Θ) + (1 − Ut )k,    c t = Ut σ 2 ,        νt (dr) = Ut K(Θ, dr).      (11)

   Taken together, the classical bandit models presented in this section can be formulated in a
convenient setting using semimartingale characteristics. This demonstrates that the only difference
between these models and ours is that the characteristics in our model depend not only on the type
of the agent, but also on an additional variable quantifying the amount of past investment.

3.3    The partially observable (p.o.) control problem
In this section, we describe: (a) what it means for the reward process R to be controlled by U and
(b) what it means for U to be non-anticipative and to depend only on observable quantities. In an
effort to make the paper self-contained, we err on the side of providing more detail.
   The first issue (a) is straightforward. As discussed in the previous sections, we found it conve-
nient to specify the distribution of the reward process by its semimartingale characteristics. It is
well-known that specifying characteristics (B, C, µ) for R is equivalent to the following martingale
problem (see Jacod and Shiryaev (2003, theorem II.2.42)): for each f ∈ Cb2 (R), the process

                                       1
      f (R) − f (R0 ) − f 0 (R− ) · B − f 00 (R− ) · C − f (R− + r) − f (R− ) − f 0 (R− )χ(r) ∗ µ
                                                                                             
                                                                                                    (12)
                                       2

is a martingale. (We use the notation · and ∗ of Jacod and Shiryaev (2003) to denote stochastic
integration with respect to semimartingales and random measures.) This brings us closer to the
stance of controlled Markov processes. The reward process itself is not Markov, but the three-
dimensional state-observation process (Θ, H, R) is, at least under constant control. It is well known


                                                     12
that Markov processes can be specified by their generator. In our case, this would be a non-local
second order differential operator. The coefficients of this operator are closely related to the drift,
volatility and jump measure defined in equation (2). This puts our model in the framework of
partially observed controlled Markov processes.16 Yet another option is to formulate an SDE for
the reward process. To account for jumps, one could try to add a term dNt to the right-hand side
of equation (7), where N is a compound Poisson process whose jump intensity depends on the type
and the control. It follows that one needs to consider a family of compound Poisson processes to
allow for arbitrary controls. Then the equation turns into an SDE driven by non-linear Lévy noise
in the sense of Kolokoltsov (2010, theorems 3.7 and 3.11).
    We now come to the second task (b) of defining what it means for U to be non-anticipative
and to depend only on observable quantities. This matter is rarely discussed in the applied bandit
literature. However, the theoretical literature shows that it can be a delicate issue. A basic
requirement is that the control process must be predictable with respect to the filtration generated
by the observation process. Ceci and Gerardi (1998) demonstrated that this requirement is sufficient
when the observation process is a counting process. Their result applies for example to the Poisson
and exponential bandit models of Keller, Rady, and Cripps (2005) and Keller and Rady (2010).
    However, the above requirement is not stringent enough in general. For example, in our model,
it would be tempting to admit any control process U that is FH,R -predictable, where FH,R is the
filtration generated by the observable processes H and R. (Generally, we will write FX = (FtX )t≥0
for the filtration generated by a process X.) But the differential equation (1) for H shows that
U can be reconstructed from H, at least when α 6= 0 and under a continuity assumption like U
being càglàd. Then FU ⊆ FH,R holds automatically and it is pointless to require U to be FH,R -
predictable. Namely, any càglàd process U is FH,R -predictable, regardless of whether it depends
on the supposedly unobserved state or not. Similar problems arise when requiring U to be FR -
predictable since U can be reconstructed from the quadratic variation process of R.17
    We use an approach popular in the early works in optimal control theory (Fleming and Nisio
(1966); Wonham (1968); Kohlmann (1982)). Namely, we require that Ut (ω) = Ut (ω 0 ) holds when-
ever the reward process satisfies Rs (ω) = Rs (ω 0 ) for all s < t. This is equivalent to defining the
control process as a functional of the reward process in the sense that U = F (R), where F is a
predictable process on Skorokhod space D(R). This space is the canonical space of càdlàg paths,
which are right-continuous functions [0, ∞) → R with left limits. It is natural to assume that every
sample path of the reward process is an element of this space. Under the strong measurability
condition on the control, we are not able to prove that the set of admissible controls is compact
and have to establish the existence of optimal controls in an indirect way. To this aim, we will
transform the partially observed problem into a problem of full observations called the separated


 16. See also Kurtz and Ocone (1985) and Kurtz and Stockbridge (1998) who introduced a filtered martingale
problem characterizing the conditional distribution of the unobserved state process and Stockbridge (2005) who
proved a separation theorem in a similar setting.
 17. Moreover, putting the reward process into a control-independent form by Zakai’s measure transform (Fleming
and Pardoux 1982) is not possible for general bandit models.


                                                      13
problem. This will be done in section 3.4.
    Before we can define control processes, we need to make precise in what way the law of the
reward process is determined by its characteristics. We follow the notation of Jacod and Shiryaev
                                                                                    s
(2003, chapter IV). P is called a solution of the martingale problem (H, R | η; B, C, µ) if P is
a probability measure on the filtered space (Ω, F, F) on which R, B, C, µ are defined such that P
coincides with η on H ⊆ F0 and R is a càdlàg (F, P)-semimartingale with characteristics (B, C, µ).
Existence, uniqueness and local uniqueness of the martingale problem are defined as usual.
    We now define control processes as predictable processes on Skorokhod space D(R) and call
them admissible if a certain martingale problem associated to them is well-posed.18 Well-posedness
of the martingale problem is exactly what is needed to establish the equivalence to the separated
problem, as can be seen from the proof of lemma 2 (in Appendix A) where the filtering equation is
derived.

Definition 1 (Admissible control process). Let R be the coordinate process on Skorokhod space
(D(R), F, F). A predictable [0, 1]-valued process F on this space is called an admissible control
process if for all θ ∈ {0, 1} and h0 , r0 ∈ R, existence and local uniqueness holds for the martingale
           s
problem (F0 , R | δr0 ; B, C, µ) on D(R), where B, C, µ, H are defined via equations (1), (2), (3)
with Θ = θ, U = F , and H0 = h0 .

    The next definition states that a p.o. control is a probability space endowed with U, Θ, H, R as
in the previous section such that the control can be written as U = F (R) for an admissible control
process F . The conditions P0 = p0 and H0 = h0 are called the initial conditions of the control.

Definition 2 (Partially observable control). Let B = (Ω, F, F, P) be a stochastic basis endowed with
a {0, 1}-valued random variable Θ, a continuous process H and a càdlàg process R. Furthermore,
let F be an admissible control process and let U = F (R) be the composition of F with R. If P solves
                            s
the martingale problem (F0 , R | η; B, C, µ), where B, C, µ, H, η are defined via equations (1), (2),
(3) and
                     η(Θ = 1) = 1 − η(Θ = 0) = p0 ,          η(H0 = h0 ) = η(R0 = 0) = 1,                      (13)

then we call the tuple U = (B, U, Θ, H, R, p0 , h0 ) a partially observable (p.o.) control with initial
conditions (p0 , h0 ). We write Up.o.
                                 p0 ,h0 for the set of all such U.

    The set of p.o. controls is not empty. Indeed, any deterministic control process is admissible.
To see this, note that the characteristics (B, C, µ) are deterministic in this case. It is well-known
that this implies existence and local uniqueness of the martingale problem, see Jacod and Shiryaev
(2003, theorem III.2.16). We will now define the value of a p.o. control.


  18. Klein and Rady (2011) require a similar condition in a multi-agent setting and give it the interpretation that
the solution can be obtained as a discrete-time limit. This interpretation is consistent with our definition since
piecewise constant control processes are admissible. Friedman and Yavin (1980) and Kohlmann (1982) make continuity
assumptions on controls to guarantee the well-posedness of the associated martingale problem. However, these
stronger assumptions are not necessary.


                                                        14
Definition 3 (Value of p.o. control). The value of a p.o. control U ∈ Up.o.
                                                                       p0 ,h0 is

                                                             Z   ∞                
                                            p.o.                           −ρt
                                        J          (U) = E            ρe         dRt ,            (14)
                                                              0

where the expectation is taken with respect to P. The value function for the p.o. control problem is
                                                       n                   o
                                p.o.                     p.o.        p.o.
                            V          (p0 , h0 ) = sup J (U) : U ∈ Up0 ,h0 .                     (15)

   Note that it follows from the bounds in assumption 1 that any control has finite and well-defined
value, see also lemma 1 below.
   We end this section with a remark on the interpretation of [0, 1]-valued as opposed to {0, 1}-
valued controls. The interpretation of Ut as the fraction of time devoted to the unknown arm has
already been given in the discussion of time changes in section 3.2. This interpretation is possible
because of the linearity of the drift, volatility and jump measure in the control (see equations (2)).
Another interpretation of Ut is as a relaxed, i.e., measure valued or randomized, control. The
measure associated to Ut is Ut δ1 (du) + (1 − Ut )δ0 (du), where δ1 and δ0 are Dirac measures on
{0, 1}. In general, one has to work with relaxed controls to get good existence results for optimal
strategies. It turns out that in our model, the additional generality provided by relaxed controls is
not necessary. Indeed, we will prove the existence of optimal non-relaxed controls in theorem 2.

3.4   The separated (se.) control problem
The p.o. control problem is modified in several ways to solve it. The first step is to transform it
into a so-called separated (se.) problem, which is standard in control theory. To derive the se. from
the p.o. problem, the unobserved type Θ is replaced by its filter Pt . The filter is the conditional
distribution of Θ given the observations up to time t. Since Θ is {0, 1}-valued, Pt can be represented
as a real number Pt ∈ [0, 1]. In economic terms, Pt is the posterior belief of the agent in being of
the high type. In the se. problem, the agent controls the fully observed process (P, H, R). Under
constant controls, this process is Markov, see Kurtz (1998). Under some regularity assumptions
on the generator of (P, H, R), it can be shown that optimal Markov strategies for the se. problem
exist (see Kurtz and Stockbridge (1998, 1999)).
   The second step is to reduce the dimension of the state space by one. This is made possible by
the fact that the evolution of the reward process R depends on the values of Θ and H, but not on
the value of R itself. This can be seen from the characteristics of the reward process in equation (2).
Our model inherits this structure from classical bandit models where the evolution of the reward
process also does not depend on its current value. We will now explain how the state variable R
can be eliminated. The details of the argument can be found in the proof of lemma 2. First, note
that FR = FH,R because H is FU -adapted and U is FR -adapted. Taking FR -optional projections,
one obtains that the FR -characteristics of R can be expressed via (P, H), which is the FR -optional
projection of (Θ, H). Then also the characteristics of (P, H), which were originally expressed in


                                                             15
terms of the characteristics of (P, H, R), can be expressed in terms of (P, H). It remains to express
the value of a strategy using (P, H) instead of (P, H, R). This can be done by replacing R by its
FR -compensator.
   In the resulting optimization problem, the agent controls the Markov process (P, H). The
evolution of the human capital process H is the same as in the partially observable problem. The
evolution of the belief process P is described by the filtering or Kushner-Stratonovic equation
(Kushner 1967). This equation depends on the characteristics of the reward process via functions
φ1 and φ2 , that are defined by the relations
                                             Z
              2
                                                                                             
         σ(h) φ1 (h) = β(1, h) − β(0, h) −        φ2 (h, r) − 1 χ(r) K(1, h, dr) + K(0, h, dr) ,
                                              R
                                                                                                      (16)
                                 K(1, h, dr)
             φ2 (h, r) =                           .
                         K(1, h, dr) + K(0, h, dr) /2

The existence of such functions can be derived from Girsanov’s theorem (Jacod and Shiryaev 2003,
theorem III.3.11) applied to P1  12 (P1 + P0 ), where for θ ∈ {0, 1}, Pθ is the measure P conditioned
on Θ = θ. The function φ1 accounts for differences in the drift and φ2 for differences in the jumps
of the reward process for high and low type agents, respectively. These differences vanish if and
only if equation (16) can be satisfied with φ1 = 0 and φ2 = 1. The meaning of φ1 and φ2 is
most clearly seen in the compound Poisson case where the jump measures K(θ, h, dr) are finite
and the truncation function χ can be set to zero, which eliminates the integral term in equation
(16). Note that we allow the jump measures K(1, h, dr) and K(0, h, dr) to have both singular and
absolutely continuous parts with respect to each other. Consequently, our model encompasses both
the exponential bandit model of Keller, Rady, and Cripps (2005), where jumps can occur only for
high type agents, and the Poisson and Levy bandit models of Keller and Rady (2010, 2012) and
Cohen and Solan (2013), where it is assumed that one jump measure is absolutely continuous with
respect to the other. Finally, note that φ1 and φ2 are independent of the choice of the truncation
function χ, as can be seen from Jacod and Shiryaev (2003, II.2.24).
   In the following definition, the filtering equation is formulated in terms of the (F R , P)-characteristics
of P . For convenience, we introduce the following notation.

     β(p, h) = pβ(1, h) + (1 − p)β(0, h),     K(p, h, dr) = pK(1, h, dr) + (1 − p)K(0, h, dr).        (17)

Definition 4 (Separated control). Let B = (Ω, F, F, P) be a stochastic basis endowed with a pre-
dictable [0, 1]-valued process U , a càdlàg [0, 1]-valued process P and a continuous R-valued process H
such that H satisfies equation (1) and such that P is an (F, P)-semimartingale with characteristics
(B, C, µ) as in equation (3) with
         Z
                                            c = U P−2 (1−P− )2 φ1 (H)2 σ(H)2 ,
                   
b = −U       q−χ(q) (j∗ K)(P− , H, dq),                                          ν = U (j∗ K)(P− , H, dq).
         R
                                                                                                      (18)



                                                    16
Here, the measure j∗ K is defined by equation (17) and
                               Z                              Z
                                                                                       1j(p,h,q)6=0 K(p, h, dq),
                                                                                   
             ∀g ∈ Cb (R) :           g(q)(j∗ K)(p, h, dq) =         g j(p, h, q)                                   (19)
                                 R                              R
                                                           pφ2 (h, q)
                               j(p, h, q) =                                      − p.                             (20)
                                              pφ2 (h, q) + (1 − p) 2 − φ2 (h, q)

If furthermore the initial conditions

                                   P(P0 = p0 ) = P(H0 = h0 ) = P(R0 = 0) = 1                                       (21)

hold, then we call the tuple U = (B, U, P, H, p0 , h0 ) a separated control with initial conditions (p0 , h0 )
and we write Use.
              p0 ,h0 for the set of all such U.

   Note that (18) implies that P is a local martingale, see Jacod and Shiryaev (2003, proposition
II.2.29). It is also bounded, so it is a martingale. Notice how the volatility and jumps of P go
to zero as P approaches the boundary of [0, 1]. Therefore the condition that P is [0, 1]-valued is
satisfied automatically, at least under some additional regularity assumptions on the coefficients
of the generator, see e.g. Simon (2000); Buckdahn et al. (2010); Filipovic, Tappe, and Teichmann
(2012).

Definition 5. The value of a se. control U ∈ Use.
                                              p0 ,h0 is

                  Z    ∞                                          Z                                    !
                             −ρt
  J se. (U) = E
                                                                                      
                           ρe     β(Pt− , Ht )Ut + k(1 − Ut ) + Ut            r − χ(r) K(Pt− , Ht , dr) dt , (22)
                   0


where the expectation is taken with respect to the measure P. The value function for the separated
control problem is
                                     V se. (p0 , h0 ) = sup J se. (U) : U ∈ Use.
                                                           
                                                                             p0 ,h0 .                              (23)

3.5    Equivalence of the p.o. and se. problem
In this section, we will prove the equivalence between the p.o. and the se. problem. This will be
our first result. It is based on a number of assumptions that we state next.
   To transform p.o. controls into se. controls of the same value, we will construct a local martin-
gale and show via Girsanov’s theorem that its stochastic exponential is equal to the belief process
Pt = E(Θ|FtR ). Then it is not hard to calculate the characteristics of P and to verify that P
solves the filtering equation, which yields the desired se. control. To prove that the stochastic
exponential agrees with the belief process, it is necessary to establish the uniform integrability of
the stochastic exponential. Sufficient conditions for this are well-known in the literature (Novikov
1980; Cheridito, Filipović, and Yor 2005; Kazamaki 1977, 1978, 1994; Cherny and Shiryaev 2000;
Mémin 1978; Karatzas and Kardaras 2007; Protter and Shimbo 2008). We use a condition that is
based on Lépingle and Mémin (1978, Théorème IV.3).


                                                           17
Assumption 2 (Novikov-style condition). There exist functions φ1 , φ2 satisfying (16) such that
the expression
                                             Z                                               
                     1
                                                          q
               Φ(h) = φ1 (h)2 σ(h)2 +
                                                                                          
                                                    1−         φ2 (h, r) 2 − φ2 (h, r)            K(1/2, h, dr)    (24)
                     8                          R

is locally bounded in h.

   Note that the expression in (24) remains invariant under interchanges of the measures P1 and P0
governing the reward process for high and low type agents. In such an interchange, φ1 is replaced
by −φ1 and φ2 by 2 − φ2 . The interpretation of equation (24) is most obvious when the jump
measures K(1, h, ·) and K(0, h, ·) are finite, such that the truncation function χ can effectively be
set to zero, and when K(1, h, ·) is absolutely continuous with respect to K(0, h, ·). In this case,

                                                   2                       s                 !2
                              β(1, h) − β(0, h)
                                                               Z
                      1                                    1                     K(1, h, dr)
               Φ(h) =                                    +              1−                          K(0, h, dr).   (25)
                      8             σ(h)                   2    R                K(0, h, dr)

Thus Φ captures differences in the drift and jumps of the reward process for high versus low type
agents. Φ is also closely tied to the Hellinger process of the measures P1 and P0 , as explained in
section 3.7.
   The following assumptions are used to approximate se. controls by step controls. This amounts
to a discretization of the control problem in time (but not in space). The motivation for this
approximation is that for se. step controls, it is possible to construct p.o. step controls of the same
value through a recursive procedure.

Assumption 3 (Well-posedness of the filtering equation under deterministic control). For any de-
terministic process U and corresponding human capital process H satisfying equation (1), existence
                                                                         s
and local uniqueness holds for the martingale problem (F0 , P | η; B, C, µ) under any initial condi-
tion η. Here, P is the coordinate process on D(R) and (B, C, µ) satisfy equation (3) with (b, c, ν)
as in equation (18). Furthermore, the process P is a.s. [0, 1]-valued under the solution measure.

Assumption 4 (Continuity of the coefficients). The expressions
                                                               Z
                                     β(θ, h),       σ(h),              g(r)K(θ, h, dr)                             (26)
                                                                   R

are continuous in h for all θ ∈ {0, 1} and all functions g ∈ Cb (R) vanishing near the origin.

Theorem 1 (Separation theorem). Let assumptions 1–4 hold. Then the value functions of the
separated and the partially observed problem are finite and agree:

                               V (p0 , h0 ) := V p.o. (p0 , h0 ) = V se. (p0 , h0 ) < ∞.                           (27)

   To establish the theorem, we use step controls that we define next. These are controls that are
constant on subsequent time intervals of a fixed length δ > 0.

                                                            18
Definition 6 (Step controls). A step process with step size δ > 0 is a (càdlàg or càglàd) process
that is constant on all intervals (ti , ti+1 ), where ti = δi for i ∈ N. Up.o.,δ
                                                                          p0 ,h0 is the set of p.o. controls
with initial condition (p0 , h0 ) whose control process U is a {0, 1}-valued step process with step size
δ. The value function corresponding to this class of controls is denoted by V p.o.,δ (p0 , h0 ). Use.,δ
                                                                                                  p0 ,h0
and V se.,δ (p0 , h0 ) are defined correspondingly.

   The separation theorem follows from a sequence of lemmas that can be found in the appendix.
We now give a verbal proof of the theorem, highlighting the role that each individual lemma plays.
Proof of theorem 1. Step 1. All value functions are well-defined and finite by lemma 1.
   Step 2. For every p.o. control, one can define a process P = E(Θ|FtR ) as the conditional
expectation of the type Θ given the past observations. It is shown in lemma 2 that P satisfies the
filtering equation (18) in the definition of se. controls. It follows that every p.o. control can be
interpreted as a se. control. Moreover, the p.o. and se. control have the same value. By taking
the supremum over all controls or step controls, one obtains that

                       V p.o. (p0 , h0 ) ≤ V se. (p0 , h0 ),   V p.o.,δ (p0 , h0 ) ≤ V se.,δ (p0 , h0 ).               (28)

   Step 3. It remains to construct for every se. control a p.o. control of at least the same value.
By a standard argument in lemma 4, se. controls can be approximated arbitrarily well by se. step
controls. Formally, this is expressed by the equation

                                          sup V se.,δ (p0 , h0 ) = V se. (p0 , h0 ).                                   (29)
                                            δ

Thus it is sufficient to show that every se. step control corresponds to a p.o. control of at least the
same value. This is done in lemma 3. In this lemma, the p.o. control is constructed recursively
for each step of the control process by stitching together solution measures to the p.o. martingale
problem under constant control. This establishes the relation

                                           V se.,δ (p0 , h0 ) ≤ V p.o.,δ (p0 , h0 )                                    (30)

Together with the results of the previous step this immediately implies that equality holds in (30).
By allowing arbitrarily small step sizes δ one obtains that the se. and p.o. value functions are finite
and agree:

         V p.o. (p0 , h0 ) ≤ V se. (p0 , h0 ) = sup V se.,δ (p0 , h0 ) ≤ sup V p.o.,δ (p0 , h0 ) ≤ V p.o. (p0 , h0 )   (31)
                                                  δ                         δ




3.6    Equivalence to optimal stopping
The next step is to reduce the stochastic control problem to an equivalent stopping problem. That
is, there are optimal stopping controls as defined below.


                                                               19
Definition 7. A control U is called a stopping control if the associated control process U is of the
form Ut = 1J0,T K (t) for a stopping time T .

   The result hinges on the monotonicity condition that the expected rewards of the unknown arm
increase while the arm is operated and decrease otherwise. Since the amount of past investment in
the unknown arm is represented by the level of human capital, this is equivalent to assuming that
the average reward from investments in the unknown arm is a non-decreasing function of the level
of human capital. The infinitesimal version of this assumption can be stated as follows:

Assumption 5 (Monotonicity condition). The relation

                                            α(0, h) ≤ 0 ≤ α(1, h)                                        (32)

holds for all h ∈ R and the expression
                                                    Z
                                                                 
                                    β(θ, h) +            r − χ(r) K(θ, h, dr)                            (33)
                                                     R

is non-decreasing in θ, h ∈ {0, 1} × R.

   We are now ready to state our main theorem. Recall that k is the reward from the known arm
and V = V p.o. = V se. (see theorem 1).

Theorem 2 (Optimal stopping). Let assumptions 1–5 hold. Then the following stopping times T ∗
are optimal.

  1. T ∗ = inf{t ≥ 0 : V (Pt , Ht ) ≤ k}.

  2. T ∗ = inf{t ≥ 0 : G(Pt , Ht ) ≤ k}, where G is defined by
                                                                                        R          
                                                                                           T
                                     Z        T                                   E 0 ρe−ρt dRt
         G(p0 , h0 ) := inf s : sup E               ρe−ρt (dRt − sdt)       ≤ 0 = sup    R         .   (34)
                                                                                            T
                                T           0                                      T   E 0 ρe−ρt dt


The suprema in the formula above are over all FP,H -stopping times T and the processes P , H, and
R are governed by the constant strategy Ut = 1 for all t.

   G formally coincides with Gittins’ index. However, the payoff distribution of inactive arms can
evolve. This is not allowed in classical bandit models, which are the object of Gittins’ (1979) theory.
The first formula in equation (34) is a continuous time version of Weber’s (1992) modification of
Whittle (1980). The second formula is the continuous time version of the original formulation of the
index by Gittins and Jones (1974). Some references for the continuous time setting are El Karoui
and Karatzas (1994, 1997) and Bank and Küchler (2007).
   An immediate consequence of theorem 2 is a characterization of optimal strategies by a curve
that is typically referred to as the decision frontier.

                                                           20
Corollary 1. There is a curve in the (p, h)-domain such that it is optimal to invest in the unknown
arm if (Pt , Ht ) lies to the right and above of the curve. Otherwise, it is optimal to invest in the
known arm.

Proof. The value function V is non-decreasing in its arguments by lemma 5 in the appendix
and bounded from below by the constant k. The desired curve is the boundary of the domain
{(p, h) : V (p, h) > k}. The characterization of optimal strategies via the position of (Pt , Ht )
relative to the curve follows from theorem 2.
   Another consequence is the indexability of our bandit model in the sense of Whittle (1988).
This means that the set of states in the (p, h)-domain where the known arm is optimal increases in
the payoff k of the known arm. This property is obvious from equation (34).

Corollary 2. Our restless bandit model is indexable in the sense of Whittle (1988). More generally,
in a multi-armed bandit model, this holds for any arm that satisfies these assumptions.

   Theorem 2 follows from a sequence of lemmas. Its proof is short and can serve as a guide to
how the lemmas are used. The core of the proof is lemma 7 where the optimality of stopping rules
is shown. It is a generalization of an argument originally developed by Berry and Fristedt (1985,
section 5.2) for classical bandits in discrete time. It turns out that our monotonicity assumption is
exactly what is needed to make the proof work for restless bandits. Once it has been established
that the problem is equivalent to optimal stopping, the characterization of optimal stopping times
via the value function and via Gittins’ index follow easily.
Proof of theorem 2.       We make repeated use of the monotonicity assumption 5. A first conse-
quence of this assumption is the monotonicity in (p0 , h0 ) and convexity in p0 of the value function.
These properties are established in lemma 5. The result is then used in lemma 6 to prove a sufficient
condition for the unique optimality of the unknown arm as an initial choice. Namely, when the
expected immediate (myopic) payoff is higher for the unknown than for the known arm, then it is
uniquely optimal to choose the unknown arm. This result is used in lemma 7 to prove that there
exist optimal stopping rules for the discretized control problem. The argument is a modification of
Berry and Fristedt (1985, theorem 5.2.2) that allows the reward to depend on the level of human
capital. Since it is already know from lemma 4 that controls can be approximated arbitrarily well
by step controls, it follows that the set of p.o. and se. controls can be restricted to stopping
controls without incurring any loss of value. Thus it has been established in the continuous time
setting that the value function is the supremum over the values of stopping controls. However, it
remains to show that optimal stopping controls exist. This is well-known for controlled Markov
processes, so the result is established with ease in lemma 8 for the se. problem. By lemma 9, an
optimal stopping rule for the se. problem yields also an optimal stopping rule for the p.o. problem.
The characterization of optimal stopping controls as in theorem 2.1 is immediate from the proof of
lemma 8. The equivalence to the formulas in theorem 2.2 is well-known, see e.g. Morimoto (1991,
theorem 2.1) or El Karoui and Karatzas (1994, proposition 3.4).



                                                 21
3.7   Asymptotic learning
In this section, we calculate the limits of the belief process and formulate conditions for asymptotic
learning, which is defined as convergence of the belief process to the true type. The result builds
upon the theory of Hellinger processes as a means to characterize the mutual singularity or absolute
continuity of measures. In the following, for θ ∈ {0, 1} and 0 < p0 < 1, let Pθ denote the measure
P conditioned on Θ = θ.
   Agents can learn their true type in two ways: either through a jump of the belief process to
Θ, or through convergence to Θ without a jump to the limit. Jumps of the belief process to zero
or one are possible only when the jump measures K(1, h, ·) and K(0, h, ·) are not equivalent. For
example, this is the case in the exponential bandits model of Keller, Rady, and Cripps (2005).
However, when the jump measures are equivalent, then learning can occur only gradually. This
kind of learning is characterized by the divergence of the Hellinger process h( 12 ) of the measures P1
and P0 . This process is closely related to the function Φ, which was defined in assumption 2 and can
be interpreted as the informativeness of the reward process about the type Θ. In fact, assumption
2 provides an upper bound on Φ and consequently on the Hellinger process. When there is also a
lower bound on Φ, then gradual convergence (without jumps to Θ) of the belief process to the true
type is equivalent to the amount of investment in the unknown arm going to infinity. This is the
content of the following theorem.

Theorem 3 (Asymptotic learning). Let assumptions 1 and 2 hold and let the initial belief be non-
doctrinaire in the sense that 0 < p0 < 1. Then the following statements hold a.s. under any
control.

 (a) Let the measures K(1, h, ·) and K(0, h, ·) be equivalent for all h. Then learning in finite time
      is impossible, i.e., 0 < Pt < 1 holds for all t ≥ 0. Moreover, asymptotic learning does not
      occur if the agent invests only a finite amount into the unknown arm, i.e.,
                                     R∞
                                 {   0       Ut dt < ∞} ⊆ {0 < P∞ < 1}    P-a.s.                  (35)


 (b) Let there be a lower bound on the informativeness of the reward process in the sense that
      inf h Φ(h) > 0 holds, where Φ(h) is defined in equation (24). Then asymptotic learning is
      guaranteed if the agent invests an infinite amount in the unknown arm, i.e.,
                                         R∞
                                     {   0    Ut dt = ∞} ⊆ {P∞ = Θ}      P-a.s.                   (36)


 (c) If the conditions of (a) and (b) are satisfied, then asymptotic learning occurs if and only if
      the agent invests an infinite amount in the unknown arm:
                                         R∞
                                     {   0    Ut dt = ∞} = {P∞ = Θ}      P-a.s.                   (37)


   The theorem is proven in the appendix. Note that theorem 3 holds under any strategy, not just

                                                       22
under the optimal one. However, it can be combined with theorem 2 to the following statement
about asymptotic learning under optimal strategies: for any agent, asymptotic learning fails with
positive probability. However, it takes place with positive probability for high type agents starting
out above the frontier.19

3.8    Trajectories in the belief–human capital space
Let us restrict our attention to a range (hmin , hmax ) of human capital that is invariant under the
evolution of human capital. This means that the process Ht never leaves the interval (hmin , hmax )
when H0 lies in this interval. We assume that conditions (a) and (b) of theorem 3 hold, which
implies that learning occurs only gradually as agents invest more and more into the unknown arm.
To exclude trivial cases, we also require the initial belief to be non-doctrinaire in the sense that
0 < p0 < 1 holds. We set out to describe the possible trajectories of an agent in belief–human
capital space (0, 1) × (hmin , hmax ) and to explore the importance of dynamic human capital in that
context.
    The results from the previous section together with the characterization of optimal strategies
in terms of the decision frontier imply that agents meet one of two fates: they either remain above
the frontier, in which case they learn their true type in the limit. Or they fall below the frontier at
some point, in which case they cannot learn their true type. A first consequence of this observation
is that in the long run, only high type agents are susceptible to making suboptimal investment
decisions. Namely, they might drop down below the frontier because of bad luck and stop learning
about their type. In contrast, all low type agents eventually choose the option they would also
choose if they knew their type. (Depending on the parameters of the model and on the level of
human capital of the agent, this might mean investing or not investing.) It follows that in the long
run, compared to a setting with full information, agents invest too little in the unknown arm. This
points to the importance of policies designed to increase investment in the unknown arm.
    The effect of dynamic as opposed to static human capital on optimal investment is best seen
by looking at the limit (P∞ , H∞ ) of the belief–human capital process as time goes to infinity. This
limit always exists as an element of [0, 1] × [hmin , hmax ] because P is a bounded martingale and H
is an integral curve to a vector field (at least after a stopping time where the agent might change
her investment strategy).
    In the case where human capital is dynamic in the strict sense that α(0, h) < 0 < α(1, h) holds
for h ∈ (hmin , hmax ), agents never converge to the frontier. Instead, their human capital converges
either to hmin or hmax , depending on whether they stop investing in the unknown arm at some
point in time or not. However, in the static case where α(1, h) = α(0, h) = 0 holds and human
capital is constant, agents above the frontier hit the frontier with positive probability and remain
there forever.


  19. This can be compared to the linear network structure in Acemoglu et al. (2011, example 1.1) where asymptotic
learning is guaranteed. This is because agents receive information at each stage regardless of whether they invest or
not.


                                                         23
    This consideration suggests that agents in the static human capital case accumulate at the
frontier. This statement can be given a precise meaning. Assume that there is a population of
agents whose initial belief and human capital is uniformly distributed. Moreover, assume that
agents have independent types such that learning from others is impossible. Alternatively, learning
could be precluded by making actions and rewards private information. Then all agents behave as
in the single player case. The distribution of the agents in the belief–human capital domain evolves
over time and converges to the distribution of (P∞ , H∞ ). Figures 1 and 2 depict the decomposition
of this distribution into absolutely continuous and singular parts with respect to the Lebesgue
measure. Singular parts are highlighted. They correspond to areas of vanishing Lebesgue measure
where nevertheless, there is a positive fraction of agents in the long term limit. Thus highlighted
areas in the graphs can be interpreted as accumulation points of agents. Notice that the frontier
is highlighted in exactly the cases where human capital is constant. In all other cases, it is not
highlighted and in fact, never occurs as a limit.
    Figures 1 and 2 differ by the position of the frontier relative to the boundary of the belief–human
capital domain. In figure 1, human capital can be so high (low) that the unknown (known) arm
is optimal for all agents, regardless of their type. In other words, human capital has a stronger
influence than the type. By contrast, in figure 2, the type has a stronger influence than human
capital, which makes the unknown (known) arm optimal for all high (low) type agents, regardless
of their level of human capital. The dominance of human capital or the type is determined by the
parameters of the model.


4    Conclusion
We discovered a class of restless bandit models of investment under uncertainty where payoffs are
allowed to depend on the history of past investments in a monotonic way. We argue that this
dynamic dependence is a defining feature of many economically important activities such as human
capital formation or job search. Agents in our model have imperfect information and learn through
observations of the reward process, which we allow to be a general semimartingale. We solve
the model by showing that stopping rules are optimal and can be characterized by an index that
formally coincides with Gittins’ index. Moreover, we characterize the learning process by giving
necessary and sufficient conditions for asymptotic learning.
    Allowing arms in a bandit to evolve as in our model results in a stark empirical prediction –
there are very few truly “marginal agents” (in the classic sense). Instead, once agents stop investing
they drift to the boundary. In this case, optimal policies to foster human capital or unemployment
policies designed to keep individuals in the workforce may have very different characteristics than
optimal policy in standard life-cycle human capital or job search models. For instance, one could
imagine that if policies can be targeted to individuals, optimal policy might wait until agents are
close to the decision frontier and then subsidize investment in the risky arm if the probability that
they are high type is large enough. Conversely, if one has to make lumpy transfers (e.g. invest in


                                                    24
a community or a school) then optimal policy may be more complicated. With a new indexable
class of restless bandit models, there are many potential avenues of future research.




                                                25
    hmax                                          hmax




        h                                            h




    hmin                                          hmin
            0             p              1               0              p              1
                 (a) Ht static, Θ = 1                          (b) Ht static, Θ = 0

    hmax                                          hmax




        h                                            h




    hmin                                          hmin
            0             p              1               0              p              1
                (c) Ht dynamic, Θ = 1                         (d) Ht dynamic, Θ = 0


Figure 1: Absolutely continuous (opaque areas) and singular (bold lines and dots) parts of the
distribution of (P∞ , H∞ ) when (P0 , H0 ) is uniformly distributed. The dashed line is the decision
frontier. It intersects the left and right boundary of the (p, h)-domain because the parameters are
such that the effect of human capital dominates the effect of Θ. Note that agents accumulate at
the frontier when Ht is constant but move away from it when Ht is dynamic.




                                                26
    hmax                                          hmax




        h                                            h




    hmin                                          hmin
            0             p              1               0              p              1
                 (a) Ht static, Θ = 1                          (b) Ht static, Θ = 0

    hmax                                          hmax




        h                                            h




    hmin                                          hmin
            0             p              1               0              p              1
                (c) Ht dynamic, Θ = 1                         (d) Ht dynamic, Θ = 0


Figure 2: Absolutely continuous (opaque areas) and singular (bold lines and dots) parts of the
distribution of (P∞ , H∞ ) when (P0 , H0 ) is uniformly distributed. The dashed line is the decision
frontier. It intersects the left and right boundary of the (p, h)-domain because the parameters are
such that the effect of Θ dominates the effect of human capital. Note that agents accumulate at
the frontier when Ht is constant but move away from it when Ht is dynamic.




                                                27
References
Acemoglu, Daron, Munther Dahleh, Ilan Lobel, and Asuman Ozdaglar. 2011. “Bayesian Learning
    in Social Networks.” The Review of Economic Studies 78, no. 4 (October): 1201.

Auer, Peter, Nicolò Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. 2002/03. “The nonstochas-
    tic multiarmed bandit problem.” SIAM J. Comput. 32 (1): 48–77 (electronic).

Bank, P., and C. Küchler. 2007. “On Gittins’ index theorem in continuous time.” Stochastic pro-
    cesses and their applications 117 (9): 1357–1371.

Banks, Jeffrey S., and Rangarajan K. Sundaram. 1992. “Denumerable-Armed Bandits.” Economet-
    rica 60, no. 5 (September): 1071–1096.

      . 1994. “Switching Costs and the Gittins Index.” Econometrica 62, no. 3 (May): 687–694.

Basu, A., A. Bose, and JK Ghosh. 1990. An Expository Review of Sequential Design and Allocation
    Rules. Technical report. Department of Statistics, Purdue University.

Bergemann, Dirk, and Juuso Välimäki. 2006. Bandit Problems. 1551. Cowles Foundation Discussion
    Papers. Cowles Foundation for Research in Economics, Yale University, January.

Berry, Donald A., and Bert Fristedt. 1985. Bandit problems : sequential allocation of experiments.
    Monographs on statistics and applied probability. London; New York: Chapman / Hall.

Bolton, Patrick, and Christopher Harris. 1999. “Strategic experimentation.” Econometrica 67 (2):
    349–374.

      . 2000. “Strategic Experimentation: The Undiscounted Case.” In Incentives, Organization
    and Public Economics. Papers in Honour of Sir James Mirrlees, edited by Peter J. Hammond
    and Gareth D. Myles, 53–68. Oxford / New York: Oxford University Press.

Buckdahn, R., M. Quincampoix, C. Rainer, and J. Teichmann. 2010. “Another proof for the equiv-
    alence between invariance of closed sets with respect to stochastic and deterministic systems.”
    Bulletin des sciences mathematiques 134 (2): 207–214.

Ceci, Claudia, and Anna Gerardi. 1998. “Partially observed control of a Markov jump process with
    counting observations: equivalence with the separated problem.” Stochastic Process. Appl. 78
    (2): 245–260.

Cheridito, Patrick, Damir Filipović, and Marc Yor. 2005. “Equivalent and absolutely continuous
    measure changes for jump-diffusion processes.” Ann. Appl. Probab. 15 (3): 1713–1732.

Chernoff, Herman, and S. N. Ray. 1965. “A Bayes sequential sampling inspection plan.” Ann. Math.
    Statist. 36:1387–1407.

Cherny, AS, and AN Shiryaev. 2000. On criteria for the uniform integrability of Brownian stochastic
    exponentials. Centre for Mathematical Physics / Stochastics, University of Aarhus.


                                                28
Cohen, A., and E. Solan. 2013. “Bandit problems with Levy payoff processes.” Mathematics of
    Operations Research 38, no. 1 (February): 92–107.

Cunha, Flavio, and James J. Heckman. 2010. Investing in Our Young People. 16201. Working
    Paper. National Bureau of Economic Research, July. http://www.nber.org/papers/w16201.

Cunha, Flavio, James J Heckman, Lance Lochner, and Dimitriy V Masterov. 2006. “Interpreting
    the evidence on life cycle skill formation.” Handbook of the Economics of Education 1:697–812.

Dobbie, Will, and Roland Fryer. 2011. Getting beneath the veil of effective schools: Evidence from
    New York City. Technical report. National Bureau of Economic Research.

El Karoui, N., and I. Karatzas. 1994. “Dynamic allocation problems in continuous time.” Ann.
    Appl. Probab. 4 (2): 255–286.

       . 1997. “Synchronization and optimality for multi-armed bandit problems in continuous
    time.” Computational and Applied Mathematics 16:117–152.

Faihe, Y., and J.P. Müller. 1998. “Behaviors coordination using restless bandits allocation indexes.”
    In From Animals to Animats 5 (Proc. 5th Int. Conf. Simulation of Adaptive Behavior).

Filipovic, D., S. Tappe, and J. Teichmann. 2012. “Invariant manifolds with boundary for jump-
    diffusions.” ArXiv e-prints (February). arXiv: 1202.1076.

Fleming, Wendell H. 1989. “Generalized solutions and convex duality in optimal control.” In Par-
    tial differential equations and the calculus of variations, 461–471. Vol. 1. Progr. Nonlinear
    Differential Equations Appl. Birkhäuser Boston.

Fleming, Wendell H., and Makiko Nisio. 1966. “On the existence of optimal stochastic controls.”
    J. Math. Mech. 15:777–794.

Fleming, Wendell H., and Étienne Pardoux. 1982. “Optimal control for partially observed diffu-
    sions.” SIAM J. Control Optim. 20 (2): 261–285.

Friedman, M., and Y. Yavin. 1980. “Optimal control of partially observable jump diffusion pro-
    cesses.” Internat. J. Systems Sci. 11 (3): 323–335.

Gittins, J. C., and D. M. Jones. 1974. “A dynamic allocation index for the sequential design of
    experiments.” In Progress in statistics (European Meeting Statisticians, Budapest, 1972), 241–
    266. Colloq. Math. Soc. János Bolyai, Vol. 9. Amsterdam: North-Holland.

Gittins, J.C. 1979. “Bandit processes and dynamic allocation indices.” Journal of the Royal Statis-
    tical Society. Series B (Methodological):148–177.

Gittins, John, Kevin Glazebrook, and Richard Weber. 2011. Multi-armed Bandit Allocation Indices.
    Wiley-Blackwell.




                                                 29
Glazebrook, KD, C. Kirkbride, and D. Ruiz-Hernandez. 2006. “Spinning plates and squad systems:
    policies for bi-directional restless bandits.” Advances in applied probability 38 (1): 95–115.

Glazebrook, KD, J. Nino-Mora, and PS Ansell. 2002. “Index policies for a class of discounted
    restless bandits.” Advances in Applied Probability 34 (4): 754–774.

Glazebrook, KD, D. Ruiz-Hernandez, and C. Kirkbride. 2006. “Some indexable families of restless
    bandit problems.” Advances in Applied Probability 38 (3): 643–672.

Jacod, Jean, and Albert N. Shiryaev. 2003. Limit theorems for stochastic processes. Second. Vol. 288.
    Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical
    Sciences]. Berlin: Springer-Verlag.

Jun, Tackseung. 2004. “A survey on the bandit problem with switching costs.” De Economist 152
    (4): 513–541.

Kallsen, J., and A.N. Shiryaev. 2002. “Time Change Representation Of Stochastic Integrals.” The-
    ory Probab. Appl. 46 (3): 522.

Karatzas, I. 1984. “Gittins indices in the dynamic allocation problem for diffusion processes.” The
    Annals of Probability:173–192.

Karatzas, Ioannis, and Constantinos Kardaras. 2007. “The numéraire portfolio in semimartingale
    financial models.” Finance Stoch. 11 (4): 447–493.

Kazamaki, N. 1977. “On a problem of Girsanov.” Tôhoku Math. J. 29 (4): 597–600.

       . 1978. “Correction: “On a problem of Girsanov” (Tôhoku Math. J. (2) 29 (1977), no. 4,
    597–600).” Tôhoku Math. J. (2) 30 (1): 175.

       . 1994. Continuous exponential martingales and BMO. viii+91. Vol. 1579. Lecture Notes in
    Mathematics. Berlin: Springer-Verlag.

Keller, Godfrey, and Sven Rady. 2010. “Strategic experimentation with poisson bandits.” Theoret-
    ical Economics 5, no. 2 (May): 275–311.

       . 2012. Breakdowns. http://epub.ub.uni-muenchen.de/14316/.

Keller, Godfrey, Sven Rady, and Martin Cripps. 2005. “Strategic experimentation with exponential
    bandits.” Econometrica 73 (1): 39–68.

Klein, Nicolas, and Sven Rady. 2011. “Negatively correlated bandits.” The Review of Economic
    Studies 78 (2): 693–732.

Klimov, GP. 1975. “Time-sharing service systems. I.” Theory of Probability & Its Applications 19
    (3): 532–551.

Kohlmann, M. 1982. “Existence of optimal controls for a partially observed semimartingale.”
    Stochastic Processes and their Applications 13 (2): 215–226.

                                                 30
Kolokoltsov, Vassili N. 2010. Nonlinear Markov processes and kinetic equations. xviii+375. Vol. 182.
    Cambridge Tracts in Mathematics. Cambridge: Cambridge University Press.

Kurtz, Thomas G. 1998. “Martingale problems for conditional distributions of Markov processes.”
    Electron. J. Probab. 3 (9): 1–29.

Kurtz, Thomas G., and Daniel Ocone. 1985. “A martingale problem for conditional distribu-
    tions and uniqueness for the nonlinear filtering equations.” In Stochastic differential systems
    (Marseille-Luminy, 1984), 224–234. Vol. 69. Lecture Notes in Control and Inform. Sci. Berlin:
    Springer.

Kurtz, Thomas G., and Richard H. Stockbridge. 1998. “Existence of Markov controls and charac-
    terization of optimal Markov controls.” SIAM J. Control Optim. 36 (2): 609–653 (electronic).

      . 1999. “Erratum: “Existence of Markov controls and characterization of optimal Markov
    controls”.” SIAM J. Control Optim. 37 (4): 1310–1311 (electronic).

Kushner, H. J. 1967. “Dynamical equations for optimal nonlinear filtering.” J. Differential Equa-
    tions 3:179–190.

Kushner, H.J., and P.G. Dupuis. 2000. Numerical methods for stochastic control problems in con-
    tinuous time. Vol. 24. Springer.

La Scala, BF, and B. Moran. 2006. “Optimal target tracking with restless bandits.” Digital Signal
    Processing 16 (5): 479–487.

Lépingle, Dominique, and Jean Mémin. 1978. “Sur l’intégrabilité uniforme des martingales expo-
    nentielles.” Z. Wahrsch. Verw. Gebiete 42 (3): 175–203.

Li, Chih-ping, and Michael J Neely. 2012. “Network utility maximization over partially observable
    markovian channels.” Performance Evaluation.

Mahajan, A., and D. Teneketzis. 2008. “Multi-armed bandit problems.” Foundations and Applica-
    tions of Sensor Management:121–151.

Mandelbaum, Avi. 1987. “Continuous multi-armed bandits and multiparameter processes.” Ann.
    Probab. 15 (4): 1527–1556.

Mazliak, Laurent. 1993. “Mixed control problem under partial observation.” Appl. Math. Optim.
    27 (1): 57–84.

McCall, B. P., and J. J. McCall. 1987. “A Sequential Study of Migration and Job Search.” Journal
    of Labor Economics 5 (4): 452–476.

Mémin, Jean. 1978. “Décompositions multiplicatives de semimartingales exponentielles et appli-
    cations.” In Séminaire de Probabilités, XII (Univ. Strasbourg, Strasbourg, 1976/1977), 35–46.
    Vol. 649. Lecture Notes in Math. Berlin: Springer.



                                                31
Morimoto, Hiroaki. 1991. “On average cost stopping time problems.” Probab. Theory Related Fields
    90 (4): 469–490.

Nino-Mora, J. 2001. “Restless bandits, partial conservation laws and indexability.” Advances in
    Applied Probability 33 (1): 76–98.

Novikov, A. A. 1980. “On conditions for uniform integrability for continuous exponential martin-
    gales.” In Stochastic differential systems (Proc. IFIP-WG 7/1 Working Conf., Vilnius, 1978),
    304–310. Vol. 25. Lecture Notes in Control and Information Sci. Berlin: Springer.

Ny, Jerome Le, Munther Dahleh, and Eric Feron. 2008. “Multi-UAV dynamic routing with partial
    observations using restless bandit allocation indices.” In American Control Conference, 2008,
    4220–4225. IEEE.

Pandey, Sandeep, Deepayan Chakrabarti, and Deepak Agarwal. 2007. “Multi-armed bandit prob-
    lems with dependent arms.” In Proceedings of the 24th international conference on Machine
    learning, 721–728. ACM.

Peskir, Goran, and Albert Shiryaev. 2006. Optimal stopping and free-boundary problems. Lectures
    in Mathematics ETH Zürich. Birkhäuser Verlag.

Pham, Huyên. 1995. “Optimal stopping of controlled jump diffusion processes and viscosity solu-
    tions.” C. R. Acad. Sci. Paris Sér. I Math. 320 (9): 1113–1118.

      . 1998. “Optimal stopping of controlled jump diffusion processes: a viscosity solution ap-
    proach.” J. Math. Systems Estim. Control 8 (1).

Powell, W.B. 2007. Approximate Dynamic Programming: Solving the curses of dimensionality.
    Vol. 703. Wiley-Interscience.

Presman, È. L., and I. N. Sonin. 1990. Sequential control with incomplete information. Economic
    Theory, Econometrics, and Mathematical Economics. San Diego, CA: Academic Press Inc.

Presman, Ernst L. 1990. “Poisson version of the two-armed bandit problem with discounting.”
    Theory of Probability & Its Applications 35 (2): 307–317.

Protter, Philip, and Kazuhiro Shimbo. 2008. “No arbitrage and general semimartingales.” Markov
    processes and related topics: A Festschrift for Thomas G. Kurtz. Beachwood, OH. Institute of
    Mathematical Statistics Collections 4:267–283.

Robbins, Herbert. 1952. “Some aspects of the sequential design of experiments.” Bull. Am. Math.
    Soc. 58:527–535.

Rothschild, Michael. 1974. “A two-armed bandit theory of market pricing.” J. Econom. Theory 9
    (2): 185–202.




                                               32
Schachermayer, W., and W. Schachinger. 1999. “Is there a predictable criterion for mutual singu-
    larity of two probability measures on a filtered space?” Teor. Veroyatnost. i Primenen. 44 (1):
    101–110.

Seierstad, Atle. 2009. Stochastic control in discrete and continuous time. New York: Springer.

Simon, Thomas. 2000. “Support theorem for jump processes.” Stochastic Process. Appl. 89 (1):
    1–30.

Stockbridge, Richard H. 2005. “A separation principle for partially observed control of singular
    stochastic processes.” Nonlinear Analysis 63:e2057–e2065.

Stroock, Daniel W., and S. R. Srinivasa Varadhan. 2006. Multidimensional diffusion processes.
    xii+338. Classics in Mathematics. Reprint of the 1997 edition. Berlin: Springer-Verlag.

Veatch, Michael H, and Lawrence M Wein. 1996. “Scheduling a make-to-stock queue: Index policies
    and hedging points.” Operations Research 44 (4): 634–647.

Washburn, R. 2008. “Application of multi-armed bandits to sensor management.” Foundations and
    Applications of Sensor Management:153–175.

Weber, Richard. 1992. “On the Gittins index for multiarmed bandits.” Ann. Appl. Probab. 2 (4):
    1024–1033.

Weber, R.R., and G. Weiss. 1990. “On an index policy for restless bandits.” Journal of Applied
    Probability:637–648.

      . 1991. “Addendum to ’On an index policy for restless bandits’.” Advances in Applied prob-
    ability:429–430.

Weitzman, Martin L. 1979. “Optimal Search for the Best Alternative.” Econometrica 47 (3): 641–
    654.

Whittle, P. 1980. “Multi-armed bandits and the Gittins index.” J. Roy. Statist. Soc. Ser. B 42 (2):
    143–149.

      . 1981. “Arm-Acquiring Bandits.” The Annals of Probability 9 (2): 284–292.

      . 1988. “Restless bandits: activity allocation in a changing world.” J. Appl. Probab. No.
    Special Vol. 25A:287–298.

Wonham, W. M. 1968. “On the separation theorem of stochastic control.” SIAM J. Control 6:312–
    326.




                                                33
5    Appendix A: Technical Appendix

Lemmas 1 through 3 are used to establish theorem 1 and lemmas 4 through 9 to establish theorem 2.
    We will say that the discrete setting is in place when we are using the following notation.

Definition 8 (Discrete setting). For δ > 0 and i ∈ N let ti = iδ. For càdlàg processes P , H, R,
etc., we write Pi , Hi , Ri for Pti , Hti , Rti . For a càglàd process like U , we write Ui for the right limit
Uti + . The value of a p.o. step control with step size δ can be expressed as

                                                       ∞
                                                                                                       !
                                                       X
                                   J p.o. (U) = E
                                                                                                   
                                                             ζi Ui γ(Θ, Hi ) + (1 − Ui )k                  ,                  (38)
                                                       i=0

where
                                                               Z     δ                                                     
             −ρδ        −ρiδ                    1                              −ρt
 ζi = (1 − e       )e          ,   γ(θ, h) =          E                   ρe         dRt (Θ0 , H0 , R0 ) = (θ, h, 0), Ut ≡ 1 . (39)
                                             1 − e−ρδ             0

Similarly, the value of a se. step control with step size δ is

                                                       ∞
                                                                                                       !
                                                       X
                                       se.
                                                                                                   
                                   J         (U) = E         ζi Ui γ(Pi , Hi ) + (1 − Ui )k                ,                  (40)
                                                       i=0

where
                                               γ(p, h) = pγ(1, h) + (1 − p)γ(0, h).                                           (41)

Lemma 1. Under assumptions 1, and 3, the value functions for the p.o. and se. problem are
well-defined and finite.

Proof. Step 1. We claim that the set of controls for the p.o. and se. problem, discretized or
not, is not empty. For the p.o. problem, it is sufficient to show that constant control processes F
are admissible. This is a consequence of Jacod and Shiryaev (2003, proposition III.2.42) stating
that existence and local uniqueness holds for the martingale problem associated to deterministic
characteristics. For the se. problem, assumption 3 implies the existence of controls with constant
control process. This proves the claim.
   Step 2. Let U be a p.o. control and let (B, C, µ) be the characteristics of R. The uniform
bound on (|r|2 ∧ |r|)K(θ, h, dr) in assumption 1 implies that the process (|r|2 ∧ |r|) ∗ µ is increasing
         R

and of integrable variation. It follows from Jacod and Shiryaev (2003, proposition II.2.29b) that
R is a special (F, P)-semimartingale. Therefore, there is a unique predictable process of integrable
variation A such that R − A is a local martingale. By the same proposition, A satisfies
                                                      Z                                                   
                                                                                             
                   dAt = β(Θ, Ht )Ut + k(1 − Ut ) + Ut                               r − χ(r) K(Θ, Ht , dr) dt.               (42)


The uniform bounds on β(θ, h) and (|r|2 ∧ |r|)K(θ, h, dr) imply that At has at most linear growth
                                 R


                                                                    34
in t. Consequently,
                                            Z    ∞                             Z      ∞                
                           p.o.                            −ρt                                −ρt
                       J          (U) = E             ρe         dRt       = E               ρe     dAt       < ∞.   (43)
                                              0                                      0

The bound on A is uniform in U . Therefore V p.o. is finite.
   Step 3. Now let U be a se. control. The process P is bounded by assumption 3. It follows by
assumption 1 that the integrand in the definition of J se. (U) in (22) is bounded. Therefore J se. (U)
is finite. The bound is uniform in the control U and therefore V se. is finite.

Lemma 2. Under assumptions 1 and 2, every p.o. control can be transformed into a se. control
with the same value, which implies

                      V p.o. (p0 , h0 ) ≤ V se. (p0 , h0 ),              V p.o.,δ (p0 , h0 ) ≤ V se.,δ (p0 , h0 ).   (44)

Proof. For a p.o. control
                                                                                              
                                             (Ω, F, F, P), U, Θ, H, R, p0 , h0                                       (45)

we let P be the unique càdlàg process satisfying Pt = E(Θ|FtR ). We claim that

                                            ((Ω, F, FR , P), U, P, H, p0 , h0 )                                      (46)

is a se. control with the same value as the p.o. one. If the p.o. control is a step control, then the
se. control is a step control as well.
   Step 1. If p0 equals zero or one, then Pt = p0 is constant and yields the desired se. control. In
the sequel, we assume that 0 < p0 < 1. Then the measure P can be conditioned on the type Θ of
the agent. For θ ∈ {0, 1}, this yields measures Pθ such that

                                     Pθ (Θ = θ) = 1,              P = p0 P1 + (1 − p0 )P0 .                          (47)

The process P is the FR -density process of the measure p0 P1 relative to P, which can be seen from
the relation       Z                 Z                               Z
                           Pt dP =        E(Θ|FtR )dP            =         ΘdP = p0 P1 (A)          for A ∈ FtR .    (48)
                      A               A                                A

   Step 2. By Jacod and Shiryaev (2003, theorems II.2.42 and III.3.40), the process

                                                                  1
  M f = f (R) − f (R0 ) − f 0 (R− ) U β(P− , H) + (1 − U )k · t − f 00 (R− )U σ(H)2 · t
                                                           
                                                                  2
                                                                0
                                                                                           
                                    − f (R− + r) − f (R− ) − f (R− )χ(r) ∗ U K(P− , H, dr)dt (49)

is an (FR , P)-martingale for any f ∈ Cb2 (R). (The centered dot and the star denote integration with




                                                                     35
respect to a semimartingale and a random measure, respectively.) Similarly, the process

  ff = f (R) − f (R0 ) − f 0 (R− ) U β(1, H) + (1 − U )k · t − 1 f 00 (R− )U σ(H)2 · t
                                                        
  M
                                                               2
                                    − f (R− + r) − f (R− ) − f 0 (R− )χ(r) ∗ U K(1, H, dr)dt (50)
                                                                                           


is an (FR , P1 )-martingale for any f ∈ Cb2 (R). The coefficients in equations (49) and (50) are related
by functions ψ1 (p, h) and ψ2 (p, h, r), which are defined for p > 0 by the relations
                                                               Z
                                                2
                                                                                       
               β(1, h) = β(p, h) + σ(h) ψ1 (p, h) +                    ψ2 (p, h, r) − 1 χ(r)K(p, h, dr),
                                                                  R                                                (51)
                                                               K(1, h, dr) = ψ2 (p, h, r)K(p, h, dr).

The existence of ψ1 and ψ2 is guaranteed by assumption 2 and by the relations

                                                                                    φ2 (h, r)
             ψ1 (p, h) = (1 − p)φ1 (h),             ψ2 (p, h, r) =                                       .        (52)
                                                                      pφ2 (h, r) + (1 − p) 2 − φ2 (h, r)

Recall that by definition, φ2 (h) ∈ [0, 2]. Consequently, the inequality
                                                                          
                                        pφ2 (h, r) + (1 − p) 2 − φ2 (h, r) ≥ 2/n                                   (53)

holds for p ≥ 1/n because the left-hand side is a convex combination of p and (1 − p). Therefore,
                                                                                   !2
                               2               2(1 − p)(φ2 (h, r) − 1)                     1                 2
            ψ2 (p, h, r) − 1        =                                                  ≤     2
                                                                                                φ2 (h, r) − 1      (54)
                                          pφ2 (h, r) + (1 − p) 2 − φ2 (h, r)                n

holds for each p ≥ 1/n. This bound can equivalently be expressed in terms of the functions
                       p
y log(y) − y + 1 or 1 − y(2 − y) instead of (y − 1)2 because the inequalities

                           y log(y) − y + 1 ≤ (y − 1)2 ≤ 4 y log(y) − y + 1 ,
                                                                           
                                                                                                                   (55)
                                 p                             p          
                            1 − y(2 − y) ≤ (y − 1)2 ≤ 2 1 − y(2 − y)                                               (56)

hold for y ∈ [0, 2]. The first inequality in (55) stems from the fact that the concave function log(y)
lies below its tangent y − 1 at 1. The second inequality in (55) holds because the left- and right-
hand side, as well as their first derivatives, coincide at y = 1, whereas the second derivative of the
right-hand side is greater. The inequalities in (56) follow from the relations
                                                p          p        
                    (y − 1)2 = 1 − y(2 − y) = 1 − y(2 − y) 1 + y(2 − y) .                                          (57)

By assumption 2, there are constants Kn such that the inequalities
                                            Z    q                         
                   2      2
                                               1 − φ2 (h, r) 2 − φ2 (h, r) K( 12 , h, dr) ≤ Kn
                                                                          
             σ(h) φ1 (h) ≤ Kn ,                                                                                    (58)


                                                             36
hold whenever |h| ≤ n. Together with the bounds in (54)–(56), this implies that for any p ≥ 1/n
and |h| ≤ n, the bounds
  Z                                                                  Z
                                                                                          2
        ψ2 (p, h, r) log ψ2 (p, h, r) − ψ2 (p, h, r) + 1 K(p, h, dr) ≤    ψ2 (p, h, r) − 1 K(p, h, dr)
                Z                                      Z                                
             1                                      2
                                  2                          q                        
        ≤ 2          φ2 (h, r) − 1 K(p, h, dr) ≤ 2        1 − φ2 (h, r) 2 − φ2 (h, r) K(p, h, dr)
            n                                      n
                                           Z                                
                                        4                                                     4
                                                      q
                                                1 − φ2 (h, r) 2 − φ2 (h, r) K( 21 , h, dr) ≤ 2 Kn , (59)
                                                                           
                                      ≤ 2
                                        n                                                     n

and
                                          ψ1 (p, h) ≤ φ2 (h) ≤ Kn                                     (60)

hold. Let Tn be the stopping time

                        Tn = inf{t ≥ 0 : Pt < 1/n or Pt− < 1/n or |Ht | > n} ∧ n.                     (61)

Moreover, let Ln be the local (FR , P)-martingale stopped at Tn given by

      Ln = ψ1 (P− , H)1J0,Tn K · Rc + ψ2 (P− , H, r) − 1       1J0,Tn K ∗ µR − Ut K(P− , H, dr)dt ,
                                                                                                
                                                                                                      (62)

where Rc is the continuous local martingale part of the process R under the measure P and µR
is the jump measure of R. Then the stochastic exponential Dn = E(Ln ) is a uniformly integrable
martingale by the bounds in equations (59) and (60) and Lépingle and Mémin (1978, Théorème
IV.3). Therefore DTnn P is a probability measure.
   Step 3. Keeping track of the terms in Ito’s formula the same way as in the proof of Jacod and
Shiryaev (2003, theorem II.2.42) shows that

                        ∂f
               Mf =        (R− ) · Rc + f (R− + r) − f (R− ) ∗ µR − U K(P− , H, dr)dt
                                                                                     
                                                                                                      (63)
                        ∂r

is the decomposition of M f into its continuous and purely discontinuous local martingale parts.
Letting M f,Tn be process M f stopped at Tn , one obtains the formula

                     ∂f
  hM f,Tn , Ln i =      (R− )ψ1 (P− , H)1J0,Tn K U σ(H)2
                     ∂r
                         + f (R− + r) − f (R− ) ψ2 (P− , H, r) − 1 1J0,Tn K ∗ U K(P− , H, dr)dt . (64)
                                                                                             


for the predictable quadratic covariation of M f,Tn and Ln . A comparison of equations (49) and
(50) shows that

                       ff,Tn = M f,Tn − hM f,Tn , Ln i = M f,Tn − 1 · hM f,Tn , Dn i.
                       M                                                                              (65)
                                                                 D−n


By Girsanovs’ theorem (theorem III.3.11), this process is an (FR , DTnn P)-martingale. It follows


                                                    37
that the measures P1 and DTnn P solve the same martingale problem stopped at Tn . Namely, under
these measures, the stopped reward process RTn has semimartingale characteristics (B Tn , C Tn , µTn )
stopped at Tn , where (B, C, µ) satisfy equations (1), (2), (3) with Θ = 1. According to definition
1 of admissible controls, local uniqueness holds for this martingale problem. It follows that DTnn P
coincides with P1 on FTRn . The characterization of P as the density process of the measure p0 P1
relative to P (see step 1) implies that P = p0 DTnn holds on J0, Tn K.
      Step 4. It remains to calculate the (FR , P)-characteristics of the filter P . We first do this on
the interval J0, Tn K, where P agrees with p0 Dn and consequently satisfies P = P− · Ln . The jumps
of P on this interval are

               ∆P = P− ∆Ln = P− ψ2 (P− , H, ∆R) − 1               1∆R6=0 = j(P− , H, ∆R)1∆R6=0 ,
                                                              
                                                                                                              (66)

where the function j is defined in equation (20). Since the jump measure of R is (FR , P)-compensated
by U K(P− , H, dr)dt, the jump measure of P is compensated by the predictable random measure

                                         µ = U (j∗ K)(P− , H, dr)dt,                                          (67)

where the push forward measure j∗ K is defined in equation (19). The quadratic variation of P on
the interval J0, Tn K is

  C = hP c , P c i = P−2 hLn,c , Ln,c i = P−2 ψ1 (P− , H)2 hRc , Rc i. = P−2 (1 − P− )2 φ1 (H)2 U σ(H)2 · t   (68)

(As before, the centered dot denotes integration.) Since P Tn is an (FR , P)-martingale, Jacod and
Shiryaev (2003, proposition II.2.29) implies that its first characteristic is
                                                                        
                                B = −(r − χ(r)) ∗ U (j∗ K)(P− , H, dr)dt .                                    (69)

Let us now use equations (67) to (69) as definitions of (B, C, µ) for all t. Moreover, for f ∈ Cb2 (R),
let

                                          1
  N f = f (P ) − f (P0 ) − f 0 (P− ) · B − f 00 (P− ) · C
                                          2
                                 − f (P− + p) − f (P− ) − f 0 (P− )χ(p) ∗ U (j∗ K)(P− , H, dp)dt . (70)
                                                                                               


Having calculated the characteristics of P on J0, Tn K, we know that N f,Tn is a martingale for each
n. Let
                              T = lim Tn = inf{t ≥ 0 : Pt− = 0 or Pt = 0}.                                    (71)
                                   n→∞

It is easily seen that PTn a.s. converges to PT : if T is infinite or ∆PT = 0, this is true by definition;
otherwise, Tn = T holds for all n > 1/∆PT . Let t ≥ 0 and let S be an FR -stopping time. Then
the previous observation implies that NSf,Tn ∧t converges a.s. to NSf,T ∧t . This convergence holds
also in L1 because N f,Tn ∧t are uniformly bounded and the dominated convergence theorem can be


                                                       38
applied. Therefore
                                        E(NSf,T ∧t ) = lim E(NSf,Tn ∧t ) = 0.                                         (72)
                                                          n→∞

The stopping time S was chosen arbitrarily. Therefore Jacod and Shiryaev (2003, Lemma I.1.44)
implies that N f,T ∧t is a uniformly integrable martingale. Since this holds for all t, it follows that
N f,T is a local martingale. By Jacod and Shiryaev (2003, lemma III.3.6), P vanishes on JT, ∞J.
Moreover, the characteristics (B, C, µ) defined in equations (67) to (69) are constant when P−
vanishes. It follows that N f coincides with N f,T . Therefore, N f is a local martingale for all
f ∈ Cb2 (R). Thus we have shown that the characteristics of P are as in definition 4 of se. controls.
We conclude that the expression in equation (46) defines a se. control.
   Step 5. It remains to show that the p.o. and se. strategy have the same value. This is easily
seen by taking FR -optional projections in equation (43), which transforms it into the expression in
equation (23).

Lemma 3. Under assumption 3, there exists for every se. step control a p.o. step control of at
least the same value, which implies the inequality V p.o.,δ ≥ V se.,δ .

Proof. We work in the discrete setting. To avoid confusion, we will mark objects of the se. problem
with a tilde. The se. control problem is that of controlling the discrete time Markov chain (Pet , H
                                                                                                   et )
                                                                                                                  i     i

with ti = iδ. Controls are restricted to be {0, 1}-valued. It is well-known that optimal Markov
controls exist for such problems, see e.g. Seierstad (2009). We will prove the lemma by showing
that every se. step Markov control corresponds to a p.o. step control of the same value.
   So we start with an optimal step Markov control Ue and write its control process in the form

                                                                    e t )1(t ,t ] .
                                                   X
                                            U
                                            et =         fi (Peti , H  i    i i+1
                                                                                                                      (73)
                                                     i


We will construct the p.o. control on the space D(R3 ) with its natural filtration F, sigma algebra
F and coordinate process X = (Θ, H, R). For (u, θ, h, r) ∈ {0, 1}2 × R2 , let Pu;θ,h,r be the unique
probability measure on F such that Θ is the constant process Θt = θ, H satisfies equation (1)
and the initial condition H0 = h, and R has semimartingale characteristics (2)–(3) and satisfies
R0 = r. By a straight-forward argument (see e.g. the proof of Jacod and Shiryaev (2003, Corollary
III.2.42)), Pu;θ,h,r depends measurably on (u, θ, h, r). This allows us to inductively define probability
measures Pn and càdlàg processes P n on D(R3 ) as follows.

           P0 = p0 Pf0 (p0 ,h0 );1,h0 ,0 + (1 − p0 )Pf0 (p0 ,h0 );0,h0 ,0 ,           Pt0 = EP0 (Θ0 |FtR ),           (74)
             n      n−1
           P =P           ⊗tn Pfn (P n−1 ,Ht                      ,                   Ptn   =   EPn (Θ0 |FtR ).       (75)
                                      tn      n );Θtn ,Htn ,Rtn



Here we have used the following notation: When P is a probability measure on D(R3 ) and (Qx )x∈R3
is a stochastic kernel from R3 to D(R3 ), then P ⊗t QX is the unique probability measure on D(R3 )
such that the law of the stopped process X t is equal to P on Ft and such that the conditional law
of the time-shifted process (Xt+s )s≥0 given Xt = x is Qx . The notation is explained and relevant


                                                              39
results are proven in Stroock and Varadhan (2006, 6.1.2, 6.1.3 and 1.2.10) for continuous processes.
The relevant results on Skorokhod space are Jacod and Shiryaev (2003, lemmas III.2.43-48), but
the notation is not used there.
   The measures Pn and Pm agree on Ftn ∧tm and the processes P n and P m a.s. agree on [0, tn ∧tm ].
Therefore there is a unique measure P that coincides with Pn on Ftn for all n. Furthermore, there
is a unique càdlàg process P that is a.s. equal to P n on [0, tn ] for all n. We define

                                                    fi (Pti , Hti )1(ti ,ti+1 ]
                                              X
                                       Ut =                                                       (76)
                                                i

and claim that
                                U = (D(R3 ), F, F, P), U, Θ, H, R, p0 , h0
                                                                                  
                                                                                                  (77)

is a p.o. control. So we have to verify that U can be written as a predictable functional of R. (We
say that a process can be written as a (predictable) functional of R if it coincides P-a.s. with F (R),
where F is an adapted (predictable) process on D(R).) We proceed by induction and claim that
for all n, the stopped processes U tn and H tn can be written as functionals of R. For n = 0 there
is nothing to prove. For the inductive step, we observe that U tn+1 and H tn+1 depend on U tn , Ptn ,
and Htn . These expressions can be written as functionals of R by the inductive assumption and
the claim follows. Therefore U can be written as a U = F (R), where F is a predictable process
on D(R). F is admissible because it is a step process. The process Θ is a.s. constant and can be
identified with a {0, 1}-valued random variable. H satisfies (1) and R has characteristics (2)-(3)
under P. Thus we have verified that (77) is a p.o. control. Then the proof of lemma 2 shows that

                                 Ub = (D(R3 ), F, FR , P), U, P, H, p0 , h0
                                                                                  
                                                                                                  (78)

is a se. control with the same value. Since U is a step process, assumption 3 implies that (P, H)
has the same distribution as (Pe, H).
                                  e It follows that

                                      J se. (U)
                                             e = J se. (U)
                                                        b = J p.o. (U).                           (79)




Lemma 4. Under assumptions 1, 3, and 4, se. controls can be approximated arbitrarily well by se.
step controls, which implies
                                     V se. (p0 , h0 ) = sup V se.,δ (p0 , h0 )                    (80)
                                                           δ

Proof. It is sufficient to show the inequality ≤ in (80). The reverse inequality holds by definition.
   Step 1. Let U ∈ Use.
                    p0 ,h0 be a se. control with control process U . The first step is to represent
the control process by the random measure
                                                                        
                               Q(dt, du) = Ut δ1 (du) + (1 − Ut )δ0 (du) dt.                      (81)



                                                         40
Q is a predictable random measure on R≥0 ×{0, 1} whose marginal on R≥0 is the Lebesgue measure.
Conversely, for any such random measure there exists a representation as in (81) with a predictable
process U , see Jacod and Shiryaev (2003, II.1.7.(i) and I.3.13). Equations (1) and (18) characterize
the evolution of the process (P, H) and are equivalent to the following martingale problem: for all
f ∈ Cb2 (R2 ), the process

                                  ZZ
                                                     ∂f
  f (Pt , Ht ) − f (P0 , H0 ) −                         (Ps− , Hs ) uα(1, Hs ) + (1 − u)α(0, Hs ))
                                       [0,t]×{0,1}   ∂h
                                             ∂f                 1 ∂2f
                                         +      (Ps− , Hs )bs +       (Ps− , Hs )ct
                                             ∂p                 2 ∂p2
                         
                         Z                                                            !
                                                             ∂f
                    +     f (Ps− + q, Hs ) − f (Ps− , Hs ) −    (Ps− , Hs )χ(q) µs (dr) Q(ds, du) (82)
                      Rn                                     ∂p

is a martingale. Moreover, the following initial condition holds:

                                             P(P0 = p0 ) = P(H0 = h0 ) = 1                                (83)

     Step 2. Let M be the space of all probability measures on R≥0 × {0, 1} whose marginal on
R≥0 is the Lebesgue measure. We endow M with the topology of vague convergence, checked on
compactly supported continuous functions. This turns M into a compact metric space. Let M be
the Borel sigma algebra on M . We give M the natural filtration M = (Mt )t≥0 generated by 1[0,t] · q
for q ∈ M . The canonical space for the se. control problem is M × D(R2 ). Elements of this space
will be denoted by (Q, P, H). We define a se. control rule to be a probability measure P on the
space M × D(R2 ) that solves the martingale problem (82) and satisfies the initial condition (83).
The value of a se. control rule is defined as
      ZZ                                               Z                                             !
                           −ρt
                                                                           
 E                       ρe     β(Pt− , Ht− )u+k(1−u)+u              r−χ(r) K(Pt− , Ht− , dr) Q(dt, du) , (84)
           [0,∞)×{0,1}


where the expectation is taken with respect to P. By the above considerations, any se. control in
the sense of definition 4 induces a se. control rule and vice versa. Therefore, the se. problem is
equivalent to maximizing (84) over all se. control rules.
     Step 3. Let (p0 , h0 ) ∈ {0, 1} × R and Q ∈ M . Then Q corresponds to a deterministic control
process U . Let D(R2 ) be Skorokhod space with its natural filtration F and sigma algebra F. By
assumption 3, there is one and only one solution measure SQ on F to the martingale problem
(82), (83). We claim that SQ is continuous in Q ∈ M . This follows from Jacod and Shiryaev
(2003, theorem IX.3.39). The verification of the conditions of the theorem is straight-forward
and goes along the lines of Jacod and Shiryaev (2003, theorem IX.4.8). The assumptions that
are needed are assumption 1 providing uniform bounds on the coefficients and establishing the
continuity of α, assumption 3 implying well-posedness of the martingale problem (82) for (P, H)
under the deterministic control process U , and assumption 4 establishing the continuity of β, σ, K


                                                             41
in an appropriate sense.
   Step 4. Let P be a se. control rule and let Z stand for (P, H). Using disintegration, P can be
written in the form
                                                                   D(R2 )
                                    P(dQ, dZ) = PM (dQ)PQ                   (dZ).                    (85)
                                          D(R2 )
Then for PM -a.e. Q, the measure PQ                is equal to the measure SQ from step 3. (We have used
assumption 3 here.) It is well-known that any measure valued control Q ∈ M can be approximated
in the vague topology by a sequence ψ n (Q) of measures of the form
                                          ∞
                                                Uin δ1 (du) + (1 − Uin )δ0 (du) 1(tni ,tni+1 ] dt,
                                          X
                      ψ n (Q)(dt, du) =
                                                                               
                                                                                                     (86)
                                          i=1

where ψ n : M → M are M-adapted mappings. This result is known under the name chattering
lemma and can be found e.g. in Mazliak (1993, theorem 2.2) or Fleming (1989). Furthermore,
tni can be chosen of the form tni = iδ n for some sequence of numbers δ n > 0. Let PM,n be the
push-forward measure of PM under ψ n . Moreover, let

                                     Pn (dQ, dZ) = PM,n (dQ)SQ (dZ).                                 (87)
                                                                              n
Then each Pn corresponds to a se. step control U n ∈ Use.,δ
                                                      p0 ,h0 as explained in step 1. By the
result of step 3, SQ is continuous in Q. It follows that Pn → P weakly. The value of the control
given by the expression in equation (84) is continuous in P with the weak topology. Therefore
J se. (U n ) → J se. (U). Thus we have shown that the value of a control rule can be approximated
arbitrarily well by the value of a step control.

Lemma 5. Under assumptions 1–5, the value function V (p0 , h0 ) is convex non-decreasing in p0 and
non-decreasing in h0 . The same statement holds about the discrete time value function V δ (p0 , h0 ).

Proof. Step 1. Let U be a se. control with deterministic control process U . Then the martingale
property of P can be used to express the value of U as follows.
                     Z   ∞                             Z                         
  J se. (U) = p0 E
                                                                     
                          β(1, Ht )Ut + k(1 − Ut ) + Ut     r − χ(r) K(1, Ht , dr) +
                      0
                                    Z ∞                                Z                       
                                                                                   
                      + (1 − p0 ) E       β(0, Ht )Ut + s(1 − Ut ) + Ut   r − χ(r) K(0, Ht , dr)  (88)
                                      0

This expression is linear in p0 and non-decreasing in (p0 , h0 ) by assumption 5.
   Step 2. Now, let U be a general se. control. As described in the proof of lemma 4, U corresponds
to a measure P on the canonical space M ×D(R2 ). Elements of this space will be denoted by (Q, Z).
Thus Q corresponds to a sample path of the control process U and Z is sample path of (P, H).
Using disintegration, P can be represented as follows.

                                                                   D(R2 )
                                    P(dQ, dZ) = PM (dQ)PQ                   (dZ),                    (89)


                                                         42
where SQ is the unique solution to the martingale problem associated to the deterministic control
Q, see step 3 in the proof of lemma 4. Let UQ be the se. control corresponding to the control rule
SQ . Then the value of U can be expressed as
                                                      Z
                                        se.
                                    J         (U) =       J se. (UQ )PM (dQ).                         (90)
                                                      M

Now step 1 implies that this is linear in p0 and non-decreasing in (p0 , h0 ). By taking the supremum
over strategies in Use.,δ       se.
                    p0 ,h0 and Up0 ,h0 , respectively, one obtains that V
                                                                          se. and V se.,δ have the desired

monotonicity and convexity properties.

Lemma 6. Let assumptions 1–5 hold and assume the discrete setting. Then it is optimal to choose
the unknown (known) arm initially in the p.o. problem if and only if it is optimal to do so in the se.
problem. Furthermore, the unknown arm is uniquely optimal as an initial choice if γ(p0 , h0 ) > k.

Proof. Step 1. We first prove the statement about the optimality of the unknown arm under the
condition γ(p0 , h0 ) > k for the se. problem. We fix the initial condition (P0 , H0 ) = (p0 , h0 ) and
work in the discretized setting with step size δ > 0. The Bellman equation states that optimal
initial choices for the se. problem are characterized by the equation
                                                                                           
  U0 ∈ arg max uγ(p0 , h0 ) + (1 − u)k + e−ρδ E V se.,δ (P1 , H1 ) U0 = u, P0 = p0 , H0 = h0 .        (91)
            u∈{0,1}


Thus the optimal initial choice for the se. problem depends on the sign of the quantity
                                                                         
  γ(p0 , h0 ) − k + e−ρδ E V se.,δ (P1 , H1 ) U0 = 1, P0 = p0 , H0 = h0
                                                                                                   
                                               − e−ρδ E V se.,δ (P1 , H1 ) U0 = 0, P0 = p0 , H0 = h0 , (92)

which is the advantage of the unknown arm over the known arm (up to multiplication by a positive
                                                                                               −
constant). Let h+
                0 be the value that H1 attains after an initial choice of the unknown arm and h0
the value after an initial choice of the known arm. By assumption 5, the inequality h−         +
                                                                                     0 ≤ h0 ≤ h0
holds. Furthermore, note that P1 = P0 under an initial choice of the known arm, as can be seen
from the characteristics of P in definition 4. Together with the monotonicity of the value function
in h0 and its convexity in p0 , this can be used to show the following estimate:
                                                                                               
  E V se.,δ (P1 , H1 ) U0 = 1, P0 = p0 , H0 = h0 − E V se.,δ (P1 , H1 ) U0 = 0, P0 = p0 , H0 = h0
                                                          
     = E V se.,δ (P1 , h+  0 ) U0 = 1, P 0 = p 0 , H0 = h0   − V se.,δ (p0 , h−
                                                                              0)                    (93)
                                                          
     ≥ E V se.,δ (P1 , h+  0 ) U0 = 1, P0 = p0 , H0 = h0 − V
                                                                 se.,δ
                                                                       (p0 , h+
                                                                              0)≥0


It follows that (92) is strictly positive when γ(p0 , h0 ) > s. In this case, the initial choice of the
unknown arm is uniquely optimal for the se. problem.
   Step 2. Optimal choices for the p.o. problem are characterized by the same equation (91) as


                                                          43
for the se. problem, with V se.,δ replaced by V p.o.,δ . These two value functions agree by theorem 1.
It follows that optimal choices for the p.o. and se. problem agree.

Lemma 7. Under assumptions 1–5, there exists for every p.o. step control a p.o. stopping control
that is at least as good.

Proof. We work in the discrete setting with δ > 0.
   Step 1. We claim that the lemma is true when the discount sequence is truncated at stage n.
The truncated discount sequence is given by

                                                    ζi = (1 − e−ρδ )e−ρiδ 1i≤n .                                                (94)

More generally, ζ could be a regular discount sequence of finite horizon n, see Berry and Fristedt
(1985, definition 5.2.1). We claim that there exists an optimal stopping control, i.e., a control
that never switches from known to the unknown arm. We prove the claim by induction on n. For
n = 0, there is nothing to prove. Now let ζ have horizon n + 1. Let U ∈ Up.o.,δ
                                                                         p0 ,h0 be an optimal
control rule for the p.o. problem. (This exists because there are optimal se. step controls that
can be transformed into optimal p.o. controls by lemma 3.) Let U = F (R) be the control process
associated to U. The inductive hypothesis allows one to assume that for i ≥ 1, Ui never switches
from the known to the unknown arm. If U0 indicates the unknown arm, the proof is complete.
Otherwise U has the form

                        U0 = 0,             Ui = 1 for i = 1, . . . , T,           Ui = 0 for i = T + 1, . . .

The stage T where the strategy changes from the unknown to the known arm is a stopping time.
Given that the known arm is chosen initially, the reward obtained at the first stage is deterministic
and does not contain any information about the type. Therefore, there is a modification of U that
does not depend on the outcome of the first stage. This makes it possible to define a control rule
U ∗ that skips the first action of U . Formally, and in terms of t instead of i, U ∗ can be defined as

                                                    Ut∗ = Ft+δ (R0∨(t−δ) )t≥0 .
                                                                             
                                                                                                                                (95)

It is easy to verify that this is an admissible control process. Let U ∗ be the corresponding p.o.
control. We claim that U ∗ is at least as good as U. Let H and H ∗ be the human capital processes
under the strategies U and U ∗ , respectively. Furthermore, let U 0 be the control associated to
choosing the known arm all the time. The advantages of U ∗ and U over U 0 are

                                                    T −1                                     T
                                                                                   !                                        !
                                                    X                                        X
        J p.o. (U ∗ ) − J p.o. (U 0 ) = E                  ζi γ(Θ, Hi∗ ) − k
                                                                                                                       
                                                                                       ≥ E         ζi−1 γ(Θ, Hi ) − k           (96)
                                                    i=0                                      i=1
                                                     T
                                                                                  !
                                                    X
              p.o.             p.o.     0
                                                                              
          J          (U) − J          (U ) = E             ζi γ(Θ, Hi ) − k           ≥ 0.                                      (97)
                                                    i=1



                                                                   44
The first inequality holds because choosing the known arm decreases ability, see assumption 5. The
second inequality holds because the optimal control U is at least as good as the control designating
the known arm at all stages. Therefore, the advantage of U ∗ over U is

                                       T                                              ∞
                                                                              !
                                                                                                                          
                                                                                          (ζi−1 − ζi ) E 1i≤T γ(Θ, Hi ) − k .
                                       X                                              X
    p.o.     ∗        p.o.
                                                                          
J          (U ) − J          (U) ≥ E         (ζi−1 − ζi ) γ(Θ, Hi ) − k           =
                                       i=1                                            i=1              |        {z          }
                                                                                                              =:bi
                                                                                                                        (98)
The increment of bi is
                                                                                    
                      bi+1 − bi = E 1i≤T γ(Θ, Hi+1 ) − γ(Θ, Hi ) + E 1i=T k − γ(Θ, Hi+1 .                               (99)

The first summand on the right-hand side is non-negative for i = 1, 2, . . . because Hi increases while
                                   R -measurability of
the unknown arm is played. By the Fi+1                                                1i=T and Hi+1 , the second summand
can be written as
                                                                                 
      E 1i=T k − γ(Θ, Hi+1 ) = E 1i=T k − γ(Pi+1 , Hi+1 ) = E 1i=T k − γ(PT +1 , HT +1 ) .

Recall that it is optimal under strategy U to play the known arm at stage T + 1. Lemma 6 shows
that this implies k ≥ γ(PT +1 , HT +1 ). This proves

                                                     bi+1 ≥ bi for i = 1, 2, . . .                                     (100)

We also have
                                                            ∞
                                                            X
                                                                  ζi bi ≥ 0                                            (101)
                                                            i=1

from equation (97). It is shown in Berry and Fristedt (1985, equation (5.2.8)) that (100) and (101)
imply
                                                                       ∞
                                                                       X
                                        J p.o. (U ∗ ) − J p.o. (U) =         (ζi−1 − ζi )bi ≥ 0                        (102)
                                                                       i=1

when ζ is regular. In our case, ζ is regular because it is a truncated geometric discount sequence.
Thus we have constructed a strategy U ∗ that is at least as good as the optimal strategy U and
never switches from the known to the unknown arm.
      Step 2. To drop the assumption that the discount sequence has a finite horizon, one ap-
proximates an arbitrary geometric (or, more generally, regular) discount sequence ζ by truncated
discount sequences ζ n with finite horizon. The argument can be found in the proof of Berry and
Fristedt (1985, theorem 5.2.2).

Lemma 8. Under assumptions 1–5, the stopping time T ∗ = inf{t : V (Pt , Ht ) ≤ k} is optimal for
the se. problem.

Proof. Let (P, H) be the process governed by the constant control Ut = 1 in the sense of defi-


                                                                  45
nition 4. This is a Feller process by assumptions 1, 3, 4 and Jacod and Shiryaev (2003, theorem
IX.4.39). Let (Pe, H)
                   e be the killed version of (P, H) with killing rate ρ, see Peskir and Shiryaev
(2006, section II.5.4). Then we define
                                          Z                            
                                                        
                    f (p, h) = ρ β(p, h) +      r − χ(r) K(p, h, dr) − k ,         f (∂) = 0,          (103)

where ∂ denotes the “cemetery point” of the killed process. Moreover, we define
                                                     Z   t
                                          It = i +           f (Pet , H
                                                                      e t )dt                          (104)
                                                     0


and X = (Pe, H,
             e I). Then X is a Feller process on the state space

                                           E = (R2 ∪ {∂}) × R.                                         (105)

Let (Px )x∈E denote the family of laws of X starting from the initial condition X0 = x. We associate
to it the following family of optimal stopping problems:

                                           W (x) = sup Ex (IT ),                                       (106)
                                                         T


where the supremum is over the set of all FX -stopping times. Since the value function V is attained
as the supremum over stopping controls by lemma 7, one obtains that

  W (p, h, i) = sup Ep,h,i (IT ) = sup Ep,h,0 (IT ) + i
                 T                  T
                           Z T                       Z                                     
                                  ρe−ρt β(Pt , Ht ) +
                                                                            
             = sup Ep,h,0                                       r − χ(r) K(Pt , Ht , dr) − k dt + i
                T             0

                                                                                     = V (p, h) − k + i (107)

for (p, h, i) ∈ R3 . We define the stopping set D as in Peskir and Shiryaev (2006, equation (2.2.5))
by
                                      h, i) ≤ i} = {(p, h) ∈ R2 : V (p, h) ≤ k} ∪ {∂} × R.
                                                                                     
           D = {(e  h, i) ∈ E : W (e
                 p, e              p, e                                                                (108)

The equality in (108) holds because W (∂, i) = i, which is obvious from the definitions. The
function W is lower semi-continuous because (Pe, H,
                                                 e I) is Feller, see Peskir and Shiryaev (2006,
equation (2.2.80)). Therefore the set D is closed. Then the right-continuity of the filtration implies
that
                           T ∗ = inf{t ≥ 0 : Xt ∈ D} = inf{t : V (Pt , Ht ) ≤ k}                       (109)

is a stopping time. Note that ∂ ∈ D, which implies P(T ∗ < ∞) = 1. Then Peskir and Shiryaev
(2006, Corollary 2.9) implies that T ∗ is optimal.

Lemma 9. Under assumption 3, there exists for each se. stopping control a p.o. stopping control


                                                         46
with the same value.

Proof. The idea of the proof is very similar to that of lemma 3. To avoid confusion, we will mark
objects of the se. problem with a tilde. Let Ue be a se. stopping control with control process U
                                                                                               e.
By working on the canonical space M × D(R2 ) for the se. problem (see the proof of lemma 4), we
can assume that the control process is FP ,H -predictable. Then up to a.s. equivalence, it can be
                                              e e

               e =1
represented as U                                                    2
                            e (t), where S is a stopping time on D(R ).
                    J0,S(Pe,H)K
   Let (D(R3 ), F, F) be the canonical path space for X = (Θ, H, R). As in lemma 3, we let Pu,x
denote the law of X under the constant control Ut = u with initial condition X0 = x. Let

                                Q = p0 P1,(1,h0 ,r0 ) + (1 − p0 )P1,(0,h0 ,r0 )                      (110)

and let Q be the unique càdlàg process satisfying Qt = EQ (Θ|FtR ). Since the process H is deter-
ministic under Q, the stopping time S(Q, H) is actually an FR -stopping time on D(R3 ). It follows
that up to a.s. equivalence, it can be written as T (R), where T is a stopping time on D(R). Let

                                          P = Q ⊗T (R) P0,XT (R)                                     (111)

be the unique probability measure on D(R3 ) such that the law of the stopped process X T (R) is equal
to Q on FT (R) and such that the conditional law of the time-shifted process (XT (R)+s )s≥0 given
XT (R) = x is P0,x . Since the process   1J0,T (R)K is piecewise constant, it follows that it is admissible
in the sense of definition 1. Thus P defines a p.o. control

                          U = (D(R3 , F, F, P), 1J0,T (R)K , Θ, H, R, p0 , h0 ) .
                                                                               
                                                                                                     (112)

Lemma 2 applied to this control shows that the characteristics of P are as required in definition 4,
which means that
                            Ub = (D(R3 , F, F, P), 1J0,T (R)K , P, H, p0 , h0 )
                                                                                  
                                                                                                     (113)

is a separated control with the same value as U. By the local uniqueness assumption 3, (P, H) is
                     e Therefore U is a p.o. stopping control with the same value as Ub and U.
equal in law to (Pe, H).                                                                    e


Proof of theorem 3.          Step 1. We calculate the Hellinger process h( 12 ) of order         1
                                                                                                 2   of the
measures P1 and P0 , which are the measure P conditioned on Θ = 1 and Θ = 0, respectively. Let
P = E(Θ|FtR ) be the belief process. By equation (48), P/p0 is the density process of P1 relative
to P and (1 − P )/(1 − p0 ) is the density process of P0 relative to P. Let

                                                     u+v √
                                         ψ(u, v) =      − uv.                                        (114)
                                                      2

and let µ(P,1−P ) (dt, dx, dy) be the third characteristic of the two-dimensional process (P, 1 − P ).




                                                      47
Let S be the first time that P or P− equals either zero or one,
                                          
                                   S = inf t ≥ 0 : Pt ∈ {0, 1} or Pt− ∈ {0, 1} .                                (115)

By Jacod and Shiryaev (2003, lemma III.3.7), P is constant on JS, ∞J. Therefore, on this interval,
hP c , P c i is constant and µ(P,1−P ) has no charge. After canceling out the terms p0 and (1 − p0 ), the
formula for h( 12 ) given in Jacod and Shiryaev (2003, theorem IV.1.33) reads as
                                                                                                       
              1       1       c   c            2             c       c         1               c      c
  h( 21 )   =            · hP , P i −                   · hP , 1 − P i +              · h1 − P , 1 − P i
              8     P−2                 P− (1 − P− )                       (1 − P− )2
                                             
                                x         y
                  +ψ 1+           ,1 +          ∗ µ(P,1−P )
                               P−        P−
                                                                               
                1     1         1            c    c              r           r                                
            =            +             · hP , P i + ψ 1 +           ,1 −            ∗ U (j∗ K)(P− , H, dr)dt
                8 P− 1 − P−                                     P−        1 − P−
                1
            =     U φ1 (H)2 σ(H)2 · tS
                8                                                                                                  !
                                            φ2 (H, r)                                 2 − φ2 (H, r)
                  +ψ                                                 ,                                          
                          P− φ2 (H, r) + (1 − P− ) 2 − φ2 (H, r) P− φ2 (H, r) + (1 − P− ) 2 − φ2 (H, r)
                  1J0,SK ∗ U K(P− , H, dr)dt
                                                  
                                                              q                         
                1
                                                Z       1  −    φ2 (H, r) 2 − φ2 (H, r)
                            2      2   S                                                     K(P− , H, dr) · tS
            =     U φ1 (H) σ(H) · t + U
                8                                   P− φ2 (H, r) + (1 − P− ) 2 − φ2 (H, r)
                                                Z                                    
                1
                                                           q
                  U φ1 (H)2 σ(H)2 · tS + U           1 − φ2 (H, r) 2 − φ2 (H, r) K(1/2, H, dr) · tS
                                                                                    
            =
                8
            = U Φ(H) · tS ,
                                                                                                                (116)
where the function Φ has been defined in equation (24) and where the superscript S means that
the process is stopped at S.
   Step 2. Let T be the first time that P reaches zero, see equation (71). We say that P jumps to
zero if PT − > 0 and claim that such jumps are not possible under the conditions of (a). To prove
this claim, note that the equivalence of the measures K(1, h, ·) and K(0, h, ·) implies the strict
inequality 0 < φ2 (h, r) < 2. It follows that the process Ln defined in equation (62) has no jumps of
size −1 and the process Dn = E(Ln ) does not jump to zero. If P had a jump to zero, then Tn = T
would hold for large enough n. But then Dn = P Tn /p0 would have a jump to zero, which is not
possible. This proves the claim. A similar argument, where the rôles of P0 and P1 are reversed,
shows that P cannot jump to one. It follows that for any stopping time τ , the following equations
hold P0 - and P1 -a.s., respectively:

            {h( 21 )τ = ∞} = {S ≤ τ, PS− = 0} = {Pτ = 0} = {Pτ = 0 or Pτ = 1}                  P0 -a.s.,        (117)
            {h( 12 )τ = ∞} = {S ≤ τ, PS− = 1} = {Pτ = 1} = {Pτ = 0 or Pτ = 1}                  P1 -a.s.         (118)




                                                           48
In equations (117) and (118), the first equality holds by Schachermayer and Schachinger (1999,
theorem 1.5). This theorem states that the divergence of the Hellinger process is equivalent to the
mutual singularity of the measures P1 and P0 , but in such a way that the singularity is not obtained
by a sudden jump of the density process to zero or one. The second equality holds because such
jumps are not possible by the previous claim. For the third equality, see Jacod and Shiryaev (2003,
proposition III.3.5.(ii)). Together with assumption 2 bounding Φ, equations (117) and (118) imply
                       Rτ
                      { 0 Ut dt < ∞} ⊆ {h( 21 )τ < ∞} = {0 < Pτ < 1}             P-a.s.                (119)

This proves (a).
                                                                            Rτ
   Step 3. Let τ be a stopping time. If S does not occur before τ and       0    Ut dt = ∞, then h( 21 )τ = ∞
because of the lower bound inf h Φ(h) > 0. Therefore,
                                Rτ
                            {   0    Ut dt = ∞} ⊆ {h( 21 )τ < ∞} ∪ {S ≤ τ }.                           (120)

Moreover, it follows from Schachermayer and Schachinger (1999, theorem 1.5) that

           {h( 21 )τ < ∞} ∪ {S ≤ τ } = {S ≤ τ, PS− = 0} ∪ {S ≤ τ } = {Pτ = Θ}             P0 -a.s.,    (121)
           {h( 21 )τ < ∞} ∪ {S ≤ τ } = {S ≤ τ, PS− = 1} ∪ {S ≤ τ } = {Pτ = Θ}             P1 -a.s.     (122)

It follows that
                                     Rτ
                                    { 0 Ut dt = ∞} ⊆ {Pτ = Θ}     P-a.s.,                              (123)

which proves (b). Finally, (c) follows from (a) and (b).




                                                     49
