                     NB. WORICNG PAP. SERIES




                          ROSEPACK   tocument No.
             Rank Degeneracy arid    LEast   Suares Problems

                               Gene Golub*
                           Stan.for University
                          Virginia Ktema**
               National Bureau of Economic Research
                              G. W. Stewart4
                          University of Maryland

                        Working Paper No. 165




 Computer Research Center for Economics and Management Science
          National Bureau of Economic Research, Inc.
                    575 Technology Square
               Cambridge, Massachusetts 02139


                              February 1977


 NBER working papers are distributed infonTally arid in limited
 nmibers for cments only. They should not be quoted without
 written    pennission.
*
 Supported    in   part by Contract No. Anriy DAHC04-75-G-0185
 NSF DCR75—131497.
Supported in part by the National Science Foundation under
  Contract No. IXR-75-08802.

Supported in part by the Office of Naval Research under
    Contract No. N0001'4-76-C-0391.
1. Introduction

       In this paper we shall be concerned with the following problem.

Let A be an m x n matrix with m n, and suppose that A          is near (in

a sense to be made precise later) a matrix B      whose rank is less than

n. Can one find a set of linearly independent col.mins of A that span
a good approximation to the column space of B?

       The solution of this problem is important in a number of applica-

tions. In this paper we shall be chiefly interested in the case where

the columns of A represent factors or carriers in a linear model

which is to be fit to a vector of observations b. In some such applica-

tions, where the elements of A can be specified exactly (e.g. the

analysis of variance), the presence of rank degeneracy in A can be

dealt with by explicit mathematical formulas and causes no essential

difficulties. In other applications, however, the presence of degeneracy

is not at all obvious, and the failure to detect it can result in meaning-

less results or even the catastrophic failure of the numerical algorithms

being used to solve the problem.

       The organization of this paper is the following. In the next sec-

tion we shall give a precise definition of approximate degeneracy in terms

of the singular value decomposition of A. In Section 3 we shall show

that   under   certain conditions there is associated with A a subspace

that   is   insensitive to how it is approximated by various choices of the

columns of A, and in Section 4 we shall apply this result to the solution

of the least squares problem. Sections 5, 6, and 7 will be concerned with

algorithms for selecting a basis for the stable subspace from among the
                               -2-

columns of A.

    The ideas underlying our approach are by no means new. We use the

singular values of the matrix A to detect degeneracy and the singular

vectors of A to rectify it. The squares of the singular values are

the eigenvalues of the         correlation matrix ATA, and the right

singular vectors are the eigenvectors of ATA, that is the principal

components of the problem. The use of principal components to eliminate

colinearities has been proposed in the literature (e.g. see [4,9,16,17]).

This paper extends these proposals in two ways. First we prove theorems

that express quantitatively the results of deciding that certain columns

of A can be ignored. Second we describe in detail how existing compu-

tational techniques can be used to realize our methods.

    A word on notation is appropriate here. We have assumed a linear

model of the form b = Ax + e, where b is an rn-vector of observations

and x is an n-vector of parameters. This is in contrast to the usual

statistical notation in which the model is written in the form y = X3 +

where y is an n-vector of observations and       is a p-vector of parameters.

The reason for this is that we wish to draw on a body of theorems and

algorithms from numerical linear algebra that have traditionally been

couched in the first notation. We feel that this dichotomy in notation be-

tween statisticians and numerical analysts has hindered communication

between the two groups. Perhaps a partial solution to this problem is the

occasional appearance of notation from numerical analysis in statistical

journals and vice versa, so that each group may have a chance to learn the

other's notation.
                                         -3-
       Throughout this paper we shall use               norms. The first is the
                                                      two

Euclidean vector norm    II   112 defined         for an n-vector x by

                                      IIxfl   = i'l x
and its   subordinate matrix    norm    defined by

                                hAil2   = sup llAxil2.
                                              Xlj2l
The second   is the Frobenius matrix          norm    defined for the m x n matrix A by

                                        =
                                IiAll
                                              i=l j=l a..
Both   these norms are consistent in the sense that


                                                            (p =   2,F)
                              iABii         hiAiliiBhl


whenever the product AB is defined. They are also unitarily invariant;

that is if U and V are orthogonal matrices then


                                              =             (p =   2,F).
                      iIAII =
                                IIUTAII           iJAVii

For more on these matrix norms see [14].


2. Rank Degeneracy

       The usual mathematical notion of rank is not very useful when the

matrices in question are not known exactly. For example, suppose that A

is an m x   n matrix that was originally of rank r < n but whose elements
have been perturbed by some small errors (e.g. rounding or measurement

errors). It is extremely unlikely that these errors will conspire to

keep the rank of A exactly equal to r; indeed iihat is most likely is
                                             -4-

that the perturbed matrix will have full rank                   n.    Nonetheless, the

nearness of A to a matrix             of   defective rank      will   often cause it to

behave erratically when it is subjected to statistical and numerical

algorithnis.

        One way of circumventing the difficulties of the mathematical

definition of rank is to specify a tolerance and say that A is numeri-

cally defective in rank        if   to within that tolerance it is near a defective

matrix. Specifically we might say that A has c-rank r with respect

to the norm IIH        if
(2.1)                    r =   inf    {rank(B): A-BfJ          e}.

However, this definition has the defect that a slight increase in c

can decrease the numerical rank. What is needed is an upper bound on
the values of c for which the numerical rank remains at least equal to

r. Such a number is provided by any number                  6 satisfying


(2.2)                    a   < 6 5 sup      {:   IIA-Bli   n     rank(B)   >   r}.

Accordingly    we make the following definition.


        Definition 2.1. A matrix A has numerical rank (6,c,r) with

respect to the norm HI           if    6,c, and r satisfy (2.1) and (2.2).


        When the norm in definition 2.1 is either the 2-norm or the Frobenius

norm, the problem of determining the numerical rank                   of   a matrix can be

solved    in terms   of the singular value decomposition of the matrix. This

decomposition, which has many          applications        (e.g. see {7]), is described in

the following theorem.
                                         -5-
        Theorem 2.2. Let A be an m x n matrix with m                n. Then there
is an orthogonal matrix U or order m and an orthogonal matrix V
of order n such that
                                       T
(2.3)                                 UAV=
                                                        0
where

                              z   =
                                      diag(a1,a2,...,a)
and


                                                    ?a O.
        For   proofs of this theorem and the results cited below see [14]. The

                a2.. . , a, which are unique, are called the singular values
numbers

of A. The columns u1,u2,. •              of U are called the left singular

vectors of A, and the columns v1,v2,...,v are called the right singular

vectors of A. The matrix A             has rank r if and only if


(2.4)                                 > 0 =


in   which case the vectors                         form an orthonormal basis for the
                              u1,u2,.
column space of A (hereafter denoted by R(A)).

        It is the intimate relation of the singular values of a matrix to

its spectral and Frobenius norms that             enables   us to characterize numeri-

cal rank in terms of singular values. Specifically the spectral norm of

A is given by the expression.


                                             =
                                      IIA2       C•l.
                                           -6-

                                          are the singular values of B = A + E, then
Moreover, if
                       2
                                        hEll2
                                                    (i =      1,2,... ,n).

In view of (2.4) this implies           that

(2.5)                         inf        hlA-B112 =      ar+1'
                           rank(B)Sr

arid   this   infijnum is actually attained for the matrix                   B    defined by

                                                 fz'\ T
(2.6)                                   B=U(
where    ' = diag(a1,a2,..    ,o 0,... , 0).

        Likewise

                                    2      2        2                   2
                                                         +         +
                              IAIIF =
                                          OI + a2

and


                              inf        hIA_BIIF
                                                        a
                                                             r+1
                                                                   +   ... + a
                                                                             fl
                           rank(B)Sr
The infimum is attained for the matrix B defined by (2.6).

        Using these facts we can characterize the notion of numerical rank.

In the following theorem we use the notation rank (5er) to mean numeri-

cal rank with respect to the norm IHI.


        Theorem 2.3. Let a1                ...          a be the singular values of A.
Then A has numerical rank (o,c,r)2 if arid only if


(2.7)
                                          -7-
Also A has numerical rank (5, c,r)F if and only if
                     2
                    ar+
                           2
                          r+l+ •••     + n5 2         2
                                                           >C2   r+l+ 2         +   2


        Proof.    We prove the result for the spectral norm, the proof for

the Frobenius norm being similar. First suppose that (2.7) holds. Then

by (2.5) if JIB-All2 < 5 we must have rank (B) >                  r.      Consequently a         satis-
fies (2.2). This also shows that


                          mm    {rank(B): JIB-All                r.
But the matrix B of (2.6) is of rank r and satisfies IIA-B112                               c.

Hence        satisfies (2.1).
        Conversely, suppose 5,, and               r       satisfy (2.1) and (2.2). Then

by (2.5), 5              Also   &
                                    °ri     for if not by (2.1) there is a matrix

B of rank r satisfying IIA-Bli <
                                            r+l'       which contradicts (2.S).a

        Because of the simplicity of the characterization (2.7) we shall

restrict ourselves to rank defectiveness measured in terms of the spectral

norm.

        We shall need two other facts about singular values in the sequel.

First define


(2.8)                     inf(A) =    inf       IIAxII2.



Then


                                      thf(A) =


where            is the smallest singular value of A. Second, let X and Y

be any matrices with orthonormal columns and let                                        >
                                                                                            k be
                                    -8-
                               T
the smgular values of C = X AY. Then


(2.9)
                                        (i =   1,2,... ,k)

and


(2.10)                      k-il °n-ii
                                                  (i =   1,2,... ,k).

3. The a-Section of R(A)

        Having confirmed that a matrix A has numerical rank (o,a,r)2

with r <   n,   one must decide what to do about it. If the singular value

decomposition has been computed as a preliminary to determining the

numerical rank, one solution naturally presents itself. This is to work

with the matrix B defined by (2.6). Because B has an explicit repre-

sentation in terms of Z', the usual difficulties associated with zero singular

values can be avoided. Moreover, the solution so obtained is the exact

solution of a small perturbation of A.

        However, this solution has the important defect that it does not

reduce the size of the problem. For example, if the problem at hand is

to approximate a vector of observations b, the procedure sketched above

will express the approximation as a linear combination of all the columns of

A, even though some of them are clearly redundant. What is needed is a

device for selecting a set of r linearly independent columns of A.

In Sections 5 and 6 we shall discuss numerical techniques for actually

making such a selection. In this section and the next we shall concern

ourselves with the question of when making such a selection is sensible.

        The main difficulty is that there are many different sets of r
                                       -9-

linearly independent columns of the matrix A, and not all these

sets may be suitable for the problem at hand. For example, if the

problem is again that of approximating a vector of observations b,
then for each set of columns we shall attempt to find a vector in
the subspace spanned by the columns that is in some sense a best
approximation to b. Now if       the    subspace determined by a set varies
widely   from set to set, then our approximation to b will not be sta-

ble. Therefore, we turn to the problem of determining when these

subspaces are stable.

      We shall attack the problem by comparing the subspaces with a

particular subspace    that   is determined by the singular value decomposition.

Let   A have numerical rank (6, s,r). Let the matrix U in (2.3) be

partitioned     in the form

                                  U =
                                         (U,I),
where U has the r columns u1,u2,. .. ,ur.           Then we shall call R(U)

the e-section of R(A). Note that the s-section of (A) is precisely the

column space of the    matrix   B defined in (2.6).

      We shall compare subspaces in terms of the difference of the ortho-
gonal proj ect ions upon them. Specifically for any matrix        X   let
denote the orthogonal projection onto R(X). Then for two          subspaces   R(X)
and R(Y) we shall measure the distance between them by IIPx-Py112 (for
the   various geometric interpretations of this number, which is related

to canonical correlations and the angle between subspaces, see [1,2,13]).

It   is known   that if Y has orthonormal columns and          has orthonormal
                                              - 10   -



columns spanning the orthogonal complement of R(X), then


 (3.1)                            'xY112 —   1k Y112.


         The selection of r columns a. ,a. ,. ..              ,a.   from the matrix
                                                 11      12
A =
      (a1,a2,.
                     ..
                          ,a)   has the following matrix interpretation. Let W

be the n x r matrix formed by taking columns                           •      from the n x n
identity matrix. Then it is easily verified that (a. ,a. ,...                     ,a.   )   = AW.
                 T                                                    11 12
Of course W W =1, so that W has orthononnal columns, and this is all

that is needed for the following comparison theorem.


        Theorem 3.1. Let A have numerical rank (5,c,r)2 and let U

be defined as above. Let W be an n x r matrix with orthonormal columns

and suppose that


(3.2)                                     inf(AW) > 0,


where inf(X) is defined by (2.8). Then


(3.3)
                                      IIPuII2
        Proof.       The matrix WTATAW is positive definite and hence has a

nonsingular positive definite square root. Set Y = AW(WTATAW)l/2.                            It

is easily verified that Y has orthonormal columns spanning R(AW).

Moreover, from (3. 2)


(3.4)                                 ITATAl/2I =

The matrix ii              also   has orthonormal columns, and they span the orthogonal
                                              - 11     -




complement of R(Ue)• It follows from                       (2.3)   that


 (3.E)                                      IIUAI2

Hence from (3.1), (3.4), and (3.5)

                          -
                              1'AW
                                     2=   JAW TATAWT) - 1/2112
                      C


                                          !IUTAII2IIWIJ2IIOTATAWYZII2




         Theorem 3.1 has the follow:ing interpretation. The number                   'y
                                                                                          measures

the linear independence of the coltmns of AW. If it is small compared to

IIAWI! then the columns of AW themselves must be nearly dependent. Thus

Theorem 3.1 says that if we can isolate a set of r columns of A that

are strongly independent, then the space spanned by them must be a good

approximation to the c-section R(U).

         However, there are limits to how far we can go with this process.

By (2.8) the number y                satisfies   cYr
                                                           ',   and by the definition of numeri-

cal rank c       r+l• Consequently, the best ratio we can obtain in (3.3) is

C•r+l/(:7r. Thus the theorem is not very meaningful unless there is a well

defined gap between or+l and ar. One cure for this problem is to in-

crease c in an attempt to find a gap; however, such a gap need not exist

 (e.g. suppose            =
                               /2      (i =   1,2,... ,n-1)).       What to do when the matrix

A exhibits a gradual rather than a precipitous decline into degeneracy

is a     difficult problem, whose solution must almost certainly depend on
additional     infoniat ion.
                                       - 12   -



        A second difficulty is that it may be impossible to obtain the

ideal ratio because in practice we must restrict our choice of W to

columns     of the identity matrix; i.e.      we must choose from among columns

of     A.   That this is a real possibility is shown by the following example.


        Example 3.2.    Let e denote the vector (1,1,... ,1)T          with n
components.      The matrix

                                                       T
                                          e' (n)
                                             e
                              An =1 n -           n


has singular values 1,1,. ..,l,0, so that it has numerical rank (1,0,n-l)2.

Thus we should like to remove a single column of A to obtain an approxi-

mation to the 0-section of A. Owing to syunnetry, it does not matter which

column we remove. If we remove the last one, the resulting matrix

has the form



                              A'=E        e(n)
                                             e        (1)T
                               n  n               n

where E       consists of the first n-i columns of the identity matrix.

Thus



                  A'          =    1
                                    [(e1                     "   (e(
                       ir         TL\o /
                                          / e'1
                                   1

                                  niT \n-l
from which it follows that
                                      - 13 -


                            e (n-i)
                      A'
                             T2                A
and


                            y= inf(A1!) :-i


      It should be observed that the factor          n2 exhibited in the

example is not extremely small. For n = 25 it is only 1/5. Unfortunately

no lower bound on y    is   known, although with the computational algorithms

to be described in Sections 5 and 6 it is easy enough to check the com-

puted value.

      A final problem associated with Theorem 3.1 is that it is not

invariant under scaling. By scaling we mean the multiplicative scaling

of rows and columns of A and not additive scaling such as the subtrac-

tion of means or a time factor from the columns of A (this latter

scaling can be handled by including the factors explicitly in the model).

Since by multiplying a column by a sufficiently small constant one can

produce as small a singular value as one desires without essentially alter-

ing the   model, Theorem   3.1 can    be coaxed    into detecting degeneracies that

are not really there. This means that one           must   look outside   the hypo-
theses of Theorem 3.1 for a natural scaling. While we are suspicious of

pat scaling strategies, we think that the following criterion is reason-

able for many applications. Specifically, the rows and columns of A

should be scaled so that the errors in the individual elements of A are
                                   - 14   -




as nearly as possible equal. This scaling has also been proposed in
[4], and an efficient algorithm for accomplishing it is described in

[5].

       The rationale for this scaling is the following. From the defini-

tion of the singular value decomposition it follows that


                            Av =   au.        (i =    1,2,... ,n).

Now if we imagine that our matrix is in error and that our tnie matrix

is A +   E, then

                                         a.u. +      Ev..
(3.6)                       (A+E)v.1      11           3.




If we have balanced our matrix as suggested above, then all of the elements
of E are roughly the same size, and Ev11f2                  IEII. Thus if   hEll2,

equation (3.6) says that up to error v is a null vector of A + E, and

the matrix is degenerate.

        We recognize that this scaling criterion raises as many questions as

it answers. An important one is what to do when such scaling cannot be

achieved. Another question is raised by the observation that in regres-

sion row scaling is equivalent to weighting observations, which amounts

to changing the model.* Is this justified simply to make Theorem 3.1

meaningful? Although this question has no easy answer, we should like to

point out that it may be appropriate to use one scaling to eliminate

colinearities in A and another for subsequent regressions.

 We are indebted to John Chambers and Roy Welsh for pointing this out.
                                         - 15   -



        In   the next   section   we are going      to   examine   the   implications   of
Theorem      3.1 for the linear least squares problem in which a vector of
observations b is optimally approximated in the 2-norm by linear coinbina-
tions of the columns of A:

                                         bAx.
In   sane applications the 2-norm is not the best possible choice, and one

may   wish    to minimize    p (b-Ax),   where p is         a function that may      not   even

be a norm. For example, in robust regression one approach is to minimize

a function that     may reduce the       influence of wild points. We shall not pur-

sue this subject here; but we believe that                 Theorem 3.1     has   important   impli-

cations for these problems. Namely, if we are searching for an approxi-

mation to b in g(A), we cannot expect the solution to be well determined

unless R(A) itself is. Theorem             3.1      provides a theoretical basis for

finding stable subspaces of R(A); however, specific theorems xmist wait

the development of a good perturbation theory for                   approximation in norms

other than the 2-norm.


4. The Linear Least Squares Problem
        In   this section we shall consider the linear least squares problem


(4.1)                     minimize J[b-AxII.

It   is well known      that this   problem always has a solution, which is unique

if and   only    if A is of full column rank. At the solution, the residual

vector


                                  r=b-Ax
                                             - 16 -


is the proj ection of b onto the orthogonal comp1nent of R(A).
     When A has rnmierical rank (5, c,r)2, the solution to (4.1) may
be large, and some of the individual components of the solution will
certainly     have large variances. If            the ratio c/S is sufficiently
small   a stable solution can        be     computed by restricting oneself to the

c-section of A. Computationally this can be done as follows. Define

UC    and     C
                  as in Section     3, and     further define

                          V = (v1,v2,. ..        V)            V    =
                                                                        (vr+i,.   ,v)
and

                                                                        =
                               =
                                   diag(a1,a2,.
                                                       .
                                                           . ,aj,           diag(a1...
Then the matrix       B of (2.6) is given by

                                   B=        2 vT.
                                        U
                                            ccc
Moreover the vector


                          x
                           c
                               =VZUTb
                                 cc c
is    the unique solution of the problem of minimizing



                                            Ib-Bx!12


that    is   of minimum   2-norn. It         is easily seen that


                          rc   =b-Ax c         =b-Bx.c
                                        - 17 -

       As we indicated     in   the last section, this solution is not entirely

satisfactory, since it involves all the columns of A, whereas we might

hope   to obtain a satisfactory representation of b in terms of r
suitably chosen columns; that is with a model having only r carriers.
It    is a consequence of Theorem      3.1 that any      set   of r reasonably inde-
pendent     columns will do, although in practice additional considerations

may make some choices preferable to others.

       Theorem 4.1. Assuming the notation and hypothesis of Theorem 3.1,

let x and rg be defined as above. Let                          be the solution of the

linear least squares problem

                                                    2
                                minimize
                                            IIb-AWy1i2


and   let   r1q be the residual

                                r=    b -
                                            AWy,.

Then

                                Ir-rW2


       Proof. By the properties of the least squares residual

r =                                         Hence
       (IPu)b     and rW =
                             (I-P)b.
                                =
                   Ir- rWII2        (Pu-PAw)bIl2         IJbj2.a


       Theorem   4.1   partially answers a question raised by Hotelling [10];

namely if carriers are chosen to eliminate dependencies, what guarantees
                                                  - 18       -




that   one such set will not fit b better than                                   another?   The answer is

that if there is a well defined gap between 6 and e, then any set of

r strongly independent columns will give approximately the same resi-

dual. However, there remains the possibility that by including more

columns        of A a considerably smaller residual could be obtained. We
stress that such a solution cannot be very stable. By (2.8) any matrix
consisting           of more than r         columns of A must have a singular value

less   than      or equal to         ,   and     it    follows from the perturbation theory

for the least squares problem [15] that the solution must be sensitive

to perturbations in A and b. (Another way of seeing this is to note

that                 is a lower bound for II(ATA) 1112, so that the solution must have

a large covariance matrix.)

       However, one might be willing to put up with the instabilities in the

solution provided it gives a good approximation to b. We shall now show

that any solution that substantially reduces the residual over r                                       is not

only unstable, it is also large.


       Theorem 4.2. Let r be defined as above. Given the vector x, let

r =    b   -   Ax.     If           > 11r112, then
                            IrII2
                                                      II r
                                                             2   1   r   l
                                                                             2
                                         1x112


       Proof. Let z =           Tx       and let



                                                  UTb =          (c
                                                 - 19   -


                                                                                  T T T
where   c    is an   n-vector. Then              if we partition z =            (z ,z ) and
        T T T                           with
c =
      (c,c)     conformally                    the previous partitions of U, V, and

Z, we have

                         2             T
                     11r112
                              = IIU (bAWTx)II


                              =      I  - (/z\              J2
                                     \d/
                                   I JIc\        \O, 112

                              =
                                   lIc-2zII + IldII

                                                                          2
                              =    lic -z z      + fl -z z          + IldIl V


Consequently


(4.2)                                  2>      - £112  + IIdfl.
                              11r112               c 2


Now   the   vector y =        vT e is       given by


                                               I Z1c
                                        yc =



so that


(4.3)                         hr
                                   C   II = hIhh + hIdhI.

From (4.2)


                              - hIdhI                                      *
                  VjrfJ                2
                                            Icc-zCC
                                                  z 112          hhchI2 - Ix 1111211
                                                                              eZ e2

                                            hIhI - £JIZ 112•
                                         - 20   -




Hence

                                        lCl2-/rIl- lid il
                 1k   112   lIll2

and fran (4.3)


                            /r,I2-ild
                                           - 4i?2-!ldll
                  lxii 2

                                    - 11r112
                            lrl


                                                                      must
        The theorem shows that even a slight decrease in the residual

result in a great increase in the size of the solution.          It is hardly

necessary to add that a large solution is seldom acceptable in practice:

it must have high variance, and it may be physically meaningless.

        The results of this section have implications for a common practice

in data analysis, namely that of fitting a large number of subsets of the

columns of A in an attempt to obtain a good fit with fewer than the full

complement of columns (for example, see [6]). We have, in effect, shown

that if the ratio             is    reasonable, this procedure is not likely to be

very productive. Any set of r independent columns will give about the

 same residual, and any larger set that significantly reduces the residual

must produce an unacceptably large solution. There are, however, two cases

where this procedure might be of some help. First when it is hoped that

 fewer than r columns can produce a good fit, and second when the c-6

 ratio is not very small. An approach to the second problem that uses the

 singular value decomposition of the augmented matrix (A,b)           is described

 in [9] and [16,17].
                                      - 21   -



5.   Extraction of Independent   Columns: the         QR Factorization

     We now turn to the problem of extracting a set of numerically inde-

pendent columns. The first method we shall consider is based on the QR

factorization of the matrix     A.    Specifically, if A is an m x n matrix

with m n, then A can be written in               the form


                                      A = QR,


where Q    has orthonormal columns (QTQ_1) and R            is upper triangular.

If A has     full   column rank, then the factorization is unique up to the

signs of the Lolumns     of Q   and   the    corresponding rows of R. It should

be noted that    the   columns of Q    form an orthonormal basis for R (A).
     A knowledge of the QR factorization of A enables one to solve the

least squares problem (4.1). Specifically, any solution x of (4.1)

must satisfy the equation


                                     Rx = QTb,


which   can be   easily solved since R is upper triangular. Moreover, since

ATA = RTR, we have


                             (ATA)l =    R1RT

so that one can use the matrix R in the factorization to estimate the
covariance   matrix of   the solution.
     An especially desirable feature          of   the QR factorization is that it

can be used to solve a truncated least squares problem in which only an
                                      - 22   -




initial   set of columns are fit. If A denotes the matrix consisting

of the first r columns of A and R11 denotes the leading principal

submatrix of order r of R then


(5.1)
                             AIr =   QIrRi

Since Rir is upper triangular and Q" has orthonormal columns,

equation (5.1) gives the QR factorization of A and can be used as

described above to solve least squares problems involving AIr.

        The basis for using the QR factorization to extract a linearly

independent set of columns from the matrix A is contained in the

following theorem.


        Theorem 5.1. Let the QR factorization of A be partitioned in the

form

                                                 /R1l R12
                         (A1,A2) =
                                                  0


where A1,Q1 E xr and R11 E Rr<r              If


                              IIR22I2 = c    < 6 =
                                                   inf(R11),

then A has rank (6,c,r)2. Moreover,

                                                  = 6.
                                     inf(A1 •)


        Proof.    Because the columns of Q            are orthonormal, the singular

values    of     A and of R are the same. Now 6 is the r-th singular

value of R11, and hence by (2.9) 6 is less than or equal to the r-th
                                          - 23   -




singular   value of A; i.e. °r >           o• Likewise      from (2.10), e
                                                                                 c7r+l.

Thus   A    has rank (8, ,   r).       Moreover, since Q1 has orthonormal columns,


                                   =                 =              =
                     inf(A1)           inf(Q1R11)        inf(R11)

       The application of this theorem is obvious. If, after having

computed the QR factorization of A, we encounter a small matrix R22

and a matrix          with a suitably large infinuin, then the columns of

A1 span a good approxination to the €-section of A.                         Because

of (5.1), we have at hand the QR factorization of A1 and can proceed

immediately to the solution of least squares problems involving A1.

There remain two problems. First how can one insure that the first r

columns of A are linearly independent, and second how can one estiiiate

inf(R11)?
       The solution to the first problem depends on the method by which the

QR factorization is computed. Probably the best numerical algorithm is

one based on Householder transformations in which the QR factorizations

A1k = Qik.J
        K are       computed successively for k =              1,2,... ,n     (e.g. see [14]).

At the k-th step, just before Qik and R are computed, there is the

possibility of replacing the k-th column of A by one of the columns

                      If the column that maximizes the (k,k) - element of
ak+],ak+2,..
R is chosen to replace ak, then there will be a tendency for indepen-

dent columns to be processed first, leaving the dependent columns at the

end of   the matrix. An ALGOL           program incorporating           this "column pivoting"
is   given in [3] and a FORTRAN program is given in (11].
                                         -24-

       Once   a satisfactory QR decomposition has been calculated,            we can

estimate      ItR22fI2 by the bound


                                         5
                              I!R22112       v11R22JJ1IIR22IL,


where


                                    = max       E x.. J
                                         j      i

and


                              IX__ =max         Z    x..I.
                                      •
                                         13     •     13



Likewiseone can estimate inf(R11) by computing R1 (an easy task
since R11 is upper triangular) and using the relations

                        inf(R )    =     -1-1                   1
                                                        ___________
                                                          11    1 11

       The procedure sketched     above is completely reliable in the sense

that   it cannot fool one into thinking a set of dependent co1tns are
independent. However, it can fail to obtain a set of linearly indepen-

dent columns, as the following example shows.


       Example   5.2.   Let        be the      matrix     of   order n illustrated below

for n = 5:
                                    - 25   -




                1       -i//2•     -i/v        -l/       -i/v'

                o        l/v7      -l/v        -l/ -l/
                o        0          1/vs       -'/14

                o        0          0           l/v
                o        0          0           0



Letting x =    (l,v'2/2,v'/4,/4/8,...               it is easily verified that


                                 Ax
                                  nfl
                                      =2%

where eT      (1,1,... ,l). Thus    A      has the approximate null vector x

and must have nearly dependent columns.        However, computing the QR factori-
zation of       even with column pivoting, leaves A undisturbed. Since

no element of A is very small, we shall have R22 void; i.e. no depen-

dent colunm will be found.

      It should be observed that    in   the above example there is no danger

of the degeneracy in An going undetected. Since R22 is void, R =                 A
and   any attempt   to estimate inf(R11) will reveal the degeneracy.

      It may be objected that the matrix A in Example 5.2 shows an

obvious sign of degeneracy; viz, its determinant         (n!)2 goes rapidly
                                                                 obtained from
to zero with increasing n. However, the matrx            lAI
by taking the absolute value of its elements, has the same determinant

yet its columns are strongly independent. Thus the example confirms a

fact well Iciown to practical computers: the value of a determinant is

worthless as an indication of singularity.
                                       - 26       -




6.    Extraction of Independent Columns: the Singular Value Decomposition

      When   the singular value decomposition of A has been computed (an
ALGOL program is given in [8] and           a    FORTRAN program in [11]), a different

way of   selecting independent columns is available. The method is based on
the   following theorem.


       Theorem 6.1. Let A have the singular value decomposition


                              T            fz
                             UAV=(

Let V be partitioned in the form



                             v= (           ci        ci.


                                            a2
                                                      v

where V1 is r x       r,   and let A be partitioned in the form


                                     A =
                                            (A1,A2),

                                                            e =          and
where A1 has r columns. Let 6 =                   °r'             ar+l
                                           = 6
                                                 inf(V1).

Then A has numerical rank (6,c,r)2 and


(6.1)                                                     y.
                                      inf(A1)


        Proof.   The fact that A has             numerical        rank (5,c,r)2 follows
ijTunediately from Theorem    2.3.     To establish (6.1), observe that if we
write
                                         - 27 -

                               AV =      = (S1,S2)

where S1 has r columns, then SS2 = 0.                     Now since A =   svT, we have

                                    -     T         AT
                                1       id           ci


        T    = 0,
Since S1S2


                    in.f(A1)   inf(S1VT1)         inf(S1)inf(VT1)

                                    =   ar inf(V ) =
                                                ci


      As with the QR factorization, Theorem 6.1 provides us with a way of

detennining when an initial set of r columns of A are independent.

Since an initial set may be degenerate, we must adopt some kind of inter-

change strategy to bring an independent set of columns into the initial

positions. If P is any pennutation matrix, then


                                T         T         fz\
                               U    (AP)(P V) = I


so   that in the singular value decomposition an interchange of columns of
A corresponds to an interchange of the corresponding rows of V. This
suggests that we exchange rows of V until inf(V1) becomes acceptably
large. One way of accomplishing this is to start with the r x n matrix

                                    =
                                        CVT1,VT2)

and   compute its QR factorization with column pivoting to force a set

of   independent columns into the first r positions. Alternatively one
                                  - 28   -




could   apply an algorithm such as Gaussian elimination with complete

pivoting to V (e.g. see [14]).
        If either of the above suggestions is followed, the final matrix

V1 will be upper triangular, and its infimum can be bounded by the method

suggested in the last section.

        If r is small,   significant savings can   be obtained by observing

that the singular values in [0,1) of V1 and 2 are the same (see

the appendix of [15] for a proof). Thus one can start with the smaller

matrix


(6.2)                      V' = (\?T1,T2)

and use the QR factorization with column pivoting to determine the

dependent columns of A. Note that when r =      n-l   the column to be stricken
                                                            T
corresponds to the largest element of the row vector V2.

        The question of whether to use the QR factorization or the singular

value decomposition is primarily one of computational efficiency. Although

Example 5.2 shows that the QR factorization can fail to isolate a set of

independent columns in a case where the singular value decomposition does,

this is an unusual phenomenon (see Example 7. 2) and in most cases the QR

factorization with column pivoting is effective in locating independent

columns. When m is not too much greater than n, the calculation of the

singular value decomposition is considerably more expensive than the

calculation of the QR factorization, and it is more efficient to stick with

the latter, if possible.
                                            - 29 -

       When m >>   n,   we can begin by computing the QR factorization of A.

The   matrix R     has the   same singular values as A, and              indeed if


(63)                                    UTiw =


is the singular value decomposition of R, then V                       is the matrix of

right   singular vectors of A. Since R is an n x n matrix, the reduc-
tion (6.3)   is computationally         far less expensive than the initial com-
putation   of R, and there seems to be no reason not to use the singular

value decomposition.


7. Examples

        In this section we shall give some examples illustrating the pre-

ceding material. The numerical computations were done in double precision

on an IBM 360; i.e. to about sixteen decimal                 digits.

       Example 7.1. This example has been deliberately chosen to be un-

complicated. For fixed         n, let
                               H
                                n
                                    =   i   -
                                                eeT,
                                                n

where eT =    (1,1,... ,l).   It is easily verified that Hn is orthogonal.

Let


                             Z =   diag(1,l,l,1,1,0,0,0,0,0)

and


                               A =
                                     H50              H10.
                                                (z)
                                   - 30       -




Then A has five nonzero singular values equal                 to   unity   and   five zero

singular values, and thus it should have five linearly independent

columns.

      The singular values of A were computed to be l,l,l,l,l,.35xl0,

0,0,0,0, so that A can be regarded as having rank (l,c,S) where

e =   io16.   The pivoting strategy described in Section 6 was used to

isolate a set of five linearly independent columns. These turned out to

be columns 1,2,4,5, and 9. The associated matrix V1 had an infijnum

of .45 which is very close to the optimal value of unity. As a final

check, we compute 'UAW11' where W =                                        is the matrix
                                                    (e1,e2,e4,e5,e9)
that selects the independent columns from A (cf. Theorem 3.1). The

result is


                          UAW2 =                  .37 x


which shows that columns 1,2,4,5, and 9 of the matrix A almost exactly

span the c-section of A.

      The QR factorization with column pivoting that is described in Sec-

tion 5 was also applied to A. The pivot columns and                   their   norms were

                               S          .89
                               4          .86
                               2          .81
                               3          .71
                               6          .44             -
                               1          .45 )(10
                               7          .13 x 10
                               8          0
                               9          0
                              10          0
                                          - 31      -



If the gap is taken to lie after the fifth vector we have
                             =   1,    1R22112 =          .20 x io16
                 inf(R,11)


Thus the QR factorization exhibits the same sharp gap as the singular
value decomposition. However, the five columns 2,3,4,5, and 6 desig-

nated as independent are different from those chosen by means of the

singular value decomposition. Nonetheless, for W =
                                                                          (e2,e3,e4,e5,e6)
we have


                                      =             x
                        UAW1
so that this choice of columns            is       as good as the one predicted by the

singular value decomposition.

    Incidentally the esti.mate of 1R22112                     using the 1- and -nonns is

                                               =    .94   x


which is not a gross overestimate.


    Example     7.2.   This is the matrix A25 of Example                    5.2.   The singular

values of this matrix        are

                                          ,a24=.31,cr25=.77 x


Again   there   is a well defined gap, and we may                  take   A to have rank

(.3l,,24)   where c =    l0'.         This time           there   is only a single dependent

vector   which can be   found      by looking for             the largest component of the

right   singular vector v25           corresponding           to c5 (cf. the coments at
equation   (6.2)). This      component, . 75, is the first, which indicates
                                    - 32   -




that   column one should be discarded. For this selection we have


                           'UAW'12 = .49 x lO.

       In principle, the QR factorization should fail to isolate a depen-

dent column of A25. However, because the elements of A25 were

entered with rounding error, the pivot order with column norms turned

out to be


                               1           1.0
                               25           .98
                               6            .88




                               24           :37        -
                                2           .15 x 10


This again gives a well defined gap and indicates that column 2 should

be thrown out (the second component of v25 is .53 so that also from the

point of view of the singular value decomposition the second column is

a candidate for rej ection). For this subspace we have


                                       =    .11 x io6.
                             UCAW
Thus the QR factorization gives only slightly worse results than the

singular value decomposition, in spite of the fact that the example was

concocted to make the QR decomposition fail.


       Example 7.3. To show that our theory may be of some use even where

there is not a sharply defined gap in the singular values, we consider the

Langley test data [12], which has frequently been cited in the literature.
                                    - 33   -




Since it is a common practice to subtract means from raw data, we have
included a column of ones in the model. Specifically the columns of

A are as follows:

          1 --   ones
          2 - - GNP Implicit Price Deflator, 1954 - 100

          3 - - GNP

          4 --   Unemployment

          5 -   - Size   of armed forces

          6 --   Noninstitutional   population      14 years old

          7 --   Time    (years)

The scaling of this data will critically affect our results. For the

purposes of this experiment we assume that columns two through six are

known to about three significant figures. Accordingly each of these

columns was multiplied by a factor that made its mean equal to 500.

The column of ones is known exactly and by the equal error scaling

criterion ought to be scaled by a factor of infinity. As an approxima-

tion we took the scaling factor to be 1010.

    The column of years can be treated in two ways. First the errors in

the time of measurement can be attributed to the column itself, which

would result in the column being assigned a low accuracy. However, we

observe that any constant bias in the time of measurement is accounted

for by the column of ones, and any other errors can be attributed to the

measured data. Consequently we have preferred to regard the years as

known exactly and scale the seventh column by iol0.

    The   singular values of the matrix        thus scaled   are
                                 - 34     -




                               .78    x iol4
                               .94       io8
                               .58 x 1O3
                               .26 x 1O3
                               .26 x io2
                               .22       io2
                               .51 x 101


Since the error in A is of order unity, the last singular value must

be regarded as pure noise, and we may take A to have rank (22,5.1,6)2.

The largest component of the seventh singular vector is the sixth and has

a. value of .90. When the sixth column is removed from the matrix, the

resulting subspace compares with U51 as follows:

                                     =   .12.
                     'U51AW112
      The relatively poor determination of the 5.1-section of A suggests

that   not much useful information can be obtained from a least squares
fit, even when the sixth column is ignored. The next gap that presents
itself is between the   fourth and fifth singular values. If we   regard A
as   having rank (260,26,4)2 and use the pivoting strategy of Section 6 to

isolate a set of four independent columns, we choose columns 1,4,5, and 7

with


                                                  .991.
                               inf(V1)

For   this choice of columns

                                           =
                          'IJ26OPAW11           0.011,
                                  - 35   -




a far more satisfactory result.

    If the QR factorization is applied to A, there results the follow-

ing sequence of pivot    columns and   norms:

                           7     .78 x io14
                           1     .94X108
                           5     .47 x 1O3
                           4     .31 x 1O3
                           2     .24 x 102
                           3     .21 x io2
                           6     .57x101

This agrees completely with the results from the singular value decomposi-

tion. Either one or three columns should be discarded, and columns 6, 2,

and 3, in that order, are candidates.

    Although these results indicate that columns 2, 3, and 6 should be

discarded from the model, they are not conclusive, since there may be

other sets containing some of these columns that give a satisfactory

approximation to the 260-section of A. However, a singular value decompos-

ition of the matrix consisting of columns 1,2,3,6, and 7 gives the singular

values

                                .78 x    io14
                                .94
                                .50 x  io2
                                 .25 io2
                                 .10 x io2

which    shows that none of these columns    is   a really good   candidate for   inclu-

sion in the model.
                                    - 36   -




    To sum  up: if the raw Longley data is taken to be accurate to three
significant figures, if years are assumed to be exact, and if means are

subtracted from the columns, then the column corresponding to noniristitu-

tional population is redundant, and        the   columns corresponding to the GM'

inplicit price deflator and   the   GNP are so nearly redundant that their

inclusion in the model will affect the stability of the residuals from any

regressions.
                                - 37   -

                              References

1. S. N. Afriat, Orthogonal and oblique projectors and the
         tics of pairs of vector spaces, Proc. Cambridge Philos. Soc. 53
           (1957) 800-816.

2. A Bjork   and G. H. Golub, Numerical methods for computing angles between
          linear subspaces, Math. Comp. 27 (1973) 579-594.

3. P. Businger and G. H. Golub, Linear least squares solutions by
         holder transformations, Numer. Math. 1 (1965) 269-276.

4. J. M. Chambers, Stabilizing linear regression against observational
         error in independent variates, unpublished manuscript, Bell
         Laboratories, vkirray Hill, New Jersey (1972).

 S. A. R. Curtis and J. K. Reid, On the automatic scaling of matrices
          for Gaussian elimination, J. Inst. Math. Appi. 10 (1972) 118-124.

 6.   C. Daniel and F. S. Wood, Fitting Equations to Data, Wiley, New
           York (1971).

 7.   G. H. Golub, Least squares, singular values, and matrix
           tions, Aplikace Matheinatiky 13 (1968) 44-51.

 8.   _________ and C. Reinsch, Singular value decomposition and least
           squares solution, Nuiner. Math. 14 (1970) 403-420.

 9. D. M. Hawkins, On the investigation of alternative regressions by
          principal component analysis, Appi. Statist. 22 (1973) 275-286.

10. H. Hotelling, The relations of the newer multivariate statistical
                                                                   —
          methods to factor analysis, Brit. J. Statist. Psychol. 10 (1957)
           69-79.

11.   C. L. Lawson and R. J. Hanson, Solving Least Squares Problems,
           Prentice-Hall, Englewood Cliffs, New Jersey (1974).

12. J. W. Longley, An appraisal of least squares programs for the electronic
          computer from the point of view of the user, J. Amer. Statist.
          Assoc. 62 (1967) 819-841.

13. G. W. Stewart, Error and perturbation bounds for subspaces associated
          with certain eigenvalue problems, SIAM Rev. 15 (1973) 727-764.

14. ____________,    Introduction   to Matrix Computations, Academic Press,
           New York (1973).
                                - 38   -



15.   _____________, On the perturbation of pseudo-inverses,
          tions and linear least squares problems, to appear SIAM Rev.

16. J. T. Webster, R. F. Gunst, and R. L. Mason, Latent root regression
         analysis, Technoinetrics 16 (1974) 513-522.

17. _____________, A comparison of least squares and    latent   root
          sion estimators, Technometrics 18 (1976) 75-83.
