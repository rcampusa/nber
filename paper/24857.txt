                              NBER WORKING PAPER SERIES




          THE PROMISE AND PITFALLS OF DIFFERENCES-IN-DIFFERENCES:
         REFLECTIONS ON ‘16 AND PREGNANT’ AND OTHER APPLICATIONS

                                       Ariella Kahn-Lang
                                          Kevin Lang

                                      Working Paper 24857
                              http://www.nber.org/papers/w24857


                     NATIONAL BUREAU OF ECONOMIC RESEARCH
                              1050 Massachusetts Avenue
                                Cambridge, MA 02138
                                     July 2018




We are grateful to the five authors of the original papers on 16 and Pregnant and to Ivan
Fernandez-Val, Shulamit Kahn, Jonathan Roth, and Adrienne Sabety for helpful comments
and suggestions. The usual caveat applies with particular force in this case. The views expressed
herein are those of the authors and do not necessarily reflect the views of the National Bureau of
Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

© 2018 by Ariella Kahn-Lang and Kevin Lang. All rights reserved. Short sections of text, not to
exceed two paragraphs, may be quoted without explicit permission provided that full credit,
including © notice, is given to the source.
The Promise and Pitfalls of Differences-in-Differences: Reflections on ‘16 and Pregnant’
and Other Applications
Ariella Kahn-Lang and Kevin Lang
NBER Working Paper No. 24857
July 2018
JEL No. C18,C21,J13

                                           ABSTRACT

We use the exchange between Kearney/Levine and Jaeger/Joyce/Kaestner on “16 and Pregnant”
to reexamine the use of DiD as a response to the failure of nature to properly design an
experiment for us. We argue that 1) any DiD paper should address why the original levels of the
experimental and control groups differed, and why this would not impact trends, 2) the parallel
trends argument requires a justification of the chosen functional form and that the use of the
interaction coefficients in probit and logit may be justified in some cases, and 3) parallel trends in
the period prior to treatment is suggestive of counterfactual parallel trends, but parallel pre-trends
is neither necessary nor sufficient for the parallel counterfactual trends condition to hold.
Importantly, the purely statistical approach uses pretesting and thus generates the wrong standard
errors. Moreover, we underline the dangers of implicitly or explicitly accepting the null
hypothesis when failing to reject the absence of a differential pre-trend.


Ariella Kahn-Lang
Harvard University
177 Winchester St
Brookline, MA 02446
Ariella_Kahn-Lang@hksphd.harvard.edu

Kevin Lang
Department of Economics
Boston University
270 Bay State Road
Boston, MA 02215
and NBER
lang@bu.edu
1         Introduction

While it has a long history that predates its renaissance as a leading approach in empirical
economics,1 differences-in-differences (DiD) is plausibly the showpiece of the credibility rev-
olution in empirical economics. Angrist and Pischke 2010 describe it as “probably the most
widely applicable design-based estimator.”
        Describing DiD, certain instrumental variables, and regression discontinuity estimators
as techniques for conducting ‘natural experiments’ can be an incredibly effective rhetorical
device. However, this terminology obscures the fact that these approaches are typically
not design-based estimators but rather solutions for a lack of a randomized experimental
design. In a true randomized experiment, barring terrible luck, the baseline characteristics,
including the baseline value of the outcome variable, of the experimental and control groups
are similar. While controlling for baseline values may (or may not) increase the precision of
the estimated treatment effect, there is no need for differences-in-differences. Simple single-
differences provide an unbiased estimate of the treatment effect. We only need DiD because
‘nature’ did not conduct a randomized trial for us.
        This is not a criticism of DiD or other ‘natural experiment’ approaches. Instrumental
variables, for example, can be a very useful solution to a lack of randomized experimental
design. But we note that in the case of instrumental variables, economists have put a lot of
thought into the interpretation of the results when instrumental variables is used to correct
this.2 In the case of DiD, similar thought is necessary for considering the interpretation of
results; this requires both logical and statistical evaluation of the identification assumptions
in DiD.
    1
     The earliest use we have been able to find is John Snow’s 1855 work on death rates from cholera in
London.
   2
     As discussed in Lang 1993 and formalized in Angrist and Imbens 1994, under certain conditions the
instrumental variables estimator captures the local effect of treatment on those individuals whose treatment
status was affected by the instruments, which Imbens and Angrist called the local average treatment effect.
This is a different experiment than the one in which treatment is randomized. This point is perhaps even
more important when assessing the results of studies based on regression discontinuity. In both cases, as
argued by Angrist and Pischke, similar results from many studies in which very different groups are affected
give us more confidence in the external validity of the results.


                                                     2
   Importantly, most of the issues we discuss are familiar from standard regression analysis.
Indeed, since DiD has a regression representation, it cannot inherently provide more com-
pelling evidence of a causal effect than regression analysis does. We face the same issues of
whether the model is properly specified and whether, conditional on the controls, the variable
of interest is orthogonal to the error term. In this case, the controls include membership in
the experimental or control group and time period indicators for before or after the ‘experi-
mental’ intervention. The variable of interest is the interaction between experimental group
and post-intervention. And, with limited data, we face the usual risks of overfitting and
loss of power when we try to address potential correlation with the error term by including
further controls.
   We use the papers on “16 and Pregnant” by Kearney and Levine 2015, hereafter KL, and
Jaeger, Joyce, and Kaestner 2018, hereafter JJK, as a jumping off point for reexamining the
use of DiD as a response to the absence of an experimental design with randomly assigned
treatment. KL examine the impact of the introduction of the show on teen pregnancy rates.
Using a DiD design comparing areas with differing MTV viewership prior to the introduction
of 16 and Pregnant, they find that the show led to a 4.3 percent reduction in teen births.
JJK contend that the identification used by KL was flawed, primarily because pre-trends
were not parallel, making the assumption of common trends under the counterfactual, as
required for DiD identification, less plausible.
   We choose these papers not because they provide egregious examples of errors, which they
do not, but because they are thoughtful but different approaches that help to underscore
some of the issues we raise. We note that KL and, therefore, JJK are somewhat unusual in
the DiD literature for two reasons. First, they have a continuum of treatments rather than an
experimental and a control group. Second, to address potential selection in 16 and Pregnant
viewership, they rely on prior MTV viewership in the relevant time slot as an instrument.
When we relate our analysis to their exchange, we will focus on the reduced form which
is the relation between the change in teen pregnancy and potential viewership. Consistent


                                               3
with our approach, much of their analysis and debate concerns whether the relation between
(potential) 16 and Pregnant viewership and teen pregnancy changed after the show went on
the air. Because our objective is to make general points rather than comments specifically
on the exchange between KL and JJK, for consistency with the usual DiD framework, we
generally discuss their findings as if there were a single treatment and therefore separate
experimental and control groups.
   We make three principal points:


  1. Any DiD paper should address why the original levels of the experimental and control
     groups differed or, in other words, why the experimental design failed. The researcher
     should then provide justification for the assertion that the same mechanism would
     not impact trends. If the researcher believes that the groups did not differ before the
     intervention, that, too, must be established. In a regression framework, this suggests
     controlling for time-varying characteristics that are correlated with group membership
     and for the interaction of these characteristics with time or post-intervention. However,
     this may also reduce power, making it difficult to reach any conclusions.

  2. Determining that two groups would have experienced parallel trends requires a justifi-
     cation of the chosen functional form. It is a mistake to view this purely as a statistical
     process. We argue that this decision should be made such that the estimated treatment
     effect is consistent with the perceived counterfactual trends and note the role of ‘theory’
     in this process. In particular, in contrast with Ai and Norton 2003, we argue that it
     is frequently appropriate to use the probit or logit coefficients on the interaction term
     in a DiD model. Importantly, it can matter whether we believe the ‘correct’ model is
     a linear probability model, probit or logit since they assume different counterfactuals.

  3. Simply comparing the trends in both groups prior to intervention, or “pre-trend test-
     ing,” is insufficient to establish parallel trends as the appropriate counterfactual in the
     treatment period. The existence of parallel trends in the period prior to treatment

                                              4
      is suggestive of parallel trends in the treatment period, but it is neither necessary
      nor sufficient for the parallel trends condition to hold. We argue that the presence
      of parallel trends in the pre-period does not guarantee these trends would have con-
      tinued in the absence of treatment. Further, failure to reject the null hypothesis of
      non-parallel trends does not confirm the existence of parallel trends (type II errors).
      In a regression context, it is natural to consider whether adding a linear (or other)
      trend interacted with group membership changes the results. Related to this, we also
      note that the purely statistical approach uses pretesting and thus generates the wrong
      standard errors.



2     DiD in the Absence of a Randomized Experiment

In the potential outcomes framework, we write E (Ygt |D = 1) to represent the expected
outcome of group g in year t if it is treated and E (Ygt |D = 0) if it is not. A standard
experiment ensures that, subject to sampling variation, potential outcomes for the control
and treatment group are the same with and without treatment. Letting S = 1, denote
membership of group g in the treatment group, this means that


                         E (Ygt |D = 1, S = 1) = E (Ygt |D = 1, S = 0)                    (1)


and
                         E (Ygt |D = 0, S = 1) = E (Ygt |D = 0, S = 0) .                  (2)

    A problem arises when persistent factors that may be correlated with the outcome of
interest are correlated with membership in the experimental group, that is that (1) and/or
(2) does not hold. In this case we do not have a properly designed experiment. The key
to identification in a DiD is that although outcome levels differ in the pre-period, outcomes
between the pre-period and the treatment period (denoted by 0 and 1) would have moved


                                               5
in parallel in the absence of treatment, that is




                           E (Yg1 |D = 0, S = 1) − E (Yg0 |D = 0, S = 1)

                       = E (Yg1 |D = 0, S = 0) − E (Yg0 |D = 0, S = 0) .                   (3)


   The implicit underlying assumption of this model is that there is some initial difference
between the groups that shifts the dependent variable vertically without affecting the slope
of the time trend. This assumption must be justified, which requires discussion of why
levels varied in the pre-period and how this should influence our interpretation of likely
counterfactual changes.
   When we do not have an experiment,there may be a case that assignment is as good
as random. In this case, DiD can still be useful for three reasons. First, controlling for
pre-period outcomes may be beneficial in that it improves precision and therefore efficiency.
Second, because DiD requires weaker assumptions than single-differences, DiD is still a valid
design even if it is unnecessary. Lastly, DiD techniques used for assessing the parallel trends
assumption can be used for testing the assumption of no systematic differences between
groups in the absence of treatment. Our discussion of testing for pre-trends in section 4
applies in this case as well.
   Although the model can be written more generally, we present the model in its more
common forms.
                                E (h (ygt ) |g, t, D) = βDgt + γt + αg .                   (4)

This can be represented in regression form


                                 h (ygt ) = c + βDgt + γt + αg + εgt .                     (5)


   In a true experiment, E (αg Dgt |t) = 0. Therefore by estimating the model only on the


                                                   6
post-period, leaving out the group dummies, which are perfectly collinear with D, we can
obtain a consistent estimate of β. When we do not have a true experiment, the assumption
that E (αg Dgt |t) = 0 is usually less compelling, and in such cases we need to control for
S, membership in the (eventually) treated group. Note that t is correlated with D by
construction. So unless we have strong evidence that there are no time effects, we must also
control for time in the regression. In a true experiment or in DiD, including the full set of
α terms, when not perfectly collinear with D, may be helpful if they absorb enough of the
error variance.
   A common case, for example probit or logit models, is when y is not observed but is a
latent variable, y ∗ , where ygt
                              ∗
                                 > 0 → Ygt = 1 and Ygt = 0 otherwise. For the most part, our
concerns will be similar except that the incidental parameters problem precludes using the
full set of α terms rather than S unless the number of observations in each group is large. If
not, the α terms must be replaced with a dummy variable for being part of the experimental
groups. However, there are some issues specific to probit or logit which we address below.
   Nothing in this section is original to us, but it serves to fix ideas and notation. Presenting
DiD in a standard regression framework helps us make our points more simply and allows for
an easy extension to a continuous treatment and even instrumental variables as in the studies
of 16 and Pregnant. Importantly, identification of a causal treatment effect is dependent on
the assumption that E (εD|t, S) = 0 (or E(εZ|t, S) = 0 for the case of IV), the regression
counterpart to the parallel trends assumption.
   The debate between KL and JJK focuses on whether the pre-period experience suggests
that a common counterfactual trend is plausible. Therefore, both papers implicitly accept
the view that they do not have a well defined experiment and that this, in turn, creates
challenges for causal inference that must be addressed. This means that the credibility
with which we can interpret β as a causal effect depends on how confident we are that
E (εD|t, g) = 0. Both sets of authors are aware of this requirement for causal interpretation
but come to different conclusions about its plausibility.


                                               7
3     Choosing a Counterfactual Functional Form

In this section, we will proceed with the assumption that, in the absence of the intervention,
the variable of interest would have changed similarly in the control and experimental groups.
But what does ‘similarly’ mean when the groups are different initially? When using a DiD
model, we need not only to establish that the two groups would have moved similarly, but
also that these patterns would have been consistent with the functional form of the chosen
model specification. As Meyer 1995 points out, unless the distribution of outcomes is initially
the same for the experimental and control groups, the effect of any changes associated with
time cannot be the same both if the model is specified in, for example, levels and if it is
specified in logarithms. Note that this is true even if the means are initially equal provided
that the distributions differ. Choosing the functional form for (5) is a key decision on the
part of the researcher and requires justification.
    Some common functional form assumptions in DiD models are that group outcomes
would have moved by the same absolute amount, by the same percentage, or according to a
logit/probit model. While all of these assumptions are plausible in certain contexts, it is often
far from obvious which functional form properly represents the counterfactual. For example,
if the probability of some event in the control group increases from .80 before the treatment
period to .82 after treatment, it is not obvious what our counterfactual should be. Say the
treatment group had a probability of .5 of this event in the pre-period. Since, on net, two
percentage points of the control group shifted, perhaps we should expect a counterfactual
probability of .52 in the treatment group. This would be consistent with the standard
linear (probability) model, in which groups move by the same absolute amount under the
counterfactual. Alternatively, since (at least on net) 10 percent of those in the control group
who had a value of zero in the pre-period shifted to a value of one, we might expect 10 percent
of the zeros in the experimental group to have shifted so that our counterfactual is a rise from
.50 to .55. Logit and probit fall in between these two counterfactuals. Based on the shape
of the logistic and normal distributions, both predict a counterfactual of approximately .53.

                                               8
We note that probit assumes that in the counterfactual, the control and treatment groups
would have moved by the same number of standard deviations (of the standard normal error)
while logit makes a similar assumption in terms of logits. Absent a theory, it is hard to make
a strong case for any of these three counterfactuals. If they give different answers, we should
have less confidence about drawing strong conclusions.
   In a highly-cited and influential paper, Ai and Norton 2003 point out that the coefficients
from probit or logit estimation of a DiD model give the ‘wrong answer.’ But the answer is
wrong only in the sense that the change in absolute probability between the pre-period and
treatment period can differ between the control and experimental groups even when the
interaction term in probit or logit is zero. Similarly, a significant interaction term from
probit or logit need not mean that the change in absolute probability differed between the
two groups.
   Implicit in Ai and Norton’s argument is that the counterfactual should be a constant
absolute change in the probability of an event between the pre-period and treatment pe-
riod, consistent with the standard linear model. We have no disagreement with the for-
mal part of Ai and Norton’s argument, but we believe that this is more a question of
choosing the appropriate counterfactual functional form than a simple question of ‘right’
or ‘wrong.’ If researchers choose to use a nonlinear model to estimate the DiD, presum-
ably they believe that this model accurately captures the effect of the explanatory vari-
ables and thus that an interaction term of 0 is consistent with the counterfactual. In this
case, adjusting the coefficient on the interaction term to test the counterfactual implicit in
the linear model would be a mistake. Blundell and Costa Dias 2009 point out that the
interaction term can be translated into a more intuitively accessible metric of the effect
in the treated group by calculating F (β0 + β1 treated group + β2 post + β3 treated ∗ post) −
F (β0 + β1 treated group + β2 post) where F is the relevant CDF.
   Athey and Imbens 2006 propose an approach they call changes-in-changes (CIC). They
assume that the no-treatment outcome is monotonic in the stochastic term and that the


                                              9
distributions of the stochastic terms within the treatment and control groups are constant
over time. One then calculates the change in the outcome at each quantile of the outcome
variable in the control group. This gives the counterfactual at each quantile and therefore
at each initial value of the outcome variable. If we are interested in the mean of the coun-
terfactual, we can reweight these changes to match the pre-period distribution of outcomes
in the experimental group.3
       This strikes us as a helpful approach, but, as discussed in the original paper, we cannot
make use of any observations in the control group that do not have a counterpart in the
experimental group in the pre-period since using them requires parametric assumptions,
and we do not have a counterfactual for any experimental observations that do not have a
counterpart in the control group in the pre-period. Note that when the outcome is binary,
deriving a point estimate of the counterfactual requires strong assumptions. Under these
assumptions, the counterfactual in the example above is given by the rise from .50 to .55.
       The reweighting in Athey and Imbens bears a superficial resemblance to the synthetic
controls technique of Abadie, Diamond, and Hainmueller 2010, but the approaches are really
quite different. The synthetic controls approach attempts to ensure parallel trends in the
pre-period. Athey and Imbens effectively ensure parallel levels, or more accurately, common
support, in the pre-period. In principle, it is then possible to test for parallel pre-trends.
       In the papers on 16 and Pregnant, because the dependent variable is the logarithm of the
teen birth rate, the assumption is that in the absence of the intervention the teen pregnancy
rate in all DMAs (designated market areas) would have fallen by a constant proportion. For
instance, a DMA with an initial rate of 45 per thousand would have experienced a decline
one and a half times that of a DMA with an initial rate of 30. This is neither obviously right
nor obviously wrong. Assessing its validity requires understanding why the teen birthrate
   3
    Under their assumptions, we can think of the observations at a given quantile in the pre- and post-periods
as representing the outcomes for a pseudo-person. By design we have rank invariance in the outcomes of
pseudo-persons. If for example, the 10th percentile outcome in the control group is 50 in the pre-period
and 65 in the post-period, we then ascribe a counterfactual outcome of 65 for every pseudo-person in the
experimental group with a pre-period outcome of 50.



                                                     10
was dropping, a topic about which Kearney and Levine are certainly more knowledgeable
than we are.
        But we can imagine many alternative hypotheses. For example, if teen pregnancies
consist of two components, intended pregnancies which are not dropping and unintended
pregnancies which are, then we might expect teen pregnancies to fall by a higher proportion
in high birth-rate areas.4 Alternatively if the rate of intended pregnancies were much more
similar across DMAs and intended pregnancies were declining, then a constant percentage
point change would be a more accurate counterfactual.
        To some extent, when sufficient data exist, these concerns can be addressed by examining
the patterns in the data prior to the ‘experiment,’ although we raise concerns about pre-trend
testing in the next section. However, in practice most researchers will not have the necessary
power to distinguish between alternative functional forms in this period. Regardless, in
the absence of a compelling case for a particular functional form for the counterfactual,
economists should consider how robust their results are to alternative choices.



4         Testing for Preexisting Trends

As DiD has grown in popularity, researchers have become increasingly aware that seemingly
similar groups may not always exhibit truly parallel trends. This may arise either because the
control group really is not a very good control group or, as discussed in the previous section,
because the functional form for the counterfactual is incorrect. Consequently, whenever
data permit, authors are now effectively required to test the assumption of parallel trends
using a test of ‘pre-trends.’ Angrist and Pischke 2010 maintain that “The most compelling
differences-in-differences-type studies report outcomes for treatment and control observations
for a period long enough to show the underlying trends, with attention focused on how
deviations from trend relate to changes in policy.”
    4
    Whether this is true obviously depends on the correlation between intended and unintended pregnancies
and their relative frequencies.



                                                   11
       The logic is that if the two groups truly would have exhibited parallel trends in the
absence of treatment, we should find that this model fits the data in the periods prior to
treatment. There are two ways that researchers generally test for parallel trends in periods
prior to treatment, or “pre-trends.” The first, frequently used when only a modest number
of time periods are available, is to replicate the model on two periods prior to the treatment.
In this case, the second period becomes the placebo treatment period. The second approach,
used by KL, considers the year prior to treatment to be the “base year’ and estimates the
difference between the control and treatment groups in each previous year relative to the
base year.5 This allows the researcher to test the null hypothesis that outcomes prior to the
treatment year exhibited parallel trends.
       Testing for parallel pre-trends arises naturally in the potential treatments approach to
DiD, which assumes that we can write potential outcomes for a group, g, in time t as Ygt =
f (D, ugt ) where ugt is a group and time error term which incorporates group specific shocks.
This framework therefore requires that f (0, ug1 ) − f (0, ug0 ) is mean independent of Dg .
Given that we do not observe f (0, ug1 ), it is natural to test for mean independence by asking
whether this holds in f (0, ug0 )−f (0, ug−1 ) , or more generally, whether f (0, ug1 )−f (0, ugτ ),
τ ≤ 0 is correlated with Dg .
       KL and JJK both follow this approach and test whether the coefficients on the treatment
group in the pre-period (relative to time 0) are individually or jointly statistically significantly
different from zero. The logic of their approach is that if, for example, the coefficients on
periods prior to the base year are positive relative to the base year, then the difference was
already declining in the pre-period. But what are they and others who use this approach
really testing? They are testing the null hypothesis that the relative differences are all
zero against the alternative that the coefficients are not equal to zero. Given the unknown
nature of the pre-trends, it would certainly be troubling if the pre-trend coefficients failed
   5
     As a reminder, because KL have a continuum of treatments, using IV they estimate a coefficient relative
to the base year. KL and JJK differ in their choice of base year and in how they group years, but these
differences appear to us to be minor.



                                                    12
this test. However, passing this test does not mean that the researcher should feel content
that significant pre-trends have been ruled out. In the case of KL, all of the coefficients are
positive and decreasing, suggesting a potential linear trend in the pre-period. In this case,
a one-sided chi-squared test evaluating whether the coefficients are all positive or directly
testing for a linear pre-trend is appropriate. In other words, if the pre-trends fail some tests
of significance but not all tests, this still suggests there is serious reason for concern on the
part of the researcher.
   Increasingly, researchers point to a statistically insignificant pre-trend test to argue that
they therefore accept the null hypothesis of parallel trends. There is no doubt that testing
for a common pre-trend plays an important role in validating the parallel trends assumption
underlying DiD. However, failing to reject that outcomes in years prior to treatment exhibit
parallel trends, should not be confused with establishing the validity of the parallel trends
counterfactual. Moreover, clearly, not rejecting the null hypothesis is not equivalent to
confirming it. Yet, in such settings, economists may be tempted to, in Brad DeLong’s
terms, “seize the high ground of the null hypothesis,” which can lead to incorrect estimation
of treatment effects. We discuss this in more detail below. Second, what we would like
to establish is not that there were parallel trends in the pre-treatment period, but rather
that there would have been in the post-period in the absence of the treatment, something
which is unfortunately unknowable. A parallel trend in the periods prior to treatment
does not guarantee that a parallel trend would have continued in the absence of treatment.
The researcher should consider whether there is reason to believe this pattern would have
continued. This requires understanding why the groups diverged in levels but otherwise
followed similar trends, in other words, a good understanding of the subject matter.
   Take, for example, the following study relying on some or all of the data in Figure 1.
We will tell you shortly what is on the y-axis, but in the interest of honest reflection and
introspection, we discourage you from reading ahead and to spend a brief period looking at
the figure.


                                               13
   It is tempting to begin hypothesizing about what caused the sudden break from the
common trend at roughly T=160. We trust that the reader would not come up with an
explanation and then test it using these same data. KL certainly deserve credit for first
noticing a pattern like that in figure 1 and then asking how to test it without relying directly
on the data that led to their hypothesis.
   Assume a researcher reads about an intervention that affected some of the treatment
group but none of the control group at T=160. We assume our researcher is honest and did
not know the pattern in the data before reading this. If she looks at the long-term pre-trend,
she will almost definitely conclude that the control and experimental groups have followed
a similar pre-trend (with some small unexplained departure around T=140). If she looks
only at the shorter period, she will probably conclude that the difference in pre-trend is a
problem with which she must grapple. It is not obvious which is correct. Here we agree with
the Kearney and Levine 2016 response to JJK. A long-run trend is not necessarily better
than a short-run trend, but the converse is also not true. Consequently, the presence of
a long-run pre-trend does not prove that the analysis based on the absence of a short-run
pre-trend is incorrect. But, in the absence of a compelling analysis of why we should prefer
one piece of evidence rather than the other, it does detract from our confidence about either
conclusion. And, as will be clear, the similar long-run trend for much of the sample period
in our example is misleading.
   The two groups in figure 1 are males (the control group) and females (the experimental
group). We have plotted average height in centimeters against age measured in months. For
reasons that we well understand (or at least we would if we had studied enough biology),
height first diverges between the sexes when females hit puberty and then diverges in the
opposite direction when males hit puberty at a later age. Despite exhibiting parallel trends
in height prior to T=135, males and female height should not be expected to continue at the
same rate of growth.
   We are confident that no economics journal would publish our fictional study, but we are


                                              14
less confident that they do not publish well-intended research on complicated psychological
or social phenomena based on a belief in the superiority of economists’ statistical methods for
uncovering causal effects. As the disagreement between KL and JJK suggests, the evidence
that in the absence of the intervention there would have been a common trend in the post-
intervention period should not and cannot be purely statistical.
       For example, JJK point out that 16 and Pregnant premiered during the onset of the Great
Recession. They argue that the recession impacted populations and areas of the country
differently, suggesting that even if trends had been parallel prior to treatment, they might
have diverged at the time of treatment even in the absence of treatment. This argument
raises a valid concern regardless of what one concludes about the pre-trend. Here again,
understanding why teen birth rates differed prior to treatment helps us assess whether the
coincidence of the timing of the show and of the Great Recession is a significant cause for
concern.
       While the absence of a pre-trend is neither a necessary nor a sufficient condition for the
absence of divergence in the counterfactual, it would be foolish to suggest that establishing
that there is no pre-trend lends no support to the null hypothesis. But when should we
conclude that there is no pre-trend? Even if we accept that we should rely on the more
recent period for testing for a pre-trend, the relation between teen pregnancy rates and
predicted viewership appears to be decreasing prior to the introduction of 16 and Pregnant
despite its statistical insignificance.6 Although we do not offer a complete solution to this
problem, we propose the following two considerations.
       The first is to test statistically the null hypothesis of non-parallel trends sufficient to elim-
inate an experimental effect. This requires considering a functional form of pre-period trends,
for example linear, and testing directly whether the confidence interval includes a sufficiently
large difference in trends to eliminate the purported experimental effect. If we cannot reject
a trend that, if continued into the post-period, would eliminate the experimental effect, we
   6
    Note that statistical insignificance is subject to our earlier caveats about the appropriate choice of test
and pre-period.


                                                      15
do not have strong support for that effect.
   We can also think of this in context of the standard regression equation. Here, it seems
more natural to ask whether it changes our estimate of the ‘causal’ effect if we include group-
specific trends or allow trends to depend on group characteristics. And, indeed some papers
have taken this approach, including Kearney and Levine in their 2016 response to JJK. If
doing so changes the interpretation of the coefficient of interest, we must be appropriately
circumspect about our conclusions. Of course, all of the usual caveats about adding ad-
ditional controls also apply in this setting. In particular, if we add sufficiently nonlinear
group-specific trends, we will inevitably render the effect of the intervention statistically in-
significant. Even if we restrict ourselves to linear trends, the loss of degrees of freedom may
dramatically reduce our power to detect a true effect of the intervention. And, if the original
regression equation is misspecified, adding further controls can increase rather than decrease
the bias. Thus, if the causal effect of the intervention increases (or decreases) with time
since implementation, the DiD approach will give the average effect of the intervention over
the post-period. The deviation from this post-period average in the treated group will be
correlated with calendar time. We can write fairly simple examples where adding a variable
for treatment group*time would result in the coefficient on treatment having the wrong sign.
   Our second recommendation is to take seriously our earlier discussion of the experimental
design. For example, this will include an analysis of what factors might explain the differences
in levels in the period prior to treatment. If we understand why the experimental and
control groups differ in levels, we may better understand whether to anticipate common or
divergent trends. For example, JJK show that the share of the population that identifies
as Hispanic is substantially lower in areas with high MTV viewership. This suggests that
there are meaningful differences between high and low viewership areas which may generate
divergent trends. One option here is to include race or ethnicity specific time trends or,
similarly, interactions between race/ethnicity and period. If this substantively changes the
interpretation of treatment effects, this should raise serious concerns about our estimates


                                               16
of the treatment effect. On the other hand, if, subject to all our other caveats, accounting
for the racial and ethnic composition of the DMAs eliminates both the initial difference and
differential trends, we will feel considerably more confident that we have solved the problem
with nature’s design of the experiment.
    Finally, we note that testing for a pre-trend is a special case of pre-testing in econometrics,
and, as is well known, the default standard errors are incorrect when we rely on pre-testing.
Roth 2018 discusses using pre-trend testing to test the parallel trends assumption as a special
case of pre-testing. He explains that using pre-trend tests as a test of parallel trends not
only risks accepting misidentified studies, but can also exacerbate the bias from violations
of parallel trends and lead to severe over-rejection of the null hypothesis. He proposes a
corrected estimator to adjust for the fact that the pre-trend test has occurred. This is
certainly an improvement, and we believe that the Roth estimator should be used in DiD
designs that rely on pre-trend testing. However, using this estimator does not eliminate the
need for logical reasoning related to parallel trends. In the case when parallel trends appear
plausible but not certain, the researcher should also perform a thorough comparison of the
differences between the treatment and control groups including demographic composition,
other factors that could have differentially affected each group, and comparison of trends as
far back as possible. JJK provide a good example of how a thorough comparison can help
the researcher logically determine whether the parallel trends assumption is credible.



5     Conclusion

In a perfect world, we would have well designed experiments which would provide clear
evidence on the causal impact of all potential policies and similar interventions. In reality,
we are far from that world. In cases without a true randomized experiment, tools like
differences-in-differences broaden the range of ‘natural experiments’ we can use to identify
causal effects, but we should not allow the use of this term to fool us. The use of the term



                                                17
‘experiment’ seems to imply a level of credibility that will rarely if ever be commensurate
with what can be expected of empirical research based on DiD. Using DiD properly requires
taking all necessary precautions - both logically and in terms of methods - to ensure that the
assumptions of DiD are met. We should not conclude from this that well-executed papers
relying on DiD are somehow invalid. Instead, we hope that our discussion highlights some
of the broader issues regarding the assumption of parallel trends required for identification
in a DiD design and what should be considered necessary for justifying this assumption.
Although we do not provide a full solution to this question, we hope to have provided a
framework for thinking about the assumption of parallel trends.
   In this paper, we highlight the discussion between KL and JJK, but our message should
not be viewed primarily as applying to estimating the impact of 16 and Pregnant, but rather
as a discussion about the implementation of differences-in-differences designs more broadly.
KL employ statistical methods that are used frequently in empirical work. Although JJK
chose to focus their analysis on the KL paper, it is likely that there are a range of other
well-known papers for which a similar analysis could be performed.
   We encourage researchers to use this as a framework for thinking about identification
in all empirical work in which there is a potential failure in experimental design, not just
DiD. This means asking, “Why did nature’s ‘experimental design’ fail, and how could that
impact my identification strategy?” Any available tests of identification should be seen as
a complement to, not a substitute for, logical reasoning. This also highlights the need for
additional research on how to evaluate identification strategies in cases where there has been
a failure of experimental design.



References

 [1] Alberto Abadie, Alexis Diamond, and Jens Hainmueller. “Synthetic Control Methods
      for Comparative Case Studies: Estimating the Effect of California’s Tobacco Control



                                             18
     Program”. In: Journal of the American Statistical Association 105.490 (2010), pp. 493–
     505.

 [2] Chunrong Ai and Edward C. Norton. “Interaction terms in logit and probit models”.
     In: Economics Letters 80.1 (2003), pp. 123–129. issn: 01651765. doi: 10.1016/S0165-
     1765(03)00032-6.

 [3] Joshua Angrist and Guido Imbens. “Identification and Estimation of Local Average
     Treatment Effects”. In: Econometrica 62.2 (1994), pp. 467–475. issn: 0012-9682. doi:
     10.2307/2951620.

 [4] Joshua Angrist and Jorn-Steffen Pischke. “The Credibility Revolution in Empirical
     Economics: How Better Research Design is Taking the Con out of Econometrics”. In:
     Journal of Economic Perspectives 24.2 (2010), pp. 3–30. doi: 10.1257/jep.24.2.3.

 [5] Susan Athey and Guido Imbens. “Identification and Inference in Nonlinear Difference-
     in-Differences Models”. In: Econometrica 74.2 (2006), 43‘–497.

 [6] Richard Blundell and Monica Costa Dias. “Alternative Approaches to Evaluation in
     Empirical Microeconomics”. In: The Journal of Human Resources 44.3 (2009), pp. 565–
     640.

 [7] David Jaeger, Theodore Joyce, and Robert Kaestner. “Does Reality TV Induce Real
     Effects? On the Questionable Association Between 16 and Pregnant and Teenage Child-
     bearing”. In: Journal of Business and Economic Statistics forthcoming (2018).

 [8] Melissa Kearney and Phillip Levine. “Does Reality TV Induce Real Effects? A Re-
     sponse to Jaeger, Joyce, and Kaestner”. In: IZA DP No. 10318 (2016).

 [9] Melissa Kearney and Phillip Levine. “Media Influences on Social Outcomes : The
     Impact of MTV ’ s ”16 and Pregnant ” on Teen Childbearing”. In: 105.12 (2015),
     pp. 3597–3632.

[10] Kevin Lang. “Ability Bias, Discount Rate Bias and the Return to Education”. 1993.


                                           19
[11] Bruce Meyer. “Natural and Quasi-Experiments in Economics”. In: Journal of Business
     and Economic Statistics 13.2 (1995), pp. 151–161.

[12] Jonathan Roth. “Should We Condition on the Test for Pre-trends in Difference-in-
     Difference Designs ?” 2018.

[13] John Snow. “On the Mode of Communication of Cholera”. In: J. Churchill, London,
     England 2nd Ed (1855).




                                          20
Figure 1: Results of a ‘Natural Experiment’




                    21
