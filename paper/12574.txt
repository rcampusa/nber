                                NBER WORKING PAPER SERIES




   UNDERSTANDING INSTRUMENTAL VARIABLES IN MODELS WITH ESSENTIAL
                         HETEROGENEITY

                                          James J. Heckman
                                            Sergio Urzua
                                          Edward J. Vytlacil

                                        Working Paper 12574
                                http://www.nber.org/papers/w12574


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                     October 2006




This project was supported by NSF grants 0241858, 0099195, and 9709873, and NIH grant R01-HD043411,
and a grant from the American Bar Foundation. An early version of this paper was presented as the
Review of Economics and Statistics Lecture at Harvard, April 2001; at Princeton University, December
2004; at a workshop on Causality in Economics, Oxford University, August 2005; and at the World
Congress of the Econometric Society, August 2005. We thank audience participants at those seminars
as well as students in Economics 350 at the University of Chicago in Winter Quarter 2005 for stimulating
comments and our discussants at the 2001 seminar, Larry Katz and Robert Moffitt, for helpful comments.
We have received additional helpful comments from the editor, Jim Stock and an anonymous referee.
Bo Honore, Derek Neal, Weerachart Kilenthong, Sergey Mityakov, Rodrigo Pinto, Jean-Marc Robin
and Jora Stixrud provided helpful comments on various drafts. Supplementary material for this paper
is available at the website http;//jenni.uchicago.edu/underiv. The views expressed herein are those
of the author(s) and do not necessarily reflect the views of the National Bureau of Economic Research.

© 2006 by James J. Heckman, Sergio Urzua, and Edward J. Vytlacil. All rights reserved. Short sections
of text, not to exceed two paragraphs, may be quoted without explicit permission provided that full
credit, including © notice, is given to the source.
Understanding Instrumental Variables in Models with Essential Heterogeneity
James J. Heckman, Sergio Urzua, and Edward J. Vytlacil
NBER Working Paper No. 12574
October 2006
JEL No. C31

                                            ABSTRACT

This paper examines the properties of instrumental variables (IV) applied to models with essential
heterogeneity, that is, models where responses to interventions are heterogeneous and agents adopt
treatments (participate in programs) with at least partial knowledge of their idiosyncratic response.
We analyze two-outcome and multiple-outcome models including ordered and unordered choice models.
We allow for transition-specific and general instruments. We generalize previous analyses by developing
weights for treatment effects for general instruments. We develop a simple test for the presence of
essential heterogeneity. We note the asymmetry of the model of essential heterogeneity: outcomes
of choices are heterogeneous in a general way; choices are not. When both choices and outcomes are
permitted to be symmetrically heterogeneous, the method of IV breaks down for estimating treatment
parameters.

James J. Heckman                                   Edward J. Vytlacil
Department of Economics                            Department of Economics
The University of Chicago                          Columbia University
1126 E. 59th Street                                1022 International Affairs Building
Chicago, IL 60637                                  420 West 118th Street
and NBER                                           New York, NY 10027
jjh@uchicago.edu                                   and NBER
                                                   ev2156@columbia.edu
Sergio Urzua
1401 East Hyde Park Blvd. #901
Chicago, IL, 60615
surzua@uchicago.edu
1         Introduction

Suppose a policy is proposed for adoption in a country. It has been tried in other countries and we
know outcomes there. We also know outcomes in countries where it was not adopted. From the
historical record, what can we conclude about the likely eﬀectiveness of the policy in countries that
have not implemented it?
        To answer this question, we build a model of counterfactuals. Let Y0 be the outcome of a country
(e.g. GDP) under a no policy regime. Y1 is the outcome if the policy is implemented. Y1 − Y0 is
the treatment eﬀect of the policy. It may vary among countries. We observe characteristics X of
various countries (e.g. level of democracy, level of population literacy, etc.). It is convenient to
decompose Y1 into its mean given X, μ1 (X), and deviation from mean, U1 . We can make a similar
decomposition for Y0 :


                                            Y1 = μ1 (X) + U1                                           (1)

                                            Y0 = μ0 (X) + U0 .


It may happen that controlling for the X, Y1 − Y0 is the same for all countries. This is the case of
homogenous treatment eﬀects given X. More likely, countries vary in their response to the policy
even after controlling for X.
        Figure 1 plots the distribution of Y1 −Y0 for a benchmark X. We explain the various parameters
mentioned in the figure later on. The special case of homogeneity arises when the distribution
collapses to its mean. It would be ideal if we could estimate the distribution of Y1 − Y0 given X and
there is research that does this.1 More often, economists focus on some mean of the distribution
displayed in Figure 1 and use a regression framework to interpret the data. To turn (1) into a
regression model, it is conventional to use a switching regression framework. Define D = 1 if a
country adopts a policy; D = 0 if it does not. The observed outcome, Y , is Y = DY1 + (1 − D)Y0 .
    1
    See Carneiro, Hansen, and Heckman (2001, 2003), Cunha, Heckman, and Navarro (2005, 2006) and the survey
in Heckman, Lochner, and Todd (2006).




                                                     1
Substituting (1) into this expression, and keeping all X implicit, we obtain


                                   Y   = Y0 + (Y1 − Y0 )D                                                           (2)

                                       = μ0 + (μ1 − μ0 + U1 − U0 )D + U0 .


Using conventional regression notation,


                                                 Y = α + βD + ε                                                     (3)


where α = μ0 , β = (Y1 − Y0 ) = μ1 − μ0 + U1 − U0 and ε = U0 . The coeﬃcient on D is the “treatment
eﬀect.”The case where β is the same for every country is the one conventionally assumed. More
elaborate versions assume that β depends on X and estimate interactions of D with X. The
case where β varies even after accounting for X is called the “random coeﬃcient”or “heterogenous
treatment eﬀect”case. A great deal of attention has been focused on this case in recent decades.
       The case where β (given X) is the same for every country is the familiar one and we develop
it first. A least squares regression of Y on D (equivalently a mean diﬀerence in outcomes between
countries with D = 1 and countries with D = 0), is possibly subject to a selection bias. Countries
that adopt the policy may be atypical in terms of their Y0 (= α + ε). Thus if countries that would
have done well in terms of unobservable ε (= U0 ) even in the absence of the policy are the ones
that adopt the policy, β estimated from OLS (or its nonparametric version–matching) is upward
biased because Cov(D, ε) > 0.
       Two main approaches have been adopted to solve this problem: (a) selection models (Gronau,
1974; Heckman, 1974, 1976a,b, 1979; Heckman and Robb, 1985, 1986; Powell, 1994) and (b) instru-
mental variable models (Heckman and Robb, 1985, 1986; Imbens and Angrist, 1994; Angrist and
Imbens, 1995; Manski and Pepper, 2000; Heckman and Vytlacil, 1999, 2000, 2005).2 This paper
   2
     Matching is also used. It is a form of nonparametric least squares that assumes that all relevant unobservables are
accurately proxied by the observables X that the analyst happens to have at his or her disposal, so (Y0 , Y1 ) ⊥
                                                                                                               ⊥D|X
(alternatively that (ε, β) ⊥
                           ⊥ D | X), where A ⊥  ⊥ B | C means that A is independent of B given C. See Heckman and
Navarro (2004) for a discussion of matching.



                                                           2
focuses on the instrumental variable (IV) approach and establishes the relationship between the
selection and IV approach using a prototypical economic model. The selection approach models
levels of conditional means. The IV approach models the slopes of the conditional means. IV does
not identify the constants estimated in selection models. In the general case with heterogeneity,
when IV is used to identify the same level parameters that are identified by control function or
selection methods, it is necessary to make the same assumptions about levels outcomes in limit sets
(“identification at infinity”) as are made in selection models.
       For the case with homogeneous responses, if there is an instrument Z with the properties that


                                                    Cov(Z, D) 6= 0                                 (4)


                                                    Cov(Z, ε) = 0                                  (5)

then standard IV identifies β, at least in large samples:

                                                           Cov(Z, Y )
                                            plim β̂ IV =              = β.3
                                                           Cov(Z, D)

If other instruments exist, each identifies β. Z produces a controlled variation in D relative to ε.
Randomization of assignment with full compliance with experimental protocols is an example of an
instrument. From the instrumental variable estimator, we can identify the eﬀect of adopting the
policy in any country since all countries respond to the policy in the same way, controlling for their
X.
       If β (= Y1 − Y0 ) varies in the population even after controlling for X, there is a distribution
of responses that cannot in general be summarized by a single number. Even if we are interested
in the mean of the distribution, a new phenomenon distinct from selection bias might arise. This
   3
       The proof is straightforward. Under general conditions (see, e.g. White, 1984),

                                                                  Cov(Z, ε)
                                               plim β̂ IV = β +
                                                                  Cov(Z, D)

and the second term on the right hand side vanishes.



                                                            3
is the problem of sorting on the gain which is distinct from sorting on the level. If β varies, even
after controlling for X, there may be sorting on the gain (Cov(β, D) 6= 0). This is the model of
essential heterogeneity.
      The application of instrumental variables to this case is more problematic. Suppose that we
augment the standard instrumental variable assumptions (4) and (5) by the following assumption:


                                               Cov(Z, β) = 0.                                         (6)


Can we identify the mean of (Y1 − Y0 ) using IV? In general we cannot.4
      To see why, let β̄ = (μ1 − μ0 ) be the mean treatment eﬀect (the mean of the distribution in
Figure 1). β = β̄ + η, where U1 − U0 = η. Write equation (3) in terms of this parameter:


                                         Y = α + β̄D + [ε + ηD] .


The error term of this equation (ε + ηD) contains two components. By assumption, Z is uncorre-
lated with ε and η. But to identify β̄, we need IV to be uncorrelated with [ε + ηD]. That requires
Z to be uncorrelated with ηD.
      If policy adoption is made without knowledge of η (= U1 − U0 ), the idiosyncratic gain to policy
adoption after controlling for the observables, then η and D are statistically independent and hence
uncorrelated, and IV identifies β̄.5 If, however, policy adoption is made with partial or full knowledge
of η, IV does not identify β̄ because E(ηD | Z) = E(η | D = 1, Z) Pr(D = 1 | Z) and if there
is sorting on the unobserved gain η, the first term is not zero. Similar calculations show that IV
does not identify the mean gain to the countries that adopt the policy (E (β | D = 1)) and many
  4
      This point was made by Heckman and Robb (1985, 1986). See also Heckman (1997).
  5
      Proof:
                                                         Cov (Z, ε + ηD)
                                       plim β̂ IV = β̄ +
                                                           Var (D, Z)
But Cov (Z, ε + ηD) = Cov (Z, ε) + Cov (Z, ηD) and Cov (Z, ηD) = E (ZηD) − E (Z) E (ηD), E (ηD) = 0 by the
assumed independence. E (ZηD) = E (η) E (ZD) by the assumed independence and hence E (ZηD) = 0 since
E (η) = 0.




                                                      4
other summary treatment parameters.6 Whether η (= U1 − U0 ) is correlated with D depends on
the quality of the data available to the empirical economist, and cannot be settled a priori. The
conservative position is to allow for such correlation. However, this rules out IV as an interesting
econometric strategy for identifying any of the familiar mean treatment parameters.
      It is remarkable then that under certain conditions Imbens and Angrist (1994) establish that in
the model with essential heterogeneity standard IV can identify an interpretable parameter. The
parameter they identify is a discrete approximation to the marginal gain parameter introduced by
Björklund and Moﬃtt (1987). Those authors demonstrate how to use a selection model to identify
the marginal gain to persons induced into a treatment status by a marginal change in the cost
of treatment. Imbens and Angrist (1994) show how to identify a discrete approximation to this
parameter using instrumental variables.
      They assume the existence of an instrument Z that takes two or more distinct values. This
is implicit in (4). If Z assumes only one value, the covariance would be zero. Strengthening the
covariance conditions of equations (5) and (6), they assume that Z is independent of β (= Y1 − Y0 )
and Y0 . Let “⊥
              ⊥”denote independence. Denote by D(z) the random variable indicating receipt
of treatment when Z is set to z. (D(z) = 1 if treatment is received; D(z) = 0 otherwise). The
Imbens-Angrist independence assumption can be written as

           ¡                  ¢
(IV-1) Z ⊥
         ⊥ Y0 , Y1 , {D(z)}z∈Z where Z is the set of possible values of Z. (Independence)

They also assume that

(IV-2) Pr(D = 1 | Z) depends on Z (Rank).

This is a standard rank condition. They supplement the standard IV assumption with what they
call a “monotonicity”assumption. It is a condition across persons. This assumption maintains that
if Z is fixed first at one and then at the other of two distinct values, Z = z and Z = z 0 , all persons
respond to the change in Z in the same way. In our policy adoption example, it states that a
  6
      See Heckman and Robb (1985, 1986), Heckman (1997) or Heckman and Vytlacil (1999).




                                                      5
movement from z to z 0 causes all countries to move toward (or against) adoption of the policy being
studied. If some adopt, others do not drop the policy in response to the same change.
   More formally, letting Di (z) be the indicator (= 1 if adopted; = 0 if not) for adoption of a
policy if Z = z for country i, for any distinct values z and z 0 , Imbens and Angrist (1994) assume

(IV-3) Di (z) ≥ Di (z 0 ) for all i or Di (z) ≤ Di (z 0 ) for all i, i = 1, . . . , I. (Monotonicity or
Uniformity)

The content in this assumption is not in the order for any person. Rather, the responses have to
be uniform across people for a given choice of z and z 0 . One possibility allowed under (IV-3) is the
existence of three values z < z 0 < z 00 such that, for all i, Di (z) ≥ Di (z 0 ) but Di (z 0 ) ≤ Di (z 00 ).
The standard usage of the term monotonicity rules out this possibility by requiring that one of
the following hold for all i: (a) z < z 0 componentwise implies Di (z) ≥ Di (z 0 ) or (b) z < z 0
componentwise implies Di (z) ≤ Di (z 0 ). Of course, if the Di (z) are monotonic in the standard
usage, they are monotonic in the sense of Imbens and Angrist.
   For any value of z 0 in the domain of definition of Z, from (IV-1) and (IV-2) and the definition of
D(z), (Y0 , Y1 , D (z 0 )) is independent of Z. For any two values of the instrument Z = z and Z = z 0 ,
we may write


                                        E (Y | Z = z) − E (Y | Z = z 0 )

                                    = E(Y0 + D(Y1 − Y0 ) | Z = z)

                                        −E (Y0 + D(Y1 − Y0 ) | Z = z 0 )

                                    = E(D(Y1 − Y0 ) | Z = z)

                                        −E (D(Y1 − Y0 ) | Z = z 0 ) .


From the independence condition (IV-1) and the definition of D(z) and D(z 0 ), we may write this




                                                     6
expression as E [(Y1 − Y0 ) (D (z) − D (z 0 ))]. Using the law of iterated expectations,


                         E (Y | Z = z) − E (Y | Z = z 0 )                                                       (7)

                     = E (Y1 − Y0 | D (z) − D (z 0 ) = 1) Pr (D (z) − D (z 0 ) = 1)

                         −E (Y1 − Y0 | D (z) − D (z 0 ) = −1) Pr (D (z) − D (z 0 ) = −1) .


By the monotonicity condition (IV-3), we eliminate one or the other term in this final expression.
Suppose that Pr(D(z) − D(z 0 ) = −1) = 0, then


                             E (Y | Z = z) − E (Y | Z = z 0 )

                         = E (Y1 − Y0 | D(z) − D(z 0 ) = 1) Pr (D(z) − D(z 0 ) = 1) .


Dividing by Pr(D(z) − D(z 0 ) = 1) = Pr(D = 1 | Z = z) − Pr(D = 1 | Z = z 0 ) for values of z and z 0
that produce distinct propensity scores, we obtain LATE:

                                             E (Y | Z = z) − E (Y | Z = z 0 )
                           LATE =                                                                               (8)
                                         Pr(D = 1 | Z = z) − Pr(D = 1 | Z = z 0 )
                                    = E (Y1 − Y0 | D(z) − D(z 0 ) = 1) .


This is the mean gain to those induced to switch from “0”to “1”by a change in Z from z 0 to z.
       This is not the mean of Y1 − Y0 (average treatment eﬀect) unless the Z assume values (z, z 0 )
such that Pr(D(z)) = 1 and Pr(D(z 0 )) = 0.7 It is also not the eﬀect of treatment on the treated
(E(Y1 − Y0 | D = 1) = E(β | D = 1)) unless the analyst has access to and uses one or more values
of z such that Pr(D(z) = 1) = 1.
       LATE depends on the particular instrument used.8 The parameter is defined by a hypothetical
manipulation of instruments. If monotonicity (uniformity) is violated, IV estimates an average
   7
     Such values of Z produce “identification at infinity,”or more accurately, limit points where P (z) = 1 and
P (z 0 ) = 0.
   8
     Dependence of the estimands on the choices of IV used to estimate models with essential heterogeneity was first
noted in Heckman and Robb (1985, 1986).



                                                         7
response of those induced to switch into the program and those induced to switch out of the
program by the change in the instrument because both terms in (7) are present.9
    If the analyst is interested in knowing the average response (β̄), the eﬀect of the policy on the
outcomes of countries that adopt it (E(β | D = 1)) or the eﬀect of the policy if a particular country
adopts it, there is no guarantee that the IV estimator comes any closer to the desired target than
the OLS estimator and indeed it may be more biased than OLS. Since diﬀerent instruments define
diﬀerent parameters, having a wealth of diﬀerent strong instruments does not improve the precision
of the estimate of any particular parameter. This is in stark contrast with the traditional model
with β ⊥
       ⊥ D. In that case, all valid instruments identify β̄. The Durbin (1954)-Wu (1973)-Hausman
(1978) test for the validity of extra instruments applies to the traditional model. In the more general
case with essential heterogeneity, since diﬀerent instruments estimate diﬀerent parameters, no clear
inference emerges from these specification tests.
    When dealing with more than two distinct values of Z, Imbens and Angrist (1994) draw on the
analysis of Yitzhaki (1989), which was refined in Yitzhaki (1996) and Yitzhaki and Schechtman
(2004), to produce a weighted average of pairwise LATE parameters where the scalar Z are ordered
to define the LATE parameter. In this case IV is a weighted average of LATE parameters with
non-negative weights.10 Imbens and Angrist generalize this result to the case of vector Z assuming
that instruments are monotonic functions of the probability of selection.
    This paper and our previous analysis build on the pioneering work of Yitzhaki and Imbens and
Angrist.11 We make the following contributions to this literature.


   1. We relate the LATE-IV approach to economic choice models. Using a choice theoretic pa-
      rameter (the marginal treatment eﬀect or MTE) introduced into the literature on selection
   9
     Angrist, Imbens, and Rubin (1996) consider the case of two way flows for the special case of a scalar instrument
when the monotonicity assumption is violated. Their analysis is a version of Yitzhaki’s (1989, 1996) analysis. He
analyzes the net eﬀect whereas they break the net eﬀect into two components corresponding to the two way flows.
  10
     We place the unpublished Yitzhaki (1989) paper at our website and summarize his essential ideas in Section 3.2
and Appendix C. He shows that two stage least squares estimators of Y on P (Z) = E (D | Z), identify weighted
averages of terms like the second terms in (8) with positive weights. See also Yitzhaki (1996) and Yitzhaki and
Schechtman (2004).
  11
     See Heckman and Vytlacil (1999, 2001c, 2005).



                                                         8
       models by Björklund and Moﬃtt (1987), it is possible to generate all treatment eﬀects as dif-
       ferent weighted averages of the MTE or of LATE. Standard linear IV can also be interpreted
       as a weighted average of MTE or LATE. Using the economic model, MTE is a limit form of
       LATE and LATE in turn is a discrete approximation to MTE, or the marginal gain function
       of Björklund and Moﬃtt (1987). A local version of instrumental variables (LIV for Local
       Instrumental Variables), distinct from standard instrumental variables, identifies the MTE.
       These theoretical constructs can be defined independently of the data.

   2. We establish the central role of the propensity score (Pr (D = 1 | Z = z) = P (z)) in both
       selection and IV models.12

   3. We show that with vector Z, and a scalar instrument constructed from Z (e.g. J(Z)), the
       weights on LATE and MTE that are implicit in standard IV are not guaranteed to be non-
       negative. Thus IV can be negative even though all pairwise LATEs and pointwise MTEs are
       positive. Certain instruments produce positive weights and avoid this interpretive problem.
       Our analysis generalizes that of Yitzhaki and Imbens-Angrist who analyze the case with
       positive weights.

   4. We show the special status of P (z) as an instrument. It always produces non-negative weights
       for MTE and LATE. It enables analysts to identify MTE or LATE. With knowledge of P (z),
       and the MTE or LATE, we can decompose any standard IV estimate into identifiable MTEs (at
       points) or LATEs (over intervals) and identifiable weights on MTE or LATE, where the weights
       can be constructed from data. This ability to decompose IV into interpretable components
       allows analysts to determine the response to treatment of persons at diﬀerent levels of the
       unobserved factors that determine treatment status.

   5. We present a simple test for essential heterogeneity (β dependent on D) that allows analysts
       to determine whether or not they can avoid the complexities that arise in the more general
  12
    Rosenbaum and Rubin (1983) establish the central role of the propensity score in matching models. Heckman
and Robb (1985, 1986) and Heckman (1980) establish the central role of the propensity score in selection models.
See also Ahn and Powell (1993) and Powell (1994).


                                                       9
      model with heterogeneity in response to treatments.

    6. We extend the analysis of IV to models with more than two outcomes. Angrist and Imbens
      (1995) analyze an ordered choice model with a scalar instrument that aﬀects choices at all
      margins. They show that in an ordered model with multiple outcomes, IV identifies a “causal
      parameter.”Their causal parameter is a weighted average of parameters that are diﬃcult to
      interpret as willingness-to-pay parameters or answers to well defined choice problems. We
      present an economically interpretable decomposition of standard IV into willingness-to-pay
      components for persons at well defined margins of choice. We show how to identify these
      components from data and how to construct the weights. We introduce transition specific
      instruments. We generalize this analysis to an unordered choice model.

    7. We show the fundamental asymmetry in the recent IV literature for models with heteroge-
      neous outcomes. Responses to treatment are permitted to be heterogeneous in a general way.
      Responses of choices to instruments are not. When heterogeneity in choice is allowed for in a
      general way, IV and local IV do not estimate interpretable treatment parameters.


    The paper is organized as follows. Section 2 discusses the IV approach to estimating choice
models. Section 3 introduces a general model with essential heterogeneity and presents its implica-
tions. Section 4 compares selection and IV models and shows that LIV estimates the derivative of a
selection model. Section 5 presents theoretical and empirical examples of the model with essential
heterogeneity. Section 6 extends the analysis to multiple outcome models. Section 7 allows choice
responses to be heterogeneous in a general way. Section 8 concludes.



2     IV in Choice Models

We adjoin a choice equation to outcome equations (1) and (2). A standard binary threshold crossing
model for D writes
                                         D = 1 [D∗ > 0] ,                                       (9)


                                                10
where 1[·] is an indicator (1[A] = 1 if A true; 0 otherwise).
      A familiar case is
                                                  D∗ = γZ − V                                       (10)

where (V ⊥
         ⊥ Z) | X (V is independent of Z given X). The propensity score or choice probability is


                            P (z) = Pr(D = 1 | Z = z) = Pr(γz > V ) = FV (γz)


where FV is the distribution of V which is assumed to be continuous. In terms of the Generalized Roy
model where C is the cost of participation in sector 1, D = 1[Y1 −Y0 −C > 0]. For a separable model
in outcomes (1) and in costs C = μC (W ) + UC , Z = (X, W ), μD (Z) = μ1 (X) − μ0 (X) − μC (W ),
V = − (U1 − U0 − UC ). In constructing examples, we use a special version where UC = 0. We call
this version the extended Roy model.13 Our analysis, however, applies to more general models.
      In the case where β (given X) is a constant under (IV-1) and (IV-2), it is not necessary to specify
the choice model to identify β. We show that in a general model with heterogenous responses, the
specification of P (z) and its relationship with the instrument play crucial roles. To see this, study
the covariance between Z and ηD discussed in the introduction. By the law of iterated expectations,
letting Z̄ denote the mean of Z

                                            ¡¡       ¢   ¢
                           Cov (Z, ηD) = E     Z − Z̄ Dη
                                            ¡¡       ¢       ¢
                                         = E Z − Z̄ η | D = 1 Pr (D = 1)
                                            ¡¡       ¢         ¢
                                         = E Z − Z̄ η | γZ > V Pr (γZ > V ) .


      Thus even if Z and η are independent, they are not independent conditional on D = 1[γZ > V ]
if η (= U1 − U0 ) is dependent on V (i.e., if the decision maker has partial knowledge of η and acts
on it). Selection models allow for this dependence (see Heckman and Robb (1985, 1986); Ahn and
 13
      The generalized Roy model allows UC 6= 0.




                                                      11
Powell (1993); Powell (1994)). Keeping X implicit and assuming that


                                          (U1 , U0 , V ) ⊥
                                                         ⊥Z                                     (11)


                           ⊥ Z), we obtain E(Y | D = 0, Z = z) = E(Y0 | D = 0, Z = z) =
(alternatively that (ε, η) ⊥
α + E(U0 | γz < V ) which can be written as


                               E(Y | D = 0, Z = z) = α + K0 (P (z)),


where the functional form of K0 is produced from the distribution of (U0 , V ). (This representation
is derived in Heckman, 1980; Heckman and Robb, 1985, 1986; Ahn and Powell, 1993; Powell, 1994.)
   Similarly,


                        E (Y | D = 1, Z = z) = E (Y1 | D = 1, Z = z)

                                               = α + β̄ + E (U1 | γz > V )

                                               = α + β̄ + K1 (P (z)),


where K0 (P (z)) and K1 (P (z)) are control functions in the sense of Heckman and Robb (1985, 1986).
Under standard conditions, we can identify β̄. Powell (1994) discusses semiparametric identification.
Because we condition on Z = z (or P (z)), correct specification of the Z plays an important role in
econometric selection methods. This sensitivity to the full set of instruments in Z appears to be
absent from the IV method.
   If β is a constant (given X), or if η (= β − β̄) is independent of V , only one instrument from
vector Z needs to be used. Missing instruments play no role in identifying mean responses but
may aﬀect the eﬃciency of the IV estimation. We establish that in a model where β is variable
and not independent of V , misspecification of Z plays an important role in interpreting what IV
estimates analogous to its role in selection models. Misspecification of Z aﬀects both approaches
to identification. This is a new phenomenon in models with heterogenous β. We now review some


                                                 12
results established in the preceding literature that form the platform on which we build.



3        A General Model with Essential Heterogeneity in Out-

         comes

We now exposit the selection model developed in Heckman and Vytlacil (1999, 2001b, 2005). Their
model for counterfactuals (potential outcomes) is more general than (1) and allows for nonseparable
errors:


                                             Y1 = μ1 (X, U1 ) ,                                              (12)

                                             Y0 = μ0 (X, U0 ) ,


where X are observed and (U1 , U0 ) are unobserved by the analyst. The X may be dependent on U0
and U1 in a general way. This model is designed to evaluate policies in place and not to extrapolate
to new environments characterized by X.14 The observed outcome is produced by equation (2).
       Choices are generated by a standard discrete choice model. We generalize choice model (9) and
(10) for D∗ , a latent utility,15


                                    D∗ = μD (Z) − V and D = 1 [D∗ ≥ 0] .                                     (13)


μD (Z) − V can be interpreted as a net utility for a person with characteristics (Z, V ). If it is
positive, D = 1 and the person selects into treatment; D = 0 otherwise. Section 7 discusses
the important role played by additive separability in the recent instrumental variable literature on
essential heterogeneity.
       In terms of the notation used in Section 1, β = Y1 − Y0 = μ1 (X, U1 ) − μ0 (X, U0 ). A special case
that links our analysis to standard models in econometrics writes Y1 = Xβ 1 +U1 and Y0 = Xβ 0 +U0
  14
     See Heckman and Vytlacil (2005, 2007a) for a study of exogeneity requirements for X in answering diﬀerent
policy questions.
  15
     A large class of latent index, threshold crossing models will have this representation. See Vytlacil (2006a).

                                                       13
so β = X (β 1 − β 0 )+(U1 − U0 ). In the case of separable outcomes, heterogeneity in β arises because
in general U1 6= U0 and people diﬀer in their X.16
      Following Heckman and Vytlacil (2005) we assume:

(A-1) (U0 , U1 , V ) are independent of Z conditional on X (Independence Condition for IV).

(A-2) The distribution of μD (Z) conditional on X is nondegenerate (Rank Condition for IV).17

(A-3) The distribution of V is continuous.18

(A-4) E |Y1 | < ∞, and E |Y0 | < ∞ (Finite Means).

(A-5) 1 > Pr (D = 1 | X) > 0 (For each X there is a treatment group and a comparison
group).

(A-6) Let X0 denote the counterfactual value of X that would have been observed if D is set to 0.
X1 is defined analogously. Thus Xd = X, for d = 0, 1 (The Xd are invariant to counterfactual
manipulations).

(A-1) and (A-2) generalize (IV-1) and (IV-2) respectively. (A-3) is a technical condition made
for convenience and is easily relaxed at some notational cost. (A-4) is needed to use standard
integration theorems and to have the mean treatment eﬀect parameters be well defined. (A-5) is a
standard requirement for any evaluation estimator that for each value of X, there be some who are
treated and some who are not. (A-6) is the requirement that receipt of treatment does not aﬀect
the realized value X, so we identify a full treatment eﬀect when we condition on X instead of a
treatment eﬀect that conditions on variables aﬀected by treatment. This assumption can be relaxed
by redefining the treatment to a set of outcomes corresponding to each Xd state.
      The separability between V and μD (Z) in the choice equation is conventional. It plays a crucial
role in justifying instrumental variable estimators in models with essential heterogeneity. It implies
monotonicity (uniformity) condition (IV-3) from choice equation (13). Fixing Z at two diﬀerent
 16
    In nonseparable cases, heterogeneity arises conditional on X even if U1 = U0 = U .
 17
    μD (·) is assumed to be a measurable function of Z given X.
 18
    The distribution is absolutely continuous with respect to Lebesgue measure.

                                                       14
values moves D(Z) in the same direction for everyone. Vytlacil (2002) shows that under indepen-
dence, rank and some regularity conditions, monotonicity (IV-3) implies the existence of a V in
representation (13). Thus the IV model for the general case and the economic choice model turn
out to have identical representations. Independence assumption (A-1), produces the condition that
everywhere Z enters the model only through P (Z). This is called index suﬃciency.
       Without any loss of generality, following the same argument surrounding (9) and (10), we may
write the model for D using the distribution of V , FV , as


                             D = 1 [FV (μD (Z)) > FV (V )] = 1 [P (Z) > UD ] ,                                  (14)


where UD = FV (V ) and P (Z) = FV (μD (Z)) = Pr(D = 1 | Z), the propensity score. Because FV is
assumed to be a continuous distribution, FV is a strictly monotonic transformation that preserves
the information in the original inequality. Note that UD is uniformly distributed by construction
(UD ∼ Unif [0, 1]).


3.1       LATE, The Marginal Treatment Eﬀect and Instrumental Variables



To understand what IV estimates in the model with general heterogeneity in response to treatment,
we define the marginal treatment eﬀect (or MTE) conditional on X and UD :19


                             ∆MTE (x, uD ) = E(Y1 − Y0 | X = x, UD = uD )

                                               = E (β | X = x, V = v) ,


for β = Y1 − Y0 and v = FV−1 (uD ), where we use both general notation and the regression specific
notation interchangeably to anchor our analysis both in the treatment eﬀect literature and in
  19
    As previously noted, the concept of the marginal treatment eﬀect and the limit form of LATE were first introduced
in the literature in the context of a parametric normal Generalized Roy Selection model by Björklund and Moﬃtt
(1987).



                                                         15
conventional econometrics. To simplify the notation, we keep the conditioning on X implicit except
when clarity of exposition dictates otherwise. Since P (Z) is a monotonic transformation of the
mean net utility μD (Z), and UD is a monotonic function of V , when we evaluate ∆MTE (uD ) at
the value P (z) = uD , it is the marginal return to agents with Z = z characteristics who are just
indiﬀerent between sector 1 and sector 0. In other words, at this point of evaluation, ∆MTE (uD )
is the gross gain of going from “0” to “1” for agents who are indiﬀerent between the sectors when
their mean utility given Z = z is μD (z) = v, so μD (z) − v = 0 which is equivalent to the event that
P (z) = FV (μD (z)) = FV (v) = uD . When Y1 and Y0 are denominated in value units, the MTE is
a willingness-to-pay measure for persons with characteristics Z = z at the specified margin.
      Under assumptions (A-1) to (A-5), Heckman and Vytlacil (1999, 2005) show that all treatment
parameters, matching estimators, IV estimators based on J (Z), a scalar function of Z, and OLS
estimators can be written as weighted averages of the MTE. Tables 1A and 1B summarize their
results for characterizing treatment eﬀects and estimators and the weights given data on P (Z) , D
and the instrument J (Z). We discuss the weights for IV in the next subsection. We show how to
construct these weights at our website, where software for doing so is available.20 Heckman and
Vytlacil (2001b, 2007b) show that these weights can be constructed and the relationships among
the parameters shown in Tables 1A and 1B hold even if a nonseparable choice model, instead of
(13), is used and even if assumption (A-2) is weakened. We discuss this result in Section 7.
      Notice that when ∆MTE does not depend on uD , all of the treatment eﬀects are the same and
that, under our assumptions, IV estimates all of them. In this case, ∆MTE can be taken outside the
integral and the weights all integrate to one. Thus, E(Y1 − Y0 | X = x) = ATE = E(Y1 − Y0 | X =
x, D = 1) = TT = MTE, and we are back to the conventional model of homogeneous responses.
This includes the case where η is nondegenerate but independent of D.
      The parameters MTE and LATE are closely related. Using the definition of D (z) in (IV-3), let
Z(x) denote the support of the distribution of Z conditional on X = x. For any (z, z 0 ) ∈ Z(x)×Z(x)
 20
      See jenni.uchicago.edu/underiv/.




                                                 16
so that P (z) > P (z 0 ), under (IV-3) and independence (A-1), LATE is:


                              ∆LATE (z 0 , z) = E (Y1 − Y0 | D (z) = 1, D (z 0 ) = 0) ,                          (15a)


i.e., the mean outcome in terms of Y1 − Y0 for persons who would be induced to switch from D = 0
to D = 1 if Z were manipulated externally from z 0 to z. As a consequence of Vytlacil’s (2002)
theorem, LATE can be written as


                                            E (Y1 − Y0 | D(z) = 1, D(z 0 ) = 0)                                  (15b)

                                       = E (Y1 − Y0 | u0D < UD < uD )

                                       = ∆LATE (uD , u0D )0


where uD = Pr(D (z) = 1) = Pr (D = 1 | Z = z) = P (z), u0D = Pr (D (z 0 ) = 1 | Z = z 0 ) =
Pr(D (z 0 ) = 1) = P (z 0 ).21 In the limit, as u0D → uD , LATE converges to MTE.
      Imbens and Angrist (1994) define the LATE parameter from hypothetical manipulations of an
instrument. Heckman and Vytlacil (1999, 2005) draw on choice theory and define the parameters in
terms of the generalized Roy Model. Their link helps to understand what IV estimates and relates
IV to choice models. We work with definition (15b) throughout the rest of this paper. It enables
us to identify the margin of UD selected by instruments, something currently not possible in results
in the previous literature on IV.
      The MTE can be identified by taking derivatives of E (Y | Z = z) with respect to P (z) (see
Heckman and Vytlacil, 1999).22 This derivative is called the local instrumental variable (LIV). For
the model of general heterogeneity, under assumptions (A-1) to (A-5), we can write (keeping the
conditioning on X = x implicit)


                                        E (Y | Z = z) = E(Y | P (Z) = p)
 21
      Assumption (A-1) implies that Pr (D (z) = 1) = Pr (D = 1 | Z = z), and Pr (D (z 0 ) = 1) = Pr (D = 1 | Z = z 0 ).
 22
      See also Heckman and Vytlacil (2005, 2007b).



                                                           17
                    E (Y | P (Z) = p) = E (DY1 + (1 − D)Y0 | P (Z) = p)

                                       = E(Y0 ) + E (D (Y1 − Y0 ) | P (Z) = p)

                                       = E(Y0 ) + E (Y1 − Y0 | D = 1) p
                                                  Z p
                                       = E(Y0 ) +     E(Y1 − Y0 | UD = uD ) duD .
                                                       0



As a consequence,
                                               ¯
                          ∂                    ¯
                             E (Y | P (Z) = p)¯¯        = E(Y1 − Y0 |UD = p).                  (16)
                          ∂p                    P (z)=p

Expression (16) shows how the derivative of E (Y | Z = z), which is the local instrumental variable
(LIV) estimand of Heckman and Vytlacil (1999), identifies the marginal treatment eﬀect (the right
hand side of this expression) over the support of P (Z). Observe that a high value of P (Z) = p
identifies MTE at a value of UD = uD that is high, i.e. that is associated with nonparticipation. It
takes a high p to compensate for the high UD = uD and bring the agent to indiﬀerence (see equation
14). Thus high p values identify returns to persons whose unobservables make them less likely to
participate in the program. Software for estimating MTE using local linear regression is described
in Appendix B and is available online at jenni.uchicago.edu/underiv.
   Under the special case where β ⊥
                                  ⊥ D (no essential heterogeneity), Y is linear in P (Z):


                                      E (Y | Z) = a + bP (Z) ,                                 (17)


where b = ∆MTE = ∆ATE = ∆TT . This representation holds whether or not Y1 and Y0 are separable
in U1 and U0 , respectively (see Heckman and Vytlacil, 2001b, 2007b). Thus a test of the linearity
of the conditional expectation of Y in terms of P (Z) is a test of whether the conventional model
or the model of essential heterogeneity generates the data. One useful empirical strategy is to test
for linearity using the variety of tests developed in the literature and to determine whether the
additional complexity introduced by the model of essential heterogeneity is warranted.
   Using the formulae presented in Tables 1A and 1B, all of the traditional treatment parameters
as well as the IV estimator using P (Z) as an instrument can be identified as weighted averages of


                                                  18
∆MTE (uD ) if P (Z) has full support. The weights can be constructed from data. If P (Z) does not
have full support, simple tight bounds on these parameters can be constructed.23


3.2       Understanding What IV Estimates

Standard IV based on J (Z), a scalar function of a vector Z, can be written as

                                               Z      1
                                     ∆IV
                                      J    =              ∆MTE (uD ) ω JIV (uD ) duD ,                         (18)
                                                  0



where
                                   E (J (Z) − E (J (Z)) | P (Z) > uD ) Pr (P (Z) > uD )
                    ωJIV (uD ) =                                                        .                      (19)
                                                     Cov (J (Z) , D)

In this expression uD is a number between zero and one. This weight depends on the choice prob-
ability P (Z). For a derivation see Appendix A. The derivation does not impose any assumptions
on the distribution of J(Z) or P (Z). Notice that J(Z) and P (Z) do not have to be continuous
random variables, and that the functional forms of P (Z) and J(Z) are general.24
       For ease of exposition, we initially assume that J(Z) and P (Z) are both continuous. This
assumption plays no essential role in any of the results of this paper and we develop the discrete
case after developing the continuous case. The weights defined in (19) can be written as

                                              R                        R1
                                                  (j − E(J (Z)))        uD
                                                                             fJ,P (j, t) dt dj
                              ω JIV (uD ) =                                                      ,             (20)
                                                               Cov (J (Z) , D)

where fJ,P is the joint density of J(Z) and P (Z) and we implicitly condition on X. The weights can
be negative or positive. Observe that ω (0) = 0 and ω (1) = 0. The weights integrate to 1,25 so even
if the weight is negative over some intervals, it must be positive over other intervals. When there is
one instrument (Z is a scalar), and assumptions (A-1) to (A-5) are satisfied, the weights are always
positive provided that J (Z) is a monotonic function of scalar Z. In this case J (Z) and P (Z)
  23
     See Heckman and Vytlacil (1999, 2001a,b, 2007b).
  24
     More precisely, J(Z) and P (Z) do not have to have distributions that are absolutely continuous with respect to
Lebesgue measure.
  25
     RR                R1
         (j − E (J(Z))) uD fJ,P (j, t) dt dj duD = Cov (J (Z) , D).

                                                                19
have the same distribution and fJ,P (j, t) collapses to a univariate distribution. The possibility of
negative weights arises when J (Z) is not a monotonic function of P (Z). It can also arise when
there are two or more instruments, and the analyst computes estimates with only one instrument
or a combination of the Z instruments that is not a monotonic function of P (Z) so that J (Z) and
P (Z) are not perfectly dependent. If the instrument is P (Z) (so J (Z) = P (Z)) then the weights
are everywhere non-negative because from (19) E(P (Z) | P (Z) > uD ) − E (P (Z)) ≥ 0. In this
case the density of (P (Z) , J (Z)) collapses to the density of P (Z). For any scalar Z we can define
J (Z) and P (Z) so that they are perfectly dependent, provided J(Z) and P (Z) are monotonic in
Z. More generally, weight (19) is positive if E(J (Z) | P (Z) > uD ) is weakly monotonic in uD .
Nonmonotonicity of this conditional expectation can produce negative weights.26
       Observe that the weights can be constructed from data on (J, P, D). Data on (J (Z) , P (Z))
pairs and (J (Z) , D) pairs (for each X value) are all that is required. We can use a smoothed
sample frequency to estimate the joint density fJ,P . Thus, given our maintained assumptions, any
property of the weight, including its positivity at any point (x, uD ), can be examined with data.
We present examples of this approach in section 5.
       As is evident from Tables 1A and 1B, the weights on ∆MTE (uD ) generating ∆IV are diﬀerent from
the weights on ∆MTE (uD ) that generate the average treatment eﬀect which is widely regarded as an
important policy parameter (see, e.g. Imbens, 2004) or from the weights associated with the policy
relevant treatment parameter which answers well-posed policy questions (Heckman and Vytlacil,
1999, 2001b, 2005, 2007b). It is not obvious why the weighted average of ∆MTE (uD ) produced by
IV is of any economic interest. Since the weights can be negative for some values of uD , ∆MTE (uD )
can be positive everywhere in uD but IV can be negative. Thus, IV may not estimate a treatment
eﬀect for any person. Therefore, a basic question is why estimate the model with IV at all given
the lack of any clear economic interpretation of the IV estimator in the general case.
       Our analysis can be extended to allow for discrete instruments, J (Z). Consider the case where
the distribution of P (Z) (conditional on X) is discrete. The support of the distribution of P (Z)
  26
   If it is weakly monotonically increasing, the claim is evident from (19). If it is decreasing, the sign of the
numerator and the denominator are both negative so the weight is nonnegative.


                                                       20
contains a finite number of values p1 < p2 < · · · < pK and the support of the instrument J (Z)
is also discrete, taking I distinct values, where I and K may be distinct. E(J(Z)|P (Z) ≥ uD ) is
constant in uD for uD within any (p , p +1 ) interval, and Pr(P (Z) ≥ uD ) is constant in uD for uD
within any (p , p +1 ) interval, and thus ωJIV (uD ) is constant in uD over any (p , p +1 ) interval. Let
λ denote the weight on the LATE for the interval ( , + 1). In this notation,

                                   Z
                        ∆IV
                         J     =       E(Y1 − Y0 |UD = uD )ω JIV (uD ) duD                                      (21)

                                   X
                                   K−1       Z     p   +1
                                                                                              1
                               =         λ                  E(Y1 − Y0 |UD = uD )                     duD
                                    =1         p                                         (p +1 − p )
                                   X
                                   K−1
                               =         ∆LATE (p , p +1 )λ .
                                    =1


       Let ji be the ith smallest value of the support of J(Z). The discrete version of (19) is

                                       P
                                       I                         P
                                                                 K
                                             (ji − E (J))             (f (ji , pt ))
                                       i=1                       t>
                                λ =                                                    (p +1 − p )              (22)
                                                   Cov (J (Z) , D)

where f (ji , pt ) is the probability frequency of (ji , pt ): the probability that J (Z) = ji and P (Z) = pt .
There is no presumption that high values of J(Z) are associated with high values of P (Z). J(Z)
can be one coordinate of Z that may be positively or negatively dependent on P (Z) which depends
on the full vector. In the case of scalar Z, as long as J(Z) and P (Z) are monotonic in Z, there
is perfect dependence between J(Z) and P (Z). In this case, the joint probability density collapses
to a univariate density and the weights have to be positive, exactly as in the case with continuous
instruments.27 Our expression for the weight on LATE generalizes the expression presented by
Imbens and Angrist (1994) who in their analysis of the case of vector Z only consider the case
where J(Z) and P (Z) are perfectly dependent because J(Z) is a monotonic function of P (Z).28
  27
     The condition for positive weights is weak monotonicity of λ in . If λ is monotone increasing in , the numerator
and the denominator are both positive. If λ is monotone decreasing, the numerator and the denominator are both
negative and the weights are positive.
  28
     In their case, I = K and f (ji , pt ) = 0, ∀ i 6= t.




                                                                 21
More generally the weights can be positive or negative for any but they must sum to 1 over the .
       Monotonicity or uniformity is a property needed with just two values of Z, Z = z1 and Z = z2 ,
to guarantee that IV estimates a treatment eﬀect. With more than two values of Z we need to
weight the LATEs and MTEs. If the instrument J(Z) shifts P (Z) in the same way for everyone,
it shifts D in the same way for everyone since D = 1 [P (Z) > UD ] and Z is independent of UD . If
J(Z) is not monotonic in P (Z), it may shift P (Z) in diﬀerent ways for diﬀerent people. Negative
weights are a tip-oﬀ of two-way flows.
       An alternative and in some ways more illuminating way to derive the weights is to follow Yitzhaki
(1989, 1996) and Yitzhaki and Schechtman (2004) who prove for a general regression function
E (Y | P (Z) = p) that a linear regression of Y on P estimates

                                          Z 1∙                      ¸
                                                 ∂E (Y | P (Z) = p)
                                β Y,P =                               ω (p) dp,                             (23)
                                           0             ∂p

where                                             R1
                                                   p
                                                       (t − E (P )) dFP (t)
                                      ω (p) =                                 ,
                                                           Var (P )

which is exactly the weight (19) when P is the instrument. Thus we can interpret (19) as the weight
     ∂E(Y |P (Z)=p)
on         ∂p
              when two-stage least squares (TSLS) based on P (Z) as the instrument is used to esti-
                                                                      ¯
                                                                      ¯
mate the “causal eﬀect” of D on Y . Under uniformity, ∂E(Y |P∂p(Z)=p) ¯ = E (Y1 − Y0 | UD = uD ) =
                                                                                  p=uD
∆MTE (uD ).29 We discuss Yitzhaki’s derivation which is an argument based on integration by parts
in Appendix C. Our analysis is more general than that of Yitzhaki (1989), Imbens and Angrist
(1994), or Angrist and Imbens (1995) because we allow for instruments that are not monotonic
functions of P (Z). Yitzhaki’s (1989) analysis is more general than that of Imbens and Angrist
(1994) because he does not impose uniformity (monotonicity).
       Our simple test for the absence of general heterogeneity based on the linearity of Y in P (Z)
(based on equation 20) applies to the case of LATE for any pair of instruments. An equivalent test
  29
    Yitzhaki’s weights are used by Angrist and Imbens (1995) to interpret what TSLS estimates in the model of
equation (23). Yitzhaki (1989) derives the finite sample weights used by Imbens and Angrist (See his paper posted
at our website). See also the refinement in Yitzhaki and Schechtman (2004).



                                                          22
is to check that all pairwise LATEs are the same over the sample support of Z.30


3.3     The Central Role of the Propensity Score

Observe that both (19) and (20) (and their counterparts for LATE (21) and (22)) contain expres-
sions involving the propensity score P (Z), the probability of selection into treatment. Under our
assumptions, it is a monotonic function of the mean utility of treatment, μD (Z). The propensity
score plays a central role in selection models as a determinant of control functions in selection mod-
els (see Heckman and Robb, 1985, 1986) as noted in Section 2. In matching models, it provides a
computationally convenient way to condition on Z (see, e.g. Rosenbaum and Rubin, 1983; Heckman
and Navarro, 2004). For the IV weight to be correctly constructed and interpreted, we need to know
the correct model for P (Z), i.e., we need to know exactly which Z determine P (Z). As previously
noted, this feature is not required in the traditional model for instrumental variables based on re-
sponse homogeneity. In that simpler framework, any instrument will identify μ1 (X) − μ0 (X) and
the choice of a particular instrument aﬀects eﬃciency but not identifiability. One can be casual
about the choice model in the traditional setup, but not in the model of choice of treatment with
essential heterogeneity. Thus, unlike the application of IV to traditional models, IV applied in the
model of essential heterogeneity depends on (a) the choice of the instrument J (Z), (b) its depen-
dence with P (Z), the true propensity score or choice probability and (c) the specification of the
propensity score (i.e., what variables go into Z). Using the propensity score one can identify LIV
and LATE and the marginal returns at values of the unobserved UD .


3.4     Monotonicity, Uniformity and Conditional Instruments

Monotonicity or uniformity condition (IV-3), is a condition on counterfactuals for the same persons
and is not testable. It rules out general heterogeneous responses to treatment choices in response
to changes in Z. The recent literature on instrumental variables with heterogeneous responses is
  30
   Note that it is possible that E (Y | Z) is linear in P (Z) only over certain intervals of UD , so there can be local
dependence and local independence of (UD , U0 , U1 ).



                                                          23
thus asymmetric. Outcome equations can be heterogeneous in a general way while choice equations
cannot be. If μD (Z) = γZ, where γ is a common coeﬃcient shared by everyone, the choice
model satisfies the uniformity property. On the other hand, if γ is a random coeﬃcient (i.e., has a
nondegenerate distribution) that can take both negative and positive values, and there are two or
more variables in Z with nondegenerate γ coeﬃcients, uniformity can be violated. Diﬀerent people
can respond to changes in Z diﬀerently, so there is non-uniformity. The uniformity condition can be
violated even when all components of γ are of the same sign if Z is a vector and γ is a nondegenerate
random variable.31
          Changing one coordinate of Z, holding the other coordinates at diﬀerent values across people, is
not the experiment that defines monotonicity or uniformity. Changing one component of Z, allowing
the other coordinates to vary across people, does not necessarily produce uniform flows toward or
against participation in the treatment status. For example, let μD (z) = γ 0 + γ 1 z1 + γ 2 z2 + γ 3 z1 z2 ,
where γ 0 , γ 1 , γ 2 and γ 3 are constants, and consider changing z1 from a common base state while
holding z2 fixed at diﬀerent values across people. If γ 3 < 0 then μD (z) does not necessarily satisfy
the uniformity condition. If we move (z1 , z2 ) as a pair from the same base values to the same
destination values z 0 , uniformity is satisfied even if γ 3 < 0, although μD (z) is not a monotonic
function of z.32
          Positive weights and uniformity are distinct issues.33 Under uniformity, and assumptions (A-
     31
      Thus if γ > 0 for each component and some components of Z are positive and others are negative, changes from
 0
z to z can increase γZ for some and decrease γZ for others since γ are diﬀerent among persons.
   32
      Associated with Z = z is the counterfactual random variable D (z). Associated with the scalar random variable
J (Z) constructed from Z is a counterfactual random variable D (j (z)) which is in general diﬀerent from D (z). The
random variable D (z) is constructed from (13) using 1[μD (z) ≥ V ]. V assumes individual specific values which
remain fixed as we set diﬀerent z values. From (A-1), Pr(D (z) = 1) = Pr(D = 1 | Z = z). The random variable
D (j) is defined by the following thought experiment. For each possible realization j of J(Z) define D (j) by setting
D (j) = D (Z (j)) where Z (j) is a random draw from the distribution of Z conditional on J(Z) = j. Set D (j) equal
to the choice that would be made given that draw of Z (j). Thus D (j) is a function of (Z (j) , uD ). As long as we
draw Z (j) randomly (so independent of Z), we have that (Z (j) , UD ) ⊥  ⊥ Z so D (j) ⊥⊥ Z. There are other possible
constructions of the counterfactual D (j) since there are diﬀerent possible distributions from which Z can be drawn,
apart from the actual distribution of Z. The advantage of this construction is that it equates the counterfactual
probability that D (j) = 1 given J (Z) = j with the population probability. If the Z were uncertain to the agent,
this would be a rational expectations assumption. See the further discussion in Appendix II posted at the website
for this paper.
   33
      When they analyze the vector case, Imbens and Angrist (1994) analyze instruments that are monotonic functions
of P (Z). Our analysis is more general and recognizes that in the vector case, IV weights may be negative or positive.



                                                         24
1) to (A-5), the weights on MTE for any particular instrument may be positive or negative. The
weights for MTE using P (Z) must be positive as we have shown so the propensity score has a special
status as an instrument. Negative weights associated with the use of J (Z) as an instrument do not
necessarily imply failure of uniformity in Z. Even if uniformity is satisfied for Z, it is not necessarily
satisfied for J (Z). Condition (IV-3) is an assumption about a vector. Fixing one combination of
Z (when J is a function of Z) or one coordinate of Z does not guarantee uniformity in J even if
there is uniformity in Z. The flow created by changing one coordinate of Z can be reversed by the
flow created by other components of Z if there is negative dependence among components, even if
ceteris paribus all components of Z aﬀect D in the same direction. We present some examples in
Section 5.
       The issues of positive weights and the existence of one way flows in response to an intervention
are conceptually distinct. Even with two values for a scalar Z, flows may be two way (see equation
(7)). If we satisfy (IV-3) for a vector, so uniformity applies, weights for a particular instrument
may be negative for certain intervals of UD (i.e., for some of the LATE parameters).
       If we condition on Z2 = z2 , . . . , ZK = zK using Z1 as an instrument, then a uniform flow
condition is satisfied. We call this conditional uniformity. By conditioning, we eﬀectively convert
the problem back to that of a scalar instrument where the weights must be positive. If uniformity
holds for Z1 , fixing the other Z at common values, one dimensional LATE/MTE analysis applies.
Clearly, the weights also have to be defined conditionally.
       The concept of conditioning on other instruments to produce positive weights for the selected
instrument is a new one, not yet appreciated in the empirical IV literature and has no counterpart
in the traditional IV model. In the conventional model, the choice of a valid instrument aﬀects
eﬃciency but not the definition of the parameters as it does in the more general case.34
       In summary, nothing in the economics of choice models guarantees that if Z is changed from
  34
    In the conventional model with homogeneous responses, a linear probability approximation to P (Z) used as an
instrument would identify the same parameter as P (Z). In the general model, the parameters identified are diﬀerent.
Replacing P (Z) by a linear probability approximation of it (e.g. E (D | Z) = πZ = J(Z)) is not guaranteed to
produce positive weights for ∆M TE (x, uD ) or ∆LATE (x, u0D , uD ), or to replicate the weights based on the correctly
specified P (Z).



                                                          25
z to z 0 , people respond in the same direction to the change. See the general expression (7). The
condition that people respond to choices in the same direction for a common change in Z across
people does not imply that D(z) is monotonic in z for any person in the usual mathematical usage
of the term monotonicity. If D(z) is monotonic in the usual usage of this term, and responses are
in the same direction for all people, then “monotonicity” or “uniformity” condition (IV-3) would
be satisfied.
   If responses to a common change of Z across persons are heterogenous in a general way, we
obtain (7) as the general case. Vytlacil’s (2002) theorem breaks down and IV cannot be expressed
in terms of a weighted average of LATE terms. Nonetheless, Yitzhaki’s characterization of IV
                                                                               ∂E(Y |P =p)
equation (23) as described in Appendix C remains valid and the weights on          ∂p
                                                                                             are positive
and of the same form as the weights obtained for MTE (or LATE) when the monotonicity condition
holds.


3.5      Treatment Eﬀects vs. Policy Eﬀects

Even if uniformity condition (IV-3) fails, IV may answer relevant policy questions. By Yitzhaki’s
result (23), IV or TSLS estimates a weighted average of marginal responses which may be pointwise
positive, zero or negative. Policies may induce some people to switch into and others to switch
out of choices, as is evident from equation (7). These net eﬀects are of interest in many policy
analyses. Thus, subsidized housing in a region supported by higher taxes may attract some to
migrate to the region and cause others to leave. The net eﬀect on earnings from the policy is
all that is required to perform cost benefit calculations of the policy on outcomes. If the housing
subsidy is the instrument and the net eﬀect of the subsidy is the parameter of interest, the issue of
monotonicity is a red herring. If the subsidy is exogenously imposed, IV estimates the net eﬀect of
the policy on mean outcomes. Only if the eﬀect of migration on earnings induced by the subsidy
on outcomes is the question of interest, and not the eﬀect of the subsidy, does uniformity emerge
as an interesting condition.




                                                 26
4     Comparing Selection and Local IV Models

We now show that local IV identifies the derivatives of a selection model. Making the X explicit,
in the standard selection model, if the U1 and U0 are scalar random variables that are additively
separable in the outcome equations, Y1 = μ1 (X) + U1 and Y0 = μ0 (X) + U0 . The control function
approach conditions on Z and D. As a consequence of index suﬃciency this is equivalent to
conditioning on P (Z) and D:


                        E (Y | X, D, Z) = μ0 (X) + [μ1 (X) − μ0 (X)] D

                                             +K1 (P (Z) , X) D

                                             +K0 (P (Z) , X) (1 − D) ,


where the control functions are


                          K1 (P (Z), X) = E(U1 | D = 1, X, P (Z))

                          K0 (P (Z), X) = E (U0 | D = 0, X, P (Z)) .


    The IV approach does not condition on D. It works with


                       E (Y | X, Z) = μ0 (X) + [μ1 (X) − μ0 (X)] P (Z)                      (24)

                                          +K1 (P (Z) , X) P (Z)

                                          +K0 (P (Z) , X) (1 − P (Z)) ,


the population mean outcome given X, Z.
    From index suﬃciency, E (Y | X, Z) = E (Y | X, P (Z)). The MTE is the derivative of this
expression with respect to P (Z), which we have defined as LIV:

                                       ¯
                     ∂E(Y | X, P (Z)) ¯¯
                                       ¯         = LIV (X, p) = MTE (X, p) .35
                         ∂P (Z)          P (Z)=p



                                                27
The distribution of P (Z) and the relationship between J (Z) and P (Z) determine the weight on
MTE.36 Under assumptions (A-1) to (A-5), along with rank and limit conditions (Heckman and
Robb, 1985; Heckman, 1990), one can identify μ1 (X), μ0 (X), K1 (P (Z) , X), and K0 (P (Z) , X).
       The selection (control function) estimator identifies the conditional means


                            E (Y1 | X, P (Z), D = 1) = μ1 (X) + K1 (X, P (Z))                                (25a)


and
                            E (Y0 | X, P (Z), D = 0) = μ0 (X) + K0 (X, P (Z)) .                              (25b)




These can be identified from nonparametric regressions of Y1 and Y0 on X, Z in each population. To
decompose these means and separate μ1 (X) from K1 (X, P (Z)) without invoking functional form
or curvature assumptions, it is necessary to have an exclusion (a Z not in X).37 In addition there
must exist a limit set for Z given X such that K1 (X, P (Z)) = 0 for Z in that limit set. Otherwise,
without functional form or curvature assumptions, it is not possible to disentangle μ1 (X) from
K1 (X, P (Z)) which may contain constants and functions of X that do not interact with P (Z) (see
Heckman (1990)). A parallel argument for Y0 shows that we require a limit set for Z given X such
that K0 (X, P (Z)) = 0. Selection models operate by identifying the components of (25a) and (25b)
and generating the treatment parameters from these components. Thus they work with levels of
the Y .
       The local IV method works with derivatives of (24) and not levels and cannot directly recover
the constant terms in (25a) and (25b). Using our analysis of LIV but applied to Y D = Y1 D and
Y (1 − D) = Y0 (1 − D), it is straightforward to use LIV to estimate the components of the MTE
  35
     Björklund and Moﬃtt (1987) analyze this marginal eﬀect for a parametric generalized Roy model.
  36
     Because LIV does not condition on D, it discards information. Lost in taking derivatives are the constants in
the model that do not interact with P (Z) in equation (24).
  37
     See Heckman and Navarro (2006) for use of semiparametric curvature restrictions in identification analysis that
do not require functional form assumptions.




                                                        28
separately. Thus we can identify


                                        μ1 (X) + E (U1 | X, UD = uD )


and
                                        μ0 (X) + E (U0 | X, UD = uD )

separately. This corresponds to what is estimated from taking the derivatives of expressions (25a)
and (25b) multiplied by P (Z) and (1 − P (Z)) respectively:38


                                         P (Z)E (Y1 | X, Z, D = 1)

                                    = P (Z)μ1 (X) + P (Z)K1 (X, P (Z))


and


                                 (1 − P (Z))E (Y0 | X, Z, D = 0)

                             = (1 − P (Z))μ0 (X) + (1 − P (Z))K0 (X, P (Z)) .


Thus the control function method works with levels, whereas the LIV approach works with slopes.
Constants that do not depend on P (Z) disappear from the estimates of the model. The level
parameters are obtained by integration using the formulae in Table 1B.
       Misspecification of P (Z) (either its functional form or its arguments) and hence of K1 (P (Z) , X)
and K0 (P (Z) , X) in general produces biased estimates of the parameters of the model under the
control function approach even if semiparametric methods are used to estimate μ0 , μ1 , K0 and K1 .
To implement the method, we need to know all of the arguments of Z. The terms K1 (P (Z) , X)
and K0 (P (Z) , X) can be nonparametrically estimated so it is only necessary to know P (Z) up
to a monotonic transformation.39 The distributions of U1 , U0 and V do not need to be specified to
  38
     Björklund and Moﬃtt (1987) use the derivative of a selection model in levels to define the marginal treatment
eﬀect.
  39
     See Heckman, Ichimura, Smith, and Todd (1998).


                                                       29
estimate control function models (see Powell, 1994).
       These problems with control function models have their counterparts in IV models. If we use
a misspecified P (Z) to identify the MTE or its components, in general we do not identify MTE or
its components. Misspecification of P (Z) plagues both approaches.
       One common criticism of selection models is that without invoking functional form assump-
tions, identification of μ1 (X) and μ0 (X) requires that P (Z) → 1 and P (Z) → 0 in limit sets.40
Identification in limit sets is sometimes called “identification at infinity.” In order to identify
ATE = E(Y1 − Y0 |X), IV methods also require that P (Z) → 1 and P (Z) → 0 in limit sets,
so an identification at infinity argument is implicit when IV is used to identify this parameter.41
The LATE parameter avoids this problem by moving the goal posts and redefining the parameter
of interest away from a level parameter like ATE or TT to a slope parameter like LATE which
diﬀerences out the unidentified constants. Alternatively, if we define the parameter of interest to
be LATE or MTE, we can use the selection model without invoking identification at infinity.
       The IV estimator is model dependent, just like the selection estimator, but in application, the
model does not have to be fully specified to obtain ∆IV using Z (or J(Z)). However, the distribution
of P (Z) and the relationship between P (Z) and J (Z) generates the weights. The interpretation
placed on ∆IV in terms of weights on ∆MTE depends crucially on the specification of P (Z). In both
control function and IV approaches for the general model of heterogeneous responses, P (Z) plays
a central role.
       Two economists using the same instrument will obtain the same point estimate using the same
data. Their interpretation of that estimate will diﬀer depending on how they specify the arguments
in P (Z), even if neither uses P (Z) as an instrument. By conditioning on P (Z), the control function
approach makes the dependence of estimates on the specification of P (Z) explicit. The IV approach
is less explicit and masks the assumptions required to economically interpret the empirical output of
an IV estimation. We now turn to some examples that demonstrate the main points of this paper.
  40
     See Imbens and Angrist (1994). Heckman (1990) establishes the identification in the limit argument for ATE in
selection models. See Heckman and Navarro (2006) for a generalization to multiple outcome models.
  41
     Thus if the support of P (Z) is not full, we cannot identify treatment on the treated or the average treatment
eﬀect. We can construct bounds. See Heckman and Vytlacil (1999, 2001a,b, 2007b).


                                                        30
5       Examples Based on Choice Theory

Return to the policy adoption example presented in Section 1. The cost of adopting the policy C is
the same across all countries. Suppose that countries choose to adopt the policy if D∗ > 0 where D∗
is the net benefit of adoption: D∗ = (Y1 − Y0 − C) and ATE = E (β) = E (Y1 − Y0 ) = μ1 −μ0 , while
treatment on the treated is E (β | D = 1) = E (Y1 − Y0 | D = 1) = μ1 − μ0 + E (U1 − U0 | D = 1).
      In this setting, the gross return to the country at the margin is C, i.e.,


                           E (Y1 − Y0 | D∗ = 0) = E (Y1 − Y0 | Y1 − Y0 = C) = C.


Figure 1 presents the standard treatment parameters for the values of the outcome and choice
parameters presented at the base of the figure. Countries that adopt the policy are above average.
In a model where the cost varies (the generalized Roy model with UC 6= 0), and C is negatively
correlated with the gain, adopting countries could be below average.42


5.1       Discrete Instruments and the Weights for LATE

Consider what instrumental variables identify in the model of choice and outcomes described below
Figure 2. Let cost C = γZ where instrument Z = (Z1 , Z2 ). Higher values of Z reduce the probability
of adopting the policy if γ ≥ 0, component by component. Consider the “standard” case depicted
in Figure 2A. Increasing both components of discrete-valued Z raises costs and hence raises the
return observed for the country at the margin by eliminating adoption in low return countries. In
general, a diﬀerent country is at the margin when diﬀerent instruments are used.
      Figure 3A plots the weights and Figure 3B the components of the weights for the LATE values
using P (Z) as an instrument for the distribution of Z shown at the base of the figure. Figure 3C
presents the LATE parameter derived using P (Z) as the instrument. The weights are positive as
predicted from equation (22) when J(Z) = P (Z). Thus the monotonicity condition for the weights
in terms of uD is satisfied. The outcome and choice parameters are the same as those used to
 42
      See, e.g. Heckman (1976a,b).


                                                    31
generate Figure 1 and 2. There are four LATE values corresponding to the five distinct values of
the propensity score for this example. The LATEs exhibit the declining pattern with uD predicted
by the Roy model.
   A more interesting case is that depicted in Figure 2B. In that graph, the same Z are used
to generate choices as in Figures 2A and 3. However in this case, the analyst uses Z1 as the
instrument, Z1 and Z2 are negatively dependent and E(Z1 | P (Z) > uD ) is not monotonic in uD .
This nonmonotonicity is evident in Figure 4B. This produces the pattern of negative weights shown
in Figure 4A. These are associated with two way flows. Increasing Z1 controlling for Z2 reduces the
probability of country policy adoption. However, we do not condition on Z2 in constructing this
Figure. It is floating. Two way flows are induced by uncontrolled variation in Z2 . For some units,
the strength of the associated variation in Z2 oﬀsets the increase in Z1 and for other units it does
not. Observe that the LATE parameters defined using P (Z) are the same in both examples. They
are just weighted diﬀerently. We discuss the random coeﬃcient choice model generating Figure 2C
in Section 7.
   The IV estimator does not identify ATE, TT or TUT given at the bottom of Figure 3. Condi-
tioning on Z2 produces positive weights, as shown in the weights in Table 2 that condition on Z2 .
Conditioning on Z2 eﬀectively converts the problem back into one with a scalar instrument and the
weights must be positive for that case.
   By Yitzhaki’s result (23), for any sample size, a regression of Y on P identifies a weighted
                                                E(Y |p )−E(Y −1 |p   −1 )
average of slopes based on ordered regressors          p −p −1
                                                                            where p > p −1 where the weights
are the positive Yitzhaki weights derived in Appendix C, Yitzhaki (1989, 1996) or in Yitzhaki
and Schechtman (2004). The weights are positive whether or not monotonicity (IV-3) holds. If
monotonicity holds, IV is a weighted average of LATEs. Otherwise it is just a weighted average of
ordered (by p ) estimators consistent with two way flows.




                                                  32
5.2    Continuous Instruments

For the case of continuous Z, we present a parallel analysis for the weights associated with the
MTE. Figure 5 plots E(Y | P (Z)) and MTE for the models generated by the parameters displayed
at the base of the figure. In cases I and II, β ⊥
                                                ⊥ D. In case I, this is trivial since β is a constant. In
case II, β is random but selection into D does not depend on β. Case III is the model with essential
heterogeneity (β ⊥
                 ⊥ D). The left hand side (Figure 5A) depicts E(Y | P (Z)) for the three cases.
                 Á
Cases I and II make E(Y | P (Z)) linear in P (Z) (see equation 17). Case III is nonlinear in P (Z).
This arises when β ⊥
                   ⊥ D. The derivative of E(Y | P (Z)) is presented in the right panel (Figure 5B).
                   Á
It is a constant for cases I and II (flat MTE) but declining in UD = P (Z) for the case with selection
on the gain. A simple test for linearity in P (Z) in the outcome equation reveals whether or not the
analyst is in cases I and II (β ⊥
                                ⊥ D) or case III (β⊥
                                                   ⊥D). Recall that we keep conditioning on X
                                                   Á
implicit.
   MTE gives the mean marginal return for persons who have utility P (Z) = uD (P (Z) = uD is
the margin of indiﬀerence). Those with low uD values have high returns. Those with high uD values
have low returns. Figure 5 highlights that MTE (and LATE) identify average returns for persons
at the margin of indiﬀerence at diﬀerent levels of the mean utility function (P (Z)).
   Figure 6A plots MTE and LATE for diﬀerent intervals of uD using the model generating Figure 5.
LATE is the chord of E(Y | P (Z)) evaluated at diﬀerent points. The relationship between LATE
and MTE is depicted in the right panel of Figure 6. LATE is the integral under the MTE curve
divided by the diﬀerence between the upper and lower limits.
   The treatment parameters associated with case III are plotted in Figure 7. The MTE is the
same as that presented in Figure 5. ATE has the same value for all p. The eﬀect of treatment on
the treated for P (Z) = p, ∆TT (p) = E(Y1 − Y0 | D = 1, P (Z) = p) declines in p (equivalently, it
declines in uD ). Treatment on the untreated given p, ∆TUT (p) = E(Y1 − Y0 | D = 0, P (Z) = p) also




                                                   33
declines in p. Observe that

                                         ∆TT (p0 )p0 − ∆TT (p)p
                            LATE(p, p0 ) =                      ,            p0 6= p,
                                                  p0 − p
                                         ∂[∆TT (p)p]
                                   MTE =              .
                                            ∂p

We can generate all of the treatment parameters from ∆TT (p).
      Matching on P = p (which is equivalent to nonparametric least squares, given P = p) produces a
biased estimator of TT(p). Matching assumes a flat MTE (average return equals marginal return).43
Therefore it is systematically biased for ∆TT (p) in a model with essential heterogeneity. Making
observables alike makes the unobservables dissimilar. Holding p constant across treatment and
control groups understates TT(p) for low p and overstates it for high p.
      We now present additional examples with continuously distributed instruments. See Figure 8.
Instrument Z is assumed to be a random vector with a distribution function given by a mixture of
two normals:
                                      Z ∼ P1 N(κ1 , Σ1 ) + P2 N(κ2 , Σ2 ),

where P1 is the proportion in population 1, P2 is the proportion in population 2 and P1 +P2 = 1. This
                                                      ˜
produces a model with continuous instruments, where E(J(Z) | P (Z) > uD ) need not be monotonic
in uD where J˜ (Z) = J (Z) − E (J (Z)). Such a data generating process for the instrument could
arise from an ecological model in which two diﬀerent populations are mixed (e.g. rural and urban
populations).44
      At our web appendix, we derive explicit instrumental variable weights on ∆MTE when Z1 (the
first element of Z) is used as the instrument, i.e., J(Z) = Z1 for this case. For simplicity we assume
that there are no X regressors. The probability of selection is generated by μD (Z) = γZ. The joint
distribution of (Z1 , γZ) is normal within each group.
      In our example, the dependence between Z1 and γZ (= FV (γz) = P (Z)) is negative in one
 43
      See Heckman and Vytlacil (2005, 2007b).
 44
      Observe that E(Z) = P1 κ1 + P2 κ2 .



                                                      34
population and positive in another. Thus in one population, as Z1 increases P (Z) increases. In the
other population as Z1 increases P (Z) decreases. If this second population is suﬃciently big (P1
is small) or the negative dependence in the second population is suﬃciently big, the weights can
                          ˜
become negative because E(J(Z) | P (Z) > uD ) is not monotonic in uD .
   We present examples for a conventional normal outcome model generated by the parameters at
the base of Figure 8. The discrete choice equation is a conventional probit as in the other examples.
The outcome equations are linear normal equations. Thus ∆MTE (v), E(Y1 − Y0 | V = v), is linear
in v:
                                                          Cov (U1 − U0 , V )
                        E (Y1 − Y0 | V = v) = μ1 − μ0 +                      v.
                                                              Var (V )

At the base of the figure, we define β̄ = μ1 − μ0 and α = μ0 . The average treatment eﬀects are the
same for all distributions of the Z.
   In each of the following examples, we show results for models with vector Z that satisfies (IV-1)
and (IV-2) and with γ > 0 componentwise, where γ is the coeﬃcient on Z in the cost equation. We
vary the weights and means of the instruments. Ceteris paribus, an increase in each component of
Z increases Pr (D = 1 | Z = z). Table 3 (at the base of Figure 8) presents treatment on the treated
(E(Y1 − Y0 |D = 1)), treatment on the untreated (E(Y1 − Y0 |D = 0)), and the average treatment
eﬀect (E(Y1 − Y0 )) produced by our model.
   In standard IV analysis, the distribution of Z does not aﬀect the probability limit of the IV
estimator. It only aﬀects its sampling distribution. Figure 8A shows three weights corresponding
to the perturbations of the variance of the instruments in the second component population Σ2
and the means (κ1 , κ2 ) shown at the base of the figure in Table 3. The MTE used in all of our
examples is plotted in Figure 8B. The MTE has the familiar shape, reported in Heckman (2001)
and Heckman, Tobias, and Vytlacil (2003) that returns are highest for those with values of v that
make them more likely to get treatment (i.e., low values of v).
   The weights ω 1 and ω3 correspond to the case where E(Z1 − E (Z1 ) | P (Z) > uD ) is not
monotonic in uD . In theses cases the relationship between Z1 and P (Z) is not the same in the
two subpopulations. The IV estimates range all over the place even though the parameters of the

                                                 35
outcome and choice model are the same.45 Only the distributions of the instruments are diﬀerent.
       Diﬀerent distributions of Z critically aﬀect the probability limit of the IV estimator in the
model of essential heterogeneity. The model of outcomes and choices is the same across all of these
examples. The MTE and ATE parameters are the same. Only the distribution of the instrument
diﬀers. The instrumental variable estimand is sometimes positive and sometimes negative, and
oscillates wildly in magnitude depending on the distribution of the instruments. The estimated
“eﬀect” is often way oﬀ the mark for any desired treatment parameter. These examples show how
uniformity in Z does not translate into uniformity in J (Z) (Z1 in this example). This sensitivity
is a phenomenon that does not appear in the conventional homogeneous response model but is a
central feature of a model with essential heterogeneity.46


5.3       Empirical Example: Using IV to Estimate “The Eﬀect” of High

          School Graduation on Wages

The previous examples demonstrate logical possibilities. This subsection shows that these logical
possibilities arise in real data. We study the eﬀects of graduating from high school on wages
using data from the National Longitudinal Survey of Youth 1979 (NLSY79). This survey gathers
information at multiple points in time on the labor market activities for men and women born in
the years 1957—1964.
       We estimate LATE using log hourly wages at age 30 as the outcome measure. Following a large
body of research (see Mare, 1980), we use the number of siblings and mother’s graduation status as
instruments. Figure 9 plots the weights on LATE using the estimated P (Z). The weights are based
on (22). The LATE parameters are both positive and negative. The weights using siblings as an
instrument are both positive and negative. The weights using P (Z) as an instrument are positive,
as they must following the analysis of Yitzhaki. The two IV estimates diﬀer from each other because
  45
     Since TT and TUT depend on the distribution of P (Z) they are not invariant to changes is the distribution of
the Z.
  46
     We note paranthetically that if we assume that P1 = 0 (or P2 = 0) that the weights are always positive even if
we use only Z1 as an instrument and Z1 and Z2 are negatively correlated. This follows from the monotonicity of
E (R | S > c) in c for vector R. See Heckman and Honoré (1990).


                                                        36
the weights are diﬀerent. The overall IV estimate is a crude summary of the underlying component
LATEs that are often large and positive and large and negative. We next turn to an extension of
our model to multiple outcomes.



6     Extensions to More than Two Outcomes

Angrist and Imbens (1995) extend their analysis of LATE to an ordered choice model with outcomes
generated by a scalar instrument that can assume multiple values. From their analysis of the eﬀect
of schooling on earnings, it is unclear even under a strengthened “monotonicity” condition, whether
IV estimates the eﬀect of a change of schooling on earnings for a well defined margin of choice. To
summarize their analysis, let S̄ be the number of possible outcome states with associated outcomes
Ys and choice indicators Ds , s = 1, . . . , S̄. The s in their analysis correspond to diﬀerent levels of
schooling. For any two instrument values Z = zi and Z = zj with zi > zj , we can define associated
indicators {Ds (zi )}S̄s=1 and {Ds (zj )}S̄s=1 , where Ds (zi ) = 1 if a person assigned instrument value zi
chooses state s. As in the two outcome model, the instrument Z is assumed to be independent of
the potential outcomes {Ys }S̄s=1 as well as the associated indicator functions defined by fixing Z at
                                                              P
zi and zj . Observed schooling for instrument zj is S(zj ) = S̄s=1 sDs (zj ). Observed outcomes with
                               P
this instrument are Y (zj ) = S̄s=1 Ys Ds (zj ). Angrist and Imbens show that IV (with Z = zi and
zj ) applied to S in a two stage least squares regression of Y on S identifies a “causal parameter”

                                      X
                                      S̄
                             IV
                           ∆      =         {E (Ys − Ys−1 | S(zi ) ≥ s > S(zj ))}                      (26)
                                      s=2
                                                Pr (S(zi ) ≥ s > S(zj ))
                                            × PS̄                             .
                                               s=2 Pr (S(z i ) ≥ s > S(z j ))

This “causal parameter” is a weighted average of the gross return from going from s − 1 to s for
persons induced by the change in the instrument to move from any schooling level below s to any
schooling level s or above. Thus the conditioning set defining the s component of IV includes people
who have schooling below s − 1 at instrument value Z = zj and people who have schooling above


                                                      37
level s at instrument value Z = zi . In this sum, the average return experienced by some of the
people in the conditioning set for each component conditional expectation does not correspond to
the average outcome corresponding to the gain in the argument of the expectation. In the case
where S̄ = 2, agents face only two choices and the margin of choice is well defined. Agents in each
conditioning set are at diﬀerent margins of choice. The weights are positive but, as noted by Angrist
and Imbens, persons can be counted multiple times in forming the weights. When they generalize
their analysis to multiple-valued instruments, they use the Yitzhaki (1989) weights.
   Whereas the weights in equation (26) can be constructed empirically, the terms in braces cannot
be identified by any standard IV procedure. We present decompositions with components that are
recoverable, whose weights can be estimated from the data and that are economically interpretable.
   We generalize LATE to a multiple outcome case where we can identify agents at diﬀerent well
defined margins of choice. Specifically, we (1) analyze both ordered and unordered choice models;
(2) analyze outcomes associated with choices at various well defined margins; and (3) develop
models with multiple instruments that can aﬀect diﬀerent margins of choice diﬀerently. With our
methods, we can define and estimate a variety of economically interpretable parameters whereas
the Angrist-Imbens analysis produces a single “causal parameter” (26) that does not answer any
well defined policy problem. We first consider an explicit ordered choice model and decompose the
IV into policy useful, identifiable, components.


6.1    Analysis of an Ordered Choice Model

Ordered choice models arise in many settings. In schooling models, there are multiple grades. One
has to complete grade s − 1 to proceed to grade s. The ordered choice model has been widely
used to fit data on schooling transitions (Harmon and Walker, 1999; Cameron and Heckman, 1998).
Its nonparametric identifiability has been studied (Carneiro, Hansen, and Heckman, 2003; Cunha,
Heckman, and Navarro, 2007). It can also be used as a duration model for dynamic treatment
eﬀects with associated outcomes as in Cunha, Heckman, and Navarro (2007). It also represents the
“vertical” model of the choice of product quality (Prescott and Visscher, 1977; Shaked and Sutton,


                                                   38
1982; Bresnahan, 1987).
       Our analysis generalizes the preceding analysis for the binary model in a parallel way. Write
potential outcomes as
                                     Ys = μs (X, Us )        s = 1, . . . , S̄.

The S̄ could be diﬀerent schooling levels or product qualities. We define latent variables DS∗ =
μD (Z) − V where


                     Ds = 1[Cs−1 (Ws−1 ) < μD (Z) − V ≤ Cs (Ws )],                s = 1, . . . , S̄,


and the cutoﬀ values satisfy


                     Cs−1 (Ws−1 ) ≤ Cs (Ws ),    C0 (W0 ) = −∞ and CS̄ (WS̄ ) = ∞.


The cutoﬀs used to define the intervals are allowed to depend on observed (by the economist)
regressors Ws . In Appendix D we extend the analysis to allow the cutoﬀs to depend on unobserved
regressors as well, following structural analysis along these lines by Carneiro, Hansen, and Heckman
                                                                                   P
(2003) and Cunha, Heckman, and Navarro (2007). Observed outcomes are: Y = S̄s=1 Ys Ds . The Z
shift the index generally, the Ws aﬀect s-specific transitions. Thus, in a schooling example, Z could
include family background variables while Ws could include college tuition or opportunity wages
for unskilled labor.47 Collect the Ws into W = (W1 , . . . , WS̄ ), and the Us into U = (U1 , . . . , US̄ ).
Larger values of Cs (Ws ) make it more likely that Ds = 1. The inequality restrictions on the Cs (Ws )
functions play a critical role in defining the model and producing its statistical implications.
       Analogous to the assumptions made for the binary outcome model, we assume

(OC-1) (Us , V ) ⊥
                 ⊥ (Z, W )|X, s = 1, . . . , S̄. (Conditional Independence of the Instruments).

(OC-2) μD (Z) is a nondegenerate random variable conditional on X and W. (Rank Condition).
  47
     Many of the instruments studied by Harmon and Walker (1999) and Card (2001) are transition-specific. Card’s
model of schooling is not suﬃciently rich to make the distinction between the Z and the W . See Heckman and
Navarro (2006) and Cunha, Heckman, and Navarro (2007) for more general models of schooling that make these
distinctions explicit.

                                                        39
(OC-3) The distribution of V is continuous.48

(OC-4) E(|Ys |) < ∞, s = 1, . . . , S̄. (Finite Means).

(OC-5) 0 < Pr(Ds = 1|X) < 1 for s = 1, . . . , S̄ for all X. (In large samples, there are some
persons in each treatment state).

(OC-6) For s = 1, . . . , S̄ −1, the distribution of Cs (Ws ) conditional on X, Z and the other Cj (Wj ),
j = 1, . . . , S̄ j 6= s, is nondegenerate and continuous.49

Assumption (OC-1) to (OC-5) play roles analogous to their counterparts in the two outcome model
(A-1) to (A-5). (OC-6) is a new condition that is key to identification of the ∆MTE defined below
for each transition. It assumes that we can vary the choice sets of agents at diﬀerent margins of
schooling choice without aﬀecting other margins of choice. A necessary condition for (OC-6) to hold
is that at least one element of Ws is nondegenerate and continuous conditional on X, Z and Cj (Wj )
for j 6= s. Intuitively, one needs an instrument (or source of variability) for each transition. The
continuity of the regressor allows us to diﬀerentiate with respect to Cs (Ws ), like we diﬀerentiated
with respect to P (Z) to estimate the MTE in the analysis of the two outcome model.
       The analysis of Angrist and Imbens (1995) discussed in the introduction to this section makes
independence and monotonicity assumptions that generalize their earlier work. They do not consider
estimation of transition-specific parameters as we do, or even transition-specific LATE. We present
a diﬀerent decomposition of the IV estimator where each component can be recovered from the
data, and where the transition-specific MTEs answer well defined and economically interpretable
policy evaluation questions.50
  48
     Absolutely continuous with respect to Lebesgue measure.
  49
     Absolutely continuous with respect to Lebesgue measure.
  50
     Vytlacil (2006b) shows that their monotonicity and independence conditions imply (and are implied by) a more
general version of the ordered choice model with stochastic thresholds, which appears in Heckman, LaLonde, and
Smith (1999); Carneiro, Hansen, and Heckman (2003) and Cunha, Heckman, and Navarro (2007) and is analyzed in
Appendix D.




                                                       40
   The probability of Ds = 1 given X, Z and W is generated by an ordered choice model:


              Pr (Ds = 1 | W, Z, X) ≡ Ps (Z, W, X)

                                        = Pr (Cs−1 (Ws−1 ) < μD (Z) − V ≤ Cs (Ws ) | X) .


Analogous to the binary case, we can define UD = FV (V |X = x) so UD ∼ Unif[0, 1] under our
assumption that the distribution of V is absolutely continuous with respect to Lebesgue measure.
The probability integral transformation used extensively in the binary choice model is somewhat
less useful for analyzing ordered choices, so we work with both UD and V in this section of the paper.
Monotonic transformations of V induce monotonic transformations of μD (Z) − Cs (Ws ), but one is
not free to form arbitrary monotonic transformations of μD (Z) and Cs (Ws ) separately. Using the
probability integral transformation, the expression for choice s is Ds = 1[FV (μD (Z)−Cs−1 (Ws−1 )) >
UD ≥ FV (μD (Z) − Cs (Ws ))]. Keeping the conditioning on X implicit, we define Ps (Z, W ) =
FV (μD (Z) − Cs−1 (Ws−1 )) − FV (μD (Z) − Cs (Ws )). It is convenient to work with the probability
                                                        ³P                ¯      ´
                                                             S̄           ¯
that S > s, π s (Z, Ws ) = FV (μD (Z) − Cs (Ws )) = Pr       j=s+1 Dj = 1 ¯ Z, Ws , π S̄ (Z, WS̄ ) = 0,

π 0 (Z, W0 ) = 1 and Ps (Z, W ) = π s−1 (Z, Ws−1 ) − π s (Z, Ws ).
   The transition-specific ∆MTE for the transition from s to s + 1 is defined in terms of UD .


                ∆MTE
                 s,s+1 (x, uD ) = E(Ys+1 − Ys | X = x, UD = uD ),     s = 1, . . . , S̄ − 1.


Alternatively, one can condition on V . Analogous to the analysis of the earlier sections of this
paper, when we set uD = π s (Z, Ws ) we obtain the mean return to persons indiﬀerent between s and
s + 1 at mean level of utility π s (Z, Ws ).
   In this notation, keeping X implicit, the mean outcome Y , conditional on (Z, W ), is the sum
of the mean outcomes conditional on each state weighted by the probability of being in each state




                                                      41
summed over all states:

                                           X
                                           S̄
                      E(Y |Z, W ) =              E(Ys | Ds = 1, Z, W ) Pr(Ds = 1 | Z, W )                        (27)
                                           s=1
                                           S̄ Z
                                           X       π s−1 (Z,Ws−1 )
                                      =                              E(Ys | UD = uD )duD ,
                                           s=1    π s (Z,Ws )



where we use conditional independence assumption (OC-1) to obtain the final expression. Analogous
to the result for the binary outcome model, we obtain the index suﬃciency restriction E(Y |Z, W ) =
E(Y | π(Z, W )), where π(Z, W ) = [π 1 (Z, W1 ), . . ., π S̄−1 (Z, WS̄−1 )]. The choice probabilities encode
all of the influence of (Z, W ) on outcomes.
    We can identify π s (z, ws ) for (z, ws ) in the support of the distribution of (Z, Ws ) from the
                               P
relationship π s (z, ws ) = Pr( S̄j=s+1 Dj = 1 | Z = z, Ws = ws ). Thus E(Y | π(Z, W ) = π) is
identified for all π in the support of π(Z, W ). Assumptions (OC-1), (OC-3), and (OC-4) imply
                                                                  ∂
that E(Y | π(Z, W ) = π) is diﬀerentiable in π. So               ∂π
                                                                    E(Y    | π(Z, W ) = π) is well-defined.51 Thus
analogous to the result obtained in the binary case

                            ∂E(Y | π(Z, W ) = π)
                                                 = ∆MTE
                                                    s,s+1 (UD = π s )                                            (28)
                                    ∂π s
                                                           = E(Ys+1 − Ys | UD = π s ).


Equation (28) is the basis for identification of the transition-specific MTE from data on (Y, Z, X).
  51
     For almost all π that are limit points of the support of distribution of π(Z, W ). We use the Lebesgue theorem
for the derivative of an integral. Under assumption (OC-6), all points in the support of the distribution of π(Z, W )
                                                               ∂
will be limit points of that support, and we thus have that ∂π   E(Y | π(Z, W ) = π) is well defined and is identified
for (a.e.) π.




                                                            42
   From index suﬃciency, we can express (27) as

                                                X
                                                S̄
                   E (Y | π(Z, W ) = π) =              E(Ys | π s ≤ UD < π s−1 )(π s−1 − π s )              (29)
                                                s=1
                                                       ⎡                                 ⎤
                                                X
                                                S̄−1
                                                    ⎢ E(Ys+1 | πs+1 ≤ UD < π s ) ⎥
                                            =       ⎣                            ⎦ πs
                                                s=1   −E(Ys | π s ≤ UD < π s−1 )
                                                +E (Y1 | π1 ≤ UD < 1)
                                              X
                                              S̄−1
                                            =      {ms+1 (π s+1 , πs ) − ms (π s , π s−1 )}πs
                                                s=1
                                                +E (Y1 | π1 ≤ UD < 1)


where ms (π s , π s−1 ) = E[Ys | πs ≤ UD < π s−1 ]. In general this expression is a nonlinear func-
tion of (π s , π s−1 ). This model has a testable restriction of index suﬃciency in the general case:
E(Y |π(Z, W ) = π) is a nonlinear function that is additive in functions of (π s , π s−1 ) so there are no
interactions between π s and π s0 if |s − s0 | > 1, i.e.,

                              ∂ 2 E(Y | π(Z, W ) = π)
                                                      =0            if |s − s0 | > 1.
                                      ∂π s ∂π s0

   Observe that if UD ⊥
                      ⊥ Us for s = 1, . . . , S̄,

                                                    X
                                                    S̄
                     E (Y | π(Z, W ) = π) =                E(Ys )(π s−1 − πs )
                                                    s=1

                                                    X
                                                    S̄−1
                                                =          [E(Ys+1 ) − E(Ys )] π s + E(Y1 ).
                                                    s=1


                                                                     PS̄−1
Defining E(Ys+1 )−E(Ys ) = ∆ATE
                            s,s+1 , E(Y | π(Z, W ) = π) =              s=1   ∆ATE
                                                                              s,s+1 π s +E(Y1 ). Thus, under full

independence, we obtain linearity of the conditional mean of Y in the π s ’s. This result generalizes
the test for the presence of essential heterogeneity presented in section 3.1 to the ordered case. We
can ignore the complexity induced by the model of essential heterogeneity if E (Y | π (Z, W ) = π) is




                                                        43
linear in the π’s and can use conventional IV estimators to identify well-defined treatment eﬀects.52


6.1.1    What do Instruments Identify in the Ordered Choice Model?

We now characterize what scalar instrument J(Z, W ) identifies. When Y is log earnings, it is
common practice to regress Y on D where D is completed years of schooling and call the coeﬃcient
on D a rate of return.53 We seek an expression for the instrumental variables estimator of the eﬀect
of D on Y in the ordered choice model:

                                               Cov(J(Z, W ), Y )
                                                                 ,                                             (30)
                                               Cov(J(Z, W ), D)

              PS̄
where D =       s=1   sDs the number of years of schooling attainment. We keep the conditioning on X
implicit. We now present the weights for IV. Their full derivation is presented in Appendix E.
                    ³                                ´
                      ˜ W ) | μD (Z) − cs (Ws ) > v Pr (μD (Z) − Cs (W ) > v), where J(Z,
   Define Ks (v) = E J(Z,                                                               ˜ W) =

J(Z, W ) − E(J(Z, W )). Thus,

                                    Cov(J, Y )
                           ∆IV
                            j  =                                                                               (31)
                                    Cov(J, D)
                                    S̄−1 Z
                                    X
                                  =        E(Ys+1 − Ys | V = v)ω(s, v) fV (v)dv,
                                      s=1


where

                                                        Ks (v)
                             ω(s, v) = PS̄     R
                                          s=1 s [Ks−1 (v) − Ks (v)] fV (v)dv
                                                 Ks (v)
                                       = PS̄−1 R                  ,
                                          s=1    Ks (v) fV (v) dv

              PS̄−1 R
and clearly     s=1     ω(s, v) fV (v) dv = 1, ω(0, v) = 0, and ω(S̄, v) = 0. We can rewrite this result
  52
     Notice that if UD ⊥
                       ⊥ Us for some s, then we obtain an expression with nonlinearities in π s , πs−1 in expression
                       Á
(29).
  53
     Heckman, Lochner, and Todd (2006) present conditions under which this economic interpretation is valid.




                                                        44
in terms of the MTE, expressed in terms of uD


                                 ∆MTE
                                  s,s+1 (uD ) = E (Ys+1 − Ys | UD = uD )




so that
                                             S̄−1 Z
                               Cov(J, Y ) X
                                         =            ∆MTE
                                                       s,s+1 (uD )ω̃(s, u) duD ,
                               Cov(J, D)   s=1

where

                                                        K̃s (uD )
                          ω̃(s, uD ) = P
                                        S̄       R1h                       i                           (32)
                                            s=1 s 0 K̃s−1 (uD ) − K̃s (uD ) duD

                                             K̃s (uD )
                                     = PS̄−1 R 1
                                        s=1 0 K̃s (uD ) duD


and
                                ³                          ´
                                  ˜ W ) | π s (Z, Ws ) > uD Pr (π s (Z, Ws ) ≥ uD ) .
                   K̃s (uD ) = E J(Z,                                                                  (33)

   Compare equations (32) and (33) for the ordered choice model to equations (19) and (20) for the
binary choice model. The numerator of the weights for the ∆MTE for a particular transition in the
ordered choice model is exactly the numerator of the weights implied for the binary choice model,
substituting π s (Z, Ws ) = Pr(D > s | Z, Ws ) for P (Z) = Pr(D = 1 | Z). The numerator for the
weights for IV in the binary choice model is driven by the connection between the instrument and
P (Z). The numerator for the weights for IV in the ordered choice model for a particular transition is
driven by the connection between the instrument and π s (Z, Ws ). The denominator of the weights is
the covariance between the instrument and D for both the binary and ordered cases. However, in the
binary case the covariance between the instrument and D is completely determined by the covari-
ance between the instrument and P (Z), while in the ordered choice case the covariance depends on
the relationship between the instrument and the full vector [π 1 (Z, W1 ), . . . , π S̄−1 (Z, WS̄−1 )]. Com-
paring our decomposition of ∆IV to decomposition (26), ours corresponds to weighting up marginal
outcomes across well defined and adjacent boundary values experienced by agents having their in-


                                                      45
struments manipulated whereas the Angrist-Imbens decomposition corresponds to outcomes not
experienced by some of the persons whose instruments are being manipulated.
   From equation (33), the IV estimator using J(Z, W ) as an instrument satisfies the follow-
ing properties. (a) The numerator of the weights on ∆MTE
                                                     s,s+1 (uD ) is non-negative for all uD if

E(J(Z, Ws ) | π s (Z, Ws ) ≥ π s ) is weakly monotonic in π s . For example, if Cov(π s (Z, Ws ), D) > 0,
setting J(Z, W ) = π s (Z, Ws ) will lead to nonnegative weights on ∆MTE
                                                                     s,s+1 (uD ), though it may lead to

negative weights on other transitions. A second property (b) is that the support of the weights on
∆MTE                                              Min, π Max) where π Min and πMax are the minimum
 s,s+1 using π s (Z, Ws ) as the instrument is (π s      s            s        s

and maximum values in the support of π s (Z, Ws ), respectively, and the support of the weights on
∆MTE                                                Min, π Max). A third property (c) is that the
 s,s+1 using any other instrument is a subset of (π s      s

weights on ∆MTE                                                                              MTE
            s,s+1 implied by using J(Z, W ) as an instrument are the same as the weights on ∆s,s+1

implied by using E(J(Z, W ) | πs (Z, W )) as the instrument.
   Suppose that the distributions of Ws , s = 1, . . . , S̄, are degenerate so that the Cs are constants
satisfying C1 < · · · < CS̄−1 . This is the classical ordered choice model. In this case, π s (Z, Ws ) =
FV (μD (Z) − Cs ) for any s = 1, . . . , S̄. For this special case, using J as an instrument will lead to
nonnegative weights on all transitions if J(Z, Ws ) is a monotonic function of μD (Z). For example,
note that μD (Z)−Cs > v can be written as μD (Z) > Cs +FV−1 (uD ). Using μD (Z) as the instrument
                                                                               ∙
                      MTE
leads to weights on ∆s,s+1 (uD ) of the form specified above with K̃s (uD ) = E(μD (Z) | μD (Z) >
                           ¸
FV (uD ) + Cs ) − E(μD (Z)) Pr(μD (Z) > FV−1 (uD ) + Cs ). Clearly, these weights will be nonnegative
  −1


for all points of evaluation and will be strictly positive for any evaluation point uD such that
1 > Pr(μD (Z) > FV−1 (uD ) + Cs ) > 0. We now present some examples of the weights for IV.


6.1.2   Examples of Weights for IV

Figures 10 and 11 plot the transition-specific MTEs and the IV weights for the models and distri-
butions of the data at the base of each of the figures. We work with a normal V and Us , so we get
linear in V MTEs from standard normal regression theory. The IV estimates using Z and W1 as
instruments are reported transition by transition, along with the overall IV representation (31) into


                                                   46
its transition-specific components.54 The IV weights are defined by equations (32) and (33). The
bottom table presents the transition-specific treatment parameters.
      In Figure 10, the IV weights based on Z and W1 are very diﬀerent. So, correspondingly, are the
IV estimates produced from each instrument, which are far oﬀ the mark of the standard treatment
parameters shown at the bottom of the table. Observe that the IV weight for W1 in the second
transition is negative for an interval of values. This accounts for the dramatically lower IV estimate
based on W1 as the instrument. Figure 11 shows a diﬀerent configuration of (Z, W1 , W2 ). This
produces negative weights for Z for both transitions and a negative weight for W1 in the second
transition. For both instruments, IV is negative even though both MTEs are positive throughout
most of their range. IV provides a misleading summary of the underlying marginal treatment
eﬀects. In digesting Figures 10 and 11, it is important to recall that all are based on the same
structural model. All have the same MTE and average treatment eﬀects. But the IV estimates are
very diﬀerent solely as a consequence of the diﬀerences in the distributions of instruments across
examples.
      These simulations show a rich variety of shapes and signs for the weights. They illustrate a main
point of this paper–that standard IV methods are not guaranteed to weight marginal treatment
eﬀects positively or to produce estimates close to any of the standard treatment eﬀects. Estimators
based on LIV and its extension to the ordered model (28) identify ∆MTE for each transition and
answer policy relevant questions. We now turn to development of a more general unordered model.
 54
      In particular, when J(Z) is used as the instrument, we decompose ∆IV J(Z) as

                                           XZ
                                           S̄−1
                            ∆IV J(Z)   =             E (Ys+1 − Ys | V = v) ω J(Z) (s, v) fV (v) dv
                                           s=1

                                           XZ
                                           S̄−1
                                       =             ∆M TE
                                                      s,s+1 (v) ω
                                                                  J(Z)
                                                                       (s, v) fV (v) dv
                                           s=1
                                           S̄−1
                                           X        IVJ(Z)
                                       =          ∆s,s+1   .
                                           s=1




                                                               47
6.2       Extension to Multiple Treatments that are Unordered

In this section, we develop a framework for multiple treatments with a choice equation that is based
on a nonparametric version of the classical multinomial choice model.55 Within this framework,
treatment eﬀects can be defined as the diﬀerence in the counterfactual outcomes that would have
been observed if the agent faced diﬀerent choice sets, i.e., the eﬀect of the individual being forced
to choose from one choice set instead of another.
       We analyze the return to the agent of choosing between option j and the next best option.
The analysis of this case is very similar to the analysis presented in Section 3 because it converts
a multiple choice problem to a binary choice problem. Exclusion restrictions allow analysts to
identify generalizations of the LATE parameter and MTE parameters corresponding to the eﬀect
of one choice versus the “next-best” alternative. This identification analysis does not require large
support assumptions.
       Consider the following model with multiple outcome states. Let J denote the agent’s choice
set, where J contains a finite number of elements. The reward (psychic and monetary) of choosing
j ∈ J is
                                             Rj (Zj ) = ϑj (Zj ) − Vj ,                                         (34)

where Zj are the agent’s observed characteristics that aﬀect the utility from choosing choice j, and
Vj is the unobserved shock to the agent’s utility from choice j.56 Let Z denote the random vector
containing all unique elements of {Zj }j∈J , i.e., Z = union of {Zj }j∈J . We write Rj (Z) for Rj (Zj ),
leaving implicit that Rj (·) only depends on those elements of Z that are contained in Zj . Let DJ ,j
be an indicator variable for whether the agent would choose option j if confronted with choice set
  55
     Heckman and Navarro (2006) and Heckman and Vytlacil (2007a) present a semiparametric analysis of identifi-
cation for the multinomial choice model.
  56
     More consistent with the notation used in the previous section, we could define Rj (Zj ) = Dj∗ . A more precise,
but tedious notation would use Rj (Zj , Vj ) but we use the simpler notation.




                                                         48
J :57                                          ⎧
                                               ⎪
                                               ⎪
                                               ⎨1     if Rj ≥ Rk         ∀k ∈ J
                                     DJ ,j =
                                               ⎪
                                               ⎪
                                               ⎩0     otherwise.

Let IJ denote the choice that would be made by the agent if confronted with choice set J : IJ =
j ⇐⇒ DJ ,j = 1. Let YJ be the outcome variable that would be observed if the agent faced choice
set J . It is
                                                      X
                                               YJ =         DJ ,j Yj ,                                         (35)
                                                      j∈J

where Yj is the potential outcome, observed only if option j is chosen. This expression generalizes
(2). We assume that Yj is determined by Yj = μj (Xj , Uj ), where Xj is a vector of the agent’s
observed characteristics and Uj is an unobserved random vector. Let X denote the random vector
containing all unique elements of {Xj }j∈J , i.e., X is the union of {Xj }j∈J . We assume that
(Z, X, IJ , YJ ) is observed.58 Define RJ as the maximum obtainable value given choice set J :

                                                               X
                                        RJ = max{Rj } =         DJ ,j Rj .
                                                j∈J
                                                               j∈J


We obtain the traditional representation of the decision process that if choice j is optimal, choice j
is better than the “next best” option:


                                            IJ = j ⇐⇒ Rj ≥ RJ \j ,


where J \j means J removing the j th element from the set. More generally, a choice with K optimal
is equivalent to the highest value obtainable from choices in K being higher than the highest value
that can be obtained from choices outside that set,


                                           IJ ∈ K ⇐⇒ RK ≥ RJ \K .
  57
    We will impose conditions such that ties, Rj = Rk for j 6= k, occur with probability zero.
  58
    One possible extension is to the case where one does not observe which choice was made, but only whether one
particular choice was made, i.e., one observes DJ ,0 but not IJ . The analysis of Thompson (1989) suggests that this
extension should be possible.


                                                        49
As we will show, this well-known representation used by Lee (1983), Dahl (2002) and others, is key
for understanding how nonparametric instrumental variables estimates the eﬀect of a given choice
versus the “next best” alternative.
       Analogous to our definition of RJ , we define RJ (z) to be the maximum attainable value given
choice set J when instruments are fixed at Z = z,


                                              RJ (z) = max{Rj (z)}.
                                                         j∈J



Thus, for example, a choice from K is optimal when instruments are fixed at Z = z if RK (z) ≥
RJ \K (z).
       We make the following assumptions, which generalize assumptions (A-1) to (A-5) for the multiple
treatment case and are presented in a parallel fashion ((B-2) is stated below):

(B-1) {(Vj , Uj )}j∈J is independent of Z conditional on X.

(B-3) The distribution of ({Vj }j∈J ) is absolutely continuous with respect to Lebesgue measure on
 Q
   R.
j∈J

(B-4) E|Yj | < ∞ for all j ∈ J .

(B-5) P r(IJ = j|X) > 0 for all j ∈ J .

       Assumptions (B-1) and (B-3) imply that Rj 6= Rk w.p.1 for j 6= k, so that argmax{Rj } is
unique w.p.1. Assumption (B-4) is required for the mean treatment parameters to be well defined.59
Assumption (B-5) requires that at least some individuals participate in each program for all X.
       Definitions of the treatment parameters only require assumptions (B-1) and (B-3) to (B-5).
However, we use exclusion restrictions to secure identification. Let Z [j] denote the jth component
of Z. Let Z [−j] denote all elements of Z except for the jth component. We will work with two
alternative assumptions for the exclusion restriction.60 Consider
  59
    It allows us to integrate to the limit.
  60
    We work here with exclusion restrictions for ease of exposition. By adapting the analysis of Cameron and
Heckman (1998) and Heckman and Navarro (2006), one can modify our analysis to encompass the case of no exclusion
restrictions if Z contains a suﬃcient number of continuous variables and there is suﬃcient variation in the ϑk function
across k.

                                                          50
(B-2a) For each j ∈ J , their exists at least one element of Z, say Z [j] , such that Z [j] is not an
              element of Zk , k 6= j, and such that the distribution of ϑj (Zj ) conditional on (X, Z [−j] ) is
              nondegenerate,


         or


(B-2b) For each j ∈ J , their exists at least one element of Z, say Z [j] , such that Z [j] is not an element
              of Zk , k 6= j, and such that the distribution of ϑj (Zj ) conditional on (X, Z [−j] ) is absolutely
              continuous with respect to Lebesgue measure.


         Assumption (B-2a) requires that the analyst be able to independently vary the index for the
  given value function. It imposes an exclusion restriction, that for any j ∈ J , Z contains an
  element such that (i) it is contained in Zj ; (ii) it is not contained in any Zk for k 6= j, and (iii)
  ϑj (·) is a nontrivial function of that element conditional on all other regressors. Assumption (B-
  2b) strengthens (B-2a) by adding a smoothness assumption. A necessary condition for (B-2b) is
  that the excluded variable have a density with respect to Lebesgue measure conditional on all
  other regressors and for ϑj (·) to be a continuous and nontrivial function of the excluded variable.61
  Assumption (B-2a) is used to identify a generalization of the LATE parameter. Assumption (B-2b)
  will be used to identify a generalization of the MTE parameter. Below, we will strengthen (B-2b)
  to a large support assumption to identify ATE though the large support assumption will not be
  required for most of our analysis. Assumptions (B-2a) and (B-2b) mirror (A-2) and are analogous
  to (OC-2) and (OC-6) in an ordered choice setting.


  6.2.1         Definition of Treatment

  Treatment eﬀects are defined as the diﬀerence in the counterfactual outcomes that would have
  been observed if the agent faced diﬀerent choice sets. For any two choice sets, K, L ⊂ J , define
    61
      (B-2b) can be easily relaxed to the weaker assumption that the support of ϑj (Zj ) conditional on (X, Z [− ] )
  contains an open interval, or further weakened to the assumption that the conditional support contains at least
  one limit point. In these cases, the analysis of this section goes through without change for points within the open
  interval or more generally for any limit point.



                                                           51
∆K,L = YK − YL , the eﬀect of the individual being forced to choose from choice set K versus choice
set L. The conventional treatment eﬀect is defined as the diﬀerence in potential outcomes between
two specified states,
                                               ∆k, = Yk − Y ,

which is nested within this framework by taking K = {k}, L = { }. It is the eﬀect for the individual
of having no choice except to choose state k versus having no choice except to choose state .
   ∆K,L will be zero for agents who make the same choice when confronted with choice set K and
choice set L. Thus, IK = IL implies ∆K,L = 0, and we have


                                   ∆K,L = 1(IL 6= IK )∆K\L,L                                    (36)
                                                       ⎛               ⎞
                                                         X
                                        = 1(IL 6= IK ) ⎝     DK,j ∆j,L ⎠ .
                                                          j∈K\L



   Two cases will be of particular importance for our analysis. First, consider choice set K = {k}
versus choice set L = J \ {k}. In this case, ∆k,J \k is the diﬀerence between the agent’s potential
outcome in state k versus the outcome that would have been observed if he or she had not been
allowed to choose state k. If IJ = k, then ∆k,J \k is the diﬀerence between the outcome in the
agent’s preferred state and the outcome in the agent’s “next-best”state. Second, consider the set
K = J versus choice set L = J \ {k}. In this case, ∆J ,J \k is the diﬀerence between the agent’s
observed outcome and what his or her outcome would have been if state k had not been available.
Note that ∆J ,J \k = DJ ,k ∆k,J \k . Thus, there is a trivial connection between the two parameters,
∆J ,J \k and ∆k,J \k . This paper focuses on ∆k,J \k , the eﬀect of being forced to choose option k
versus being denied option k. However, one can exploit equation (36) to use the results for ∆k,J \k
to obtain results for ∆J ,J \k .




                                                     52
6.2.2   Treatment Parameters

The conventional definition of the average treatment eﬀect (ATE) is ∆ATE
                                                                     k, (x, z) = E(∆k, |X =

x, Z = z), which immediately generalizes to the class of parameters just discussed: ∆ATE
                                                                                     K,L (x, z) =

E(∆K,L |X = x, Z = z). The conventional definition of the treatment on the treated (TT) parameter
is ∆TT                                                              TT
    k, (x, z) = E(∆k, |X = x, Z = z, IJ = k), which generalizes to ∆K,L (x, z) = E(∆K,L |X =

x, Z = z, IJ ∈ K).
   We generalize the MTE parameter to be the average eﬀect conditional on being indiﬀerent
between the best option among choice set K versus the best option among choice set L at some
fixed value of the instruments, Z = z:


                       ∆MTE
                        K,L (x, z) = E (∆K,L | X = x, Z = z, RK (z) = RL (z)) .                (37)


We generalize the LATE parameter to be the average eﬀect for someone for whom the optimal
choice in choice set K is preferred to the optimal choice in choice set L at Z = z̃, but who prefers
the optimal choice in choice set L to the optimal choice in choice set K at Z = z:
                                     ⎛         ¯                                         ⎞
                                               ¯
                                               ¯ X = x, Z ∈ {z, z̃} , RK (z̃) ≥ RL (z̃),
                                    ⎜          ¯                                         ⎟
                ∆LATE
                 K,L (x, z, z̃) = E ⎝∆K,L      ¯                                         ⎠.    (38)
                                               ¯          RL (z) ≥ RK (z)
                                               ¯


An important special case of this parameter arises when z = z̃ except for elements that enter the
index functions only for choices in K and not for any choice in L. In that special case, expression
(38) simplifies to

                                           ⎛         ¯                        ⎞
                                                     ¯
                                                     ¯   X = x, Z ∈ {z, z̃} ,
                                          ⎜          ¯                        ⎟
                      ∆LATE
                       K,L (x, z, z̃) = E ⎝∆K,L      ¯                        ⎠
                                                     ¯ R (z̃) ≥ R (z) ≥ R (z)
                                                     ¯ K         L         K



since RL (z) = RL (z̃) in this special case.
   We have defined each of these parameters as conditional not only on X but also on the
“instruments”Z. In general, the parameters will depend on the Z evaluation point. For example,

                                                     53
                                                                                           P
∆ATE
  K,L (x, z) will in general depend on the z evaluation point. To see this, note that YK =     DK,k Yk ,
                                                                                           k∈K
           P
and YL =       DL, Y . By independence assumption (B-1), we have that Z ⊥⊥ {Yj }j∈J | X, but DK,k
                  ∈L
and DL, will be dependent on Z conditional on X and thus YK − YL will in general be dependent
on Z conditional on X.62 In other words, even though Z is conditionally independent of each indi-
vidual potential outcome, it is correlated with which choice is optimal within the sets K and L and
thus is related to YK − YL .


6.2.3          Identification: Eﬀect of Option j Versus Next Best Alternative

We now establish identification of treatment parameters corresponding to averages of ∆j,J \j , the
eﬀect of choosing option j versus the preferred option in J if j were not available.63 Recall that
Z [j] is the vector of elements of Zj that do not enter any other choice index, and that Z [−j] is a
vector of all elements of Z not in Z [j] . The Z [j] thus act as shifters attracting people into or out of
j, but not aﬀecting the valuations in the arguments of the other choice functions. We can develop
a parallel analysis to the binary case developed earlier in this paper if we condition on Z [−j] . We
obtain monotonicity or uniformity in this model if the movements among states induced by Z [j] are
the same for all persons conditional on Z [−j] = z [−j] and X = x. For example, ceteris paribus if
Z [j] = z [j] increases, Rj (Zj ) increases but the Rk (Zk ) are not aﬀected, so the flow is toward state
j.
          Let DJ ,j be an indicator variable denoting whether option j is selected.

                                                µ                        ¶
                                     DJ ,j   = 1 Rj (Zj ) ≥ max {R (Z )}                                           (39)
                                                               6=j
                                                µ                            ¶
                                             = 1 ϑj (Zj ) ≥ Vj + max {R (Z )}
                                                                   6=j
                                                ³               ´
                                             = 1 ϑj (Zj ) ≥ Ṽj ,

                                                           ¡              ¢
where Ṽj = Vj + max 6=j {R (Z )}. Thus we obtain DJ ,j = 1 Pj (Zj ) ≥ UDj , where UDj =
FṼj (Vj + max 6=j {R (Z )} | Z [−j] = z [−j] ), where FṼj is the cdf of Ṽj given Z [−j] = z [−j] . In a
     62
          An exception is if K = {k}, L = { }, i.e., both sets are singletons.
     63
          Heckman and Vytlacil (2007b) consider the identification of other parameters in the general unordered case.

                                                             54
format parallel to the binary model, we write


                                         Y = DJ ,j Yj + (1 − DJ ,j ) YJ \j ,                                       (40)


where YJ \j is the outcome that would be observed if option j were not available. This case is just
a version of the binary case developed in previous sections of the paper. We can define MTE as

                            ¡                                                    ¢
                           E Yj − YJ \j | X = x, Z = z, ϑj (zj ) − Vj = RJ \j (z) .


Recall that we have to condition on Z = z because the choice sets are defined over the max of
elements in J \ j (see equation (39)).
    We now show that our identification strategies presented in the preceding part of this paper
extend naturally to the identification of treatment parameters for ∆j,J \j . In particular, it is possible
to recover LATE and MTE parameters for ∆j,J \j by use of discrete change IV methods and local
instrumental variable methods, respectively. Averages of the eﬀect of option j versus the next best
alternative are the easiest eﬀects to study using instrumental variable methods and are natural
generalizations of our two outcome analysis.64
    Consider identification of treatment parameters corresponding to averages of ∆j,J \j using either
a discrete change, Wald form for the instrumental variables estimand or using the local instrumental
variables (LIV) estimand.65 The discrete change, instrumental variables estimand will allow us to
recover a version of the local average treatment eﬀect (LATE) parameter.66 Let Z [−j] denote the
                                                                                £          ¤
excluded variable for option j with properties assumed in (B-2a). We let z = z [−j] , z [j] and
  64
     Heckman and Navarro (2006) consider identification of other parameters but they use identification at infinity
arguments not required for standard IV. See the comprehensive discussion in Heckman and Vytlacil (2007b).
  65
     The estimand is the population version of the estimator.
  66
     We use the Z directly in the following manipulations instead of manipulating the {ϑj (Zj )} indices. One can
modify the following analysis to directly use {ϑj (Zj )}, with the disadvantage of requiring identification of {ϑj (Zj )}
(e.g. by an identification at infinity argument) but with the advantage of being able to follow the analysis of Cameron
and Heckman (1998), Chen, Heckman, and Vytlacil (1998, 1999) and Heckman and Navarro (2006) in not requiring
an exclusion restriction if Z contains a suﬃcient number of continuous variables and there is suﬃcient variation in
the ϑk function across k. See Heckman and Vytlacil (2007b) for a more general analysis.




                                                           55
    £                ¤
z̃ = z̃ [−j] , z̃ [j] be two values of Z where we only manipulate Z [j] . Define


                           ∆Wald
                            j    (x, z [−j] , z [j] , z̃ [j] )
                                   E(Y |X = x, Z = z̃) − E(Y |X = x, Z = z)
                       =                                                            ,
                           Pr(DJ ,j = 1|X = x, Z = z̃) − Pr(DJ ,j = 1|X = x, Z = z)

where for notational convenience we assume that Z [j] is the last component of Z. Without loss
of generality, we assume that ϑj (z̃) > ϑj (z). The local instrumental variables estimator (LIV)
estimand introduced in Heckman (1997), and developed further in Heckman and Vytlacil (1999,
2001b) allows us to recover a version of the Marginal Treatment Eﬀect (MTE) parameter. Impose
(B-2b), and let Z [j] denote the excluded variable for option j with properties assumed in (B-2b).
Our results are invariant to which particular variable satisfying (B-2b) is used if there are more
than one variable with the property assumed in (B-2b). Define

                                                               ∂
                                                              ∂z [j]
                                                                     E(Y   | X = x, Z = z)
                               ∆LIV
                                j (x, z)        ≡     ∂
                                                                                             .                    (41)
                                                     ∂z [j]
                                                              Pr(DJ ,j = 1 | X = x, Z = z)

∆LIV                                 Wald
 j (x, z) is thus the limit form of ∆j    (x, z [−j] , z [j] , z̃ [j] ) as z̃ [j] approaches z [j] . Given our previous
assumptions, one can easily show that this limit exists w.p.1. We prove the following identification
theorem.

Theorem 1.          1. Assume (B-1), (B-3) to (B-5) and (B-2a). Then ∆Wald
                                                                      j    (x, z [−j] , z [j] , z̃ [j] ) =
      ∆LATE
       j,J \j (x, z, z̃) where z̃ = (z
                                       [−j] [j]
                                           , z̃ ).

   2. Assume (B-1), (B-3) to (B-5) and (B-2b). Then ∆LIV         MTE
                                                     j (x, z) = ∆j,J \j (x, z).


Proof. See Appendix F.

    The intuition underlying the proof is simple. Under (B-1), (B-3) to (B-5) and (B-2a) we can
convert the problem of comparing the outcome under j with the outcome under the next best
option. This is an IV version of the selection modelling analysis of Dahl (2002). ∆LATE
                                                                                   j,J \j (x, z, z̃) is

the average eﬀect of switching to state j from state IJ \j for individuals who would choose IJ \j at


                                                                    56
Z = z but would choose j at Z = z̃. ∆MTE
                                     j,J \j (x, z) is the average eﬀect of switching to state j from

state IJ \j (the best option besides state j) for individuals who are indiﬀerent between state j and
IJ \j at the given values of the selection indices (at Z = z, i.e., at {ϑk (Zk ) = ϑk (zk )}k∈J ).
   The mean outcome in state j versus state IJ \j (the next best option) is a weighted average
over k ∈ J \ j of the eﬀect of state j versus state k, conditional on k being the next best option,
weighted by the probability that k is the next best option. For example, for the LATE parameter,
                                    ⎛        ¯                               ⎞
                                             ¯
                                             ¯ X = x, Z ∈ {z, z̃} ,
                                   ⎜         ¯                               ⎟
            ∆LATE
             j,J \j (x, z, z̃) = E ⎝∆j,J \j ¯                                ⎠
                                             ¯ R (z̃) ≥ R (z) ≥ R (z)
                                             ¯ j           J \j         j
                                        ⎡    ⎛           ¯                           ⎞      ⎤
                                                         ¯
                                                         ¯ Z ∈ {z, z̃} ,
                                        ⎢    ⎜           ¯                           ⎟      ⎥
                                        ⎢ Pr ⎝IJ \j = k ¯                            ⎠      ⎥
                                        ⎢                ¯ R (z̃) ≥ R (z) ≥ R (z)           ⎥
                                        ⎢                ¯ j            J \j     j          ⎥
                                  X ⎢   ⎢
                                              ⎛       ¯
                                                      ¯
                                                                                   ⎞        ⎥
                                                                                            ⎥
                               =        ⎢             ¯ X = x, Z ∈ {z, z̃} ,                ⎥,
                                        ⎢     ⎜       ¯                            ⎟        ⎥
                                 k∈J \j ⎢     ⎜       ¯                            ⎟        ⎥
                                        ⎢ ×E ⎜∆j,k ¯ R (z̃) ≥ R (z) ≥ R (z), ⎟              ⎥
                                        ⎢     ⎜       ¯ j          J \j        j   ⎟        ⎥
                                        ⎣     ⎝       ¯                            ⎠        ⎦
                                                      ¯
                                                      ¯ IJ \j = k


where we use the fact that RJ \j (z) = RJ \j (z̃) since z = z̃ except for one component that only
enters the index for the jth option. How heavily each option is weighted in this average depends on

                          ¡                                                         ¢
                        Pr IJ \j = k | Z ∈ {z, z̃} , Rj (z̃j ) ≥ Rk (zk ) ≥ Rj (zj ) ,


which in turn depends on {ϑk (zk )}k∈J \j . The higher ϑk (zk ), holding the other indices constant, the
larger the weight given to state k as the base state.
   The LIV and Wald estimands depend on the z evaluation point. Alternatively, one can define
averaged versions of the LIV and Wald estimands that will recover averaged versions of the MTE




                                                     57
and LATE parameters,67

                                  Z
                                      ∆Wald
                                       j    (x, z [−j] , z [j] , z̃ [j] )dFZ [−j] (z [−j] )
                                  Z
                             =  ∆LATE
                                  j,J \j (x, z, z̃)dFZ [−j] (z
                                                               [−j]
                                                                    )
                                ⎛          ¯                                                  ⎞
                                           ¯
                                           ¯                  X = x,
                                ⎜          ¯                                                  ⎟
                                ⎜          ¯                                                  ⎟
                             = E⎜          ¯
                                ⎜∆j,J \j ¯ Rj (Z , z̃ )
                                                    [−j] [j]                                  ⎟,
                                                                                              ⎟
                                ⎝          ¯                                                  ⎠
                                           ¯
                                           ¯ ≥ RJ \j (Z [−j] ) ≥ Rj (Z [−j] , z [j] )


and

                     Z                                  Z
                         ∆LIV
                          j (x, z)dFZ (z)          =   ∆MTE
                                                        j,J \j (x, z)dFZ (z)
                                                      ¡                                  ¢
                                                   = E ∆j,J \j |X = x, Rj (Z) = RJ \j (Z) .


       Thus far, we have only considered identification of LATE and MTE, and not of the more standard
treatment parameters ATE and TT. However, following Heckman and Vytlacil (1999), LATE can
approximate ATE or TT arbitrarily well given the appropriate support conditions. Theorem 1 shows
that we can use Wald estimands to identify LATE for ∆j,J \j , and we can thus adapt Heckman and
Vytlacil (1999) to identify ATE or TT for ∆j,J \j . With suitable modification of the weights, their
analysis, summarized in Section 3, goes through as before. Suppose that Z [j] satisfies the properties
assumed in (B-2a), and suppose that: (i) the support of the distribution of Z [j] conditional on
all other elements of Z is the full real line; (ii) ϑj (zj ) → ∞ as z [j] → ∞, and ϑj (zj ) → −∞ as
z [j] → −∞. Then ∆ATE                LATE
                  j,J \j (x, z) and ∆j    (x, z [−j] , z [j] , z̃ [j] ) are arbitrarily close when evaluated at a
suﬃciently large value of z̃ [j] and a suﬃciently small value of z [j] . Following Heckman and Vytlacil
(1999), ∆TT                 LATE
         j,J \j (x, z) and ∆j    (x, z [−j] , z [j] , z̃ [j] ) are arbitrarily close for suﬃciently small z [j] . Our
discussion has focused on the Wald estimands. Alternatively we could also follow Heckman and
Vytlacil (1999, 2001b, 2005) in expressing ATE and TT as integrated versions of MTE. By theorem
  67
   We assume that the support of Z [−j] conditional on (Z̃ [j] , X) is the same as the support of Z [−j] conditional on
  [j]
(Z , X).



                                                                58
1, we can use LIV to identify MTE and can thus express ATE and TT as integrated versions of the
LIV estimand.
                                 ¡              ¢                ¡              ¢
       For a general instrument J Z [j] , Z [−j] constructed from Z [j] , Z [−j] , which we denote as J [j] ,
we can obtain a parallel construction to the characterization of standard IV given in equation 18:

                                                     Z   1       ¡         ¢ [j] ¡ ¢
                                        ∆IV
                                         J [j]   =           ∆MTE x, z, uDj ωJIV uDj duDj ,                    (42)
                                                     0



where                            £         ¡ ¢                  ¤ ¡                                ¢
                      J [j]     E J [j] − E J [j] | Pj (Z) ≥ uDj Pr Pj (Z) ≥ uDj | Z [−j] = z [−j]
                  ω IV        =                                                                      ,         (43)
                                                          Cov(Z [j] , DJ ,j )

where uDj is defined at the beginning of this section and where we keep the conditioning on X = x
implicit.
       Note that from Theorem 1, we obtain that

            ∂
           ∂z [j]
                  E   [Y | X = x, Z = z]           ∂E[Y | X = x, Z = z]
                              ∂Pj (z)
                                                 =
                                                          ∂Pj (z)
                               ∂z [j]
                                                    £                                                    ¤
                                                 = E Yj − YJ \j | X = x, Z = z, ϑj (Zj ) − Vj = RJ \j (Z)


so we obtain that LIV identifies MTE and linear IV is a weighted average of LIV with the weights
summing to one. These results mirror the results established in the binary case.
                                                    P
   In the literature on the eﬀects of schooling (S = j∈J jDJ ,j ) on earnings (YJ ), it is conventional
to instrument S. Our website presents an analysis of this case. For the general unordered case,

                                                                        Cov(J [j] , YJ )
                                                             ∆IV
                                                              J [j] =
                                                                        Cov(J [j] , S)

can be decomposed into economically interpretable components where the weights can be identified
but the objects being weighted cannot be identified using local instrumental variables or LATE
without making large support assumptions. However, the components can be identified using a
structural model.68
  68
       See Heckman and Vytlacil (2007a) and Heckman and Navarro (2006) for analyses of semiparametric identification


                                                                         59
    The trick we have used in this section comparing outcomes in j to the next best option converts
a general unordered multiple outcome model into a two outcome setup. This eﬀectively partitions
YJ into two components, as in (40). Thus we write


                                      YJ = DJ ,j Yj + (1 − DJ ,j ) YJ \j ,


where
                                            X       DJ ,
                                  YJ \j =                   Y × 1 (DJ ,j 6= 1) .
                                            6=j
                                                  1 − DJ ,j
                                            ∈J

In the more general unordered case with three or more choices, to analyze IV estimates of the eﬀect
                                    P
of S on YJ , we must work with YJ = k∈J DJ ,k Yk and make multiple comparisons across potential
outcomes. This requires us to move outside of the LATE/LIV framework, which is inherently based
on binary comparisons.69 We consider models that do not impose additive separability in choice
equation (13). This includes a general random coeﬃcient model.



7     Relaxing Additive Separability in the Choice Equation

      and allowing for Random Coeﬃcient Choice Models

The analysis of this paper and the entire recent literature on instrumental variables estimators for
models with essential heterogeneity relies critically on the assumption that the treatment choice
equation can be represented in additively separable form (13). The implied uniformity condition
imparts an asymmetry to the entire instrumental variable enterprise. Uniformity also underlies
conventional selection models.
of structural models that can identify all treatment eﬀects and the components of the IV decompositions. See
Heckman and Vytlacil (2007b) and Heckman and Urzua (2006) for further analyses of this case.
  69
     If we partition YJ into two components based on general sets K, L, each with two or more elements, the choice
equation in general is no longer characterized by an additive separability in the error assumption, discussed in
Heckman and Vytlacil (2005) and in the next section, that is required to justify application of LATE and LIV to
identify the MTE. The ordered case previously analyzed has a local property which compares adjacent choices and
eﬀectively makes binary comparisons.




                                                         60
       Responses are permitted to be heterogeneous in a general way, but choices of treatment are
not. In the absence of additive separability, or uniformity, the instrumental variable identification
strategy breaks down. Parameters can be defined as weighted averages of an MTE but MTE and the
derived parameters cannot be identified using any instrumental variables strategy (see Heckman and
Vytlacil, 2001b, 2005, 2007b). This point applies to models with two or more potential outcomes.
For simplicity of exposition, we only analyze the two outcome case.
       One natural benchmark nonseparable model is a random coeﬃcient model of choice D =
                                                      ⊥ (Z, U0 , U1 ). If γ is a random coeﬃ-
1 [γZ ≥ 0] where γ is a random coeﬃcient vector and γ ⊥
cient with a nondegenerate distribution and with components that take both positive and negative
values, uniformity (“monotonicity”) can be violated. Figure 2C illustrates this violation. Unifor-
mity can also be violated if we change one coordinate of Z but fail to control for movements in the
other coordinates. See Figure 2B.
       To consider a more general case, relax the separability assumption of equation (13) to consider
latent choice index
                                       D∗ = μD (Z, V ) ,         D = 1 [D∗ ≥ 0] ,                                     (44)

where μD (Z, V ) is not necessarily additively separable in Z and V , and V is not necessarily a
scalar. In the random coeﬃcient example, V = γ. We maintain assumptions (A-1)-(A-5), with
(A-3) suitably modified for the random coeﬃcient case.70
       In the additively separable case, the MTE has three equivalent interpretations: (i) UD (= FV (V ))
is the only unobservable in the first stage decision rule, and MTE is the average eﬀect of treatment
given the unobserved characteristics in the decision rule (UD = uD ); (ii) MTE is the average eﬀect
  70
    In special cases, (44) can be expressed in additively separable form. For example if D∗ is weakly separable in
Z and V , D∗ = μD (θ (Z) , V ) where θ (Z) is a scalar function, and μD (θ(Z), V ) is strictly increasing in its first
argument, and V is a scalar, for any V , then we can write (44) in the same form as (13):
                                                       ³           ´
                                                D = 1 θ (Z) ≥ Ve

where Ṽ = μ−1               e ⊥ Z | X, and the inverse function is expressed with respect to the first argument.
              D (0, V ) and V ⊥
See Vytlacil (2006a) who considers the vector V case. Vytlacil (2002) shows that any model that does not satisfy
uniformity (or “monotonicity”) will not have a representation in this form. In the random coeﬃcient case where
Z = (1, Z1 ) where Z1 is a scalar, and γ = (γ 0 , γ 1 ) if γ 1 > 0 for all realizations, we can write the choice rule in the
form of (13): γ 1 Z1 > −γ 0 ⇒ Z > − γγ 0 and V = − γγ 0 . However, this trick does not work in the general case.
                                       1               1




                                                            61
of treatment given that the individual would be indiﬀerent between treatment or not if P (Z) = uD ,
where P (Z) is a mean utility function; (iii) the MTE is an average eﬀect conditional on the additive
error term from the first stage choice model. Under all interpretations of the MTE, and under the
assumptions (A-1) to (A-5), MTE can be identified by LIV. The MTE does not depend on Z and
hence it is invariant to policies that shift Z. The MTE integrates up to generate all treatment
eﬀects, policy eﬀects and IV estimands.
       The three definitions are not the same in the general nonseparable case (44). Heckman and
Vytlacil (2001b, 2005, 2007b) extend MTE to the nonseparable case. Local instrumental variables
(LIV) is a weighted average of the MTE with possibly negative weights and does not identify MTE.
Thus, if uniformity does not hold, the definition of the MTE allows one to integrate it up to obtain
all of the treatment eﬀects. However, the instrumental variables estimator does not identify LATE
or MTE.


7.1        Failure of Index Suﬃciency in General Nonseparable Models

For any version of the nonseparable model, except those that can be transformed to separa-
bility, index suﬃciency fails.         To see this most directly, assume that μD (Z, V ) is a continu-
ous random variable.71          Define Ω (z) = {v : μD (z, v) ≥ 0}.    In the additively separable case,
P (z) ≡ Pr (D = 1 | Z = z) = Pr (V ∈ Ω (z)), P (z) = P (z 0 ) ⇔ Ω (z) = Ω (z 0 ). This produces
index suﬃciency so the propensity score orders the unobservables generating choices. In the more
general case (44), it is possible to have (z, z 0 ) values such that P (z) = P (z 0 ) and Ω (z) 6= Ω (z 0 ) so
index suﬃciency does not hold. The Z’s enter the model more generally, and the propensity score
no longer plays the central role it plays in separable models.


7.2        The Support of the Propensity Score

The nonseparable model can also restrict the support of P (Z). For example, consider a normal
random coeﬃcient choice model with a scalar regressor (Z = (1, Z1 )). Assume γ 0 ∼ N (0, σ 20 ),
  71
       Absolutely continuous with respect to Lebesgue measure.


                                                         62
γ 1 ∼ N (γ̄ 1 , σ 21 ), and γ 0 ⊥
                                ⊥ γ 1 . Then

                                                        Ã                      !
                                                                 γ̄ 1 z1
                                          P (z1 ) = Φ       p                      ,
                                                             σ 20 + σ 21 z12

where Φ is the cumulative distribution of a standard normal. If the support of Z1 is R, in the stan-
dard additive model (σ 21 = 0), P (z1 ) has support [0, 1]. When σ 21 > 0, the support is strictly within
                                                                                 ³           ³ ´´
the unit interval.72 In the special case when σ 20 = 0, the support is one point P (z) = Φ σγ̄ 11 . We
cannot, in general, identify ATE, TT or any treatment eﬀect requiring the endpoints 0 or 1 using
IV or control function strategies.73 In addition, the IV weights presented in Section 3 no longer
apply. IV now fails as a method for estimating interpretable causal eﬀects and treatment eﬀects.
Other approaches to estimation must be adopted if a fully symmetric model of heterogeneity is
entertained.


7.3     Violations of Uniformity

The uniformity or monotonicity assumption can be violated for any vector Z. One source of
violations is nonseparability between Z and V in (44). The random coeﬃcient model model is
one intuitive model where separability fails. Even if (44) is separable in Z and V , uniformity may
fail in the case of vector Z, where we use only one function of Z as the instrument, and do not
condition on the remaining sources of variation in Z, as we demonstrated by examples in Section 5.
If we condition appropriately, we retain monotonicity but get a new form of instrumental variable
estimator that is sensitive to the specification of the Z not used as an instrument.
                    h ³       ´ ³         ´i
  72
     The interval is Φ −|γ
                        σ1
                           1|
                               , Φ |γ 1 |
                                    σ1       .
  73
     The random coeﬃcient model for choice may explain the support problems for P (Z) noted by many analysts.
See Heckman, Ichimura, Smith, and Todd (1998).




                                                        63
8     Summary and Conclusions

This paper considers the application of the method of instrumental variables to models where
responses to treatment are heterogeneous, agents make treatment choices based in part on this
heterogeneity and some components of heterogeneity are unobserved by the economist. We call
this a model with essential heterogeneity. Intuitions about IV that are valid for the homogeneous
model are often applied inappropriately to the model of essential heterogeneity. In a model with
essential heterogeneity, diﬀerent instruments satisfying the traditional definition of an instrumental
variable define diﬀerent economic parameters. This is not the case in the classical IV literature that
assumes that responses to treatment are homogeneous. Since diﬀerent instruments identify diﬀerent
parameters, the traditional emphasis in the econometric theory literature of eﬃciently combining
instruments, or using Durbin-Wu-Hausman tests to check for endogeneity by comparing estimates
from diﬀerent instruments, is inappropriate.
    In the model with essential heterogeneity, the specification of the choice equation (Pr(D = 1 | Z))
aﬀects the interpretation of any IV estimator. This feature is absent in the classical model where
specification of the full instrument list and choice model is irrelevant to the interpretation of what
IV estimates. Two economists using the same valid instrument and the same outcome equations but
maintaining diﬀerent models of economic choice will interpret the same point estimate diﬀerently.
So will two economists using the same instrument and the same Z variables in P (Z) but using
distributions of Z that are diﬀerent. The agnostic and robust features of IV in its classical setting
disappear in a model with essential heterogeneity. We develop a simple procedure which can be
applied to test whether, in a given data set, the analyst has to worry about the complications
resulting from essential heterogeneity or whether they can be ignored in identifying treatment
parameters.
    We clarify the concept of monotonicity introduced by Imbens and Angrist (1994) and note that
uniformity is a better term for their concept. Additionally, we show that this concept is not the same
as the term “monotonicity”used in the literature to define positive IV weights on treatment eﬀects.
IV weights may be nonpositive even when uniformity is satisfied for a vector Z, if an instrument other

                                                  64
than P (Z) (or a function of P (Z)) is used. Uniformity plus conditioning on unused instruments
are required to produce positive weights in the case of vector Z. We demonstrate these points with
both theoretical and empirical examples.
   Positivity of weights is required to interpret IV estimates as treatment eﬀects. We argue, how-
ever, that many interesting policy questions do not require treatment eﬀects. Policy eﬀects and
treatment eﬀects are distinct. We develop new software for estimating MTE and the weights for
the two outcome model.
   We also compare the method of IV with the method of control functions. In the more general
setting studied here, the method of control functions is explicit in formulating its identifying as-
sumptions and recovers interpretable parameters. We establish a strong relationship between LIV
and LATE and control function models. LIV and LATE estimate the derivatives (diﬀerences) of the
level functions identified by the control function approach. When we use IV and its extensions to
answer the traditional questions addressed by the control function method, the same large sample
support assumptions are required to identify model intercepts.
   We highlight the central role of the propensity score in IV and control function methods. Us-
ing the propensity score and the distributions of X and Z we can generate instrument-invariant
parameters and weights for any instrument from a common set of parameters. The propensity
score or choice probability is more than a computational device, as it is in matching. It shows
up as a fundamental feature of both IV and control function models in the presence of essential
heterogeneity.
   We develop both ordered and unordered choice models with associated outcomes that extend
the binary choice model for essential heterogeneity. The unordered model extends the two outcome
model in a natural way. The ordered model places some special structure on it. In the context
of the ordered model, we define transition-specific treatment parameters (∆MTE
                                                                           s,s+1 (u)). We show

how to estimate these parameters using transition-specific instruments. These instruments identify
parameters that can be linked to specific choice models.
   We explain why the model of essential heterogeneity as currently formulated in the recent litera-


                                                65
ture on instrumental variables is asymmetric. It features heterogeneity (nonuniformity) of responses
to treatment but assumes uniformity in response to the variables generating choice of treatment.
We present new results for a random coeﬃcient model that allows for nonuniformity in responses
of choices to instruments and responses of outcomes to treatment.




                                                66
A     Deriving the IV Weights on MTE

We consider instrumental variables conditional on X = x using a general function of Z as an
instrument. Let J(Z) be any function of Z such that Cov(J(Z), D | X = x) 6= 0. Consider the
population analog of the IV estimator,


                       [Cov (J (Z) , Y | X = x)] / [Cov (J (Z) , D | X = x)] .


First consider the numerator of this expression,


     Cov (J (Z) , Y | X = x) = E ([J (Z) − E (J (Z) | X = x)] Y | X = x)

                              = E ((J (Z) − E (J (Z) | X = x)) (Y0 + D (Y1 − Y0 )) | X = x)

                              = E ((J (Z) − E (J (Z) | X = x)) D (Y1 − Y0 ) | X = x)


where the second equality comes from substituting in the definition of Y and the third equality fol-
                                                                       ˜
lows from assumption conditional independence assumption (A-2). Define J(Z) ≡ J(Z) − E(J(Z) |
X = x). Then


               Cov (J (Z) , Y | X = x)
                 ³                                         ´
           =       ˜
               E J(Z)   1[UD ≤ P (Z)] (Y1 − Y0 ) | X = x
                 ³                                                        ´
           =       ˜
               E J(Z) 1[UD ≤ P (Z)] E (Y1 − Y0 | X = x, Z, UD ) | X = x
                 ³                                                     ´
           =       ˜
               E J(Z)   1[UD ≤ P (Z)] E (Y1 − Y0 | X = x, UD ) | X = x
                 ⎛ ³                                      ´ ¯¯      ⎞
                         ˜                                   ¯
                 ⎜ E J(Z) 1[UD ≤ P (Z)] | X = x, UD ¯               ⎟
           =   E⎝                                            ¯ X = x⎠
                                                             ¯
                                 ×E (Y1 − Y0 | X = x, UD ) ¯
                 ⎧                                                         ⎫
               Z ⎪
                 ⎨ E(J(Z)                                                  ⎪
                        ˜     | X = x, P (Z) ≥ uD ) Pr(P (Z) ≥ uD | X = x) ⎬
           =                                                                 duD
                 ⎪
                 ⎩                         ×E (Y − Y | X = x, U = u ) ⎪    ⎭
                                                    1   0           D     D
               Z
           =                      ˜
                   ∆MTE (x, uD )E(J(Z) | X = x, P (Z) ≥ uD ) Pr(P (Z) ≥ uD | X = x)duD ,


                                                   67
where the first equality follows from plugging in the model for D; the second equality follows from
the law of iterated expectations with the inside expectation conditional on (X = x, Z, UD ); the
third equality follows from conditional independence assumption (A-2); the fourth equality follows
from Fubini’s Theorem and the law of iterated expectations with the inside expectation conditional
on (X = x, UD = uD ); the fifth equality follows from the normalization that UD is distributed
uniformly [0, 1] conditional on X; and the final equality follows from plugging in the definition of
∆MTE . Next consider the denominator of the IV estimand. Observe that by iterated expectations


                      Cov (J (Z) , D | X = x) = Cov (J (Z) , P (Z) | X = x) .


Thus, the population analog of the IV estimator is given by

                                     Z
                                         ∆MTE (x, uD )ω (x, uD ) duD                          (A.1)


where                                    ⎛                                     ⎞
                                             ˜
                                         ⎜ E(J(Z) | X = x, P (Z) ≥ uD ) ⎟
                                         ⎝                              ⎠
                                             × Pr(P (Z) ≥ uD | X = x)
                          ω (x, uD ) =                                             .          (A.2)
                                             Cov (J (Z) , P (Z) | X = x)

where by assumption Cov (J (Z) , P (Z) | X = x) 6= 0.
   If J(Z) and P (Z) are continuous random variables then a second interpretation of the weight
can be derived from (A.2) by noting that

                          Z                             Z   1
                              (j − E (J (Z) | X = x))           fP,J (t, j | X = x) dt dj
                                                         uD
                          Z
                      =       (j − E (J (Z) | X = x)) fJ (j | X = x)
                                  Z 1
                                ×     fP |J,X (t | J(Z) = j, X = x) dt dj.
                                   uD




                                                   68
Write

                                    Z   1
                                            fP |J,X (t | J(Z) = j, X = x) dt
                                      uD

                                = 1 − FP |J,X (uD | J(Z) = j, X = x)

                                = SP |J(Z),X (uD | J(Z) = j, X = x)


where SP |J,X (uD | J(Z) = j, X = x) is the probability of (P (Z) ≥ uD ) given J (Z) = j and X = x.
Likewise, Pr[P (Z) > UD | J(Z), X] = SP |J,X (UD | J(Z), X). Using these results, we may write the
weight as


                           ω (x, uD )
                                ¡                                        ¢
                           Cov J (Z) , SP |J,X (uD | J(Z), X = x) | X = x
                         =      ¡                                         ¢.
                           Cov J (Z) , SP |J,X (UD | J(Z), X = x) | X = x

For fixed uD and x evaluation points, SP |J,X (uD | J(Z), X = x) is a function of the random variable
J(Z). The numerator of the preceding expression is the covariance between J(Z) and the probability
that the random variable P (Z) is greater than the evaluation point uD conditional on J(Z).
   For a fixed x evaluation point, SP |J,X (UD | J(Z), X = x) is a given function of the random
variables UD and J(Z). The denominator of the above expression is the covariance between J(Z) and
the probability that the random variable P (Z) is greater than the random variable UD conditional
on J(Z) and X = x.
   Thus, it is clear that if the covariance between J (Z) and the conditional probability that
(P (Z) > uD ) given J (Z) is positive for all uD , then the weights are positive. The condition
is trivially satisfied if J (Z) = P (Z), so the weights are positive and IV estimates a gross treatment
eﬀect.
   If the J (Z) and P (Z) are discrete valued, we obtain expressions and (21) and (22) in the text.




                                                     69
B     Computational Aspects: Estimating the MTE, the Treat-

      ment Parameters, and the Weights

We illustrate the computational aspects of this paper using the linear and separable version of the
model of essential heterogeneity introduced in Section 3. More precisely, we consider the following
framework:


                                     Y1 = α + ϕ + β 1 X + U1

                                     Y0 = α + β 0 X + U0

                                     I = γZ − V                                              (B.1)
                                         ⎧
                                         ⎪
                                         ⎨ 1 if I > 0
                                     D =
                                         ⎪
                                         ⎩ 0 if I ≤ 0


where (U0 , U1 , V ) are independent of Z conditional on X, but U0 , U1 and V are not independent
(even conditioning on X).
    Using the same arguments presented in Section 3, we can show that


                   E (Y |X = x, P (Z) = p) = α + β 0 x + ((β 1 − β 0 ) x) p + K(p),          (B.2)


where P (Z) represents the propensity score or probability of selection (Pr(D = 1|Z)), p is a par-
ticular evaluation value of the propensity score and


                            K(p) = ϕp + E (U1 − U0 |D = 1, P (Z) = p) p.                     (B.3)


Equations (B.2) and (B.3) are closely related to the control function approach (see Section 4).




                                                 70
B.1        The Estimation of the Propensity Score and The Identification of

           the Relevant Support

The first step in the computation of the MTE is to estimate the probability of participation or
propensity score, Pr(D = 1|Z = z) = P (z). This probability can be estimated using diﬀerent
methods. In this appendix, we assume V ∼ N(0, 1) and thus estimate P (z) using a probit model.
    b denote the estimated value of γ in equation (B.1). The predicted value of the propensity
Let γ
score (conditional on Z = z), Pb(z), is then computed as Pb(z) = Pr(b
                                                                    γ Z > V |Z = z) = Φ(b
                                                                                        γ z) where
Φ represents the cumulative distribution function of a standard normal random variable.
       The predicted values of the propensity score allow us to define the values of uD over which the
MTE can be identified. In particular, as emphasized by Heckman and Vytlacil (2001b), identifica-
tion of the MTE depends critically on the support of the propensity score.74 The larger the support
of the propensity score, the bigger the set over which the MTE can be identified.
       In order to define the relevant support we first estimate the frequencies of the predicted propen-
sity scores in the samples of treated (D = 1) and untreated (D = 0) individuals. These frequencies
can be computed using smoothed sample histograms. In both subsamples the grid Γ of values of
Pb(z) specifies the number of points at which the histogram is to be evaluated.
       Let P denote the set of evaluation points (coming from the grid) such that


                    P = {p ∈ Γ | < Pr(Pb(z) = p | D = )} with = 0, 1 and > 0,


so P represents the set of values of p for which we compute frequencies in the range ( , 1] using the
subsample of individuals reporting D =            ( = 0, 1). Notice that the extreme value 0 is excluded
from P . Finally, if we denote by P the set of evaluation points used to define the relevant support
  74
   Heckman, Ichimura, Smith, and Todd (1998, 1996), and Heckman, Ichimura, and Todd (1998) also discuss the
importance of the propensity score. They present empirical evidence that failure of the full support condition is a
major source of evaluation bias.




                                                        71
of the propensity score, we have that

                     \
               P = P0 P1
                   n     ¯      ³                                            ´o
                         ¯
                 = p ∈ Γ ¯ < min Pr(Pb(z) = p | D = 0), Pr(Pb(z) = p | D = 1)


for    > 0. Therefore, the MTE is defined only for those evaluations of Pb(z) for which we obtain
positive frequencies for both subsamples.
      In practice, after identifying the relevant or common support of the propensity score, it is
necessary to adjust the sample. In particular, the observations for which Pb(z) is contained in the
common support are kept. The rest of the sample is dropped. From this point on, our analysis
refers to the resulting sample.


B.2       Semiparametric Estimation of the Marginal Treatment Eﬀect in

          Practice

Before presenting the steps used in computing the semiparametric estimate of the MTE, recall
equation (16) and make the conditioning on X explicit:

                                                                ¯
                      LIV              ∂E(Y |X = x, P (Z) = p) ¯¯
                    ∆       (x, uD ) =                          ¯     = ∆MTE (x, uD ).
                                                ∂p               p=uD


This expression indicates that in general the computation of the MTE involves the estimation of
the partial derivative of the expectation of the outcome Y (conditional on X = x and P (Z) = p)
with respect to p. This is the method of local instrumental variables introduced in Heckman and
Vytlacil (2001b). However, since we are considering the linear and separable version of the model
of essential heterogeneity, we can use equations (B.2) and (B.3) to show that

                                              ¯                                ¯
                     ∂E(Y |X = x, P (Z) = p) ¯¯                        ∂K (p) ¯¯
                                              ¯      = (β 1 − β 0 )x +                       (B.4)
                              ∂p                p=uD                    ∂p ¯p=uD




                                                     72
                                                                                                       ∂K(p)
 Thus, in order to compute the MTE we need to estimate values for (β 1 − β 0 ) and                      ∂p
                                                                                                             .   Notice
 that without additional assumptions, the estimation of this last term requires the utilization of
 nonparametric techniques.
        Diﬀerent approaches can be used in the estimation of (B.4). The following steps describe a
 semiparametric one.75


Step 1 We first estimate the coeﬃcients β 0 and (β 1 − β 0 ) in (B.2) using a nonparametric version
          of the double residual regression procedure.76 In order to do so, we start by fitting a local
          linear regression (LLR) of each regressor in (B.2) on the predicted propensity score Pb(z).
          Notice that if nX represents the number of variables in X, this step involves the estimation
          of 2 × nX local linear regressions. This is because equation (B.2) also contains terms of the
          form Xk Pb(z) for k = 1, . . . , nX . We use the k-th regressor in (B.2), Xk , to illustrate the
          LLR procedure. Let Xk (j) and Pb(z(j)) denote the values of the k-th regressor and predicted
          propensity score for the j-th individual, respectively, the latter evaluated at the Z (j) that is
          observed for the individual. The estimation of the LLR of Xk on Pb(z) requires obtaining the
          values of {θ0 (p), θ1 (p)} for a set of values of p contained in the support of Pb(z) such that
                                                           ⎧ ³                               ´2 ⎫
                                                   N ⎪
                                                     ⎨                                          ⎪
                                                   X          Xk (j) − θ0 − θ1 (Pb(z(j)) − p) ⎬
                       {θ0 (p), θ1 (p)} = argmin                                                       ,
                                           {θ0 ,θ1 } j=1   ⎪
                                                           ⎩ ×Ψ((Pb(z(j)) − p)/h)                  ⎪
                                                                                                   ⎭


          where Ψ(·) and h represent the kernel function and the bandwidth, respectively and where θ0
          and θ1 are parameters.77 In practice, we can use the set of all values of Pb(z) to define the set
          of evaluation points (p) in the LLR. This allows us to estimate the predicted value of Xk for
   75
      A FORTRAN code implementing this routine is available at jenni.uchicago.edu/underiv.
   76
      In the textbook case Y = λ1 X1 + λ2 X2 + where is assumed independent of X1 and X2 , a double residual
 regression procedure estimates λ2 using two stages. In the first stage, the estimated residuals of regressions of Y
 on X2 and X1 on X2 are computed. Let εY and εX1 denote these estimated residuals. In the second stage, λ2 is
 estimated from the regression of εY on εX1 .
   77
      The selection of optimal bandwidth is extensively studied in the nonparametric literature. In the code available
 on our website two procedures computing optimal bandwidth in the context of local regressions are implemented.
 The first one is the standard leave-one-out crossvalidation procedure. The second procedure is the refined bandwidth
 selector described in Section 4.6 of Fan and Gijbels (1996). Our code allows the utilization of three diﬀerent kernel
 functions: Epanechnikov, Gaussian and Biweight kernel functions.


                                                            73
        each individual in the sample.78

            bk (j) denote the predicted value of Xk for the j-th individual. This procedure is repeated
        Let X
        for each of the 2 × nX regressors in the outcome equations.

                                                           bk (k = 1, . . . , 2 × nX ), we now generate
Step 2 Given the predicted values of the 2 × nX regressors X
        the residual for each regressor k and person j,


                                                      bk (j) with k = 1, . . . , 2 × nX .
                                  ebXk (j) = Xk (j) − X


        We denote by b                                                       eXk (N))0 , and by ebX the matrix
                                                  eXk (1), ebXk (2), . . . , b
                     eXk the vector of residuals (b
        of residuals such that its k-th column contains the vector b
                                                                   eXk .

Step 3 As in the standard double residual regression procedure, we also need to estimate a LLR of
        Y on Pb(z). The same procedure as the one described in Step 1 is used in this case. Let Yb (j)
        denote the resulting predicted value of outcome Y for the j-th individual.

Step 4 With Yb (j) in hand, we generate the residual associated with outcome Y for each person j,


                                                  eY (j) = Y (j) − Yb (j).
                                                  b


                                                                                                      eY (N))0 .
                                                                                      eY (1), . . . , b
        Following the notation used before, we denote by ebY the vector of residuals (b

Step 5 Finally, we can estimate the values of β 0 and (β 1 − β 0 ) in (B.2) from a regression of b
                                                                                                 eY on
        eb0X . Specifically,
                                          h                 i
                                           βb0 , (β\
                                                   1 − β 0 )     e0X ebX ]−1 [b
                                                              = [b            e0X ebY ].

        Heckman, Ichimura, Smith, and Todd (1998) use a similar double residual regression ar-
        gument to characterize the selection bias in a semiparametric setup that arises from using
        nonexperimental data.
   78
     An alternative could be to use P as the set of evaluation points. In this case, in order to compute the predicted
 value of Xk for each individual, it would be necessary to replace its value of the predicted propensity scores by the
 closest value in P.


                                                           74
Step 6 From equation (B.4) we observe that after obtaining the estimated value of (β 1 − β 0 ), only
        ∂K (p)/ ∂p remains to be estimated. However, with the estimated values of β 0 and (β 1 − β 0 )
        in hand, this term can be estimated using standard nonparametric techniques. To see why,
        notice that we can write
                                                        ³     ´
                                                  Ye = K Pb(Z) + e
                                                                 v,                                           (B.5)
                                  ³            ´                                     ³        ´
               e           b          \          b                                      b
        where Y = Y − β 0 X − (β 1 − β 0 )X P (Z) and, as before, we assume E ve|P (z), X = 0.
                                                                                      ³    ´.
                                                                                        b
        Then, it is clear from (B.5) that the problem reduces to the estimation of ∂K P (z)   ∂ Pb(z),
                  ³      ´                                                      ³             ´
                    b                                                             e        b
        where K P (z) can be interpreted as the conditional expectation E Y |P (Z) = P (z) .

            b1 (p) denote the nonparametric estimator of ∂K (p)/ ∂p. Notice that we define this
        Let ϑ
        estimator as a function of p instead of Pb(z). This is because, unlike the case of the LLR
        estimators described in Step 1, we now use a subset of values of Pb(z) to define the set of
        points (p) on which our estimator is evaluated. In particular, we use the set P to define this
        set of evaluation points. As shown above, P contains the values of Pb(z) for which we obtain
                                                                        b1 (p) is computed as
        positive frequencies in both the D = 0 and D = 1 samples. Thus, ϑ
                                                          ⎧ ³               ³            ´´2 ⎫
                                                  N ⎪
                                                  X ⎨         e               b              ⎪
                                                                                             ⎬
                                                             Y (j) − ϑ0 − ϑ1 P (z(j)) − p
                     {ϑ0 (p), ϑ1 (p)} = argmin                                                         ,
                                          {ϑ0 ,ϑ1 } j=1   ⎪
                                                          ⎩ ×Ψ((Pb(z(j)) − p)/h)                   ⎪
                                                                                                   ⎭


        where as before Ψ(·) and h represent the kernel function and the bandwidth, respectively.79

Step 7 The LIV estimator of the MTE is finally computed as follows:
                                                                    ¯
                                                             \
                                                             ∂K (p) ¯
                                                                    ¯
                            ∆LIV (x, uD ) = (β\
                                              1 − β 0 )0
                                                         x +        ¯            [ (x, uD )
                                                                               = MTE
                                                              ∂p ¯
                                                                        p=uD


        and is evaluated over the set of p’s contained in P.
   79
      The code posted on our website allows the utilization of local polynomials of higher order to approximate
 K ³(p), and so the derivative
                    ´          is computed according to the selected order. It also includes the alternative of using
                b
 E Y |P (Z) = P (z) to compute a discrete version of the derivative. Furthermore, it allows the estimation of the
 MTE under the assumption of joint normalilty of the error terms.



                                                           75
 B.3     The IV Weights

 Let J be the instrument. For simplicity we assume that J is a scalar. The extension to the vector
 case is trivial. Then, as we have shown in (19) in the text, the IV weight is:

                                  ⎛                          ⎞
                                  ⎜ E(J | P (Z) > uD , X = x) ⎟
                                  ⎝                           ⎠ Pr(P (Z) > uD | X = x)
                                          −E(J|X = x)
                 ω J (x, uD ) =                                                                  (B.6)
                                                   Cov(J, D | X = x)

     In order to compute the weight:

                      b
Step 1 We approximate E(J|X = x) using a linear projection, i.e., we assume J = λ0 X + V where
                           b
       E(V |X = x) = 0, so E(J|X        b0 x.
                                 = x) = λ

Step 2 For each value of uD we generate the auxiliary indicator function 1 [P (Z) > uD ] which is equal
       to 1 if the argument of the function is true and 0 otherwise.

Step 3 We use linear projections to estimate E(J|X = x, P (Z) > uD ). More precisely, we use
       OLS to estimate the equation J(uD ) = λ0J(uD ) X + V using only the observations for which
                                                                               b
       1 [P (Z) > uD ] = 1. Since we assume E(V |X = x, P (Z) > uD ) = 0, then E(J|X = x, P (Z) >
              b0
       uD ) = λJ(uD ) x.


Step 4 Since Pr(P (Z) > uD |X = x) = Pr(1 [P (Z) > uD ] = 1|X = x) we use a probit model (for each
                                                       c (Z) > uD |X = x) denote the estimated
       value of uD ) to estimate this probability. Let Pr(P
       probability.

Step 5 We repeat steps 2,3 and 4 for each value of uD. .

            b
Step 6 With E(J|X       b
                  = x), E(J|X                       c (Z) > uD |X = x) in hand we can
                              = x, P (Z) > uD ) and Pr(P




                                                     76
      compute the numerator of (B.6). In order to get the denominator, we use the fact that

                         Z
                             ω JIV (x, uD )duD
                                1
                     =
                         Cov(J, D|X = x)
                             ⎛                                              ⎞
                          Z
                             ⎜ (E (J | P (Z) > uD , X = x) − E (J | X = x)) ⎟
                         × ⎝                                                ⎠ duD
                                         × Pr (P (Z) > uD | X = x)
                     = 1


      so with the numerator in hand, it is straightforward to obtain the value of the covariance
      (conditional on X).


B.4     The Treatment Parameter Weights

We use the Treatment on the Treated (TT) parameter to illustrate the computation of the treatment
parameter weights. The TT weight is:

                                                  Pr(P (Z) > uD |X = x)
                             ω TT (x, uD ) = R
                                                 Pr(P (Z) > uD |X = x)duD

                             c (Z) > uD |X = x) to estimate the ω TT (x, uD ). As in the case of
and consequently, we can use Pr(P
                                            c (Z) > uD |X = x) in hand, we can directly obtain the
ω JIV (x, uD ), with the estimated value of Pr(P
           R                                         R
value for Pr(P (Z) > uD |X = x)duD , using the fact ω TT (x, uD )duD = 1.


B.5     The IV and Treatment Parameter Estimators

The MTE and the weights can be used to construct the diﬀerent estimators. In particular, if ∆IV
                                                                                             J (x)

denotes the IV estimator obtained by using the instrument J we know that:

                                           Z
                              ∆IV
                               J (x)   =       MTE (x, uD ) ωJIV (x, uD )duD .




                                                      77
Likewise,
                                                Z
                                  TT
                               ∆       (x) =        MTE (x, uD ) ωTT (x, uD )duD ,

where ∆TT (x) represents the TT estimator conditional on X = x. Similar expressions exist for the
other treatment parameters. Therefore, provided with ∆LIV (x, uD ) and the estimated values for the
                       b IV
weights we can compute ∆           b TT (x). These estimators depend on the particular value of
                         J (x) and ∆

X considered. In order to compute their unconditional estimated values we need to integrate X
out. More precisely, we need to compute

                                                        Z
                                            ∆IV
                                             J      =        ∆IV
                                                              J (x)dFX (x)




and
                                                    Z
                                           TT
                                        ∆       =        ∆TT (x)dFX|D=1 (x).

     In practice we replace FX (·) and FX|D=1 (·) by their empirical analogs FbX (·) and FbX|D=1 (·) leading
to

                                                     Z
                                       ∆IV
                                        J       =           ∆IV    b
                                                             J (x)dFX (x),
                                                     Z
                                       ∆ TT
                                                =           ∆TT (x)dFbX|D=1 (x).



C       Yitzhaki’s Theorem (Yitzhaki, 1989)

Assume (Y, X) i.i.d., E(|Y |) < ∞, E(|X|) < ∞, g(X) = E(Y | X), g0 (X) exists and E (|g0 (x)|) <
∞. Let μY = E(Y ) and μX = E(X). Then,

                                                             Z   ∞
                                        Cov(Y, X)
                                                  =                  g 0 (t) ω(t) dt,
                                         Var(X)                −∞




                                                              78
where

                                       Z ∞
                                   1
                         ω(t) =            (x − μX ) fX (x) dx
                                Var(X) t
                                   1
                              =        E (X − μX | X > t) Pr (X > t) .
                                Var(X)

   Proof.


                       Cov(Y, X) = Cov (E(Y | X), X) = Cov (g(X), X)
                                   Z ∞
                                 =     g(t)(t − μX ) fX (t) dt
                                             −∞



Integration by parts implies that

                                         Z                     ¯∞
                                             t                 ¯
                              = g(t)       (x − μX ) fX (x) dx¯¯
                                      −∞                        −∞
                                  Z ∞          Z t
                                −       g0 (t)     (x − μX ) fX (x) dx dt
                                    −∞          −∞
                                Z ∞         Z ∞
                                      0
                              =      g (t)       (x − μX ) fX (x) dx dt,
                                    −∞            t



since E (X − μX ) = 0 and the first term in the first expression vanishes.
   Therefore,
                                    Z   ∞
                     Cov(Y, X) =            g 0 (t) E (X − μX | X > t) Pr (X > t) dt
                                     −∞

∴ Thus
                                    1
                        ω(t) =          E (X − μX | X > t) Pr (X > t) . ¥
                                 Var(X)

   Notice that:


(i) The weights are non-negative (ω (t) ≥ 0).

(ii) They integrate to one (use an integration by parts formula)

(iii) = 0 at t = −∞, ∞

                                                      79
We get the formula in the text when in place of X, we use P (Z) and the domain of P (Z) is suitably
defined. We apply Yitzhaki’s result to the treatment eﬀect model:


                                               Y = α + βD + ε,




                       E (Y | P (Z)) = α + E (β | D = 1, P (Z)) P (Z)

                                       = α + E (β | P (Z) > uD , P (Z)) P (Z)

                                       = g(P (Z)).


By the law of iterated expectations, we eliminate the conditioning on D = 1. Using our previous
results for OLS,
                                                            Z
                                      Cov (Y, P (Z))
                                 IV =                =          g 0 (t) ω(t) dt,
                                      Cov (D, P (Z))
                                                                    ¯
                             0      ∂ [E (β | D = 1, P (Z))] P (Z) ¯¯
                            g (t) =                                 ¯         ,
                                                ∂P (Z)                P (Z)=t
                                           R1
                                           t
                                                [ϕ − E(P (Z))] fP (ϕ) dϕ
                                  ω(t) =                                 .
                                                    Cov(P (Z), D)

Under (A-2) to (A-5) and separability, g 0 (t) = ∆MTE (t) but g 0 (t) = LIV, for P (Z) as an instrument.



D      Generalized Ordered Choice Model with Stochastic Thresh-

       olds

The ordered choice model presented in the text with parameterized, but nonstochastic, thresholds
is analyzed in Cameron and Heckman (1998) who establish its nonparametric identifiability under
the conditions they specify. Treating the Ws (or components of it) as unobservables, we obtain the
generalized ordered choice model analyzed in Carneiro, Hansen, and Heckman (2003) and Cunha,
Heckman, and Navarro (2007). In this Appendix, we present the main properties of this more


                                                      80
   general model.
      The thresholds are now written as Qs + Cs (Ws ) in place of Cs (Ws ), where Qs is a random
   variable. In addition to the order on the Cs (Ws ) in the text, we impose the order Qs + Cs (Ws ) ≥
   Qs−1 + Cs−1 (Ws−1 ), s = 2, . . . , S̄ − 1. We impose the requirement that QS̄ = ∞ and Q0 = −∞.
   The latent index Ds∗ is as defined in the text, but now


                         Ds = 1 [Cs−1 (Ws−1 ) + Qs−1 < μD (Z) − V ≤ Cs (Ws ) + Qs ]

                              = 1[   s−1 (Z, Ws−1 )   − Qs−1 > V ≥       s (Z, Ws )   − Qs ],


   where   s (Z, Ws )   = μD (Z) − Cs (Ws ). Using the fact that         s (Z, Ws )   − Qs <     s−1 (Z, Ws−1 )   − Qs−1 ,
   we obtain


                                   1[   s−1 (Z, Ws−1 )   − Qs−1 > V ≥      s (Z, Ws )   − Qs ]

                               = 1[V + Qs−1 <          s−1 (Z, Ws−1 )]


                                   −1 [V + Qs ≤        s (Z, Ws )] .




   The nonparametric identifiability of this choice model is established in Carneiro, Hansen, and
   Heckman (2003) and Cunha, Heckman, and Navarro (2007). We retain assumptions (OC-2) to
   (OC-6), but alter (OC-1) to


(OC-1)0 (Qs , Us , V ) ⊥
                       ⊥ (Z, W ) | X,        s = 1, . . . , S̄.


      Vytlacil (2006b) shows that this model with no transition specific instruments (with Ws de-
   generate for each s) implies and is implied by the independence and monotonicity conditions of
   Angrist and Imbens (1995) for an ordered model. Define Q = (Q1 , . . . , QS̄ ). Redefine π s (Z, Ws ) =
   FV +Qs (μD (Z) + Cs (Ws )) and define π(Z, W ) = [π 1 (Z, W1 ), . . . , π S̄−1 (Z, WS̄−1 )]. Redefine UD,s =




                                                             81
FV +Qs (V + Qs ). We have that


                                     E(Y | Z, W )
                                         ⎛      ⎡                       ⎤ ¯          ⎞
                                                                              ¯
                                                                              ¯
                                         ⎜ X ⎢ s−1 (Z, Ws−1 ) − Qs−1 ⎥ ¯
                                            S̄
                                                                                     ⎟
                                   = E⎝        1⎣                       ⎦ Ys ¯ Z, W ⎠
                                                  > V ≥ s (Z, Ws ) − Qs       ¯
                                           s=1                                ¯
                                          ⎛                                           ⎞
                                     X ⎜ E (1[V + Qs−1 < s−1 (Z, Ws−1 )]Ys | Z, W ) ⎟
                                      S̄
                                   =      ⎝                                           ⎠
                                     s=1       −E (1[V + Qs ≤ s (Z, Ws )]Ys | Z, W )



                                         ⎛                                               ⎞
                                             R   s−1 (Z,Ws−1 )
                                    X
                                    S̄
                                         ⎜   −∞
                                                    E (Ys | V + Qs−1 = t) dFV +Qs−1 (t) ⎟
                            =            ⎝
                                         R s (Z,Ws )                                    ⎠
                              s=1       − −∞         E (Ys | V + Qs = t) dFV +Qs (t)
                                  ⎛                                         ⎞
                                    R πs−1 (Z,Ws−1 )
                              X⎜ 0
                               S̄
                                                     E (Ys | UD,s−1 = t) dt ⎟
                            =     ⎝     R π (Z,Ws )                         ⎠.
                              s=1     − 0s           E (Ys | UD,s = t) dt


We thus have the index suﬃciency restriction that E(Y | Z, W ) = E(Y | π(Z, W )), and in the
                ∂
general case   ∂π s
                      [E(Y | π(Z, W ) = π)] = E(Ys+1 − Ys | UD,s = π s ). Also, notice that we have the
                         ∂2
restriction that      ∂π s ∂π s0
                                   [E(Y | π(Z, W ) = π)] = 0 if |s − s0 | > 1. Under full independence between
Us and V + Qs , s = 1, . . . , S̄, we can test full independence for the more general choice model by
testing for linearity of E(Y | π(Z, W ) = π) in π.
   Define
                                      ∆MTE
                                       s,s+1 (x, u) = E(Ys+1 − Ys | X = x, UD,s = u),


so that our result above can be rewritten as

                                          ∂
                                              E(Y | π(Z, W ) = π) = ∆MTE
                                                                     s,s+1 (x, π s ).
                                         ∂π s
                                                                                               ³P                         ´
                                                                                                 S̄
Since π(Z, Ws ) can be nonparametrically identified immediately from π s (Z, Ws ) = Pr             j=s+1 Dj = 1 | Z, Ws

we have that the above oﬀset equality immediately implies identification of MTE for all evaluation


                                                                 82
points within the appropriate support.
   The policy relevant treatment eﬀect is defined analogously. Recall that Hsa is defined as the
cumulative distribution function of μD (Z) − Cs (Ws ). We have that


             Ea (Ya ) = Ea (E(Y | V, Q, Z, W ))
                           ⎛      ⎡                                       ⎤                   ⎞
                           ⎜X ⎢ s−1 (Z, Ws−1 ) − Qs−1
                              S̄
                                                                          ⎥                    ⎟
                      = Ea ⎝     1⎣                                       ⎦ E(Ys | V, Q, Z, W )⎠
                             s=1     > V ≥ s (Z, Ws ) − Qs
                           ⎛      ⎡                                       ⎤               ⎞
                           ⎜X ⎢ s−1 (Z, Ws−1 ) − Qs−1
                              S̄
                                                                          ⎥             ⎟
                      = Ea ⎝     1⎣                                       ⎦ E(Ys | V, Q)⎠
                             s=1     > V ≥ s (Z, Ws ) − Qs
                           X
                           S̄
                                     ¡                                              ¢
                      =            Ea E(Ys | V, Q){Hsa (V + Qs ) − Hs−1
                                                                    a
                                                                        (V + Qs−1 )}
                           s=1
                                     ⎛                                          ⎞
                           S̄ Z
                           X         ⎜ E(Ys | V = v, Q = q)               ⎟
                      =              ⎝                                    ⎠ dFV,Q (v, q)
                                          a              a
                           s=1         ·{Hs (v + qs ) − Hs−1 (v + qs−1 )}

                               ⎛                                                          ⎞
                                       R
                         X
                         S̄
                               ⎜      E(Ys | V + Qs =          t)Hsa (t)dFV +Qs (t)
                                                                               ⎟
                     =         ⎝    R                                          ⎠
                                                          a
                         s=1       − E(Ys | V + Qs−1 = t)Hs−1 (t)dFV +Qs−1 (t)

where V , Qs enter additively, and


                  ∆PRTE
                   a,a0 = Ea0 (Y ) − Ea (Y )
                                  ⎛                       ⎞
                               Z
                          X ⎜ E(Ys+1 − Ys | V + Qs = t) ⎟
                          S̄−1
                        =         ⎝                       ⎠ dFV +Qs (t).
                                                    0
                          s=1        ·{Hsa (t) − Hsa (t)}


Alternatively, we can express this result in terms of MTE,
                                             ⎛                                        ⎞
                                                   R
                                      X
                                      S̄
                                         ⎜             E(Ys | UD,s = t)H̃sa (t)dt⎟
                         Ea (Ya ) =      ⎝        R                              ⎠
                                                                       a
                                       s=1       − E(Ys | UD,s−1 = t)H̃s−1 (t)dt




                                                          83
so that


                  ∆PRTE
                   a,a0 = Ea0 (Y ) − Ea (Y )
                          S̄−1 Z ³
                          X                                                    ´
                                                                          0
                        =          E(Ys+1 − Ys | UD,s = t){H̃sa (t) − H̃sa (t)} dt
                               s=1



where H̃sa is the cumulative distribution function of the random variable FUD,s (μD (Z) − Cs (Ws )).



E     Derivation of the Weights for IV in the Ordered Choice

      Model

We first derive Cov(J(Z, W ), Y ). Its derivation is typical of the other terms needed to form (30)
                      ˜ W ) = J(Z, W ) − E(J(Z, W )), we obtain, since Cov(J(Z, W ), Y ) =
in the text. Defining J(Z,
  ³           ´
E J˜ (Z, W ) Y ,

                                ⎡                      ⎡                              ⎤                ⎤
                                  XS̄
                                                            s (Z, Ws )
            ˜ W )Y ) = E ⎢
          E(J(Z,         ⎣ ˜
                          J(Z, W )
                                       ⎢
                                      1⎣
                                                                                      ⎥                 ⎥
                                                                                      ⎦ E(Ys | V, Z, W )⎦
                                              s=1          ≤V <     s−1 (Z, Ws−1 )
                                     ⎡                 ⎡                              ⎤           ⎤
                              X
                              S̄
                                      ⎢˜       ⎢            s (Z, Ws )                ⎥           ⎥
                          =         E ⎣J(Z, W)1⎣                                      ⎦ E(Ys | V )⎦
                              s=1                          ≤V <      s−1 (Z, Ws−1 )



where the first equality comes from the definition of Y and the law of iterated expectations, and the
second equality follows from linearity of expectations and independence assumption (OC-1). Let
Hs (·) equal Hsa (·) for a equal to the policy that characterizes the observed data, i.e., Hs (·) is the
cumulative distribution function of      s (Z, Ws ),




                                Hsa (t) = Pr( s (Z, Ws ) ≤ t)

                                           = Pr(μD (Z) − Cs (Ws ) ≤ t).




                                                           84
Using the law of iterated expectations, we obtain


                     ˜ W )Y )
                 E(J(Z,
                       ⎡ ⎛           ⎛                          ⎞¯ ⎞              ⎤
                                                                  ¯
                 X ⎢ ⎜
                  S̄
                                        1[V < s−1 (Z, Ws−1 )] ⎟¯ ⎟¯
               =             ˜ W)⎜
                     E ⎣E ⎝ J(Z,     ⎝
                                                                                  ⎥
                                                                ⎠¯ V ⎠ E (Ys | V )⎦
                                         −1[V ≤ s (Z, Ws )]       ¯
                 s=1                                              ¯
                 XS̄ Z
               =        [E(Ys | V = v){Ks−1 (v) − Ks (v)}] fV (v)dv
                   s=1
                   S̄−1 Z
                   X
               =            [E(Ys+1 − Ys | V = v)Ks (v)] fV (v)dv
                   s=1

                ³                                ´
                  ˜ W) |
where Ks (v) = E J(Z,            s (Z, Ws )   > v (1 − Hs (v)) and we use the fact that KS̄ (v) = K0 (v) =
0. Now consider the denominator of the IV estimand,
                                    ⎡                      ⎡                            ⎤⎤
                                                  X
                                                  S̄
                                                               s (Z, Ws )
              ˜ W )) = E ⎢
           E(DJ(Z,         ˜ W)
                         ⎣J(Z,
                                                           ⎢
                                                         s1⎣
                                                                                        ⎥⎥
                                                                                        ⎦⎦
                                                   s=1         ≤V <    s−1 (Z, Ws−1 )

                               X
                               S̄    h                                                       i
                             =         ˜ W )1[ s (Z, Ws ) ≤ V <
                                  s E J(Z,                                     s−1 (Z, Ws−1 )]
                                 s=1
                                           ⎡ ⎛                 ⎛                ⎞¯ ⎞⎤
                                                                                 ¯
                                 X
                                 S̄
                                                        1[V  <      (Z, W    )]  ¯
                                       ⎢ ⎜˜          ⎜          s−1      s−1    ⎟¯ ⎟⎥
                             =     s E ⎣E ⎝ J(Z, W ) ⎝                          ⎠¯ V ⎠⎦
                                                        −1[V ≤ s (Z, Ws )]       ¯
                               s=1                                               ¯
                               XS̄ Z
                             =     s [Ks−1 (v) − Ks (v)] fV (v)dv
                                 s=1
                                 S̄−1 Z
                                 X
                             =            Ks (v) fV (v)dv.
                                 s=1


Collecting results, we obtain an expression for the IV estimand (30):

                                        S̄−1 Z
                     Cov(J, Y ) X
                               =                 E(Ys+1 − Ys | V = v)ω(s, v) fV (v)dv
                     Cov(J, D)   s=1




                                                          85
where

                                                       Ks (v)
                             ω(s, v) = PS̄    R
                                         s=1 s [Ks−1 (v) − Ks (v)] fV (v)dv
                                                Ks (v)
                                      = PS̄−1 R
                                         s=1    Ks (v) fV (v)dv

and clearly
                 S̄−1 Z
                 X
                          ω(s, v) fV (v)dv = 1,    ω(0, v) = 0,      and       ω(S̄, v) = 0.
                 s=1




F       Proof of Theorem 1

Proof. The basic idea is that we can bring the model back to a two choice set up of j versus
the “next best”option. We prove the result for the second assertion, that ∆LIV
                                                                           j   (x, z) recovers
the marginal treatment eﬀect parameter. The first assertion, that ∆Wald
                                                                   j    (x, z [−j] , z [j] , z̃ [j] ) recovers
a LATE parameter, follows from a trivial modification to the same proof strategy. Recall that
RJ \j (z) = maxi∈J \j {Ri (z)} and that IJ \j = argmaxi∈J \j (Ri (Z)). We may write Y = YJ \j +
DJ ,j (Yj − YJ \j ). We have


                                 Pr (DJ ,j = 1 | X = x, Z = z)
                                    ¡                                   ¢
                               = Pr Rj (zj ) > RJ \j (z) | X = x, Z = z
                                    ¡                                       ¢
                               = Pr ϑj (zj ) ≥ RJ \j (z) − Vj | X = x, Z = z .


Using independence assumption (B-1), RJ \j (z) − Vj is independent of Z conditional on X, so that


                                    Pr (DJ ,j = 1 | X = x, Z = z)
                                       ¡                                ¢
                                  = Pr ϑj (zj ) ≥ RJ \j (z) − Vj | X = x .




                                                     86
ϑk (·) does not depend on z [j] for k 6= j by assumption (B-2b), and thus RJ \j (z) does not depend on
z [j] , and we will therefore with an abuse of notation write RJ \j (z [−j] ) for RJ \j (z). Write F (·; x, z [−j] )
for the distribution function of RJ \j (z [−j] ) − Vj conditional on X = x. Then


                                            Pr (DJ ,j = 1 | X = x, Z = z)

                                       = F (ϑj (zj ); x, z [−j] ),


and

                                        ∂
                                          [j]
                                              Pr (DJ ,j = 1 | X = x, Z = z)
                                      ∙∂z               ¸
                                          ∂               ¡                   ¢
                                    =       [j]
                                                ϑj (zj ) f ϑj (zj ); x, z [−j] ,
                                        ∂z

where f (·; x, z [−j] ) is the density of RJ \j (z [−j] ) − Vj conditional on X = x. Consider

                                           ¡                       ¢
                   E (Y | X = x, Z = z) = E YJ \j | X = x, Z = z
                                             ¡                                 ¢
                                          +E DJ ,j (Yj − YJ \j ) | X = x, Z = z .

                                                                  ¡                    ¢
As a consequence of (B-1), (B-2b), (B-3) and (B-4), we have that E YJ \j | X = x, Z = z does not
depend on z [j] . Using the assumptions and the law of iterated expectations, we may write

                        ¡                                   ¢
                      E DJ ,j (Yj − YJ \j ) | X = x, Z = z
                                 ⎛            ¯                             ⎞
                      Z ϑj (z)                ¯
                                              ¯ X = x, Z = z,
                                 ⎜            ¯                             ⎟           [−j]
                    =          E ⎝Yj − YJ \j ¯                              ⎠ f (t; x, z ) dt
                       −∞                     ¯ R (z [−j] ) − V = t
                                              ¯ J \j            j
                                 ⎛            ¯                              ⎞
                      Z ϑj (z)                ¯
                                              ¯ X = x, Z [−j] = z [−j] ,
                                 ⎜            ¯                              ⎟
                    =          E ⎝Yj − YJ \j ¯                               ⎠ f (t; x, z [−j] ) dt.
                       −∞                     ¯ R (z [−j] ) − V = t
                                              ¯ J \j            j




                                                         87
Thus,

                                   ∂
                                     [j]
                                         E (Y | X = x, Z = z)
                                 ∂z⎛                ¯                          ⎞
                                                    ¯
                                                    ¯ X = x, Z [−j] = z [−j] ,
                                    ⎜               ¯                          ⎟
                               = E ⎝Yj − YJ \j ¯                               ⎠
                                                    ¯ R (z) = R (z)
                                                    ¯ j          J \j
                                   "              #
                                       ∂
                                 ·          ϑ (z ) f (ϑj (zj )).
                                         [j] j j
                                     ∂zj

Combining results, we have

                                     ∂
                                    ∂z [j]
                                           E   (Y |X = x, Z = z)
                            ∂
                            Pr(DJ ,j = 1|X = x, Z = z)
                           ∂z [j]
                          ¡                                                        ¢
                       = E Yj − YJ \j | X = x, Z [−j] = z [−j] , Rj (z) = RJ \j (z) .


Finally, noting that

                          ¡                                                        ¢
                         E Yj − YJ \j | X = x, Z [−j] = z [−j] , Rj (z) = RJ \j (z)
                          ¡                                                ¢
                       = E Yj − YJ \j | X = x, Z = z, Rj (z) = RJ \j (z)


provides the stated result. The proof for the LATE result follows from a parallel argument using
discrete changes in the instrument.




                                                           88
References

Ahn, H. and J. Powell (1993, July). Semiparametric estimation of censored selection models with
  a nonparametric selection mechanism. Journal of Econometrics 58 (1-2), 3—29.

Angrist, J. D. and G. W. Imbens (1995, June). Two-stage least squares estimation of average
  causal eﬀects in models with variable treatment intensity. Journal of the American Statistical
  Association 90 (430), 431—442.

Angrist, J. D., G. W. Imbens, and D. Rubin (1996). Identification of causal eﬀects using instrumental
  variables. Journal of the American Statistical Association 91 (434), 444—455.

Björklund, A. and R. Moﬃtt (1987, February). The estimation of wage gains and welfare gains in
  self-selection. Review of Economics and Statistics 69 (1), 42—49.

Bresnahan, T. F. (1987, June). Competition and collusion in the american automobile industry:
  The 1955 price war. Journal of Industrial Economics 35 (4), 457—482.

Cameron, S. V. and J. J. Heckman (1998, April). Life cycle schooling and dynamic selection bias:
  Models and evidence for five cohorts of american males. Journal of Political Economy 106 (2),
  262—333.

Card, D. (2001, September). Estimating the return to schooling: Progress on some persistent
  econometric problems. Econometrica 69 (5), 1127—1160.

Carneiro, P., K. Hansen, and J. J. Heckman (2001, Fall). Removing the veil of ignorance in assessing
  the distributional impacts of social policies. Swedish Economic Policy Review 8 (2), 273—301.

Carneiro, P., K. Hansen, and J. J. Heckman (2003, May). Estimating distributions of treatment
  eﬀects with an application to the returns to schooling and measurement of the eﬀects of uncer-
  tainty on college choice. International Economic Review 44 (2), 361—422. 2001 Lawrence R. Klein
  Lecture.



                                                 89
Chen, X., J. J. Heckman, and E. J. Vytlacil (1998). Non/Semiparametric identification and estima-
  tion of a dynamic discrete-time discrete-choice models with unobserved heterogeneity. Unpub-
  lished working paper, University of Chicago, Department of Economics.

Chen, X., J. J. Heckman, and E. J. Vytlacil (1999). Identification and square-root-n eﬃcient
  estimation of semiparametric panel data models with binary dependent variables and a latent
  factor. Unpublished working paper, University of Chicago, Department of Economics.

Cunha, F., J. J. Heckman, and S. Navarro (2005, April). Separating uncertainty from heterogeneity
  in life cycle earnings, the 2004 hicks lecture. Oxford Economic Papers 57 (2), 191—261.

Cunha, F., J. J. Heckman, and S. Navarro (2006). Counterfactual analysis of inequality and social
  mobility. In S. L. Morgan, D. B. Grusky, and G. S. Fields (Eds.), Mobility and Inequality: Fron-
  tiers of Research in Sociology and Economics, Chapter 4, pp. 290—348. Stanford, CA: Stanford
  University Press.

Cunha, F., J. J. Heckman, and S. Navarro (2007). The identification and economic content of
  ordered choice models with stochastic cutoﬀs. International Economic Review. Under revision.

Dahl, G. B. (2002, November). Mobility and the return to education: Testing a roy model with
  multiple markets. Econometrica 70 (6), 2367—2420.

Durbin, J. (1954). Errors in variables. Review of the International Statistical Institute 22, 23—32.

Fan, J. and I. Gijbels (1996). Local Polynomial Modelling and its Applications. New York: Chapman
  and Hall.

Gronau, R. (1974, November-December). Wage comparisons — a selectivity bias. Journal of Political
  Economy 82 (6), 1119—43.

Harmon, C. and I. Walker (1999, April). The marginal and average returns to schooling in the uk.
  European Economic Review 43 (4-6), 879—887.



                                                 90
Hausman, J. A. (1978, November). Specification tests in econometrics. Econometrica 46 (6), 1251—
  1272.

Heckman, J. J. (1974, July). Shadow prices, market wages, and labor supply. Econometrica 42 (4),
  679—694.

Heckman, J. J. (1976a, December). The common structure of statistical models of truncation,
  sample selection and limited dependent variables and a simple estimator for such models. Annals
  of Economic and Social Measurement 5 (4), 475—492.

Heckman, J. J. (1976b). Simultaneous equation models with both continuous and discrete endoge-
  nous variables with and without structural shift in the equations. In S. Goldfeld and R. Quandt
  (Eds.), Studies in Nonlinear Estimation, pp. 235—272. Cambridge, MA: Ballinger Publishing
  Company.

Heckman, J. J. (1979, January). Sample selection bias as a specification error. Econometrica 47 (1),
  153—162.

Heckman, J. J. (1980). Addendum to sample selection bias as a specification error. In E. Stroms-
  dorfer and G. Farkas (Eds.), Evaluation Studies Review Annual, Volume 5. Beverly Hills: Sage
  Publications.

Heckman, J. J. (1990, May). Varieties of selection bias. American Economic Review 80 (2), 313—318.

Heckman, J. J. (1997, Summer). Instrumental variables: A study of implicit behavioral assumptions
  used in making program evaluations. Journal of Human Resources 32 (3), 441—462. Addendum
  published vol. 33 no. 1 (Winter 1998).

Heckman, J. J. (2001, August). Micro data, heterogeneity, and the evaluation of public policy:
  Nobel lecture. Journal of Political Economy 109 (4), 673—748.

Heckman, J. J. and B. E. Honoré (1990, September). The empirical content of the Roy model.
  Econometrica 58 (5), 1121—1149.

                                                91
Heckman, J. J., H. Ichimura, J. Smith, and P. E. Todd (1996). Sources of selection bias in eval-
  uating social programs: An interpretation of conventional measures and evidence on the eﬀec-
  tiveness of matching as a program evaluation method. Proceedings of the National Academy of
  Sciences 93 (23), 13416—13420.

Heckman, J. J., H. Ichimura, J. Smith, and P. E. Todd (1998, September). Characterizing selection
  bias using experimental data. Econometrica 66 (5), 1017—1098.

Heckman, J. J., H. Ichimura, and P. E. Todd (1998, April). Matching as an econometric evaluation
  estimator. Review of Economic Studies 65 (223), 261—294.

Heckman, J. J., R. J. LaLonde, and J. A. Smith (1999). The economics and econometrics of active
  labor market programs. In O. Ashenfelter and D. Card (Eds.), Handbook of Labor Economics,
  Volume 3A, Chapter 31, pp. 1865—2097. New York: North-Holland.

Heckman, J. J., L. J. Lochner, and P. E. Todd (2006). Earnings equations and rates of return:
  The Mincer equation and beyond. In E. A. Hanushek and F. Welch (Eds.), Handbook of the
  Economics of Education. Amsterdam: North-Holland. forthcoming.

Heckman, J. J. and S. Navarro (2004, February). Using matching, instrumental variables, and
  control functions to estimate economic choice models. Review of Economics and Statistics 86 (1),
  30—57.

Heckman, J. J. and S. Navarro (2006). Dynamic discrete choice and dynamic treatment eﬀects.
  Journal of Econometrics, forthcoming.

Heckman, J. J. and R. Robb (1985). Alternative methods for evaluating the impact of interventions.
  In J. Heckman and B. Singer (Eds.), Longitudinal Analysis of Labor Market Data, Volume 10,
  pp. 156—245. New York: Cambridge University Press.

Heckman, J. J. and R. Robb (1986). Alternative methods for solving the problem of selection bias
  in evaluating the impact of treatments on outcomes. In H. Wainer (Ed.), Drawing Inferences


                                               92
  from Self-Selected Samples, pp. 63—107. New York: Springer-Verlag. Reprinted in 2000, Mahwah,
  NJ: Lawrence Erlbaum Associates.

Heckman, J. J., J. L. Tobias, and E. J. Vytlacil (2003, August). Simple estimators for treatment
  parameters in a latent variable framework. Review of Economics and Statistics 85 (3), 748—754.

Heckman, J. J. and S. Urzua (2006). Interpreting iv estimates of the eﬀect of schooling on earnings.
  Unpublished manuscript, University of Chicago, Department of Economics.

Heckman, J. J. and E. J. Vytlacil (1999, April). Local instrumental variables and latent variable
  models for identifying and bounding treatment eﬀects. Proceedings of the National Academy of
  Sciences 96, 4730—4734.

Heckman, J. J. and E. J. Vytlacil (2000, January). The relationship between treatment parameters
  within a latent variable framework. Economics Letters 66 (1), 33—39.

Heckman, J. J. and E. J. Vytlacil (2001a). Instrumental variables, selection models, and tight bounds
  on the average treatment eﬀect. In M. Lechner and F. Pfeiﬀer (Eds.), Econometric Evaluation of
  Labour Market Policies, pp. 1—15. New York: Center for European Economic Research.

Heckman, J. J. and E. J. Vytlacil (2001b). Local instrumental variables. In C. Hsiao, K. Morimune,
  and J. L. Powell (Eds.), Nonlinear Statistical Modeling: Proceedings of the Thirteenth Interna-
  tional Symposium in Economic Theory and Econometrics: Essays in Honor of Takeshi Amemiya,
  pp. 1—46. New York: Cambridge University Press.

Heckman, J. J. and E. J. Vytlacil (2001c, May). Policy-relevant treatment eﬀects. American
  Economic Review 91 (2), 107—111.

Heckman, J. J. and E. J. Vytlacil (2005, May). Structural equations, treatment eﬀects and econo-
  metric policy evaluation. Econometrica 73 (3), 669—738.

Heckman, J. J. and E. J. Vytlacil (2007a). Econometric evaluation of social programs, part I: Causal



                                                 93
  models, structural models and econometric policy evaluation. In J. Heckman and E. Leamer
  (Eds.), Handbook of Econometrics, Volume 6. Amsterdam: Elsevier. Forthcoming.

Heckman, J. J. and E. J. Vytlacil (2007b). Econometric evaluation of social programs, part II:
  Using the marginal treatment eﬀect to organize alternative economic estimators to evaluate social
  programs and to forecast their eﬀects in new environments. In J. Heckman and E. Leamer (Eds.),
  Handbook of Econometrics, Volume 6. Amsterdam: Elsevier. Forthcoming.

Imbens, G. W. (2004, February). Nonparametric estimation of average treatment eﬀects under
  exogeneity: A review. Review of Economics and Statistics 86 (1), 4 — 29.

Imbens, G. W. and J. D. Angrist (1994, March). Identification and estimation of local average
  treatment eﬀects. Econometrica 62 (2), 467—475.

Lee, L.-F. (1983, March). Generalized econometric models with selectivity. Econometrica 51 (2),
  507—512.

Manski, C. F. and J. V. Pepper (2000, July). Monotone instrumental variables: With an application
  to the returns to schooling. Econometrica 68 (4), 997—1010.

Mare, R. D. (1980, June). Social background and school continuation decisions. Journal of the
  American Statistical Association 75 (370), 295—305.

Powell, J. L. (1994). Estimation of semiparametric models. In R. Engle and D. McFadden (Eds.),
  Handbook of Econometrics, Volume 4, pp. 2443—2521. Amsterdam: Elsevier.

Prescott, E. C. and M. Visscher (1977, Autumn). Sequential location among firms with foresight.
  Bell Journal of Economics 8 (2), 378—893.

Rosenbaum, P. R. and D. B. Rubin (1983, April). The central role of the propensity score in
  observational studies for causal eﬀects. Biometrika 70 (1), 41—55.

Shaked, A. and J. Sutton (1982, January). Relaxing price competition through product diﬀerenti-
  ation. Review of Economic Studies 49 (1), 3—13.

                                                94
Thompson, T. S. (1989). Identification of semiparametric discrete choice models. Discussion Paper
  249, University of Minnesota Center for Economic Research, Minneapolis, MN.

Vytlacil, E. J. (2002, January). Independence, monotonicity, and latent index models: An equiva-
  lence result. Econometrica 70 (1), 331—341.

Vytlacil, E. J. (2006a). A note on additive separability and latent index models of binary choice:
  Representation results. Oxford Bulletin of Economics and Statistics forthcoming.

Vytlacil, E. J. (2006b, August). Ordered discrete choice selection models: Equivalence, nonequiva-
  lence, and representation results. Review of Economics and Statistics 88 (3). Forthcoming.

White, H. (1984). Asymptotic Theory for Econometricians. Orlando, FL: Academic Press.

Wu, D. (1973, July). Alternative tests of independence between stochastic regressors and distur-
  bances. Econometrica 41 (4), 733—750.

Yitzhaki, S. (1989). On using linear regression in welfare economics. Working Paper 217, Depart-
  ment of Economics, Hebrew University.

Yitzhaki, S. (1996, October). On using linear regressions in welfare economics. Journal of Business
  and Economic Statistics 14 (4), 478—486.

Yitzhaki, S. and E. Schechtman (2004). The gini instrumental variable, or the “double instrumental
  variable” estimator. Metron 62 (3), 287—313.




                                                 95
                                                  Table 1A
                            Treatment Eﬀects and Estimands as Weighted Averages
                                      of the Marginal Treatment Eﬀect



                                             1
ATE(x) = E (Y1 − Y0 | X = x) =                0
                                                  ∆MTE (x, uD ) duD
                                                    1
TT(x) = E (Y1 − Y0 | X = x, D = 1) =                 0
                                                         ∆MTE (x, uD ) ωTT (x, uD ) duD
                                                      1
TUT(x) = E (Y1 − Y0 | X = x, D = 0) =                    0
                                                                 ∆MTE (x, uD ) ωTUT (x, uD ) duD
                                                                                                         1
Policy Relevant Treatment Eﬀect (x) = E (Ya | X = x) − E (Ya | X = x) =                                  0
                                                                                                              ∆MTE (x, uD ) ωPRTE (x, uD ) duD

    for two policies a and a that aﬀect the Z but not the X
            1
IVJ (x) =   0
                 ∆MTE (x, uD ) ωIV
                                J
                                   (x, uD ) duD , given instrument J
            1
OLS(x) =     0
                 ∆MTE (x, uD ) ωOLS (x, uD ) duD


                                                                          Table 1B
                                                                          Weights


       ωATE (x, uD ) = 1
                                                                    1
                               1
       ωTT (x, uD ) =          uD
                                       f (p | X = x)dp
                                                                 E(P | X = x)
                                uD                                         1
       ωTUT (x, uD ) =             0
                                         f (p|X = x) dp
                                                                     E ((1 − P ) |X = x)
                                                                         
                                   FPa ,X (uD ) − FPa ,X (uD )
       ωPRTE (x, uD ) =
                                               ∆P
                                                                                                                          1
        J                      1
       ωIV (x, uD ) =          uD
                                  (J(Z)     − E(J(Z) | X = x))                    fJ,P |X (j, t | X = x) dt dj
                                                                                                                     Cov(J(Z), D | X = x)

                          E(U1 | X = x, UD = uD ) ω1 (x, uD ) − E(U0 | X = x, UD = uD ) ω0 (x, uD )
       ωOLS (x, uD ) = 1 +
                                                       ∆MTE (x, uD )
                                              1
                                                         
                       1
       ω1 (x, uD ) = uD f (p | X = x) dp
                                            E(P | X = x)
                          uD                                      1
       ω0 (x, uD ) =       0
                                   f (p | X = x) dp
                                                             E((1 − P ) | X = x)

       Source: Heckman and Vytlacil (2005)
                    Figure 1. Distribution of Gains
                          The Roy Economy

                               U1 − U0 ⊥
                                       
                                       ⊥D


0.25
                         TT
       Return to Marginal Agent
                        AT E
 0.2                    T UT



0.15




 0.1




0.05




  0
  5                                     0 0.2   C=1.5                  5
                                    β = Y1 - Y0

                    TT= 2.666, TUT= −0.632
                Return to Marginal Agent = C = 1.5
                     ATE = µ1 − µ0 = β̄ = 0.2



                                  The Model

                Outcomes                         Choice Model
                                                   
                                                         1 if D∗ > 0
       Y1 = µ1 + U1 = α + β̄ + U1             D=
                                                         0 if D∗ ≤ 0
         Y0 = µ0 + U0 = α + U0

                               General Case

                            (U1 − U0 ) ⊥
                                       
                                       ⊥D
                           ATE = TT = TUT


                   The Researcher Observes (Y, D, C)

                Y = α + βD + U0 where β = Y1 − Y0


                            Parameterization

       α = 0.67      (U1 , U0 ) ∼ N (0, Σ)
                                              D∗ = Y1 − Y0 − C
                                1    −0.9
        β̄ = 0.2     Σ=                                 C = 1.5
                             −0.9     1
                                                                         Figure 2. Monotonicity
                                                                       The Extended Roy Economy

                 A. Standard Case                                    B. Changing Z1 without Controlling for Z2                                         C. Random Coeﬃcient Case
                                                                                                                                                                               ~
                                                                        0.25
                                                                                                        γz                                                                 γ z=~
                                                                                                                                                                           ~   γz
                                                                                                                                             0.25   ~
0.25
                                                                                                                   z =(0,1)                         γ =( 0.5,0.5)
                                              z =(0,1)                                                                                              ~
                                                                                                                                                    ~
                                                                                                                   z' =(1,1)                        γ =(-0.5,0.5)
                                              z' =(1,1)                 0.2                                                                  0.2    z =(0,1)
0.2                                                                                                                z'' =(1,-1)
                                                                                                                                                    z' =(1,1)
                                                                        0.15                                                                 0.15
0.15
                                                                                                                                                                    ~
                                                                                                                                                                    ~
                                                                                              γ z''                                                                 γ z'
                                                                        0.1                                                                   0.1
0.1


                                                                        0.05                                    γ z'                         0.05                                    ~
                                                                                                                                                                                     γ z'
0.05                        γz         γ z'

                                                                           0                                                                    0 -5
   0                                                                           -5                     0 0.5 1                    5                                         0 0.5 1          5
       -5                   0 0.5 1                         5
                         β = Y1 - Y0                                                              β = Y1 - Y0                                                          β = Y1 - Y0



                                 Outcomes                                                                        Choice Model
                                                                                                       
                                                                                                          1 if Y1 − Y0 − γZ > 0
                         Y1 = α + β̄ + U1                                                    D=
                                                                                                          0 if Y1 − Y0 − γZ ≤ 0
                            Y0 = α + U0                                                                with γZ = γ1 Z1 + γ2 Z2

                                                                                    Parameterization
                                                                             
                                                                 1       −0.9
                 (U1 , U0 ) ∼ N (0, Σ) , Σ =                                    ,       α = 0.67, β̄ = 0.2, γ = (0.5, 0.5) (except in Case C)
                                                                −0.9      1

                                                                    Z1 = {−1, 0, 1} and Z2 = {−1, 0, 1}


            A. Standard Case                     B. Changing Z1 without Controlling for Z2                                           C. Random Coeﬃcient Case

           z −→ z                                            z −→ z  or z −→ z                                                             z −→ z 
  z = (0, 1) and z  = (1, 1)                       z = (0, 1), z  = (1, 1) and z  = (1, −1)                                       z = (0, 1) and z  = (1, 1)

                                                                                                                                  γ is a random vector
                                                                                                                           γ = (0.5, 0.5) and γ = (−0.5, 0.5)
                                                                                                                         where γ and γ are two realizations of γ

             D(γz) ≥ D(γz  )                         D(γz) ≥ D(γz  ) or D(γz) < D(γz  )                             D γz ≥ D γz  and D (γz) < D (γz  )

            For all individuals                           Depending on the value of z  or z                                        Depending on value of γ
               Figure 3. IV Weight and Its Components under Discrete Instruments when P (Z) is the Instrument
                                               The Extended Roy Economy
                            A. IV Weights                               B. E(P (Z)|P (Z) > p ) and E(P (Z))
            0.8
                                                                                           1
            0.7

            0.6                                                                          0.8

            0.5                                                                                  E(P(Z))=0.5663
                                                                                         0.6
            0.4

            0.3                                                                          0.4


            0.2
                                                                                         0.2

            0.1
                                                                                           0
              0                                                                                   E(P(Z)|P(Z)>p1) E(P(Z)|P(Z)>p )E(P(Z)|P(Z)>p ) E(P(Z)|P(Z)>p )
                          λ        λ             λ3        λ                                                                   2              3               4
                           1        2                       4
           -0. 1                                                                     -0.2




                                                       C. Local Average Treatment Eﬀects
                                            0.8


                                            0.6


                                            0.4


                                            0.2
                                                                                         LATE                     LATE
                                                                                     ∆                        ∆
                                                                                         0.540,0.640              0.640,0.730
                                             0
                                                       LATE
                                                      ∆              ∆ LATE
                                                       0.340,0.438     0.438,0.540
                                           -0.2


                                           -0.4


                                           -0.6


                                           -0.8




                                        The model is the same as the one presented below Figure 2.

                                                                                                   K−1
                        ATE = 0.2, TT = 0.5942, TUT = −0.4823 and ∆IV
                                                                   P (Z) =                                   ∆LATE (p , p+1 ) λ = −0.09
                                                                                                       =1


                                                                                                                                        
LATE                       E (Y |P (Z) = p+1 ) − E (Y |P (Z) = p )         β (p+1 − p ) + σU1 −U0 φ Φ−1 (1 − p+1 ) − φ Φ−1 (1 − p )
∆       (p , p+1 )     =                                               =
                                            p+1 − p                                                     p+1 − p
                                        K                    K                              K
                                            (pi − E (P (Z)))      f (pi , pt )                    (pt − E (P (Z))) f (pt )
                                        i=1                   t>                             t>
                   λ    = (p+1 − p )                                        = (p+1 − p )
                                                   Cov (Z1 , D)                                       Cov (Z1 , D)

                               Joint Probability Distribution of (Z1 , Z2 ) and the Propensity Score
    (joint probabilities in ordinary type (Pr(Z1 = z1 , Z2 = z2 )); propensity score in italics (Pr (D = 1|Z1 = z1 , Z2 = z2 )))
                                               Z1 \Z2      −1         0          1
                                                 −1       0.02       0.02      0.36
                                                        0.7309 0.6402 0.5409
                                                  0        0.3       0.01      0.03
                                                        0.6402 0.5409 0.4388
                                                  1        0.2       0.05      0.01
                                                        0.5409 0.4388 0.3408
                                                     Cov(Z1 , Z2 ) = −0.5468
               Figure 4. IV Weight and Its Components under Discrete Instruments when Z1 is the Instrument
                                              The Extended Roy Economy
                           A. IV Weights                                 B. E(Z1 |P (Z) > p ) and E(Z1 )

                                                                                        E(Z |P(Z)>p )   E(Z1|P(Z)>p2) E(Z1|P(Z)>p3) E(Z1|P(Z)>p4)
                                                                                           1       1


          1                                                                  0
                                                                          E(Z1)=-0.14
        0.8                                                               -0. 2


        0.6
                                                                          -0.4


        0.4
                                                                          -0.6

        0.2
                                     λ                                    -0.8
                                      3
          0
                    λ        λ                λ4                            -1
                     1        2
        -0.2




The model is the same as the one presented below Figure 2. The values of the treatment parameters are the same as the
                                           ones presented below Figure 3.
                                                   K−1
                                          ∆IV
                                           Z1 =          ∆LATE (p , p+1 ) λ = 0.1833
                                                   =1

                                                          
                                                          I                        
                                                                                   K
                                                                (z1,i − E (Z1 ))         f (z1,i , pt )
                                                          i=1                      t>
                                     λ = (p+1 − p )
                                                                       Cov (Z1 , D)




                            Joint Probability Distribution of (Z1 , Z2 ) and the Propensity Score
 (joint probabilities in ordinary type (Pr(Z1 = z1 , Z2 = z2 )); propensity score in italics (Pr (D = 1|Z1 = z1 , Z2 = z2 )))
                                            Z1 \Z2      −1         0          1
                                              −1       0.02       0.02      0.36
                                                     0.7309 0.6402 0.5409
                                               0        0.3       0.01      0.03
                                                     0.6402 0.5409 0.4388
                                               1        0.2       0.05      0.01
                                                     0.5409 0.4388 0.3408
                                                  Cov(Z1 , Z2 ) = −0.5468
 Table 2. The Conditional Instrumental Variable Estimator ∆IV  Z1 |Z2 =z2 and Conditional Local Average Treatment Eﬀect
                            LATE
                                                      
                           ∆     (p , p+1 |Z2 = z2 ) when Z1 is the Instrument (given Z2 = z2 )
                                                The Extended Roy Economy

                                                                        Z2 = −1        Z2 = 0       Z2 = 1

                                               P (−1, Z2 ) = p3          0.7309        0.6402        0.5409
                                                P (0, Z2 ) = p2          0.6402        0.5409        0.4388
                                                P (1, Z2 ) = p1          0.5409        0.4388        0.3408


                                                       λ1                0.8418        0.5384        0.2860
                                                       λ2                0.1582        0.4616        0.7140


                                                ∆LATE (p1 , p2 )        −0.2475       0.2497         0.7470
                                                ∆LATE (p2 , p3 )        −0.7448       −0.2475        0.2497


                                                   ∆IV
                                                    Z1 |Z2 =z2          −0.3262        0.0202        0.3920


                                       The model is the same as the one presented below Figure 2



                                     I−1                                                 I−1
                  ∆IV
                   Z1 |Z2 =z2 =            ∆LATE (p , p+1 |Z2 = z2 ) λ|Z2 =z2 =             ∆LATE (p , p+1 |Z2 = z2 ) λ|Z2 =z2
                                     =1                                                 =1



                                                            E (Y |P (Z) = p+1 , Z2 = z2 ) − E (Y |P (Z) = p , Z2 = z2 )
                     ∆LATE (p , p+1 |Z2 = z2 ) =
                                                                                     p+1 − p

                           
                           I                                 
                                                             I                                                
                                                                                                              I
                                 (z1,i − E (Z1 |Z2 = z2 ))         f (z1,i , pt |Z2 = z2 )                          (z1,t − E (Z1 |Z2 = z2 )) f (z1,t , pt |Z2 = z2 )
                           i=1                               t>                                              t>
λ|Z2 =z2 = (p+1 − p )                                                                     = (p+1 − p )
                                                  Cov (Z1 , D)                                                                     Cov (Z1 , D)

                                 Probability Distribution of Z1 Conditional on Z2 (Pr(Z1 = z1 |Z2 = z2 ))
                                  z1 Pr(Z1 = z1 |Z2 = −1) Pr(Z1 = z1 |Z2 = 0) Pr(Z1 = z1 |Z2 = 1)
                                 −1           0.0385                  0.25                  0.9
                                  0           0.5769                 0.125                0.075
                                  1           0.3846                 0.625                0.025
          Figure 5. Conditional Expectation of Y on P (Z) and the Marginal Treatment Eﬀect (MTE)
                                         The Extended Roy Economy

                     A. E(Y |P (Z) = p)                                                     B. ∆MTE (uD )
 2                                                                   5
                                                                                                                 Cases I and II
1.8                                                                  4                                           Case III

1.6                                                                  3

1.4                                                                  2

1.2                                                                  1

 1                                                                   0

0.8                                                                 −1

0.6                                                                 −2

0.4                                                                 −3

0.2                                           Cases I and II        −4
                                              Case III
 0                                                                  −5
      0        0.2       0.4         0.6          0.8           1        0         0.2        0.4          0.6         0.8        1
                                p                                                                   p=u
                                                                                                       D




                                            Outcomes                             Choice Model
                                                                                   
                                                                                         1 if D∗ > 0
                                     Y1 = α + β̄ + U1                         D=
                                                                                         0 if D∗ ≤ 0
                                           Y0 = α + U0

                          Case I                               Case II                          Case III

                       U1 = U0                         U1 − U0 ⊥⊥ D                           U1 − U0 ⊥
                                                                                                      
                                                                                                      ⊥D
                β̄ =ATE=TT=TUT=IV                 β̄ =ATE=TT=TUT=IV                      β̄ =ATE=TT=TUT=IV



                                                        Parameterization


               Cases I, II and III           Cases II and III                               Case III

                     α = 0.67            (U1 , U0 )∼ N (0, Σ)                      D∗ = Y1 − Y0 − γZ
                                                     1    −0.9
                     β̄ = 0.2          with Σ =                                       Z ∼ N (µZ , ΣZ )
                                                    −0.9    1                                                     
                                                                                                       9         −2
                                                                             µZ = (2, −2) and ΣZ =
                                                                                                      −2         9
                                                                                       γ = (0.5, 0.5)
                                           Figure 6. The Local Average Treatment Eﬀect
                                                    The Extended Roy Economy

          A. E(Y |P (Z) = p) and ∆LATE (p , p+1 )                                         B. ∆MTE (uD ) and ∆LATE (p , p+1 )
  2
                                                                                  5
1.8
                                                                                  4
1.6                                                                                         A
                                                                                  3
1.4
                                                                                  2
1.2                                                                                                      B          − Area(E,F,G,H)/0.3=LATE(0.6,0.9)=−1.17
                                                                                  1
                                           LATE(0.6,0.9)= -1.17
 1           LATE(0.1,0.35)= 1.719                                                                                            E                  F
                                                                                  0
0.8                                                                                          C           D
                                                                                                                              G
                                                                                 −1

0.6
                                                                                 −2 Area(A,B,C,D)/0.25=LATE(0.1,0.35)=1.719

0.4                                                                                                                                              H
                                                                                 −3

0.2                                                                              −4

 0                                                                               −5
      0         0.2          0.4          0.6         0.8           1                 0           0.2         0.4            0.6          0.8            1
                                                                                                                     u =p
                                     p                                                                                D




                                                                                                             p
                                                                                                              +1

                                                                                                                    ∆MTE (uD )duD
                                           E (Y |P (Z) = p+1 ) − E (Y |P (Z) = p )                         p
                      ∆LATE (p , p+1 ) =                                           =
                                                          p+1 − p                                                  p+1 − p


                                                            ∆LATE (0.6, 0.9)      = −1.17
                                                            LATE
                                                        ∆          (0.1, 0.35)    =         1.719



                                                 Outcomes                                        Choice Model
                                                                                                  
                                                                                                   1 if D∗ > 0
                                            Y1 = α + β̄ + U1                               D=
                                                                                                   0 if D∗ ≤ 0
                                                Y0 = α + U0                               with D∗ = Y1 − Y0 − γZ

                                                                  Parameterization

                                                (U1 , U0 ) ∼ N (0, Σ) and Z ∼ N (µZ , ΣZ )
                                                                                           
                                             1          −0.9                             9 −2
                                         Σ=                    , µZ = (2, −2) and ΣZ =
                                            −0.9         1                              −2 9

                                                      α = 0.67, β̄ = 0.2, γ = (0.5, 0.5)
                       Figure 7. Treatment Parameters and OLS/Matching as a function of P (Z) = p


                                  5

                                  4            T T (p)

                                  3

                                  2                        M T E (p)
                                  1       ATE(p)

                                  0                                                     Matching(p)

                                 -1
                                                                  T U T (p)
                                 -2

                                 -3

                                 -4

                                 -5

                                      0            0.2             0.4            0.6      0.8          1
                                                                              p

         Parameter                                            Deﬁnition                                          Under Assumptions (*)

Marginal Treatment Eﬀect                       E [Y1 − Y0 |D∗ = 0, P (Z) = p]                                     β̄ + σU1 −U0 Φ−1 (1 − p)

 Average Treatment Eﬀect                                 E [Y1 − Y0 |P (Z) = p]                                                 β

                                                                                                                                 φ(Φ−1 (1−p))
 Treatment on the Treated                      E [Y1 − Y0 |D∗ > 0, P (Z) = p]                                     β̄ + σU1 −U0        p
                                                                                                                                 φ(Φ−1 (1−p))
Treatment on the Untreated                     E [Y1 − Y0 |D∗ ≤ 0, P (Z) = p]                                     β̄ −   σU1 −U0
                                                                                                                                  1−p
                                                                                                             2
                                                                                                            σU   −σU1 ,U0       1−2p
                                                                                                                                                     
  OLS/Matching on P (Z)           E [Y1 |D∗ > 0, P (Z) = p] − E [Y0 |D∗ ≤ 0, P (Z) = p]          β̄ +        √ 1
                                                                                                                 σU1 −U0       p(1−p)   φ Φ−1 (1 − p)


Note: Φ (·) and φ (·) represent the cdf and pdf of a standard normal distribution, respectively. Φ−1 (·) represents the inverse of Φ (·) .

                        (*): The model in this case is the same as the one presented below Figure 6.
             Figure 8. Marginal Treatment Eﬀect and IV Weights using Z1 as the Instrument when
                      Z = (Z1 , Z2 ) ∼ p1 N (κ1 , Σ1 ) + p2 N (κ2 , Σ2 ) for diﬀerent values of Σ2

                     A. IV Weights                                                                        B. ∆MTE (v)

   1                                                                             4
                                                           ω1
 0.8
                                                           ω2                    3
 0.6                                                       ω3
                                                                                 2
 0.4

 0.2                                                                             1


   0                                                                             0

-0.2
                                                                               -1
-0.4
                                                                               -2
-0.6

-0.8                                                                           -3


-1.0                                                                           -4
   -4   -3      -2    -1       0       1       2      3         4               -4      -3      -2       -1    0        1   2   3   4
                               v                                                                               v



                              Outcomes                                           Choice Model
                                                                                    
                                                                              1 if D∗ > 0
                           Y1 = α + β̄ + U1                                 D=
                                                                              0 if D∗ ≤ 0
                                                                 ∗
                             Y0 = α + U0                        D = Y1 − Y0 − γZ and V = − (U1 − U0 )

                                                     Parameterization
                                                                              
                                                         1                −0.9
                           (U1 , U0 ) ∼ N (0, Σ) ,   Σ=                          , α = 0.67, β̄ = 0.2
                                                        −0.9               1


                                       Z = (Z1 , Z2 ) ∼ p1 N (κ1 , Σ1 ) + p2 N (κ2 , Σ2 )
                                                                                         
                                                                                1.4 0.5
                                      p1 = 0.45, p2 = 0.55         ; Σ1 =
                                                                                0.5 1.4

                                   Cov(Z1 , γZ) = γΣ11 = 0.98         ;       γ = (0.2, 1.4)

                    Table 3. IV estimator and        Cov(Z2 , γZ) associated with each value of Σ2
 Weights           Σ2               κ1                κ2        IV      ATE     TT      TUT      Cov(Z2 , γZ) = γΣ12
                          
                0.6 −0.5                               
 ω1                                0 0               0 0       0.434     0.2   1.401 −1.175           −0.58
              −0.5    0.6
                        
               0.6 0.1                                  
 ω2                                0 0               0 0            0.078       0.2     1.378        −1.145         0.26
              0.1  0.6    
                0.6 −0.3                                
 ω3                               0 −1               0 1            −2.261      0.2     1.310        −0.859        −0.30
               −0.3 0.6
                        Figure 9. IV Weights - The Eﬀect of Graduating from High School
                           Sample of High School Dropouts and High School Graduates
                                             White Males - NLSY79

        A. Weights: Number of Siblings as Instrument                                      B. Weights: Propensity Score as Instrument
       0.6                                                                        0.5


       0.5

                                                                                   0.4
       0.4


       0.3
                                                                                  0.3

       0.2

                                                                                  0.2
       0.1


        0
                                                                                  0.1

      −0.1


      −0.2
                                                                                    0

             λ1   λ2   λ3   λ4    λ5       λ6      λ7      λ8     λ9                             λ1       λ2      λ3      λ4   λ5   λ6   λ7   λ8   λ9

                                           C. The Local Average Treatment Eﬀects
                                    10                                        LATE
                                                                            ∆ 0.86,0.89                            LATE
                                                                                                 LATE             ∆0.94,1.0
                                       5     LATE            LATE                               ∆ 0.90,0.93
                                            ∆ 0.54,0.61     ∆0.72,0.85
                                       0
                                                     LATE                                                 LATE
                                                    ∆ 0.61,0.72                                          ∆0.93,0.94
                                    -5

                                   -10

                                   -15

                                   -20

                                   -25


                                   -30

                                                                                          LATE
                                   -35
                                                                      LATE              ∆ 0.89,0.90
                                                                    ∆ 0.85,0.86
                                   -40




  Y = Log per-hour wage at age 30, Z1 = Number of Siblings in 1979, Z2 = Mother is a High School Graduate
                                       
                                         1 if High School Graduate
                                   D=
                                         0 if High School Dropout


                                                      IV Estimates
                                 (boostrap std. errors in parenthesis - 100 replications)
                                         Instrument                       Value
                                         Number of Siblings in 1979       0.115
                                                                         (0.695)
                                         Propensity Score                 0.316
                                                                         (0.110)

                        Joint Probability Distribution of (Z1 , Z2 ) and the Propensity Score
(joint probabilities Pr(Z1 = z1 , Z2 = z2 ) in ordinary type; propensity score Pr (D = 1|Z1 = z1 , Z2 = z2 ) in italics)
                                    Z2 \Z1       0      1        2       3      4
                                       0       0.07   0.03     0.47 0.121 0.06
                                                1.0   0.54    0.86     0.72   0.61
                                       1      0.039 0.139 0.165 0.266 0.121
                                               0.94   0.89    0.90     0.85   0.93
                             Cov(Z1 , Z2 ) = −0.066 - Number of Observations = 1, 702
                                                  Figure 10. Treatment Parameters and IV
                                    The Generalized Ordered Choice Roy Model under Normality: Case I
                                A. Z as Instrument                                     B. W1 as Instrument

           0.8                                                                    0.8
                                                            ∆MTE    v                                                             ∆MTE
                                                                                                                                   1,2 ( )
                                                                                                                                          v
           0.7                                               1,2 ( )              0.7
                                                             MTE
                                                            ∆2,3 (v)                                                              ∆MTE
                                                                                                                                   2,3 ) (v
           0.6                                              ω(1, v )              0.6                                             ω(1, v )
                                                            ω(2, v )                                                              ω(2, v )
           0.5                                                                    0.5

           0.4                                                                    0.4

           0.3                                                                    0.3

           0.2                                                                    0.2

           0.1                                                                    0.1

            0                                                                       0

        -0. 1                                                                    -0. 1

        -0. 2                                                                    -0. 2
            4     3         2      1       0      1    2     3          4            4   3      2     1      0      1     2         3         4
                                           v                                                                 v

                                            Outcomes                                                Choice Model
                                       Y1 = α + β1 + U1                                      Ds = 1[Ws−1 < γZ − V  Ws ]
                                       Y2 = α + β2 + U2                                               s = 1, 2, 3
                                       Y3 = α + β3 + U3

                                                                    Parameterization

                          (U1 , U2 , U3 , V ) ∼ N (0, ΣU V ) , (Z, W1 , W2 ) ∼ N (µZW , ΣZW ) and W0 = −∞; W3 = ∞.
                      ⎡                                     ⎤
                       1           0.16         0.2  −0.3                                           ⎡                               ⎤
                   ⎢ 0.16                                                                             0.1              0        0
                                   0.64        0.16 −0.32 ⎥
           ΣU V   =⎢
                   ⎣ 0.2
                                                            ⎥ , µZW = (−0.6, −1.08, 0.08) and ΣZW = ⎣ 0               0.1     −0.09 ⎦
                                   0.16          1   −0.4 ⎦
                                                                                                       0             −0.09    0.25
                     −0.3          −0.32       −0.4    1
                                                Cov(U2 − U1 , V ) = −0.02 Cov(U3 − U2 , V ) = −0.08
                                                        β1 = 0; β2 = 0.025; β3 = 0.3; γ = 1

                                                       IV Estimates and Their Components∗
                                                              Parameter     Value
                                                              ∆IVZ        0.1489
                                                                ∆IV12
                                                                      Z
                                                                           0 .0117
                                                                   IVZ
                                                                ∆23        0 .1372
                                                              ∆IVW1       0.0017
                                                                  IV
                                                                ∆12 W1     0 .0325
                                                                  IV
                                                                ∆23 W1    −0 .0308

                                                      Treatment Parameters and Their Values
                                                               Parameter            Value
                                                           ATE12 = E (Y2 − Y1 )      0.025
                                                           ATE23 = E (Y3 − Y2 )      0.275
                                                        TT12 = E (Y2 − Y1 |D2 = 1)  0.0271
                                                        TT23 = E (Y3 − Y2 |D3 = 1)  0.1871
                                                       TUT12 = E (Y2 − Y1 |D1 = 1) 0.0047
                                                       TUT23 = E (Y3 − Y2 |D2 = 1) 0.2854
  ∗ ∆IVZ    is decomposed as:
                            Z                                          Z
                                                                                                                   IV      IV
                    ∆IVZ =    E (Y2 − Y1 |V = v) ω Z (1, v) fV (v) dv + E (Y3 − Y2 |V = v) ω Z (2, v) fV (v) dv = ∆12 Z + ∆23 Z

An analogous decomposition applies to ∆IVW1 .
                                                 Figure 11. Treatment Parameters and IV
                                   The Generalized Ordered Choice Roy Model under Normality: Case II
                               A. Z as Instrument                                    B. W1 as Instrument

            6                                                                       1.5                                     ∆MTE
                                                                                                                             1,2 ( )
                                                                                                                                    L
                                                            ∆MTE
                                                             1,2 ( )
                                                                    L
                                                                                                                            ∆MTE
                                                                                                                             2,3 ) (L
                                                            ∆MTE
                                                             2,3 ) (L
            4                                                                         1                                     ω(1, L )
                                                            ω(1, L )                                                        ω(2, L )
                                                            ω(2, L )
            2                                                                       0.5



            0                                                                         0


                                                                                   -0. 5
           -2


                                                                                     -1
           -4


                                                                                   -1. 5
           -6

            4        3     2      1     0      1     2        3         4             4    3    2    1      0   1     2      3          4
                                        L                                                                   L

                                      Outcomes                                                        Choice Model
                                 Y1 = α + β1 + U1                                              Ds = 1[Ws−1 < γZ − V  Ws ]
                                 Y2 = α + β2 + U2                                                       s = 1, 2, 3
                                 Y3 = α + β3 + U3

                                                                        Parameterization

                     (U1 , U2 , U3 , V ) ∼ N (0, ΣU V ) ,     (Z, W1 , W2 ) ∼ N (µZW , ΣZW ) and         W0 = −∞; W3 = ∞.
                 ⎡                                   ⎤
                     1           0.16   0.2    −0.3                                            ⎡                                     ⎤
                 ⎢ 0.16                                                                           0.1               0.092     −0.036
                                 0.64   0.16 −0.32 ⎥
    ΣU V        =⎢
                 ⎣ 0.2
                                                     ⎥ , µZW = (−0.6, −1.08, 0.08) and ΣZW = ⎣ 0.092                 0.1      −0.09 ⎦
                                 0.16     1    −0.4 ⎦
                                                                                                 −0.036             −0.09      0.25
                   −0.3         −0.32   −0.4    1
                                            Cov(U2 − U1 , V ) = −0.02 Cov(U3 − U2 , V ) = −0.08
                                                    β1 = 0; β2 = 0.025; β3 = 0.3; γ = 1


                                                     IV Estimates and Their Components†
                                                             Parameter    Value
                                                             ∆IVZ       −1.8091
                                                               ∆IV
                                                                 12
                                                                    Z
                                                                         0.2866
                                                                 IVZ
                                                               ∆23      -2.0957
                                                             ∆IVW1      −0.4284
                                                                IVW1
                                                              ∆12        0.0909
                                                                IV
                                                              ∆23 W1    -0.5193

                                                   Treatment Parameters and Their Values
                                                            Parameter            Value
                                                        ATE12 = E (Y2 − Y1 )      0.025
                                                        ATE23 = E (Y3 − Y2 )      0.275
                                                     TT12 = E (Y2 − Y1 |D2 = 1)  0.0283
                                                     TT23 = E (Y3 − Y2 |D3 = 1)  0.1754
                                                    TUT12 = E (Y2 − Y1 |D1 = 1) 0.0025
                                                    TUT23 = E (Y3 − Y2 |D2 = 1) 0.2898




† See   the footnote below Figure 10 for details of the decomposition of ∆IVZ . An analogous decomposition is used for ∆IVW1 .
