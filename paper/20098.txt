                                 NBER WORKING PAPER SERIES




       COMMUNICATING UNCERTAINTY IN OFFICIAL ECONOMIC STATISTICS

                                           Charles F. Manski

                                         Working Paper 20098
                                 http://www.nber.org/papers/w20098


                       NATIONAL BUREAU OF ECONOMIC RESEARCH
                                1050 Massachusetts Avenue
                                  Cambridge, MA 02138
                                       May 2014




This research was supported in part by National Science Foundation grant SES-1129475. I am grateful
to Robert Barbera, Bruce Spencer, Misa Tanaka, David Wessel, and Jonathan Wright for valuable
comments and discussions. I have benefited from the opportunity to present this work at the April
2014 Bank of England Interdisciplinary Workshop on the Role of Uncertainty in Central Bank Policy.
The views expressed herein are those of the author and do not necessarily reflect the views of the National
Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2014 by Charles F. Manski. All rights reserved. Short sections of text, not to exceed two paragraphs,
may be quoted without explicit permission provided that full credit, including © notice, is given to
the source.
Communicating Uncertainty in Official Economic Statistics
Charles F. Manski
NBER Working Paper No. 20098
May 2014
JEL No. C82,E01,I32

                                           ABSTRACT

Federal statistical agencies in the United States and analogous agencies elsewhere commonly report
official economic statistics as point estimates, without accompanying measures of error. Users of
the statistics may incorrectly view them as error-free or may incorrectly conjecture error magnitudes.
This paper discusses strategies to mitigate misinterpretation of official statistics by communicating
uncertainty to the public. Sampling error can be measured using established statistical principles.
The challenge is to satisfactorily measure the various forms of non-sampling error. I find it useful
to distinguish transitory statistical uncertainty, permanent statistical uncertainty, and conceptual
uncertainty. I illustrate how each arises as the Bureau of Economic Analysis periodically revises GDP
estimates, the Census Bureau generates household income statistics from surveys with non-response,
and the Bureau of Labor Statistics seasonally adjusts employment statistics.


Charles F. Manski
Department of Economics
Northwestern University
2001 Sheridan Road
Evanston, IL 60208
and NBER
cfmanski@northwestern.edu
1. Introduction



        Government statistical agencies commonly report official economic statistics as point estimates,

without accompanying measures of error. Agency publications documenting the data and methods used to

produce official statistics may acknowledge verbally that the estimates are subject to sampling and

nonsampling error, but the publications typically do not quantify error magnitudes. News releases that

communicate official statistics to the public present the estimates with little if any mention of the possibility

of error. Some prominent American examples include the reporting of GDP, household income, and

employment statistics by the Bureau of Economic Analysis (BEA), the Census Bureau, and the Bureau of

Labor Statistics (BLS).

        Reporting official statistics as point estimates manifests a common tendency of policy analysts to

project incredible certitude, encouraging policy makers and the public to believe that errors are small and

inconsequential (Manski, 2011, 2013a). In the absence of agency guidance, persons who understand that

official statistics are subject to error must fend for themselves and conjecture the error magnitudes. Thus,

users of official statistics may misinterpret the information that the statistics provide.

        Statistical agencies could mitigate misinterpretation of official statistics if they were to measure

uncertainty and report it in their news releases and technical publications. Why is it important to

communicate uncertainty in official statistics? A broad reason is that governments, firms, and individuals

use the statistics when making numerous important decisions. The quality of decisions may suffer if decision

makers incorrectly take reported statistics at face value or incorrectly conjecture error magnitudes. For

example, a central bank monitoring official statistics on GDP growth, inflation, and unemployment may mis-

evaluate the status of the economy and consequently set inappropriate monetary policy.                  Agency

communication of uncertainty would enable decision makers to better understand the information actually

available regarding key economic variables.

        Using established statistical principles, agencies could report sampling error for levels and temporal
                                                       2

changes in important statistics based on survey data. For example, the BLS could report confidence intervals

for key employment statistics, such as the level and month-to-month changes in the unemployment rate. The

Census Bureau could do likewise for key income statistics, such as the poverty rate and median household

income.

          It is more challenging for agencies to report nonsampling errors for official statistics. There are

many sources of such errors and there has been no consensus about how to measure them. Yet these facts

do not justify ignoring nonsampling error. Having agency analytical staffs make good-faith efforts to

measure nonsampling error would be more informative than having agencies report official statistics as if

they are truths.

          Beyond the traditional distinction between sampling and nonsampling error, I find it useful to

distinguish transitory statistical uncertainty, permanent statistical uncertainty, and conceptual uncertainty.

Transitory statistical uncertainty arises because data collection takes time. Agencies sometimes release a

preliminary estimate of an official statistic in an early stage of data collection and revise the estimate as new

data arrives. Hence, uncertainty may be substantial early on but diminish as data accumulates.

          Permanent statistical uncertainty arises from incompleteness or inadequacy of data collection that

does not diminish with time. Sampling uncertainty stemming from the finite size of survey samples is

permanent. So is uncertainty stemming from nonresponse and from the possibility that some respondents

may provide inaccurate data.

          Conceptual uncertainty arises from incomplete understanding of the information that official

statistics provide about well-defined economic concepts or from lack of clarity in the concepts themselves.

Thus, conceptual uncertainty concerns the interpretation of statistics rather than their magnitudes.

          This paper discusses some prominent examples of these forms of uncertainty and considers how

agencies might constructively communicate the uncertainty. To illustrate transitory statistical uncertainty,

Section 2 describes BEA initial measurement of GDP and the ensuing process of revising the estimate as new
                                                      3

data arrives. Section 3 discusses permanent statistical uncertainty due to nonresponse in sample surveys,

using nonresponse to employment and income questions in the Current Population Survey to illustrate.

Section 4 discusses the conceptual uncertainty inherent in seasonal adjustment of economic statistics.

Section 5 concludes.




2. Transitory Uncertainty: Revisions in National Income Accounts



2.1. Bureau of Economic Analysis Reporting of GDP



        The Bureau of Economic Analysis of the U.S. Department of Commerce reports quarterly estimates

of GDP. The BEA initially reports an advance estimate based on incomplete data available one month after

the end of a quarter. It reports second and third estimates after two and three months, when additional data

become available. In the summer of each year, when more extensive data collected on an annual rather than

quarterly basis become available, BEA reports a first annual estimate and then revises it further in

subsequent years. Every five years, the BEA re-evaluates its operational definition of GDP and accordingly

makes yet further revisions to the historical GDP record. I will not discuss the five-year revisions here

because they generate conceptual rather than statistical uncertainty.

        An article describing the measurement of GDP explains the reasons for revisions as follows

(Landefeld, Seskin, and Fraumeni, 2008, p. 194):

        "For the initial monthly estimates of quarterly GDP, data on about 25 percent of GDP—especially

        in the service sector—are not available, and so these sectors of the economy are estimated based on

        past trends and whatever related data are available. . . . . The initial monthly estimates of quarterly

        GDP based on these extrapolations are revised as more complete data become available. . . . . The
                                                      4

        successive revisions can be significant, but the initial estimates provide a snapshot of economic

        activity much like the first few seconds of a Polaroid photograph in which an image is fuzzy, but as

        the developing process continues, the details become clearer."

Although this passage recognizes that initial quarterly estimates of GDP growth are subject to error, BEA

practice has been to report these estimates without providing quantitative measures of uncertainty.

        For example, a November 29, 2012 news release compared advance and second estimates, stating

(Bureau of Economic Analysis, 2012):

        "Real gross domestic product . . . . increased at an annual rate of 2.7 percent in the third quarter of

        2012. . . . . The GDP estimate released today is based on more complete source data than were

        available for the ‘advance’ estimate issued last month. In the advance estimate, the increase in real

        GDP was 2.0 percent."

Thus, the second quarterly estimate of annual GDP growth differed from the advance estimate by 0.7

percentage points. While the news release observed that the advance estimate was based on incomplete data,

it did not acknowledge that the second estimate was likewise based on incomplete data and would be further

revised a month later and then annually for several years.

        How large do the revisions to the BEA estimates tend to be? The 0.7 percentage point revision

quoted above is close to the mean absolute revision (MAR) from month to month. Fixler, Greenaway-

McGrevy, and Grimm (2011) report that the MAR from the advance estimates to the second estimates of real

GDP is 0.5 percentage points, that from the advance estimates to the third estimates is 0.6 percentage points,

and that from the second to the third estimates is 0.3 percentage points. Considering the period 1983–2009,

they report that the overall MARs to the advance, second, and third quarterly estimates (comparing these

estimates with the latest available for the relevant quarter) were 1.31, 1.29, and 1.32 percentage points.

Observing that the magnitude of the revisions tends not to diminish with time, despite the availability of more

data when forming the second and third estimates, the authors state (p. 12): "The lack of declines in the
                                                     5

MARs of GDP in successive vintages of current quarterly estimates is a phenomenon that has been noted in

nearly all of BEA’s analyses of revisions."



2.2. The Substantive Significance of Revisions



        While the magnitudes of revisions to BEA estimates of GDP are straightforward to compute, the

substantive significance of the revisions is a matter of interpretation. Fixler, Greenaway-McGrevy, and

Grimm (2011) remark at their beginning of their article that (p. 9): "Economic policy decisions should not

need to be reconsidered in the light of revisions to GDP estimates, and policymakers should be able to rely

on the early estimates as accurate indicators of the state of the economy." They provide an upbeat absolute

perspective in the conclusion to their article, stating (p. 30): "The estimates of GDP and GDI are accurate;

the MARs for both measures are modestly above 1.0 percentage point." Landefeld, Seskin, and Fraumeni

(2008) provide an upbeat comparative perspective, stating (p. 213) : In terms of international comparisons,

the U.S. national accounts meet or exceed internationally accepted levels of accuracy and comparability. The

U.S. real GDP estimates appear to be at least as accurate—based on a comparison of GDP revisions across

countries—as the corresponding estimates from other major developed countries."

        Croushore (2011) offers a considerably more cautionary perspective, stating (p. 73): "Until recently,

macroeconomists assumed that data revisions were small and random and thus had no effect on structural

modeling, policy analysis, or forecasting. But realtime research has shown that this assumption is false and

that data revisions matter in many unexpected ways." To illustrate, he gives a notable example (p. 73):

        "In January 2009, in the middle of the financial crisis that began in September 2008, the initial

        release of the national income accounts showed a decline in real gross domestic product (GDP) of

        3.8 percent (at an annual rate) for the fourth quarter—a bad number for sure but not as bad as might

        be expected considering the damage caused by the financial meltdown. But one month later, the
                                                      6

        GDP growth rate was revised down by 2.4 percentage points, showing a decline in real GDP of 6.2

        percent and confirming that the U.S. economy was in the middle of the worst recession in over

        twenty-five years. The 2.4 percentage point downward revision from the initial release to the first

        revised number was the largest revision ever recorded for quarterly real GDP. Real-time data

        analysis of the history of revisions of real GDP shows us that the largest revision came at a very

        inopportune moment."

        This example is an extreme case, but it provides a stark warning that BEA revisions to GDP may be

quite large and occur at times when vital policy decisions must be made. Leaving aside the singular event

of the financial crisis, I view the MARs reported by Fixler, Greenaway-McGrevy, and Grimm (2011) for the

period 1983–2009 as too large to warrant their upbeat conclusion that BEA quarterly estimates of GDP are

accurate. A statement made by Croushore (2011) seems more on the mark (p. 77): "If monetary policy

depends on short term growth rates, then clearly policy mistakes could be made if the central bank does not

account for data uncertainty."



2.3. Measurement of Transitory Uncertainty



        Informative communication of the transitory uncertainty of GDP estimates should be relatively easy

to accomplish. The historical record of BEA revisions has been made accessible for study in two "real-time"

data sets maintained by the Philadelphia and St. Louis Federal Reserve Banks. See Croushore (2011) for

details regarding these data sets and similar ones for other nations.

        Forward-looking measurement of transitory uncertainty in GDP estimates is straightforward if one

finds it credible to assume that the revision process is time-stationary. Then historical estimates of the

magnitudes of revisions can credibly be extrapolated to measure the uncertainty of future revisions. A

particularly simple extrapolation would be to suppose that the historical overall MAR of 1.3 percentage
                                                       7

points reported by Fixler, Greenaway-McGrevy, and Grimm (2011) will persist going forward. More

broadly, it may be credible to suppose that the empirical distribution of revisions will persist going forward.

        More refined measures of uncertainty can be developed by studying how the magnitude and direction

of historical revisions have varied with the state of the economy. Such refinements may be important to the

extent that the nature of revisions tends to vary over the business cycle (see Croushore, 2011, Sec. 2.2). If

there is reason to think that the historical pattern of variation of revisions with the state of the economy will

persist into the future, then it would be appropriate to measure the transitory uncertainty of future GDP

estimates in a manner that conditions on the state of the economy.

        A notable precedent for probabilistic communication of the transitory uncertainty in GDP estimates

is the periodic release of fan charts by the Bank of England, the Norges Bank in Norway, and some other

central banks. I describe the British practice below. In the absence of BEA reporting of uncertainty in its

GDP estimates, the Federal Reserve could usefully emulate this practice.



2.4. Reporting GDP Growth by the Bank of England



        In the United Kingdom, the Office for National Statistics (ONS) reports quarterly estimates of GDP

in a manner similar to the BEA, with no quantitative measurement of uncertainty despite the fact that the

estimates are revised regularly. For example, the April 2014 edition of the monthly Economic Review of the

ONS describes the most recent GDP revisions as follows (U. K. Office for National Statistics, 2014, p. 1):

“The Quarterly National Accounts left Gross Domestic Product (GDP) growth in the final quarter of 2013

unrevised at 0.7%, but reduced annual growth in 2013 to 1.7%, largely as a consequence of lower than

previously estimated household expenditure.”

        Unlike those in the United States, users of GDP estimates in the UK have ready access to a measure

of uncertainty in the fan charts reported by the Bank of England in its monthly Inflation Report. Figure 1
                                                      8

reproduces a fan chart for annual GDP growth in the February 2014 Inflation Report (Bank of England,

2014). The part of the plot showing growth from late 2013 on is a probabilistic forecast that expresses the

uncertainty of the Bank’s Monetary Policy Committee regarding future GDP growth. The part of the plot

showing growth in the period 2009 through mid 2013 is a probabilistic forecast that expresses uncertainty

regarding the revisions that ONS will henceforth make to its estimates of past GDP. The Bank explains as

follows (p. 7): “In the GDP fan chart, the distribution to the left of the vertical dashed line reflects the

likelihood of revisions to the data over the past.”

        Observe that Figure 1 expresses considerable uncertainty about GDP growth in the period

2010–2013. Moreover, it expresses comparable uncertainty about growth in the recent past and the near

future. Thus, the Bank judges that future ONS revisions to estimates of past GDP may be large in magnitude.



                               Figure 1: February 2014 UK GDP Fan Chart
                                       (Source: Bank of England)
                                                     9

3. Permanent Uncertainty: Nonresponse in Sample Surveys



3.1. Nonresponse to Income and Employment Questions in the Current Population Survey



        Nonresponse is common in the surveys used to compute important official statistics. Unit and item

nonresponse may make key data missing for substantial fractions of the persons sampled. Nevertheless, the

reporting of official statistics obscures the potential implications of nonresponse. Census reporting of

income statistics and BLS reporting of the unemployment rate provide apt examples.



Census Reporting of Income Statistics

        Each year the Census Bureau reports statistics on the household income distribution based on data

collected in the Annual Social and Economic (ASEC) Supplement to the Current Population Survey (CPS).

There is considerable nonresponse to the CPS income questions. During the period 2002-2012, 7 to 9 percent

of the sampled households yielded no income data due to unit nonresponse and 41 to 47 percent of the

interviewed households yielded incomplete income data due to item nonresponse. Nevertheless, Census

Bureau publications give the impression that national statistics on the income distribution are exact.

        For example, in a news release issued September 12, 2012, the Census Bureau declared (U. S.

Census Bureau, 2012A): "The nation’s official poverty rate in 2011 was 15.0 percent, with 46.2 million

people in poverty. After three consecutive years of increases, neither the poverty rate nor the number of

people in poverty were statistically different from the 2010 estimates." Thus, the Census release provided

point estimates, acknowledged but did not quantify sampling error, and did not mention the nonsampling

error that may arise from nonresponse or misreporting.

        I find it intriguing that the Census Bureau has not even provided quantitative measures of sampling

error. A Census Bureau publication gives this explanation for the decision of the Bureau not to report
                                                         10

standard errors for income statistics (U. S. Census Bureau, 2012B, p. 7):

        "While it is possible to compute and present an estimate of the standard error based on the survey

        data for each estimate in a report, there are a number of reasons why this is not done. A presentation

        of the individual standard errors would be of limited use, since one could not possibly predict all of

        the combinations of results that may be of interest to data users. Additionally, data users have access

        to CPS microdata files, and it is impossible to compute in advance the standard error for every

        estimate one might obtain from those data sets."

This reasoning explains why the Bureau cannot measure sampling error for every logically possible

application of the CPS data. It does not explain why the Bureau chooses not to report sampling error for the

income statistics that it highlights in news releases.

        Interestingly, a positive role model for quantitative measurement of sampling error in news releases

of official statistics can be found elsewhere in the Census Bureau. Each month the Census Bureau and the

Department of Housing and Urban Development jointly release statistics on new residential home sales in

the previous month. Expression of uncertainty is prominent in these releases. For example, the one for

March 2014 begins this way (U. S. Census Bureau, 2014):

        “Sales of new single-family houses in March 2014 were at a seasonally adjusted annual rate of

        384,000, according to estimates released jointly today by the U.S. Census Bureau and the

        Department of Housing and Urban Development. This is 14.5 percent (±12.9%) below the revised

        February rate of 449,000 and is 13.3 percent (±9.9%) below the March 2013 estimate of 443,000.”

The Explanatory Notes section of the release states “All ranges given for percent changes are 90-percent

confidence intervals and account only for sampling variability.” This transparent expression of sampling

uncertainty could easily be emulated when the Census Bureau reports income statistics.
                                                      11

BLS Reporting of Employment Statistics

        On the first Friday of each month, the BLS issues The Employment Situation, a monthly news release

reporting official employment statistics for the previous month. For example, the BLS reported this on

October 5, 2012 (U. S. Bureau of Labor Statistics, 2012): "The unemployment rate decreased to 7.8 percent

in September, and total nonfarm payroll employment rose by 114,000." The unemployment-rate statistic is

based on data on households sampled in the CPS. The one on nonfarm employment is based on data

collected from employer establishments sampled in the Current Employment Statistics survey (CES).

        The BLS monthly news release reports employment statistics as point estimates, without measures

of potential error. A Technical Note issued with the news release contains a section on Reliability of the

estimates that acknowledges the possible presence of errors, beginning with the statement "Statistics based

on the household and establishment surveys are subject to both sampling and nonsampling error." The

section describes the conventional use of standard errors and confidence intervals to measure sampling error,

providing a few numerical illustrations.

        The Technical Note then turns to nonsampling errors, stating that they "can occur for many reasons,

including the failure to sample a segment of the population, inability to obtain information for all respondents

in the sample, inability or unwillingness of respondents to provide correct information on a timely basis,

mistakes made by respondents, and errors made in the collection or processing of the data." The Note does

not indicate the magnitudes of the nonsampling errors that may be present in the employment statistics.



3.2. Nonresponse Imputations and Weights



        To deal with survey nonresponse, statistical agencies use traditional but untenable assumptions,

namely that nonresponse is random conditional on specified observed covariates. These assumptions are

implemented as weights for unit nonresponse and imputations for item nonresponse.
                                                    12

        In particular, the Census Bureau applies hot-deck imputations to the CPS and other surveys,

describing the hot deck this way (U. S. Census Bureau, 2006, p. 9-2):

        "This method assigns a missing value from a record with similar characteristics, which is the hot

        deck. Hot decks are defined by variables such as age, race, and sex. Other characteristics used in

        hot decks vary depending on the nature of the unanswered question. For instance, most labor force

        questions use age, race, sex, and occasionally another correlated labor force item such as full- or

        part-time status."

Thus the agency staff select a vector of covariates for which response is complete and compute the empirical

distribution of the outcome of interest among sample members who have this covariate value and who report

their outcomes. An outcome is imputed to a sample member with missing data by drawing a realization at

random from the available empirical distribution.

        The CPS documentation of hot-deck imputation offers no evidence that the method yields an

outcome distribution for missing data that is close to the actual distribution of such outcomes. However,

another Census Bureau document describing the American Housing Survey is revealing. The document

states (U. S. Census Bureau, 2011):

        "Some people refuse the interview or do not know the answers. When the entire interview is

        missing, other similar interviews represent the missing ones . . . . For most missing answers, an

        answer from a similar household is copied. The Census Bureau does not know how close the

        imputed values are to the actual values."

Indeed, lack of knowledge of the closeness of imputed values to actual ones is common.

        A potentially fruitful way to assess the closeness of imputed values to actual ones is to enrich the

available data by matching CPS respondents to administrative records that provide information on their

income and employment. For example, Hokayem, Bollinger, and Ziliak (2014) obtain Social Security

earnings records for CPS respondents. The authors caution that Social Security records exclude various
                                                       13

forms of income. Hence, they do not consider the records to provide the true values of missing data.

Nevertheless, their joint analysis of the available CPS and Social Security data yields a cautionary conclusion

(p. 5):

          "Our results suggest that assumption of missing at random, even conditional on known

          characteristics, is not valid in modern data. Hence any correction which assumes missing completely

          at random or missing at random, such as the hot deck procedure, is likely to be biased. We show that

          the ASEC underestimates the number of persons in poverty by an average of about 1.0 percentage

          point."



3.3. Measurement of Uncertainty due to Nonresponse



          Modern econometric research on partial identification has shown how to measure uncertainty due

to nonresponse without making any assumptions about the nature of the missing data. See Manski (1989,

1994, 2003) and Horowitz and Manski (1998, 2000) inter alia. A notable early precedent in the statistics

literature was the Cochran, Mosteller, and Tukey (1954) consideration of the potential implications of

nonresponse in the Kinsey survey of male sexual behavior. However, subsequent statistical research on

analysis of surveys with nonresponse did not follow up and instead recommended generation of point

estimates under assumptions of random nonresponse. See, for example, Little and Rubin (1987).

          The basic idea in partial identification analysis, simply enough, is to contemplate all the values that

the missing data might take. Doing so, the available data yield interval rather than point estimates of official

statistics. The literature derives the forms of these intervals for population means, quantiles, and other

parameters of possible interest. Econometricians have also shown how to form confidence intervals that

jointly measure sampling error and potential nonresponse error (e.g., Horowitz and Manski, 2000; Imbens

and Manski, 2004). Thus, we know how to measure uncertainty for official statistics with survey
                                                        14

nonresponse.

          To illustrate, Manski (2013b) used ASEC data collected in 2002-2012 to form interval estimates of

median household income and the fraction of families with income below the official poverty threshold in

the years 2001-2011. I used monthly CPS data to form interval estimates of the unemployment rate in March

of 2002-2012. I provided one set of estimates that take into account item nonresponse alone and another that

recognizes unit response as well. The estimates show vividly that item nonresponse poses a huge potential

problem for inference on the American income distribution, and that unit nonresponse exacerbates the

problem. While item nonresponse is a relatively minor source of error for the unemployment rate, unit

nonresponse is highly consequential.

          Interval estimates of official statistics that place no assumptions on the values of missing data are

credible, easy to understand, and simple to compute. One might therefore think that it would be standard

practice for government statistical agencies to report them. I am, however, unaware of any official statistics

reported this way.

          In personal communications, colleagues have sometimes remarked that interval estimates obtained

without assumptions on nonresponse are not reported because they are "too wide to be informative." Indeed,

the illustrative empirical analysis of Manski (2013b) shows that, with current nonresponse rates, the intervals

are quite wide. For example, the interval estimate for the family poverty rate in 2011 is [0.139, 0.339] if only

item nonresponse is acknowledged and is [0.128, 0.390] if both item and unit nonreponse are recognized.

The analogous interval estimates for the unemployment rate in March 2012 are [0.078, 0.090] and [0.071,

0.162].

          Nevertheless, I would argue that the Census Bureau and the BLS should report such intervals. Even

when wide, interval estimates obtained without assumptions on nonresponse are valuable for two reasons.

First, they are maximally credible in the sense that they express all possible values of the statistic of interest,

whatever values the missing data may take. Second, they make explicit the fundamental role that
                                                    15

assumptions play in inferential methods that yield tighter findings. Wide bounds reflect real data

uncertainties that cannot be washed away by assumptions lacking credibility.

        The above argument does not imply, of course, that statistical agencies should entirely refrain from

making assumptions about nonresponse. Interval estimates obtained with no assumptions may be excessively

conservative if agency analysts have some understanding of the nature of nonresponse. There is much middle

ground between interval estimation with no assumptions and the standard practice of point estimation

assuming that nonresponse is conditionally random. The middle ground obtains interval estimates based on

assumptions that may include random nonresponse as one among various possibilities. It is unlikely that any

one middle-ground assumption will be appropriate in all settings. Manski (2013b) poses some alternatives

that statistical agencies may want to consider.




4. Conceptual Uncertainty: Seasonal Adjustment of Official Statistics



        I wrote in the Introduction that conceptual uncertainty arises from incomplete understanding of the

information that official statistics provide about well-defined economic concepts or from lack of clarity in

the concepts themselves. Section 2 mentioned one example, this being that the BEA re-evaluates its

operational definition of GDP every five years and revises the historical GDP record accordingly.

        Another example arises in BLS measurement of unemployment.               The BLS classifies CPS

respondents as unemployed if they "do not have a job, have actively looked for work in the prior 4 weeks,

and are currently available for work" (www.bls.gov/cps/cps_htgm.htm#unemployed). Responses to a

sequence of CPS questions are used to determine whether a person has "actively looked for work in the prior

4 weeks" and is "currently available for work." The notions of actively looking for work and being currently

available for work are inherently vague to some degree, so there is resulting uncertainty about how
                                                     16

unemployment statistics should be interpreted.



4.1. Seasonal Adjustment in Principle and Practice



        A particularly troublesome conceptual uncertainty arises in the conventional practice of seasonally

adjusting official statistics, including quarterly GDP estimates and monthly unemployment rates. Viewed

from a sufficiently high altitude, the purpose of seasonal adjustment appears straightforward to explain.

However, it is much less clear from ground level how one should actually perform seasonal adjustment.

        The BLS explains seasonal adjustment of employment statistics this way (U. S. Bureau of Labor

Statistics (2001):

        "What is seasonal adjustment? Seasonal adjustment is a statistical technique that attempts to

        measure and remove the influences of predictable seasonal patterns to reveal how employment and

        unemployment change from month to month. Over the course of a year, the size of the labor force,

        the levels of employment and unemployment, and other measures of labor market activity undergo

        fluctuations due to seasonal events including changes in weather, harvests, major holidays, and

        school schedules. Because these seasonal events follow a more or less regular pattern each year,

        their influence on statistical trends can be eliminated by seasonally adjusting the statistics from

        month to month. These seasonal adjustments make it easier to observe the cyclical, underlying trend,

        and other nonseasonal movements in the series."

The explanation is heuristically appealing but it does not specify how, in practice, one may "remove the

influences of predictable seasonal patterns." Views on appropriate ways to perform seasonal adjustment

have long varied among econometricians. See, for example, the exchange between Granger (1979) and Sims

(1979) as well as the recent contribution of Wright (2013).

        In practice, the BLS uses the X-12-ARIMA method, developed by the Census Bureau and described
                                                      17

in Findley et al. (1998). The X-12 method, along with its predecessor X-11 and successor X-13, may be a

sophisticated and successful algorithm for seasonal adjustment. Or it may be an unfathomable black box

containing a complex set of statistical operations that lack economic foundation. Wright (2013) eloquently

expresses the difficulty of understanding X-12, writing (p. 2):

        "Most academics treat seasonal adjustment as a very mundane job, rumored to be undertaken by

        hobbits living in holes in the ground. I believe that this is a terrible mistake, but one in which the

        statistical agencies share at least a little of the blame. Statistical agencies emphasize SA data (and

        in some cases don't even publish NSA data), and while they generally document their seasonal

        adjustment process thoroughly, it is not always done in a way that facilitates replication, or

        encourages entry into this research area."

Wright's remark that statistical agencies sometimes do not publish non-seasonally-adjusted (NSA) data refers

particularly to a decision of the BEA to stop publication of NSA GDP data. He comments on this as follows

(p. 10): "It is very unfortunate that for the most basic measure of economic activity in the largest country in

the world, researchers are effectively prevented from evaluating any difficulties associated with seasonal

adjustment."

        Understanding the practice of seasonal adjustment matters because, as Wright states (p. 1): "Seasonal

adjustment is extraordinarily consequential." He gives this example concerning BLS reporting of estimates

of non-farm payrolls (p. 1): "In terms of monthly changes, the average absolute difference between the

seasonally adjusted (SA) and not-seasonally-adjusted (NSA) number is 660,000, which dwarfs the normal

month-over-month variation in the SA data. All this implies that we should think very carefully about how

seasonal adjustment is done."
                                                        18

4.2. Measurement of Uncertainty Associated with Seasonal Adjustment



        There presently exists no clearly appropriate way to measure the conceptual uncertainty associated

with seasonal adjustment. The Census Bureau's X-12 is an algorithm, not a method based on a well-specified

dynamic theory of the economy. Hence, it is not obvious how to evaluate the extent to which it accomplishes

the objective of removing the influences of predictable seasonal patterns. One might perhaps juxtapose X-

12 with other seemingly reasonable algorithms, perform seasonal adjustment with each one, and view the

range of resulting estimates as a measure of conceptual uncertainty.

        More principled ways to evaluate uncertainty may open up if statistical agencies were to use a

seasonal adjustment method derived from a well-specified model of the economy. One could then assess the

sensitivity of seasonally adjusted estimates to variation in the parameters and the basic structure of the model.

        A more radical departure from present practice would be to abandon seasonal adjustment and leave

it to the users of official statistics to interpret unadjusted statistics as they choose. Publication of unadjusted

statistics should be particularly valuable to users who want to make year-to-year rather than month-to-month

comparison of statistics. Suppose, for example, that one wants to compare unemployment in March 2013

and March 2014. It is arguably more reasonable to compare the unadjusted estimates for these months than

to compare the seasonally adjusted estimates. Comparison of unadjusted estimates for the same month each

year (March in this case) sensibly removes the "influences of predictable seasonal patterns" that the BLS

noted in its 2001 document. Moreover, it compares data actually collected in the two months of interest. In

contrast, the seasonally adjusted estimates for March 2013 and March 2014 are comprised of data collected

not only in these months but over a lengthy prior period.
                                                      19

5. Conclusion



        The National Research Council (NRC) publication Principles and Practices for a Federal Statistical

Agency recommends that agencies adhere to various good practices. Practice 4, titled "Openness About

Sources and Limitations of the Data Provided," states this (National Research Council, 2013, p. 18):

        "A statistical agency should be open about the strengths and limitations of its data, taking as much

        care to understand and explain how its statistics may fall short of accuracy as it does to produce

        accurate data. Data releases from a statistical program should be accompanied by a full description

        of the purpose of the program; the methods and assumptions used for data collection, processing,

        and reporting; what is known and not known about the quality and relevance of the data; sufficient

        information for estimating variability in the data; appropriate methods for analysis that take account

        of variability and other sources of error; and the results of research on the methods and data."

        As documented in this paper, federal statistical agencies typically pay only lip service to this practice

in their reporting of official economic statistics. The norm has been to acknowledge potential errors verbally

but not quantitatively. The news releases and technical documentation published by statistical agencies may

caution readers that point estimates of official statistics are subject to sampling and nonsampling errors but

agency publications typically do not measure the errors. They do not, in the words of the NRC, provide

"appropriate methods for analysis that take account of variability and other sources of error."

        Nor do agencies justify the ways that they use incomplete and imperfect data to produce the point

estimates they report. I have called attention to several prevalent agency practices that lack justification:

use of trend extrapolations to form advance GDP estimates, imputation of missing data in sample surveys,

and use of the X-12 algorithm to seasonally adjust statistics. In these and other respects, agencies commonly

do not make clear "what is known and not known about the quality and relevance of the data."

         This paper has suggested ways to measure the transitory statistical uncertainty in estimates of
                                                        20

official statistics based on incomplete data and the permanent statistical uncertainty stemming from survey

nonresponse. I have also called attention to the conceptual uncertainty in seasonal adjustment. Statistical

agencies would better inform policymakers and the public if they were to measure and communicate these

and other significant uncertainties in official statistics.

           An open question is how agency communication of uncertainty would affect policymaking and

private decision making. We now have little understanding of the ways that users of official statistics

interpret them. Some may mistakenly take the statistics at face value. Others may conjecture that the

statistics are prone to errors of varying directions and magnitudes. We know essentially nothing about how

decision making would change if statistical agencies were to communicate uncertainty regularly and

transparently. I urge behavioral and social scientists to initiate empirical studies that would shed light on this

subject.
                                                   21

References


Bank of England (2014), Inflation Report Fan Charts February 2014,
www.bankofengland.co.uk/publications/Documents/inflationreport/2014/ir14febfc.pdf, accessed April 26,
2014.

Bureau of Economic Analysis, U. S. Department of Commerce (2012), News Release: Gross Domestic
Product, November 29,
www.bea.gov/newsreleases/national/gdp/gdpnewsrelease.htm, accessed April 26, 2014.

Cochran, W., F. Mosteller, and J. Tukey (1954), Statistical Problems of the Kinsey Report on Sexual
Behavior in the Human Male, Washington, DC: American Statistical Association.

Croushore, D. (2011), "Frontiers of Real-Time Data Analysis," Journal of Economic Literature, 49, 72-100.

Findley, D., B. Monsell, W. Bell, M. Otto and B. Chen (1998), "New Capabilities and Methods of the X-12-
ARIMA Seasonal-Adjustment Program," Journal of Business & Economic Statistics, 16, 127-152.

Fixler, D., R. Greenaway-McGrevy, and B. Grimm (2011), "Revisions to GDP, GDI, and Their Major
Components," Survey of Current Business, 91 (7), 9-31.

Granger, C. (1979), "Seasonality: Causation, Interpretation, and Implications," in A. Zellner (editor),
Seasonal Analysis of Economic Time Series, National Bureau of Economic Research, 33-46.

Hokayem, C., C. Bollinger, and J. Ziliak, "The Role of CPS Nonresponse on the Level and Trend in Poverty,"
University of Kentucky Center for Poverty Research Discussion Paper Series, DP2014-05.

Horowitz, J. and C. Manski (1998), "Censoring of Outcomes and Regressors due to Survey Nonresponse:
Identification and Estimation Using Weights and Imputations," Journal of Econometrics, 84, 37–58.

Horowitz, J. and C. Manski (2000), "Nonparametric Analysis of Randomized Experiments with Missing
Covariate and Outcome Data," Journal of the American Statistical Association, 95, 77–84.

Imbens, G. and C. Manski (2004), "Confidence Intervals for Partially Identified Parameters," Econometrica,
72, 1845–1857.

Landefeld, J., E. Seskin, and B. Fraumeni (2008), "Taking the Pulse of the Economy: Measuring GDP,"
Journal of Economic Perspectives, 22, 193-216.

Little, R. and D. Rubin (1987), Statistical Analysis with Missing Data, New York: Wiley.

Manski, C. (1989), "Anatomy of the Selection Problem," Journal of Human Resources, 24, 343–360.

Manski, C. (1994), "The Selection Problem," in C. Sims (editor) Advances in Econometrics, Sixth World
Congress, Cambridge: Cambridge University Press.

Manski, C. (2003), Partial Identification of Probability Distributions, New York: Springer-Verlag.
                                                    22

Manski, C. (2011), "Policy Analysis with Incredible Certitude," The Economic Journal, 121, F261-F289.

Manski, C. (2013a), Public Policy in an Uncertain World, Cambridge, MA: Harvard University Press.

Manski, C. (2013b), "Credible Interval Estimates for Official Statistics with Survey Nonresponse,"
Department of Economics, Northwestern University.

National Research Council (2013), Principles and Practices for a Federal Statistical Agency: Fifth Edition,
Washington, DC: The National Academies Press.

Sims, C. (1979), "Comments on 'Seasonality: Causation, Interpretation, and Implications,' by Clive W. J.
Granger," in A. Zellner (editor), Seasonal Analysis of Economic Time Series, National Bureau of Economic
Research, 47-49.

U. K. Office for National Statistics (2013), Economic Review, April 2014,
www.ons.gov.uk/ons/dcp171766_358477.pdf, accessed April 26, 2014.

U. S. Bureau of Labor Statistics (2012), Employment Situation News Release, October 5,
www.bls.gov/news.release/archives/empsit_10052012.htm, accessed April 26, 2014.

U. S. Bureau of Labor Statistics (2001), Labor Force Statistics from the Current Population Survey,
www.bls.gov/cps/seasfaq.htm, accessed April 26, 2014.

U. S. Census Bureau (2006), Current Population Survey Design and Methodology, Technical Paper 66,
Washington, DC: U. S. Census Bureau.

U.S. Census Bureau (2011), Current Housing Reports, Series H150/09, American Housing Survey for the
United States: 2009, Washington, DC: U.S. Government Printing Office.

U. S. Census Bureau (2012A), Income, Poverty and Health Insurance Coverage in the United States: 2011,
September 12, www.census.gov/newsroom/releases/archives/income_wealth/cb12-172.html, accessed April
26, 2014.

U. S. Census Bureau (2012B), Source and Accuracy of Estimates for Income, Poverty, and Health Insurance
Coverage in the United States: 2011, Publication P60_243sa, www.census.gov/hhes/www/p60_243sa.pdf,
accessed April 26, 2014.

U. S. Census Bureau (2014), New Residential Sales in March 2014,
www.census.gov/construction/nrs/pdf/newressales.pdf, accessed April 26, 2014.

Wright, J. (2013), "Unseasonal Seasonals?" Department of Economics, Johns Hopkins University.
