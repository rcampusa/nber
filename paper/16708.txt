                                NBER WORKING PAPER SERIES




                  ONE-NODE QUADRATURE BEATS MONTE CARLO:
               A GENERALIZED STOCHASTIC SIMULATION ALGORITHM

                                           Kenneth Judd
                                            Lilia Maliar
                                           Serguei Maliar

                                        Working Paper 16708
                                http://www.nber.org/papers/w16708


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                     January 2011




Lilia Maliar and Serguei Maliar acknowledge support from the Hoover Institution at Stanford University,
the Ivie, the Ministerio de Ciencia e Innovación and FEDER funds under the project SEJ-2007-62656.
The views expressed herein are those of the authors and do not necessarily reflect the views of the
National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

© 2011 by Kenneth Judd, Lilia Maliar, and Serguei Maliar. All rights reserved. Short sections of text,
not to exceed two paragraphs, may be quoted without explicit permission provided that full credit,
including © notice, is given to the source.
One-node Quadrature Beats Monte Carlo: A Generalized Stochastic Simulation Algorithm
Kenneth Judd, Lilia Maliar, and Serguei Maliar
NBER Working Paper No. 16708
January 2011
JEL No. C63

                                           ABSTRACT

In conventional stochastic simulation algorithms, Monte Carlo integration and curve fitting are merged
together and implemented by means of regression. We perform a decomposition of the solution error
and show that regression does a good job in curve fitting but a poor job in integration, which leads
to low accuracy of solutions. We propose a generalized notion of stochastic simulation approach in
which integration and curve fitting are separated. We specifically allow for the use of deterministic
(quadrature and monomial) integration methods which are more accurate than the conventional Monte
Carlo method. We achieve accuracy of solutions that is orders of magnitude higher than that of the
conventional stochastic simulation algorithms.


Kenneth Judd                                      Serguei Maliar
Hoover Institution                                Department of Economics
Stanford University                               University of Alicante
Stanford, CA 94305-6010                           Campus San Vicente del Raspeig
and NBER                                          Ap. Correos 99, 03080 Alicante, Spain
kennethjudd@mac.com                               maliars@merlin.fae.ua.es

Lilia Maliar
Department of Economics
University of Alicante
Campus San Vicente del Raspeig
Ap. Correos 99, 03080 Alicante, Spain
maliarl@merlin.fae.ua.es
1       Introduction
Numerical methods for solving dynamic economic models diﬀer substantially
in their accuracy and speed. For example, in the comparison studies by Den
Haan (2010), and Kollmann, Maliar, Malin and Pichler (2011) (henceforth,
KMMP), these diﬀerences amount to several orders of magnitude. To de-
velop eﬃcient solution methods, we must understand what accounts for the
diﬀerences across methods and identify computational techniques that lead
to best results. This is not easy to do because existing solution methods dif-
fer in many dimensions (solution domain, interpolation method, integration
method, iterative procedure, etc.) and contain steps that are not directly
comparable.
    In the present paper, we assess computational techniques that are part
of the basis of two broad classes of solution methods, the stochastic simu-
lation and projection.1 The first class approximates solutions on a set of
simulated points using Monte Carlo integration, while the second class ap-
proximates solutions on a fixed grid of points using deterministic numerical
integration. Furthermore, in stochastic simulation methods, the integration
and curve-fitting steps are merged into one by means of regression, whereas in
projection methods, these steps are performed separately. Finally, stochastic
simulation methods are simpler to implement and are relatively less expen-
sive in high-dimensional problems than projection methods, but they are also
less accurate; see Maliar, Maliar and Judd (2011) and KMMP (2011).
    We introduce a notion of generalized stochastic simulation algorithm
() that breaks down the fusion of integration and curve-fitting that
is present in the conventional stochastic simulation algorithm. We specifi-
cally allow for the use of deterministic (quadrature and monomial) integra-
tion methods which are characteristic for the projection class of algorithms.
 eﬀectively lies between a pure stochastic simulation and pure projec-
tion algorithms and includes both of these algorithms as limiting cases. By
separating the integration and curve-fitting steps, we are able to decompose
the total solution error into errors associated with integration, curve-fitting
and the choice of a solution domain.
    Within our generalized framework, we show that errors associated with
    1
    Examples of stochastic simulation methods are Den Haan and Marcet (1990), Smith
(1991), Maliar and Maliar (2005), Judd, Maliar and Maliar (2010b) (henceforth, JMM),
and examples of projection methods are Judd (1992), Christiano and Fisher (2000),
Krueger and Kubler (2004), and JMM (2010a).


                                         2
curve-fitting decrease rapidly with the degree of the approximating polyno-
mial function. Furthermore, errors associated with a stochastically gener-
ated solution domain decrease rapidly with the simulation length. Finally,
the integration errors also decrease with the simulation length but slowly.2
Under the conventional one-node Monte Carlo method, the integration er-
rors are considerably larger than other kinds of errors and restrict the overall
accuracy of solutions. After we substitute the conventional one-node Monte-
Carlo integration method with a one-node Gauss-Hermite quadrature inte-
gration method, the integration errors decrease dramatically. As a result,
 combines high accuracy of projection methods and low cost in high-
dimensional problems of stochastic simulation methods.3
    We first test the performance of  on the example of the standard
representative-agent growth model. We find that for a wide range of parame-
ters, replacing conventional one-node Monte Carlo integration with one-node
Gauss-Hermite quadrature integration reduces the solution errors by about
2 − 3 orders of magnitude, while replacing it with several-node quadrature
integration reduces such errors by up to 5 orders of magnitude. For example,
for Monte Carlo integration with with 1, 30 and 3000 nodes, the solution
errors (measured by the size of Euler equation errors on a stochastic simula-
tion) are at most 51 · 10−4 , 31 · 10−4 and 12 · 10−5 , respectively, while for
quadrature integration with 1, 2 and 10 nodes, the solution errors are at most
63 · 10−7 , 17 · 10−9 and 16 · 10−9 , respectively. Surprisingly, the quadrature
integration method with just one node leads to more accurate solutions than
a Monte Carlo method with thousands of nodes.
    We next study the performance of  in the context of a heterogeneous-
agent growth model, namely, we consider a multi-country model with up to
thirty heterogeneous countries. We find that for a second-degree polynomial,
the one-node Gauss-Hermite quadrature integration method produces errors
that are up to two orders of magnitude smaller than those produced by the
conventional one-node Monte Carlo integration method. For example, after
we substitute one-node Monte Carlo integration by one-node quadrature in-
   2
      Galant and Nychka (1987) analyze a relation between the choice of an approximating
polynomial function and the sample size in the context of semi-parametric maximum
likelihood estimation.
    3
      Stochastic simulation algorithms operate on the ergodic set realized in equilibrium.
This allows to avoid costs associated with finding a solution in areas of state space that
are never realized in equilibrium. The higher is the dimensionality of the problem, the
larger is the gain from focusing on the ergodic set; see JMM (2010b) for a discussion.


                                            3
tegration, the solution error in a thirty-country model goes down from 5·10−3
to 25 · 10−5 .
    We finally point out that the use of numerically stable approximation
methods is crucial for the successful performance of . The problem of
recovering policy functions from simulated data is often ill-conditioned, and
standard least-squares methods (such as ordinary least-squares and Gauss-
Newton methods) work only under low-degree polynomial approximations.
JMM (2009, 2010b) describe a variety of approximation methods that can
handle ill-conditioned problems in the context of stochastic simulation al-
gorithms. The numerically stable approximation methods include the least-
squares method based on SVD, Tikhonov regularization, least absolute devia-
tion methods and principal components method. These methods help restore
numerical stability under high-degree polynomial approximations and allow
us to achieve high accuracy of solutions.
    The rest of the paper is as follows: In Section 2, we describe the represen-
tative agent model (the heterogeneous agents model is outlined in Appendix
A). In Section 3, we present . In Section 4, we discuss the determinants
of accuracy of . In Section 5, we describe the numerical experiments
performed for both the representative- and heterogeneous-agent models. Fi-
nally, in Section 6, we conclude.


2     The model
We study the standard representative-agent neoclassical stochastic growth
model:
                                      X∞
                          max      0        ( )                  (1)
                          { +1 }=0∞
                                                 =0

                      s.t.  + +1 = (1 − )  +   ( )               (2)
                                                          ¡      ¢
                 ln +1 =  ln  + +1     +1 ∼ N 0  2               (3)
where initial condition (0  0 ) is given. Here,  is the operator of condi-
tional expectation;  ,  and  are, respectively, consumption, capital and
productivity level;  ∈ (0 1) is the discount factor;  ∈ (0 1] is the deprecia-
tion rate of capital;  ∈ (−1 1) is the autocorrelation coeﬃcient; and  ≥ 0
is the standard deviation. The utility and production functions,  and  , re-
spectively, are assumed to be strictly increasing, continuously diﬀerentiable
and concave.

                                             4
   An interior solution to problem (1) − (3) satisfies the following Euler
equation:
              0 ( ) =  {0 (+1 ) [1 −  + +1  0 (+1 )]}  (4)
where 0 and  0 are the first derivatives of the utility and production func-
tions, respectively. In this paper, we look for a solution to problem (1) − (3)
in the form of capital policy function, +1 =  (   ), satisfying conditions
(2) − (4). To approximate the capital policy function, we use the following
representation of Euler equation (4),

                               +1 =  [ (    +1 )]                           (5)

where  (    +1 ) is given by

                                        0 (+1 )
               (    +1 ) ≡               [1 −  + +1  0 (+1 )] +1       (6)
                                         0 ( )

with  and +1 being determined by conditions (2) and (3) and by the
capital policy function,  (   ). Condition (5) holds because 0 ( ) 6= 0
and because +1 is -measurable.4


3       Generalized stochastic simulation algorithm
We parameterize end-of-period capital, +1 , in the left side of the Euler
equation (5) with a flexible functional form,

                                   +1 = Ψ (   ; )                                (7)

where  is a vector of coeﬃcients. To find , the generalized stochastic
simulation algorithm () proceeds as follows:
    Choose a simulation length,  . Fix initial condition (0  0 ). Draw a
sequence of productivity shocks, { }=1 . Compute a sequence of produc-
tivity levels, { }=0 , using (3), and fix it for all simulations. For initial
iteration  = 1, fix a vector of coeﬃcients  (1) .
    4
     In a similar way, one can use Euler equation (4) to express other -measurable variables,
e.g., ln (+1 ),  and 0 ( ). JMM (2009) show that the choice of a policy function to
approximate can aﬀect the numerical stability of stochastic simulation methods.




                                                 5
                                                                  ³             ´
• Step 1. On iteration , use the assumed policy function Ψ    ;  ()
  in (7) to compute forward the capital path, {+1 }=0 , for the given
  sequence { }=0 . Calculate the consumption path, { }=0 , from
  budget constraint (2).

• Step 2. Approximate conditional expectation (integral)  [ (    +1 )]
  in (5) for  = 0   − 1 as a weighted sum of the integrand, , evalu-
  ated in  nodes:
                                    X
                                    
                              ≡          (    +1 )         (8)
                                    =1

  where  is defined in (6) and {+1 }=1 and { }=1 are
  integration nodes and weights, respectively; +1 is given by (7); 
  follows directly from (2); and +1 is determined by (2) conditional
  on future productivity shock +1 with +1 =  exp (+1 ) and
  +2 ≡ Ψ (Ψ (   ; )   exp (+1 ) ; ).

• Step 3. Find a vector of coeﬃcients     b that minimizes the distance
  between  and Ψ (   ; ) by running a regression.

• Step 4. Compute the vector of coeﬃcients to be used on the next-
  iteration  (+1) using fixed-point iteration, namely,
                                                         b
                               (+1) = (1 − )  () +                   (9)

  where  ∈ (0 1) is a damping parameter.

  Iterate on Steps 1-4 until convergence is achieved,
                             ¯                 ¯
                       1 X ¯¯ +1 − +1 ¯¯
                               ()      (+1)
                             ¯                 ¯  10−                 (10)
                       =1 ¯        ()
                                    +1       ¯
         n      o           n        o
            ()                (+1)
  where +1           and +1                 are the capital paths obtained
                 =1                      =1
  on iterations  and  + 1, respectively, and parameter   0 determines
  the convergence criterion. (Note that convergence of the capital path
  implies convergence of the polynomial coeﬃcients, ).


                                            6
    is similar to the stochastic simulation algorithm () in JMM
(2009, 2010b) except for the integration procedure in Step 2.  relies on
a specific type of Monte Carlo integration, whereas  can use any inte-
gration methods including those unrelated to the estimated density function.
Formula (8) represents two alternative integration methods, Monte Carlo and
Gauss-Hermite quadrature ones, however other methods can be used as well.

Monte Carlo integration method For each period  and each node ,
an -node Monte Carlo integration method,  (), draws  shocks for
the next period, {+1 }=1 , and computes (8) by assigning equal weights
to all nodes, i.e.  = 1 for all  and . We refer to  using the
-node Monte Carlo integration method as  − (). One-node
Monte Carlo integration,  (1), is the conventional integration procedure
used by stochastic simulation methods; see Den Haan and Marcet (1990) and
JMM (2009, 2010b). It approximates the conditional expectation in (5) by
the value of integrand (6) realized in period  + 1, i.e.,  = 1, +11 = +1
and 1 = 1 (this implies that +11 =  exp (+1 )).

Gauss-Hermite quadrature integration method An -node Gauss-
Hermite quadrature integration method,  (), evaluates (8) using nodes
and weights that are constructed by Gauss-Hermite quadrature integration.
For example, two-node Gauss-Hermite quadrature,  (2), uses +11 = −,
+12 =  and 1 = 2 = 12 , and three-node Gauss-Hermite quadrature,
                                 q              q               √
 (3), uses +11 = 0, +12 = 32 , +13 = − 32 and 1 = 2 3  , 2 =
       √
3 = 6 for all ; see Judd (1998 p.261). We refer to  using -node
Gauss-Hermite quadrature integration as −(). A one-node Gauss-
Hermite quadrature integration method,  (1), approximates the conditional
expectation in (5) by the value of integrand (6) under the assumption of zero
shock in period  + 1, i.e.,  = 1, +11 = 0 and 1 = 1 (this implies that
+11 =  ).


4     Determinants of accuracy
We now explore factors that determine the accuracy of . First, since
the integration method used in Step 2 is not exact, we have an integration



                                       7
error  , namely,
                                     =  [·] −                                 (11)
where  [·] is a compact notation for the exact conditional expectation in
(5), and  is the approximation given by (8).
    Second, since we use a finite-degree polynomial for approximation, there
is an error from omitting high-degree polynomial terms (curve-fitting error).
Let Ψ (   ; ) =    be a complete
                                         ¡ ordinary   polynomial ¢of degree 
composed of +1 terms, where  = 1          2   
                                                 2
                                                                         ∈ R1×(+1)
                              0
                   
and  = ( 0   1     ) ∈ R(+1)×1 . Furthermore, let ∞  ∞ be an
infinite-degree polynomial, which is equivalent to the true policy function
 (   ). Then, the error from omitting high-degree polynomial terms is

                         ∞−  ∞− ≡  ∞  ∞ −                     (12)
               ¡              ¢0                    ¡              ¢0
where  ∞ ≡ 0∞   ∞−1 ∈ R ×∞ ,   ≡ 0   −1 ∈ R ×(+1) ,
                                                         0
and   is the initial segment of  ∞ = ( ∞     ∞
                                           0   1  ) ∈ R
                                                             ∞×1
                                                                 .
   If we use an approximating polynomial of an infinite degree,  ∞  ∞ , but
our integration method produces errors of type (11), the regression model in
Step 3 is
                                 =  ∞  ∞ +                          (13)
where  ≡ (0 [·]    −1 [·])0 ∈ R ×1 and  ≡ (0    −1 )0 ∈ R ×1 . Let us
assume that the coeﬃcients in regression equation (13) are estimated using
the ordinary least-squares (OLS) method,
           £           ¤                    £           ¤
       b∞ = ( ∞ )0  ∞ −1 ( ∞ )0  =  ∞ + ( ∞ )0  ∞ −1 ( ∞ )0 
                                                                                    (14)
          ∞
where   b denotes the OLS estimator of  ∞ .5 The more accurate is the
                                                                             b∞
integration rule, the smaller is the integration error, , and the closer is 
to  ∞ .
    Since, in practice, we use a finite- rather than infinite-degree polynomial,
the regression model in Step 3 is misspecified,

                                    =     +                                   (15)
   5
    OLS is used in the discussion of this section for the sake of expository convenience.
In the context of the stochastic simulation class of methods, OLS is numerically unstable.
See JMM (2009, 2010b) for a description of numerically stable approaches that can be
used in the approximation step of a stochastic simulation algorithm.


                                            8
where  ≡ (0    −1 )0 ∈ R ×1 is an error term. Substituting the true
regression model, (13), into the OLS estimator corresponding to (15) and
using (12), we get
        £          ¤
  b = (  )0   −1 (  )0 ( ∞  ∞ + ) =
  
          £            ¤−1  0       £         ¤−1  0 ∞− ∞−
       + (  )0      ( )  + (  )0      ( )        (16)

As a result, the OLS estimator, b , contains errors of both types, (11) and
(12). To measure accuracy of a solution, we compute the diﬀerence in capital
allocations produced by the true policy function,  ∞  ∞ , and our approxi-
                          b ,
mate policy function,   
                                
                          b = −   − [  −  ]  ∞−  ∞− 
           ≡  ∞ ∞ −                                                     (17)
                                                     £         ¤−1  0
where  is a  ×  identity matrix, and   ≡   (  )0      ( ) is
a matrix known in econometrics as a projection matrix.
    Below, we assess the magnitude of errors of types (11) and (12) in our
model. We do not intend to give formal proofs concerning bounds on the
errors but lay out a series of arguments that helps us expose what we find
later to be quantitatively important.

Integration errors Let us consider an infinitely dimensional polynomial
approximation  ∞  ∞ leading to estimator (14), and let us assume that the
integration error,  , is  with zero mean and constant variance,  2 (i.e.,
we neglect a bias resulting from omitting the high-degree polynomial terms).
Under these assumptions, we have the standard version of the Central Limit
Theorem, namely, the asymptotic distributions of OLS estimator (14) and
the capital-allocation errors in (17) are given, respectively, by
               √ ³              ´         ³ £          ¤      ´
                   b −   ∼ N 0 ( ∞ )0  ∞ −1  2                      (18)
                                                             
                           √               ¡       ¢
                                ∼ N 0  ∞  2                          (19)

Thus,
   √ a rate of convergence of our approximate solution to the true solution
is  . This means that to increase accuracy of the solution by an order
of magnitude, we must increase the simulation length,  , by two orders of
magnitude.


                                        9
    Consider the conventional one-node Monte Carlo integration method,
 (1), which approximates the expectation in (5) with the next-period
realization of the integrand, so that the integration error is

                     =  [ (    +1 )] −  (    +1 )         (20)

where  (    +1 ) is given by (6). In a typical real business cycle model,
fluctuations in variables like  (    +1 ) amount to several percents. If in-
tegration error (20) is on average 1%, (i.e.,   = 001), then regression model
(13) with  = 10 000 observations allows      √ us to approximate the conditional
expectation with accuracy of about     = 10−4 . To achieve accuracy of
10−5 , we need to increase the simulation length to  = 1 000 000. Thus,
a very long simulation is needed to approximate the conditional expecta-
tion with a high degree of accuracy if the integration method used is not
suﬃciently accurate.6
    We now present an integration theorem regarding our second integration
method, the Gauss-Hermite quadrature one; see, e.g., Judd (1998, p. 261).

Theorem 1 Under an -node Gauss-Hermite quadrature integration method,
the integration error is equal to
                                                  √
                                                !  (2)
            =  [ (    +1 )] −  =         (    )  (21)
                                               2 (2)! 3
                                                                             (2)
where  (    +1 ) and  are given by (6) and (8), respectively; 3 de-
notes a 2-th order partial derivative of  with respect to the third argument;
and  is some real number,  ∈ (−∞ ∞).

    For a function  that is smooth and has little curvature, integration error
 decreases rapidly
              √       with the number of quadrature nodes, . This is because
the term 2!(2)!
               
                   goes rapidly to zero with . For example, for  equal to 1,
2, 3 and 10, this term is 044, 0037, 00018 and 3 × 10−15 , respectively. In
particular, Gauss-Hermite quadrature integration is exact for functions that
                                                      (2)
are linear in  since for such functions, we have 3 (    ) = 0 for all
 ≥ 1.
  6
    The√Monte Carlo integration method with  nodes,  (), has the convergence
rate of   and has the cost proportional to  . Roughly speaking, an increase in the
number of integration nodes from  = 1 to   1 under   () has the same eﬀect on
accuracy and cost as an increase in the simulation length from  to   under  (1).

                                            10
    The properties of the integration errors produced by Gauss-Hermite quadra-
ture are in general unknown. If  has a∞ non-zero expected value, the Central
Limit Theorem does not apply, and     b does not converge to  ∞ asymptot-
ically. However, provided that the integration errors are very small in size,
our approximation to the conditional expectation will be still very accurate.

Error from omitting high-degree polynomial terms (curve-fitting
error) Suppose now that the only source of errors in (16) and (17) is the
omission of high-degree polynomial terms,  ∞−  ∞− (i.e., we assume that
the integration procedure is exact so that integration errors are absent). Be-
low, we evaluate errors resulting from omitting high-degree polynomial terms
in a one-dimensional case under the Chebyshev polynomial representation;
see Judd (1998, p. 209). Chebyshev polynomial is defined with the following
recursive formula: 0 () = 1, 1 () =  and  () = 2−1 ()−−2 ()
for  ≥ 2.

Theorem 2 Let a function  : [−1 1] → R be  times diﬀerentiable and
have a Chebyshev polynomial representation

                               1      X∞
                         () =  0 +        ()                    (22)
                               2      =1

where { }=0∞ are the Chebyshev polynomial coeﬃcients. Then, there
exists a constant  such that | | ≤  ,  ≥ 1.

    Thus, the Chebyshev polynomial coeﬃcients decline rapidly with the
polynomial degree  if function  () is suﬃciently smooth. Provided that
the coeﬃcients on high-degree polynomial terms are small in size, the er-
ror from omitting such terms,  ∞−  ∞− , is also small in size. Other
polynomial representations (e.g., ordinary polynomials, Hermite polynomi-
als, Legendre polynomials) are linear combinations of Chebyshev polynomials
so that result (22) applies to them as well.

Errors associated with stochastically generated solution domain
In the above discussion, we treated the dependent variables,   , in the re-
gression as being exogenous. However, such variables are constructed by
 endogenously as a part of the solution procedure. It might happen
that the simulated points produced by  do not adequately represent

                                     11
the relevant solution domain. First, the simulation length,  , might be not
suﬃciently large. Second, the solution might depend on a specific random
sequence of productivity levels, { }=0 . Third, the capital series pro-
duced by our approximate policy function,       b , might diﬀer from those
implied by the true policy function,  ∞  ∞ . Finally, stochastic simulation
overrepresents the center of the ergodic distribution and underrepresents the
tails of the ergodic distribution. The above factors can be also important for
accuracy of solutions.


5       Numerical experiments
In this section, we discuss the implementation details of  and describe
the results of our numerical experiments. We first study the representative-
agent model of Section 2, and we later extend this model to include hetero-
geneous agents.

5.1     Representative-agent model
We investigate how the simulation length, polynomial degree and integra-
tion method aﬀect accuracy of the  solutions in the context of the
representative-agent model.

5.1.1    Implementation
We assume a constant relative risk aversion (CRRA) utility function,  ( ) =
1− −1
 
  1−
        , with risk-aversion coeﬃcient  ∈ (0 ∞) and a Cobb-Douglas produc-
tion function,  ( ) =  , with capital share  = 036. We set the discount
factor and depreciation rate at  = 099 and  = 0025, respectively. We
set the damping parameter for fixed-point iteration in (9) at  = 01, and
we set the convergence parameter in (10) at  = 9 (i.e., we target nine-digit
precision).
    We implement one- and -node Monte Carlo and quadrature integration
methods as discussed in Section 3. For a detailed description of quadrature
and monomial formulas and examples of their use, see JMM (2010a). Our
regression method in Step 3 is the numerically stable least-squares method
based on singular value decomposition, SVD; see JMM (2009, 2010b).


                                      12
    For each experiment, we report the time necessary for computing a solu-
tion (in seconds) and unit-free Euler equation errors (defined as the diﬀerence
between the left and right sides of (5) divided by +1 ) on a stochastic sim-
ulation of 1,000 observations. To compute conditional expectation in the
test, we use an accurate ten-node Gauss-Hermite quadrature rule. We run
the computational experiments on a desktop computer ASUS with Intel(R)
Core(TM)2 Quad CPU Q9400 (2.66 GHz). Our programs are written in
Matlab, version 7.6.0.324 (R2008a).

5.1.2   Monte Carlo integration
We first solve model (1) − (3) using  − (). We parameterize the
CRRA utility function by  = 1, and we parameterize the process for shocks
(3) by  = 095 and  = 001. We implement Monte Carlo integration using
 randomly drawn nodes in each simulated point with  ∈ {1 30 3000}; we
vary the simulation length by  ∈ {30 300 3000 10000}, and we compute
polynomial approximations up to degree five,  ∈ {1  5}. −(1)
corresponds to  as is used in JMM (2009). The results are reported in
Table 1.
    When the simulation length,  , and the number of integration nodes, ,
are small,  − () fails to deliver high-degree polynomial approx-
imations and the accuracy of solutions is low. Increasing  and  helps
restore numerical stability and increase accuracy. The highest accuracy is
achieved in the experiment with the largest values of  and  considered
(i.e.,  = 10 000 and  = 3 000) under the third-degree polynomial,  = 3,
namely, the Euler equation errors are of size 10−6 .

5.1.3   Gauss-Hermite quadrature integration
We next solve model (1) − (3) using  − (). We specifically repeat
the experiments reported in Section 5.1.2 using Gauss-Hermite quadrature
integration with  ∈ {1 2 10} nodes instead of the Monte Carlo integration
with  ∈ {1 30 3000} nodes. We report the results in Table 2.
    The accuracy of the solutions increases with the polynomial degree, the
sample size and the number of integration nodes. Any of these three factors
can restrict the overall accuracy of solutions. Specifically, if we use a rigid
first-degree polynomial, the maximum error is of order 10−4 ; if we use a
small sample size of  = 30, it is of order 10−5 ; and if we use the least


                                      13
accurate one-node integration rule, it is of order 10−7 . In turn, under our
most accurate case of fifth-degree polynomial,  = 10 000 and the most
accurate ten-node integration, the maximum error is of order 10−9 . Under
quadrature integration, the simulation length plays a less important role
than under Monte Carlo integration. This is because in the former case, the
simulation length aﬀects only the solution domain, while in the latter case, it
aﬀects both the solution domain and the number of integration nodes. The
key result of Table 2 is that the change in the integration procedure leads
to a large increase in accuracy. The least accurate solution, produced by
 − () under  = 30 and  = 1, is still more accurate than the
most accurate solution, delivered by −() under  = 10 000 and
 = 3000. In order  − () had a comparable degree of accuracy
as  − (), its running time must be of orders of magnitude larger.

5.1.4   Sensitivity experiments
In Table 3, we present the results of the sensitivity experiments, namely,
we consider  ∈ {02 1 5},  ∈ {095 099} and  ∈ {001 003}. We use
 = 10 000, polynomial approximations up to degree five and two alternative
integration methods such as  (1) and  () with  = {1 2 10}.
    As is seen from the table, the tendencies, observed in the sensitivity ex-
periments, are similar to those observed in Tables 1 and 2. Under some para-
meterizations, the diﬀerence between the mean and maximum errors is large
(up to two orders of magnitude), which indicates that the approximation ob-
tained is not uniformly accurate. Since stochastic simulation overrepresents
the center and underrepresents the tails of the ergodic distribution, the ac-
curacy of solutions on tails might be low. To achieve more uniform accuracy,
we should use an approximation method that minimizes the maximum error
instead of the least-squares error. Finally, we find that the results are not
visibly aﬀected by a specific random draw of productivity shocks.

5.2     Heterogeneous-agent economy
In this section, we apply  for solving a multi-country (heterogeneous-
agent) variant of representative-agent model (1) − (3). A formal description
of the heterogeneous-agent model, the parameters choice and the implemen-
tation details are provided in Appendix A. We compute the second-degree
polynomial approximations, and we employ two alternative integration meth-

                                      14
ods, namely, the conventional (one-node) Monte Carlo integration method,
 (1), and the one-node Gauss-Hermite quadrature integration method,
 (1). The results are shown in Table 4.
    Under  (1), the accuracy of solutions visibly depends on the sample
size,  , relative to the dimensionality of the problem,  (the number of the
regression coeﬃcients,  + 1). For a given  , an increase in  leads to a lower
degree of accuracy because there are more regression coeﬃcients to identify;
for example, under  = 10 000, going from  = 2 to  = 30 increases
the mean solution error by about a factor of 10, namely, from 83 · 10−5 to
93·10−4 . For a given , an increase in  leads to a higher degree of accuracy;
for example, under  = 8, going from  = 10 000 to  = 100 000 decreases
the mean solution error by about a factor√of 3, namely, from 24 · 10−4 to
74 · 10−5 . These results are in line with a  -rate of convergence predicted
by the Central Limit Theorem.
    Replacing  (1) with  (1) increases the accuracy of solutions between
one and two orders of magnitude. A reduction in the simulation length from
 = 10 000 to  = 3 000 has virtually no eﬀect on the accuracy of solutions.
With a small number of countries, we can reduce  even further without a
visible decrease in accuracy (these experiments are not reported), however,
we must have at least as many observations as the polynomial coeﬃcients to
identify; for example, for the model with  = 30, we cannot reduce  below
 + 1 = 1 891.
    The computational time for  (1) and  (1) is similar: it ranges from 3
minutes to 3 hours depending on the number of countries, , and simulation
length,  . This time is quite modest given that we use a standard desktop
computer and solve computationally costly high-dimensional problems.
    Finally, in the studied models,  −  (1) delivers accuracy that is
comparable to the highest accuracy achieved in the literature; see KMMP
(2011) for accuracy comparison of diﬀerent solution methods. If  (1) hap-
pens to be not suﬃciently accurate in other applications, we can extend
 to include more accurate integration methods such as low-cost non-
product monomial integration rules; see JMM (2010a).


6    Conclusion
In this paper, we generalize the stochastic simulation approach to make it
compatible with both Monte Carlo and deterministic (quadrature and mono-

                                      15
mial) integration methods. At the conceptual level, our analysis provides a
link between the stochastic simulation and projection classes of algorithms
and makes it possible to assess diﬀerent kinds of approximation errors. At
the practical level, our generalized framework allows us to select the integra-
tion method most suitable for a given application. Any existing stochastic
simulation algorithm can be easily extended to include deterministic inte-
gration. A version of  based on one-node Gauss-Hermite quadrature
integration is particularly easy to implement and is as simple and intuitive
as the conventional stochastic simulation method based on one-node Monte
Carlo integration.  can solve high-dimensional problems that are com-
putationally demanding and even intractable for earlier solution procedures,
and it delivers accuracy comparable to the highest accuracy attained in the
literature.


References
 [1] Christiano, L. and D. Fisher, (2000). Algorithms for solving dynamic
     models with occasionally binding constraints. Journal of Economic Dy-
     namics and Control 24, 1179-1232.
 [2] Den Haan, W. and A. Marcet, (1990). Solving the stochastic growth
     model by parameterizing expectations. Journal of Business and Eco-
     nomic Statistics 8, 31-34.
 [3] Gallant, R. and D. Nychka (1987). Semi-Nonparametric maximum like-
     lihood estimation. Econometrica 55, 363-390.
 [4] Judd, K., (1992). Projection methods for solving aggregate growth mod-
     els. Journal of Economic Theory 58, 410-452.
 [5] Judd, K., (1998). Numerical Methods in Economics. London, England:
     The MIT Press, Cambridge Massachusetts.
 [6] Judd, K., L. Maliar and S. Maliar, (2009). Numerically stable stochas-
     tic simulation approaches for solving dynamic economic models, NBER
     working paper 15296.
 [7] Judd, K., L. Maliar and S. Maliar, (2010a). A cluster-grid projection
     method: solving problems with high dimensionality, NBER working pa-
     per 15965.

                                      16
 [8] Judd, K., L. Maliar and S. Maliar, (2010b). Numerically stable stochas-
     tic simulation approaches for solving dynamic economic models. Manu-
     script.

 [9] Kollmann, R., S. Maliar, B. Malin and P. Pichler, (2011). Comparison
     of solutions to the multi-country real business cycle model. Journal of
     Economic Dynamics and Control 35, 186-202.

[10] Krueger, D. and F. Kubler, (2004). Computing equilibrium in OLG
     models with production. Journal of Economic Dynamics and Control
     28, 1411-1436.

[11] Maliar, L. and S. Maliar, (2005). Solving nonlinear stochastic growth
     models: iterating on value function by simulations. Economics Letters
     87, 135-140.

[12] Maliar, S., L. Maliar and K. Judd, (2011). Solving the multi-country real
     business cycle model using ergodic set methods. Journal of Economic
     Dynamic and Control 35, 207-228.

[13] Smith, A., (1991). Solving stochastic dynamic programming problems
     using rules of thumb. Queen’s University. Economics Department. Dis-
     cussion Paper 816.


7    Appendix A
In this section, we describe the heterogeneous-agent model studied in Section
5.2. For the sake of comparison, we consider the same setup as the one used
in JMM (2010a) and JMM (2010b) for testing the performance of the cluster-
grid and stochastic simulation approaches, respectively (the notation in this
section corresponds to that in JMM, 2010b).
    A world economy consists of  countries. Each country is populated by
a representative consumer. The social planner’s problem is
                                                  "∞               #
                                             X
                                                  X         ¡    ¢
                            max           0                    (23)
                              =1
                     { +1 }=0∞ =1      =0




                                     17
subject to

               X
                               X
                                                X
                                                                        X
                                                                         
                                                                                      ¡ ¢
                         +          
                                      +1   =            (1 − ) +                (24)
                =1             =1              =1                     =1
                                        ¡             ¢
                  ln +1 =  ln  + +1 +  +1 ,  = 1                  (25)
                             ©   ª=1
where initial condition 0  0                 is given, and  ∼ N (0  2 ),   ∼
N (0 2 ). We denote by  , +1   
                                         , +1 and   a country’s  consumption,
end-of-period capital, productivity level and welfare weight, respectively. A
common productivity shock, +1 , and a country-specific technology shock,
 +1 , determines country’s  productivity level. The utility and production
functions,  and   , are both increasing, continuously diﬀerentiable and
concave.
      We assume that  =  and           
                                      P =  for  = 1  , which implies that
                    
 = 1 and that  =  ≡ 1 =1  for  = 1  . We parameterize the
                                                             ³©         ª=1 ´
                                                    
capital policy functions of each country, +1          =                  , by a
                 ³©        ª 
                                      ´
polynomial Ψ    =1 ;   using a set of Euler equations:
             ½ 0                                      ¾      ³©               ´
                 (+1 ) £                ¡  ¢¤                   ª
 +1  =   0             1 −  + +1  +1 +1 ≈ Ψ    =1 ;   
                                         0
                    ( )
                                                                             (26)
for  = 1  . Note that since the countries are identical in their fundamen-
tals, we could have computed just one capital policy function for all countries.
However, we treat the countries as fully heterogeneous and compute a sep-
arate policy function for each country considered. This approach allows us
to make a judgement regarding accuracy and costs in models with hetero-
geneous fundamentals.  for solving the heterogeneous-agent model is
similar to the one presented in Section 3 for the case of the representative-
agent model, however, instead of doing each step just once, we do it for each
country  = 1  .
    We parameterize the model by  = 1;  = 095 and  = 001. We set
the damping parameter for the first- and second-degree polynomial approx-
imations at  = 01 and  = 005, respectively; we use  = 6 (i.e., six-digit
precision) for all experiments. To estimate the regression coeﬃcients, we
normalize the data and use the OLS method. In the studied model, data
normalization is suﬃcient for stabilizing the stochastic simulation approach

                                                       18
under low-degree polynomial approximations; see JMM (2009, 2010b) for
a discussion. An important advantage of linear approximation methods is
that they can be vectorized in multi-dimensional applications; this enables
us to iterate on policy functions of all heterogeneous countries simultaneously
which reduces the costs considerably. To compute the Euler equation errors,
we evaluate the conditional expectation in (26) using a 2-node monomial
rule; JMM (2010a) shows that this rule is suﬃciently accurate for the purpose
of accuracy checks.




                                      19
Table 1. Solving the benchmark model using Monte Carlo integration.
                         T=30                        T=300                          T=3000                       T=10000
                emean      emax    CPU     emean       emax     CPU       emean       emax      CPU    emean       emax     CPU
  Monte Carlo integration with P=1 draw
  1st degree 8,98(-4) 2,16(-3)          1 2,97(-4)   1,36(-3)        4   5,38(-5)    3,05(-4)   1(1)   5,5(-5)    3,0(-4)   1(1)
  2nd degree      -          -        -   3,80(-4)   2,03(-3)        3   9,26(-5)    3,94(-4)   1(1)   4,0(-5)    2,0(-4)   3(1)
  3rd degree      -          -        -      -          -        -       1,03(-4)    5,91(-4)   1(1)   5,1(-5)    3,3(-4)   2(1)
  4th degree      -          -        -      -          -        -       1,38(-4)    1,01(-3)   1(1)   5,4(-5)    9,0(-4)   3(1)
  5th degree      -          -        -      -          -        -       1,53(-4)    1,64(-3)   1(1)   7,1(-5)    5,1(-4)   3(1)
  Monte Carlo integration with P=30 draws
  1st degree 1,28(-4) 7,10(-4)          4 6,00(-5)   3,15(-4)   1(1)     4,40(-5)    2,58(-4)   6(1)   4,7(-5)    3,0(-4)   2(2)
  2nd degree      -          -        -   1,84(-4)   1,02(-3)   1(1)     1,60(-5)    6,68(-5)   4(1)   8,6(-6)    4,1(-5)   2(2)
  3rd degree      -          -        -   2,27(-4)   2,61(-3)   1(1)     1,89(-5)    8,87(-5)   5(1)   1,5(-5)    1,4(-4)   1(2)
  4th degree      -          -        -   5,79(-4)   1,02(-2)   5(1)     2,04(-5)    2,59(-4)   5(1)   1,6(-5)    1,6(-4)   1(2)
  5th degree      -          -        -      -          -        -       2,96(-5)    3,35(-4)   6(1)   1,7(-5)    3,1(-4)   1(2)
  Monte Carlo with integration with P=3000 draws
  1st degree 1,28(-4) 5,26(-4) 4(2) 5,41(-5)         3,82(-4)   6(2)     4,36(-5)    2,86(-4)   3(3)   4,9(-5)    3,1(-4)   7(3)
  2nd degree 7,28(-4) 5,39(-3) 3(2) 1,39(-5)         1,14(-4)   6(2)     1,74(-6)    1,92(-5)   1(3)   1,2(-6)    1,0(-5)   5(3)
  3rd degree 2,08(-3) 9,51(-2) 6(2) 3,07(-5)         3,55(-4)   5(2)     2,29(-6)    4,00(-5)   1(3)   1,1(-6)    9,5(-6)   2(3)
  4th degree      -          -        -   1,32(-4)   3,19(-3)   4(2)     2,45(-6)    5,58(-5)   1(3)   1,1(-6)    1,0(-5)   3(3)
  5th degree      -          -        -   7,66(-4)   2,68(-2)   5(2)     2,51(-6)    2,73(-5)   1(3)   1,2(-6)    1,2(-5)   2(3)
Remark: emean and emax are the average and maximum Euler equation errors, respectively; CPU is computational time in
seconds; and the notation x(m) means x·10m.
Table 2. Solving the benchmark model using the Gauss-Hermite quadrature integration method.
                       T=30                      T=300                           T=3000                      T=10000
               emean     emax    CPU     emean      emax     CPU      emean         emax     CPU     emean      emax     CPU
 Gauss-Hermite quadrature with P=1 node
 1st degree 4,49(-5) 2,66(-4)      2 5,19(-5)     3,62(-4)       5    4,50(-5)    2,90(-4)   1(1)    5,0(-5)   3,0(-4)   5(1)
 2nd degree 1,05(-6) 1,12(-5)      2 1,18(-6)     1,32(-5)       4    9,11(-7)    9,29(-6)   1(1)    1,1(-6)   1,0(-5)   4(1)
 3rd degree 1,92(-6) 3,25(-5) 8(-2) 4,84(-7)      1,45(-6)       1    4,83(-7)    7,07(-7)      8    4,8(-7)   9,5(-7)   3(1)
 4th degree 8,05(-7) 1,78(-5) 1(-1) 4,82(-7)      6,41(-7)   6(-1)    4,86(-7)    6,30(-7)      3    4,9(-7)   6,3(-7)   2(1)
 5th degree 5,57(-7) 7,61(-6) 2(-1) 4,90(-7)      6,63(-7)   9(-2)    4,86(-7)    6,26(-7)      1    4,9(-7)   6,3(-7)      9
 Gauss-Hermite quadrature with P=2 nodes
 1st degree 4,68(-5) 2,71(-4)      2 5,19(-5)     3,68(-4)       4    4,36(-5)    2,85(-4)   1(1)    4,8(-5)   3,1(-4)   5(1)
 2nd degree 9,08(-7) 1,15(-5)      2 1,03(-6)     1,33(-5)       2    7,62(-7)    9,60(-6)   1(1)    8,6(-7)   1,0(-5)   3(1)
 3rd degree 1,34(-6) 2,00(-5) 1(-1) 4,77(-8)      1,15(-6)       2    2,46(-8)    3,56(-7)      8    3,3(-8)   6,5(-7)   2(1)
 4th degree 9,25(-7) 1,71(-5) 1(-1) 3,27(-9)      8,65(-8)       1    1,24(-9)    2,23(-8)      4    2,1(-9)   4,1(-8)   1(1)
 5th degree 3,74(-7) 1,08(-5) 2(-1) 1,57(-8)      4,21(-7)   6(-2)   5,21(-10)    7,22(-9)      2   1,1(-10)   1,7(-9)      9
 Gauss-Hermite quadrature with P=10 nodes
 1st degree 4,68(-5) 2,71(-4)      1 5,19(-5)     3,68(-4)       8    4,36(-5)    2,85(-4)   4(1)    4,8(-5)   3,1(-4)   2(2)
 2nd degree 9,08(-7) 1,15(-5)      2 1,03(-6)     1,33(-5)       8    7,62(-7)    9,60(-6)   3(1)    8,6(-7)   1,0(-5)   1(2)
 3rd degree 1,34(-6) 2,00(-5) 1(-1) 4,78(-8)      1,15(-6)       2    2,46(-8)    3,56(-7)   2(1)    3,3(-8)   6,5(-7)   5(1)
 4th degree 9,25(-7) 1,71(-5) 1(-1) 3,28(-9)      8,65(-8)   7(-1)    1,25(-9)    2,25(-8)   1(1)    2,1(-9)   4,1(-8)   4(1)
 5th degree 3,03(-7) 6,79(-6) 2(-1) 1,57(-8)      4,22(-7)   9(-2)   5,24(-10)    7,00(-9)      4   1,1(-10)   1,6(-9)   2(1)
Remark: emean and emax are the average and maximum Euler equation errors, respectively; CPU is computational time in
seconds; and the notation x(m) means x·10m.
 Table 3. Comparison of the Monte Carlo and Gauss-Hermite quadrature integration methods: sensitivity results.
                       γ = 1/5                    γ=5                        ρ = 0.99                     σ = 0.03
               emean      emax    CPU     emean    emax    CPU     emean       emax     CPU     emean        emax     CPU
  Monte Carlo integration with P=1 random draw
  1st degree    1,9(-5)    1,9(-4)    8 4,5(-4) 3,1(-3)     8(1)   1,1(-4)    7,1(-4)      8   4,50(-4)    3,12(-3)   1(1)
  2nd degree    1,2(-5)    1,2(-4) 1(1) 2,2(-4) 3,8(-4)     2(2)   6,3(-5)    1,4(-4)   4(1)   1,20(-4)    5,39(-4)   5(1)
  3rd degree    1,5(-5)    1,3(-4) 1(1) 2,7(-4) 2,3(-3)     2(2)   1,1(-4)    7,9(-4)   3(1)   1,57(-4)    1,12(-3)   4(1)
  4th degree    1,9(-5)    4,6(-4) 1(1) 3,4(-4) 3,4(-3)     3(2)   1,1(-4)    9,6(-4)   4(1)   1,65(-4)    2,51(-3)   5(1)
  5th degree    2,2(-5)    4,2(-4) 2(1)     -      -         -     1,4(-4)    1,3(-3)   4(1)   2,11(-4)    2,72(-3)   6(1)
  Gauss-Hermite quadrature with P=1 node
  1st degree    1,5(-5)    1,4(-4) 2(1) 4,3(-4) 3,2(-3)     3(2)   1,2(-4)    7,2(-4)   5(1)   5,03(-4)    3,21(-3)   5(1)
  2nd degree    2,2(-6)    4,1(-6) 2(1) 1,1(-4) 2,3(-4)     3(2)   6,0(-6)    3,0(-5)   4(1)   2,94(-5)    3,14(-4)   5(1)
  3rd degree    2,1(-6)    2,3(-6) 2(1) 1,1(-4) 1,4(-4)     2(2)   5,1(-6)    6,8(-6)   3(1)   5,44(-6)    6,43(-5)   4(1)
  4th degree    2,1(-6)    2,2(-6) 1(1) 1,1(-4) 1,3(-4)     2(2)   5,1(-6)    5,7(-6)   2(1)   4,35(-6)    1,24(-5)   3(1)
  5th degree    2,1(-6)    2,2(-6)    4 1,1(-4) 1,3(-4)     1(2)   5,1(-6)    5,7(-6)   2(1)   4,36(-6)    8,17(-6)   2(1)
  Gauss-Hermite quadrature with P=2 nodes
  1st degree    1,4(-5)    1,5(-4) 2(1) 4,5(-4) 3,2(-3)     3(2)   1,2(-4)    7,3(-4)   4(1)   4,91(-4)    3,22(-3)   5(1)
  2nd degree    2,6(-7)    3,2(-6) 2(1) 1,3(-5) 1,6(-4)     3(2)   3,5(-6)    3,3(-5)   4(1)   2,67(-5)    3,23(-4)   5(1)
  3rd degree    1,0(-8)    1,9(-7) 1(1) 7,0(-7) 1,5(-5)     2(2)   2,9(-7)    4,8(-6)   3(1)   3,18(-6)    6,61(-5)   4(1)
  4th degree 4,1(-10)      7,6(-9)    6 6,0(-8) 1,5(-6)     1(2)   3,8(-8)    5,6(-7)   2(1)   5,54(-7)    1,33(-5)   3(1)
  5th degree 8,8(-11) 5,4(-10)        2 5,7(-9) 1,5(-7)     1(2)   5,3(-9)    6,6(-8)   2(1)   9,18(-8)    2,30(-6)   2(1)
  Gauss-Hermite quadrature with P=10 nodes
  1st degree    1,4(-5)    1,5(-4) 8(1) 4,5(-4) 3,2(-3)     9(2) 1,2(-4) 7,3(-4) 2(2) 4,91(-4) 3,22(-3)                2(2)
  2nd degree    2,6(-7)    3,2(-6) 5(1) 1,3(-5) 1,6(-4)     7(2) 3,5(-6) 3,3(-5) 2(2) 2,67(-5) 3,23(-4)                2(2)
  3rd degree    1,0(-8)    1,9(-7) 3(1) 7,0(-7) 1,5(-5)     5(2) 2,9(-7) 4,8(-6) 1(2) 3,18(-6) 6,61(-5)                1(2)
  4th degree 4,2(-10)      7,6(-9) 1(1) 6,0(-8) 1,5(-6)     3(2) 3,8(-8) 5,6(-7) 7(1) 5,55(-7) 1,33(-5)                8(1)
  5th degree 6,5(-11) 5,0(-10)        5 5,4(-9) 1,5(-7)     3(2) 5,3(-9) 6,6(-8) 6(1) 9,18(-8) 2,31(-6)                6(1)
Remark: emean and emax are the average and maximum Euler    equation errors, respectively; CPU is computational       time in
seconds; and the notation x(m) means x·10m.
Table 4. Solving the multi-country model using Monte-Carlo and Gauss-Hermite quadrature integration: comparison results.

                  MC(1), T = 10,000        MC(1), T = 100,000          Q(1), T=10,000                Q(1), T=3000
   J     n 1
                emean    emax CPU        emean     emax CPU        emean      emax      CPU     emean      emax      CPU
 J=2      15    8.3(-5) 1.1(-3) 8(2)     2.6(-5) 2.4(-4) 9(3)     7,19(-6)   4,06(-5)   2(3)   7,31(-6)   3,59(-5)   6(2)
 J=4      45    1.4(-4) 1.3(-3) 1(3)     4.5(-5) 2.7(-4) 1(4)     9,56(-6)   4,54(-5)   4(3)   9,60(-6)   4,59(-5)   1(3)
 J=6      91    2.0(-4) 1.6(-3) 2(3)     5.7(-5) 3.5(-4) 2(4)     1,04(-5)   3,55(-5)   5(3)   1,02(-5)   3,53(-5)   1(3)
 J=8     153    2.4(-4) 1.4(-3) 3(3)     7.4(-5) 4.5(-4) 2(4)     1,09(-5)   3,20(-5)   9(3)   1,06(-5)   3,22(-5)   2(3)
 J=10    231    2.9(-4) 1.7(-3) 5(3)        -        -      -     1,10(-5)   3,01(-5)   1(4)   1,08(-5)   3,09(-5)   5(3)
 J=12    325    3.6(-4) 2.1(-3) 6(3)        -        -      -     1,12(-5)   2,97(-5)   2(4)   1,09(-5)   3,01(-5)   2(3)
 J=16    561    4.4(-4) 2.4(-3) 2(4)        -        -      -     1,13(-5)   3,17(-5)   3(4)   1,11(-5)   3,56(-5)   5(3)
 J=20    861    5.6(-4) 3.1(-3) 2(4)        -        -      -     1,14(-5)   2,93(-5)   5(4)   1,12(-5)   3,75(-5)   6(3)
 J=30   1891    9.3(-4) 5.0(-3) 1(5)        -        -      -     1,15(-5)   2,54(-5)   1(5)   1,10(-5)   3,81(-5)   2(4)
Remark: J is the number of countries; n 1 is the number of polynomial coefficients in the parameterized decision rule of
one country; emean and emax are the average and maximum Euler equation errors, respectively; T is the simulation length;
CPU is computational time in seconds; and the notation x(m) means x·10m.
